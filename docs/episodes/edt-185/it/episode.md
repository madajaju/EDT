---
layout: posts
title: "Storia delle Applicazioni Centrate sui Dati (rivisitata)"
number: 185
permalink: episode-EDT185-it
lang: it
nav_exclude: true
nav_order: 185
tags:
    - virtualization
    - cloudcomputing
    - datalineage
    - datamanagement
    - cybersecurity
    - zerotrust
    - multicloud

date: 2024-02-08T08:00:00.000Z
guests:
    - Darren W Pulsipher

img: thumbnail.jpg
image: thumbnail.jpg
summary: "Il primo episodio di questo podcast è stato pubblicato 185 episodi fa. In questo episodio, il conduttore Darren Pulsipher ripropone l'episodio uno per fornire informazioni aggiornate sulla storia dello sviluppo di applicazioni incentrate sui dati. Discute di come le nuove tecnologie come il computing edge e l'IA hanno influenzato la generazione di dati e la necessità di una migliore gestione dei dati."
video: "https://youtu.be/WGZv8FYSyT4"
description: "Il primo episodio di questo podcast è stato pubblicato 185 episodi fa. In questo episodio, il conduttore Darren Pulsipher ripropone l'episodio uno per fornire informazioni aggiornate sulla storia dello sviluppo di applicazioni incentrate sui dati. Discute di come le nuove tecnologie come il computing edge e l'IA hanno influenzato la generazione di dati e la necessità di una migliore gestione dei dati."
---

<div>
{% include transistor.html id="0bc9a4a7" title="#185 History of Data-centrical Applications (revisited)" %}

{% include youtube.html id="WGZv8FYSyT4" %}
</div>

---

## Elaborazione Precoce dei Dati

Nei primi giorni dell'informatica, le applicazioni erano costruite per trasformare i dati da una forma all'altra in un output prezioso. I primi computer come l'ENIAC e la macchina di Turing per decifrare il codice Enigma funzionavano prendendo dati, elaborandoli tramite un'applicazione e inviandoli alla memoria. Col tempo, la tecnologia è avanzata da hardware specializzato a sistemi più generalizzati con CPU e capacità di rete. Questo ha permesso la condivisione dei dati tra i sistemi, consentendo nuove applicazioni.

## Emergenza della Virtualizzazione

Negli anni '90 e 2000, la tecnologia di virtualizzazione ha permesso l'incapsulamento di interi sistemi in macchine virtuali. Ciò ha scollegato l'applicazione dall'hardware, aumentando la portabilità. Con l'ascesa di Linux, le macchine virtuali potevano ora funzionare su processori x86 di massa, riducendo i costi e gli ostacoli all'ingresso. La virtualizzazione ha aumentato la facilità d'uso, ma ha introdotto nuove preoccupazioni per la sicurezza e le prestazioni.

## L'ascesa del Cloud Computing

Il cloud computing è basato sulla virtualizzazione, fornendo un facile accesso su richiesta alle risorse di calcolo attraverso internet. Ciò ha permesso alle organizzazioni di ridurre le spese iniziali e i costi operativi. Tuttavia, passare al cloud ha comportato sfide in termini di sicurezza, prestazioni e integrazione. Il modello pay-as-you-go del cloud ha abilitato nuovi casi d'uso e ha reso il consumo delle risorse tecnologiche più facile nel complesso.

## Contenitorizzazione e Nuova Complessità

La containerizzazione ha ulteriormente astratto le applicazioni dall'infrastruttura impacchettando le app con i loro runtime, configurazioni e dipendenze, aumentando così la portabilità e la complessità nella gestione di applicazioni e dati distribuiti attraverso vari ambienti. La località dei dati è diventata una preoccupazione chiave, contraddicendo le ipotesi che i dati siano disponibili ovunque. Questa evoluzione ha comportato significative nuove implicazioni per la sicurezza.

## Rifocalizzazione sui Dati

Per affrontare queste sfide, nuove architetture come i mesh di dati e la gestione delle informazioni distribuite si concentrano sulla localizzazione dei dati, sulla governance, sulla gestione del ciclo di vita e sull'orchestrazione. I dati devono essere contestualizzati attraverso applicazioni, infrastrutture e utenti per fornire valore aziendale in modo sicuro. Tecnologie come l'IA stanno guidando la crescita dei dati in modo esponenziale negli ambienti edge. Capacità di gestione dei dati più robuste sono fondamentali per superare la complessità e il rischio.

## Preoccupazioni sulla sicurezza con la distribuzione dei dati

La distribuzione di dati e applicazioni su ambienti edge ha aumentato massicciamente la superficie di attacco. I principi di zero trust vengono applicati per migliorare la sicurezza, con un focus su controlli di identità e accesso, nonché rilevamento, crittografia e radici hardware di fiducia.

## L'Architettura di Edgemere

L'architettura Edgemere fornisce un modello per implementare la sicurezza in stack tecnologici moderni e complessi che includono hardware, virtualizzazione, cloud, dati e applicazioni. L'applicazione olistica dei principi del zero trust a questi strati è fondamentale per la gestione del rischio. Le capacità di cybersecurity robuste come la crittografia e i controlli di accesso sono essenziali per fornire valore aziendale dai dati nella nuova era di sistemi altamente distribuiti e interconnessi.



<details>
<summary> Podcast Transcript </summary>

<p></p>

</details>
