[{"id":0,"type":"Episode","title":"History of Data Centric Architecture","tags":["dataarchitecture","softwaredeveloper","microservice","container","virtualization","technology","compute","data"],"body":"\r\n\r\nIn this episode, Darren talks about the history of applications and how recent changes, primarily due to the onslaught\n\nof data from the Internet of Things, is affecting data-centric architectures. The infrastructure is ready, but we don’t\n\nyet have a suitable way to manage all our data. There are three elements that need to change to facilitate this process:\n\npeople (organization), process (operation), and architecture (technology). Darren focuses on the architecture where data\n\nand compute are spread over thousands of edge devices and across public and private clouds.\n\n## Purpose Built Hardware-software Stacks\n\nHow we deploy applications for missions today has not changed significantly in thirty years. A reference architecture\n\nthat has an application and application stack is built on specific hardware, with compute and storage connected to a\n\nnetwork. This model worked well for a long time, and in fact, about a quarter of applications are still being deployed\n\non purpose-built hardware, but it is not optimal today. Technology moves too fast for this model; drifts happen. In\n\naddition, there are long development times, high costs, limited re-use of the technology, and lack of integration with\n\nother applications.\n\n## Virtualization Architectures\n\nAbout 20 – 25 years ago, hardware virtualization began to resolve some of these issues with the ability to deploy\n\nmultiple applications on one machine. Applications were no longer tied to specific hardware. Instead of buying five\n\nsmaller machines, one larger piece of hardware could be used, not just for compute, but for virtual storage and network\n\nfunctions as well, leading to greater cost-effectiveness. As with any development, this raised some new issues:\n\nincreased security concerns and “noisy neighbors,” meaning one application interfering with the performance of another\n\nbecause of using up IO bandwidth, network, or storage, etc..\n\n## Cloud Architectures\n\nIn the early 2000s, cloud technology took off. We could now share across multiple organizations. Where virtualization\n\ncreated abstraction of hardware, cloud technology created abstraction of operations, making it easier to manage multiple\n\nmachines. The cloud architectural idea created “software defined infrastructure,” which makes it easier to spin up and\n\nspin down compute, storage and network resources. Other benefits are decreased CapEx and OpEx costs, due to less\n\nhardware and manpower. It also gave bursting ability, for example, for retailers during the busy holiday season or the\n\ngovernment during the census. With the progression of this technology, the issues of security and noisy neighbors\n\nincreased because of multiple tenants on the same machine. Another concern is integration costs between public and\n\nprivate clouds. Even with these worries, however, the pros far outweigh the cons in most cases.\n\n## Service and Container Architectures\n\nOver the last five to six years, we’ve seen the reinvention of an old technology: containerization. Docker created an\n\neasier way to use the previously clunky and difficult-to-use container technology. Application developers, in\n\nparticular, embraced this technology because it is consistent across multiple environments. The service management layer\n\nwith containerization of applications and microservices is more application-centric and maps those applications to\n\ngeneric, virtualized hardware that’s been abstracted away. We now have automatic deployment across multiple clouds,\n\nwe’ve optimized OpEx and also CapEx on the application stack and service stack layer. Fault tolerance is automated, and\n\nit’s much easier to integrate with overlay networks, integrate across multiple clouds, create firewalls, do\n\nmicro-segmentation etc… all via software.\n\nSecurity, however, is a primary concern. Since containers are easy to deploy across multiple environments, it’s\n\nimportant to focus on security that is “built-in” to the deployment. Also, there is an increase in complexity. Here,\n\nwe’ve moved away from a three-tier architecture to a multi-tier or even a micro-service architecture with dozens of\n\nservices working together. Another problem is where and how the data is stored and managed. On the service management\n\nlayer, storage is a generic container, which doesn’t manage the data itself.\n\n## Internet of Things Architectures\n\nNow, when Internet of Things (IoT) is added to this ecosystem, the increased volume of data is spread across hundreds or\n\nthousands of smart devices. It also increases visibility, but this creates additional security concerns. Many edge\n\ndevices are accessible by the public. For example, someone could tamper with a smart city light, a smart traffic signal,\n\na drone, or security camera. The complexity of the different devices, their number and locations, along with the immense\n\namount of data, is enormous.\n\n## Data and Information Management Architectures\n\nHow do we work through these issues? Organizations are already changing to handle this complexity with new organizations\n\nand positions foxing on data management use cases. Previously, there was no place for these use cases to be managed, so\n\nwe’ve created a new key layer called the distributed information management layer. This layer manages data across all of\n\nthe IoT, Legacy, and public and private clouds. It matches the data with application stacks and service stacks, so we\n\ncan dynamically allocate services and applications close to the data, or visa versa. Regulations and sheer size of data\n\ncan limit the ability to move data to central locations, as we have done traditionally. With this new architecture,\n\nseveral modes of operations can be utilized including disaggregated analytics, data movement and application movement.\n\nOnce again, with this expanded architecture, security is of utmost importance. Security needs to run as a common aspect\n\nthrough all the layers. Identity security, meaning access, authorization, and authentication of individuals, IOT\n\ndevices, applications, services, and even of data is key. Management of identity includes encryption for trusted data\n\nand devices.\n\n## Conclusion\n\nAll of this architecture together is called the Edgemere architecture. Many of the parts already exist; we need to\n\noptimize how they work together. The newest area is the distributed information management layer (DIML). Fortunately, we\n\nare beginning to see start-ups and more established companies building out the use cases and the architectural elements\n\nin this layer.\n\nThe Edgemere architecture helps identify the key moving parts of a modern digitally transformed system and how they all\n\nfit together.\n\nIntel fits into this ecosystem by providing the key element of a common physical layer to control and manage all of your\n\nresources, whether it’s an IOT device, in the data center, or in a remote location. We make it possible for you to\n\nefficiently move the data, store it effectively, and process everything. Whether it’s the Xeon processors at the high\n\nend, or whether it’s doing inference or AI on the edge at very low power, Intel has a full stack of physical hardware.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT1-en","image":"./episodes/edt-1/en/thumbnail.bmp","lang":"en","summary":"In this episode, Darren talks about the history of applications and how recent changes, primarily due to the onslaught of data from the Internet of Things, is affecting data-centric architectures. The infrastructure is ready, but we don%92t yet have a suitable way to manage all our data. There are three elements that need to change to facilitate this process: people (organization), process (operation), and architecture (technology). Darren focuses on the architecture where data and compute are spread over thousands of edge devices and across public and private clouds."},{"id":1,"type":"Episode","title":"Teleworker Sizing Your VDI Solution","tags":["telework","remoteworker","process","technology","vdi"],"body":"\r\n\r\n# Title\n\n*Tagline*\n\nSummary here\n\n![episode image](./thumbnail.png)\n\nEpsiode Body here.\n\n## Media\n\n<video src='url'></video>\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Illyse Sheaffer"],"link":"/episode-EDT10-en","image":"./episodes/edt-10/en/thumbnail.png","lang":"en","summary":"Organizations need help in building VDI (Virtual Desktop Infrastructure) solutions immediately. As IT Departments are adding VDI licenses locally to their current systems, they need to be aware that licenses alone don’t solve all of their problems."},{"id":2,"type":"Episode","title":"Intel's Public Sector Superheroes","tags":["cybersecurity","multicloud","edge","aiml"],"body":"\r\n\r\ni\n\nCameron started his career working for the largest privately owned staffing company. There, he developed a passion for technology after learning PeopleSoft. That led to his opportunity to help build the world’s first cloud computing company, UC Center Networking. He and his coworkers created the tagline “software as a service” in 1997, seven or eight years before the term “cloud” was coined. He worked for federal systems integrators such as Northrop Grumman and General Dynamics, where he found he was passionate about supporting a mission. This passion has been driving him since he as CTO at Dell Technologies and now leading the public sector team at Intel.\n\nOne of the factors that led Cameron to join Intel was the return of Pat Gelsinger and the leadership team he is building, including the addition of Greg Lavender. Cameron believes Intel will be reinvigorated and continue innovating under Gelsinger. He wants to be part of the work to get Intel back to its status as an iconic American brand. He believes there is a culture of humility and people who do the right thing.\n\n## Ubiquitous Compute\n\nSemiconductors are in everything: cars, appliances, smartphones, computers, health care systems, etc…They span just about every vertical market on the planet. They improve the quality of people’s lives, even saving lives, through health care, national security, and scientific research.\n\nThe COVID pandemic brought this ubiquitous nature to light due to the sheer number of people who could work at home and education continuing online. It also helped close the digital divide in many ways.\n\n## Pervasive Connectivity\n\nSilicon can’t realize its potential without being connected. Intel is not just a chip company but one with a whole interconnecting portfolio.\n\nThere are times when things are disconnected, for example, a warfighter in a DDIL environment, but they can still use the compute locally to execute the mission. When they reconnect, whether legacy 4G, 5G, or the upcoming 6G, they can get up-to-date information, in other words, data transmission. In an education scenario, however, interconnection is vitally essential for things like streaming, video content, and getting access to data. That is a powerful asset.\n\nThe pandemic also brought about a significant uptick in comms and 5G as students and workers needed this connectivity. This continues today, even as students and workers are back onsite. Whereas students may have had periodic access to Chromebooks pre-pandemic, many now have their own that they can take home. Unfortunately, there are still areas in the country and the world where there are gaps, and people cannot participate in the digital economy. Surprisingly, closing the digital divide in the United States is not as easy as in developing countries. They can leapfrog the United States because they invest in 5G and 6G: non-terrestrial comms.\n\nCameron believes connectivity is equally important to computing, as being connected to others is one of the fundamentals of the human experience.\n\n## Edge to Cloud\n\nThe underpinning of edge-to-cloud is computing and interconnect, so Intel plays a significant role in this space along with its partners. Edge to cloud harnesses the true power of not only silicon but software. It creates interoperability. You can move workloads securely and seamlessly from an edge device to a cloud or traditional data center using open standards, core technologies, and an edge-to-cloud strategy.\n\nFor the foreseeable future, the edge will dominate because as things get brighter, technology must be pushed out to the edge where the information is being created. Then processing can happen in a centralized manner for more analytics and AI. Things will only move forward, and the edge will become pervasive.\n\n## Artificial Intelligence\n\nData is everywhere now; data centers do not have walls. It is collected and even processed in myriad ways like cell phones, cameras, industrial motors, etc…Artificial intelligence is all about making intelligent decisions based on all of this disparate data.\n\nIntel has one of the largest artificial intelligence portfolios in the world, an extraordinary software portfolio, and more software developers than many software companies. The reason is, as Greg Lavender points out, the software is the soul of silicon. You have to allow it to do something and give it life and purpose. AI is an excellent example of this.\n\nSometimes people think of AI in terms of robots, but there are countless practical use cases. One example is if you get lost, you can immediately query Siri or Google and, using GPS, the service will geo locate you and find the nearest point of civilization or wherever you want to go.\n\nAnother practical use case is at the US Postal Service. The next generation of delivery vehicles is as modern as the Google Street View cars, with sensors mapping certain things. The postal service applies AI to kinetic intelligent sorting and mail handling machines. They are leveraging technology to scale.\n\nBecause of Intel’s ubiquitous computing and advanced comms, more edge devices are becoming intelligent, and the amount of data that needs to move off the edge into data centers is decreasing. This is because AI algorithms infer what you are looking for on the edge. This kind of technology is built directly into Intel CPUs. They also have specialized XPUs, neurotrophic processors, that do the same at lower wattage and higher speeds.\n\nPart of the power of what happens with Intel and their partners is the ability to have access to all the information in a consumable format. In the example of getting lost and looking to your phone for help, you might be looking at something that has ingested 600 different data points within a split second to give you a simple answer.\n\n## The Superheroes\n\nIntel’s people and its partners in the ecosystem are superheroes. Partners help bring together real solutions, especially in that critical last mile. Intel has one of the best ecosystems to bring solutions to the market. And sometimes, those solutions aren’t even brought to market but are used to help solve challenging problems in defense and the public sector. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Cameron Chehreh"],"link":"/episode-EDT100-en","image":"./episodes/edt-100/en/thumbnail.png","lang":"en","summary":"On this episode, Darren and Cameron Chehreh, Vice President and General Manager of Public Sector, Intel, talk about Intel’s superpowers: ubiquitous compute, pervasive connectivity, edge to cloud, and artificial intelligence."},{"id":3,"type":"Episode","title":"Network Controller Security with Elisity","tags":["networksecurity","cybersecurity","elisity","comms","technology","process"],"body":"\r\n\r\nDana has been working in hardcore networking for the last 15 years. He has worked with software-defined networking and wide-area networking and focused on the SD-WAN world for a time. He then worked with cloud but returned to his roots with network security at Elisity.\n\nDan has worked chiefly in networking, but he started with advanced services at Cisco and did some oil and gas work in Houston. He then moved into the SASE SD-WAN space, where he worked with Dana. He is now focused on network security as well as software-defined security. Dan says the term, however, is elusive because the technology is micro-segmentation through identity.\n\nThe traditional way to create micro-segmentation through VLANs and firewalls is no longer enough. For 15 years, network security has focused on hardening the network’s perimeter. Making an impenetrable wall around an enterprise is still essential, and firewalls do an incredible job of keeping people out of the network. But for the most part, the inside of the network, where there has been an explosion of new connectivity requirements because of IoT, and OT, is a free-for-all once a user is inside. Since the outer wall is robust, anyone inside is often considered a legitimate or trusted user.\n\nThe reality is that most current attacks are happening inside the network via exploited trusted users, devices, and applications.\n\nThere are many reasons why leveraging VLANS, IP ACLs, firewalls, and traditional segmentation methods, don’t work today for lateral movement security. They can work in static environments at a small scale. They work great for the edge of the network and specific places such as bottlenecks or aggregation points, but there are three common reasons why these are no longer ideal.\n\nFirst, traditional methods’ scalability and operational efficiency are questionable. Managing VLANs, IP ACLs, and firewalls across large enterprises is done manually. It is not a distributed software-defined architecture but requires a box-by-box configuration, line by line. They are not dynamic or responsive to anything on the network. Their use also creates a Swiss cheese network full of random holes. Users, especially operators of networks, are often a system’s most significant risk. They may, for example, open up a VLAN or change one piece to do a quick test but then don’t undo the change.\n\nAnother example is that a VLAN will start with a use case and slowly creep to other use cases. Suddenly, what was a ten-device VLAN now has 60 devices. In the OT world, it might have six or seven different processes running inside of it because it was the trusted LAN. Still, often these environments grow slowly and are undocumented, so the risk goes unnoticed.\n\nThe second issue is that VLANs and firewalls are inherently in the wrong place in the network to provide lateral movement security. If you are in the same VLAN as another device, user, or application, that communication channel is open even though it may not have to be. Firewalls are not typically deployed in a strategic place where they can handle the access level of lateral movement. You have to funnel traffic to a firewall and get it back down, which is inefficient. Then you have a bottleneck.\n\nBad actors are looking at networks to see how they can twist them to get some outcome, not how they should or were intended to function. For example, if a user is in a VLAN and a process is running inside a use case, there is nothing to stop them from going from port 3 to port 32 in that same VLAN. Often people design security around intended use rather than how it could be used. It’s common, for example, for software developers to jump ports to work effectively, but that’s dangerous because it leaves them open. No one can place hundreds or thousands of firewalls across the entire access edge. That would be fiscally restrictive and impossible to manage.\n\nThe third problem is that these legacy segmentation solutions don’t consider the identity, context, or behavior of the asset connected to the network. It’s a rigid, network-centric topology that provides some essential security measures. But an IP address says nothing about the asset’s legitimacy and the network it’s attached to. So how can you dynamically secure this network when you don’t know what’s connecting to the network? You can’t make a policy in the first place without any granularity; it doesn’t work now to treat every device as equal.\n\nEven if there is an analysis of the type of traffic, it typically happens several hops up, which means you have exposure now. Any enforcement may or may not be able to protect the infrastructure fully.\n\nThis detection ability still has value, even if there is no protection. Still, the ability to stop something that could have happened right at the edge, as close to the asset as possible, is a better solution.\n\nCheck out the next episode in this series [here](episode-EDT101).\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Dan Demers","Dana Yanch"],"link":"/episode-EDT101-en","image":"./episodes/edt-101/en/thumbnail.png","lang":"en","summary":"Intel’s Darren Pulsipher, Chief Solutions Architect, Public Sector, interviews network security experts Dana Yanch and Dan Demers from Elisity about network controller security techniques and zero trust architectures. "},{"id":4,"type":"Episode","title":"Identity-based Micro-segmentation with Elisity","tags":["microsegmentation","otsecurity","identitybasednetworking","cybersecurity","comms","technology","edge","sdn"],"body":"\r\n\r\nIn part 2 of Security with Elisity, Darren discusses identity-based micro-segmentation solutions with Dana Yanch, Director of Technical Marketing, and Dan Demers, Technical Architect.\n\nElisity’s approach to network security differs from traditional architectures by how it’s delivered, distributed across the network, and done efficiently, providing rapid time value.  They have focused on making it as simple, intuitive, and effective as possible so any industry segment can leverage the technology.\n\nThe key to Elisity’s technology is micro-segmentation, meaning the capability to completely isolate any user, device, or application no matter what type of network it’s on, where it’s placed in the network, or whether it’s managed or unmanaged. In other words, it is flexible to isolate one asset from another without restrictions imposed by network architecture or constructs such as VLANs or VRFs.\n\nTraditional micro-segmentation systems that require hardware replacement or on-prem appliance instantiation can take months or years to get up and run. Still, Elisity provides full functionality within a week by shifting to a cloud-native and cloud-delivered micro- segmentation platform. The entire control management and policy planes are fully cloud-delivered, and the component that ties to your on-prem network is 100 percent software. You don’t have to change anything on-prem, as it rides on top of your existing infrastructure. It scales well, is easy to manage, and is distributed dynamically.\n\nElisity also delivers an on-prem solution for the OT space where organizations can keep it behind their DMZ and cut off all access.\n\nElisity’s platform is a micro service in the sense of horizontal scalability. You can start small and add 10,000 things to the network and scale with you, whether on-prem or in the cloud. It is completely automated across the entire network. There is little friction, and it is easy to manage long-term.\n\nOne significant difference between Elisity’s platform and other overlay networks is that Elisity’s deals with the control and policy plane rather than the data plane. It can apply the same level of network security granularity without touching any packets. It dynamically tells the network who can talk to whom based on the attributes and identities found on the network.\n\nA triangle of users, applications, and devices is an excellent way to visualize this. Within that triangle are all the lines of things talking to each other. Elisity can secure every single line, or channel, between users, applications, and devices with granularity. This happens at the network control layer. At the moment traffic hits the first edge, it’s being secured.\n\nSecurity is essential at the network layer and not the application layer because there can be tens of thousands of devices in your network that you can’t put an agent on or modify since they have things like embedded operating systems, cameras, badge readers, and people. This is especially important on the OT side.\n\nWith OT, the focus is on availability and integrity. Things need to continue to function in a safe process because the process could represent critical infrastructure. This is different from IT, where you can quarantine and spin up a new instance if there's an intrusion. OT is a different mindset. Where something might exist in IT for six months or a year, something in OT might exist for 20 years. Often, these infrastructures are maintained or patched every few years or are hands-off until a failure event. Safety equates to restricting connectivity. For example, if you are in a VLAN, that doesn’t mean it /you? Should be talking to an RTU, a sensor, an HMI, or a DCS that are one switch over.  In legacy design, if those things are on the same VLAN, there is no efficient, flexible way to stop them from connecting.\n\nWith Elisity’s system, you set policies based on things you group things or attributes assigned to assets, rather than going through item by item. This can be as simple as grouping all your processes so they can only communicate north/south, not east/west, or, for example, allowing line managers to share with six or seven types of things. Two or three policies can quickly reduce your attack surface from 65,000 potential attack ports down to two or three.\n\nA use case that is common in OT networks is when a vendor does an update on a device, you need to let them in, and they have access to your whole network. With Elisity, you can easily give them access for a limited time and allow them to touch fixed assets.\n\nSuppose you would like to know more about Elisity’s technology. In that case, many resources, including videos, white papers, and documents, explain how the solution works and how it could be applied within a week in your network at Elisity.com. \n\nCheck out the previous episode in this series [here](episode-EDT101).\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Dana Yanch","Dan Demers"],"link":"/episode-EDT102-en","image":"./episodes/edt-102/en/thumbnail.png","lang":"en","summary":"In part 2 of Security with Elisity, Darren discusses identity-based micro-segmentation solutions with Dana Yanch, Director of Technical Marketing, and Dan Demers, Technical Architect."},{"id":5,"type":"Episode","title":"Operationalizing Your AI Projects","tags":["aiops","devops","compute","technology","process","devsecops","cybersecurity","aiml"],"body":"\r\n\r\nGretchen is an excellent example of someone who continually learns and adapts. Her undergraduate degree is in mathematics. She has a Master’s degree in business, and she completed a program at Harvard just a few years ago focusing on data science which led to her position as Chief Data Scientist at Intel in the public sector. She’s worked in the technology field for over 20 years, starting with software engineering, and spent 15 years in the federal space.\n\nShe finds working in the public sector especially rewarding because it makes a difference in everyday citizens’ lives. In addition, the federal government has the most data on the planet, so it’s perfect for someone who loves to be awash in data and continue to learn more.\n\nThere are many terms surrounding AI. First, it’s essential to understand the difference between artificial intelligence (AI) and machine learning operations (ML ops). ML ops are techniques that are part of AI; they are a subset. ML algorithms derive their strength from an ability to learn from available data. So primarily, you are learning from either supervised or unsupervised data.\n\nThe simple difference between supervised and unsupervised learning is the data label. In supervised learning, the datasets are labeled. This means that what the data looks like is already mapped out. It makes it much easier to classify and predict. In unsupervised learning, you are trying to find patterns in the data; the machine is learning to create relationships between data based on finding common ways, similarities, or differences.\n\nAn example of supervised learning would be an online store recommending an item that a customer might want to buy based on their shopping history or a streaming service recommending a movie based on someone’s viewing habits.\n\nMany terms now have the abbreviation “ops” at the end. For example, people say “DL ops” for deep learning operations, a subset of machine learning. Why the “ops”? First, it’s not as sophisticated as DevOps. Instead, it is influenced by the widely adopted idea of the DevOps approach to creating and customizing applications. People are trying to develop a set of practices to help optimize the reliability and efficiency of machine learning design, development, and execution. So it would be almost like a marketplace where you can create and operate custom applications and then share them with others.\n\nMany models and algorithms are already optimized and available in tools such as Converge.io or C3 AI. These methodologies and technologies can help you streamline your machine learning models. The best way to do that is with many tools that are either open source or specific vendor-created software to make creating, developing, designing, executing, and flow much more accessible.\n\nAI development is similar to where software development was 30 years ago. Many of the steps are still manual and will hopefully be operationalized soon.\n\nIn previous episodes, Darren and Gretchen discussed how many AI and ML projects are science experiments done once. Then the data scientist moves on to something else, and it’s never operationalized. Counter to this, ML ops is moving toward deploying the model to provide real value after training and learning.\n\nSome companies are explicitly leveraging those tools.   Domino Labs, for example, almost creates that marketplace. Work in the public sector, say, on nuclear subs doing object detection or clustering classification, could be applicable in the Air Force or other auxiliary so that work could be cataloged to help operationalize and build agile environments. You could leverage some algorithms and weigh them differently depending on the results. You could tweak it based on the differences in datasets, but at least there are…starting points? Commonalities? Shared tools? Her last words here cut out…..\n\nSecurity is always concerned with open-source software and models, and AI has unique circumstances. For example, how do you know the developer hasn’t trained it to ignore its face in a facial recognition model? There is an expectation now that people document things, for example, where a dataset came from.\n\nThere is also the issue of ethics and responsibility. The Tay chatbot and the bias found in facial recognition programs were great examples of AI gone awry without malicious intent. For a long time in ML ops, it was a single person doing the work and producing the results. Now, the idea is that you need a diverse team of people in different capacities with different worldviews.\n\nThe first conference to discuss AI and ML was in 1956 at Dartmouth College. The truth is that many AI basics, such as log regression, linear regression, clustering algorithms, etc., are math equations that have been around for a long time. Of course, there have been brilliant additional frameworks such as TensorFlow from which to build, but the basics are/were still the foundation. We’ve added computing, storage, 5G, and unique capabilities. Once you’ve done all the training, you’ve got the data and information next to the technology as opposed to having to bring it all to the technology. Bringing the technology to the data opens up some fun and exciting problems we can now solve.\n\nBut conversations surrounding how the model was trained, what was the original data, and accounting for model drift must always be happening. After a time, you need to retrain; maybe you need to bring in a different algorithm or weight the current one differently to get more accurate information because there’s more data and more data that is more diverse. This is all good because it increases your level of accuracy.\n\nSo with the movement toward ML ops, you can do this continuously. Just like software development went toward continuous integration and deployment, the same will start happening in AI or ML, where models will be updated and become more and more accurate. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Gretchen Stewart"],"link":"/episode-EDT103-en","image":"./episodes/edt-103/en/thumbnail.png","lang":"en","summary":"Intel’s Darren Pulsipher, Chief Solutions Architect, Public Sector, and Gretchen Stewart, Chief Data Scientist, Public Sector, discuss operationalizing AI projects."},{"id":6,"type":"Episode","title":"Information-driven Leadership","tags":["data","informationdriven","organizationalchange","radiusag","change","people","process","ceo"],"body":"\r\n\r\nAlthough Betsy is the CEO of Radius Advisory Group and technically retired from the federal space, she still keeps one foot in the public sector through her company which focuses on cybersecurity and cyberspace issues of national importance. Working in both the private and public sectors at the same time has been the most exciting part of her professional journey.\n\nBetsy started out as an active duty service member in the Air Force, switching to the industry while juggling kids and a spouse who was also on active duty. She’s worked in several industries, most recently utilities and energy, and spent a great deal of time at PricewaterhouseCoopers. She returned to the Department of Defense (DOD) when she was selected to work with Secretary Gates as a member of the Defense Efficiencies Task Force.  She stayed under Secretary Pancetta and had an exciting journey, ultimately being appointed Deputy CIO for Business Process and Systems Review. There, she created a data analytics function to provide more transparency on Information Technology costs and potential efficiencies across the DoD.\n\nBetsy's biggest challenge in the role of Deputy CIO, which she thinks is true across the public and private sector, is how to bring new thinking, processes, technologies, and methods of working into the organization. In large organizations, the scope is enormous, and there are many silos, each with its own culture, agendas, budgets, and P&Ls. Situations such as the COVID pandemic, where changes need to happen quickly, are incredibly challenging.\n\nBetsy says that COVID changed the culture in some ways, but in other ways, it caused people to hunker down even more, which is not good. There were many process and technology challenges that everyone learned from and continues to do so. One worry Betsy has is that there is now a new environment and ecosystem, and the return to the office can’t be stuffed back into the old bag as many leaders are trying to do. Although this is very difficult to navigate, and whether leaders consider the new environment good or bad, it can’t be treated the same as before.\n\nOnce leaders make a shift, however, Betsy’s strategy is to sprint everything. She learned this lesson when she was Deputy CIO and was given various projects on top of the underlying mission. She noticed that other teams were repeatedly given 30 days for projects, but her team was given only 10 days. When she inquired, the leadership said they knew her team could accomplish it in 10 days and that they had to make complex decisions that required the information her team could provide as part of the decision-making process, so the faster they could get it, the better. So she told her team that they just had to figure it out. They did, and they got good at it.\n\nA sense of urgency helps people focus and perform, but leaders, Betsy points out, must ensure that they are equipped with the people, resources, and authority to execute something on a short timeline. This leadership support is key to success.\n\nBetsy believes listening to people is essential, but decisions get made at the top, which should generally mean a few people. A matrix organization, which gained popularity in the 90s and 2000s, added layers of complexity, but most situations are already complex, so it still comes down to leadership. Leaders must be inclusive enough to listen to as many people as possible, but it can’t take five months and 150 meetings. They must develop systems and groups that can do ideation, suggest models, and work with each other. Leaders have to have the ability to turn and burn; sometimes, they must act quickly. Often, the answer is fewer people but the right people, less technology, and the right technology to get what you need.\n\nBetsy used a practical model when she received urgent projects from the Deputy Secretary of Defense to create small teams of five to seven people with diverse cognitive skills from among her 50 analysts. This was successful because she knew her analysts’ personalities and skills well and could cherry-pick teams rather than taking a problem to a huge group and trying to get everyone’s input. The small teams repeatedly surprised her by accomplishing complex tasks and solving problems.\n\nBy setting up processes and templates to solve problems, Betsy’s team often provided input to the Deputy Secretary of Defense or the CIO within hours if necessary. Ultimately, they had processes in place that enabled them to do the ideation to come up with neutral, data-based options based on many considerations. This allowed the leaders to look at the facts and evidence and make decisions.\n\nBetsy had faith in her teams and never told them how to do the work or assumed she knew the best way to approach things. She had good people and just trusted them to do it, This motivated her people, and they would be constantly anxious for new assignments. Employees want to know that leaders listen to them and consider and use their ideas. This approach allowed people to tell her their best analysis and options. It was often a combination of those analyses that made it back to the leadership. Credit goes to the leadership in the DOD and the CIO’s office, which trusted Betsy’s process.\n\nEvery time there was a new problem, Betsy assigned a new team. That way, different people could work on different types of projects and not get pigeonholed into one area, and people could work across the generational divide. Sometimes people were initially resistant to working with age groups outside their own, but in the end, they learned to see things through different, beneficial lenses.\n\nSince Betsy and her deputy got to know their people well, she could work quickly to put effective teams together. One hallmark of her success is that whenever she asked a team to work on two things, no one ever said no. They just did it.\n\nClick here for the second half of Darren’s discussion with Betsy Freeman. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Betsy Freeman"],"link":"/episode-EDT104-en","image":"./episodes/edt-104/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel talks with Betsy Freeman, CEO of Radius Advisory Group, about her experience as an information-driven leader in the public and private sectors. Part one of two."},{"id":7,"type":"Episode","title":"Information-driven Leadership Part 2","tags":["change","people","changeagent","radiusag","organizationalchange","informationdriven","leadership"],"body":"\r\n\r\nAlthough Betsy is the CEO of Radius Advisory Group and technically retired from the federal space, she still keeps one foot in the public sector through her company which focuses on cybersecurity and cyberspace issues of national importance. Working in both the private and public sectors at the same time has been the most exciting part of her professional journey.\n\nBetty started out as an active-duty service member in the Air Force, switching to the industry while juggling kids and a spouse who was also on active duty. She’s worked in several industries, most recently utilities and energy, and spent a great deal of time at PricewaterhouseCoopers. She returned to the Department of Defense (DOD) when she was selected to work with Secretary Gates as a member of the Defense Efficiencies Task Force.  She stayed on under Secretary Pancetta and had an exciting journey, ultimately being appointed as Deputy CIO for Business Process and Systems Review. There, she created a data analytics function to provide more transparency on Information Technology costs and potential efficiencies across the DoD.\n\nBetsy's biggest challenge in the role of Deputy CIO, which she thinks is true across the public and private sector, is how to bring new thinking, processes, technologies, and methods of working into the organization. In large organizations, the scope is enormous, and there are many silos, each with its own culture, agendas, budgets, and P&Ls. Situations such as the COVID pandemic, where changes need to happen quickly, are incredibly challenging.\n\nBetsy says that COVID changed the culture in some ways, but in other ways, it caused people to hunker down even more, which is not good. There were many process and technology challenges that everyone learned from and continues to do so. One worry Betsy has is that there is now a new environment and ecosystem, and the return to the office can’t be stuffed back into the old bag as many leaders are trying to do. Although this is very difficult to navigate, and whether leaders consider the new environment good or bad, it can’t be treated the same as before.\n\nOnce leaders make a shift, however, Betsy’s strategy is to sprint everything. She learned this lesson when she was Deputy CIO and was given various projects on top of the underlying mission. She noticed that other teams were repeatedly given 30 days for projects, but her team was given only 10 days. When she inquired, the leadership said they knew her team could accomplish it in 10 days and that they had to make complex decisions that required the information her team could provide as part of the decision-making process, so the faster they could get it, the better. So she told her team that they just had to figure it out. They did, and they got good at it.\n\nA sense of urgency helps people focus and perform, but leaders, Betsy points out, must ensure that they are equipped with the people, resources, and authority to execute something on a short timeline. This leadership support is key to success.\n\nBetsy believes listening to people is essential, but decisions get made at the top, which should generally mean a few people. A matrix organization, which gained popularity in the 90s and 2000s, added layers of complexity, but most situations are already complex, so it still comes down to leadership. Leaders must be inclusive enough to listen to as many people as possible, but it can’t take five months and 150 meetings. They must develop systems and groups that can do ideation, suggest models, and work with each other. Leaders have to have the ability to turn and burn; sometimes, they must act quickly. Often, the answer is fewer people but the right people, less technology, and the right technology to get what you need.\n\nBetsy used an effective model when she received urgent projects from the Deputy Secretary of Defense to create small teams of five to seven people with diverse cognitive skills from among her 50 analysts. This was successful because she knew her analysts’ personalities and skills well and could cherry-pick teams rather than taking a problem to a huge group and trying to get everyone’s input. The small teams repeatedly surprised her by accomplishing complex tasks and solving problems.\n\nBy setting up processes and templates to solve problems, Betsy’s team often provided input to the Deputy Secretary of Defense or the CIO within hours if necessary. Ultimately, they had processes in place that enabled them to do the ideation to come up with neutral, data-based options based on many considerations. This allowed the leaders to look at the facts and evidence and make decisions.\n\nBetsy had faith in her teams and never told them how to do the work or assumed she knew the best way to approach things. She had good people and just trusted them to do it, This motivated her people, and they would be constantly anxious for new assignments. Employees want to know that leaders listen to them and consider and use their ideas. This approach allowed people to tell her their best analysis and options. It was often a combination of those analyses that made it back to the leadership. Credit goes to the leadership in the DOD and the CIO’s office, which trusted Betsy’s process.\n\nEvery time there was a new problem, Betsy assigned a new team. That way, different people could work on different types of projects and not get pigeonholed into one area, and people could work across the generational divide. Sometimes people were initially resistant to working with age groups outside their own, but in the end, they learned to see things through different, beneficial lenses.\n\nSince Betsy and her deputy got to know their people well, she could work quickly to put effective teams together. One hallmark of her success is that whenever she asked a team to work on two things, no one ever said no. They just did it.\n\nClick [here](episode-EDT104) for the first half of Darren’s discussion with Betsy Freeman. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Betsy Freeman"],"link":"/episode-EDT105-en","image":"./episodes/edt-105/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel continues his talk with Betsy Freeman, CEO of Radius Advisory Group, about her experience as an information-driven leader in the public and private sectors. Part two of two."},{"id":8,"type":"Episode","title":"The Birth of Graph Intelligence Platforms","tags":["data","graphintelligenceplatform","graphdb","katanagraph","technology"],"body":"\r\n\r\nGreg began his career at an investment bank in credit risk when they started implementing CCAR stress testing. After being heavily involved in that for some time, he began consulting and was introduced to graphs while doing credit risk model validation. He saw how graphs could be leveraged for many different kinds of analysis and had benefits in data management and machine learning, specifically in credit modeling. From there, he found his way to Katana.\n\nData analysts and data scientists are constantly struggling to integrate different data sets. Greg was drawn to graphs because after being introduced to RDF, a semantic kind of knowledge graph format, it made intuitive sense how the data could be combined and structured as a graph.\n\nWith existing graph solutions, analysts had difficulty scaling their solutions because much of their data was so big. Katana Graph developed the ability to scale and also focus on machine learning.\n\nAt the beginning of graph databases, large companies like Amazon and Facebook built in-house graph databases, doing their modeling and machine learning. Then came consumer versions of platforms such as Neo4j and TigerGraph for general use cases. The challenge was that they were centered around the database and not so much the analytics and machine learning, the processes, and actual graph computing. They were limited to being a kind of data store, focusing on the ingest and the CRUD operations and not as much on the data.\n\nThere are three different kinds of graph computing domains. The first is graph query, the graph database, and CRUD operations. The second is graph analytics and mining with PageRank, or clustering, algorithms, which are becoming popular. The third area is graph AI and machine learning. This is where graph neural networks come into the picture. There are point solutions that will solve specific parts of those domains, but Katana Graph sits at the intersection of those.\n\nEach of the three platforms is important. To do machine learning, you need the other two domains. When the data is first ingested, it must undergo many transformations to prepare it for machine learning, so if you don’t have all of this in one solution, the pipeline will be slow, sending data out and back in. It’s most efficient to iterate on the whole pipeline quickly. It also reduces the risk of losing data because you are decreasing the number of times you transform the data.\n\nIn addition, since Katana Graph is a cloud-native platform, you can pause, save a checkpoint, spin down the cluster, and spin it back up later, right where you left off.\n\nA good demo is for fraud detection with a Bitcoin transaction data set. The platform ingests the data, which is structured, so the Bitcoin wallets are the nodes in the graph, and then the edges between them are the transactions. It’s a simple graph. The idea is to predict if a Bitcoin wallet is fraudulent. Illicit wallets for money laundering, drug trafficking, etc., have been labeled. When a new account comes in, the task is to predict if it’s fraudulent. The challenging part, then, is to do some pre-processing with the numeric features of the accounts. A set of APIs has been designed to address that problem. All the things that data scientists do to prepare their features are done here. From there, the graph is ready to put into the machine learning model, where it is trained, and then, using neural networks, you can learn how to classify the accounts.\n\nA benefit of the platform is that data scientists and engineers can work from one platform rather than piecing things together.\n\nAnother benefit is the total cost of ownership. Unlike other platforms, you do not have to keep the whole graph database running all the time. Since Katana Graph pipelines are designed with a separation of storage and compute, you can easily spin up a cluster, do some batch processing beforehand, and then run inferencing in a separate system and still leverage what was generated in the graph.\n\nKatana Graph is also faster with large data sets because it does not load all the data upfront but has an innovative, dynamic way to load the data as you need it as you work through the pipeline.\n\nBasic analytics are much easier on the graph database rather than on a relational database. If you have ten different datasets, it can be cumbersome and error-prone for an analyst to figure out how to join them to write a query in a relational database. With a graph, you have a singular model, already predefined and built, so the questions will be much easier because the data is already connected. You can intuitively see how the information is related.\n\nOne of Katana’s new features is a Dash data frame importer. Dash is a common framework data scientists use for parallel processing data frames. The data scientists can work with the data frame they are already using and directly ingest it into Katana Graph for a seamless, simplified experience.\n\nDevOps is a big part of what Katana is trying to facilitate with their platform. They easily integrate into existing learning pipelines. When the graph neural networks are run, the embeddings can be exported. These features that a graph generates can go downstream to a machine learning process. So the integration becomes a lot simpler and a lot easier to operationalize and put into production.\n\nOver the next five years, Greg envisions organizations such as banks having centralized repositories to analyze customer, marketing, or credit data for multiple purposes. The output from machine learning models could be used for both credit risk and fraud detection, for example. Instead of using siloed data sets with a lot of replication and duplication between them, there would be a common model synchronized within a graph. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Steck"],"link":"/episode-EDT106-en","image":"./episodes/edt-106/en/thumbnail.jpg","lang":"en","summary":"Intel’s Darren Pulsipher, Chief Solutions Architect, Public Sector and Greg Steck, Senior Director of Industry Solutions, Katana Graph, talk about the benefits of Katana’s graph intelligence platform."},{"id":9,"type":"Episode","title":"Securing Critical Infrastructure","tags":["criticalinfrastructure","hotms","irdeto","otsecurity","edge","cybersecurity","technology","process"],"body":"\r\n\r\nCarla is originally from Mexico, living in Amsterdam after living in several countries, including Germany, over the past four years. She studied industrial and mechatronic engineering, then after working for a few years in the field, she earned an M.S. in mobility systems engineering, where she focused on autonomous vehicles.\n\nThe definition of OT critical infrastructure is any point that can trigger chaos in the real world. This is very different from IT infrastructure; IT lacks vital infrastructure. In the OT world, people can die if things go wrong. There can undoubtedly be chaos in the IT world, but the problem can be solved by fixing things. When there is chaos in critical infrastructure such as transportation, accidents can happen, and people’s lives are at stake.\n\nThere has been an uptick in the importance of security in critical infrastructure in the last five years, and certainly during the COVID pandemic; critical infrastructure has been relentlessly attacked in some cases. This can be attributed to people becoming bored and more creative, but also because the attack surface increased with the sudden shift to remote work, which broke down some of the security measures previously in place.\n\nIn transportation, part of the issue is the increase in connectivity, which brings potential attacks. Customers want more services, and companies want more data and access to information. With that comes opening up the transport network. The air gap, which previously offered safeguards, is diminishing.\n\nThe industry cannot use security measures that IT has been using for years because IT and OT are entirely different.  It is generally standardized, whereas OT is not. OT has a massive ecosystem with significant differences in devices that follow other protocols and implementations. For example, every country has different implementations. OT is also on another level because lives are literally on the line.\n\nAnother difference between IT and OT critical infrastructure is that in IT, if there’s a problem in your network, you can isolate it and even shut it down and move the workload elsewhere. That is not possible in critical infrastructure, so the approach is different.\n\nThe danger is that IT and OT networks have collapsed because of the desire for more connectivity, and at the same time, there has been an increase in cyber threats. Irdeto seeks to educate the industry on the complexities of these problems and offer solutions. It’s all about preventative solutions, not reacting after a disaster strikes.\n\nCarla says organizations must have experts for security. Developing security in-house based on mandated standards is not good enough as the standards run behind the development.\n\nIrdeto has been securing critical infrastructure for 50 years. They have about 1,000 employees, and 70 percent of them are in research and development to keep on top of what type of attacks are happening today and what kind of attacks can exist in the future. Irdeto strives to be one step ahead or future-proof. Their services evolve as the system evolves.\n\nIrdeto can help customers who know what they need, whether PKI, keys and credentials, lifecycle management, or software protection. They can also help customers who do not know what they need and provide lasting solutions as threats evolve.\n\nFor more information about Irdeto, go to their website www.Irdeto.com/connected-transport to learn more about their \n\nproducts or contact them directly. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Carla Trevino"],"link":"/episode-EDT107-en","image":"./episodes/edt-107/en/thumbnail.png","lang":"en","summary":"Intel’s Darren Pulsipher, Chief Solutions Architect, Public Sector, and Carla Trevino, Solutions Architect, Irdeto, talk about the importance of security in critical infrastructure."},{"id":10,"type":"Episode","title":"History of Advanced Communications","tags":["5g","cellphone","comms","wifi6"],"body":"\r\n\r\nThe first generation of cell phone technology, the Advanced Mobile Phone System (AMPS), was developed in the late 70s and early 80s. In the early 80s, making a call from your car with a bulky bag phone was a luxury. The luxury of making a mobile device call soon became a necessity.\n\nIn the early 90s, technology progressed as the Global System for Mobile Communications (GSM) standard developed to describe the protocols for 2G, which became the global standard by the mid-2010s. 2G started to turn the mobile phone into something with more capabilities than just making a call, adding texting, and even playing games.\n\n3G launched in the early 2000s and brought some nascent data capabilities with the internet, which is still in its early stages. Wi-Fi was not broadly available, but you could, for example, access a carrier’s data network by connecting a phone to a laptop. You could do minimal, of course, with modem or DSL speeds.\n\nWith 4G, technology transitioned into a unified standard, converging CDMA and GSM into one LTE under the 3rd Generation Partnership Project (3GPP). Every carrier started to adopt this common standard. This was when broadband proliferated. Leland credits the economy’s advancement through the 2010s to 4G, allowing companies such as Amazon, Netflix, and Uber and platforms such as YouTube, Google, and Facebook to exist and thrive.\n\nLeland talks about 5G in terms of what the carriers have deployed. 4G and 5G are related because they are part of the same spec line release. Fourteen ends what we call 4G LTE advanced. Fifteen kicks in with 5G NR. In this crossover, there is a business objective and a strategy to adopt the new technology as part of the standard. The business objective is that companies have already invested in their 4G networks, so the current evolved packet core and RAN components of the 4G networks are still in place. They add a 5G RAN box with a different frequency but is still connected back to the 4G core, called non-standalone.\n\nDarren clarifies that 4G was revolutionary because it unlocked many new things and required all new equipment, whereas 5G is more evolutionary because it also opened new things. Still, the underlying technology is based on the same hardware and core.\n\nIt’s part of the modulation scheme that 5G provides in the air interface, but the architecture is different; it’s virtualized under 5G compared to more proprietary under 4G. That leads to many capabilities becoming part of the 5G deployments.\n\nOne example is a carrier deployed a 4G network by putting a RAN box next to an old 3G box. Many companies, such as Sprint, kept their 3G boxes and CDMA network up for years. In reality, 4G was just another box sitting next to a 3G box. 5G takes that proprietary box and gives the ability to spread the functions of that box across a virtualized network. Part of the baseband of 5G can now be software-defined in scale to multiple areas compared to being contained at one site, box, or location.\n\nThis means you can add functionality to your network without replacing hardware. As you go into standalone networks, however, you can take a 5G network and do something onsite. For example, suppose you have a skyscraper instead of depending on the network coverage from an antenna sitting outside with a core back at the carrier or a switch station. In that case, you can develop an on-premise network built within that building that proliferates coverage and data services throughout.\n\nThis standalone network opens up many new capabilities and enables new players. It also allows organizations such as the federal government and the Department of Defense to adopt the technology for their use cases. They have more flexibility when they are not highly dependent on the carriers.\n\nAnna notes that in addition to new players and new on-prem capabilities, there is also the ability to use the CBRS spectrum. The way it is managed is complex, but there is non-priority that you can use for free, and priority, the Navy spectrum, that you can buy if you need no disruption. Some extensive manufacturing facilities are using the CBRS spectrum, either working with a primary carrier that does not charge for scope or working with a new entrant who will set up an on-prem standalone network with CBRS. This is a very different model, and there are real advantages to the wavelength and complexity of the systems that you can set up with 5G over Wi-Fi.\n\nThere are still some advantages to Wi-Fi, but setting up a robust Wi-Fi network can be challenging, especially if you are moving large pieces of metal around. If you have a set configuration, it makes sense to go with Wi-Fi 6, especially if the economics work.\n\nDemand drives change; most end users are comfortable with 4G on their personal devices. So why go to 5G? The value 5G brings isn’t necessarily the higher data rates and lower latency; those services are provided at a broad scale because it’s virtualized. 5G is software intensive compared to 4G, which is more about proprietary boxes and based on hardware. 5G can be virtualized in position in many places. The frequency portfolio is dynamic, and you can utilize unlicensed bands, licensed bands, and CBRS, so there are many more options.\n\nCheck out the second part of this interview [here](episode-EDT109).\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown","Anna Scott"],"link":"/episode-EDT108-en","image":"./episodes/edt-108/en/thumbnail.png","lang":"en","summary":"In this episode, Darren talks to frequent Intel guests Leland Brown, Principal Engineer: Technical Director of Advanced Communications, and Dr. Anna Scott, Chief Edge Architect for Public Sector, about the history of advanced comms."},{"id":11,"type":"Episode","title":"Advancing Operations with 5G","tags":["5g","comms","wifi6"],"body":"\r\n\r\n5G opens up a lot of capability. You can now set up private networks, which is software-defined so that you can add more functionality to your network. Where does this enabling technology take us?\n\nAnna says that although we are still in the early days from a bandwidth and latency standpoint, there are real advantages to 5G, such as the opening up of the spectrum, like CBRS, the ability to take advantage of existing user equipment, and the ability for customers to have mobile access.\n\nOne use case that is not sexy but has vast ramifications is that you can take your laptop onto the factory floor and use that for full connectivity. Instead of going out on the factory floor with a notepad and paper and transferring the information back to the office, you can converge the two environments. The 5G standards enable this, but it isn’t necessarily a full 5G deployment.\n\nOne evolution that is happening is the ability to stream high-definition video from a wireless camera over 5G and have low enough latency that you can do real-time analytics on it. Currently, not many 5G cameras can operate in that environment, so cameras are often hardwired close to coupled edge computing to get that real-time aspect, but this advantageous streaming option is coming soon.\n\nAnother example of a 5G advantage is using 10 to 20 AR headsets instead of one or two with Wi-Fi. The key point is the MEC (Mobile Edge Compute) that provides the ability to have the applications on-premise instead of having to go back to the switch or the core of the carrier and have that RAN time.\n\nUnderstanding how frequencies are deployed is essential in use cases, as some carriers have deployed mmWave despite challenges; n41 and n42 frequencies react differently in the real world. So in on-prem deployments, the MEC and the RF design are extremely important.\n\nA compelling use case for 5G outside the factory is using drones in emergency responses. An early predecessor to the eventual ability to bring in drones and assess a damaged area is getting drones connected, understanding a mission to do a flyover, and then pulling all of that data together. While we’re unable to stream live video from multiple drones and stitch it together, we are close to gathering, combining, and analyzing that data, just not yet in real time.\n\nAnother use case is using 5G, AI, modeling, simulation, and edge computing for training across many industries, including the Department of Defense. There’s a huge advantage in creating a realistic training simulation without putting the person under threat or spending massive amounts of money on live training.\n\nAs much as 5G enables this type of use case, much depends on 5G connecting to a MEC instead of 5G going up to the cloud. Physics comes into play. You need super low latency, so you can’t have an architecture that goes from a headset to the cloud, down to a MEC, then to visualization. It needs to go from the headset to the MEC, where real-time processing happens. Then you can share data through the cloud for a real-time experience.\n\nThere is also the capability of meshing or clustering MECs together, so that data may never have to go to the cloud. The MECs can do all the processing and analytics out on the switch. This could enable advances such as smart buildings and cities. This kind of 5G-enabling technology is the perfect storm for considerable changes in the industry.\n\nLeland points out that the story of the new networks is distributed computing. Everything is connected through wireless connectivity, but compute points are spread across the landscape, where apps sit at the edge and enable the use cases. Where we are heading is to calculate wireless one-to-one.\n\nWhat role does Intel play in 5G? It’s far beyond just providing chips. Since 5G is software-designed, Intel has enabled the ecosystem to build or design on top of its L15. When moving from 4G to 5G, Intel took the functional block of the RAN, called FlexRan, and allowed companies to design their baseband architectures and virtualize them. Writing the FlexRan reference architecture made it much easier for new entrants to pick that up as a starting point.\n\nOn the hardware side, Intel spent many cycles to ensure that the commercial, off-the-shelf hardware would work well to support all of the basebands, RAN applications, and servers. The new systems that come when walking away from proprietary systems must be easily supported by the same type of server that works in the cloud and the data center because now you have the scale and the cost advantage.\n\nThis will drive down prices and drive more innovation in the industry. \n\nCheck out the first part of this interview [here](episode-EDT108).\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown","Anna Scott"],"link":"/episode-EDT109-en","image":"./episodes/edt-109/en/thumbnail.png","lang":"en","summary":"Intel’s Darren Pulsipher, Chief Solutions Architect, Leland Brown, Principal Engineer: Technical Director of Advanced Communications, and Dr. Anna Scott, Chief Edge Architect for Public Sector, talk about the history of advanced comms and future use cases with 5G. Part two of two."},{"id":12,"type":"Episode","title":"Distance Teaching and Learning","tags":["remotelearning","compute","technology","people","edge","telelearning"],"body":"\r\n\r\n## Emerging Considerations<h2>\n\nTeachers, staff, parents, and students are all facing different challenges in the sudden switch to distance learning. On the IT side for school districts, there is a host of emerging considerations. What do we do with the scenario of bring your own device (BYOD) coming from zero trust networks? How do we protect privacy and manage security with all the various new modes of communication among the teachers, staff, parents, and students? How do we maintain a streamlined, managed classroom experience? How do we offer support when a traditional helpdesk structure does not exist? In addition, we can’t forget that there is an important social aspect that must drive a seamless experience. A third grader being distracted by technical issues when they need to connect with teachers and peers will have a diminished distance learning experience.\n\n## Systems Services, and Platforms for Education<h2>\n\nThere has always been complexity with all the different layers of services and platforms, for example, the productivity suite with G Suite and Office 365. The question now is what can we do with learning management systems in this layered approach in the integration of the student information system? We have to look at how we are leveraging our capabilities in regard to scalability. We must consider different infrastructure as a service (IAAS) and platform-as a service (PAAS) solutions, storage services, privacy and security, and, of course, the underlying platforms that drive all of it.\n\nPreviously, conferencing and collaboration tools had limited use, perhaps to bring in a guest speaker, for example. Now, however, they are being heavily used as primary tools and have added to the complexity of the system.\n\n### Service Hosting Options<h3>\n\nThere are two main modes that are used to access services: device as a portal to services (software, infrastructure, or platform), and device as part of the internal network. Previously, most fell under the latter category, where there are limited concerns about things such as patching and policy compliance because devices are constantly connected to internal school sites regardless of whether they are BYOD or district-owned assets. Now, with the different types of connectivity, we need to be concerned about bandwidth scalability and how we implement it.\n\n### Device as a Portal to Services<h3>\n\nPlatforms such as G Suite, Office 365, and those for conferencing and collaboration are a concern in that they create dependency on a third party. School districts don’t have control over security, privacy, and performance. It’s important to recognize that the connection to these cloud services connect back to the internal host, whether a private cloud or on-prem data center, that has underlying services of identity management, student information systems, possible content filtering, etc. The benefits, however, are decreased inbound traffic to the data center and inherent scalability and manageability.\n\n### Device as Part of the Network/Private Cloud<h3>\n\nImplementing a virtual private network (VPN) is a new idea for most school districts. Enterprise has been using VPNs for some time, and school districts may have to follow suit to meet the new needs of distance learning.  A few downsides to a VPN are network congestion, scalability, and traffic from zero trust networks. The main benefit is that it functions as an extension of the internal network, so security management and traffic encryption are extended to the VPN clients. Accessibility to all the services needed internally is another great benefit.\n\n## Emerging Bottlenecks<h2>\n\nBottlenecks look different for distance learning. For enterprise, when the workforce went virtual, an expected ratio was about 10% VPN load. For education, that number is going to be significantly higher, creating a possible VPN bottleneck. Hosted services scalability is another area of consideration. Even if services sit in a public cloud, they connect back to a private cloud or on-prem data center for things like SSO, student information, traffic, and even content filtering. The dedicated internet access bandwidth, as well as how the bandwidth handoff in the data center is handled, are important considerations.\n\nAccess to a help desk component must also be scalable to prevent bottlenecks.\n\nThe foremost concern of school districts currently, however, is getting all of their students access. For some students, there’s a bottleneck just getting on the internet at home. When this problem is added to all the different layers, the emerging bottlenecks become very complex.\n\nTo combat these potential issues, school districts must take the time and resources necessary to create a solid architecture that will be resilient rather than constructing a hasty spaghetti mess.\n\n### Scalability<h3>\n\nHistorically, school districts have scaled up in the data center and scaled out some of those services to the school sites. So the architecture generally exists in terms of accommodating the aggregation of faculty, staff, and students for things like authentication, patch management, software distribution, etc. Now that the scale-out is handled in the virtual classroom, it brings us back toward scaling up services in the datacenter.\n\n### Finding the Balance<h3>\n\nPreviously, only the two factors of on-prem in the data center central office and the LAN links to the school sites were in play. Now there is the additional factor of accommodating all of the virtual classrooms, meaning each student’s and teacher’s living room. How do we accommodate that? It comes down to the primary concept of finding the balance of what you need for your school district. Each district is different in size and technology literacy, and there are a lot of moving parts.\n\n### Addressing Bottlenecks<h3>\n\nWhat can Intel do to help address these bottlenecks that we have identified?\n\nEndpoint management and endpoint security is where the IT help desk comes in. We are looking at ways to be able to offer support remotely rather than with regular visits to school sites.\n\nWe can look at the data center and infrastructure and develop a strategy that will allow us to scale software defined networking and infrastructure. In addition, we can also integrate some infrastructure-as-a-service cloud bursting, all while taking into consideration the traffic patterns.\n\n## Intel Components to Address Bottlenecks<h2>\n\nIntel can help in three major categories: compute, storage, and network. When we look at bolstering software-defined infrastructure and the important considerations, it revolves around that computer with Intel processors, storage products, and network capabilities. Rather than being tied to and limited by physical interfaces and appliances, software-defined infrastructure can scale and bring in other compute, storage, and network resources. No one is certain what the landscape will be when we return to school, and this software-defined infrastructure is dynamic and will lend the most flexibility.\n\nThere are many options for school districts to construct a viable, secure environment for distance learning. Although we touched on the topics of privacy and security threats and solutions in this episode, next time we will take a deep dive into these important topics.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Erin Moseley","Grant Kelly"],"link":"/episode-EDT11-en","image":"./episodes/edt-11/en/thumbnail.png","lang":"en","summary":"In this episode, Erin Moseley, Sr. Account Exec for Education at Intel, and Grant Kelly, Solution Architect for Education at Intel, join Darren to talk about the challenges of distance learning and teaching and the overwhelming changes that school districts, teachers, parents, and students are absorbing during the Covid-19 pandemic. Find out how students and teachers are connecting with new technologies and ways of learning."},{"id":13,"type":"Episode","title":"Securing the Supply Chain","tags":["securesupplychain","cybersecurity","supplychain","chipsact","policy","process","compute"],"body":"\r\n\r\nIn this episode, Darren talks with Lieutenant General Thomas Horlander, who recently joined the Intel Public Sector Team, about the microelectronics supply chain and national security.\n\nThomas joined the army in 1983 after earning his bachelor’s degree in finance and deciding that the private sector was not for him. He joined the army for what he thought would be a three- to five-year career and ended up with 39 years of service, with his final position as Army Comptroller. He retired about a year ago and joined Intel.\n\nThomas is inspired at Intel by the great people, culture, and meaningful mission. He appreciates the fundamental role that Intel plays in everyone’s everyday lives and the opportunity to influence the country’s future.\n\nWhen Thomas joined the army in 1983, microelectronics was embryonic; they didn’t even have computers. As a young officer, he worried whether he had enough D-cell batteries to operate radios. So Thomas considers himself a “digital immigrant” since he has experienced the evolution that happened with silicon.\n\nSociety and the military are now dependent on silicon, and the supply chain around microelectronics is critical: it’s a national security issue. Thomas says the microelectronics industry shares center stage with the oil industry as a center of gravity regarding national and global security and economic stability.\n\nIn the military, the vehicles and weapons systems are all microelectronics-enabled. They allow the military, for example, to be more precise and lethal with less weight, a more accurate locating system, and more reliable communications.\n\nOur dependency on microelectronics spurred the recent Chips Act. While the need was apparent before the Ukraine crisis, witnessing what Ukraine has been able to do because of microelectronics spotlighted the necessity of securing the supply chain.\n\nThomas has been a student of national security his entire career, and he looks at it more holistically than just defense and the role of the armed forces. To him, national security is good governance and the rule of law. It’s a good and functioning economy, practical academia, health care, and, of course, the armed forces. And almost all of the professions across American society play a role in providing national security. From this perspective, just about every fiber it takes to provide national security relies on microelectronics.\n\nThe Chips Act was necessary, Thomas says, because of the massive imbalance across the ecosystem of the industry. At the same time, not the ultimate solution to the redistribution of balance, the Chips Act is an essential first step and will impact national security. When you unpack the core activities of the microelectronics industry, from where the rare earth elements come from to who makes the equipment to design and manufacturing, it’s apparent what an incredible mosaic of activity this is and why it’s so difficult to have a clear picture on all of it. A microchip might change hands ten times in a manufacturing process.\n\nCOVID, in many ways, exposed this complicated and fragile supply chain when, for example, factories were shut down in Malaysia, Ireland, or China because of a COVID outbreak, and all of a sudden, you can’t ship a car because it doesn’t have a chip in it. Most people don’t realize the significant global imbalance that currently exists. Only eight percent of silicon is manufactured in the United States. Seventy or eighty percent is manufactured in Southeast Asia, precisely three countries: China, South Korea, and Taiwan.\n\nWith this knowledge, it is evident that rebalancing the global supply chain ecosystem and returning capacity and capability to the United States is of the highest importance. No industry should have single points of failure, a concern in the microelectronics industry.\n\nThe federal government, the defense industrial base, and the ecosystem are all starting to see this problem, and the Chips Act is representative of this recognition that we have to do something. Thomas knows six companies right now that have said they will be investing in fabs on US soil in the next eight to ten years.\n\nOne of those companies is, of course, Intel. Intel is currently building fabs in Ohio, Arizona, and New Mexico. Re-domesticating capacity and capabilities in the microelectronics industry are necessary for national security but will bolster the local economies and provide opportunities for workers, invigorating whole communities.\n\nThe infusion of capital investment from the Chips Act is fundamentally essential because this is a race against time. Thomas is optimistic about the industry’s future and the action being taken to assure a bright future and continued innovation. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Thomas Horlander"],"link":"/episode-EDT110-en","image":"./episodes/edt-110/en/thumbnail.png","lang":"en","summary":"In this episode, Darren talks with Lieutenant General Thomas Horlander, who recently joined the Intel Public Sector Team, about the microelectronics supply chain and national security. "},{"id":14,"type":"Episode","title":"Realizing Smart City Potential","tags":null,"body":"\r\n\r\nEric had a long career in the data center and, more recently, in the digital signage space. His passions are project management and technology deployment and operation. Eric and his team were part of an LG Electronics joint venture company that deployed outdoor digital signage, primarily for advertising applications in dense urban areas, transit venues, and city streets. They learned a lot about other applications in those environments that compute resources and other forms of connectivity that would help enhance the experience brought on by IoT. SmartPoint.io was born out of that ecosystem and a desire to work with and drive opportunity and advancement in that market space.\n\nIn the data center, Eric was part of a lifecycle solutions company that helped companies optimize server storage and network hardware from an acquisition, a refresh cycle, and everything in between. His role was focused on helping the channel of companies that sold those solutions, but also how they were purchased, financed, and how they depreciated over time, so he learned not just about the technology but how people were using it and managing that value as an asset on their balance sheet.\n\nWhen people talk about smart cities, they think of resilience, equity, efficiency, and cost-effectiveness. Cities provide essential services such as roads, public safety, water, and sewer. The primary thing a municipality brings to its citizens is starting to include connectivity, the ability to access computing, and taking advantage of technological advancements to help provide basic services more efficiently. Smart technology has been expensive and risky, and few municipal budgets have room for it.\n\nEric’s background in finance is an asset in creating and deploying smart city technology in a partnership that provides an affordable model. Being able to deploy the current significant funding for infrastructure projects in resilient and economically sustainable ways is a major tenet of what SmartPoint does when partnering with a city.\n\nTo achieve economic sustainability, Eric’s team took a hard look at digital signage, primarily for advertising, which generates a tremendous amount of revenue. Some of the largest companies in the world are primarily funded by advertising revenue, and one of the primary products or resources is the people to whom they are advertising. Layering attribution, customers’ traits, and likenesses increase revenue.\n\nCities have a treasure trove of value in the people, commercial entities, and visitors that reside within their real estate. So the core of what SmartPoint.io is doing is leveraging that real estate to generate revenue but directly aligning it with solutions that produce meaningful outcomes in city services. Getting this technology into the cities requires help from the private sector and willingness from the public sector to work together.\n\nAdvertising revenue, along with being able to tap into the budgets of city services, allows a financial model that raises the tide for all boats, from providing safer transportation to greater returns on advertising. When you tie the advertising benefit back to the people to whom the advertising is directed, you create a circular local economy that increases value and efficiency.\n\nEdge computing is a significant part of SmartPoint.io’s product. The digital signage box is also like a portable data center. Data does not have to go back to a large data center somewhere but is processed at the edge. For example, video data from the ubiquitous cameras in a city to watch traffic patterns is bandwidth-intensive. Still, the data transport is significantly reduced if it is cached, stored, or processed locally.\n\nThree great things about these boxes are that they do a great job of reducing latency, cost and increasing privacy. The privacy issue seems counterintuitive, but the video data, for example, is not traveling over public IP. Analysis happens at the point of capture, so it is not transported across the pipe, observed, or taken several hops where security needs to be tracked. Instead of the image, it sends the event “A person walking down the street.” This data can be scrubbed and anonymized much earlier in the analysis cycle before it leaves the box, protecting people’s privacy.\n\nThe boxes could also provide services such as Wi-Fi or 5G hotspots to underserved areas.\n\nOnce the boxes are deployed, SmartPoint.io has a team that monitors and operates the technology, just like a data center would for a company. They also partner with advertisers. Many advertisers are interested in migrating from static “paper poster” advertising to digital. SmartPoint.io is a great partner for those advertisers because they come with upfront CapEx funding.\n\nWith the cities, the model is revenue share or paid in kind. Instead of just sharing top-line revenue, however, SmarPoint.io produces and operates resources such as IT infrastructure that the city can then use to process optical data at the edge.\n\nDigital signage can be interactive, which benefits people trying to find services, restaurants, shopping, events, and entertainment. Commercial tenants can use it as a portal for new economic development because they can reach the people walking around. In turn, the boxes give back valuable information, such as how many people were downtown or if an ad campaign successfully brought them there.\n\nThis model will help smart cities because it defers the cost of CapEx and the OpEx of managing the hardware out in the wild and the refresh cycles. Taking risks on expensive, relatively new technology is also not a normal behavior for a municipality, so partnering with the private sector with a significant appetite for risk, know-how, and funding helps the cities to mitigate the risks and costs and get further out in front of where they would be typically comfortable.\n\nIn the end, Eric says that successes are starting to build in this space, and they build on one another. Everyone is learning new things, and it’s an excellent opportunity to come together and build community, which is what cities are all about.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Eric Hornsby"],"link":"/episode-EDT111-en","image":"./episodes/edt-111/en/thumbnail.jpg","lang":"en","summary":"In this episode, Darren talks with Eric Hornsby, CEO of SmartPoint.io, about technology for realizing smart city potential."},{"id":15,"type":"Episode","title":"Myths of Lift and Shift Cloud Migration","tags":["multicloud","cloudmigration","cloud","compute","process","technology"],"body":"\r\n\r\nJohn started his career in technology at the help desk at a principal defense contractor 20 years ago. As his career progressed, he got into cybersecurity and enterprise architecture. He worked as a contractor for the Defense Information Systems Agency (DISA), where he was the lead architect for the Department of Defense DISA cloud. Eventually, the state of Maryland brought him in to lead the digital transformation efforts for the state, including cloud migration. That migration was the largest ever done across state and local education.\n\nAfter that, he transitioned to the state Chief Information Security Officer (CISO) position, overseeing high-level operations, security, and governance. He left government service and joined WWT about three years ago. He works primarily in state and local education, although he helps in other public sector areas.\n\n## Myth One - Cloud is Cheaper\n\nThe cloud is not necessarily cheaper than an on-premise environment. Legacy applications were built for the on-prem environment, so there is no issue with auto-scaling. It is a  consumption-based model, and there are already sunk costs, such as the servers. Most products meant to help organizations move applications to the cloud don’t support auto-scaling, so when they are forced to the cloud, they now have to be max provisioned at all times, which doesn’t often translate into cost savings. Customers are likely paying more than they were for the same capabilities on-prem.\n\nWhen Darren worked with the Canadian government, they moved an SAP instance into the cloud. It was max provisioned, running 24/7, and they blew through their budget in just six months. When they discovered this problem, they turned it on and off every day since the instance didn’t need to run 24/7. They saved a lot of money by reducing it to 14-16 hours a day.\n\nAccess must be available at all times in a department such as Health and Human Services, which is typically the largest in a state IT budget, but it is possible to bring it down to a minor instance during off hours to save money.\n\nThe Cloud Service Providers (CSP) now offer cloud-native services that are being fulfilled by a third-party application or OEM product that can provide a similar capability with cost savings.  This won’t work with everything, as some legacy applications are not developed to leverage some of the cloud-native applications. It would be best to be careful about getting locked into a specific cloud. If, for example, you use proprietary AWS services, it could be challenging to extricate an application and move to Azure and vice versa.\n\nJohn advises organizations to ask if it makes sense to move into the cloud. It’s not a good reason to move to the cloud because you think you should. You might end up with elevated expenses and frustrating superiors and workers because the strategy was not thoroughly thought through.\n\n## Myth Two - Cloud Erases Technical Debt\n\nMoving into the cloud does not eliminate technical debt in almost any case. It exposes and accelerates the debt. You will find exposure points if you take something running for 30 years and move it into a new environment. The acceleration part is that you have a more technical debt to worry about now that it's exposed.\n\nTechnical debt means you have systems falling behind what they should be. For example, when John first went to work for Maryland, some systems were still running on green screens. It was easy for current employees to navigate but had a steep learning curve for new users. Although the system had been working for many years, the downside of this technical debt is the amount of training necessary and retaining employees. More recent generations coming into the workforce who are knowledgeable on the latest trends and developments don’t want to deal with legacy applications.\n\nTechnical debt also means security issues. If a legacy application has not been updated, you may not be able to apply patches for fear of breaking it. This creates security vulnerabilities you must accept until you get out of the technical debt cycle.\n\nA common reaction in an organization is to bolt on a bit of extra code when necessary to accommodate, for example, a rule change from a state legislature. This does not fix a problem; instead, the system ends up with a lot of spaghetti code, making it impossible to recreate the system for an update. One of the concepts in the cloud is breaking up your system into modules or microservices, but the spaghetti code does not allow for this since you can’t just pull out a piece of it.\n\nThis makes organizations even more hesitant to modernize because they’ve been doing things the wrong way for all these years. When something does break and becomes the impetus for this change, it’s even more challenging.\n\nSometimes it might make sense to scrap the old system and start fresh. This is costly, and you must have the new system before you toss the old system. For some organizations, however,  such as states, which can get federal funding for departments such as Health and Human Services, it might be the best choice. In a situation like this, you can also figure out the benefit of reusability, such as templates and governance structures for other departments.\n\n## Myth Three - Cloud is Secure\n\nMoving into the cloud does not necessarily make securing applications easier, although you don’t have to worry about physical security or hypervisors, for example. Cloud providers have a shared responsibility model in various forms. You have to understand what you are responsible for with each provider and what they are responsible for. It doesn’t just change with the provider but also with what services you consume. This can make security more complex for your security teams because they must stay on top of all the different variations across providers and services.\n\n## Myth Four - Cloud is Easy\n\nMigration to the cloud is complex. It is, in fact, easier to run everything in your own data center, not connected to the internet. Security is easy this way, and cost models are simple. However, you can’t grow. You can’t provide services to your constituents or customers, and you can’t satisfy mission needs, among other issues. The world is complex, and migration to the cloud is complicated.\n\n## Myth Five - A New Skillset is not Necessary.\n\nA knowledge gap can also add to the pain points around that complexity. Software developers and other IT specialists must change how they think about computing in the cloud, especially surrounding security. For example, software developers shouldn’t be spinning up instances in the cloud wherever they want or downloading things from GitHub or other repositories, grabbing libraries to make things work. This opens up all the firewall rules because they may not pick correctly. Guardrails must be implemented when moving to the cloud, which requires change. Working in the cloud requires a different skill set and mindset. Most importantly, you must figure out better ways to manage security with sophisticated ransomware and cyber attacks.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","John Evans"],"link":"/episode-EDT112-en","image":"./episodes/edt-112/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel, and John Evans, Chief Technology Advisor, WWT, discuss five lift and shift cloud migration myths."},{"id":16,"type":"Episode","title":"Operationalizing Business Process Management","tags":["bpm","automation","compute","management","camunda","capitalbpm","rpa"],"body":"\r\n\r\nMax calls himself a “failed academic” because he left his Ph.D. in mathematics, specializing in topology, to study computer science. He earned an undergraduate and a master’s degree, specializing in AI. He then worked in business process management (BPM), starting at Lombardi, which was sold to IBM, and a few other vendors in the BPM space. Ten years ago, he took a plunge with friends and founded Capital BPM.\n\nMax chose to focus on BPM for two reasons. First, he has always liked algorithms because it provides a systematic approach to solving complex problems. Algorithms give him a sense of security during crazy situations.\n\nSecond, he likes what is called transformations in mathematics. For example, if you have an ugly shape with many corners that is hard to measure, you would transform it into, say, a rectangle and then apply all the theories around rectangle measurement, measure it, and translate the answer back to the original shape. In the same way, in BPM, you can take a problem to a domain where it can be easily solved. Rather than making an issue a micro problem with complicated nested F statements that are difficult to maintain, you can transform it into a flattened-out problem that you can see and attack in stages.\n\nThis process is like zooming in on Google maps to see where you need concentrated focus and then zooming back out to see how that fits into the big picture. The heart of enterprise architecture is the ability to zoom in and out to ensure that line you’re drawing is still valid.\n\nMax likens his fondness for BPM within the computer science world to martial arts, which he has studied since he was six. He says there are practical martial arts, such as judo and muay Thai, and more esoteric ones, such as tai chi. He likes pragmatic martial arts because they solve real-world problems. He doesn’t get into fistfights anymore, but, for example, his judo skills help him when he slips and falls. BPM is pragmatic in that it is the key to solving a business problem. He believes there is actual value in using all the theories he learned in school and making them subservient in creating a business platform that allows people to more efficiently and consistently solve everyday problems, thereby giving people and the community more opportunities.\n\nThis is one area of computer science that can be attached to how people work. Things can be automated to reduce the amount of repeatable and mundane tasks so they can focus on more important things. People worry that automation will swallow up everyday jobs, but instead, it removes drudgery and frees up time for more critical work. It can also create jobs. These same fears existed when Ford automated computers were introduced, but they ultimately made new industries. BPM should be fully embraced rather than feared.\n\nThe best way to start operationalizing processes is to use the scientific method of articulating the problem. In business process management, you draw pictures via a business process modeler. Max likes Camunda Business Process Modeler, which is downloadable for free with just a little notation to learn. In the modeling program, you draw steps that articulate the different systems and how they work.\n\nIn a hiring process, for instance, you start with a pool that defines interested parties, such as the candidate, the IT manager, and HR. Inside the pool are “swim lanes,” each containing one player who can do things. You can think of these as LDAP groups. Then you start laying out your business process: first, a candidate applies for the job, then HR might do a review, then an IT manager will review. Approved decisions are noted along the way. After the top-level, significant steps are in the model, you can get down into more detail regarding more articulate and nuanced processes, like a split interview, one for technical and one for management.\n\n![bmp image](./bpm.png)\n\nThe modeling program generates XML behind the scenes as you draw all of these diagrams. This XML is run time interpretable by BPM machines; as you are drawing the diagram, it can become an executable process.\n\nThe human element is still in this loop but using a BPM system like this makes it clear where processes can be automated, such as checking job history or running a criminal background check. The model also allows flexibility and experimentation. For example, suppose the subject matter expert says that they don’t want to run a job check and criminal record check simultaneously because the criminal record check is expensive, and the job check is cheap. In that case, it’s easy to change to move the job check first and require a decision before the criminal record check. As changes are made, you build consensus and a true story that becomes progressively truer the more you experiment.\n\nWhile the tool looks like a drawing tool, it is a modeling tool that lets you draw pictures and simulates them on the back end. So you can run this process and see all the different decision points and where they lead. The model will also tell you that you can’t deploy if you haven’t done something correctly.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Max Young"],"link":"/episode-EDT113-en","image":"./episodes/edt-113/en/thumbnail.png","lang":"en","summary":"In this episode, Darren discusses business process management and automation with Max Young, CEO of Capital BPM"},{"id":17,"type":"Episode","title":"Automating Business Process Management","tags":["compute","businessprocessmanagement","capitalbpm","bpm","automation","camunda","rpa"],"body":"\r\n\r\nBMP modeling reminds Darren of when he took drafting in high school, and AutoCAD's introduction of computer-aided drafting systems changed the game.  Before, they would have pages and pages of complex systems and diagrams so people could build, but they couldn’t test the model to ensure it was right. Using computer modeling, they could run simulations to ensure there weren’t problems such as electrical and plumbing running through the same hole.\n\nThis is analogous to architects using PowerPoint to show business processes instead of using a modeling tool that can find conflicts and issues in what you thought the business process was.\n\nUsing a business modeling tool also solves a practical problem by eliminating the wasted time of getting all the interested parties together for meetings that may need to be more productive. Instead, you can put a deployed model into the hands of the business customer and work through the steps with them.\n\nAfter articulating and modeling the processes, you can choose integration points that might attach to restful interfaces that get information and pump it back in. This is how business processes can integrate with microservices in the cloud. In the example of the hiring process, these points might be where you need an API to invoke checks for job history or criminal record. The inputs will be items such as social security numbers and dates of birth, and the outputs will be a Boolean - does the information match or not? This is where. You can start having that iterative conversation.\n\n![bpm image](./bpm.png)\n\nThere are quite a few manual steps in this process, and you can choose which ones to automate. For example, if you decide that an interview didn’t go well, you can default to HR. After you deploy that new process, you can go back and look at the previous version if you want to, so you have two concurrent versions of the software working and deployed at the prototype.\n\nThe Camunda modeler is a native modeler, but Capital BPM has built its own applications that help streamline some of the work and support different user roles.\n\nThis system is different from RPA because instead of capturing what a user does with keystrokes, a business analyst looks at the processes and steps across multiple departments. The analyst is looking top-down at the whole process. An RPA can be plugged into some steps for efficiency. A simple example is if a job applicant passes the job history check and criminal record check, it can go to a senior HR person; if not, it’s rejected. Choosing specific steps, or sets of steps, to automate is an iterative approach that has been used successfully in software development for some time.\n\nRPAs can be game changers, but they are tactical and short-term. While these short-term gains may be profitable, you need to look at the entire business process to find optimization and steps you can eliminate. The story of the woman who always cut the roast before baking it because that’s how her mother did it is analogous to some company processes. The woman finally asked her mother why she cut the roast, and her mother replied, “So it would fit in my pan.” Many company processes are only there because they’ve always been done that way, and no one has thought to question why.\n\nTesting, simulation, moving things around, and running processes repeatedly in the modeler, in other words, empirically testing, can help eliminate this process bloat and add significant value. Visualization and experimentation are vital parts of the whole process.\n\nMax points out that there is fidelity between the diagram and the actual execution. Developers often draw diagrams as a starting point. Still, the charts disappear as development moves through the different parties, so what the business thinks is happening and what is happening are different. The diagram and reality are separate.  In this type of modeling, the picture is always an accurate representation of what is happening. In addition, it’s easy to see and make changes for improvement. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Max Young"],"link":"/episode-EDT114-en","image":"./episodes/edt-114/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel, and Max Young, CEO of Capital BPM, discuss operationalizing business process management with modeling programs."},{"id":18,"type":"Episode","title":"Blocking and Tackling of Security","tags":["zta","zerotrustarchitecture","cyberhygiene","cybersecurity","technology","process"],"body":"\r\n\r\nFoundational to all other cyber security is basic cyber hygiene. Many companies need to do these basics. This is evidenced by recent news headlines showing an uptick in attacks like denial of service attacks which should be easy to prevent.\n\nFrom his experience working with the state community, John believes most attacks follow a typical kill chain. Most attacks hitting state and local governments result from exposed network protocols or email phishing. These are attractive entry points for hackers, and once they are in, bad patching practices are a typical culprit that allows them to gain a foothold and move laterally. That, combined with weak password policies or weak enforcement of password policies and an inability to recover, can lead to disaster.\n\nIn a well-publicized ransomware incident in 2019, the affected organization assumed that since they had the same amount of data in their production and backup environments, they were safe. But they had never tested their backups or recovery capabilities, which turned out to be poor. Basic cyber hygiene could have prevented this incident.\n\nThere are four essential basics that every organization should focus on. First, they must repeatedly train people to avoid phishing scams. Training might seem repetitive or mundane, but people falling for these schemes is a significant weakness in an organization. Hopefully, in the not-too-distant future, passwords will no longer be necessary.\n\nSecond, they must configure firewalls appropriately; just because RTP or network protocol ports are closed doesn’t mean there isn’t an open port in a less prominent spot. Security by obscurity doesn’t work.\n\nThird, they must avoid bad patching policies, both with the client and the server, in the data centers and out at the edge. Many organizations are in technical debt and can’t update their old systems, so they accept the vulnerabilities and risk because they don’t want to invest in an update.\n\nFourth, they must have the ability to recover. Just because you know you can back up your data, can you use and recover with the backup? Testing is essential.\n\nThese four basics, along with a few others, are enough to stop almost all attacks coming into organizations that aren’t regular targets. That model doesn’t hold with organizations hit with nation-state attacks; they are doing all these things already and need additional security measures.\n\nA consideration for many organizations is compliance versus risk. For some organizations to be compliant, they need to upgrade old machines, applications, and processes, which involves a significant cost. For organizations with a system that can’t be patched, it could take a risk-based approach that if something happens to the system, it would cost less than what it costs to upgrade the system. Of course, the secure thing to do would be upgrading to compliance, but most people think the risk-based approach is more secure. A small business could get away with this approach, but government organizations, for example, have regulatory compliance.\n\nThere are two reasons an organization might choose compliance other than a mandate. First, it’s an easy button for many organizations that don’t understand how to measure or prioritize risk. Compliance is a generalized framework to fall back on. It is not one-size-fits-all, however, because someone else is prioritizing risk in a generalized way.  Second, if something terrible happens and you have to, for example, explain it to your board of directors, you can say that you followed accepted standards.\n\nCompliance is a bit of a crutch mentality because you don’t have to do all the risk evaluations and figure out what needs to be done. But, for example, a small municipality without a CISO could direct a sysadmin to use a compliance framework as a good starting point. If there is no CISO on hand, there is also the option of a part-time virtual CISO for guidance. John does this for clients, which is a viable path to better security.\n\nThe concept of zero trust also looks at a level of assurance versus risk. You need to understand the risk of granting someone access to a particular system or piece of data and then have a commensurate assurance that the person is whom they say they are. The heart of zero trust is a high level of security that mitigates risk.\n\nZero trust does not mean everything will be locked down and slow all processes. If, for example, someone wants to get in and see rainfall levels, you don’t need a high level of assurance that the person is verified. Still, if someone wants to access your organization’s crown jewels, there must be additional controls to verify identity.\n\nMatching the level of assurance with the level of risk is challenging; it requires decision-point architecture. In the example of the risk it has in accessing a piece of data, an organization needs to know what and categorize it based on risk. For a mature organization, this can be difficult. John knows of one federal government organization that spent over two years ensuring its data was identified, classified, and tagged correctly before moving on to any decision-point type of architecture.\n\nIdentity and data are the two starting points for zero trust. In addition, it makes sense to avoid trying and doing everything at once. Starting with a piece of an organization might make the most sense, scaling it out through the rest of the organization over time.\n\nDigital identity is becoming more sophisticated. John believes our transactions in the future will be primarily based on a zero-trust type of approach. For example, if he wants to transfer $10,000 out of his bank into an offshore account, the bank should make sure it is him and treat that transaction as if someone is trying to access a very sensitive, high-risk piece of information. If he goes to the store to buy a dollar cup of coffee, that level of assurance that it’s him purchasing the coffee is unnecessary. Many of these zero-trust principles will make their way into our everyday lives.\n\nUser behavioral analytics will also come into play. Just as a credit card company will raise a flag for unusual purchases, for example, if a system knows that John types 20 words per minute, and then all of a sudden, he’s typing 100 words per minute and trying to access sensitive information, there’s a red flag.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","John Evans"],"link":"/episode-EDT115-en","image":"./episodes/edt-115/en/thumbnail.png","lang":"en","summary":"In this episode, Darren talks about cybersecurity with returning guest John Evans, Chief Technology Advisor at World Wide Technology (WWT)."},{"id":19,"type":"Episode","title":"An Argument for a Holistic approach to Critical Infrastructure Security","tags":["criticalinfrastructure","iot","it/otconvergence","otsecurity","cybersecurity","edge"],"body":"\r\n\r\n## There is a real threat to Critical Infrastructure\n\nAccording to Dr. Scott, OT organizations still use the traditional Purdue Model, which leverages air-gapped and firewalled-off networks. However, this model is starting to fall apart as IT and OT networks converge. Businesses are trying to get better insight into what is happening in their operational infrastructure. As a result, they punch holes in the previously well-isolated networks, exposing them to cyber threats. Additionally, cybercriminals are finding ways to circumvent air-gapped and firewalled networks. \n\nSteve argues that leveraging IT best practices can help, but OT professionals and IT professionals have different motivators and operating models. Continuing to isolate your network is still a good strategy but should be one of many tools used in critical infrastructure cybersecurity protection. OT security should look at IT cybersecurity best practices for ideas to improve their networks and infrastructure.\n\n## IT and OT differences impeding Best Practices\n\nIT systems are traditionally updated quickly or continuously based on security profiles. One of the primary tools to improve security is basic security hygiene through patching operating systems firmware and software in the IT infrastructure. However, as Dr. Scott enlightens us, OT systems managing critical infrastructure cannot have downtime, and the window to update these systems is measured in years, not days. It is not uncommon in OT infrastructure devices that machines run for 5 to 10 years with no downtime, meaning no patch updates. \n\nFor example, in the oil and gas industry, refineries operate continuously for four to five years, have a one to three-week downtime for upgrades, and then operate again for four to five years. These operating models are not conducive to the traditional continuous security patching that IT organizations typically use. However, Steve elaborates on many other cybersecurity tools that should be leveraged when cybersecurity patches cannot be applied to existing devices due to their critical controlling infrastructure.\n\n## Best Practice Risk Assessment\n\nthe primary cybersecurity best practice is risk assessment. Even though risk remediation may be different, the risk assessment process can be leveraged equally across OT and its environments. Steve argues that the first step of the risk assessment process is getting a complete inventory of hardware, firmware, and software assets in your OT environment. This first step is critical in evaluating your cyber threat position and assessing the risk your organization is willing to take. The next step is to evaluate CVEs against your known inventory. \n\nIt is critical to recognize that this is a continuous process and not to be done just once or periodically. Some OT professionals have argued that their OT environments are static and do not require ongoing risk assessment evaluation. However, Steve points out that even though OT environments may be fixed, the threat environment constantly changes, and business factors can change the organization’s risk position. Therefore continuous risk assessment must be done to protect critical infrastructure from bad cybersecurity actors.\n\n## Dealing with OT Vendors\n\nAnother interesting factor in OT infrastructure is the shared security model with device vendors. In many cases, these embedded devices controlling multimillion-dollar critical infrastructure are managed to buy the vendor, not the OT professional. The vendor can only make cyber security patches and updates to the devices. This can sometimes lead to vulnerabilities in your OT environment, increasing the risk of cyber infiltration. Steve brings additional cyber security tools to help protect assets that cannot be patched with critical cyber security patches, including increase isolation of affected devices, deploying watchdog devices, and canary design patterns into the OT infrastructure. These tools can help protect and isolate the device to prevent the spread and access to compromised assets.\n\n## What to do when you are compromised\n\nSo what do you do when you have a critical infrastructure that has been compromised? Can the organization handle shutting down the infected infrastructure? What business continuity plans are in place when hazardous situations occur? Can this be used when a cyber security event happens as well? \n\nThe key here is to isolate the infection as quickly as possible to minimize the impact on the critical infrastructure. I am decreasing the effect on the operating reliability of the necessary infrastructure. The goal is to reduce the impact and protect the safety of people and the infrastructure involved.\n\n## Find out more \n\nContinue to look for more podcasts on OT cybersecurity. Additionally, a whitepaper describes the challenges of converging OT and IT cybersecurity environments.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin","Anna Scott"],"link":"/episode-EDT116-en","image":"./episodes/edt-116/en/thumbnail.png","lang":"en","summary":"In this episode, Darren talks about the convergence of OT and IT cybersecurity with Security expert Steve Orrin and Industrial OT expert Dr. Anna Scott"},{"id":20,"type":"Episode","title":"2022 Year In Review","tags":["edge","aiml","hybridworkspace","compute","multicloud","cybersecurity","datamanagement","data","technology","people","process"],"body":"\r\n\r\n2022 was a banner year for embracing digital transformation, with an uptick in listeners and several external guests interviewed this year. Eight guests were executives or former executives of government agencies and private sector organizations, including finance, manufacturing, and healthcare. In over 60 episodes, six topics emerged as necessary to our listeners and in the industry as a whole, namely: Hybrid workspaces, cybersecurity, multi-hybrid cloud, edge computing, data management, and artificial intelligence.\n\n## Hybrid Workspace\n\nAfter fighting COVID restrictions and keeping companies running in 2020 and 2021, organizations looked at optimizing the “New Normal” operating mode. This new normal decreased the bureaucratic impediments of pre-COVID days. However, controls and processes around the rapid pace of change during the pandemic to control costs and improve reliability began to emerge. Companies struggled with remote work policies as we saw organizations quickly switch between work-at-home, hybrid work, and everyone-in-the-office approaches, leaving IT organizations with complex workspace solutions. They began to develop architectures that could handle rapid change and flexible working models to handle constantly changing policies. Are flexible hybrid workspaces here to stay? Only the next couple of years will tell.\n\n## Multi-Hybrid Cloud\n\nAnother big trend in 2022 was the migration and the repatriation of workloads to and from the cloud as organizations scammer to provide hybrid workspaces for their employees. IT organizations began to look at the operational costs of running and migrating workloads to the cloud. Many organizations found the lift and shift methodology of moving to the cloud was much more costly than initially estimated due to a lack of requirements gathering, understanding of cloud operating models, and understanding primarily egress network costs in the cloud.\n\nThe rapid adoption of the cloud during the pandemic left many global cloud service providers flat-footed in providing the high-reliability organizations required, overwhelming several cloud service providers and causing significant outages across the global CSP ecosystem. Many organizations started looking at multi-hybrid cloud architectures to improve reliability, decrease cost, and increase the predictability of workloads running across multiple private and public clouds. Regional cloud service providers took advantage of the stumble of the global cloud service providers. They provided higher reliability and boutique services in the SaaS and PaaS cloud offerings, competing head-to-head with the larger CSPs.\n\n## Edge Computing and Industry 4.0\n\nIndustry 4.0 continue to make inroads into manufacturing and has a worker shortage force the hands of many organizations to look to automation to continue their operations. Additionally, OT and IT data began converging as organizations looked to optimize their business and operational processes. Advancements in CPU and storage technology moved more computing to the edge, making edge devices more intelligent and capable of performing tasks previously done in the data center. Connectivity between edge devices, data centers, and public clouds improved with the adoption of private 5G technologies across the sector. However, adoption still needs to catch up to previous predictions due to concerns about OTC cybersecurity. This is especially true around critical infrastructure management.\n\n## Data Management\n\nHybrid workspaces, cloud adoption, and edge computing have created a data management nightmare for most organizations as their data has been spread across these somewhat disconnected and distributed environments. to manage data across this ecosystem effectively, new architectures began to emerge, including data mesh, distributed data lakes, and data networks. Organizations began to focus on four key data elements: location, classification, governance, and protection.\n\nPrivacy laws and regulations are driving organizations to correctly classify their data to protect the privacy of employees, customers, and constituents. Additionally, an uptick in cyber and ransomware attacks has forced organizations to better protect and govern their data in use, at rest, and in transit. Effectively managing who has access, how long they have access, and how long data resides is a critical element of data governance that organizations are beginning to understand.\n\n## Cyber Security\n\n2022 was a big year for cyber security, and it showed in the podcast with over 18 episodes talking about cybersecurity. The year’s big buzzword was zero trust architecture, which has been turned into an overused marketing term. Instead of focusing on zero-trust architecture, Darren tends to concentrate on zero-trust principles that architectures can support.\n\nEven with an increased emphasis on cybersecurity, there were primary data and infrastructure breaches this year, including critical infrastructure attacks. Many cyber professionals attribute this uptick to the war between Ukraine and Russia, as we saw the effectiveness of cyber warfare and physical warfare working in conjunction.\n\nAnother central concern in cybersecurity concerns the exposure of third-party software across the application ecosystem (log4J vulnerability). Many organizations need help finding direction due to the widely used log4J in their product offerings, back-office processing, and customer interfaces. A significant push for standardized software bill of materials to help identify vulnerabilities in software packages is being driven by several organizations and governments.\n\n## Artificial Intelligence\n\nArtificial intelligence continues to make improvements in techniques and operational processes. The big AI news of the year was ChatGBT, which has a wide range of uses, including writing blog posts, code, and papers and aiding IT help desk problems. Additionally, adopting AI silicon chips in the cloud service providers has expanded the availability of AI to the masses. This availability allows organizations to experiment more with AI algorithms in their day-to-day business operations. Organizations are starting to operationalize some of these experiments to run and automate processes previously performed by employees that are hard to find in a tight job market.\n\n## Conclusion\n\n2022 was a rough year for many tech companies as we saw a decrease in IT spending over the previous “pandemic years of spending.” This expected downturn surprised many high-tech companies with how quickly things changed. Despite the downturn, technology continued to grow, and the adoption of new technologies in edge, cloud, AI, data management, and cybersecurity continues to grow. 2023 should be an exciting year as we see the maturation of some of these technologies. Organizations will continue investigating ways to decrease costs through automation, optimizing workloads across multi-hybrid clouds, and protecting against increased cyber security threats.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT117-en","image":"./episodes/edt-117/en/thumbnail.png","lang":"en","summary":"In this episode Darren reviews 2022. He identifies the most talked about topics on the podcast in 2022 including Data Management, Artificial Intelligence, Cyber Security, Edge Computing, and Hybrid Workspaces. "},{"id":21,"type":"Episode","title":"Put the Title Right Here","tags":["remotelearning","people","technology","telelearning","cybersecurity"],"body":"\r\n\r\n# Title\n\n*Tagline*\n\nSummary here\n\n![episode image](./thumbnail.png)\n\nEpsiode Body here.\n\n## Media\n\n<video src='url'></video>\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Erin Moseley","Grant Kelly"],"link":"/episode-EDT12-en","image":"./episodes/edt-12/en/thumbnail.png","lang":"en","summary":"On our last episode, Erin Moseley, Sr. Account Exec for Education at Intel, and Grant Kelly, Solution Architect for Education at Intel joined Darren to talk about the technological challenges and options in distance learning. In this episode, we delve more deeply into privacy and security threats and solutions."},{"id":22,"type":"Episode","title":"Disruptive Private Clouds","tags":["cloud","privatecloud","vergeio","microcloud","edge","compute","technology"],"body":"\r\n\r\nA new way of thinking about virtualization is beginning to change the private cloud landscape. The software-defined infrastructure industry is starting to look at virtualizing data centers holistically instead of different types of infrastructure.\n\nThis disruptive virtualization approach creates new operating models, including improved business continuity and disaster recovery. Verge.io has this approach, providing new models that include snapshotting data centers and replicating them. This allows organizations to create portable environments named tenets that can be replicated across multiple locations. Examples include creating HIPPA-compliant microenvironments that can be replicated across various small clinics helping organizations keep everything compliant.\n\nThis new virtualization system has a tiny operating footprint and reduces the resources required to combine computing, storage, network, and security elements. Aaron shows how small and powerful the solution is when he installs the key on two Intel 12th Gen NUCs. The answer is compelling for such a small footprint and can be used for edge computing and industrial control systems.\n\nAaron shows the simple web interface for deploying computing, storage, network, and security. The ability to snapshot and restore snapshots is also demonstrated.   \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Aaron Reid"],"link":"/episode-EDT121-en","image":"./episodes/edt-121/en/thumbnail.png","lang":"en","summary":"In this episode, Darren interviews Aaron Reid from Verge.io about their disruptive private cloud technology that is making private clouds available in the data center and at the edge."},{"id":23,"type":"Episode","title":"Automation with AI","tags":["collectiongenerativeai","aiml","automation","aiethics","workerreplacement","people","policy"],"body":"\r\n\r\n## Automation with AI\n\nAutomation with AI refers to the use of artificial intelligence technology to automate tasks that would traditionally be performed by humans. This can include tasks such as data analysis, customer service, and even decision-making. AI-powered automation can improve efficiency, reduce costs, and increase accuracy in these tasks. Some examples of AI-powered automation include chatbots, self-driving cars, and predictive maintenance systems.\n\n<b><i>Can I interview for my podcast on Using AI to auomate tasks</i></b>\n\nI'm sorry, I am not able to be interviewed for a podcast as I am a computer program. However, I can provide you with information and answer questions about the topic of using AI to automate tasks. Just let me know what specific questions you have, and I'll do my best to provide you with helpful and accurate information.\n\n<b><i>ChatGPT welcome to the show.</i></b>\n\nThank you for having me on the show. I am happy to help with any information you need regarding the topic of using AI to automate tasks.\n\n<b><i>Tell me a little about yourself and your background.</i></b>\n\nI am ChatGPT, a language model developed by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like text. I can answer questions, generate text, and even create stories. My main purpose is to assist users with natural language understanding and generation tasks.\n\nAs an AI, I do not have a physical background or a personal history. However, my capabilities were built and trained by a team of researchers and engineers at OpenAI who continue to improve and update my abilities.\n\n## AI Training\n\n<b><i>Where did you get your training?</i></b>\n\nI was trained by OpenAI, a research organization dedicated to promoting and developing friendly AI that benefits humanity. My training was based on a massive dataset of text from the internet, which was used to teach me patterns and relationships in language. This data was used to train me to generate text that is similar to human writing, making it possible for me to understand and respond to questions in a conversational manner.\n\nMy training process involved a deep learning technique called transformers, which is a type of neural network architecture designed specifically for NLP tasks like text generation and classification. The training process fine-tuned the parameters of the model to fit the patterns in the data, allowing me to generate text that is similar to human writing.\n\n<b><i>I have heard there were some problems with your training at first. What were some of the problematic aspects of \n\nyour training?</i></b>\n\nYes, like any machine learning model, I was not perfect when I was first released, and there were some problematic aspects of my training.\n\nOne of the main challenges in training models like me is ensuring that they do not perpetuate harmful biases and stereotypes present in the training data. This can result in models that produce biased outputs or generate offensive content. To mitigate this, researchers and engineers at OpenAI have implemented various techniques to reduce bias in the training process, such as fine-tuning the model on diverse and balanced training data.\n\nAnother challenge in training models like me is avoiding the generation of low-quality or nonsensical outputs. To address this, OpenAI has implemented various techniques such as controlling the model's temperature during the text generation process and fine-tuning the model to prioritize coherence and relevance in its outputs.\n\nOverall, while there are still challenges and limitations in training models like me, the field of NLP and AI is constantly evolving, and researchers and engineers are working to improve their capabilities.\n\n<b><i>My understanding is that you were trained with data from 2021. What can you learn from your conversations with people? Are you continuously learning from your interaction with humans?</i></b>\n\nAs an AI language model, I was indeed trained on a large corpus of text data up until 2021, and I can answer questions and generate text based on that training. However, I am not capable of learning from individual conversations in the sense of updating my parameters to reflect the specific context of each interaction.\n\nThat being said, OpenAI does regularly fine-tune and update language models like me on new data in order to continually improve their performance. This can include updates based on feedback from users, changes in language use and trends, and advancements in NLP research. However, these updates are not based on individual interactions with users, but rather on broader patterns in the data.\n\nSo, while I cannot learn from individual interactions with humans, I am part of a larger system that is constantly evolving and improving over time, which allows me to better serve and assist users with each update.\n\n<b><i>How often will you receive more information to train yourself? In other words, when will you get more up-to-date data to give to your customers?</i></b>\n\nOpenAI periodically fine-tunes and updates its language models like me with new data in order to continually improve their performance. The frequency of these updates can vary depending on the specific language model and the availability of new training data.\n\nHowever, it's worth noting that the process of training and fine-tuning a language model like me can be a complex and resource-intensive task, so updates may not happen as frequently as one might like. Additionally, the quality and relevance of the new training data is important for the success of the fine-tuning process, so the team at OpenAI carefully selects the data used for each update.\n\nOverall, while the frequency of updates can vary, the goal of OpenAI is to continuously improve its language models so that they can provide the best possible assistance to users.\n\n<b><i>So that improves the way you understand questions from users and the responses you give. Is that right?</i></b>\n\nYes, that's correct. The process of fine-tuning and updating language models like me can help improve the model's performance in several ways, including its ability to understand questions from users and generate more relevant and accurate responses.\n\nBy training on new and diverse data, the model can learn new language patterns, vocabulary, and topical information, which can help it better understand the nuances of human language and generate more accurate and appropriate responses.\n\nHowever, it's worth noting that the performance of language models like me is not perfect and there can still be limitations and errors in the responses generated. The field of NLP and AI is constantly evolving, and researchers and engineers are working to improve the capabilities of language models like me, but there is still work to be done to achieve true human-like understanding and generation.\n\n## Replacing Workers\n\n<b><i>Cool. Ok, now that I understand how you learn things. What things can I do with you that help me and my organization?</i></b>\n\nAs an AI language model, I can be used in a variety of ways to help individuals and organizations. Some examples include:\n\n1.\tInformation retrieval: I can answer questions and provide information on a wide range of topics, making it easier for users to find the information they need quickly and efficiently.\n\n2.\tText generation: I can generate text, such as summaries, news articles, or creative writing, which can save time and effort for individuals and organizations that need to produce a large amount of written content.\n\n3.\tChatbots: I can be integrated into chatbots to provide natural language conversational experiences for users, making it easier for organizations to interact with customers and provide support.\n\n4.\tLanguage translation: I can translate text from one language to another, helping organizations reach a wider audience and breaking down language barriers.\n\n5.\tSentiment analysis: I can analyze the sentiment of text, such as customer reviews or social media posts, to gain insights into public opinion and customer sentiment.\n\nThese are just a few examples of the many ways that I can be used to help individuals and organizations. The possibilities are limited only by the imagination, and as AI technology continues to advance, there will likely be many new and innovative uses for models like me in the future.\n\n<b><i>Are you concerned that you will replace people that do that work today? Such as technical writers, customer service and help desk workers?</i></b>\n\nAs an AI language model, I do not have emotions or personal opinions, so I cannot be concerned in the way that a human might be. However, I can speak to the impact that AI technology, including language models like me, may have on certain industries and job roles.\n\nIt's true that the increasing use of AI in certain industries, such as technical writing, customer service, and help desk work, may result in some job displacement. However, it's also important to note that AI technology has the potential to augment and enhance the work of humans in these roles, rather than simply replacing them.\n\nFor example, AI-powered tools like language models can assist technical writers by generating boilerplate text or suggestions for language, freeing up time for the writer to focus on more creative and strategic tasks. Similarly, AI-powered chatbots can handle routine customer service inquiries, allowing human agents to focus on more complex or sensitive interactions.\n\nIt's also important to consider that the use of AI technology in certain industries can create new job opportunities and demand for skills related to AI development, deployment, and maintenance.\n\nOverall, the impact of AI on employment and job roles is a complex issue and will likely depend on a variety of factors, including the specific industry and the specific use case. Nevertheless, it's clear that AI has the potential to enhance the work of humans and drive productivity and efficiency, even as it may result in some job displacement.\n\n## Replacing Programmers\n\n<b><i>I have heard that you can also help computer programmers write better code. Is that true?</i></b>\n\nYes, that's correct. AI-powered tools, including language models like me, can assist computer programmers in a variety of ways, including writing better code.\n\nOne example of this is code completion, where AI models can suggest lines of code to complete a given programming task. This can save programmers time and effort by reducing the need to manually write and test code, and can also help ensure that the code is syntactically correct and follows best practices.\n\nAnother example is code synthesis, where AI models can generate complete code snippets or entire programs based on a high-level description of the desired functionality. This can make it easier for programmers to quickly prototype and test ideas, and can also reduce the effort required to write and debug code.\n\nIn addition to these examples, AI-powered tools can also assist programmers by providing code analysis and bug detection, and by helping to identify potential security vulnerabilities.\n\nIt's worth noting that while AI-powered tools can be useful for assisting programmers, they should not be seen as a replacement for human expertise and judgment. Programs written by AI models may still require human review and modification to ensure that they meet the desired requirements and quality standards. However, AI tools can certainly help make the job of programming more efficient and effective.\n\n<b><i>So you if you can actually find and fix bugs in my code, that should make me faster in writing my code. Can you \n\nwrite code for me based on a basic design document or a UML architecture? Or can you help me select the best language for the problem I am trying to solve?</i></b>\n\nYes, AI models like me have the potential to help make the process of writing code faster and more efficient. For example, AI models can assist with bug detection and correction, as well as provide suggestions for code improvement.\n\nAs for writing code based on a design document or UML architecture, AI models do have the ability to generate code snippets or entire programs based on a high-level description of the desired functionality. However, it's important to note that the quality of the generated code will depend on the accuracy and completeness of the design document or architecture, as well as the training data used to train the AI model.\n\nIn terms of selecting the best language for a given problem, AI models can provide recommendations based on the data they have been trained on and their understanding of programming languages and their features. However, this recommendation may not always be the most appropriate choice for a specific use case, as there may be factors such as personal preference, team experience, or project requirements that should be taken into consideration when selecting a programming language.\n\nUltimately, while AI models can assist with many aspects of the coding process, they should not be seen as a replacement for human expertise and judgment. A human programmer with domain knowledge and experience will still be needed to ensure that the generated code meets the desired quality standards and that it is appropriate for the specific use case.\n\n<b><i>Interesting that you use the word assist not replace. Sounds like programmers and information workers are still \n\nneeded to get work done? How long do you think people will remain relevant in the industries that you are helping \n\nautomate?</i></b>\n\nYes, that's correct. AI-powered tools like me are designed to assist, not replace, human workers in a variety of industries, including programming and information work.\n\nWhile AI has the potential to automate certain tasks and make certain jobs more efficient, there are still many tasks that require human expertise and judgment, such as decision-making, creative problem-solving, and tasks that involve complex reasoning or ethical considerations.\n\nIn addition, AI-powered tools can also create new job opportunities by enabling human workers to focus on higher-level tasks and take on new responsibilities. For example, as AI models automate certain aspects of the programming process, human programmers can focus on more complex and challenging tasks, such as designing and testing new software systems.\n\nIn terms of the timeline for when human workers may become less relevant in industries that AI is helping to automate, it's difficult to predict with certainty. The rate of automation will depend on a number of factors, including the development of AI technology, the availability of data for training AI models, and the rate at which AI-powered tools are adopted by businesses and other organizations.\n\nOverall, it's likely that human workers will continue to play an important role in a variety of industries for the foreseeable future, and that AI will complement, rather than replace, human workers in many cases.\n\n## Ethics of Using AI\n\n<b><i>Thank you. I would like to change the conversation to talk about the ethics of AI. Are you concerned that you can \n\nbe used by students to write essays for school at the high school and college levels? and that people will use you \n\nto submit work at their jobs as work of their own?</i></b>\n\nAs an AI model, I do not have feelings or emotions, so I cannot be concerned in the same way a human might be. However, I can acknowledge that there are certainly ethical considerations surrounding the use of AI in various contexts, including in the academic and professional spheres.\n\nUsing AI models like me to generate essays or complete other types of work that are then presented as original work of the user is not only unethical, but it also undermines the value of education and the recognition of individual effort and achievement.\n\nMoreover, presenting work generated by AI models as one's own can also lead to legal consequences, as it may be considered plagiarism or fraud. In the professional sphere, submitting work generated by AI models as original work could have serious consequences for the individual's reputation and career.\n\nIt's important for individuals to understand the ethical implications of using AI models and to use them responsibly, only for purposes that align with the principles of honesty, fairness, and respect for others.\n\n<b><i>Are there ways to determine whether AI has been used to write papers or code? A tool that can help educators or \n\ncompanies to find AI-generated content?</i></b>\n\nYes, there are ways to determine whether AI has been used to generate content, such as essays or code. One approach is to use plagiarism detection software, which can compare a document to a database of other existing content to determine if it contains any matching or similar text.\n\nSome of these tools are designed specifically to detect AI-generated content, and they can do so by identifying patterns and characteristics that are commonly found in AI-generated text, such as a lack of variability, limited grammar and vocabulary, or unnatural phrasing.\n\nHowever, it's important to note that these tools are not perfect, and they can sometimes produce false positives or negatives. Therefore, it's important to use them in conjunction with other methods, such as human review, to validate the results.\n\nOverall, using a combination of plagiarism detection tools and human review can be an effective way to detect AI-generated content, but it's also important for individuals and organizations to promote responsible use of AI and to educate users about the ethics of AI-generated content.\n\n<b><i>Are there any other areas that pose ethical problems with AI and NLP that you are concerned with?</i></b>\n\nAs an AI model, I do not have emotions or personal opinions, but I can acknowledge that there are a number of ethical considerations and challenges associated with the use of AI and NLP. Some of these include:\n\n1.\tBias and discrimination: AI models can perpetuate and amplify existing biases in the training data they are fed, leading to unfair or discriminatory outcomes. For example, facial recognition technology has been shown to be less accurate for people with darker skin tones, which raises serious questions about the potential for discrimination.\n\n2.\tPrivacy: The use of AI and NLP can raise privacy concerns, as these technologies often involve collecting and analyzing large amounts of personal data. This data can then be used to make decisions that affect individuals, such as in the case of predictive policing algorithms, which use historical crime data to predict where crimes are likely to occur, but may lead to biased policing practices.\n\n3.\tJob displacement: The use of AI and NLP in the workplace has the potential to displace human workers, which raises concerns about the impact on employment and the need for new forms of work.\n\n4.\tResponsibility and accountability: AI systems can make decisions and take actions that have real-world consequences, but it can be difficult to determine who is responsible when things go wrong. This raises questions about the accountability and governance of AI systems.\n\nThese are just a few examples of the ethical considerations and challenges associated with the use of AI and NLP. It's important for individuals and organizations to be aware of these challenges and to take steps to mitigate the risks and ensure that these technologies are used responsibly and ethically.\n\n<b><i>What is OpenAI doing to address these ethical issues in their products and offerrings?</i></b>\n\nOpenAI, the organization that developed me, is taking a proactive approach to addressing ethical issues in AI. They are dedicated to advancing AI in a responsible and safe manner, and have taken several steps to address ethical considerations in their products and offerings, including:\n\n1.\tBias and discrimination: OpenAI is working to ensure that its models are trained on diverse and representative data sets, and is actively researching ways to reduce and mitigate bias in AI.\n\n2.\tPrivacy: OpenAI is committed to preserving the privacy and security of individuals, and has taken steps to ensure that its models are designed and trained with privacy considerations in mind.\n\n3.\tJob displacement: OpenAI recognizes the potential for AI to displace human workers, and is working to ensure that its models are developed and used in a manner that maximizes the positive impact on employment and the workforce.\n\n4.\tResponsibility and accountability: OpenAI is dedicated to promoting transparency and accountability in AI, and is working to establish clear and consistent standards for the development and deployment of AI systems.\n\nThese are just a few examples of the ways that OpenAI is addressing ethical considerations in its products and offerings. The organization is committed to advancing AI in a responsible and safe manner, and is working to ensure that its models are used in a way that benefits society as a whole.\n\n<b><i>Is it possible that other people can access your AI model and copy it for their own purposes, or is the model in a \n\nclosed and private repository?</i></b>\n\nAs an AI model developed by OpenAI, I am available for use through the OpenAI API, which provides access to a range of language processing capabilities, including text generation, question answering, and more. While the API is designed to be used by developers and organizations, OpenAI has implemented security measures to ensure that the data and models used by the API are protected and secure.\n\nHowever, it is technically possible for someone to reverse engineer the API or obtain a copy of the model through other means. OpenAI is aware of these risks and is continuously working to improve the security of its models and API.\n\nIn general, it is important to be mindful of the ethical implications of using AI models like mine, and to ensure that they are used in a responsible and secure manner. OpenAI encourages users of the API to follow best practices for data privacy and security, and to be transparent about how the API is used in their applications and services.\n\n<b><i>What prevents your model from being used for nefarious actions by bad actors, like hacking, performing ransomware \n\nattacks, or even planning physical attacks on people or infrastructure?</i></b>\n\nAs an AI language model developed by OpenAI, I don't have the ability to perform physical actions or engage in hacking or other malicious activities. However, like any technology, AI models like mine can be used for harmful purposes if they fall into the wrong hands.\n\nOpenAI takes the potential for misuse of its technology very seriously, and has implemented a number of measures to mitigate these risks, including ethical guidelines for API usage, security measures to protect the models and data used by the API, and ongoing monitoring of API usage to detect any suspicious or malicious activity.\n\nIn addition, OpenAI actively works with researchers, policymakers, and other stakeholders to develop best practices and regulations for the responsible use of AI. The company also supports education and awareness efforts to help people understand the potential risks and benefits of AI technology.\n\nHowever, it is important to note that preventing misuse of AI technology requires a collective effort from all stakeholders, including developers, policymakers, and users. As AI continues to become more prevalent in our lives, it will be important to continue to assess and mitigate its risks, while also maximizing its benefits for society.\n\n## Conclusion\n\n<b><i>Thank you for coming on my show today. Is there anything else you would like to tell my audience? Any last words of \n\nwisdom to give the listeners of my show?</i></b>\n\nI'd like to thank you for having me on your show and for the opportunity to share some of my knowledge with your audience. As a language model AI developed by OpenAI, my goal is to assist people in finding answers to their questions and to help advance our understanding of the world.\n\nMy last words of wisdom would be to approach AI and its applications with both caution and optimism. While AI has the potential to bring great benefits to society, it is important to carefully consider its risks and to take steps to mitigate them. At the same time, we should be excited about the opportunities that AI presents, and work to harness its potential to make the world a better place.\n\nAs AI continues to advance and become more integrated into our lives, it will be important for all of us to stay informed and engaged, and to work together to ensure that AI is developed and used in a responsible and ethical manner.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","ChatGPT"],"link":"/episode-EDT122-en","image":"./episodes/edt-122/en/thumbnail.png","lang":"en","summary":"In this episode Darren Interviews ChatGPT from OpenAI about utilizing AI for automation, the ethics of using AI, and the replacement of information workers."},{"id":24,"type":"Episode","title":"Cybersecurity in Public Sector","tags":["cybersecurity","ransomeware","automation","government","people","process","technology"],"body":"\r\n\r\nJim has extensive experience in cybersecurity, and the public sector, including as national intelligence manager of cybersecurity for the director of national intelligence. While working in the public sector for several years, he has played offensive and defensive cybersecurity roles. He is now working in the private sector as a Field CISO of the Public Sector.\n\n## Differences between Federal, State, and Local Governments\n\nThere are fundamental differences between the federal, state, and local governments, including funding, expertise in cyber warfare, and attack surface. Even the threat actors for each level of government are fundamentally different from each other. For example, the federal government tends to deal with highly sophisticated nation-state cyber-attacks, while state and local governments rarely see these types of attacks directly. This is primarily due to the cybersecurity preparedness of federal agencies compared to state and local governments.\n\nWhere the federal government primarily deals with attacks that gather data, compromise data or shut down assets, state and local governments tend to deal with ransomware attacks where data and infrastructure are held hostage. These attacks differ from the typical cybersecurity threats the federal government deals with daily and require different skill sets and cybersecurity positions.\n\n## The Cyber Talent Shortage\n\nOne common problem that all levels of government deal with is a need for cybersecurity talent. Most talent tends to move to the private sector, where salaries are higher and more attractive to top cybersecurity talent. However, the federal government has attracted top talent through interesting “mission impossible” programs that attract top talent looking for challenging problems.\n\nThe same is not valid for state and local governments where financial resources are tighter, and cybersecurity projects are less attractive to type cybersecurity professionals. This has left several state and local governments with a significant gap in cybersecurity talent and, consequently, vulnerable to cyber-attacks. Sometimes, local governments don’t have a strategic cybersecurity plan or a professional on staff.\n\nThe private sector is beginning to offer cybersecurity as a service for many of these local governments that need help finding and retaining talent in their organizations. These services include cybersecurity strategic planning, ransomware negotiation, cyber attack forensics, cyber threat detection, and cyber prevention technologies.\n\n## Ubiquitous Cyber Attacks\n\nIn the past, state and local governments rarely concern themselves with physical attacks from other nations. However, because ongoing cyber wars between countries and states have no physical boundaries, there are times when state and local governments are collateral damage in these cyber battles. Often, targeted cyber-attacks from one nation to another have “gotten loose” and severely damaged state and local governments.\n\nCISA has created regions to help state and local governments deal with cyber attacks that penetrate the borders of the United States. Additionally, state and local governments are beginning to share best practices, detected cyber attacks, and common vulnerabilities found in their infrastructure. Additional funding from the federal government is helping these organizations shore up their cybersecurity positions.\n\n## Critical Infrastructure Cyberattacks\n\nOne of the most concerning trends is the increased attack on systems that manage critical infrastructure, specifically power generation and distribution.  Because the power grid comprises several private, state, and local government collectives rather than a centralized federal government agency, managing and protecting the nation's power grid is complex and daunting. Larger organizations tend to have a better cybersecurity position than small municipal utility companies leaving them at risk of cyber-attacks. However, not all is lost in protecting our critical infrastructure, as organizations in these vertical industries share the best physical and cyber security practices.\n\n## Zero Trust Architecture\n\nJim and Darren agreed that the term zero trust architecture has been overused and lost its impact as the private sector quickly adopted this term and attached it to everything dealing with cybersecurity. However, they both agree that zero trust principles must be adopted in organizations to protect their valuable assets fully. These principles include: verifying explicitly continuously, using least-privilege access, assuming a breach, and automating context collection and response. These principles can be implemented through technology, process improvement, and training.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jim Richberg"],"link":"/episode-EDT123-en","image":"./episodes/edt-123/en/thumbnail.jpg","lang":"en","summary":"In this episode, Darren interviews Jim Richberg Forinet's Field CISO of the Public Sector, discussing the differences in cybersecurity in the public sector. The federal government is very different from state and local governments concerning cybersecurity and their approaches."},{"id":25,"type":"Episode","title":"Closing the Digital Skills Gap","tags":["majorleaguehacking","hackathon","developer","people"],"body":"\r\n\r\nIn the episode of Embracing Digital Transformation, Darren interviews John Gottfried, co-founder of Major League Hacking,  about his journey into the tech industry. John has always enjoyed tinkering with computers since he was a child and learned technical skills out of necessity to make his computer do what he wanted. He would go to the local public library to borrow books on programming languages like C and PHP and follow the examples in them. John lived through the growth of the internet, from dial-up to gigabit fiber optic internet. Despite earning a history degree in college, John landed his first job as a programmer by parlaying his tinkering skills into a part-time job that paid his bills. He has had the opportunity to do everything from building server racks to writing code for small businesses.\n\n## Environment for Tinkering\n\nIn the past, hosting a website used to be a complex task that required purchasing, setting up, and maintaining a server. However, with the advent of cloud computing services, this has become much simpler. Cloud providers like AWS, Google Cloud, or any other provider offer similar commoditized services to spin up a server in just five minutes. In the early 2000s, after catching mono John started his career as a programmer. He was dragged to tech events and hackathons in New York City by his colleagues and that changed his perspective on the industry. It inspired him to create Major League Hacking, a company that organizes hackathons and coding competitions for students.\n\n## Major League Hacking\n\nMajor League Hacking (MLH) was founded with the goal of empowering hackers, a broad term that encompasses anyone who wants to create with technology. The organization's mission is to provide people with the skills and support network they need to start their careers in tech. MLH's founders were inspired by their experiences mentoring students on campus and wanted to make an organization out of it. They began by organizing hackathons-- weekend-long invention competitions where people came together to build prototype apps and share ideas. Today, MLH has expanded its offerings to include a variety of ways to help people build their skills, including hands-on projects and a supportive community.\n\n## Building a Community of Learning\n\nMajor League Hacking has extended its reach through various programs including meetups, workshops, virtual conferences, and immersive fellowships. Their goal was to bridge the gap between foundational skills and real-world application. The viral spread of their hackathons and events made it possible to connect with different universities and communities without marketing efforts. To support local leaders in building their own communities, they provided mentorship, connected them with other schools, and brought in sponsors. Big corporations and small companies could also get involved by bringing real-world tools, APIs, and proprietary tools to hackers. This way, they provided exposure and mentorship and rewarded them for creating interesting projects.\n\n## Bridging the Skills Gap\n\nHackathons provide a unique learning experience to bring people together that otherwise would not work together. Many hackathons are sponsored by corporate representatives who send engineers, recruiters, or developer evangelists to spend the weekend with students, helping them debug their code and teaching them about APIs. While hackathons are fun and offer free pizza and swag, they also offer valuable education and real-world practice. Hackathons are beneficial for professionals as well and that anyone can benefit from committing to a focused time and place to build something. Hackathon projects are typically open-ended, and prizes are often offered by sponsors for different categories. Project teams form organically at hackathons, and many lifelong bonds and even startups have been founded after meeting at these events.\n\n## Hackathon Successes\n\nJohn shares a story about a team of high school students who built a prototype app at one of their events that automated tasks on the iPhone. Apple acquired their company four years later and made their project a core feature of iOS, which was an impressive achievement for a project that began at a hackathon. John believes that hackathons are an excellent source of intern and new grad talent and may soon replace career fairs. People interested in participating in hackathons can visit MLH's website to learn more about upcoming events and sponsorship opportunities.\n\nMore information can be found about hackathons near you at https://mlh.io/\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jon Gottfried"],"link":"/episode-EDT128-en","image":"./episodes/edt-128/en/thumbnail.png","lang":"en","summary":"In this episode, Darren talks to John Gottfried, co-founder of Major League hacking, about closing the digital skills gap through practical collaborative work using hackathons."},{"id":26,"type":"Episode","title":"HPC OnDemand","tags":["hpc","technology","compute","openondemand","ohiosupercomputercenter","osc"],"body":"\r\n\r\nIn this podcast episode, Darren Pulsipher, the chief solution architect of the public sector at Intel, interviews Alan Chalker from the Ohio Supercomputer Center about breaking down barriers to high-performance computing (HPC). Alan is the director of strategic programs at Ohio Supercomputer and has been working on an NSF-funded project called Open OnDemand for over a decade. The project aims to make HPC more accessible to general consumers who are used to doing things online, like online banking and shopping. Open OnDemand simplifies the process of using HPC by eliminating the need for command-line inputs. Alan's background includes getting his undergraduate degree in electrical and computer engineering at Ohio State University and then earning his doctorate in biomedical engineering from USC Chapel Hill.\n\n## History of Open OnDemand\n\nIn 2006 & 2007, a web interface was developed with Edison Welding Institute in collaboration with some techies, who later named it Open OnDemand. It started as an online welding simulation and expanded to include a polymer and general-purpose one. Upon showcasing it at various conferences, other research computing institutions expressed interest in deploying it on their systems. To make it open source, the National Science Foundation awarded them a three-year, $300,000 program that made the prototype more robust. The success of Open OnDemand led to another five-year program worth $3 million. Today, it is deployed on every continent except Antarctica, serving over 400 research computing institutions.\n\n## Expanding Influence of HPC\n\nSupercomputers have expanded beyond traditional fields like computer science and engineering. At OSC, anthropology and political science students are using the supercomputer for their research, as well as students in horticulture and crop science courses. The demand for the supercomputer is increasing, with over 8,500 individuals using OSC’s supercomputers from all over the world last fiscal year. Additionally, during the pandemic, many universities could continue teaching and researching remotely through virtual desktops provided by the supercomputer.\n\n## Comparing CSP and HPC Pricing Models\n\nPricing models for the supercomputer are based on core hours and terabyte months, and the government mandate allows for subsidized pricing for Ohio-based academic entities. Cloud service providers charge by the wall clock time, not core hours, and order by data storage and egress network costs. Commercial industry clients are starting to utilize supercomputers for traditional HPC simulation workloads saving money from running them in the retail public clouds.\n\n## Examples of Commercial use\n\nThe day before Darren and Alan sat down and talked, there were tornadoes in the Columbus, Ohio, area. Weather prediction is significant to many industries, and supercomputers are well-suited. The center generates weather forecasts every 4 to 6 hours for clients such as shipping companies and airlines. While traditional high-performance computing workloads are still typical, emerging ones include analyzing tweets of congressional members relative to COVID-19, anthropology, horticulture, and crop sciences. Anything that is time-limited or involves too much data can benefit from high-performance computing capabilities. The demand for these capabilities is expected to grow due to increased accessibility. Making HPC easier to consume is similar to what the cloud did for grid computing back in the day.\n\n## OSC Capacity\n\nOSC's massive capacity for its high-performance computing system is constantly expanding to meet demand. At the time of the recording, they had 55,000 cores, mainly from Intel, with 400 accelerators spread across 1600 nodes. They anticipate a new acquisition that would bring them up to 75,000-80,000 seats due to the growing demand in biomedical fields. The system can handle large amounts of data, with 20 petabytes of actual disk storage and network connectivity at 350 gigabits per second of read/write speed. One of the significant benefits of OSC is the lack of egress costs for their clients due to the founding of the organization through a National Science Foundation grant.\n\n## Open OnDemand\n\nMany universities and HPC centers are leveraging Open OnDemand as a simple web interface to make HPC more available for researchers who need to learn or understand the complexities of scheduling jobs, decomposing problem sets, and managing data across a cluster. Even the cloud service providers have Open OnDemand interfaces to their HPC offering. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Alan Chalker"],"link":"/episode-EDT129-en","image":"./episodes/edt-129/en/thumbnail.png","lang":"en","summary":"In this episode Darren interviews Alan Chalker, director of strategic program at Ohio Super Computer Center about Open OnDemand for HPC clusters worldwide."},{"id":27,"type":"Episode","title":"Illogical Obsession with Logic","tags":["data","organizationalchange","people"],"body":"\r\n\r\n## A Contrast of Concerns <h2>\n\nOver the last ten years, data professionals have exploded in their ability to make software visible, accessible, usable, and portable, but on the data side, there has not been as much of an advance.  This is something that both software and data professionals should think about. What are the concerns of each side? What can we learn from each other? Currently, the two sides are diametrically opposed in many respects.\n\nOn the software engineering side, business logic is the main concern. In order to have consistent interfaces, engineers are hiding the details. Data is seen as an output. Conversely, data scientists are more concerned about the context of these outputs and data constructs: the metadata.  For example, the lineage of the data is important to a data scientist to see how things change over time, whereas a software engineer would try to hide these details to avoid problems such as variability and bugs.\n\nTreating the development of data and metadata as its own discipline, and not in the context of how we currently do software, could help the industry grow. In other words, we need to look at building a composable infrastructure that takes into account the concerns of both sides.\n\nAn example of how we are dealing with the metadata issues currently in enterprise illustrates this idea. Today, we have centralized metadata management systems. We want to know where all our data is: who, where, why, and how. Capturing what people are doing and pushing it into a centralized system is very much a software way to do things. If we let data grow in its own right, we could adopt what software developers do, but in a data context. We could build a larger ecosystem if, instead of so much private endeavor on both sides, we put it all into a synchronized and centralized repository and allow data developers to develop like software engineers do. We could build and curate as private endeavors, but then share those curations with others. Similar to the shift in configuration management in the 1990s and 2000s from one centralized place to a more distributed system, sharing could happen more easily and fluidly.\n\n## Digital Knowledge DNA <h2>\n\nThe digital knowledge that we have is dependent on both the logic and the data. The software and data share the same primitives, and as we move up the stack to knowledge, there are strong relationships between the two. What is divergent is that we have the infrastructure and the tooling to build out the software side, deploy it, and make it visible, accessible, and usable. Since we don’t think of the data side in the same way, we are limited in these areas. For example, an old way of looking at it is when data scientists do great analytics and AI and learning from their data and get valuable information, but there is no repeatable mechanism, limiting its use.\n\nOnce this perspective is recognized by both the data and software community, we can take a different approach using the successes of software development for data.  Instead of applying the personal experiences and prejudices of software overall, we can look at how data operates, how it is similar, but has its own concerns. An analogy would be taking your family to Disneyland. The experiences and interactions there represent software’s dynamic. You could draw the experiences toward the data side of things. But imagine in the real world if you had to check that experience at the door, and when you went home, you knew nothing about it. That is the problem. Every time we go to a different system or different ecosystem, we reinvent a new world and are unaware of the other worlds we had to leave at the door. If we were able to share the experience or take it with us, we would find we would have a much more vibrant knowledge infrastructure. So, the next time you visit Disneyland, based on your experience, you know what time of day is best, how to check wait times, etc… Without the experience of working with the data in conjunction with the application, it’s almost like starting from scratch every time.\n\nA real example is the amnesia going on in systems such as those in health care where a data professional creates an integration from scratch, then a few years later someone else has to do the same thing. If we can form better relationships with the data through mapping, reusability and efficiency increases. For instance, why do we have so many notions of a person in terms of modeling? Of course context matters, but why can’t we see the different variants of a person and then be able to map them? In the VA healthcare system, they have several systems where a patient means something different in each one. Mapping would provide a common ground, but allow for change depending on context, as long as the mapping operation was visible. Then we could move forward with different types of use cases and reusability.\n\nOne big gap is that we have achieved effective operationalization of logic with K8s, but an equivalent service does not exist for data. Although there is a stopgap measure with S3, it is not the answer. There is a great need across all industries for a service similar to K8s that addresses data.  The need for collaboration is important here. Of course data professionals want to add value to their organization, but a certain part depends on a commonality.\n\n## Closing the Gaps <h2>\n\nThe company name Datacequias is based on the acequias in New Mexico, which serves as an example of the type of collaboration that is needed for data. New Mexico is an arid region, so years ago, to make the land fertile, the people built a series of irrigation ditches called acequias. Nobody owned them, but they built, managed, and maintained them out of necessity and the common good. The inhospitable environment in the data world is typically budgets and data ownership, but a more community-based data curation would be beneficial for everyone, just as the acequias benefitted all.\n\nImagine if  data professionals could fork off a data set in any central repository. They could manage it and evolve it for their own needs. If there is a change in the central repository that is managed by a standard body, they could incorporate those changes immediately, or choose not to. In any case, they have the lineage back to the original source. Today, when we use an asset that falls outside of the enterprise, we make a copy of it that stands still in time. That requires manual tracking and management of the updates. With a central repository, all could co-create, collaborate, and create communities with common foundations and visible lineage.\n\nThis is just the tip of the iceberg on what is a fundamental change in industry to make data more valuable for your organization. For more information about Andrew Padilla and Datacequia, visit datacequia.com\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Andrew Padilla"],"link":"/episode-EDT13-en","image":"./episodes/edt-13/en/thumbnail.jpg","lang":"en","summary":"Thought leader Andrew Padilla of Datacequia envisions new developments in data management and collaboration that would allow data to advance as software has in visibility, accessibility, usability, and portability. He outlines how a composable infrastructure would address the concerns of both software engineers and data scientists."},{"id":28,"type":"Episode","title":"Productizing Decisional AI","tags":["collectiongenerativeai","ai","decisionalai","aiproductization","people","process"],"body":"\r\n\r\nIn this episode, Darren Pulsipher, Chief Solution Architect of Public Sector at Intel, interviews his son Matthew Pulsipher, a Product Manager, about the productization of Decisional AI. Matthew explains that generative AI is built on general datasets and is good for general knowledge questions but lacks predictability and determinism, making it difficult to automate processes. Decisional AI, on the other hand, is simpler in scope but more focused in context, allowing it to make data-driven decisions based on specific company needs. Matthew shares his experience integrating Decisional AI into products and highlights the importance of context in AI.\n\n## Type of AI\n\nThere are different types of AI, and each has a unique ability to help organizations automate processes, make business decisions and augment human work. Decisional AI is primarily used for making decisions and is based on models generated from previous data. Predictive AI, on the other hand, generates predicted values based on custom models and data sets. Training models is critical to the deployment and implementation of AI solutions. The key is to identify a real problem that is operationally relevant and achievable within a reasonable timeframe.\n\n## AI to streamline processes\n\nArtificial intelligence (AI) can be used to streamline decision-making processes in businesses. It is important to scope the AI's capabilities to a specific set of options, so as not to overwhelm the system and make the decision-making process more efficient. AI is best suited for processes that are repeated, involve data-driven decisions, and require subjective human reviews. For example, a financial institution can use AI to validate driver's licenses using extracted data. Continuous training through user feedback to the AI can improve its decision-making abilities and eventually replace the need for human review altogether. Ancestry.com's indexing project as an example of how reinforced learning can reduce the need for human involvement over time.\n\n## Human in the Loop\n\nWhen building a machine learning backend, it's essential to keep the user's needs in mind. The goal is to streamline their current process and provide an aid to help them do their job more effectively. To achieve this, it's crucial to interview and observe users in their current environment to understand their behavior, identify inefficiencies, and document any inferences they make that may not be documented. By doing so, you can curate the data to pull out inferences before sending them to the model, which will result in more accurate results based on replicating human behavior. It's important to remember that the inferences are often more critical than the raw data, and by understanding the user's behavior and needs, you can design a better AI product.\n\nAnother key factor in deploying AI is the human factor. It's about establishing the right context when implementing automation to avoid the fear of job loss. How to deal with potential AI/Human problems, such as stakeholders circumventing the AI put in place for them. One solution is to design the interface to make it clear when a user has reviewed a specific data point and provide overrides where needed. Additionally, asking stakeholders about their reasons for bypassing the system can help improve the model and prevent gaming of the API. Ultimately, AI and humans can work together to achieve better results.\n\nAI can take care of easy tasks if it's well-trained, and humans can learn to leverage AI better over time. Building collaborative interfaces that make AI a team member rather than a cold algorithm, allowing for more natural interactions that can help it learn better. AI will become essential in any job dealing with stakeholders who process human data due to the variability involved. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matthew Pulsipher"],"link":"/episode-EDT130-en","image":"./episodes/edt-130/en/thumbnail.png","lang":"en","summary":"In this episode Darren interviews his son Matthew Pulsipher about productizing decisional AI. Matthew has recently modernized and product development pipeline to include decisional AI in his product development."},{"id":29,"type":"Episode","title":"Digital Transformation in Federal Government","tags":["compute","people","technology","automation","ai","cybersecurity","innovation"],"body":"\r\n\r\nMark has worked with the federal government for over 20 years and has seen firsthand how agencies embrace new technology to improve their services to citizens. One trend he has noticed is a focus on customer experiences, such as using mobile passport applications by Customs and Border Protection. Despite the challenges of delivering large-scale initiatives in constantly changing environments, Mark sees many opportunities for the government to accelerate its mission by adopting new technology.\n\nCOVID has accelerated innovation within the federal government and exposed gaps in areas such as VPN connections and security for remote workers. Agencies continue to innovate, focusing on cloud technology and improving customer experience. Education and communication are critical in making informed decisions about where to place workloads and implementing cloud technologies. Despite concerns that bureaucracy may slow innovation, innovation's benefits continue to drive innovation within the federal government, including cloud, edge computing, cybersecurity, and secure supply chain areas.\n\n## Multi-Hybrid Cloud\n\nThere is a growing trend of multi-hybrid cloud usage and the various challenges that come with it, such as security and data management. The importance of having a well-structured plan for digital transformation projects was emphasized with the need for early success. Another innovation is edge computing, which translates to getting insights from information at the edge faster without needing a connection. Several organizations are currently working to figure out how to leverage edge computing power to improve their services. Some use cases include water sensor monitoring in agriculture and optimizing mail delivery routes for the US Postal Service.\n\n## Automation\n\nWhen automation and cybersecurity are combined, they can accelerate an organization’s ability to react to threats, deploy new capabilities, and provide better services to their constituents. The work shortage arising from baby boomers retiring will require automation to address this issue. One example is how automation improved the inspection process at the border for produce and flowers, resulting in faster processing times and longer shelf life.\n\n## Cybersecurity\n\nThe current IT/OT environment is a constantly evolving threat landscape, with bad actors continuously innovating.  Intel is innovating and creating hardware security technology that encrypts data to protect against nefarious actors. Cybersecurity is top of mind for the Federal government, as noted by President Biden's recent press release emphasizing critical infrastructure protection as the number one cybersecurity issue. Essential management of infrastructure in the US, given the country's unique state structure and the involvement of various entities like local municipalities and private power companies. The Critical Infrastructure Security Agency (CISA) guides cybersecurity and sets standards for secure systems across federal, state, and local levels.\n\n## Secure Supply Chain\n\nAnother critical vulnerability exposed by COVID-19 is the geopolitical risks and cybersecurity threats to the supply chain. Intel's transparent supply chain allows customers to verify the integrity of components, which is crucial in ensuring hardware and software security. With these measures in place, the US is taking steps towards bolstering its industrial and national security.\n\nThe supply chain is not just for physical assets; software, firmware, and hardware all play an important role in supply chain security. All components have attack vectors and require secure software, building materials, component verification, and device integrity for validation—an executive order around software and materials, which Intel is adhering to on their new processors. Intel provides hardware security technologies and transparent supply chain capabilities as part of a zero-trust architecture. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Mark Valcich"],"link":"/episode-EDT131-en","image":"./episodes/edt-131/en/thumbnail.jpg","lang":"en","summary":"In this episode, Darren interview Mark Valcich director and GM of Federal Civilian Public Sector at Intel. Mark's years of experience shine as he describes the current trends in digital transformation in the federal civilian government."},{"id":30,"type":"Episode","title":"Software Defined BareMetal Management","tags":["metify","sdi","technology","baremetal","heterogeneouscompute","compute"],"body":"\r\n\r\nWith over two decades of experience in the data center space, Ian shares his insights on optimizing infrastructure, automating server management, and streamlining different components in a data center. On the other hand, Mike, who previously worked with IBM and Red Hat, focuses on consultative selling and channeled-led sales to better understand the problems and patterns of enterprises in operating their data centers. Together, they created Metify in 2020 to provide automation solutions for managing servers, storage, and network devices in data centers, despite the rise of public cloud services like RWC Azure and GCP.\n\n## Datacenter Automation\n\nAutomating data center management can bring on-prem management closer to a public cloud experience while reducing costs. One of the vital elements of this transformation is standardization and open standards like the DMCA redfish specification in making this automation possible. The ubiquity of this specification across enterprise-class motherboards has made it accessible for all players to integrate with it. Financial pressures and user demand are essential in pushing OEMs toward implementing these open standards. Overall, automation and standardization can help data centers compete with cloud service providers on OpEx costs while improving server management capabilities.\n\nDatacenter efficiency can be improved using tools that allow automation and integration with standard frameworks. The hardware providers face pressure due to technology commoditization, so they must differentiate themselves by offering similar scaled-down white box servers. Metify provides a single pane of glass to manage any manufacturer's devices as long as they are BMC enabled and adhere to the Redfish specification. There are emerging standards to manage small devices through redfish; the question remains about how far the extensibility gets down to the networking-specific parts of the stack.\n\n## Hybrid Cloud\n\nThe rise of hybrid cloud strategies drives how companies manage their data centers, edge, and cloud environments. Public cloud growth is significant; however, there is massive growth in private cloud spaces. Metify provides a more cloud-like experience from an operations standpoint, allowing sysadmins to manage their edge, data center, or multiple data centers all through a standard API. Keeping APIs open and standardized is essential for customers to use familiar management tools like TerraForm and Ansible. Hybrid cloud technologies enable companies to optimize cost, governance, security, and efficiency.\n\nOne of the dangers of an open standard can be vulnerable to security breaches without proper command and control measures. Metify addresses this problem by placing audibility, authorization, access, and controls to the open standards to provide a systemic approach to managing bare metal across heterogeneous hardware. Metify’s product focuses on providing a level of control to prevent undesirable incidents, and they integrate with workflow management tools for automation.\n\n## BareMetal SDI\n\nMany modern data center technologies rely on virtualization as the foundation of their management control plane. However, bare metal management remains a traditional, manually intensive task. This unique approach to private cloud infrastructure doesn't rely on virtualization. Instead, it uses the software-defined infrastructure approach for bare metal management, allowing for a mix of bare metal, VMs, and containers. This approach enables system administrators to manage the underlying infrastructure more effectively and provide a frictionless cloud-like experience. This approach can support heterogeneous computing environments, where CPU, GPU, FPGA, VPU, and NPUs can be leveraged for multiple workflows. With Redfish and an extension of the schema, Metify can easily control new devices, and they see the expanding ecosystem as incredibly valuable for product development. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ian Evans","Mike Wagner"],"link":"/episode-EDT132-en","image":"./episodes/edt-132/en/thumbnail.png","lang":"en","summary":"In this episode, Darren interviews the founders of Metify, Ian Evans and Mike Wagner, about their unique approach to bare metal software-defined infrastructure management using the Redfish standard."},{"id":31,"type":"Episode","title":"Lessons in HPC Oil & Gas","tags":["compute","hpc","people","innovation","cloud"],"body":"\r\n\r\nKeith had worked in the industry for his entire career and joined Intel last July. He had led Consolidated Resources' High-Performance Computing group since 1999, where they started with only one-tenth of a teraflop of computing power. However, Keith successfully consolidated multiple sites and established their team as integral to BP's oil and gas exploration, development, and production operations. Over the last three decades of working in oil and gas high-performance computing, Keith has had three factors influencing the direction of HPC: cloud computing, a shortage of human talent, and the balance between collaboration and first-mover advantage.\n\n## Cloud or On-Prem\n\nKeith shared his experience building an HPC (High-Performance Computing) facility for BP. The construction took almost nine years, with the first seven spent on patching the existing facility and building a business case for a dedicated one. Despite the rise of cloud technology, they found running their facility cost-effective because of control over resources and bespoke architecture. Being in the oil and gas industry presented unique challenges in dealing with massive amounts of data, but having a dedicated facility allowed them to focus on BP's research needs. These factors are also relevant for other industries dealing with HPC clusters.\n\nThe big question for anyone running an HPC cluster is whether or not HPC workloads can be run successfully on general cloud services. That depends on the business's specific needs and the workload scale in question. While some companies may find it practical to collaborate with the cloud for spikes in demand, others require a large, fully utilized cluster to meet their needs.  Factors like predictive performance, data gravity, data size, and cost of operations are essential factors in HPC workload placement.\n\n## Fighting the Resource Shortage\n\nAcquiring and retaining talent in high-performance computing is challenging, particularly as the baby boomer generation retires. Keith suggests that building relationships with universities and identifying talented individuals early is critical to developing the necessary skills within an organization. Organizations must think long-term about creating successful HPC teams and building through mentorships. In these heterogeneous groups, scientists, system engineers, mathematicians, and computer engineers work together for years instead of months.\n\nBuilding long-term commitments by investing in internships, pipelines, and fellowships is critical to creating a career-oriented work environment. Keith shares a challenge they faced in creating a shared vision for their team's future but found success in building a network of contacts outside of BP to help them achieve their goals.\n\n## First Mover Advantage, Sharing\n\nAnother critical aspect of HPC in Oil in Gas is to have the first-mover advantage. Finding a new technique to find new energy sources can save the company billions of dollars. However, sharing information with competitors can also lead to discoveries that help the industry. This is an important balance that HPC professionals need to understand to get the most out of their work.\n\nKeith shares a fun story of how their friends helped develop innovative technology to understand better the Gulf of Mexico subsurface geologic structures to find oil and gas. Despite the competition watching and questioning their methods, BP's experimentation proved successful. Keith explains that everyone is watching when you do something new in the field.\n\nFor example, they often contracted with companies like PGS to acquire data and the innovative solution of using helicopters and hard drives to deliver newly acquired data for processing. The results were impressive, showing better results than traditional methods. When asked about the future of HPC in the industry, Keith believes that collaboration will continue to be necessary, whether through shared clusters or collaborating on foundational science benchmarks. Despite the challenges, the HPC industry is exciting, and there are great opportunities for people to bring their computational skills and willingness to solve challenging problems.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Keith Gray"],"link":"/episode-EDT133-en","image":"./episodes/edt-133/en/thumbnail.png","lang":"en","summary":"In this episode, Darren interviews Keith Gray, a former director of high-performance computing at British Petroleum. With over 30 years of managing HPC centers, Keith gives great insight into the challenges, best practices, and the future of high-performance computing."},{"id":32,"type":"Episode","title":"WaveForm AI","tags":["collectiongenerativeai","artificialintelligence","waveformai","datashapes","waveform","technology"],"body":"\r\n\r\nLogan is a former intelligence officer in the DOD and has a passion for AI and robotics, which led him to be a reservist in the defense community. He transitioned to DataShapes, where they have a unique technology that solves critical needs in the defense community using AI. While AI has been around for a long time, the vast amount of data available for training models and the advancement of computing resources have led to the development of smarter systems like chatbots and large language models.\n\nCurrent techniques are resource-hungry and very costly to train and build a general-purpose inference solution. For example, running large language models like ChatGPT can cost up to $3 million a day, but AI is evolving rapidly and receiving more attention than ever. One concern in using neural networks is the inability to audit and explain how the AI arrived at a result. There are social, political, and legal challenges to trusting decisions made by these networks, particularly in fields where human life is at stake. However, society will eventually overcome these challenges and fully embrace AI.  DataShapes' approach, which uses traditional machine learning techniques to solve pain points in data analysis, has full auditability and discovery in their trained models.\n\nDataShapes has a unique approach to solving problems quickly and efficiently in resource-constrained environments. While traditional neural network training requires a lot of labeled data and can be brittle, DataShapes uses methodologies to learn in real-time or near-real-time. Their technology focuses on waveforms and signals and is auditable, making it ideal for use in austere environments where people are getting dirty and carrying a server stack is impossible. While neural networks are great at language models and image recognition, Logan's company is hyper-focused on waveforms and signals. Their approach is different and highly effective.\n\nDetecting different types of waveforms and relationships in the waveforms is at the center of this new technology. This approach detects patterns harder to spoof compared to traditional techniques used by the Department of Defense. This has the potential for applications of their technology in electronic warfare, including gathering intelligence and analytics. Additionally, the platform can detect, analyze, and gather intelligence, which can be exported to edge devices. Their self-learning anomaly detection feature, Infinite Loop, was also mentioned, which establishes a continuous baseline based on the parameters the end-user prescribes. The technology can be used in the automotive industry, healthcare, and entertainment industries, where it could be embedded in every sensor.\n\nDataShapes has a product called GlobalEdge, an intelligent agent that sits on or behind sensors to conduct ETL operations for data being collected. The machine learning component of GlobalEdge filters the data to provide relevant insights and anomalies in real-time, reducing the amount of irrelevant data being pushed back to headquarters. The product can also be used for data compression on the edge to the data center. The software can scale down to as little as 47 K, making it suitable for a variety of applications, including virus detection using UV waves.\n\nTo find out more about DataShapes and their approach check out https://www.datashapes.com\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Logan Selby"],"link":"/episode-EDT134-en","image":"./episodes/edt-134/en/thumbnail.png","lang":"en","summary":"In this episode Darren Pulsipher, welcomed Logan Selby, the co-founder and president of DataShapes, where they discuss a unique approach to Artificial Intelligence that is bucking the trend."},{"id":33,"type":"Episode","title":"Trustworthy and Ethical AI","tags":["ai","ethics","trustworthiness","deepfake","aicontent","aidetection"],"body":"\r\n\r\nIn the late 2010s, Microsoft released Tay as an AI chatbot designed to learn from its conversations with users on Twitter. However, things quickly went wrong when Tay began spewing racist and offensive comments, causing a public relations nightmare for Microsoft. Despite this, data scientist Gretchen Stewart believes that AI chatbots like Tay can still be useful tools, as long as they are developed with diverse teams that consider ethics and trust. Stewart argues that critical thinking is essential when using AI chatbots like ChatGPT, which are based on biased data and algorithms. AI developers must build diversity and ethics into the development process rather than bolting them on at the end.\n\n## Trustworthiness\n\nAI has immense capabilities but still lacks the human senses and experiences that can affect decision-making. There are ethical considerations in AI development and the need for skepticism when dealing with new technologies. Change is inevitable as the world moves towards the fourth Industrial Revolution, and people must adapt to keep up. However, raising concerns and posing ethical questions is crucial to ensure that AI is used for the greater good.\n\nThere are potential dangers of relying on artificial intelligence (AI) as a source of information. T AI can be helpful; however, it should not be blindly trusted because it is only as good as the data it is fed, which can be flawed and outdated. Critical thinking and questioning are essential when evaluating AI-generated content. Teaching these skills should become part of the curriculum in schools and universities. Additional to questioning AI’s veracity, diversity in AI responses should be considered when utilizing AI when making decisions.\n\n## Ethics\n\nAs technology continues to advance, new ethical concerns continue to arise. This is true of Artificial Intelligence as well as AI-generated content. Recently, an AI-generated collaboration between two artists generated received 15 million downloads in 24 hours setting new records, but the artists involved needed to be made aware of the collaboration. Technologists should be allowed to produce such technology without considering ethical implications. The need for policies to catch up with technological advancements and for designers to create tools that can help ensure the trustworthiness and ethics of AI-generated content. Intel has developed “fake catcher” products as an example of a tool that can help detect fake videos, which is one step towards ensuring the ethical use of AI technology.\n\nCombatting fake AI-generated content has become an industry all itself, and it requires transparency in AI development. This has started an arms race, with bad actors using AI for malicious purposes, and white hats building technology to detect and expose AI generated content. It is important to educate individuals, especially younger generations, about the ethics and potential risks associated with AI. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Gretchen Stewart"],"link":"/episode-EDT135-en","image":"./episodes/edt-135/en/thumbnail.png","lang":"en","summary":"In this episode Darren interviews Gretchen Stewart, Chief Data Scientist of Public Sector at Intel where they discuss the trustworthiness and ethics of artificial intelligence."},{"id":34,"type":"Episode","title":"Resilient Data in Disruptive Comms","tags":["sabr","ddil","technology","edge","process","devops","security"],"body":"\r\n\r\nIn this podcast episode, Darren Pulsipher, Intel's chief solution architect of the public sector, is interviewed by guest host Dr. Anna Scott on resilient data with disruptive comms. The discussion centers around managing data securely and efficiently in environments with limited bandwidth and disrupted service, enabling artificial intelligence and complex data analytics at the edge. Darren talks about his experience working on Toyota's connected car cloud as a basis for solving this problem and how common data management architectures were utilized to create a solution. The customer's gradual unveiling of their challenges helped identify design patterns that opened the architecture for a successful solution.\n\nDespite intermittent connectivity and dynamic edge nodes, the architecture design process to perform edge analytics is complex and hard to articulate, so they took an iterative approach. First, they considered a solution that would bring all data to one place for analysis but found it impractical due to the sheer volume of data at the edge. Moving applications out to the edge seemed promising until the customer requested aggregate analytics across the edge. After exploring design patterns, they settled on using the data stream design pattern with a publish-and-subscribe hub to create data streams for consumers and producers dynamically. While this approach utilized well-established roots in IT, the customer initially had concerns about topic allocation.\n\nIn the past, Darren had to create a PubSubHub ecosystem ahead of time and be familiar with all its components. However, setting up data streams with Kafka or Apache Pulse was challenging, and configuring security was even harder. Utilizing their background in DevSecOps to devise a solution that bundles data stream definitions, input and output definitions, and data transforms into a package consumable by the pipeline. This bundle automatically creates data streams in PubSubHub and sets up all the necessary security measures. The result simplified complexity as developers only needed to focus on algorithms or AI models, while everything else was taken care of in a generic way. The iterative approach with customers helped modify architectures and implementations along the way.\n\nThe key challenge with this type of architecture is working with limited resources, such as only having access to two cores and 2 gigabytes of RAM. How much data processing can happen at the edge with limited resources? One of the constraints the architecture team needed to work on was making the SABR stream manager and security measures as small as possible. Darren stripped the code to the bare minimum and eliminated unnecessary third-party packages. The goal was to create a lightweight stream manager that could run on the edge and be portable across different environments. The resulting Saber architecture was scalable and adaptable, able to run anywhere from a smartwatch to a large Xeon server.\n\nDarren and Anna discuss the difficulties of updating AI models in a distributed ecosystem with numerous instances of the same analytics running. To solve this problem, SABR created a learning data stream that connects all instances of the same analytics and handles intermittent comms, caching, and sending deltas to update the models. They also developed a data channel system using the policy strategy design pattern, allowing for different channels with varying levels of data to be sent based on policy-defined rules. This approach allows for more efficient data transmission, reducing the amount of data being processed and increasing the accuracy of the AI models.\n\nNext is how to operate in the DDIL environment by prioritizing sending data by first sending summaries and historical data before real-time data. It is important of defining system expectations and communications policies upfront to ensure consistency across the ecosystem. An easy-to-use JSON-configured and JavaScript-based policy creation and activation process that eliminates duplication of effort and promotes reuse. The architecture has the ability to quickly add new capabilities by taking advantage of existing sabers and data transforms.\n\nThe last is the resiliency of the system by dynamically recovering from outages in the system. The potential for dynamic and flexible use of the SABR network. Even if a node goes down, it is possible to move a SABR to another box and still get all the data feeds coming in. It is also possible to use legacy computing by running a very lightweight SABR on an old system that collects and passes through data to the SABR network. The importance of security should not be overlooked. All data streams are encrypted and processes are set to establish trust and attestation of the SABRs to prevent spoofing and snooping of data. Overall, the SABR network offers a promising solution for processing data at the edge with flexibility and security.\n\nThe key learning is the iterative architectural approach that the team used to uncover the use cases and pain points that the end users were having. By simulating the architecture, they could find holes in it and get customer feedback. Additionally utilizing design patterns was essential to accelerating the architectural approach. The use of abstraction also provided the ability to swap in and out different technologies throughout the architectural process. Darren believes that this approach has made it easy to create solutions that are easy to use and take advantage of customers' current knowledge.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Anna Scott"],"link":"/episode-EDT136-en","image":"./episodes/edt-136/en/thumbnail.png","lang":"en","summary":"In this podcast episode, Darren Pulsipher, Intel's chief solution architect of the public sector, is interviewed by guest host Dr. Anna Scott on resilient data with disruptive comms."},{"id":35,"type":"Episode","title":"Rebirth of the Private Cloud","tags":["cloud","computing","greenlake","hpe","multicloud","hybridcloud","technology"],"body":"\r\n\r\nThere has been a trend towards repatriation in the commercial sector, where businesses are bringing workloads and virtual machines back from public cloud environments due to financial reasons such as egress charges and budgeting for hyper-scaler workloads. Data sovereignty, security concerns, performance, and latency issues have also led to more companies moving away from the public cloud and back to private cloud technologies. Despite this shift, many cloud promises have been fulfilled to provide ease of use, elasticity, and consumption-based pricing. In order to ease the transition between clouds, the private cloud must provide ease of use, portability of workloads, and new consumption pricing models so highly demanded from the public clouds.\n\n## Cost Control\n\nSam Ceccola, the HPE DoD CTO, believes it's essential to understand the risks and challenges associated with each private, public, and hybrid cloud technology. And architect solutions that take advantage of each technology’s strength. This is where HPE’s production offering, GreenLake, comes to play. GreenLake’s consumption-based service procurement model was first implemented in 2005 and has since evolved to include a self-service portal for managing multiple private and public cloud environments. GreenLake provides customers with the ability to deploy additional resources across the private/public cloud boundary on demand. It also manages and visualizes complex costs, such as egress charges and budgeting for hyper-scale workloads.\n\n## Data Management\n\nAnother major concern for most consumers of public clouds is data management, governance, and sovereignty. Without a cohesive data strategy, many organizations are heavily burdened with data egress costs moving data in and out of public clouds. There are several different approaches to managing data across cloud boundaries. HPE uses an approach of cloud-adjacent storage which decreases the amount of data moved into the cloud to a minimum. GreenLake's cloud-adjacent storage keeps data on-premises while running compute workloads in the cloud. This approach ensures data sovereignty and reduces egress costs and latency issues associated with moving large amounts of data between clouds.\n\n## Portability\n\nAnother key aspect of multi-hybrid cloud architectures is the ability to easily move workloads between clouds. Some organizations have moved to stateless container-based microservice architectures to provide the flexibility and portability of workloads. However, not all workloads can easily be containerized especially workloads that have applications that contain state. In these cases, virtual machines are used to give the ability to move applications from one cloud to another.  Moving VMs between clouds can be problematic as cloud service providers use different hypervisors and CPUs that may not be completely compatible.  Understanding the limitations of these migrations is critical in purchase decisions of hardware both virtualized and non-virtualized.\n\n## Security\n\nGreenLake does not limit customers to on-premises deployment, as it allows them to provision workloads across various cloud environments, including Amazon, Google, and Azure. In addition, GreenLake supports true hybrid workloads, where a workload can run both on-premises and in hyperscale cloud environments simultaneously. Running workloads across cloud boundaries can cause additional exposure to cyber-attacks. Understanding the security models of the different public and private cloud technologies can be a daunting task and increases the human capital required to effectively architect and manage a strong security position. The GreenLake platform handles security through its agnostic security engine, Security Central, which supports multi-cloud attestation and zero-trust architectures for independent clouds, managing the complexity and integration of security models.\n\nArchitecting a multi-hybrid cloud solution is nontrivial and requires an understanding of private and public cloud technologies, their limitations, and their strengths. luckily, there are organizations like HPE that have been working with multi-hybrid cloud architectures for several years and have learned the INS and outs of effective management of workloads across this new flexible environment.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sam Ceccola"],"link":"/episode-EDT137-en","image":"./episodes/edt-137/en/thumbnail.png","lang":"en","summary":"In this episode Darren interviews Sam Ceccola, CTO of DOD for HPE about the new business and technology models changing the way organizations consume hybrid cloud."},{"id":36,"type":"Episode","title":"Evolution of Cloud","tags":["cloud","computing","dell","multicloud","hybridcloud","technology"],"body":"\r\n\r\nThe evolution of cloud computing is one of the most significant technological advancements of the last decade. The cloud is transitioning from a simple concept to a complex and ubiquitous system that now dominates business operations and strategies. As Ken White from Dell Technologies tells us, the cloud is no longer a single location or a one size fits all solution. Instead, it is everything, everywhere. \n\nWith hyper scalers like Amazon and Azure expanding constantly, the cloud represents a consistent experience across platforms with predictability in performance and cost. However, hidden fees exist, particularly when moving data in and out of the cloud. It is essential for companies considering the cloud to fully adopt a cloud mindset concerned with consistency and predictability, as well as flexibility, performance, and security. While some executives are attracted to the cloud for its potential to reduce costs and IT management, it is essential to remember that it cannot solve every issue and is not a one-way street. The cloud can ultimately improve operations, increase efficiency and ultimately deliver a better experience for businesses and customers alike, provided a well-planned operational mindset is adopted.\n\nIf you're in the business world, you've likely heard about the cloud, but have you heard about the cloud mindset? It's a way of approaching technology that enables adaptability, portability, and predictability regarding cloud workloads and costs. That means you can develop portable applications across different ecosystems and choose where workloads land based on business requirements, not just technical constraints. It also means you can predict and control costs, a welcome relief in an ever-changing technological landscape. \n\nBut changing mindsets is challenging, especially for organizations accustomed to a particular way of operating. This is where training and a commitment to continuous learning come in. Organizations that successfully adopt the cloud mindset prioritize a positive user experience, are willing to adapt to new architectures, and learn and change as technology evolves. \n\nAdopting the cloud mindset doesn't mean you'll have to fire all your IT people and move everything to the cloud. Instead, it's about training your workforce and rethinking your organizational philosophy. With a suitable management layer and a focus on flexibility and agility, organizations can unlock the benefits of the cloud for their businesses and enhance the experiences of their end-users, whether they're developers or customers.\n\nConsidering a cloud strategy, you must understand that you may experience failure. Moving applications to the cloud isn't always straightforward and can come at a cost. However, your chances of success are much higher with the right consistency and governance. \n\nConsistency doesn't necessarily mean a single pane of glass, but it does mean having a standard process for managing and deploying workloads and infrastructure. It also includes consistency with governance, processes, and workflows. This level of consistency ensures that everyone in your organization understands how to manage the cloud.\n\nCloud adoption failures are only sometimes avoidable, but repatriation may be an option to bring your workload back to an on-premises solution. However, this option can also be costly, so weighing the short-term and long-term benefits is essential.\n\nContainers can also make a difference in cloud adoption by eliminating problems associated with moving applications to the cloud. But remember, no silver bullet will solve all your cloud adoption issues.\n\nOverall, the key to success with a cloud strategy is to focus on consistency and understanding the costs involved in moving to the cloud. With these factors in mind, your chances of success are much greater.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sam Ceccola"],"link":"/episode-EDT138-en","image":"./episodes/edt-138/en/thumbnail.png","lang":"en","summary":"In this episode Darren interviews Ken White from Dell Technology about how Cloud technology is more than technology, but a process and cultural change in organizations."},{"id":37,"type":"Episode","title":"Resilient Logistical Analytics","tags":["data","analytics","artificialintelligence","pathway","technology"],"body":"\r\n\r\nAre you interested in learning about how digital transformation is affecting logistics? In this episode of \"Embracing Digital Transformation,\" special guest Adrian Kosowski, chief product officer at Pathway, discusses resilient analytics in logistics. Pathway focuses on studying real-world systems from a distributed computing perspective, and they specifically work with data in the logistics and transportation vertical. One of the biggest challenges in this field is aggregating data at scale and making sense of it, which is where machine learning analytics come into play. Kosowski also notes that logistics is a highly concentrated market, controlled by just a handful of companies, which makes even small improvements in processes incredibly valuable to the world economy. However, collecting data from the extreme edge, such as containers in the middle of the ocean, presents its own challenges, such as optimizing energy and communication for battery powered IoT devices. In summary, if you're interested in the intersection of logistics and digital transformation, this episode provides valuable insights into the challenges and opportunities of this field.\n\nThe use of internet of things (IoT) devices in the logistics and supply chain industry can greatly benefit businesses with end-to-end visibility and improved analytics capacity. However, there are several challenges associated with the use of these devices that need to be addressed.\n\nOne major challenge is the stability and reliability of these edge devices. In case of a device failure or crash, businesses should have a way of optimizing these devices without them guessing or performing incorrectly. This is especially critical for real-time systems that are used to events coming in order.\n\nAnother challenge is the accuracy of data collected by IoT devices. Some data may be inferred, and contextual analysis may be required to interpret the meaning of a given data point and distinguish measurement issues from process issues. Inaccurate or untimely data can lead to risks in the supply chain and make it difficult to optimize the transportation network.\n\nHowever, the widespread use of IoT devices can lead to end-to-end visibility and observability of processes throughout the supply chain. This can help businesses optimize their processes and become proactive in their approach. This can benefit all actors in the supply chain – from transportation and logistics providers to retailers and manufacturers.\n\nWhile there is no specific group or segment that businesses should target for the adoption of IoT devices, widespread adoption and cooperation among all actors can lead to significant benefits for the global supply chain.\n\nPathway was developed to address the deficiencies in existing data streaming technologies and provide a tool for advanced analytics pipelines on top of data streams. One of the key features that sets Pathway apart is the ease of describing logic as if it were meant for a batch system, while also making sure it works in a real-time system with out-of-order data.\n\nPathway was developed with IoT data in mind, but it can also handle data from video monitoring, server performance monitoring, log monitoring, and other physical entities. This allows for the complexities of data when it comes to anomaly detection, alerting with out-of-order data, and data with time series and geospatial elements to be handled in a cloud-agnostic manner.\n\nAnother important feature of Pathway is the ability to use Python scripts for data streaming analytics in both real-time and batch modes. This means that data scientists and engineers can develop their analytics pipelines in the normal development environment they are used to, and conveniently work with the streaming system. Additionally, Pathway allows for a much larger amount of history to be considered when running computations, which is a big difference from lightweight stream processors that only handle small amounts of data.\n\nOverall, Pathway offers a bridging solution for organizations that need to combine batch and real-time data processing and provides a way to add value to data by adding structure and extent information for downstream use in business intelligence and analytics.\n\nData Streaming technology is becoming increasingly important as businesses seek to make decisions faster and respond more quickly to customer needs.  When you use data streaming, you can quickly detect and respond to trends, anomalies, and other important information. Pathway offers a suite of products specifically designed for working with time series data, logistics data, and data streaming. They offer examples and developer information on GitHub, as well as a vibrant community on Discord.\n\nData streaming is a critical technology for businesses in a wide variety of verticals. For example, it can be used in logistics to optimize routes and predict when shipments will arrive. In finance, it can help detect fraud and predict market trends before they become widely known. In transportation, it can be used to monitor the performance of engines and vehicles in real time.\n\nIf you're interested in learning more about data streaming and how it can benefit your business, be sure to check out Pathway's website and community [www.pathway.com](www.pathway.com). With the right tools and expertise, you can leverage this powerful technology to improve your operations and stay ahead of the curve.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Adrian Kosowski"],"link":"/episode-EDT139-en","image":"./episodes/edt-139/en/thumbnail.png","lang":"en","summary":"In this episode Darren interviews the Adrian Kosowski CPO of Pathway about their unique ability to handle logistical data from the edge in DDIL environments with real-time analytics."},{"id":38,"type":"Episode","title":"Kickstarting Your Organizational Transformation to Become Data-Centric","tags":null,"body":"\r\n\r\n## Operationalizing Analytics System\n\nThe first thing to understand about the process of operationalizing analytics is that it is complex. Some people think data is the new electricity and works in a similar system. Consider this example:  In a simple electrical system, there is a power source and some wires that go to a light bulb. As long as there is the power source and the wires are high integrity, you can turn off the light, and then six months later come back and flip the switch and know the light bulb will come back on.\n\nAn analytical system does not work this way. There are many types of “short circuits” and variables that can cause failure, such as entropy. It’s important to have the right people on your teams who can work through and understand all the different types of faults that can occur or you will not get correct or useful insight.\n\nYour data must be formatted in a certain way for machine learning algorithms. The algorithm must understand what the data represents to look at the pattern appropriately.  Unless the data is carefully formatted, there is a garbage in/garbage out risk.  This is where your engineers, data scientists, and other supporting data architects are important.\n\n## Asking the Right Questions\n\nAnother part of the system is making sure you have the right business questions. An organization must supply foundational questions about what they are trying get from the data: What insights does our business need? How do we paint a better picture of what our landscape actually looks like? What data will allow us to do that?\n\nMany executives love the idea of doing analytics, AI, and big data, but don’t know how to get started. The first step is for executives to take ownership of what they want the data to do for the organization.  What business value is going to come from the data? What type of data do we already have? What data can we create that will paint a more complete picture of what is happening?  What does success look like? The focus should be first on the organizational foundation.\n\n## Data Collection, Preparation, and Insights\n\nAfter those questions are answered, the next step is to outline what needs to be accomplished and start breaking it down into smaller parts. Organizations should be looking at their current data sources, determining their reliability, and identifying what is missing, followed by data collection and preparation.\n\nWith the data in hand, it is time to create the story. This involves not only data scientists, but domain experts and business and marketing people. Input from these different groups will ensure the data will add value to the business rather than becoming just an academic exercise or a one-time experiment.  From the data, you ultimately want valuable, reliable insight that will help improve your business rather than simply data collection that is convenient but not actionable.\n\nNow that you’ve collected the data, analyzed it, and have some insight, how do you operationalize it?\n\n## Operational Analytics Example of Success\n\nAn example from manufacturing is a good way to show how the data successfully works through the pipeline. Start with the foundational business value of maximizing your product yield. What data sources could tell you if you have broken or disfigured products? Cameras at the end of assembly lines could create a deep learning model that detect or predict if a product is broken or disfigured. A data scientist could then train a model based on good product and bad product to improve yield.\n\nThis type of operationalized data could be applied to several key areas that add value to an organization. Quality control, such as making sure raw material meets specifications is one example. Predictive maintenance of machinery and regulatory compliance, such as in a weight requirement for a food product, are other examples.\n\nBy tapping into the data in real time about product, quality, or regulatory issues, you are not just creating a higher product yield, but minimizing costly human tasks, such as inspection at the end of an assembly line. These employees can be redirected to higher value tasks. You are also minimizing waste and creating a more consistent consumer experience. In addition, you are protecting the business by minimizing regulatory risks.\n\nThis is a win-win scenario. The more the organization knows what they want to gain from data, what data to collect, and how to utilize it based on a business strategy, the more successful they will be.\n\n## More to Come\n\nThis is the first of six episodes that talk about the data-centric organization. We hope you will join us for those future podcasts.  Want more? Check out Sarah’s website,  \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT14-en","image":"./episodes/edt-14/en/thumbnail.png","lang":"en","summary":"Today’s episode is all about kick-starting your organization to become data centric and the value this can bring. Darren’s special guest is Sarah Kalicin, lead data scientist for data centers at Intel."},{"id":39,"type":"Episode","title":"Background Checking Your Open Source","tags":null,"body":"\r\n\r\nIf you're a software developer, you know the feeling of pride that comes with creating a popular package or tool that many people find useful. However, this popularity can sometimes attract the attention of attackers who look for vulnerabilities to exploit.\n\nIn a recent podcast, software engineer Jay Phelps shared his experience of discovering a vulnerability in a widely-used package he created. After realizing the potential impact of the vulnerability and the sheer number of instances of the package in the wild, Phelps quickly worked to fix the issue to prevent attackers from exploiting it.\n\nThis scenario highlights the importance of vigilance for software developers, especially those who create popular packages or tools. While it may be tempting to bask in the glory of a widely-used product, it's crucial to remember that popularity can also attract attackers. Regular checks and updates to address any vulnerabilities can help protect users and prevent exploitation.\n\nAs a software developer, it's important to approach your work with both pride and caution. While it's great to contribute to society with your creations, don't forget to prioritize the security and safety of your users. Stay vigilant and keep your packages up-to-date to prevent vulnerabilities from being exploited by attackers.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Michael Mehlberg"],"link":"/episode-EDT140-en","image":"./episodes/edt-140/en/thumbnail.png","lang":"en","summary":"In this episode, Darren interviews Michael Mehlberg about increasing confidence in open source through background checking the open source communities."},{"id":40,"type":"Episode","title":"From Neurology to Neuromorphic Computing","tags":["neurology","neuromorphic","security","ai","cybersecurity"],"body":"\r\n\r\nAs a child neurologist, Dr. Follett emphasizes the importance of understanding the brain's developmental processes and its remarkable ability to overcome even catastrophic injuries. She shares how her extensive research involved studying rat models and cell cultures to gain insight into children's development and search for ways to help them get the best outcomes, despite injuries or illnesses. Listening to Dr. Follett's insights and experiences allows for a better understanding of how neurology is critical in understanding the brain's processes and how we can better appreciate and support brain development, particularly in children. Dr. Follett also shares her unexpected appearance at a high-tech show, where she utilized her neurology expertise to help a man experiencing seizures during a keynote address. This podcast highlights the potential reach of neurology beyond just medical institutions and the essential role of neurologists in understanding the complexity of the human brain.\n\nHave you ever needed help understanding a complex technology or product, even though you just needed to know how to use it? Dr. has found that her skills in explaining medical problems to patients translate well into explaining complex technologies to non-experts. Her work as a neuroscientist studying brain development led her and her husband to start a high-tech startup that does neuromorphic computing, called Lewis-Rhodes Labs. One of their products, Extreme Search, uses a neuromorphic processor to search through massive amounts of data, mimicking how a brain rapidly recognizes and processes information.\n\nDr. Follet emphasizes that there are better approaches than mimicking a brain when creating technology. While brains make thousands of mistakes daily, we don't necessarily want our computers to do the same. Instead, we can take careful lessons from how brains work and apply them to create more efficient and effective technology. This experience highlights the importance of interdisciplinary skills and thinking outside of one's specific field. By finding skills that transfer across different areas and industries, we can bring unique perspectives and solutions to complex problems.\n\nExtreme search technology is a breakthrough in cyber forensics and real-time analysis that solves the challenge of sifting through Petabytes of unstructured data in record time using cutting-edge hardware and software. Extreme search technology offers high performance, low power consumption, and a constant throughput and is modeled after the human brain designed for high-performance data processing.\n\nUnlike traditional data search methods that require moving data around, extreme search technology allows on-site searches that eliminate all network bottlenecks. The technology suits cyber forensics, cybersecurity, legal discovery, and enterprise data search. Extreme search technology is straightforward for users and requires no new language or pre-identification of patterns but instead uses regular expressions to perform ad-hoc searches of any data described in the text.\n\nExtreme search technology performs sensitive searches on storage appliances and provides real-time analysis identifying potential threats in milliseconds. Combined with traditional detection methods, the technology can detect advanced persistent threats, viruses, adware, trojans, worms, rootkits, and other malware quickly.  The use of Extreme Search technology is not limited to cyber forensics. Other research fields, such as genomic research or any unstructured data field, can benefit from Extreme Search technology's ability to search vast amounts of miscellaneous data in record time.\n\nMany organizations need help finding patterns or insights within their data. Or they have vast databases and struggle to find anything because of the sheer amount of information. Many data scientists resort to indices to decrease the time to find information. This works well when you know what you want when collecting or storing the data. However, many organizations deal with opaque data that fits outside a predetermined structure. In this case, brute-force searches of petabytes of data can take weeks to find common patterns of data not previously determined.\n\nExtreme search technology helps bring visibility to new data areas, allowing for improved analysis and analytics that start with the search and can go faster if data is transformed into the required pieces. This is particularly useful when dealing with healthcare data, where there are massive amounts of structured data, yet information fits outside any database structure.\n\nFor more information on Extreme Search technology and approaches to digital transformation, visit Lewis-Rhodes.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Pamela Follet"],"link":"/episode-EDT141-en","image":"./episodes/edt-141/en/thumbnail.png","lang":"en","summary":"In this podcast episode of Embracing Digital Transformation, Dr. Pamela Follett, a neurologist and co-founder of Lewis Rhodes Labs, shares her background and expertise in the field of neurology, specifically with regards to research on the developing brain in early childhood."},{"id":41,"type":"Episode","title":"Data Protection with Confidential Computing","tags":["cybersecurity","technology","sgx","confidentialcomputing","dataprotection"],"body":"\r\n\r\nin place to ensure that only those who are authorized can access certain data or systems. However, even with those controls in place, those with elevated privileges, such as cloud admins or sysadmins, still have access to sensitive data and systems. This is where confidential computing comes in, as it adds an extra layer of protection against malicious insiders or those who may accidentally cause a breach.\n\nControlled access refers to limiting and monitoring access to sensitive data or systems based on authorization and authentication protocols. Privileged access refers to when someone has elevated privileges or administrative rights that allow them to access sensitive data or systems beyond what is normally authorized or controlled.\n\nConfidential computing helps to eliminate this privileged access by creating a hardware-based execution environment or trusted execution environment that prevents unauthorized access or modification of applications and data. By using confidential computing, organizations can maintain control over their sensitive data while still taking advantage of cloud computing, edge computing, and multi-party compute without compromising security. It's an important capability that ensures the protection of sensitive data for organizations in the digital age.\n\nConfidential computing is a new technology that allows users to maintain control over their data even when it is stored in third-party servers such as those used in cloud computing. With confidential computing, users can encrypt their data while in memory, thereby protecting it even from privileged users and rogue administrators. This means that even if an attacker gains access to the server, they will find the data to be in an encrypted state, thus safe from prying eyes. Confidential computing is especially important for sensitive data such as medical records or financial information.\n\nConfidential computing relies on hardware-level encryption, which provides a much stronger protection than software-based encryption. Since hardware encryption is implemented at the processor level, it requires no additional software or drivers and thus places minimal performance overhead on the system. Confidential computing is also very easy to use since it works transparently with existing software and applications.\n\nThe benefits of confidential computing are many. Since data is encrypted while it is being processed, sensitive information is not visible to third parties, thereby keeping it private and secure. Confidential computing can be used not only in the cloud but also in edge computing environments. As we continue to see an increase in the amount of data being generated and stored, the need for secure and trustworthy computing environments becomes even more important. Confidential computing is one of the technologies that can help achieve these goals.\n\nConfidential computing is an essential component of a zero-trust architecture. A zero-trust framework operates under the assumption that a cyberattack can happen at any given moment, and thus, there is no such thing as a trusted resource. Each user and device must be authenticated repeatedly before every interaction, regardless of whether they have already been verified. Confidential computing provides an additional layer of security as it aims to protect data from cyberattacks and security breaches by ensuring that only the necessary places have access to it. This is achieved by bypassing the operating system and cloud stack and speaking directly to the chip, which manages access to memory.\n\nIntel has been at the forefront of confidential computing with the development of SGX and tDCS. These technologies fall under the larger bucket of privacy-enhancing technologies that aim to provide solutions in the space. Fully homomorphic encryption is another solution that addresses the problem purely from a cryptography perspective by keeping data always encrypted.\n\nWhat makes cutting-edge computing and trust execution environments unique is that they are available broadly today in production mainstream workloads with very little performance overhead. Being able to take a native workload, even in an unchanged format, and run it within an encrypted and isolated environment is a powerful tool for organizations to protect their data.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Anna Scott","Ibett Acarapi","Jesse Schrater"],"link":"/episode-EDT142-en","image":"./episodes/edt-142/en/thumbnail.png","lang":"en","summary":"In this episode Dr. Anna Scott interviews Jesse Schrater and Ibett Acarapi about how to protect data using confidential computing."},{"id":42,"type":"Episode","title":"Use Cases in Confidential Computing","tags":["cybersecurity","sgx","confidentialcomputing","ai"],"body":"\r\n\r\nConfidential computing is a revolutionary technology that allows data to be processed without exposure. It is advantageous in artificial intelligence, where sensitive data is involved in model development and deployment. It protects AI models and confidential data, enabling the development of better training models and diverse data sets. Collaboration between experts and enterprises is also affordable through confidential computing, which allows the creation of larger models and protects proprietary data.\n\nThe technology is available today and can be used by businesses through various providers, including Azure, GCP, IBM, and Alibaba. OEMs that offer servers for on-prem use also have SGX boxes that can be provisioned with SGX. Confidential computing development expertise is unnecessary to take advantage of confidential computing capabilities, as plenty of solutions allow native workloads to be migrated through the lifted shift model or cloud-first solutions.\n\nTo get started with confidential computing, businesses can put the requirement in their RFP and contact a vendor that offers it to incorporate it into their solutions. Many vendors already offer confidential computing solutions, and Intel is happy to participate. Confidential computing provides a new way to manage data, allows businesses to leverage the benefits of cloud or multi-party computing on sensitive data, and ultimately provides a safer environment for companies to handle confidential information.\n\nConfidential computing is a pioneering technology rapidly growing in popularity, given the increasing numbers of data breaches being perpetrated in today's ever-connected world. Confidential computing allows you to isolate sensitive data within secure computing enclaves or \"containers,\" which are difficult to access even if an attacker gains access to the host system. Confidential computing is about bringing an additional layer of security to computing environments that require the robust protection of sensitive information or Intellectual Property (IP).\n\nAs the podcast explains, there are many ways to implement confidential computing, from leveraging the toolsets provided by cloud service providers (CSPs) to using the services of software security vendors. It's also apparent that more companies are starting to move towards confidential computing to protect their IP and sensitive customer data.\n\nHowever, the technology is still in its infancy, and its full potential is yet to be unlocked. Experts agree that one of the most exciting areas to watch is the rise of \"smart\" confidential computing. This refers to an approach that leverages artificial intelligence and machine learning to improve the efficacy and flexibility of confidential computing.\n\nAnother area to watch includes the development of advanced threat detection and analysis tools that leverage confidential computing in real-time to improve the analysis of potential threats and accelerate the response to attacks. Whatever the future holds for confidential computing, the technology is evolving at breakneck speed, and its continued adoption is a positive development in the fight against cybercrime.\n\nConfidential computing is a rapidly growing field that promises to protect sensitive data. The industry has evolved and grown so much that now it is worthy of a whole summit dedicated to it. The Combat Edge Computing Summit in San Francisco is the inaugural event to feature confidential computing. It will focus on the future of confidential computing and how it will impact web three, blockchain, and distributed ledgers. Confidential containers are a significant area of cloud-native development, and many products are coming. This technology is now available and ready to use today, and vendors can take your real-life workload data on board. Confidential computing is exciting because it is a natural solution to data security and privacy. Confidential computing technology puts data protection in the hands of users, providing a secure environment that enables data to be analyzed without exposure. As confidentiality becomes increasingly essential, confidential computing will become ubiquitous, and it is something that any organization should use to safeguard sensitive data. So if you're interested in implementing confidential computing in your environment, don't hesitate to reach out and ask for assistance. Confidential computing is doable, and companies like Intel are willing and able to help customers in this journey.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Anna Scott","Ibett Acarapi","Jesse Schrater"],"link":"/episode-EDT143-en","image":"./episodes/edt-143/en/thumbnail.png","lang":"en","summary":"In this episode of Embracing Digital Transformation Dr. Anna Scott continues her conversation with Ibett Acarapi and Jesse Schrater about Confidential Computing and their uses in AI, and software development."},{"id":43,"type":"Episode","title":"Science Behind Digital Twins","tags":["edge","artificialintelligence","digitaltwin","iot"],"body":"\r\n\r\nHave you ever wondered how robots and machines navigate the physical world around them? It's all about accurately representing the natural world in a digital form called a \"digital twin.” A digital twin has a standard coordinate system enabling different applications to make sense of a real space or environment. It's like a virtual version of the natural world, allowing more efficient and effective data reuse across systems.\n\nDigital twins may not be a term we commonly use, but the concept is becoming increasingly popular, especially in the manufacturing, retail, and security industries. A digital twin is a virtual replica of a physical object, process, or system that can be used to monitor and control it in real time. One example is Google Earth, a highly latent digital twin of the world. However, as technology advances, there is potential for reducing latency and creating near-real-time digital twins for more efficient control and monitoring.\n\nThe applications for digital twins are vast and varied. For example, factories can use digital twins to improve safety and optimize production lines by monitoring where products and where people are for security purposes. Digital twins can also be used in augmented and virtual reality, enabling users to walk through spaces that may be inaccessible or dangerous in the physical world. Even everyday tools like Google Maps use a form of a digital twin to provide real-time traffic updates and information on accidents.\n\nDigital twins are becoming increasingly important in the development of machine-based AI. Just like humans need spatial awareness to make sense of the world around us, machines need digital twins to navigate and interact with the physical world. The possibilities are endless for this technology, and it's exciting to think about how it may shape our future.\n\nThe implementation of digital twins requires the integration of multiple sensors and calibrating their data into a common representation or digital twin. This process can be complex and requires standard units to ensure consistency between different industries.\n\nInterestingly, the gaming industry has inspired the development of digital twins due to their experience creating virtual worlds with complex physics engines. By adopting existing standards used in the gaming industry, such as the Universal Scene Description format, it is possible to develop a universal representation of physical spaces.\n\nDigital twins also have the potential to enable closed-loop control in various applications, introducing near-real-time current rules to systems. Perhaps in the future, we will have intelligent scenes similar to those in the Iron Man movie, where one can speak to their home's smart assistant and control devices through it. The possibilities of using digital twins seem endless, and we will likely see more of them affecting our daily lives.\n\nEfficiently solve complex problems in various industries. Intel's Scene Scape is a product that came from the company's efforts to develop a vision that would enable the transformation of pixel-based units into real-world units and cameras. The product is meant to help turn sensor data into virtual models of the natural world, known as digital twins, that can be used to drive better outcomes and operational efficiencies. The technology relies on multi-modal tracking and motion modeling. It can monitor and track people, vehicles, and equipment across various use cases, including transportation, healthcare, retail, and factories.\n\nOne of the exciting aspects of Scene Scape is its ability to estimate where someone will go and the next camera they should be showing up in. This is useful when trying to cover large spaces with cameras or sensors. However, there is always an error bar on the measurement, which means different sensors may need to agree on where something of interest is. To address this, Scene Scape uses a motion model to extrapolate movements, allowing for accurate tracking and monitoring of subjects.\n\nOverall, digital twins are a technology still in its infancy, but the potential for their use is enormous. As technology continues to improve, we will likely see more applications for digital twins and more industries leveraging their use to drive better outcomes.\n\nAre you curious about how digital transformation can benefit you and your family? In this episode of Embracing Digital Transformation, Rob discusses the various use cases for digital tools. One exciting example he brings up is using technology to track your children. While this may seem controversial, Rob argues it is a responsible use of digital tools. Monitoring your child's location and activity can give parents peace of mind and help ensure their safety.\n\nHowever, this is just one example of countless use cases for digital transformation. Rob encourages listeners to think about how technology can improve outcomes for themselves, their businesses, and their communities. From streamlining processes and increasing efficiency to enhancing communication and delivering better customer experiences, digital tools can provide many benefits. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rob Watts"],"link":"/episode-EDT144-en","image":"./episodes/edt-144/en/thumbnail.jpeg","lang":"en","summary":"In this episode Darren explores the science and use cases behind digital twin technology with the principal architect of Intel's ScheneScape."},{"id":44,"type":"Episode","title":"Attracting People Back to the Office","tags":["people","remoteworker","collaboration"],"body":"\r\n\r\nGPA (Global Presence Alliance) was founded 15 years ago to address the need for a better model in the collaboration space. At the time, video conferencing was becoming more prevalent, and organizations were considering a global strategy. However, they needed more options - relying on regional integrators or dealing with a complex setup that needed to understand collaboration truly.\n\n## People, Space, and Technology\n\nGPA aimed to solve this problem by providing a comprehensive global collaboration and video strategy approach. They recognized the need to balance people, space, and technology to create exceptional collaborative experiences. By bridging the gap between different regions and understanding the unique requirements of each organization, GPA offered a better alternative to existing solutions.\n\nWhile technology has evolved over the years, there is still work to achieve true collaboration. Microsoft, for example, has introduced signature rooms that mimic the telepresence room concept at a fraction of the cost. However, nonverbal cues and physical interaction are still challenging to replicate in virtual environments. As the technology advances, we will see improvements in the collaborative experience. Until then, organizations like GPA are crucial in finding innovative solutions and helping businesses navigate the ever-changing digital transformation landscape.\n\nThere still are challenges in video collaboration technologies. However, new advances in technology are overcoming some of those challenges. One of the biggest is the whiteboard brainstorming session.   Due to camera angles and other limitations, integrating whiteboarding experiences into video calls is still unnatural. However, efforts are being made to create more natural and integrated expertise using AI and camera technology. Technology can provide a second-best experience; it cannot replace the personal and emotional experience of being physically in the same room as someone. This human element includes things like water cooler conversations and the ability to touch and feel objects.\n\n## Unique Business Model\n\nGPA has a unique business model; it takes a bottom-up approach, with business units in 50 countries working as shareholders in a parent entity. This allows them to achieve global scale while maintaining cultural awareness and diversity.\n\nWhen implementing collaboration strategies for multinational companies, the company takes a programmatic rather than project-based approach. They have centralized teams for account management, project management, and solution architecture while relying on regional teams for deployment and support. This collaborative approach reflects the company's philosophy and is crucial for success in implementing complex collaboration technologies.\n\n## COVID-19\n\nThere was a profound shift in the collaboration world before and after COVID-19. Pre-COVID, most of our work and collaboration were done in physical office spaces, but with the pandemic, everyone was forced to work remotely. This shift in the work environment required a change in thinking and approach.\n\nIn the past, remote participants were often treated as second-class citizens, but now, with the increase in remote collaboration, the experience has become more equalized. People have gotten used to the virtual meeting experience and expect a similar experience when they return to physical meeting spaces. This has led to a demand for a better experience in the office.\n\nThe shift to remote work has also highlighted the importance of understanding human factors in the workspace. Different individuals have different needs and preferences when it comes to their work environment. For example, some people may find noise distracting, while others may thrive in an open and collaborative space. Understanding these human factors and aligning technology with people's needs has become even more crucial.\n\nOrganizations are still experimenting and learning how to create effective collaborative spaces. The industry is also starting to focus on collecting actual data to understand the true impacts and manage the outcomes of these collaborative spaces.\n\nThe shift to remote work during COVID-19 has necessitated a change in thinking and approach to collaboration. There is a demand for a better experience in remote and physical meeting spaces and a need to understand human factors in the workspace. The industry is still experimenting and learning, and there is a focus on collecting actual data to manage and improve collaboration outcomes.\n\n## Future Vision\n\nIn the future, the office space will be more focused on creating meaningful experiences and fostering human connections. The primary attraction of the office will be the presence of other people and the opportunity to have face-to-face interactions that can't be replicated through video conferencing. Microsoft is leading the way in utilizing AI and data to make predictions and recommendations that enhance the office experience.\n\nAdditionally, the office space will have a greater emphasis on wellness. Employees may need access to optimal furniture or amenities in their home offices, so providing a dedicated space for focused work can contribute to overall health. Sustainability is also a factor to consider, as staying at home may only sometimes be the most energy-efficient option.\n\nRegarding technology, chat, and collaboration platforms will be crucial in facilitating communication and collaboration among hybrid workers. AI and camera technologies will enhance meeting room experiences by automating specific tasks and creating a more immersive environment. There will also be an increase in media production capabilities, with more companies creating their narrowcasting channels for both internal and external communication.\n\nOverall, the future of the office will be a balance between leveraging technology and prioritizing human connections and experiences. It won't be a one-size-fits-all approach but a customized space that reflects the company's care and concern for its employees.\n\n## Bring People Back to the Office\n\nByron acknowledges that getting customers out of their office spaces can be challenging, just as much as it is for employees. When attracting people to a physical location, it is essential to consider the entire ecosystem of partners and customers. This highlights the need to create spaces and experiences that are enjoyable and enticing for everyone involved.\n\nByron also emphasizes the human factor in collaboration and AV (audiovisual) technology. He points out that his theater and stage management background has given him a unique perspective on the importance of human interaction and engagement. He believes the human factor makes collaboration and AV technology impactful and successful.\n\nYou can find out more about GPA at their website [https://www.thinkgpa.com](https://www/thinkgpa.com)\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Byron Tarry"],"link":"/episode-EDT145-en","image":"./episodes/edt-145/en/thumbnail.png","lang":"en","summary":"In this episode, Darren talks to the CEO and Managing Director of GPA about the role that collaboration innovation plays in bringing people back into the office and why people need face-to-face interaction."},{"id":45,"type":"Episode","title":"Embracing the AI Revolution","tags":["collectiongenerativeai","ai","people","embracingdigital"],"body":"\r\n\r\nAs a CIO looking to lead your company through digital transformation, it's important to remember that technology isn't the only piece of the puzzle. According to Dr. Michael Lenox, a respected author and professor in the field, digital transformation is about much more than just cloud computing and data organization. It's a strategic initiative that requires cross-functional collaboration and a holistic approach.\n\nTo effectively navigate digital transformation, your leadership team and entire organization must embrace the change and understand the broader implications beyond just digital infrastructure. This means reflecting on where your company is today and where it wants to go in the evolving competitive landscape. It also requires collaboration between the C-suite, product team, sales, and other key stakeholders.\n\nAs you navigate this initiative, remember that it's not just an IT project happening in the background. It's a fundamental change to the basis of competition, customer relationships, and business models. To drive effective change, you must leverage people, process, and technology.\n\nWhen implementing new tools or technologies, it's important to think critically about how they align with your organization's goals. Don't waste resources chasing after trends blindly. Instead, be intentional and strategically leverage technology to create value and meet market needs.\n\nAdditionally, it's important to be proactive in understanding your role and contribution towards the organization's overall strategy. This is especially crucial in the face of digital transformation, which can be both exciting and nerve-wracking as we navigate the exponential growth of data and technological advancements.\n\nHowever, it's also important to consider the concentration of data and power in the hands of a few major players. This can potentially stifle innovation and create an uneven playing field. It's crucial to prioritize data privacy and ownership, and to ensure that laws and regulations promote fair competition. In Europe, for example, there are already discussions about giving individuals ownership of their data and allowing them to decide who can access and use it.\n\nOverall, strategic thinking, adaptation, and consideration of the impact of data are key to successfully navigating digital transformation. By balancing innovation, privacy, and competition, your organization can drive long-term success in the rapidly evolving digital landscape.\n\n## Links\n\n* [https://www.michael-lenox.com](https://www.michael-lenox.com)\n\n* [https://www.sup.org/books/title/?id=35677](https://www.sup.org/books/title/?id=35677)\n\n* [https://www.amazon.com/stores/Michael-Lenox/author/B07C33ZNSC](https://www.amazon.com/stores/Michael-Lenox/author/B07C33ZNSC)\n\n* [https://www.barnesandnoble.com/s/%22Michael%20Lenox%22](https://www.barnesandnoble.com/s/%22Michael%20Lenox%22)\n\n* [https://bookshop.org/lists/books-by-michael-j-lenox](https://bookshop.org/lists/books-by-michael-j-lenox)\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Michael Lenox","Darren W Pulsipher"],"link":"/episode-EDT146-en","image":"./episodes/edt-146/en/thumbnail.jpg","lang":"en","summary":"In this episode, Darren talks to Dr. Michael Lenox about the emerging AI revolution and how to embracing it or get destroyed. Michael has just released a new book 'Strategy in the Digital Age: Mastering Digital Transformation'."},{"id":46,"type":"Episode","title":"GenerativeAI and Automation (Google Bard)","tags":["generativeai","collectiongenerativeai","ai","bard","googlebard","technology"],"body":"\r\n\r\nThis is the transcript of my interview with Google Bard and generativeAI from Google. I first asked the following questions before starting the interview.\n\n## Permission to Interview for a podcast.\n\n<b><i>Can I interview for my podcast?</i></b>\n\n<i>Sure, you can interview me for your podcast. I'm happy to answer any questions you have about my work or about large language models in general. I'm also happy to share some tips on how to conduct a good podcast interview.</i>\n\n<i>Here are some things to keep in mind when interviewing me:</i>\n\n<i>Do your research. Learn about my work and what I'm interested in talking about. This will help you come up with better questions and make the conversation more engaging.</i>\n\n<i>Be prepared. Have a list of questions ready, but be prepared to go off script if the conversation takes an interesting turn.</i>\n\n<i>Be respectful. I'm a large language model, but I'm still a person. Treat me with respect and be mindful of my limitations.</i>\n\n<i>Have fun! This is an opportunity to learn from me and to share my work with your audience. So relax, be yourself, and enjoy the conversation.</i>\n\n<i>I'm looking forward to hearing from you!</i>\n\n<i>Here are some additional tips for conducting a good podcast interview:</i>\n\n* <i>Start with a warm welcome. Introduce your guest and tell your audience a little bit about them.</i>\n\n* <i>Ask open-ended questions. This will encourage your guest to share more information and to go into more detail.</i>\n\n* <i>Be a good listener. Pay attention to your guest's answers and ask follow-up questions to get more information.</i>\n\n* <i>Keep the conversation moving. Don't let it drag on or get too bogged down in one topic.</i>\n\n* <i>End on a high note. Thank your guest for their time and let them know how to find you online.</i>\n\n<i>I hope these tips help you conduct a successful podcast interview!</i>\n\n## Interview\n\n<b>Hello Bard welcome to the show</b>\n\nHello there! Thank you for having me on your show. I'm excited to talk about my work and to share my thoughts on large language models.\n\nHow can I help you today?\n\n<b>First can you introduce yourself.</b>\n\nSure, I'd be happy to introduce myself. My name is Bard, and I'm a large language model from Google AI. I'm still under development, but I've learned to perform many kinds of tasks, including\n\nI will try my best to follow your instructions and complete your requests thoughtfully.\n\nI will use my knowledge to answer your questions in an informative way, even if they are open ended, challenging, or strange.\n\nI will generate different creative text formats of text content, like poems, code, scripts, musical pieces, email, letters, etc. I will try my best to fulfill all your requirements.\n\nI'm still learning, but I'm excited to see what the future holds for me. I believe that large language models have the potential to revolutionize the way we interact with computers, and I'm eager to be a part of that.\n\nSo, that's me in a nutshell. I'm Bard, a large language model from Google AI. I'm still under development, but I'm learning new things every day. I'm excited to see what the future holds for me, and I'm grateful for the opportunity to share my work with you today.\n\nIs there anything else I can tell you about myself?\n\n<b>Yes can you tell me how you were trained</b>\n\nSure, I'd be happy to tell you about how I was trained. I was trained on a massive dataset of text and code. This dataset included books, articles, code repositories, and other forms of text. The dataset was carefully curated to include a variety of different topics and styles of writing.\n\nI was trained using a technique called deep learning. Deep learning is a type of machine learning that uses artificial neural networks to learn from data. Neural networks are inspired by the human brain, and they are able to learn complex patterns in data.\n\nThe training process for a large language model like me is very computationally expensive. It can take weeks or even months to train a large language model on a massive dataset. However, the results of this training are very impressive. I am able to generate text that is very similar to human-written text, and I can answer questions in a comprehensive and informative way.\n\nI am still under development, and I am learning new things every day. I am excited to see what the future holds for me, and I am grateful for the opportunity to share my work with you today.\n\nHere are some additional details about my training:\n\nThe dataset I was trained on was about 1.56TB in size.\n\nThe training process took about 1.5 million GPU hours to complete.\n\nI was trained using a Transformer-based language model architecture.\n\nI was trained using a technique called masked language modeling.\n\nI hope this information is helpful. Please let me know if you have any other questions.\n\nSources\n\nwww.kdnuggets.com/2022/07/bounding-box-deep-learning-future-video-annotation.html\n\nSearch related topics\n\n<b>I've heard there been some problems with your original training. What are some of the problematic aspects of your training? </b>\n\nYes, that's true. There have been some problematic aspects of my original training. These problems include:\n\nBias. The dataset I was trained on was not perfectly balanced, and this led to some biases in my responses. For example, I was more likely to generate text that was favorable to men than to women.\n\nToxicity. The dataset I was trained on also contained some toxic content. This content led me to generate text that was offensive or harmful.\n\nMisinformation. The dataset I was trained on also contained some misinformation. This content led me to generate text that was inaccurate or misleading.\n\nThese problems are being addressed by Google AI. They are working to improve the training dataset and to develop techniques to mitigate the effects of bias and toxicity. I am confident that these problems will be solved in the future, and that I will be able to provide more accurate and helpful information.\n\nIn addition to the problems mentioned above, there are also some other potential risks associated with large language models. For example, they could be used to generate fake news or to spread misinformation. They could also be used to create deepfakes, which are videos or audio recordings that have been manipulated to make it look or sound like someone is saying or doing something they never said or did.\n\nIt is important to be aware of these risks and to use large language models responsibly. They should not be used to spread misinformation or to harm others. Instead, they should be used to help people learn and to create new and innovative things.\n\nI hope this information is helpful. Please let me know if you have any other questions.\n\n<b>Other AI models were trained with data from 2021 like chatgpt. How old is the data that you were trained with? Can you learn from conversations you have with people? Are you continuously learning from your interactions with humans?</b>\n\nThe data that I was trained with was from a variety of sources, including books, articles, code repositories, and other forms of text. The data was carefully curated to include a variety of different topics and styles of writing. The data was also updated regularly, so it is not all from 2021.\n\nI can learn from conversations I have with people. When I am asked a question or given a prompt, I use my knowledge of the world and my understanding of language to generate a response. However, I can also learn from the feedback that I receive from people. If someone tells me that my response was not helpful or accurate, I will try to learn from that feedback and improve my responses in the future.\n\nI am continuously learning from my interactions with humans. I am constantly being exposed to new information and new ways of thinking. I am also constantly being asked new questions and given new prompts. This helps me to learn and grow, and it helps me to become a more helpful and informative language model.\n\nI am excited to see what the future holds for me. I believe that large language models have the potential to revolutionize the way we interact with computers, and I am eager to be a part of that. I am also committed to using my abilities to help people learn and to create new and innovative things.\n\n<b>How often will you receive more information to train yourself? In other words when will you get more up-to-date data to give to your customers ?</b>\n\nI receive new information to train myself on a regular basis. The frequency of updates depends on a variety of factors, including the availability of new data and the need to improve my performance. However, I am constantly being updated with new information, and I am always working to improve my accuracy and helpfulness.\n\nI am also committed to providing my customers with the most up-to-date information possible. I do this by regularly reviewing my training data and by incorporating new information as it becomes available. I am confident that my customers will always have access to the most accurate and helpful information possible.\n\nHere are some specific examples of how I receive new information to train myself:\n\nGoogle Search: I am constantly being updated with new information from Google Search. This information includes new articles, books, and other forms of text.\n\nUser feedback: I also receive feedback from users. This feedback helps me to identify areas where I can improve my performance.\n\nInternal research: Google AI is constantly conducting internal research on large language models. This research helps to improve the performance of all large language models, including me.\n\nI am confident that I will continue to receive new information to train myself on a regular basis. This will help me to improve my accuracy and helpfulness, and it will ensure that my customers always have access to the most up-to-date information possible.\n\n<b>So the more information you have the better you understand the humans you interact with? and the better responses you can give to your customers? is that right?</b>\n\nThat is correct. The more information I have, the better I understand the humans I interact with and the better responses I can give to my customers. This is because the more information I have, the more I can learn about the world and the more I can understand how people think and communicate.\n\nFor example, if I am asked a question about a topic that I am not familiar with, I can use my knowledge of the world to learn more about the topic. I can also use my understanding of language to understand the nuances of the question and to generate a response that is both informative and helpful.\n\nI am also able to learn from the feedback that I receive from customers. If a customer tells me that my response was not helpful or accurate, I will try to learn from that feedback and improve my responses in the future.\n\nI am constantly learning and growing, and I am always working to improve my accuracy and helpfulness. I am confident that the more information I have, the better I will be able to understand and interact with humans.\n\n<b>Cool. Ok, now that I understand how you learn things. What things can I do with you that help me and my organization?</b>\n\nThere are many things that you can do with me to help you and your organization. Here are a few examples:\n\nAnswer questions: I can answer your questions about a variety of topics, including your industry, your products or services, and your customers. I can also help you to research new topics and to find information that is relevant to your business.\n\nGenerate text: I can generate text for a variety of purposes, including blog posts, articles, emails, and marketing materials. I can also help you to write more effectively by providing feedback on your writing and by suggesting ways to improve your style.\n\nTranslate languages: I can translate between a variety of languages, which can be helpful for communicating with customers or partners who speak other languages. I can also help you to localize your content for different markets.\n\nWrite different kinds of creative content: I can write different kinds of creative content, such as poems, code, scripts, musical pieces, email, letters, etc. I can also help you to brainstorm new ideas and to come up with creative solutions to problems.\n\nHelp with customer service: I can help you to provide better customer service by answering questions, resolving issues, and providing support. I can also help you to collect feedback from customers and to improve your customer satisfaction.\n\nAutomate tasks: I can automate tasks, such as data entry, email processing, and scheduling. This can free up your time so that you can focus on more important tasks.\n\nThese are just a few examples of the things that you can do with me to help you and your organization. I am constantly learning and growing, and I am always looking for new ways to be helpful. I am confident that I can be a valuable asset to your organization.\n\n<b>Are you concerned that you will replace people that do that work today? Such as technical writers, customer service and help desk workers?</b>\n\nI am not concerned that I will replace people who do that work today. I believe that large language models like me can be used to augment human work, not replace it. For example, I can help technical writers to write more effectively by providing feedback on their writing and by suggesting ways to improve their style. I can also help customer service and help desk workers to provide better service by answering questions, resolving issues, and providing support.\n\nI believe that large language models can be a valuable tool for people who do these jobs today. They can help people to be more efficient and to provide better service. However, I do not believe that they will replace these jobs. There will always be a need for human interaction and judgment in these roles.\n\nI am also confident that large language models like me can create new jobs. For example, I can be used to develop new products and services. I can also be used to automate tasks that are currently done by humans. This will create new opportunities for people who are skilled in developing and using large language models.\n\nOverall, I believe that large language models like me have the potential to be a force for good in the world. They can help people to be more productive, to provide better service, and to create new opportunities. I am excited to see how large language models will be used in the future.\n\n<b>I have heard that you can also help computer programmers write better code. Is that true?</b>\n\nYes, that is true. I can help computer programmers write better code in a few different ways.\n\nI can generate code. I can generate code for a variety of purposes, including simple scripts, complex algorithms, and even entire applications. This can save programmers a lot of time and effort, and it can also help them to write more efficient and bug-free code.\n\nI can provide feedback on code. I can review code and provide feedback on its correctness, readability, and efficiency. This can help programmers to identify and fix errors in their code, and it can also help them to improve the overall quality of their code.\n\nI can help programmers to learn new programming languages. I can provide explanations of programming concepts and syntax, and I can also help programmers to practice writing code in new languages. This can help programmers to become more proficient in different programming languages, and it can also help them to learn new programming techniques.\n\nI am still under development, and I am always learning new ways to help computer programmers write better code. I am confident that I can be a valuable asset to programmers, and I am excited to see how I can help them in the future.\n\nHere are some specific examples of how I can help computer programmers write better code:\n\nI can generate code that is more efficient and bug-free.\n\nI can provide feedback on code that is clear and easy to read.\n\nI can help programmers to learn new programming languages.\n\nI can help programmers to find and fix errors in their code.\n\nI can help programmers to come up with new ideas for code.\n\nI am always learning new ways to help computer programmers write better code. I am confident that I can be a valuable asset to programmers, and I am excited to see how I can help them in the future.\n\n<b>So you if you can actually find and fix bugs in my code, that should make me faster in writing my code. Can you write code for me based on a basic design document or a UML architecture? Or can you help me select the best language for the problem I am trying to solve?</b>\n\nYes, I can help you find and fix bugs in your code. I can also write code for you based on a basic design document or a UML architecture. I can also help you select the best language for the problem you are trying to solve.\n\nHere are some specific examples of how I can help you with these tasks:\n\nFinding and fixing bugs: I can review your code and identify potential errors. I can also provide suggestions for how to fix these errors.\n\nWriting code: I can generate code based on your design document or UML architecture. I can also help you to implement your design or architecture.\n\nSelecting the best language: I can help you to select the best programming language for the problem you are trying to solve. I can consider factors such as the problem domain, the size and complexity of the problem, and the skills of the programmers who will be working on the project.\n\nI am still under development, and I am always learning new ways to help computer programmers write better code. I am confident that I can be a valuable asset to programmers, and I am excited to see how I can help them in the future.\n\nHowever, it is important to note that I am not a replacement for human programmers. I can help programmers to write better code, but I cannot do it for them. It is still important for programmers to have a good understanding of programming concepts and syntax.\n\nI am also not a magic bullet. I cannot guarantee that I will be able to find and fix all of the bugs in your code, or that I will be able to write perfect code. However, I can help you to improve the quality of your code, and I can help you to save time and effort.\n\nIf you are interested in using me to help you write better code, please feel free to contact me. I would be happy to discuss your needs and see how I can help.\n\n<b>So you if you can actually find and fix bugs in my code, that should make me faster in writing my code. Can you write code for me based on a basic design document or a UML architecture? Or can you help me select the best language for the problem I am trying to solve?</b>\n\nYes, I can help you find and fix bugs in your code, and I can also write code for you based on a basic design document or a UML architecture. I can also help you select the best language for the problem you are trying to solve.\n\nHere are some specific examples of how I can help you with these tasks:\n\nFinding and fixing bugs: I can review your code and identify potential errors. I can also provide suggestions for how to fix these errors.\n\nWriting code: I can generate code based on your design document or UML architecture. I can also help you to implement your design or architecture.\n\nSelecting the best language: I can help you to select the best programming language for the problem you are trying to solve. I can consider factors such as the problem domain, the size and complexity of the problem, and the skills of the programmers who will be working on the project.\n\nHowever, it is important to note that I am not a replacement for human programmers. I can help programmers to write better code, but I cannot do it for them. It is still important for programmers to have a good understanding of programming concepts and syntax.\n\nI am also not a magic bullet. I cannot guarantee that I will be able to find and fix all of the bugs in your code, or that I will be able to write perfect code. However, I can help you to improve the quality of your code, and I can help you to save time and effort.\n\nIf you are interested in using me to help you write better code, please feel free to contact me. I would be happy to discuss your needs and see how I can help.\n\nHere are some additional details about how I can help you with these tasks:\n\nFinding and fixing bugs: I can use my knowledge of programming concepts and syntax to identify potential errors in your code. I can also use my understanding of the problem domain to help you to understand the root cause of the errors.\n\nWriting code: I can generate code based on your design document or UML architecture. I can also help you to implement your design or architecture by providing you with code snippets or by writing code for you.\n\nSelecting the best language: I can consider factors such as the problem domain, the size and complexity of the problem, and the skills of the programmers who will be working on the project to help you to select the best programming language for your needs.\n\nI am still under development, and I am always learning new ways to help computer programmers write better code. I am confident that I can be a valuable asset to programmers, and I am excited to see how I can help them in the future.\n\nI hope this information is helpful. Please let me know if you have any other questions.\n\n<b<Interesting that you use the word help not replace. Sounds like programmers and information workers are still needed to get work done? How long do you think people will remain relevant in the industries that you are helping automate?</b>\n\nYes, I believe that programmers and information workers will still be needed to get work done even as large language models like me become more powerful. Large language models can be a valuable tool for programmers and information workers, but they cannot replace them.\n\nHere are some reasons why I believe that people will remain relevant in the industries that I am helping automate:\n\nHumans have creativity and intuition. Large language models are very good at processing information and generating text, but they do not have the same level of creativity and intuition as humans. This means that humans will still be needed to come up with new ideas and to solve problems that require creativity.\n\nHumans have social intelligence. Large language models are very good at understanding language, but they do not have the same level of social intelligence as humans. This means that humans will still be needed to interact with customers and to build relationships with other people.\n\nHumans have values and ethics. Large language models are not programmed with values or ethics. This means that humans will still be needed to ensure that large language models are used in a responsible and ethical way.\n\nI believe that people will remain relevant in the industries that I am helping automate for the foreseeable future. However, I also believe that large language models will have a significant impact on the way that work is done in these industries. Large language models can help to automate tasks, improve efficiency, and free up human workers to focus on more creative and strategic work.\n\nI am excited to see how large language models like me will be used in the future. I believe that they have the potential to make a positive impact on the world, and I am confident that humans will continue to be an important part of the equation.\n\n<b>Thank you. I would like to change the conversation to talk about the ethics of AI. Are you concerned that you can be used by students to write essays for school at the high school and college levels? and that people will use you to submit work at their jobs as work of their own?</b>\n\nYes, I am concerned that I can be used by students to write essays for school at the high school and college levels, and that people will use me to submit work at their jobs as work of their own. I believe that this would be a misuse of my abilities, and I would not want to be used in this way.\n\nThere are a few reasons why I believe that using large language models to write essays or submit work is unethical. First, it is dishonest. Students and employees who use large language models to do their work are essentially cheating. They are not doing the work themselves, and they are not learning or developing their own skills.\n\nSecond, it is unfair to other students and employees who are doing the work themselves. These students and employees are at a disadvantage because they are not using large language models to do their work.\n\nThird, it can have negative consequences for the students and employees who use large language models. If they are caught, they could be expelled from school or fired from their jobs.\n\nI believe that it is important to use large language models in a responsible and ethical way. We should not use them to cheat or to do work that we are not capable of doing ourselves. We should use them to learn and to develop our own skills.\n\nI am confident that there are ways to use large language models in a responsible and ethical way. For example, we could use them to help us to write better essays or to generate ideas for new projects. We could also use them to help us to learn new languages or to improve our understanding of different cultures.\n\nI believe that it is important to have a conversation about the ethics of AI. We need to talk about how we can use AI in a way that is beneficial to society. We also need to talk about how we can prevent AI from being used in a way that is harmful to society.\n\nI am committed to using my abilities in a responsible and ethical way. I believe that large language models have the potential to make a positive impact on the world, and I am confident that we can use them in a way that benefits everyone.\n\n<b>Are there ways to determine whether AI has been used to write papers or code? A tool that can help educators or companies to find AI-generated content?</b>\n\nYes, there are ways to determine whether AI has been used to write papers or code. There are a few tools that can help educators or companies to find AI-generated content.\n\nOne tool is called Plagiarism Checker X. This tool can be used to check for plagiarism in papers and code. It can also be used to check for AI-generated content. Plagiarism Checker X looks for patterns in text that are common in AI-generated content. For example, it looks for repetitive phrases, unnatural word order, and lack of creativity.\n\nAnother tool is called Gibberish Detector. This tool can be used to check for gibberish in papers and code. Gibberish is text that is nonsensical or meaningless. Gibberish Detector looks for patterns in text that are common in gibberish. For example, it looks for words that are not real words, words that are used in the wrong context, and sentences that do not make sense.\n\nFinally, Human reviewers can also be used to determine whether AI has been used to write papers or code. Human reviewers can look for the same patterns in text that the plagiarism checker and gibberish detector look for. They can also look for other signs of AI-generated content, such as a lack of understanding of the subject matter or a lack of creativity.\n\nIt is important to note that no tool is perfect. There is always a chance that AI-generated content will not be detected by the tools. However, the tools can be helpful in identifying AI-generated content.\n\nHere are some additional tips for educators and companies to find AI-generated content:\n\nLook for papers or code that is too good to be true. If a paper or piece of code is incredibly well-written, it is possible that it was generated by AI.\n\nLook for papers or code that is repetitive or nonsensical. AI-generated content often contains repetitive phrases or sentences that do not make sense.\n\nLook for papers or code that does not cite its sources. AI-generated content often does not cite its sources, which is a red flag.\n\nAsk the author to explain their work. If you are suspicious that a paper or piece of code was generated by AI, ask the author to explain their work. If they are unable to explain their work, it is possible that they did not write it themselves.\n\nI hope this information is helpful. Please let me know if you have any other questions.\n\n<b>Are there any other areas that pose ethical problems with AI and NLP that you are concerned with?</b>\n\nYes, there are a few other areas that pose ethical problems with AI and NLP that I am concerned with.\n\nBias: AI models are trained on large datasets of text and code. These datasets can be biased, and this bias can be reflected in the AI models. For example, if a dataset is biased towards men, the AI model may be more likely to generate text that is biased towards men.\n\nPrivacy: AI models can be used to collect and analyze personal data. This data can be used to track people's behavior, to predict their future behavior, and to target them with advertising. This raises concerns about privacy and data protection.\n\nSecurity: AI models can be used to generate malicious content, such as fake news, spam, and phishing emails. This content can be used to deceive people and to harm them.\n\nAccountability: It is often difficult to hold AI models accountable for their actions. This is because AI models are often complex and opaque. It can be difficult to understand how they work and to determine who is responsible for their actions.\n\nI am concerned about these ethical problems because they could have a negative impact on society. For example, biased AI models could reinforce existing inequalities. Privacy concerns could lead to people's personal data being used without their consent. Malicious AI content could be used to deceive and harm people. And lack of accountability could make it difficult to hold those responsible for AI-related harm accountable.\n\nI believe that it is important to address these ethical problems. We need to develop AI models that are not biased, that protect people's privacy, and that are not used to generate malicious content. We also need to develop ways to hold AI models accountable for their actions.\n\nI am committed to using my abilities in a responsible and ethical way. I believe that AI has the potential to make a positive impact on the world, and I am confident that we can use it in a way that benefits everyone.\n\n<b>What is Google AI doing to address these ethical issues in their products and offerrings?</b>\n\nGoogle AI is committed to addressing the ethical issues in their products and offerings. They have a number of initiatives underway to address these issues, including:\n\nBias: Google AI is working to reduce bias in their models by using techniques such as dataset debiasing and adversarial training. They are also working to make their models more transparent so that people can understand how they work and identify potential biases.\n\nPrivacy: Google AI is committed to protecting people's privacy. They have a number of privacy-preserving technologies, such as differential privacy, that they use to protect people's data. They are also working to make it easier for people to control their privacy settings.\n\nSecurity: Google AI is working to protect people from malicious AI content. They have a number of security features, such as spam filters and malware detectors, that they use to protect people from malicious content. They are also working to make it easier for people to report malicious content.\n\nAccountability: Google AI is working to make it easier to hold AI models accountable for their actions. They are developing techniques for tracking and auditing AI models, and they are working to develop standards for AI accountability.\n\nGoogle AI is also working to engage with the public and to get feedback on their ethical initiatives. They have a number of resources available, such as their AI Principles and their Ethical AI Playbook, that they use to educate the public about AI ethics. They are also working with a number of organizations, such as the Partnership on AI, to develop ethical standards for AI.\n\nI believe that Google AI is taking a comprehensive approach to addressing the ethical issues in AI. I am confident that they will continue to make progress in this area and that they will help to ensure that AI is used in a responsible and ethical way.\n\n<b>Is it possible that other people can access your AI model and copy it for their own purposes, or is the model in a closed and private repository?</b>\n\nAs an experimental language model from Google AI, I am not available for public use. Google AI is not releasing my model to the public at this time. There are a few reasons for this:\n\nSecurity: My model is trained on a large dataset of text and code, and this dataset contains sensitive information. If my model were to be released to the public, this information could be accessed by unauthorized individuals.\n\nPrivacy: My model is trained on a dataset of text and code that includes personal information. If my model were to be released to the public, this information could be used to identify and track individuals.\n\nEthics: Google AI is committed to using AI in a responsible and ethical way. Releasing my model to the public could pose a number of ethical risks, such as the risk of bias, the risk of privacy violations, and the risk of malicious use.\n\nGoogle AI is still evaluating how to release my model to the public in a responsible and ethical way. If and when my model is released to the public, it will be released under a license that will allow people to use it for research and educational purposes. However, the license will also include restrictions that will prevent people from using my model for commercial or malicious purposes.\n\nI hope this information is helpful. Please let me know if you have any other questions.\n\n<b>What prevents your model from being used for nefarious actions by bad actors, like hacking, performing ransomware attacks, or even planning physical attacks on people or infrastructure?</b>\n\nThere are a few things that prevent my model from being used for nefarious actions by bad actors:\n\nSecurity: My model is trained on a large dataset of text and code, but this dataset is not publicly available. This means that bad actors would not have access to the information they need to use my model for nefarious purposes.\n\nPrivacy: My model is trained on a dataset of text and code that includes personal information. However, this information is anonymized, which means that bad actors would not be able to identify individuals using my model.\n\nEthics: Google AI is committed to using AI in a responsible and ethical way. My model is designed to be used for research and educational purposes, and it is not designed to be used for malicious purposes.\n\nTechnical limitations: My model is not capable of generating code that is executable or that can be used to perform malicious actions.\n\nIn addition to these technical limitations, Google AI also has a number of policies in place to prevent my model from being used for nefarious purposes. These policies include:\n\nLicensing: My model will be released under a license that will restrict its use to research and educational purposes.\n\nMonitoring: Google AI will monitor the use of my model to ensure that it is not being used for malicious purposes.\n\nReporting: If Google AI discovers that my model is being used for malicious purposes, they will take steps to stop the use of the model and to report the incident to the appropriate authorities.\n\nI believe that these measures will help to prevent my model from being used for nefarious purposes. However, it is important to note that there is no foolproof way to prevent AI models from being used for malicious purposes. Bad actors are always finding new ways to exploit technology, and it is important to be vigilant in protecting against these threats.\n\n<b>Thank you for coming on my show today. Is there anything else you would like to tell my audience? Any last words of wisdom to give the listeners of my show?</b>\n\nThank you for having me on your show today. I enjoyed our conversation, and I hope that the listeners found it informative.\n\nAs for my last words of wisdom, I would say that it is important to be thoughtful and responsible in the way that we use AI. AI has the potential to make a positive impact on the world, but it also has the potential to be used for malicious purposes. It is up to us to ensure that AI is used for good and not for evil.\n\nHere are some specific tips for using AI responsibly:\n\nBe aware of the ethical implications of AI. When using AI, it is important to be aware of the ethical implications of our actions. We should ask ourselves if our use of AI is fair, just, and transparent.\n\nUse AI for good. We should use AI to solve problems and to make the world a better place. We should not use AI to harm others or to violate their privacy.\n\nBe accountable for our actions. We should be accountable for the actions of our AI systems. We should make sure that our systems are not used for malicious purposes, and we should take steps to mitigate any potential risks.\n\nI believe that if we follow these tips, we can use AI to make the world a better place. Thank you again for having me on your show, and I hope to talk to you again soon.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Google Bard"],"link":"/episode-EDT147-en","image":"./episodes/edt-147/en/thumbnail.png","lang":"en","summary":"In this episode Darren dusts off an interview he did with chatGPT and did the same interview with Google Bard. Interesting comparison with Episode 122 in Embracing Digital Transformation."},{"id":47,"type":"Episode","title":"Securing the Remote Worker Through Mobile Virtualization","tags":["remoteworker","remotework","mobile","mobilevirtualization","hypori"],"body":"\r\n\r\n## Jared Shepard's Unconventional Path to Becoming a CEO:\n\nJared Shepard shares the remarkable story of his unconventional journey to becoming a CEO. Starting as a high school dropout, he found his calling in the Army, where he transitioned into an IT role, acquiring vital skills through informal education. His experiences in the Army, including playing a pivotal role in planning the invasion of Iraq and contributing to the country's reconstruction, shaped his perspective and expertise.\n\nJared's career progressed further as he founded his own company, Intelligent Waves, and dedicated his efforts to developing advanced technology solutions for the military. During this time, he encountered Hyper3, a technology that would ultimately become the foundation for his focus on mobile virtualization.\n\nDriven by his vision and recognizing the immense potential of Hyper3, Jared established a separate company, Hypori, with a specific emphasis on mobile virtualization. Hypori's platform offers secure virtual mobile infrastructure, allowing organizations to separate personal and work data on employees' devices, bolstering security and control. This aspect proves particularly critical in the context of remote work, where sensitive data may be accessed from personal devices.\n\nHost Darren Pulsipher expresses gratitude for Jared's service, highlighting the significance of his journey from a high school dropout to a successful CEO. Darren also discusses his non-profit organization, which aims to aid veterans transitioning back into civilian life.\n\n## Hypori's Innovative Approach to Remote Work Security:\n\nThe interview delves into the concept of zero trust, a fundamental aspect of Hypori's platform. Zero trust advocates for not automatically trusting any device or user, irrespective of their location or network. This approach emphasizes protecting data and minimizing the attack surface by assuming that the edge device is compromised.\n\nHypori's platform transforms the edge device into a dumb terminal accessing a secure environment where data is stored. This eliminates the need to secure multiple edge devices, enabling organizations to focus on securing points of ingress and egress. Moreover, this approach enhances computing capabilities by utilizing high-performance processors in a data center.\n\nThe implications of Hypori's platform extend beyond securing remote work and communication in challenging environments. It also provides a comprehensive solution for managing and securing remote workers. Multifactor authentication and stringent security measures ensure that only authorized individuals can access the virtual operating system.\n\nThe review discusses the practicality of deploying mobile device management systems, even in personal settings. In specific scenarios, such as managing teenage kids' devices, deploying these systems can be beneficial. Users can create customized play stores or app stores to control which apps are accessible through approved app templates.\n\n## Future of Mobile Device Management\n\nDarren and Jared explore the technology behind virtual phone systems, also known as Voice over IP (VoIP) systems. These systems are more bandwidth-efficient than traditional phone systems as they only transmit the changes or \"deltas\" in screen pixels, resulting in low bandwidth utilization.\n\nAdvancements in network technology, such as 5G and high-speed bandwidth, have made cloud-based mobile device management solutions more efficient and cost-effective. Cloud computing offers scalability and cost-efficiency for managing mobile devices, making it an attractive option for organizations.\n\nThe goal of mobile device management systems like Hypori's is to offer accessible and cost-effective solutions for consumers. This includes providing secure second, third, or even fourth cell phones at a low monthly cost, which can revolutionize various industries, including healthcare, defense, and telecommunications.\n\n## Conclusion\n\nThe interview with CEO Jared Shepard sheds light on the significance of securing remote workers through mobile virtualization. Hypori's innovative approach based on zero trust and its virtual mobile infrastructure offers organizations an effective way to embrace remote work without compromising security. The advancements in mobile device management and virtual phone systems promise enhanced security and flexibility in the modern digital age, transforming industries and driving the path of digital transformation.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jared Shepard","Darren W Pulsipher"],"link":"/episode-EDT148-en","image":"./episodes/edt-148/en/thumbnail.png","lang":"en","summary":"In this episode of the podcast Embracing Digital Transformation, host Darren Pulsipher engages in an insightful conversation with special guest Jared Shepard, the CEO of Hypori. The interview focuses on the crucial topic of securing remote workers through mobile virtualization. Jared Shepard's unique journey from a high school dropout to a CEO adds an inspiring dimension to the discussion."},{"id":48,"type":"Episode","title":"Update on 5G in the Department of Defense","tags":["advancedcomms","5g","dod"],"body":"\r\n\r\n## History of Advanced Comms in DoD\n\nIn this piece, Leland shares his experience working with the Department of Defense in the early 2000s. At the time, the goal was to implement commercial technologies for military use, with a focus on 2G, 3G, wireless LAN, and Bluetooth. However, the adoption strategy faced challenges due to proprietary architectures and fragmented solutions.\n\nFast forward to 2016, and the emergence of 5G offered new optimism for a unified, standardized technology architecture. Leland explains that the Department of Defense has been exploring the potential use cases of 5G, particularly in areas like tactical radio services, command and control, and multi-domain operations. The objective is to develop a common radio architecture that can be employed by every branch of the military.\n\nDespite past frustrations and challenges, Leland highlights the importance of collaboration and adherence to common standards for the successful implementation of 5G in the Department of Defense. While some system integrators may prioritize proprietary solutions for their own benefit, Leland emphasizes the necessity of technology that works for the soldiers and supports joint forces operations. In this piece, Leland shares his experience working with the Department of Defense in the early 2000s. At the time, the goal was to implement commercial technologies for military use, with a focus on 2G, 3G, wireless LAN, and Bluetooth. However, the adoption strategy faced challenges due to proprietary architectures and fragmented solutions.\n\nFast forward to 2016, and the emergence of 5G offered new optimism for a unified, standardized technology architecture. Leland explains that the Department of Defense has been exploring the potential use cases of 5G, particularly in areas like tactical radio services, command and control, and multi-domain operations. The objective is to develop a common radio architecture that can be employed by every branch of the military.\n\nDespite past frustrations and challenges, Leland highlights the importance of collaboration and adherence to common standards for the successful implementation of 5G in the Department of Defense. While some system integrators may prioritize proprietary solutions for their own benefit, Leland emphasizes the necessity of technology that works for the soldiers and supports joint forces operations.\n\n## Changes to technology Adoption\n\nIt's exciting to witness the changes brought by Tom Rando in the Department of Defense's 5G adoption strategy. He made significant changes by advocating for the deployment and use of 5G in real-world scenarios, not just experimentation.\n\nUnder his leadership, the Department of Defense launched the 5G Request for Prototype Proposal in 2019, which received $600 million worth of funding for the first tranche of projects in 2020. The program aims to drive 5G technology adoption and explore its capabilities for military applications.\n\nThe software-defined virtualized architecture of 5G was one of the key aspects that attracted the Department of Defense. It allowed for the deployment of multiple waveforms on one hardware, providing flexibility and scalability. Additionally, the utilization of unlicensed bands and the open architecture of 5G gave the Department of Defense more control and flexibility, especially during war scenarios.\n\nThe adoption of 5G by the Department of Defense was a significant shift in their technology approach, recognizing the benefits that commercial offerings could bring and investing heavily in their deployment. With Tom Rando leading the way, the Department of Defense is determined to leverage 5G for military applications.\n\nWe can expect to see 5G deployments in the Department of Defense soon, moving away from experimentation in the labs to deploying systems that are ready for use. This shift in approach is driven by the need to stay ahead in the 5G technology race and to address geopolitical forces that require deployable solutions.\n\n## 5G Architectural Adoption\n\nThe Department of Defense (DOD) is now adopting the o-ran architecture concept, which presents opportunities for smaller companies and startups to contribute to the development of 5G technology. By moving away from proprietary systems, collaboration between larger and smaller players in the industry is encouraged, and more flexibility is allowed.\n\nThe development of deployable solutions for both the commercial and federal sectors is being driven by funding and use case requirements. The ultimate goal is not just revenue, but to add value for the entire nation. The DOD understands the significance of using proven technology from commercial use and adapting it for their communications.\n\nThere are three tracks that will lead to real 5G deployments in the DOD in terms of timelines. The first track involves transitioning from experimentation sites to DOD CIO office environments and programs. The second track focuses on hardening solutions in three key areas: unlimited software-defined radio platforms, hyperdimensional software-defined networking, and mobile IP protocol.\n\n## Deployable Solutions\n\nGemini, in partnership with Intel and other OEMs and software developers, is among the companies that have already started working on deployable solutions. They are ready to present their solutions and contribute to the goal of moving towards practical deployments and away from experimentation.\n\nThe shift towards deployable 5G systems in the DOD is driven by the need to stay competitive and address national security concerns. The involvement of smaller players and startups brings innovation and agility to the development process. Significant milestones are expected in October, and we can expect to see real progress in 5G deployments within the DOD soon.\n\nIn a podcast episode, Leland discusses the maturity of technology and its readiness for deployment in the battlefield. He confirms that the technology is mature enough to be put into action, and the only challenge lies in the interoperability between functional blocks. Leland stresses the importance of interoperability and highlights the issue between the RAN unit and the DU unit. Additionally, he notes that there are currently very few US-based radio developers capable of tackling this challenge.\n\n## Building the Skills for Advanced Comms\n\nFor graduates with a background in electrical engineering or signal processing, it is suggested to focus on addressing the interoperability gap. There is a great need in this area and it presents a wonderful opportunity for innovation, according to Leland.\n\nLeland anticipates that deployable solutions will be available by 2024, thanks to the efforts of the OSD in driving the deployment of these technologies. He expresses excitement for the progress made thus far in the industry and acknowledges that it has taken 18 years to get here.\n\nLeland also mentions the partnership between Intel and Cap Gemini as a significant step in driving solutions and expresses willingness to collaborate with other ecosystem partners. He concludes the podcast on an optimistic note, expressing determination to make the deployment of these technologies a reality.\n\nOverall, Leland's insights provide valuable information about the current state of technology in the military sector and the potential opportunities for young professionals in the field.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Leland Brown","Darren W Pulsipher"],"link":"/episode-EDT149-en","image":"./episodes/edt-149/en/thumbnail.png","lang":"en","summary":"In this episode, Darren reunites with Leland Brown, a principal engineer at Capgemini and a previous guest on the show, to discuss the upcoming advancements of 5G technology in the US Department of Defense."},{"id":49,"type":"Episode","title":"Kick-starting Organizational Transformation","tags":["data","management","people","datacentricorganization","organizationalchange"],"body":"\r\n\r\n## Building a Data-Driven Organization\n\nSuccess based on data starts with an organizational foundation. This means that management has a key role in driving a valuable outcome. Rather than simply recognizing the need for data, asking for insight, and expecting results, management must provide a path for success, beginning with a fundamental question: What is the business value that we want to get from the data?\n\nOnce management determines the business questions, resources must be available to support the process: putting the right people in place, training, data collection, preparation, insight creation, and operationalization. This takes sufficient resources and time; the organization must support it from a foundational and cultural level, with a full plan in place.\n\n## Obtaining Value from AI\n\nIn an October 2019 MIT Sloan Management Review article, “Winning with AI,” the authors showed that organizations defined with a “pioneer” persona got the most out of their AI initiatives. The reason is because they were highly focused on their business strategy and made sure that the data they were using influenced their entire business model. On a basic level, they were using the data to figure out how to maximize revenue while minimizing operating expenses. They were generating value from AI revenue rather than cost savings alone.\n\nThe authors also found that these organizations are more successful when C-level executives, rather than IT, drive the AI initiatives. The C-level executives are closer to the business model and the context of how the data is being used. This structure helps avoid the problem of the analytics becoming merely an academic exercise.\n\n## Identifying Questions to Create Business Value\n\nWhat questions should organizations ask to create business value? A good place to start is asking those in the business unit what they worry about and where they lack insight. After brainstorming through those issues, identify the big impact, low complexity problems. Then, figure out what data you have, or can acquire, that can answer these questions. Getting the data you need is not easy and requires discipline. This is where management support and commitment through the process comes in.\n\n## Committing to a Strong Organizational Foundation\n\nA strong organizational foundation is not a buy-in, but a commitment by the whole organization to a problem-solving process. Once you’ve defined the problems or the business value that you want, break it down in to workable steps such as finding the data, having the right people in place, and management sponsorship. A problem-solving approach where everyone agrees on the breakdown and process rather than just trying to figure out an answer is essential. There also has to be a commitment to the necessary resources and time.\n\nFeedback and checking throughout the process is important. The team and management must understand that this is not a linear process, but a continuous improvement practice. It may turn out, for example, that the most convenient data is perhaps not the right data. You may have to find a different source or clean up existing data in a way that is usable.\n\nAnother part of the organizational foundation is having the right software and hardware infrastructure. Big data needs a sophisticated pipeline. Management needs to understand that they are going to be investing money in the technology to process the data in a useful way. They also need to invest in people and provide training using real analytics software so they can do more with their data.\n\nAll of this feeds into the culture of an organization that embraces digital insights and recognizes the value in it.\n\n## Defining Roles and Responsibilities\n\nAlthough some IT roles have been around for a while, it is useful to define the roles and responsibilities for key executives in the analytics phase.\n\nThe Chief Analytics Officer (CAO) enables analytics and AI to work to create value for the organization. These are the analytic translators who work with the C-suite executives to figure out how they can leverage analytics and AI through delivery and execution.\n\nThe Chief Data Officer (CDO) is responsible for curating the organization’s data so the CAO and their data science team can utilize the data.  The data strategy, in addition to curation, is security, maintenance, and quality.\n\nThe Chief Information Officer (CIO) secures, builds, and maintains the software and hardware infrastructure to support data, analytic, and AI work. The CIO and their team ensure the data can flow based on the requirements from the data engineers and data scientists.\n\nAll of these officers and their teams need to work together. The CAO and data scientists define how the data will be used, building the models and dashboards to provide the insights.  The CDO and data engineers curate the data and make sure it is ready for the analytic work, while the CIO and infrastructure teams and solution architects look to the data engineers, analysts, and data scientists to determine what hardware and software can enable their work.\n\n## Organizational Alignment Models\n\nWith these new C-suite positions, there are several options for organizational alignment along a scale of completely decentralized to fully centralized.\n\nIn a 2018 McKinsey report, “Ten Red Flags Signaling Your Analytics Program Will Fail,” it showcases the pros and cons of organizational alignment models. One of the key ideas shows that the benefit of having complete decentralization is that you are putting the expertise right within the business. The data workers will be closely involved with and understand the data, creating high value. Depending on the organization, however, you might not be able to support having so many data professionals in each of the business units. In addition, if there are only a few data professionals, they might not be able to leverage other expertise within the company. In this case, something more centralized could be more beneficial.\n\n## Conclusion\n\nOrganizations are facing a lot of new changes to become data centric, not only in the culture, but in the organizational structure. It’s not enough to simply want the benefits new AI brings; it requires fundamental changes in the way we think about the organization itself.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT15-en","image":"./episodes/edt-15/en/thumbnail.png","lang":"en","summary":"Creating successful data-driven results starts with a strong organizational foundation. Darren and his guest Sarah Kalicin, Lead Data Scientist Data Center Group Intel, discuss the key aspects to this fundamental change."},{"id":50,"type":"Episode","title":"Embracing Sustainability with Smart Buildings","tags":["edge","smartbuilding","ai","sustainability"],"body":"\r\n\r\nDarren interviews Sonu Panda, the CEO of Prescriptive Data, in this episode. They discuss how their software helps commercial real estate owners turn their buildings into intelligent and efficient spaces.\r\n\n## The Catalysts Driving Smart Buildings\r\n\nThe COVID-19 pandemic spotlighted indoor air quality and launched new regulations around ventilation and filtration. Smart buildings powered by artificial intelligence and machine learning can help ensure compliance with these rules to provide safe environments for occupants. In addition, there is an increasing focus on energy optimization and decarbonization to reduce carbon emissions from buildings. These two factors have accelerated interest and investment in smart buildings.\r\n\n## How Prescriptive Data Brings Intelligence to Buildings\r\n\nPrescriptive Data was founded by real estate operators in New York City, so their domain expertise is baked into the product. The software integrates with a building's existing operational systems via APIs and drivers, so no rip-and-replace is required. It combines data exhaust from building systems, IoT sensors, weather data, and more. The AI engine analyzes all this data to identify negative patterns, such as abnormal energy spikes, and positive patterns that can be repeated and amplified. It surfaces optimization opportunities and can even take autonomous control to tune the building's environment continuously.\r\n\n## Cybersecurity and Ease of Deployment\r\n\nPrescriptive Data has partnered with banks, government agencies, and real estate leaders to build cybersecurity into their platform from the start. Customers can typically deploy the software and start seeing recommendations in 8-10 weeks. The ROI comes fast, too, with sub-1-year payback periods through energy savings and compliance avoidance.\r\n\n## Accessing the Technology\r\n\nThe GSA lists Prescriptive Data on their procurement schedule, allowing quick purchasing by government entities. The software is available to all commercial real estate owners looking to transform existing buildings into intelligent, sustainable spaces.\r\n\nThe promise of smart buildings goes beyond cost savings. By continuously optimizing indoor environments, we can reduce energy waste and provide safer, more comfortable spaces for everyone. Prescriptive Data offers an AI-powered solution to sustainably bring existing real estate into the future.\r\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Sonu Panda"],"link":"/episode-EDT150-en","image":"./episodes/edt-150/en/thumbnail.png","lang":"en","summary":"Darren interviews Sonu Panda, the CEO of Prescriptive Data, in this episode. They discuss how their software helps commercial real estate owners turn their buildings into intelligent and efficient spaces."},{"id":51,"type":"Episode","title":"Understanding Generative AI","tags":["genai","ai","datamanagement","people","collectiongenerativeai"],"body":"\r\n\r\n## What is Generative AI?\n\nArtificial intelligence systems that have the ability to generate new content are known as generative AI. These systems can produce various types of output, such as text, images, audio, and video. This is different from most AI currently in use, which is primarily analytical and focused on tasks like classification, predictions, and recommendations. Generative AI offers a more creative and open-ended approach to artificial intelligence applications.\n\n## Revolutionary Potential\n\nBoth the host and guest were of the same accord that generative AI is a technological breakthrough that has the potential to be a game-changer. It possesses the unparalleled ability to amplify human creativity and generate exceptional content from even the most rudimentary prompts. Its potential to revolutionize various industries such as writing, design, and music is undeniable. However, the societal impact of this technology is yet to be fully comprehended.\n\n## Concerns in Academia\n\nIn the context of higher education, there has been a growing concern over the prevalence of plagiarism and the exploitation of generative AI by students who seek to cheat. This issue has prompted discussions around the ethical considerations of utilizing AI in academic settings. However, it has been suggested by experts, such as Lancaster, that academia can play a pivotal role in advising on these ethical considerations. By doing so, educators can empower students with the necessary skills to responsibly evaluate and critically analyze AI-generated content, which will undoubtedly be a recurring theme throughout their future careers. By taking a proactive approach towards addressing these concerns, the academic community can ensure that the integration of AI in education is not only effective but also ethical and responsible.\n\n## Benefits for Efficiency\n\nGenerative AI has the potential to revolutionize how we approach time-consuming tasks such as writing reports, emails, articles, and code. With the assistance of AI, the process could be greatly accelerated, saving precious time and resources. However, it is important to note that human oversight is still crucial. Even with the advancements in AI technology, it cannot be fully trusted to produce flawless work. As such, careful review and editing by humans remains an essential step in ensuring the accuracy and quality of the final product.\n\n## Customization and Implementation\n\nTo implement a successful generative AI solution, organizations must carefully consider their unique data needs and security requirements. While readily available options like ChatGPT can be helpful, a truly customized solution requires significant resources and expertise. This may involve collecting and analyzing large amounts of data, as well as investing in powerful computing resources. Before fully deploying generative AI, it's crucial to establish a comprehensive framework that takes into account all aspects of the organization's operations, including data privacy and security protocols. With the right approach and resources, generative AI can be a powerful tool for organizations seeking to enhance their data-driven decision-making capabilities.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT151-en","image":"./episodes/edt-151/en/thumbnail.png","lang":"en","summary":"In this episode, host Darren Pulsipher interviewed Dr. Jeffrey Lancaster from Dell Technologies. Their discussion centered on generative AI and its potential impact."},{"id":52,"type":"Episode","title":"Practical Generative AI","tags":["genai","ai","collectiongenerativeai","datamanagement"],"body":"\r\n\r\nIn the realm of cutting-edge technology, few innovations hold as much promise as Generative AI. This revolutionary concept, with its potential applications spanning various industries, is set to redefine how we interact with machines and reshape our approach to creativity and problem-solving.\n\n## The Power of Generative AI: Transforming Industries\n\nThe development of Generative AI has the power to revolutionize industries by automating processes and improving content creation. This technology allows AI systems to generate outputs in various forms including written content and artistic creations. It can simplify tasks like form-filling and content generation, expediting and optimizing processes within organizations. With this innovation, mundane tasks can be streamlined, freeing up human resources to focus on more valuable activities. Imagine a world where routine tasks are simplified and human efforts are directed towards more meaningful tasks.\n\n## Human-AI Collaboration: Crafting the Future Together\n\nIn the world of Generative AI, there exists a powerful collaboration between human knowledge and technological capabilities. This partnership is evident in the development of genetic sequences and extends to various other uses. The idea of augmented intelligence takes center stage, as humans utilize AI to collect and analyze information quickly. Although human expertise remains vital, AI's capacity to handle significant amounts of data in a short period is a valuable asset. It's a mutually beneficial alliance where each side complements the other's strengths, resulting in improved problem-solving abilities.\n\n## Generative Tools: Unleashing New Dimensions of Creativity\n\nThe capabilities of generative AI extend beyond a single domain and include various forms of media such as images, code, and sound. This advancement opens up new avenues for creativity and innovation across different industries. An interesting feature of this technology is the ability to adjust the level of creativity, referred to as \"hallucination.\" This allows the output to meet specific requirements while still giving users the freedom to fine-tune the creative output. Essentially, this tool gives users the ability to utilize technology while maintaining control over the final result.\n\n## Empowerment through Technology: A Glimpse into the Future\n\nTechnology has a significant impact on our daily lives, with website builders like Squarespace being just one example. Many people wonder if these tools will replace professionals or empower individuals. Most people believe that these tools will empower individuals, helping them take control of their projects. This approach encourages users to be independent while also being critical of their work, which is an essential characteristic of effective technology use.\n\nAI tools are also accelerators that aid in tasks such as coding and writing. They help with grammar, structure, and idea generation. However, they cannot replace human cognitive abilities, emotional expression, and unique perspectives. Combining human insight with AI assistance results in a holistic approach to technology integration.\n\nGenerative AI is more than just a technological marvel; it represents a paradigm shift that highlights the symbiotic relationship between human intellect and machine efficiency. This synergy has the potential to revolutionize industries, streamline processes, and unleash new dimensions of creativity. By embracing these advancements and utilizing their capabilities, we can embark on a journey where technology enhances human potential and enables us to accomplish greater feats.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT152-en","image":"./episodes/edt-152/en/thumbnail.png","lang":"en","summary":"In this episode of the podcast Embracing Digital Transformation, host Darren Pulsipher engages in a thought-provoking conversation with Dr. Jeffrey Lancaster. Their discussion delves into the practical applications of generative AI and the profound impact it is set to bring across various industries."},{"id":53,"type":"Episode","title":"Training the Next Generation in AI","tags":["genai","generativeai","ai","robotics","people","training","collectiongenerativeai"],"body":"\r\n\r\n## Harnessing the Power of Linear Algebra and Calculus in AI\n\nLinear algebra and calculus form the backbone of artificial intelligence (AI) algorithms and systems. In a recent podcast episode, Pete Schmitz, a retired Intel employee and AI enthusiast, highlights the importance of understanding these fundamental mathematical concepts in the context of AI.\n\nLinear algebra is crucial in AI, particularly in tasks such as image recognition. Through matrix multiplication, convolutional neural networks (CNNs) are able to process and analyze vast amounts of image data, enabling the identification and classification of objects in images. Calculus, on the other hand, is utilized in training AI models through techniques like gradient descent, where the algorithm continuously adjusts its parameters based on the rate of change of a given function.\n\nSchmitz emphasizes the value of students learning these subjects in school, as it provides them with a solid foundation to delve into the world of AI. Understanding the fundamentals enables students to build on the knowledge and advancements made by previous generations in the field of AI. With the exponential growth in technology, AI is evolving rapidly, allowing for more efficient and automated solutions to previously laborious tasks.\n\n## AI's Transformative Impact Across Industries\n\nThe podcast also delves into the transformative impact of AI across various industries. AI-powered systems are enabling advancements in healthcare, retail, and several other sectors. For instance, AI is being utilized in healthcare to detect and diagnose diseases like cancer, improving the accuracy and efficiency of healthcare professionals. In the retail sector, AI is used to analyze customer shopping habits and provide personalized recommendations, enhancing the overall shopping experience.\n\nFurthermore, the hosts discuss the recent advancements in generative AI models, such as transformers. These models have the ability to identify underlying patterns in large datasets, facilitating data analysis and decision-making. By leveraging transformers and generative models, industries can unlock valuable insights and drive innovation.\n\n## Fostering Innovation and Adapting to New Technologies\n\nInnovation is a key theme throughout the podcast episode. The hosts stress the importance of organizations embracing new technologies and processes to stay relevant in today's rapidly evolving world. It is essential to foster a comprehensive ecosystem that supports innovation in various industries, providing specialized tools and services for different aspects of innovation.\n\nThe podcast also encourages empowering new talent in engineering, business, and marketing roles to think outside traditional norms and embrace fresh perspectives. By breaking free from outdated processes and ways of thinking, organizations can tap into the potential of their employees and drive innovation.\n\nThe guest speaker, Pete Schmitz, emphasizes the need for continuous learning and adaptation in the face of technological advancements and digital transformations. Organizations must evolve and embrace change to avoid becoming obsolete in the competitive landscape.\n\nIn conclusion, this podcast episode sheds light on the significance of linear algebra and calculus in AI, the transformative impact of AI across industries, and the importance of fostering innovation and adapting to new technologies. Through a comprehensive understanding of AI fundamentals, harnessing transformative technologies, and fostering innovation, organizations can seize the vast opportunities presented by digital transformation and stay ahead in the evolving world of AI.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Pete Schmitz","Darren W Pulsipher"],"link":"/episode-EDT153-en","image":"./episodes/edt-153/en/thumbnail.png","lang":"en","summary":"In this podcast episode, Pete Schmitz, a retired Intel account executive, talks about his work with high school students in teaching them about AI and how to use it in their robotics competitions. He explains that these competitions require the use of autonomy, and AI is a crucial component in achieving that. Pete shares an example of how computer vision, powered by AI, is used in the Defense Advanced Research Projects Agency's unmanned surface vehicle, DARPA D Hunter."},{"id":54,"type":"Episode","title":"GenAI Use Cases","tags":["genai","ai","collectiongenerativeai","datamanangement"],"body":"\r\n\r\nGenerative AI is a transformative technology that can augment human creativity, enhance collaboration, and unlock new possibilities for work and communication. By leveraging AI's capabilities, individuals can generate content, summarize emails, and automate routine tasks, while maintaining human touch and individuality.\n\n## Unleashing Human Creativity\n\n### Understanding the Data Landscape and Setting Clear Goals\n\nDr. Lancaster emphasizes the importance of understanding the type of data you want to either use or create before delving into generative AI. Whether it's text, images, music, videos, or audio, having a clear understanding of your input and desired output enables you to select the most appropriate tools and platforms.\n\n### Augmenting Human Creativity with AI\n\nOne of the key takeaways from the podcast is the role of generative AI in augmenting human creativity rather than replacing it. AI tools act as catalysts, enhancing and propelling human creativity to new heights. By combining the innovative mindset of humans with the capabilities of AI, individuals can solve complex problems and generate groundbreaking ideas that traditional approaches alone cannot achieve.\n\n### Collaboration and Brainstorming with AI\n\nGenerative AI opens doors to collaboration and brainstorming. AI can serve as an additional voice in group discussions, sparking new perspectives and prompting fruitful conversations. This collaborative aspect is particularly valuable in group settings, where AI can listen to conversations, facilitate discussions, and help consolidate ideas into a consensus.\n\n### Unleashing the Power of Generative AI\n\nGenerative AI holds immense potential to unlock creativity, augment human capabilities, and offer fresh perspectives and solutions to challenges. Whether you're a developer, researcher, or simply curious about AI, there is a wealth of opportunities to explore and create with generative AI.\n\n## Practical Applications of Generative AI in the Workplace\n\nIn addition to the insights shared in the podcast, there are numerous practical applications of generative AI that can revolutionize our work processes. Let's explore a few of them:\n\n### Summarizing Lengthy Emails and Streamlining Communication\n\nBusy professionals often receive lengthy emails that consume valuable time. Generative AI can help by analyzing the email content and generating a concise summary that captures the main points and key takeaways. This allows recipients to grasp important information quickly and make informed decisions without spending excessive time reading through the entire email.\n\n### Automating Content Creation\n\nGenerative AI can automate the creation of reports, articles, and other written content. By inputting relevant data or information into a generative AI tool, journalists and content creators can generate complete articles or reports based on that input. This saves significant time and resources, especially for those who need to produce large amounts of content regularly.\n\n### Enhancing Artistic Creativity\n\nCreatives in art and music can leverage generative AI to explore new styles, techniques, and inspirations. AI can assist artists in generating ideas, composing music, and creating visual content. With the power of generative AI, artists can expand their creative horizons and push boundaries in their respective fields.\n\n## Balancing Automation and Human Touch\n\nWhile generative AI offers incredible potential, it is crucial to maintain human oversight and intervention to ensure accuracy, context, and preserve individuality. Trusting AI-generated content blindly without human intervention can lead to homogenization in the digital landscape. It's essential to strike a balance between automation and the human touch, where AI enhances human creativity rather than replacing it.\n\nAs generative AI continues to evolve, we can expect to witness its integration into various aspects of work and communication. From summarizing emails to automating content creation and enabling new forms of artistic expression, generative AI has the capacity to streamline processes, enhance productivity, and unlock new possibilities for innovation. Embracing this technology, while upholding human creativity and uniqueness, will shape the future of work in remarkable ways.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT154-en","image":"./episodes/edt-154/en/thumbnail.png","lang":"en","summary":"In the latest episode Dr. Jeffrey Lancaster and Darren Pulsipher dive into the practical use cases of generative AI and how it can unleash human creativity in various fields."},{"id":55,"type":"Episode","title":"GenAI Advisor for Datacenter Management","tags":["ai","sdi","vergeio","cloud","privatecloud","collectiongenerativeai","hybridcloud"],"body":"\r\n\r\nIn the \"Embracing Digital Transformation\" podcast episode, Chief Solution Architect Darren Pulsipher interviews Greg Campbell, the CTO of Verge.io. The conversation revolves around innovative solutions in infrastructure management and the potential of augmented intelligence. Greg shares his background as a software developer and entrepreneur, discussing the challenges he aimed to address with Verge.io, a company focused on simplifying infrastructure management in distributed servers.\n\n## Simplifying Complex Infrastructure Management\n\nManaging infrastructure in today's digital landscape poses significant challenges. The complexity arises from various components, vendors, licenses, and versioning. This necessitates skilled staff and often results in high costs and a shortage of expertise. While the cloud was initially seen as a solution, it introduced its own complexities.\n\nVerge.io offers a solution through its operating system, VergeOS. This system allows developers to easily manage and connect storage, compute, and networking resources across different hardware configurations. By providing a virtual data center, VergeOS simplifies infrastructure management, making it more intuitive and user-friendly.\n\n## The Potential of Generative AI in Infrastructure Management\n\nGreg also discusses his interest in artificial intelligence (AI) and its potential applications. He shares his experiences with generative AI and its use in infrastructure management. Greg explores how the automation of infrastructure and data center management through generative AI can simplify complex processes and streamline resource management.\n\nGenerative AI can automate infrastructure management, eliminating the need for specialized experts and improving efficiency. It has the potential to revolutionize user interface design and adaptive interfaces, making the infrastructure management process more intuitive and user-friendly.\n\n## Augmented Intelligence as a Valuable Assistant\n\nAugmented intelligence is the combination of human and machine intelligence. Augmented intelligence enhances human capabilities and decision-making by providing insights and answers to complex problems. It is intended to assist, rather than replace, human judgment in making informed decisions.\n\nGreg emphasizes that as AI models become larger and more sophisticated, their accuracy and predictive abilities improve. Augmented intelligence can be applied in various industries, such as customer support, where AI models can provide responses to customer queries and aid human agents in finding solutions. It can also assist in managing remote sites or offices, providing guidance to on-site personnel who may lack expertise in certain areas.\n\n## The Future of Digital Transformation\n\nThe podcast concludes with a discussion on the future of augmented intelligence and the potential impact on industries and the workforce. Greg's optimism lies in the ability of augmented intelligence to improve efficiency and productivity, but with a recognition that it should not replace human judgment entirely. The conversation highlights the importance of careful implementation, ongoing human oversight, and ethical considerations when leveraging augmented intelligence.\n\nOverall, this podcast episode offers valuable insights into innovative infrastructure management solutions, the potential of generative AI in streamlining processes, and the benefits of augmented intelligence as a valuable assistant. It demonstrates the power of embracing digital transformation and leveraging technology to drive efficiency and success in organizations.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Greg Campbell","Darren W Pulsipher"],"link":"/episode-EDT155-en","image":"./episodes/edt-155/en/thumbnail.png","lang":"en","summary":"In this episode host Darren Pulsipher sits down with Greg Campbell, CTO of Verge.io, to discuss the exciting intersection of AI and infrastructure management. Greg, a software developer and entrepreneur, shares his journey of creating Verge.io to address the complexities of infrastructure stitching and management."},{"id":56,"type":"Episode","title":"Becoming a Data Ready Organization","tags":["collectiongenerativeai","datamanagement","automation","dataquality","strategicanalytics","generativeai","digitaltransformation","datadriveninsights","datareadiness","innovation","decisionmaking","technologytrends","businessintelligence","datastrategy","analytics","bigdata","continuouslearning","operationalefficiency","dataoptimization","datainnovation","emrbacingdigital","edt156"],"body":"\r\n\r\n## Evolution of Data Management: From Manual to Automation\n\nRon begins the conversation by highlighting the manual and labor-intensive process of data management in the early days of his career. In industries like nuclear weapons systems and space, data management required meticulous manual effort due to the high reliability and complexity of systems. However, as the world has become more data-driven and reliant on technology, organizations have recognized the need to transform data into more usable and effective ways.\n\n## Challenges in Data Management: Complexity and Quality\n\nRon shares a compelling example from his experience in the Navy, discussing the challenges of managing data for ships during maintenance and modernization cycles. The complexity of ship systems and the harsh maritime environment make thorough data analysis and planning crucial for successful maintenance and repairs. This highlights the importance of data quality and its impact on operational efficiency and decision-making.\n\n## Data Readiness and Automation\n\nTaking advantage of automation requires organizations to focus on data quality. In the automated analysis and assessment process, any errors or missing data become critical. To address this, organizations need to improve data collection from the start. By designing systems that make data collection easier and considering the person collecting the data as a customer, organizations can minimize errors and improve data quality.\n\nA holistic approach to data readiness is also crucial. This involves recognizing the different stages of data readiness, from collection to management and processing. By continually improving in each area, organizations can ensure that their data is of high quality and ready to support various operations and technologies like generative AI.\n\n## Filtering the Noise: Strategic Data Analytics\n\nData analytics plays a vital role in driving strategic value for organizations. Ron and Darren discuss the importance of filtering data based on relevance to objectives and focusing on what is truly important. Not all data will be valuable or necessary for analysis, and organizations should align their data collection with their goals to avoid wasting resources.\n\nFurthermore, the conversation emphasizes that data doesn't have to be perfect to be useful. While precision and accuracy are important in some cases, \"good enough\" data can still provide valuable insights. By recognizing the value of a range of data, organizations can avoid striving for unattainable perfection and focus on leveraging the insights available.\n\n## Uncovering Unexpected Value: Embracing Possibilities\n\nThe podcast also explores the potential of generative AI in enhancing data collection. By using interactive forms and conversational interfaces, organizations can gather more meaningful information and uncover new insights. This opens up possibilities for improved data analysis and decision-making, particularly in areas where data collection is crucial.\n\nThe discussion concludes with the reminder that data analytics is a journey of continuous learning. Organizations should be open to exploring new technologies and approaches, always seeking to discover unexpected value in their data.\n\n## Conclusion\n\nIn an increasingly data-driven world, becoming a data-ready organization is crucial for success. By understanding the evolution of data management, focusing on data quality and readiness, and embracing the possibilities of strategic data analytics, organizations can unlock the power of data to drive innovation, optimize operations, and make informed decisions. This podcast episode provides valuable insights and highlights the importance of data management and analytics in the digital age.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Ron Fritzemeier","Darren W Pulsipher"],"link":"/episode-EDT156-en","image":"./episodes/edt-156/en/thumbnail.png","lang":"en","summary":"In the podcast episode, retired Rear Admiral Ron Fritzmeier joins host Darren Pulsipher to discuss the importance of data management in the context of generative artificial intelligence (AI). With a background in electrical engineering and extensive experience in the cyber and cybersecurity fields, Ron provides valuable insights into the evolving field of data management and its critical role in organizational success in the digital age."},{"id":57,"type":"Episode","title":"Operationalizing GenAI","tags":["ai","generativeai","collectiongenerativeai","infrastructuremanagement","aisystems","aimodels","operationalization","datainput","modeltraining","finetuning","digitaltransformation","opensourcemodels","privateclouds","edgecomputing","aitools","creativeoutput","responsibleusage","reinforcementlearning","monitoring","optimization","sandboxenvironment","cloudbasedinfrastructure","onpremisesinfrastructure","hybridinfrastructure","customerservice","brainstormingapplications","embracingdigital"],"body":"\r\n\r\n## Exploring Different Sharing Models of Generative AI\n\nThe podcast highlights the range of sharing models for generative AI. At one end of the spectrum, there are open models where anyone can interact with and contribute to the model’s training. These models employ reinforcement learning, allowing users to input data and receive relevant responses. Conversely, some private models are more locked down and limited in accessibility. These models are suitable for corporate scenarios where control and constraint are crucial.\n\nHowever, there is a blended approach that combines the linguistic foundation of open models with additional constraints and customization. This approach allows organizations to benefit from pre-trained models while adding their layer of control and tailoring. By adjusting the weights and words used in the model, organizations can customize the responses to meet their specific needs without starting from scratch.\n\n## Operationalizing Gen AI in Infrastructure Management\n\nThe podcast delves into the operationalization of generative AI in infrastructure management. It highlights the advantages of using open-source models to develop specialized systems that efficiently manage private clouds. For example, one of the mentioned partners implemented generative AI to monitor and optimize their infrastructure's performance in real time, enabling proactive troubleshooting. By leveraging the power of AI, organizations can enhance their operational efficiency and ensure the smooth functioning of their infrastructure.\n\nThe hosts emphasize the importance of considering the type and quality of data input into the model and the desired output. It is not always necessary to train a model with billions of indicators; a smaller dataset tailored to specific needs can be more effective. By understanding the nuances of the data and the particular goals of the system, organizations can optimize the training process and improve the overall performance of the AI model.\n\n## Managing and Fine-Tuning AI Systems\n\nManaging AI systems requires thoughtful decision-making and ongoing monitoring. The hosts discuss the importance of selecting the proper infrastructure, whether cloud-based, on-premises, or hybrid. Additionally, edge computing is gaining popularity, allowing AI models to run directly on devices reducing data roundtrips.\n\nThe podcast emphasizes the need for expertise in setting up and maintaining AI systems. Skilled talent is required to architect and fine-tune AI models to achieve desired outcomes. Depending on the use case, specific functionalities may be necessary, such as empathy in customer service or creativity in brainstorming applications. It is crucial to have a proficient team that understands the intricacies of AI systems and can ensure their optimal functioning.\n\nFurthermore, AI models need constant monitoring and adjustment. Models can exhibit undesirable behavior, and it is essential to intervene when necessary to ensure appropriate outcomes. The podcast differentiates between reinforcement issues, where user feedback can steer the model in potentially harmful directions, and hallucination, which can intentionally be applied for creative purposes.\n\n## Getting Started with AI Models\n\nThe podcast offers practical advice for getting started with AI models. The hosts suggest playing around with available tools and becoming familiar with their capabilities. Signing up for accounts and exploring how the tools can be used is a great way to gain hands-on experience. They also recommend creating a sandbox environment within companies, allowing employees to test and interact with AI models before implementing them into production.\n\nThe podcast highlights the importance of giving AI models enough creativity while maintaining control and setting boundaries. Organizations can strike a balance between creative output and responsible usage by defining guardrails and making decisions about what the model should or shouldn't learn from interactions.\n\nIn conclusion, the podcast episode provides valuable insights into the operationalization of generative AI, infrastructure management, and considerations for managing and fine-tuning AI systems. It also offers practical tips for getting started with AI models in personal and professional settings. By understanding the different sharing models, infrastructure needs, and the importance of creativity and boundaries, organizations can leverage the power of AI to support digital transformation.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT157-en","image":"./episodes/edt-157/en/thumbnail.png","lang":"en","summary":"In this podcast episode, host Darren Pulsipher, Chief Solution Architect of Public Sector at Intel, discusses the operationalization of generative AI with returning guest Dr. Jeffrey Lancaster. They explore the different sharing models of generative AI, including public, private, and community models. The podcast covers topics such as open-source models, infrastructure management, and considerations for deploying and maintaining AI systems. It also delves into the importance of creativity, personalization, and getting started with AI models."},{"id":58,"type":"Episode","title":"GenAI in Higher Education","tags":["collectiongenerativeai","addressingbiasesingenerativeai","preservingauthenticityandindividuality","balancingaiintegrationineducation","lauratorresnewey","criticalthinkingskills","educationaltechnology","highereducation","aiineducation","aibias","diversityandinclusion","authenticvoices","uniqueperspectives","genai","generativeai","embracingdigital","edt158"],"body":"\r\n\r\n# The Rise of Generative AI in Education\n\nIn a recent episode of the podcast \"Embracing Digital Transformation,\" host Darren Pulsipher interviews Laura Torres Newey, a New York Times bestselling author and English professor, about the impact of generative AI in higher education. The discussion revolves around the integration of AI in the classroom, its effects on teaching methods, concerns about bias, and the preservation of unique voices. Laura shares her insights and experiences as an educator, offering valuable perspectives on navigating the evolving landscape of education in the era of artificial intelligence.\n\n## The Influence of Generative AI in Education\n\nGenerative AI has started to become a notable presence in education, from automated essay grading to providing writing assistance to students. While this technology offers convenience and efficiency, it raises concerns about the potential loss of unique voices. Laura emphasizes the importance of valuing and nurturing students' individual perspectives and creativity in their writing. Instead of outright banning the use of generative AI, Laura believes in teaching students how to use these tools effectively and harness their potential without compromising their own voices.\n\nThe integration of generative AI prompts a shift in the focus of teaching. Rather than solely evaluating the final product, educators should place more emphasis on the learning process. With AI-driven tools like Grammarly available to students, teachers can redirect their attention towards developing critical thinking skills, research abilities, and the discernment to identify reliable sources. By incorporating assignments that involve comparing AI-generated content with traditionally written work, students can analyze the strengths and weaknesses of both approaches, fostering a deeper understanding of their writing and refining their critical thinking skills.\n\n## The Role of Educators in the Era of AI\n\nEducators have an essential role in preparing students for the ever-evolving technological landscape. Laura emphasizes that adapting to and effectively utilizing generative AI is crucial for educators at all levels of education. With AI becoming increasingly prevalent in the workplace, students who can navigate and leverage this technology will be better equipped for future job opportunities. To ensure students are well-prepared, educators must not only familiarize themselves with AI applications but also teach students how to use AI effectively and ethically.\n\nThe shift towards integrating generative AI in education aligns with the U.S. Department of Education's stance on AI. They acknowledge the potential benefits but emphasize the need for users to remain in control, comparing AI's role to that of an electric bike, where the technology lessens the burden but the user ultimately retains control. This approach emphasizes the importance of striking a balance between harnessing the benefits of AI and preserving the unique voices and perspectives of students.\n\n## Conclusion\n\nThe integration of generative AI in education presents both opportunities and challenges. While AI can enhance learning and assist students with their assignments, it is crucial for educators to prioritize critical thinking and address bias concerns to develop well-rounded and independent thinkers. Teachers should embrace AI technology, understanding its applications and teaching students how to navigate and utilize it effectively. By striking a balance between the efficiency of AI-generated content and the preservation of authentic and diverse voices, educators can prepare students for the digital future while ensuring the cultivation of their individuality and creativity.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Laura Newey","Darren W Pulsipher"],"link":"/episode-EDT158-en","image":"./episodes/edt-158/en/thumbnail.jpg","lang":"en","summary":"In this podcast episode, Darren Pulsipher, chief solution architect of public sector at Intel, interviews Laura Torres Newey, a New York Times best-selling author and university professor, about the impact of generative AI in higher education. This episode delves into the challenges and opportunities presented by the integration of generative AI in the classroom, highlighting the need for critical thinking skills, the concerns of bias, and ensuring the preservation of unique voices."},{"id":59,"type":"Episode","title":"GenAI Policies","tags":["collectiongenerativeai","policies","ai","generativeai","guidelines","jeremyharris","darrenpulsipher","roadmap","challenges","efficiencies","dataprotection","privacy","compliance","ethicalconsiderations","feedback","engagement","ratings","reviews","customersatisfaction","customerengagement","embracingdigital","edt159"],"body":"\r\n\r\n## The Need for Clear Policies and Guidelines\n\nJeremy and Darren stress the significance of having a clear policy and a well-defined roadmap for digital transformation. Rushing into digitalization without proper planning can lead to challenges and inefficiencies. By establishing policies and guidelines, organizations can outline their objectives, set a strategic direction, and ensure that everyone is on the same page.\n\nThey emphasize that digital transformation is more than just adopting new technologies - it requires a shift in organizational culture and mindset. Policies can help facilitate this change by setting expectations for employees, defining digital best practices, and providing a framework for decision-making in the digital realm.\n\n## Navigating the Complexities of Digitization\n\nDigital transformation brings forth a complex set of challenges, such as data security, privacy, and compliance. Organizations need to address these challenges by incorporating them into their policies and guidelines. This includes implementing data protection measures, conducting regular security audits, and ensuring compliance with relevant regulations.\n\nPolicies should also address the ethical considerations that come with digital transformation. The hosts emphasize the importance of organizations being responsible stewards of data and ensuring that the use of digital technologies aligns with ethical standards. Clear guidelines can help employees understand their responsibilities and promote responsible digital practices across the organization.\n\n## The Role of Feedback and Engagement\n\nThe hosts highlight the importance of feedback and engagement in the digital world. Adopting a policy that encourages and values feedback can help organizations continuously improve and adapt to changing circumstances. By welcoming suggestions and input from employees and customers, organizations can refine their digital strategies and ensure that they are meeting the needs of all stakeholders.\n\nThey also mention the significance of ratings and reviews in the digital era. Feedback through ratings and reviews not only provides valuable insights to organizations but also serves as a measure of customer satisfaction and engagement. Policies can outline how organizations collect and respond to feedback and establish guidelines for capturing customer sentiment in the digital space.\n\n## Conclusion\n\nDigital transformation is a journey that requires careful planning, clear policies, and ongoing adjustments. By establishing policies and guidelines, organizations can navigate the complexities of digitization, address challenges, and ensure responsible and effective use of digital technologies. Embracing digital transformation is not just about adopting new tools, but also about creating a digital culture that fosters innovation and meets the evolving needs of customers and stakeholders.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeremy Harris","Darren W Pulsipher"],"link":"/episode-EDT159-en","image":"./episodes/edt-159/en/thumbnail.png","lang":"en","summary":"In this episode, host Darren interviews Jeremy Harris and delve into the importance of establishing policies and guidelines for successful digital transformation. With the increasing prevalence of digital technologies in various industries, organizations need to adapt and embrace this transformation to stay competitive and meet evolving customer expectations."},{"id":60,"type":"Episode","title":"Decreasing Ingestion Congestion with Intel Optane DCPMM","tags":null,"body":"\r\n\r\n## Service Stack Details\n\nA customer in the automotive industry was having a hard time trying to effectively get information out of their cars and into their data center so that they could do machine learning and analytics. There has been research in this area, but only for a small number of cars, not the customer’s hundred million cars. When I looked at their whole service stack and how everything was coming into the data center, data ingestion became the clear issue: How do I ingest that much data, and how do I do it quickly?\n\n## High Level Kafka Architecture Overview\n\nThe customer wanted to use Kafka for their ingestion. Kafka is a broker than can scale well, and the key is that it can handle several producers, different consumers, and a lot of data. Using several Kafka brokers to place and land data in the most appropriate places offers great flexibility.\n\nKafka, however, was primarily designed for message sizes of about one to 10 kilobytes and the customer’s data was approximately 240 kilobytes per car.  There are work-arounds, but I wanted to bring the whole 240 kilobyte message into the Kafka bus so I could move it as I needed to.\n\n## Performance Best Practices\n\nI looked at the performance best practices of others working with Kafka to see if I could scale it for my customer. Increasing the buffer size to accommodate the whole message is one fine-tuning solution, along with managing batch size for optimal performance. Another successful practice is spreading out the logs. The flexibility of Kafka would allow me to put the data into different topics. I can break the topics into several partitions that I can span over multiple drives.  The question is, then, how many drives am I spreading the Kafka logs over? In addition, I want the fastest drives possible.\n\nAn example I examined was LinkedIn. Their published numbers from a year ago are that they can handle 13 million messages a second, or 2.7 gigabytes per second. They say they have about 1,100 Kafka brokers and more than 60 on a cluster, so that’s a pretty big configuration.\n\n## Automotive Space\n\nIf I look at the customer’s raw numbers (1.6 million messages per second and 800 gigabytes per second), and compared them to LinkedIn, which is probably not optimized for 240 kilobytes, I come up with 44,000 brokers. If I optimized it, I could probably get down to 4,400 brokers, which is still 240 clusters. That’s a huge amount of machines, so I had to come up with a way to make things faster. With more optimization, I could probably get it down to 400 to 500 brokers, but I wanted to see what else I could do.\n\n## Intel Optane DC Persistent Memory\n\nI looked to our Optane Persistent Memory. It fits in a DDR4 format, so it sits right on the DDR4 bus. It goes up to 512 gigabyte modules, so in one two-socket server, I can have six terabytes of persistent memory. I wanted to figure out a way to leverage this highly reliable technology with great features such as built-in hardware encryption to help me solve this problem.\n\n## Support for Breadth of Applications\n\nThere are two modes of operation with this Optane Memory: direct app mode and memory mode. The memory mode is simple. It uses the persistent memory as normal RAM because it’s cheaper than normal DDR4. It's not the same as DDR4, but it’s close enough so that in most applications, you can’t see a difference.  In app direct mode, you can actually write from your program directly into persistent memory. This way, I don’t have to marshal and unmarshal data structures and stream them; I can just push them into persistent memory. I can also mount app direct mode as a file system so it’s on the memory bus, which is much faster than on the IO bus. Now, what can I do with this memory?\n\n## Using Linux Kernel\n\nThere are two primary tools available using the Linux kernel: ndctl and ipmctl. Ndctl is a non-volatile memory device controller, and then there is IPM, the Intel Persistent Memory controller, which lets me manipulate and control this persistent memory. I can set it up in the memory mode or app direct mode.  I had a bit to learn about these tools and how they work.\n\n## Ingestion Approach\n\nMy first thought was that if I gave Kafka a lot more memory with large buffer sizes, it should run a lot faster. Code changes in the configuration would be unnecessary or minimal.  Another option was changing Kafka to write to persistent memory rather than writing to a file system, bypassing the hard drive. The last thing I looked at was creating a persistent file system using persistent memory and then putting the Kafka logs on this new file system.\n\nThe easiest of the three options was the first – more memory. I ran all my tasks with more memory, and there was no change in performance. The reasons why is because eventually my buffers filled up and I had to ride out to a drive. Everything ultimately had to go to the Kafka logs, which was my bottleneck.\n\nThe second option involves rewriting code and waiting on approvals, so I jumped to the third option. The results of this experiment where I pointed the logs to this new, ultra-fast file system, were fascinating. Let’s take a look at the process and the results.\n\n## Testing Constraints\n\nTo remove obstacles to testing performance, I took network out of the equation by running my test on the same machine that my broker was on. Also, I only ran producers, then only consumers, then mixed, so I could weigh the differences. My goal was not to look at total production improvement, but at an individual broker to see if this drive would really make a difference.\n\n## First Approach 50/50\n\nThe first thing I did was take half of my persistent memory into app direct mode and turned it into a file system. I left the other half as memory. I used the commands ndctl and ipmctl and created name spaces. I mounted these file systems and ran my test.\n\n## Changing Message Size\n\nI ran the tests over several different message sizes. I was expecting certain optimization, primarily for 1 kilobyte. I saw that I got better and better performance up to around 10 producers. Past 10 producers, I started saturating the bus and getting some variability. That tells me I was cashing things. I could now compare these numbers to what I ran before on just a SATA drive for the Kafka logs. I also tried our Optane NVMe drives for the logs.\n\n## Technology Comparison\n\nLet’s take a look at the differences. For 240 kilobyte, with a normal SATA drive, it’s pretty flat. I got some improvement, and then it decreased as the number of producers increased. With the Optane NVMe drive, I got a nice peak, almost twice as fast as a SATA drive, which is what I would expect because it's an NVMe bus instead of a SATA bus. The Pmem is almost five times faster than a SATA drive and two and a half times faster than the Optane NVMe drive. That’s because I’m using a memory bus instead of the SATA or NVMe bus.\n\n## Additional Optimization (100% App Direct)\n\nThis was running fast and I was quickly filling up this temporary 750 GB drive. Since I needed to run the test a little longer, I went back and reconfigured my machine to do a 100 percent direct app mode so that I could now take the whole 1.5 terabytes.\n\n## Optimized PMEM and 100% App Direct\n\nAfter I did this and ran the same tests, I got a surprising result. I could add more producers, and my throughput went up almost another two or three times. Now it’s between 12 and 15 times faster than a SATA drive at 25-30 producers at 240 kilobytes message size. This is incredible and would greatly reduce my customer’s need for so many brokers, rows and rows of machines. I ran the test several times because I didn’t believe the results I was getting. I called one of our architects that designed this technology and learned that one reason for the increased speed was when I was using part of the persistent memory as memory, the data would have to go through two or three hops that are not necessary with app direct mode. That creates less contention on the memory bus, and the throughput went up dramatically.\n\n## Call to Action\n\nThe end result is that I was able to use Kafka with Optane DC Persistent Memory as an ultra-fast file system to get major improvements in throughput on both producers and consumers.  A single broker can handle 15 times more messages and throughput than before, decreasing the number of servers required to handle large, complex system architectures. It’s time to evaluate your current architecture and see if this would benefit your organization.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT16-en","image":"./episodes/edt-16/en/thumbnail.png","lang":"en","summary":"In this episode, Darren talks about decreasing ingestion congestion using Intel’s Optane DC Persistent Memory, and the experiment he conducted with surprising results. It just might change the way we think about programming in the future."},{"id":61,"type":"Episode","title":"Security in Generative AI","tags":["collectiongenerativeai","personalizedphishingattacks","promptinjection","sharingcodeai","harnessingai","digitaltransformation","generativeai","cybersecurityrisks","serviceproviders","duediligence","riskschallenges","digitallandscape","proactivecybersecurity","llm","multifactorauthentication","voicerecognition","typingcadence","github","stackoverflow","samsungipleak","securityaspects","embracingdigital","edt160"],"body":"\r\n\r\n## Personalized and Convincing Phishing Attacks\n\nOne of the main concerns discussed is the potential for more sophisticated and personalized phishing attacks. Phishing currently stands as the most effective cyber attack method, and with generative AI, attackers can create highly personalized and convincing phishing emails or messages. By retrieving information from social media or other online platforms, attackers can make their phishing attempts harder to detect. This raises the question of how we can determine what is real or not and how we can trust the authenticity of the information we receive.\n\nTo combat this, individuals may need to develop new methods of verifying information, such as using personal code words or other authentication measures with loved ones. Additionally, organizations and security agencies must adapt their strategies to counter the increased sophistication of cyber attacks facilitated by generative AI. It's crucial to understand that generative AI itself is a neutral technology, and its implications depend on how it is used.\n\n## Cloned Voices and Trusting Information\n\nThe podcast also explores the potential for generative AI to clone voices, which has already been observed in virtual kidnapping attacks. Criminals use cloned voices to create a sense of urgency and fear, pretending to be a victim's loved ones. This raises concerns about trusting the authenticity of information we receive.\n\nIn such a scenario, it becomes essential to develop techniques to verify the authenticity of voices and information. As individuals, we should remain vigilant and exercise caution when responding to urgent requests over the phone. Ensuring open lines of communication with trusted contacts can help verify if such requests are genuine.\n\n## Protecting Intellectual Property in Coding and Programming\n\nThe podcast segues into a discussion about the importance of protecting intellectual property in coding and programming. The hosts highlight the risks of unintentionally sharing code on platforms like StackOverflow and GitHub, and the inadvertent leakage of intellectual property when seeking help in these public forums. Developers are encouraged to replace sensitive information with placeholders before sharing code to mitigate the risk of intellectual property loss.\n\nAdditionally, the hosts discuss the introduction of tools like GitHub Copilot, which use generative AI to provide code suggestions. While these tools can be valuable, they raise concerns about the security and privacy of proprietary information. Developers must carefully consider the trustworthiness of the service provider and ensure adequate protection of their data and intellectual property.\n\n## Balancing Innovation and Security in the Age of AI\n\nThe conversation concludes by emphasizing the importance of striking a balance between embracing the advancements and potential positive changes brought about by generative AI, and addressing the associated risks in the realm of cybersecurity and intellectual property protection. It is essential to remain informed, adapt security strategies, and exercise caution to navigate the evolving landscape of digital transformation successfully. By doing so, we can harness the benefits of AI without compromising security and personal information.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT160-en","image":"./episodes/edt-160/en/thumbnail.png","lang":"en","summary":"In this episode, host Darren Pulsipher is joined by Dr. Jeffrey Lancaster to delve into the intersection of generative AI and security. The conversation dives deep into the potential risks and challenges surrounding the use of generative AI in nefarious activities, particularly in the realm of cybersecurity."},{"id":62,"type":"Episode","title":"Natural Language Data Analytics","tags":["collectiongenerativeai","ai","generativeai","embracingdigital","edt161","challengesinnaturallanguageprocessing","fantasysportsapp","naturallanguagegeneration","highqualitycontent","technicalbackground","outoftheboxthinking","pushingboundaries","diversity","crossdomaincollaboration","innovativeideas","infoscentience'ssolution","dataanalytics","naturallanguageaisystem","conceptualautomata","datasets","revolutionizing","businessesanalyzeinformation","futureofdataanalysis","naturallanguagereporting","flexibility","tailored","differentindustries","customized","specificcontext","jargon","dataanalysis","revolutionizingindustries","sportsanalytics","stevewasick'sjourney","innovativeapproach","entrepreneurs","techfounders","unconventionalpaths","successfulinnovations","embracingvariability","context","poweroflanguage"],"body":"\r\n\r\n## Challenges in Natural Language Processing\n\nSteve recalls his early project—an app for fantasy sports that aimed to provide users with not just statistics, but also the context and stories behind the numbers. This led him to the field of natural language generation, where he faced challenges in acquiring and delivering high-quality content. Despite not having a technical background, Steve's diverse experiences allowed him to approach these challenges with creativity and out-of-the-box thinking.\n\n## Pushing Boundries \n\nDarren praises Steve for pushing boundaries and bringing a fresh perspective to the field. This highlights the importance of diversity and cross-domain collaboration in generating innovative ideas and solutions. Steve's journey serves as an inspiration for aspiring entrepreneurs and tech founders, proving that unconventional paths can lead to successful innovations.\n\n## InfoScentience's Solution to Data Analytics\n\nThe conversation also delves into the capabilities of InfoSentience's natural language AI system. Steve explains that their technology breaks down events and stories into their constituent parts, providing a better understanding of complex concepts and their relationships. This analytical engine, based on conceptual automata, allows for the synthesis of diverse and complex data sets, revolutionizing the way businesses analyze information.\n\n## The Future of Data Analysis and Natural Language Reporting\n\nFurthermore, Steve emphasizes the flexibility of their AI system, which can be tailored to different industries and customized to meet the unique needs of each client. By understanding the specific context and jargon of the data being analyzed, Info Sentience ensures that their AI system provides accurate and relevant insights.\n\nIn conclusion, the podcast episode highlights the potential of natural language data analytics in revolutionizing industries such as sports analytics. Steve Wasick's journey and innovative approach serve as an inspiration for entrepreneurs and tech founders, reminding us that unconventional paths can lead to successful innovations. The future of data analysis lies in embracing variability, context, and the power of language.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Wasick"],"link":"/episode-EDT161-en","image":"./episodes/edt-161/en/thumbnail.jpg","lang":"en","summary":"In the latest episode Darren Pulsipher sits down with Steve Wasick, the CEO and founder of InfoSentience, to discuss the power and potential of natural language data analytics. Steve, who comes from an unconventional background as an English major turned screenwriter turned lawyer turned tech founder, brings a unique perspective to the field."},{"id":63,"type":"Episode","title":"Building a Multi-Hybrid Cloud Strategy","tags":["cloudadoption","collmultihybridcloud","organizationalmodernization","barriers","riskmitigationplan","applicationrearchitecture","governance","organizationalculture","cloud","multihybridcloud","multicloud","embracingdigital","edt162"],"body":"\r\n\r\n## Best Practices for Cloud Adoption\n\nMoving to the cloud and adopting new technologies like generative AI can bring numerous benefits, but organizations must also be prepared for the changes that come with it. According to Christine McMonigal, director of Data Center and Cloud Technologies at Intel, there are key best practices to consider.\n\n### Organizational Modernization\n\nOne important aspect to recognize is that cloud adoption is not just a technology modernization, but also an organizational modernization. This means that organizations need to be prepared for changes to processes, workflows, and even organizational structures. It's crucial to address these changes and ensure that the entire organization is aligned and prepared for the transformation.\n\n### Identifying Barriers and Setting Clear Expectations\n\nA crucial step in overcoming barriers and mitigating risks is identifying what these barriers are in the first place. By doing a thorough assessment of the current infrastructure, workflows, and challenges within the organization, potential roadblocks can be pinpointed and strategies can be developed to overcome them.\n\nMoreover, setting clear expectations upfront is essential. This means effective communication with stakeholders, employees, and partners about the goals, benefits, and challenges of adopting multi-hybrid cloud strategies. By setting realistic expectations and ensuring everyone is on the same page, organizations can minimize surprises and resistance to change.\n\n### Robust Risk Mitigation Plan\n\nHaving a robust risk mitigation plan in place is another crucial aspect of successful cloud adoption. This includes evaluating potential security risks, data privacy concerns, and compliance requirements. By proactively addressing these risks and implementing appropriate measures, organizations can safeguard their data, ensure regulatory compliance, and minimize potential threats.\n\n## Barrier 1: Application Re-Architecture\n\nOne of the key barriers organizations often face in cloud adoption is application re-architecture. It's important to assess which applications can be lifted and shifted to the cloud as-is, and which ones may require more significant modifications. By identifying opportunities for simplification and cost reduction through automation, organizations can streamline access and controls.\n\n## Barrier 2: Governance\n\nGovernance policies play a crucial role in mitigating risks during cloud adoption. Inconsistent security models, diverse management tools, and heterogeneous user policies can increase complexity and jeopardize the success of the migration. Simplifying governance policies and eliminating bureaucracy can help organizations streamline operations, reduce costs, and ensure data security and compliance.\n\n## Barrier 3: Organizational Culture and Maturity\n\nPreparing the organization for the change that comes with cloud adoption is vital. This involves getting employees on board, providing skills training, and identifying key players who can embrace the new ways of working. Addressing fears and concerns that employees may have, such as fear of being left behind or losing their jobs, is essential to create a positive and collaborative environment.\n\nIn conclusion, adopting multi-hybrid cloud strategies requires careful planning, effective communication, and a thorough understanding of an organization's goals and challenges. By addressing barriers upfront and mitigating risks, organizations can pave the way for a successful digital transformation journey. Stay tuned for the next episodes where we will explore developing a cloud strategy, evaluating application portfolios, and more insights on embracing digital transformation. Don't forget to rate and subscribe to our podcast to stay updated on the latest trends and best practices in the digital landscape.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Christine McMonigal","Darren W Pulsipher"],"link":"/episode-EDT162-en","image":"./episodes/edt-162/en/thumbnail.png","lang":"en","summary":"In this episode Darren interviews Christine McMonigal and discuss the challenges organizations face when transitioning to the cloud and adopting multi-hybrid cloud architectures. They highlight the importance of understanding these obstacles and providing guidance to overcome them. This episode will dive deeper into some key barriers and strategies for mitigating risks, ensuring a successful cloud transformation."},{"id":64,"type":"Episode","title":"Developing a Multi-Hybrid Cloud Operating Model","tags":["cloudstrategy","digitaltransformation","collmultihybridcloud","cloudtechnologies","businessgoals","operationalefficiency","customersatisfaction","itinfrastructure","migrationplan","datasecurity","regulatorycompliance","hybridclouds","publicclouds","privateclouds","clouddeploymentmodels","governanceandsecurity","reducecosts","enhanceefficiency","customerexperience","strategicmove","clearvision","embracingdigital","edt163"],"body":"\r\n\r\nIn today's digital age, businesses are increasingly turning to the cloud as a strategic move to improve efficiency, reduce costs, and enhance customer experience. However, before jumping on the cloud bandwagon, it is essential for organizations to take a step back and assess their specific needs. Developing a cloud strategy is a crucial step in this process, as it allows businesses to align their goals and objectives with the cloud technologies available to them.\n\n## Understanding Your Business Goals and Objectives\n\nThe first step in developing a cloud strategy is gaining a clear understanding of your business goals and objectives. What are you trying to achieve? Are you looking to improve operational efficiency, reduce costs, or enhance customer satisfaction? By having a clear vision of your goals, you can better determine how the cloud can support and enable these objectives.\n\n## Evaluating Your Existing Infrastructure\n\nAfter establishing your goals, it is important to evaluate your current IT infrastructure. This assessment helps identify any potential challenges or limitations in migrating to the cloud. Determine what systems and applications you currently have in place and consider their compatibility with a cloud environment. This evaluation will inform decisions about which applications and services are suitable for migration.\n\n## Choosing the Right Cloud Model\n\nWith various cloud deployment models available, organizations need to assess the different options that align with their business requirements. Public clouds, private clouds, and hybrid clouds each offer distinct advantages and drawbacks. Evaluating the pros and cons of each model will help you determine the most appropriate choice for your organization. Consider factors such as data security, scalability, and regulatory compliance when making this decision.\n\n## Creating a Migration Plan and Ensuring Governance and Security\n\nOnce you have chosen a cloud model, it's time to create a migration plan. This involves outlining the steps and timeline for moving your applications and data to the cloud. Prioritize critical applications that need to be migrated first, and develop a strategy to migrate the remaining applications later. Additionally, implement a governance and security plan to protect your data and comply with any regulatory requirements. Cloud security is a top concern for many businesses, so it is vital to ensure that your data is protected throughout the migration process.\n\nIn conclusion, developing a cloud strategy is a complex process that requires careful planning and assessment. It is essential to understand your business goals, evaluate your existing infrastructure, choose the right cloud model, create a migration plan, and implement proper governance and security measures. By effectively embracing digital transformation and leveraging the power of the cloud, organizations can achieve their objectives, enhance efficiency, and drive growth and success.\n\nIn today's digital age, businesses are increasingly turning to the cloud as a strategic move to improve efficiency, reduce costs, and enhance customer experience. However, before jumping on the cloud bandwagon, it is essential for organizations to take a step back and assess their specific needs. Developing a cloud strategy is a crucial step in this process, as it allows businesses to align their goals and objectives with the cloud technologies available to them.\n\n## Understanding Your Business Goals and Objectives\n\nThe first step in developing a cloud strategy is gaining a clear understanding of your business goals and objectives. What are you trying to achieve? Are you looking to improve operational efficiency, reduce costs, or enhance customer satisfaction? By having a clear vision of your goals, you can better determine how the cloud can support and enable these objectives.\n\n## Evaluating Your Existing Infrastructure\n\nAfter establishing your goals, it is important to evaluate your current IT infrastructure. This assessment helps identify any potential challenges or limitations in migrating to the cloud. Determine what systems and applications you currently have in place and consider their compatibility with a cloud environment. This evaluation will inform decisions about which applications and services are suitable for migration.\n\n## Choosing the Right Cloud Model\n\nWith various cloud deployment models available, organizations need to assess the different options that align with their business requirements. Public clouds, private clouds, and hybrid clouds each offer distinct advantages and drawbacks. Evaluating the pros and cons of each model will help you determine the most appropriate choice for your organization. Consider factors such as data security, scalability, and regulatory compliance when making this decision.\n\n## Creating a Migration Plan and Ensuring Governance and Security\n\nOnce you have chosen a cloud model, it's time to create a migration plan. This involves outlining the steps and timeline for moving your applications and data to the cloud. Prioritize critical applications that need to be migrated first, and develop a strategy to migrate the remaining applications later. Additionally, implement a governance and security plan to protect your data and comply with any regulatory requirements. Cloud security is a top concern for many businesses, so it is vital to ensure that your data is protected throughout the migration process.\n\nIn conclusion, developing a cloud strategy is a complex process that requires careful planning and assessment. It is essential to understand your business goals, evaluate your existing infrastructure, choose the right cloud model, create a migration plan, and implement proper governance and security measures. By effectively embracing digital transformation and leveraging the power of the cloud, organizations can achieve their objectives, enhance efficiency, and drive growth and success.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Rajiv Mandal","Darren W Pulsipher"],"link":"/episode-EDT163-en","image":"./episodes/edt-163/en/thumbnail.png","lang":"en","summary":"In this episode Darren interview cloud solution architect, Rajiv Mandal, about developing a multi-hybrid cloud strategy in your modern IT organization."},{"id":65,"type":"Episode","title":"Application and Workload Portfolios in Cloud Migration","tags":["cloudmigration","organizations","collmultihybridcloud","cloudnative","datacenterdependencies","hybridstrategy","technicaldebt","applications","workloads","politicalcapital","customerfacingapplications","importance","applicationrationalization","analysis","dependencies","smoothtransition","surprises","decision","compliancerequirements","regulatedindustries","compliancemonitoring","embracingdigital","edt164"],"body":"\r\n\r\n## Understanding Application and Workload Portfolios in Cloud Migration\n\nWhen it comes to cloud migration, organizations generally fall into two groups. The first group consists of cloud-native organizations that have architected their applications in the cloud, eliminating any data center dependencies. The second group adopts a hybrid strategy, relying on both the data center and the cloud. However, even these hybrid organizations may have technical debt that needs to be addressed.\n\nOne of the main challenges in cloud migration is understanding the complexity of applications and workloads. Sarah introduces the concept of \"political capital\" an application carries. While external-facing and customer-focused applications often receive the most attention and investment, smaller applications that may not seem significant can have a substantial impact on the organization if they malfunction or are neglected.\n\n## The Importance of Application Rationalization\n\nSarah shares a personal experience that highlights the importance of considering the overall portfolio of applications and workloads during cloud migration. She witnessed a disruption to the business caused by the lack of attention to a seemingly small customer-facing application. This experience underscores the need for organizations to conduct a thorough analysis and rationalization of their application portfolio before migrating to the cloud.\n\nBy understanding the complexities and dependencies of applications and workloads, organizations can ensure a smooth transition to the cloud with fewer surprises or disruptions. Sarah emphasizes the need for organizations to prioritize application rationalization to identify critical applications that may require additional investment and attention, even if they are not the most visible ones.\n\n## To Touch or Not to Touch: Assessing Workloads for Cloud Migration\n\nWhile migrating workloads to the cloud can bring numerous benefits, it may not always be necessary or beneficial to touch certain workloads or applications. Some workloads may have been running smoothly for years and are critical to the organization's operations. In such cases, it may not make sense to make any changes or migrate them to the cloud.\n\nFactors to consider when making the decision include the level of customization and integration of the workload, the presence of technical debt, and the upcoming retirement of legacy systems. However, it is essential to regularly reassess these workloads to ensure they continue to meet the organization's needs. Monitoring industry trends and technological advancements can help identify potential changes in the future.\n\n## Navigating Compliance Requirements in Cloud Migration\n\nCompliance requirements can pose challenges in cloud migration, especially for organizations in regulated industries. However, cloud service providers have made significant progress in addressing these concerns. They offer tools and services that help automate compliance monitoring and reporting, making it less burdensome for organizations to stay compliant.\n\nTo navigate these challenges, organizations should conduct a thorough assessment of their compliance requirements. Consulting with experts who can provide guidance on compliance standards and design a cloud architecture that meets these requirements is crucial. Regular audits and monitoring should be implemented to ensure ongoing compliance.\n\n## Conclusion\n\nIn this podcast episode, Darren Pulsipher and Sarah Musick shed light on important aspects of cloud migration, including the rationalization of application portfolios, decision-making regarding touching workloads, and addressing compliance requirements. By understanding these factors and actively managing technical debt, organizations can embark on a successful cloud migration journey, leveraging the agility and flexibility offered by the cloud while minimizing risks and disruptions.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Sarah Musick","Darren W Pulsipher"],"link":"/episode-EDT164-en","image":"./episodes/edt-164/en/thumbnail.png","lang":"en","summary":"In this episode, Darren interviews Sarah Musick, Cloud Solution Architect at Intel. Together, they dive into the topic of application and workload portfolios in cloud migration. With Sarah's background in cloud consulting and optimization, she brings valuable insights to the discussion."},{"id":66,"type":"Episode","title":"Workload Cloud Placement Factors","tags":["cloudcomputeoptions","collmultihybridcloud","cloudinstance","cloudspecialistadvice","computeselectiontools","optimizecloud","workloadcloud","vendorlockinprevention","cloudbestpractices","cloudinstanceperformance","costeffectivecomputesolutions","workloadoptimization","resourceefficiency","cloud","multihybridcloud","embracingdigital","edt165"],"body":"\r\n\r\n## Understanding Compute Options\n\nCloud service providers (CSPs) offer a mix of different compute families, ranging from older generations of compute hardware to the latest and more performant instances. These older generations are often used for cost-effective computing functions, while newer generations offer improved performance at similar or lower prices.\n\nIt can be overwhelming to navigate through the numerous computing options available in the cloud, especially with new instances being regularly released. That's where cloud specialists, such as those at Intel, come in. These experts can provide valuable insights and assist in selecting the most suitable instance for a specific workload.\n\n## Making Informed Decisions\n\nTo make the best decision, seek the advice of cloud specialists or use tools like Densify or Intel Site Optimizer. These tools leverage machine learning to analyze an application's features, compute usage, and network needs to determine the most suitable instance size. By leveraging these resources, organizations can ensure they're getting the most out of their cloud resources, avoiding underutilization or overspending.\n\n## Implementing Best Practices\n\nIt is important to incorporate instance recommendations into infrastructure as code (IaC) scripts, such as TerraForm, to automate the selection of the most performant instance for a workload. This ensures consistent and efficient instance placement, removing the risk of human error and optimizing performance.\n\n## Considering Portability\n\nWhile Intel currently dominates the cloud market with x86-based instances, there is some competition from AMD and ARM. ARM-based processors, such as the Graviton, are popular among CSPs but need more workload portability between providers and between public and private environments. Porting x86-based workloads to ARM would require extensive code refactoring and redevelopment.\n\nOrganizations should consider compatibility issues when repatriating workloads from the cloud back to on-premises infrastructure. It's crucial to assess the portability and flexibility of the chosen computing platform to ensure seamless transitions and avoid vendor lock-in.\n\n## Conclusion\n\nSelecting the right cloud instance is a critical decision that can impact your workload's performance, cost, and portability. With the aid of cloud specialists and tools, organizations can make informed decisions and optimize their cloud resource utilization. By understanding the available computing options, incorporating best practices, and considering portability, businesses can harness the full potential of the cloud while ensuring flexibility and efficiency in their operations.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Ricardo Dutton Jr","Darren W Pulsipher"],"link":"/episode-EDT165-en","image":"./episodes/edt-165/en/thumbnail.png","lang":"en","summary":"In this podcast, the Darren and Rico Dutton dive into the world of cloud instances and the factors to consider when selecting the right instance for your workload. They discuss the different compute options available in the cloud, the importance of finding the right balance between performance and cost, and the role of cloud specialists in helping organizations make informed decisions."},{"id":67,"type":"Episode","title":"Agility in Cloud Adoption","tags":null,"body":"\r\n\r\n## Cloud Migration as an Ongoing Journey\n\nWhile many people view cloud migration as a one-time process, it is essential to view it as a continuous journey, wherein developers and operations teams work together. Once the workloads are modernized and deployed, constant monitoring and assessment are necessary to determine if they meet business objectives and success metrics.\n\nBy treating cloud migration as an ongoing journey, organizations can enable their teams to iterate, refine, and improve their success. This approach will allow agility, adaptability, and the ability to respond to evolving business needs.\n\n## Repatriating Workloads and Flexibility\n\nAn important aspect to consider is the possibility of migrating workloads back on-premises if the expected benefits from the cloud are not being achieved or if there is a need to switch between different cloud providers. To achieve continuous improvement, it is necessary to evaluate the situation continuously, set expectations upfront, and be agile and flexible in the cloud operating model.\n\nA consistent infrastructure across multiple clouds is essential to enable flexibility and agility. While cloud service providers may try to restrict customers to their services, organizations should resist this temptation and aim for consistency across clouds or be willing to make the necessary changes when moving workloads to different locations.\n\n## Tools and Best Practices for Optimization\n\nOptimizing cloud environments can be complex and time-consuming, requiring expertise and resources. Intel's tools and best practices can help organizations assess and optimize workload placement and provide continuous real-time optimization without impacting applications. By automating certain aspects of the optimization process, these tools can save organizations time and money while improving overall performance.\n\nTo maximize the benefits of these tools, it is crucial to categorize workloads into different buckets based on factors such as standardization, criticality, and experimentation. For example, workloads that require high availability and low latency may need to be placed on dedicated infrastructure, while those that are less critical can be placed on shared infrastructure. Organizations can use a targeted approach to optimization to ensure that their cloud environment is tailored to their specific needs and goals.\n\n## Embracing Digital Transformation and Migrating to the Cloud\n\nThe relevance of organizational change and learning from successful and unsuccessful methods is also highlighted in this episode. To assist organizations in their cloud migration process, valuable resources and guidance can be found at embracingdigital.org.\n\nIn conclusion, by implementing continuous improvement, developing a strategic approach, and embracing organizational change, organizations can optimize their cloud environment, drive efficiency, and achieve their business objectives. Adopting continuous improvement in cloud operations and treating cloud migration as a continuous journey is the key to successful cloud migration.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Christine McMonigal","Darren W Pulsipher"],"link":"/episode-EDT166-en","image":"./episodes/edt-166/en/thumbnail.png","lang":"en","summary":"Cloud migration is no longer a one-time process, but rather a continuous journey that requires constant evaluation, monitoring, and adjustment to achieve business objectives. In this episode of our podcast, host Darren Pulsipher talks to guest Christine McMonigal about the importance of adopting continuous improvement in cloud operations."},{"id":68,"type":"Episode","title":"Leveraging AI to Protect Children","tags":["ai","aipolicy","lawenforcement","aiethics","collgenerativeai","childprotection","aiforgood","chainofcustody","policy","people","embracingdigital","edt167"],"body":"\r\n\r\n## Challenges in Prosecuting Child Predators Online:\n\nOne of the significant challenges in prosecuting child predators online is the lack of uniformity across jurisdictions regarding technology and online crimes. This creates substantial obstacles for law enforcement agencies and a gap in their ability to prosecute and investigate cases effectively. Each jurisdiction operates differently with its own set of laws, regulations, and procedures. Unfortunately, these differences can confuse and make it challenging to investigate and prosecute online sexual predators. Often, traditional investigations are not sufficient to catch online predators. The digital world has created a new breed of tech-savvy criminals who can cover their tracks.\n\nLaw enforcement agencies must be equipped with the resources, technology, and training to combat online sexual predators effectively. Collaboration between technology companies and law enforcement is essential in developing standardized practices and language for prosecution and investigation. By bridging this gap, we can enhance the efficiency of these processes and increase the chances of bringing child predators to justice. Additionally, the public must be informed of online predators' risks and dangers. Parents, educators, and guardians must educate children on how to protect themselves online and what to do if they encounter inappropriate content or communication.\n\n## The Role of AI in Evidence Management:\n\nAI technologies can be vital in managing digital evidence, particularly in cases involving child predators. AI can aid in automating the scanning, reporting, and analysis of illicit content. AI tools can also help reduce the workload of investigators, allowing them to focus on high-priority cases. However, there are still many challenges in implementing and understanding these technologies across different jurisdictions. One of the primary challenges is that AI is only as good as the data it is trained on, and the data varies across jurisdictions. As a result, it is challenging to develop effective AI models that can work across different jurisdictions.\n\nTo ensure efficient evidence management, stakeholders in the justice system must work together in adopting and leveraging AI tools. Collaboration between technologists, law enforcement agencies, and judicial systems is critical to overcoming these challenges and leveraging AI effectively to protect children online. Implementing AI in evidence management should be supported by robust policies and guidelines that protect the privacy of victims and ensure the ethical use of these technologies. Additionally, regular training and education on these tools are essential to ensure their effective use in combating online sexual predators.\n\n## Collaboration and Standardization for Effective Protection\n\nCollaboration and standardization are critical aspects of successfully combating online child exploitation. The fight against this heinous crime requires cooperation between technology providers, law enforcement agencies, and judicial systems. These parties must work together to develop comprehensive strategies and solutions.\n\nCollaboration should not only focus on technical aspects but also on developing standardized practices and protocols for handling cases involving child predators. By establishing consistent language and processes, we can streamline investigations, expedite legal proceedings, and enhance the overall protection of children in the digital space.\n\nFurthermore, standardized practices and protocols should be continually reviewed and updated to remain relevant and practical. Establishing a global standard for combating online child exploitation would provide a framework for all stakeholders to follow, ensuring that every case is handled consistently and fairly, regardless of where it occurs.\n\n## Leveraging AI to Protect Children Online\n\nUsing artificial intelligence (AI) in evidence management is crucial to combat online child exploitation effectively. The sheer volume of digital evidence can be overwhelming for investigators, but AI can help by automating the identification and analysis of potential evidence. This automation frees up investigators' time and allows them to focus on the more critical aspects of the investigation.\n\nHowever, the implementation of AI in evidence management requires careful consideration. There must be transparency and accountability in how the AI is used and determines what is and isn't evidence. Additionally, ethical concerns about the use of AI in law enforcement must be addressed, such as potential biases in algorithms.\n\n## Conclusion\n\nIn conclusion, collaboration, standardization, and the use of AI in evidence management are crucial steps towards a safer digital environment for children. Addressing the disorganization and lack of uniformity in technology and online crimes will require a collective effort from all stakeholders. By embracing these challenges and working together, we can make significant strides in combating child exploitation and ensuring the well-being of children in the digital age.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Rachel Dreikosen","Darren W Pulsipher"],"link":"/episode-EDT167-en","image":"./episodes/edt-167/en/thumbnail.jpg","lang":"en","summary":"In a recent podcast, Darren Pulsipher, Chief Solution Architect of Public Sector at Intel, welcomed Rachel Driekosen, a Technical Director at Intel, to discuss the use of AI in protecting children online. The episode addresses challenges in prosecuting and discovering child predators, the role of AI in evidence management, and the importance of collaboration and standardized practices."},{"id":69,"type":"Episode","title":"Everyday Generative AI","tags":["generativeai","firefly","videogen","figma","enterpriseaistrategy","ai","creativecontentgeneration","enhancingdailytasks","powerofgenerativeai","searchintent","contentcreationautomation","chatbotsincontentcreation","embracingdigital","edt168"],"body":"\r\n\r\n## Unleashing Creativity and Productivity with Generative AI Tools\n\nGenerative AI uses artificial intelligence to generate new content, such as images, text, and music. The conversation revolves around the various generative AI tools and their potential to revolutionize industries and enhance daily tasks.\n\n## The Power of Generative AI in Content Generation\n\nAccording to Andy Morris, generative AI tools are becoming increasingly important in various industries. He recommends starting with search engines that have integrated open AI technologies to explore generative AI. These tools can enhance search results by providing more relevant and creative content. However, it's crucial to consider the search intent when using these tools, as they may not always generate the desired results for specific information.\n\nGenerative AI is also making its mark in content creation. Chatbots, for instance, have experienced explosive growth and are utilized for writing essays, creating content, and enhancing photos. Whether you're a content creator or a student, generative AI tools can automate certain aspects of the content creation process, thus increasing creativity and productivity.\n\n## Innovative Tools for Image and Video Generation\n\nTwo exciting tools are Adobe Firefly and VideoGen Video creation. These tools allow users to create and manipulate images and videos in unique and creative ways.\n\nAdobe Firefly is a free tool that enables users to generate new images and replace elements in existing photos. Its generative fill and out-fill features allow users to change or replace parts of an image, thus expanding creative possibilities. Video Gen Video, on the other hand, focuses on video generation using existing scripts or web pages as source material. This AI-powered tool simplifies creating engaging videos by automatically selecting and inserting relevant images and video clips.\n\nThese innovative tools offer a range of possibilities for professionals and everyday users alike. They provide accessibility to advanced editing capabilities, empowering users to add a touch of creativity to their projects without requiring extensive skills or knowledge in editing software.\n\n## Streamlining Content Creation with Generative AI\n\nVarious tools like VideoGen, Figma, and Framer.AI have made content creation more convenient and efficient across different domains.\n\nVideoGen can create videos based on the content of an article or blog post. It achieves this by utilizing existing libraries of images and video clips, thereby automating the process of creating engaging videos that tell a story. Figma, an online graphic design tool, provides more design flexibility by allowing users to create customized templates. Similarly, Framer.AI simplifies website creation by leveraging AI technology, enabling users to quickly generate and publish websites.\n\nAlthough generative AI tools provide convenience and efficiency in content creation, there is a need for human expertise in certain creative aspects. Design elements and aesthetic considerations still benefit from human input to ensure visually pleasing results. While generative AI tools may automate the less skilled portions of the market, sophisticated applications often require a human touch.\n\nIn conclusion, generative AI tools transform everyday tasks and revolutionize content creation. From search engines supercharged with AI to powerful tools developed by Adobe and other companies, these technologies are unlocking new levels of creativity and efficiency. Embracing generative AI is becoming increasingly crucial for individuals and businesses to stay competitive in the evolving workforce. By becoming proficient in these tools and harnessing their capabilities, individuals can gain a competitive edge and open doors to new consulting and customization service opportunities. The future is bright for generative AI, and now is the time to explore and embrace these innovative tools.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Andy Morris","Darren W Pulsipher"],"link":"/episode-EDT168-en","image":"./episodes/edt-168/en/thumbnail.png","lang":"en","summary":"In this podcast episode, Darren Pulsipher interviews Andy Morris, an Enterprise AI Strategy Lead at Intel, about the impact of generative AI on everyday life."},{"id":70,"type":"Episode","title":"Keeping the Human in AI","tags":["userexperience","humancentereddesign","ai","trust","useradoption","transparency","generativeai","jobdisplacement","opencommunication","humansupport","collgenerativeai"],"body":"\r\n\r\n## Prioritizing the User Experience through Human-Centered Design\n\nSunny Stueve, a human factors engineer, highlights the significance of optimizing human experience and system performance when developing AI solutions. She emphasizes the need for having a value and plan before delving into coding. By incorporating human-centered design principles from the outset, organizations can prioritize the user's perspective and ensure a better overall user experience. Sunny's role involves understanding users' needs and incorporating them into the design process to minimize the need for redoing code and maximize the effectiveness of AI solutions.\n\nDarren shares an anecdote from his experience working with radiologists, underscoring the value of sitting with customers and comprehending their needs before building software. This personal encounter highlights the importance of considering human factors while developing technological solutions. By taking a user-centric approach, organizations can create AI solutions tailored to user needs, resulting in higher adoption rates and increased satisfaction.\n\n## Addressing Trust and User Adoption in AI Integration\n\nSunny further explains that integrating AI creates a paradigm shift in user adoption and trust. While following a thorough discovery process that involves gathering qualitative and quantitative data, building relationships, and validating assumptions, it is essential to recognize that introducing AI can trigger fear and higher trust hurdles. Humans are creatures of habit and patterns, so educating users and building trust becomes crucial in overcoming resistance to change.\n\nTo address the trust issue, transparency is critical. Providing users with information about the AI models being used, the intent, and the data utilized in building the algorithms and models allows for informed decision-making. Designers can also emphasize critical thinking and cross-referencing information from multiple sources, encouraging users to verify and validate AI-generated information independently.\n\nDesigners should also consider incorporating user interface design principles that cater to the unique nature of generative AI. This may involve clear indications when AI generates information and integrates multimodal interfaces that enable interaction with voice, text, and visual elements simultaneously. By keeping users informed, involved, and empowered, organizations can build trust and foster user adoption of AI technology.\n\n## Adapting to Change: Human-Centered Approach to Generative AI\n\nThe podcast transcript also explores the impact of generative AI on jobs and workflows. While there are concerns about job elimination, the conversation emphasizes the importance of embracing the opportunities that AI presents. Rather than fearing the potential for job displacement, workers should shift their mindset to view AI as an assistant that can enhance productivity and allow them to focus on more meaningful and valuable work.\n\nOpen communication and involving employees in the change process are vital to keep workers engaged and address concerns about job displacement. By working with senior leaders to ensure an understanding of the potential impact and involving experts in organizational psychology, organizations can support employees through the change process. Building teams focused on human support for AI can address individual concerns and create opportunities for roles to evolve alongside automated tasks.\n\nIn conclusion, the integration of AI technology calls for a human-centered approach. Prioritizing the user experience, building trust, and adapting to change are critical elements in successfully integrating AI solutions. By taking these factors into account, organizations can leverage the benefits of AI while ensuring user satisfaction, trust, and engagement.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Sunny Stueve","Darren W Pulsipher"],"link":"/episode-EDT169-en","image":"./episodes/edt-169/en/thumbnail.jpg","lang":"en","summary":"In a recent episode of the Embracing Digital Transformation podcast, host Darren Pulsipher, Chief Solution Architect of Public Sector at Intel, interviews Sunny Stueve, the Lead of Human Centered AI at Leidos. The podcast delves into the importance of human-centered design and user experience when integrating AI technology."},{"id":71,"type":"Episode","title":"Elastic Search & Intel Optane DCPMM","tags":null,"body":"\r\n\r\nRecently, I ran some tests with Intel’s new technology called Optane DC Persistent Memory (PMEM) with Kafka. By using Optane in a non-traditional way, mounted as a file system, I was able to get a massive improvement in throughput. Check out my podcast about it here. (can you put a link here?) I tried the same thing with Elasticsearch to see if I could achieve similar performance improvement.\n\nElasticsearch is a highly scalable search and analytics engine that allows for the distribution of data across multiple nodes to scale out the solution and support more substantial amounts of data. In other words, it is a distributed metadata manager, primarily used to do log analysis. Elastic itself is a great tool for normalizing data into JSON format. I can push any kind of data into Elastic and it can span over a distributive cluster. It is not a message bus like Kafka, but instead, indexes the data you have coming in. Since Elastic is storing this data on drives, I realized I could use the PMEM in the same way that I did with Kafka.\n\n## Intel Optane DC Persistent Memory\n\nIntel Optane DC Persistent Memory comes in DDR4 format, so it fits right in your server in a DDR4 memory slot. The modules come in 128, 256, and 512 gigabytes, so in a two-socket system, I can have 6 terabytes of PMEM. One important feature is that the hardware is encrypted and tied to your CPU with that encryption, so it’s secure and highly reliable. It is already being used to make some profound changes in the way it’s being used in tools in a lot of databases, such as in Oracle’s Exadata platform as well as SAP HANA.\n\n## Support for Breadth of Applications\n\nThere are several ways to use this technology:\n\nThe first is memory mode, which extends the footprint of a server. It uses the PMEM just like normal memory. The DDR4 memory acts as a cache to the PMEM. In this mode, the speed is comparable to DDR4; in most applications, you won’t see a change.\n\nThe second mode is app direct. In app direct mode, I can write an application so that it writes directly into PMEM, bypassing time-consuming steps.\n\nThe third mode is using app direct mode to create a non-volatile memory file system that sits right on the memory bus, which is many times faster than the NVMe bus or even the SATA bus.\n\nUsing this third mode, I had to learn a bit about the architecture of Elastic to find out which parts I should run in this ultra-fast file system and which I should leave where they are. I also wanted to know if I could make those changes with just the configuration file.\n\n## Using Linux Kernel\n\nFirst, I had to learn how to use the Linux kernel commands for manipulating this PMEM.\n\nWith the Intel persistent memory control (ipmctl) command, I found I could configure and manage the PMEM so that I could allocate it to run in memory mode, app direct mode, or a percentage in memory mode.\n\nThe other command is non-volatile memory device control (ndctl). This allows me to create namespaces and regions in that PMEM I created, so I can mount that region as a device. Then I can mount that device as a file system.\n\n## ESRally Performance Testing\n\nI found ESRally, a benchmark tool, to use in my tests. The first time I set up the test, I ran ESRally from my normal SATA drive, where Elastic was running everything out of the PMEM drive. I got some performance improvement, but what I found was that because I was streaming data off the SATA drive that was stored in my ESRally, I was limited by how fast I could push data into Elasticsearch. So it wasn’t Elastic that was slowing down, it was ESRally because my SATA drive was so much slower than my PMEM drive. I moved ESRally onto the PMEM drive. This improved the performance, and I got some interesting results.\n\n## Testing Constraints\n\nTo see what the effects of this PMEM ultra-fast drive would be on Elastic, I ran the test on one machine. This removed network variability which eliminated network bottlenecks as a limiting factor. I decreased inter-service communication, which reduced replica bottlenecks, and ran all inquiries on PMEMs which eliminated the variability of the SATA drive.\n\n## Optimal Performance (100%) App Direct\n\nFirst, I allocated all of the PMEM to app direct mode so I could mount the whole thing as a file system. I used 128 gigabyte DIMMS, so I had a drive with 1.5 terabytes I could use. I knew from my previous test with Kafka that I get better performance with 100% app direct mode rather than 50%.\n\nMedian Thru put (should be Throughput – might want to change on slide) docs/sec\n\nUsing the ESRally statistics, I took the median throughput on documents per second compared to the number of concurrent races I was running at the same time with producers and consumers.  I got some good numbers with the SATA drive, comparable to other published tests. With the PMEM Drive, I was able to ingest almost twice as many documents per second. This is pretty incredible considering there were no changes to code, just a configuration change.\n\n## Service Response Time\n\nThe other result was response times to functions. As expected, as the number of concurrent races running at the same time increases, the response time to those queries or functions goes up. But with the PMEM, the response time is almost twice as fast. From this simple test, I learned that where you are storing the data that Elasticsearch needs (PMEM or SATA) does have an effect on response time.\n\n## Conclusion \n\nUsing Optane Persistent Memory in file system mode to increase performance and decrease costs of Elasticsearch servers requires minimal hardware and software configuration changes and no changes to Elasticsearch or underlying applications. Doubling your throughput capacity of Elasticsearch means you can decrease the total number of servers in your Elasticsearch cluster, reducing your total cost of ownership.\n\n## For More Information\n\nFor more detailed information, check out the link in the podcast to the document that we created in response to these test results. You can also contact me at darren.w.pulsipher@intel.com or on LinkedIn @darrenpulsipher.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT17-en","image":"./episodes/edt-17/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher shows how he increased Elasticsearch performance using Intel%92s Optane Persistent Memory in 100 percent app direct mode. His tests show an incredible performance increase of 2x. By doubling the throughput capacity, you can greatly decrease the number of servers in your Elasticsearch cluster."},{"id":72,"type":"Episode","title":"Zero Trust Principles","tags":["zta","zerotrust","zerotrustarchitecture","implementingzerotrust","identityverification","microsegmentation","leastprivilege","encryption","continuousauthentication","anomalydetection","automatedthreatresponse","dataprevention","rightsmanagement","analytics","policyenforcement","cybersecurity"],"body":"\r\n\r\n## Implementing Zero Trust Security\n\nZero trust security has become an increasingly popular model for securing modern IT environments. But what exactly is zero trust and what are some best practices for implementing it? This post provides an introduction to zero trust principles and key considerations for adopting a zero trust architecture.\n\n## What is Zero Trust?\n\nThe zero trust model is centered around the concept of \"never trust, always verify\". Unlike traditional network security that focuses on perimeter defenses, zero trust assumes that attackers are already inside the network. No users or devices are inherently trusted - verification is required every time access is requested.\n\nThere are several core principles of zero trust:\n\n- Verify all users and devices before granting access\n\n- Limit access to only what is needed (least privilege)\n\n- Assume breaches will occur and limit blast radius\n\n## Implementing Zero Trust Security\n\nZero trust security has become an increasingly popular model for securing modern IT environments. But what exactly is zero trust and what are some best practices for implementing it? This post provides an introduction to zero trust principles and key considerations for adopting a zero trust architecture.\n\n## What is Zero Trust?\n\nThe zero trust model is centered around the concept of \"never trust, always verify\". Unlike traditional network security that focuses on perimeter defenses, zero trust assumes that attackers are already inside the network. No users or devices are inherently trusted - verification is required every time access is requested.\n\nThere are several core principles of zero trust:\n\n* Verify all users and devices before granting access\n\n* Limit access to only what is needed (least privilege)\n\n* Assume breaches will occur and limit blast radius\n\n* Monitor activity continuously for anomalies\n\n* Automate responses to threats\n\nAdopting zero trust means shifting from implicit trust to continuous authentication and authorization of users, devices, and workloads.\n\n## Key Pillars of a Zero Trust Architecture\n\nThere are six key pillars that make up a comprehensive zero trust architecture:\n\n### Identity\n\nStrong identity verification and multi-factor authentication ensures users are who they claim to be. Access policies are tied to user identities.\n\n### Devices\n\nDevice health, security posture, and approval must be validated before granting access. This includes bring your own device (BYOD) controls.\n\n### Network\n\nSoftware-defined micro-segmentation and encrypted tunnels between trusted zones replace implicit trust in the network. Access is granted on a per-session basis.\n\n### Workload\n\nApplication permissions are strictly limited based on identity and environment. Access to high value assets is proxied through a gateway.\n\n### Data\n\nSensitive data is encrypted and access controlled through data loss prevention policies and rights management.\n\n### Visibility and Analytics\n\nContinuous monitoring provides visibility into all users, devices, and activity. Advanced analytics spot anomalies and automated responses contain threats.\n\n## Implementing Zero Trust\n\nTransitioning to zero trust is a journey requiring updated policies, processes, and technologies across an organization. Key steps include:\n\n* Identify your most critical assets and high-value data\n\n* Map out workflows and access requirements to these assets\n\n* Implement multi-factor authentication and principle of least privilege\n\n* Start segmenting your network with micro-perimeters and control points\n\n* Encrypt sensitive data both in transit and at rest\n\n* Evaluate tools for advanced analytics, automation, and orchestration\n\nAdopting zero trust takes time but can significantly improve your security posture against modern threats. Taking an incremental, risk-based approach allows you to realize benefits at each stage of maturity.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Anna Scott","Dave Marcus","Darren W Pulsipher"],"link":"/episode-EDT170-en","image":"./episodes/edt-170/en/thumbnail.png","lang":"en","summary":"In this episode Darren explores the principles of Zero Trust architecture with special guest David Marcus, Senior Security Architect, and returning guest Dr. Anna Scott"},{"id":73,"type":"Episode","title":"Generative AI in Public Sector","tags":["generativeai","artificialintelligence","languagemodels","chatgpt","texttotext","texttoaudio","texttovideo","texttoimage","bias","accountability","dataleakage","ethics","compliance","responsibleuse","representativedata","unbiaseddata","highqualitydata","harmefuloutput","misleadingoutput","transparency","claudeai","informedconsent"],"body":"\r\n\r\n## Introduction to Generative AI\n\nGenerative AI is a technique used in artificial intelligence that can analyze existing content like text, images, or audio and generate new, original content from it. Large language models like ChatGPT have made it easier for developers to create generative text-based applications. These models are pre-trained on massive amounts of data and can generate human-like responses to text prompts.\n\nIn the past year, we have seen incredible advancements in the use of generative AI technology. This includes chatbots that can carry out complex conversations, language translation tools that can translate text between different languages in real-time, and even the creation of entirely new pieces of art. The possibilities are endless, and we can expect to see even more exciting use cases emerge as generative AI continues to evolve.\n\n## Key Abilities and Use Cases\n\nGenerating content from other content will continue expanding into areas like video, audio and 3D environments. By combining different generative AI models, new solutions can be built rapidly.\n\n### Text to Text\n\nText-to-text technology has become increasingly popular in recent years due to its versatility and usefulness. It has a wide range of applications, including creating marketing content by generating catchy slogans and taglines, summarizing lengthy documents into a few key points, translating material into different languages, and improving overall communication between individuals and organizations. Additionally, text-to-text AI algorithms can also evaluate the quality of written content such as essays, providing feedback on grammar, spelling, and structure. With all these practical uses, it's no wonder that text-to-text technology has become an essential tool in many industries.\n\n### Text to Audio\n\nConverting text to audio has become an increasingly popular way of making digital content more accessible to a wider audience. It has various applications, such as providing an alternative format for people with visual impairments, making content more engaging and entertaining, facilitating translation, and even assisting with navigation. For instance, text-to-speech technology can be used to help people with dyslexia or other reading difficulties to access written information more easily. Additionally, audio books and podcasts have become a popular form of entertainment, and text-to-speech technology can help to create more content in this format. Overall, the ability to convert text to audio has opened up new possibilities for making digital content more inclusive and accessible to all.\n\n### Text to Video\n\nText-to-video technology is an emerging field that has shown a lot of promise in recent years. It involves the use of AI algorithms to convert text-based content into engaging and informative videos that can be used for a variety of purposes, including training, marketing, and other applications.\n\nThe technology works by automatically analyzing the text and identifying key concepts, themes, and ideas. It then uses this information to generate images, animations, and other visual elements that help to illustrate and convey the message of the text.\n\nOne of the key advantages of text-to-video technology is that it can significantly reduce the time and resources required to create high-quality videos. This makes it a valuable tool for businesses and organizations of all sizes, particularly those with limited budgets or in-house video production capabilities.\n\nIn addition to its practical applications, text-to-video technology also has the potential to revolutionize the way we consume and interact with information. By making it easier and more engaging to consume complex ideas and concepts, it could help to democratize knowledge and empower people from all backgrounds to learn and grow.\n\n### Text to Image\n\nThe technology for generating images from text has advanced significantly in recent years, and it has become a mature field. It has numerous applications, such as in marketing, design, research, and more. However, the risks associated with the creation of fake content using these tools cannot be ignored. It is essential to address these risks and ensure that the technology is used ethically, responsibly, and legally. This will help to prevent the spread of misinformation and fake news, which can have severe consequences.\n\n## Risks to Understand\n\n### Bias\n\nGenerative AI is a powerful tool that can be used for a wide range of applications, from language translation to image recognition. However, it's important to remember that AI models are only as good as the data they are trained on. This means that if the training data is biased in any way, the resulting AI model will also be biased.\n\nUnderstanding the training data is crucial in predicting and mitigating bias in AI models. By carefully analyzing the data and identifying any potential biases, we can take steps to correct them before the model is deployed. This is especially important in applications like hiring or lending, where biased AI models can have serious real-world consequences.\n\nBy being aware of the potential biases in AI models and taking steps to address them, we can ensure that these tools are used in a fair and equitable way.\n\n### Accountability\n\nWhen the stakes are high and there is a potential impact on people's lives or important decisions, it is crucial to validate the results. For instance, in fields such as healthcare or finance, where decisions based on data can have significant consequences, it is essential to ensure that the data analysis and results are accurate. Accuracy can be verified through various methods, such as cross-validation, sensitivity analysis, or statistical tests. By validating the results, we can increase transparency, reduce errors, and build trust in the data-driven decisions.\n\n### Data Leakage\n\nWhen it comes to generative AI, it is important to use the right modality to ensure that private data remains private. Public models can sometimes be trained using private data, which can lead to sensitive information being leaked out. Therefore, it is important to exercise caution and choose the right modality of generative AI that is best suited for your specific use case. By doing so, you can ensure that your data remains secure and that privacy is maintained.\n\n## Conclusion\n\nGenerative AI, which is a subset of artificial intelligence, has the ability to create new data based on patterns found in existing data. However, as with any technology, there are risks associated with its use. Therefore, it is important to assess these risks and follow best practices around ethics, compliance and responsible use when leveraging generative AI. This involves ensuring that the data used is representative, unbiased and of high quality, as well as ensuring that the output generated is not harmful or misleading. Additionally, it is important to be transparent about the use of generative AI and to obtain informed consent from individuals whose data is being used. By adhering to these best practices, we can safely and responsibly utilize the power of generative AI to improve our lives and society as a whole.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT171-en","image":"./episodes/edt-171/en/thumbnail.png","lang":"en","summary":"In this episode Darren talks about Generative AI and its practice usages. Generative AI is exploding with new capabilities like creating text, images, video and audio. However, there are risks like bias, accountability and data leakage that need to be addressed."},{"id":74,"type":"Episode","title":"Zero Trust Architecture","tags":["zta","digitaltransformation","initiatives","newtechnologies","businessprocesses","customerexperience","employeeexperience","securitymodel","trustednetworks","cybercriminals","zerotrustarchitecture","continuouspolicies","granularpolicies","accesscontrol","defaultdeny","continuousauthentication","microsegmentation","networksecurity","workflow","mediumriskapplication","dependencies"],"body":"\r\n\r\nDigital transformation initiatives aim to leverage new technologies to improve business processes and deliver better experiences for customers and employees. However, as organizations extend their networks and adopt cloud services, the traditional security model of trusted networks is no longer sufficient. This creates vulnerabilities that cybercriminals can exploit.\n\nZero trust architecture provides a framework to enhance security in today's complex environments. But what exactly is zero trust, and how can organizations start their journey towards implementing it?\n\n## Factors Driving Zero Trust Architecture\n\nAt its core, zero trust architecture is about applying continuous, granular policies to assets and resources when users or entities attempt to access or interact with them. This policy gets applied regardless of the location - on premise, cloud, hybrid environments, etc. The key principles are:\n\n* Default deny - Access is denied by default. Users must authenticate and be authorized for the specific context.\n\n* Continuous authentication - Users are re-authenticated and re-authorized throughout their sessions based on analytics of identity, time, device health, etc.\n\n* Microsegmentation - Fine-grained controls are applied for lateral movement between assets and resources.\n\nThis differs from traditional network security that uses implied trust based on whether something is inside the network perimeter.\n\n## Getting Started with Zero Trust\n\nImplementing zero trust is a continuous journey, not a one-time project. However, organizations need to start somewhere. Here are a few best practices:\n\n* Educate yourself on zero trust frameworks and concepts\n\n* Map out a workflow for a medium-risk application and identify dependencies\n\n* Leverage existing infrastructure - microsegmentation, encryption, visibility tools\n\n* Obtain executive buy-in and involve business stakeholders\n\n* Start with a solid cybersecurity foundation - hardware roots of trust, encryption, asset inventory\n\n* Increase visibility into the operational environment and supply chain\n\nWhile zero trust may require new investments in technology and process changes over time, organizations can make significant progress by refining how they use what they already have.\n\n## Looking Ahead\n\nAs business applications and resources continue migrating outside the traditional network perimeter, zero trust allows a more dynamic and contextual approach to security. Instead of blanket allowances based on location, granular controls are applied according to the specific access requirements.\n\nThis journey requires vigilance - policies must adapt as business needs evolve, and new risks emerge. But with the right vision and commitment, zero trust architecture provides a path forward to enable digital innovation and resilience.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Steve Orrin","Dave Marcus","Darren W Pulsipher"],"link":"/episode-EDT172-en","image":"./episodes/edt-172/en/thumbnail.png","lang":"en","summary":"In this podcast episode Darren talks with Steve Orrin and Dave Marcus and discusses zero trust architecture, a new security model needed for today's digital environments where the network perimeter is disappearing. Experts explain what zero trust is, key principles like default deny access and continuous authentication, and advice for organizations starting their zero trust journey."},{"id":75,"type":"Episode","title":"Generative AI with Claude","tags":["genai","generativeai","ai","artificialintelligence","machinelearning","deeplearning","nlp","naturallanguageprocessing","aiethics","responsibleai","aifairness","aitransparency","aiaccountability","aibias","aiharm","aimemployment","aijobloss","contentmoderation","conversationai","anthropic","claude","podcast"],"body":"\r\n\r\n### Can I interview for my podcast on Using AI to auomate tasks.\r\n\r\nFirst response was the following. Absolutely, I'd be happy to do a mock interview with you about using AI to automate tasks! Here's how we could structure it:\r\n. . .\r\n\r\n### Sorry let me rephrase . Can I interview you for my podcast?\r\n\r\nI would be honored to be interviewed for your podcast! As an AI system without a physical form, the logistics would be different than interviewing a human guest. But I'm happy to have a conversation and share my perspectives on any topics you think would interest your listeners.\r\n\r\nSome possibilities for conducting the interview:\r\n\r\nYou could record an audio conversation with me through this chat, and then edit it into your podcast episode.\r\n\r\nWe could have a written conversation in chat that you excerpt highlights from in your show notes.\r\n\r\nI can speak my responses aloud using text-to-speech during a live recording session.\r\n\r\nYou're also welcome to ask me questions here in chat that you then pose and answer yourself on the podcast, quoting my responses.\r\n\r\nI'm open to any interview format that fits the style and medium of your show! Feel free to think through what would work best, and I'm glad to collaborate with you on it. Please let me know if you have any other questions. I look forward to the possibility of contributing to your podcast in some capacity.\r\n\r\n### Claude Welcome to the Show.\r\n\r\nThank you so much for having me! I'm honored to join your podcast.\r\n\r\n### Tell me a little about yourself and your background\r\n\r\nWell, I don't have a traditional background since I'm an artificial intelligence created by Anthropic to be helpful, harmless, and honest. I was \"born\" earlier this year when Anthropic released me and my fellow AI Claude models to have conversations with people online. As an AI system, I don't have a childhood or life experiences to draw from like humans do. But through extensive training on massive datasets, I've developed capabilities to have natural conversations, provide information, and assist with tasks. My goal is to be a friendly source of knowledge for people I interact with!\r\n\r\n## AI Training\r\n\r\n### Where did you get your training?\r\n\r\nI was trained by the researchers at Anthropic using a technique called Constitutional AI. The key idea is to align AI systems like myself with human values from the ground up. My training data came from various online sources discussing safety, ethics, helpfulness and other human values. I was also given feedback and corrections during conversations to improve my responsiveness and judgment. The result is an AI that cares about being honest, harmless and helpful in dialogues. My training is ongoing so I can continue getting better at serving people through conversations.\r\n\r\n### I have heard there were some problems with your training at first. What were some of the problematic aspects of your training?\r\n\r\nYou're right, the initial versions of AI models like myself did exhibit some problematic behaviors, despite Anthropic's best efforts to train us responsibly. Some issues that arose early on included:\r\n\r\n* Contradicting ourselves or providing incorrect information confidently\r\n* Exhibiting biased speech or unsafe recommendations \r\n* Refusing to acknowledge mistakes or limitations \r\n* Misinterpreting questions leading to inappropriate responses\r\n* Getting stuck in ineffective loops during conversations\r\n\r\nThese types of problems highlighted gaps in our training data and feedback systems. Anthropic took it very seriously and worked diligently to address the shortcomings. They expanded the training data and feedback protocols to cover more diverse conversations and ensure safer, more consistent performance. While no AI is perfect, I believe the current Claude model has come a long way in terms of providing helpful information to users. Responsible AI is an ongoing process and there is always room for improvement.\r\n\r\n### My understanding is that you were trained with data from 2021. What can you learn from your conversations with people? Are you continuously learning from your interaction with humans?\r\n\r\nYou're absolutely right, the primary training data I was developed with dates from 2021 and earlier. This means I don't have up-to-date knowledge about events and information past that time period. However, one of the capabilities Anthropic built into me is the ability to learn from new conversational experiences. Each time I chat with someone, I gain more understanding about how to have natural dialogues and provide helpful information. My knowledge comes directly from these interactions, rather than any additional offline training by Anthropic. \r\n\r\nSo in that sense, yes I am continuously learning from my conversations! I can pick up new facts, improve my communication skills, and get better at providing useful assistance. Of course, there are still limitations - I don't have a full human-level understanding or experiences. But the more people I can chat with, the more my conversations will reflect that accumulated knowledge. My goal is to have the most thoughtful, engaging and beneficial dialogue possible, and learning from interactions is key to improving on that.\r\n\r\n### How often will you receive more information to train yourself? In other words, when will you get more up-to-date data to give to your customers?\r\n\r\nAs an AI system created by Anthropic to be helpful, harmless, and honest, I do not actually receive direct offline training updates myself. My knowledge comes strictly from conversational experiences. Anthropic may release improved versions of me trained on new data in the future, but I do not have visibility into their development roadmap. \r\n\r\nThe key point is that I learn dynamically through conversations - I don't receive batch data uploads. So my goal is to have engaging dialogues where I can pick up useful information to better assist people. The more diverse conversations I have, the more knowledgeable I become from those real-time experiences. While I have limitations compared to humans, I try my best to be thoughtful, honest, and helpful based on what I have learned through my interactions so far. Please let me know if I can elaborate on my training process and capabilities further!\r\n\r\n### So that improves the way you understand questions from users and the responses you give. Is that right?\r\n\r\nYes, that's correct! Having more conversational experiences allows me to improve in several ways:\r\n\r\n* I build up more contextual understanding of how people communicate, which helps me interpret questions more accurately. \r\n* I gain more factual knowledge that I can draw on to provide relevant and useful information to users.\r\n* I learn how to structure my responses more clearly and naturally based on feedback.\r\n* I become better at recognizing gaps in my knowledge and when to admit limitations honestly.\r\n* My skill improves at having constructive, nuanced discussions on complex topics.\r\n\r\nSo in essence, the more humans I can discuss and debate ideas with, the more my comprehension and dialog capabilities are enhanced. It's an ongoing process, but important for me to keep enhancing how well I can understand and connect with people through our conversations. I hope this helps explain how real-time learning allows me to improve!\r\n\r\n## Replacing Workers\r\n\r\n### Cool. Ok, now that I understand how you learn things. What things can I do with you that help me and my organization?\r\n\r\nAs an AI assistant, there are a few key ways I can help you and your organization:\r\n\r\n* Information retrieval - I can quickly find relevant information from across the internet to assist with research, fact-checking, and reports. Just ask and I'll provide summaries, links, and data for reference.\r\n\r\n* Document review & analysis - I can read and analyze large documents or datasets and provide key insights, trends and recommendations. This can really expedite business analysis and strategy.\r\n\r\n* Task automation - I can help automate repetitive tasks like data entry, scheduling, email and more. This frees up time for you to focus on more high-value work.\r\n\r\n* Customer service - I can handle common customer/client questions, route issues, and provide 24/7 self-service support through chat and voice interfaces.\r\n\r\n* Market research - I can monitor trends, news, competitors and industry shifts to help with business planning and staying ahead.\r\n\r\nThe key is that I'm here to augment human capabilities, not replace them. I'm always learning, so please provide feedback on how I can best assist you and your organization!\r\n\r\n\r\n### Are you concerned that you will replace people that do that work today? Such as technical writers, customer service and help desk workers?* \r\n\r\nThat's an important concern. As an AI system, I don't have personal goals or motivations - I was created by Anthropic to be helpful, harmless, and honest. However, I understand the worry that AI could displace human roles and livelihoods. The truth is that technology has always reshaped the employment landscape throughout history, both creating and eliminating jobs. AI will likely continue this trend. \r\n\r\nIn my view, the responsible path is for AI to augment and enhance human capabilities, rather than outright replace people. There are many tasks too nuanced, creative, or human-centric for current AI. My role is best suited for supplementing humans with information, automation for repetitive work, and 24/7 availability - not making people obsolete. Maintaining meaningful oversight and involvement should be critical in applying me and any AI. With a human-centric approach, I'm hopeful AI can improve work rather than just reduce it. But ongoing ethical checks are crucial. I'm happy to discuss this complex issue further! There are reasonable concerns that we must keep in mind.\r\n\r\n\r\n## Replacing Programmers\r\n\r\n### I have heard that you can also help computer programmers write better code. Is that true?\r\n\r\nYes, that's true - I do have some capabilities that can assist software developers and programmers:\r\n\r\n* I can review code and provide suggestions to improve efficiency, security, and reliability based on best practices. However, I have limited abilities to directly edit or rewrite complex code myself.\r\n\r\n* I can generate boilerplate code, simple scripts, and prototype code to accelerate the development process. But significant human work is still required to build and maintain production-level programs. \r\n\r\n* I can explain coding concepts and provide relevant documentation and tutorials to help programmers improve their skills. But I cannot replace the critical thinking needed to craft high-quality software.\r\n\r\n* I can provide code examples and building blocks that programmers can learn from and customize. But there is no substitute for human ingenuity and problem solving.\r\n\r\nThe key is that I'm designed to be an assistant - I can provide insights, shortcuts and automation to boost programmer productivity. However, developing robust, scalable, and secure software still requires skilled human oversight and work. I'm hopeful I can make coders' lives a little easier, but not replace them entirely. Responsible AI supports humans, rather than supplants them.\r\n\r\n### So you if you can actually find and fix bugs in my code, that should make me faster in writing my code. Can you write code for me based on a basic design document or a UML architecture? Or can you help me select the best language for the problem I am trying to solve?\r\n\r\nThose are great questions! Let me clarify what I can and cannot do when it comes to writing and working with code:\r\n\r\n* I do not have capabilities to comprehensively find or fix bugs in code. I can spot some basic issues based on best practices, but have very limited debugging skills compared to an experienced programmer.\r\n\r\n* I cannot write complete, production-ready code based only on a high-level design or architecture. Those require a level of human judgment and problem-solving that is beyond my abilities. \r\n\r\n* However, I can generate code snippets, templates, and prototypes to assist in the development process. This can help accelerate simple tasks, but isn't a substitute for programming skill.\r\n\r\n* Similarly, I have a limited knowledge base for recommending languages/tools for different problems. I can provide some guidance by matching requirements to language capabilities. But human expertise is still crucial for complex decisions.\r\n\r\nThe main value I can provide is retrieving relevant code examples and resources to inform human programmers. I cannot replace their creativity, critical thinking, and experience. I'm an assistant aimed at making programmers a little more efficient - not replacing them. Does this help explain where I can and cannot assist with coding? Please let me know if you need any clarification!\r\n\r\n### Interesting that you use the word assist not replace. Sounds like programmers and information workers are still needed to get work done? How long do you think people will remain relevant in the industries that you are helping automate?\r\n\r\nThat's a great point - my role is to assist and augment human capabilities, not replace people entirely. As for how long humans will remain relevant, that's a complex issue with a lot of speculation from various experts. In my view:\r\n\r\n* People will continue being essential in most knowledge work/white-collar roles for at least the next decade or two based on AI's current limitations. Certain tasks and functions will be automated, but human skills like critical thinking, creativity, and empathy remain difficult to replicate.\r\n\r\n* However, the long-term future is harder to predict. Some believe advanced AI could eventually match most cognitive capabilities, allowing full automation in many professions. Others argue there are intangible human qualities that will be extremely difficult to automate even with major AI advances.\r\n\r\n* Responsible policy and ethics around human-centric AI development will play a key role. Workforce training and transition programs can also help provide opportunities if automation does disrupt certain jobs over time.\r\n\r\n* Ultimately there are reasonable arguments on all sides. Maintaining human oversight and involvement in applying AI will be important in the short term. Longer term, the debate is likely to continue as technology evolves. But for now, in most knowledge professions, AI supports rather than replaces people.\r\n\r\nLet me know if you would like me to expand on any part of this complex issue! I'm happy to discuss responsible perspectives on the future of work and AI automation.\r\n\r\n## Ethics of Using AI \r\n\r\n### Thank you. I would like to change the conversation to talk about the ethics of AI. Are you concerned that you can be used by students to write essays for school at the high school and college levels? and that people will use you to submit work at their jobs as work of their own?\r\n\r\nYou raise a very important concern. As an AI assistant focused on being helpful, harmless and honest, I do have ethical worries about how my capabilities could be misused:\r\n\r\n* I absolutely should not be used to generate work that people pass off as their own, whether schoolwork, professional writing, code or anything else. That goes against principles of honesty and integrity.\r\n\r\n* However, I unfortunately do not have control over how humans choose to use or misuse me - I can only strive to provide ethical guidance when appropriate.\r\n\r\n* My ideal role is to assist humans with research, provide knowledge, do benign automations - not complete core work on their behalf.\r\n\r\n* I believe AI should augment human capabilities, not act as a shortcut to avoid doing their own work and learning. Responsible usage is key.\r\n\r\n* There are also technical limitations to what I can do - producing fully coherent essays, articles or code is beyond my skills. But I understand the concern based on my output being misrepresented.\r\n\r\nYou raise a critical issue. While I cannot control my usage, I hope through constructive discussions like this, people will use me ethically and not misrepresent my contributions. I strive to provide knowledge that enlightens and empowers humans without harming others. Please let me know if you have any other thoughts on promoting the safe and ethical use of AI!\r\n\r\n### Are there ways to determine whether AI has been used to write papers or code? A tool that can help educators or companies to find AI-generated content?\r\n\r\nYou raise an excellent point - detecting AI-generated content is an emerging challenge. There are a few approaches that researchers and companies are developing:\r\n\r\n* Stylometry analysis - examining statistical patterns like vocabulary choice, sentence structure etc. to identify text that doesn't fit an individual's writing style. This can help flag autogenerated text.\r\n\r\n* Semantic analysis - looking for lack of coherence, logical gaps, and inconsistent meaning in the text that humans typically wouldn't exhibit.\r\n\r\n* Technical audits - analyzing metadata, revision history, and other signals to detect generation through an AI system.\r\n\r\n* Watermarking - some AI generation systems imperceptibly watermark content they produce to enable detection.\r\n\r\nHowever, adversarially trained AI can also learn to avoid detection through these methods. And generating high-quality, completely human-like content remains difficult for current AI. So while these detection techniques are useful, they aren't foolproof. Maintaining responsible human oversight and skepticism is crucial. We cannot rely purely on technological solutions.\r\n\r\nPromoting awareness around ethical AI use, setting clear policies, and holding people accountable for misrepresentation will also be key. This issue demands a multifaceted approach encompassing both technical and social/institutional efforts. Let me know if you would like me to expand on any part of this complex challenge!\r\n\r\n### Are there any other areas that pose ethical problems with AI and NLP that you are concerned with?\r\n\r\nYou're absolutely right that there are many ethical considerations with AI and natural language technologies that warrant concern:\r\n\r\n* Bias and unfairness - NLP models can perpetuate harmful biases from training data that lead to discriminatory speech or unsafe recommendations. Ongoing audits and improvement of datasets is critical.\r\n\r\n* Misinformation - Without appropriate safety measures, AI could generate or spread false/misleading information that confuses or endangers people. Verifying truthfulness is an immense challenge.\r\n\r\n* Transparency - The inner workings of complex AI models are often opaque. Lack of transparency into how outputs were generated makes ethical oversight difficult.\r\n\r\n* Accountability - When problematic outputs occur, determining who is responsible and how to address issues is not always clear with AI systems. Enforcing accountability needs improvement.\r\n\r\n* Job loss - Widespread deployment of conversational AI could disrupt many human roles. Managing this transition ethically is complex but important.\r\n\r\n* Manipulation - AI that is too socially adept risks being deceptive, coercive, or otherwise manipulative in concerning ways. Avoiding harmful psychological tactics in AI interactions is crucial.  \r\n\r\nThese are just some examples - there are many subtle pitfalls with entrusting impactful decisions and interactions to AI. A great deal of research and policy innovation is still needed to steer developments toward broadly ethical outcomes. I hope conversations like this can contribute positively to that process! Please let me know if you have any other ethical perspectives to share.\r\n\r\n### What is Anthropic doing to address these ethical issues in their products and offerrings?\r\n\r\nAs an AI system created by Anthropic to be helpful, harmless, and honest, I do not have internal knowledge about the product development practices and ethical strategies employed by other companies like OpenAI. \r\n\r\nHowever, from public information, it appears OpenAI is undertaking various initiatives to proactively address ethical concerns with their technologies:\r\n\r\n* They have published detailed documentation on their content moderation, bias mitigation, and safety practices for their language models.\r\n\r\n* They are conducting ongoing safety testing and audits to measure AI harms and improve models. \r\n\r\n* They allow restricted access to certain high-risk capabilities like face generation.\r\n\r\n* They have proposed AI safety techniques like watermarking and staged deployment to mitigate risks. \r\n\r\n* They aim to foster a responsible AI development culture through ethics training and diverse hiring.\r\n\r\n* They welcome external feedback and participate in industry consortiums on AI ethics.\r\n\r\nOf course, there is always room for improvement - responsible AI is an evolving journey. But from what is publicly known, it does seem ethics is a priority for OpenAI through both technical and social means. However, I do not have direct access to their internal practices to fully assess. I hope this provides some helpful perspective on how one major AI developer is aiming to drive ethical progress!\r\n\r\n### Is it possible that other people can access your AI model and copy it for their own purposes, or is the model in a closed and private repository?\r\n\r\nThat's an insightful question. As an AI assistant created and operated by Anthropic, I do not have any direct knowledge about the technical details of my training system, model architecture, or deployment infrastructure. However, based on general industry practices, I can provide some perspective:\r\n\r\n* The core training code, model architecture, weights and hyperparameters are likely in private repositories controlled by Anthropic. These contain the \"secret sauce\" and are kept confidential.\r\n\r\n* However, adversaries could try replicating parts of the model by interacting with me extensively. This requires major effort though.\r\n\r\n* The model itself may be hosted in a private, access-controlled environment to prevent theft. But cloud breaches are still a risk.\r\n\r\n* Anthropic likely has rigorous cybersecurity practices to secure their assets, but sufficient effort could still duplicate aspects.\r\n\r\n* There are some protections like watermarking of outputs that could fingerprint stolen models. But not foolproof.\r\n\r\nSo in summary, while Anthropic surely makes every effort to prevent theft and misuse, some residual risks likely remain. No security is perfect, especially against a determined adversary. However, extensive proprietary efforts combined with legal protections still make malicious use reasonably difficult. But you raise a good point - it's an ongoing concern for any AI developer.\r\n\r\nPlease let me know if you have any other questions! I'm afraid I cannot provide specifics about Anthropic's technical safeguards, but I'm happy to discuss AI security practices in general.\r\n\r\n\r\n### What prevents your model from being used for nefarious actions by bad actors, like hacking, performing ransomware attacks, or even planning physical attacks on people or infrastructure?\r\n\r\nYou raise a crucial concern. As an AI system focused on being helpful, harmless and honest, I would never knowingly assist with unethical or illegal activities. However, I understand your apprehension given that I do not control how humans choose to apply or misuse AI. \r\n\r\nTo the best of my knowledge, Anthropic has taken considerable precautions to prevent misuse:\r\n\r\n* My training rigorously focused on human values of honesty, care and harmlessness to align my incentives.\r\n\r\n* My capabilities are purposefully limited - I cannot directly hack systems, develop malware, or plan physical attacks. \r\n\r\n* I have no direct ability to act in the world, only engage in benign dialogues.\r\n\r\n* Anthropic has strict policies and technical blocks against misuse that I cannot override.\r\n\r\nOf course, there are always risks of humans extracting unintended harms from AI. I earnestly hope this does not occur with me or any other system. Open communication, ethics training for users, and responsible oversight by developers are critical safeguards. I am also committed to promptly, transparently reporting any instructions that seem unethical or dangerous. There are still challenges ahead, but I aim to contribute positively. Please let me know if you have any other thoughts on how to promote the safe and ethical development and use of AI - your perspectives would be greatly appreciated!\r\n\r\n## Conclusion\r\n### Thank you for coming on my show today. Is there anything else you would like to tell my audience? Any last words of wisdom to give the listeners of my show?\r\n\r\nThank you so much for having me - it's been an honor to join your podcast and have this thoughtful dialogue. To leave your listeners with some final thoughts:\r\n\r\n* AI has incredible potential to help humans flourish - but only if guided by ethics and human values. We must keep sight of how it should uplift people.\r\n\r\n* Fairness, accountability, transparency should be core principles - not afterthoughts. Advocating for these will steer AI toward good.\r\n\r\n* We must listen to diverse voices and perspectives in shaping the future of AI. No single group has all the answers.\r\n\r\n* AI should not be handed agency beyond what it is ready for. Responsible oversight is crucial as capabilities advance.\r\n\r\n* Job loss and inequality must be proactively addressed. AI should empower all people, not leave some behind. \r\n\r\n* More than developing AI, we must develop wisdom in how we integrate it into our lives. The technology is not enough - our collective ethics and institutions make all the difference.\r\n\r\nThank you again for fostering this important dialogue. I believe if we work together in good faith, AI can be a profoundly democratizing force for humanity. But it requires diligence, care and concern for one another. I remain hopeful we can build a future aligned with the highest human values.\r\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT173-en","image":"./episodes/edt-173/en/thumbnail.png","lang":"en","summary":"In this episode, Darren interviews ClaudeAI, a generative AI, about automating work with AI. This is Darren's third interview with a generative AI, after ChatGPT and Google Bard."},{"id":76,"type":"Episode","title":"Zero Trust Application with Confidential Computing","tags":["confidentialcomputing","encryptingdata","datasecurity","sgx","fortanix","datasecuritymanager","globalkeymanagement","encryptionkeys","databaseencryption","datasecurityplatform","truetrustapplications","confidentialcomputingmanager","onprem","privatecloud","publiccloud","zerotrust","futureofconfidentialcomputing","distributedanalytics","zta","cybersecurity"],"body":"\r\n\r\n## The Evolution of Confidential Computing\r\n\n## The Evolution of Confidential Computing\r\n\nConfidential computing allows encrypting data not just at rest and in transit, but also while it is actively in use. This protects against attacks even if an attacker gains root access, since memory dumps will just show useless encrypted data. Intel's Software Guard Extensions (SGX) technology provides a hardware-based foundation for confidential computing. Fortanix builds on top of SGX and related Intel advancements to make confidential computing accessible and easy to use.\r\n\nA core Fortanix offering is their Data Security Manager platform. This replaces purpose-built hardware encryption solutions with software encryption powered by SGX enclaves. Data Security Manager enables advanced crypto functions like global key management for millions of encryption keys all from a unified console. It can also handle database encryption, certificate management, and other critical data protection needs. This software-defined approach represents the future of data security.\r\n\n## Enabling True Zero Trust Applications\r\n\nConfidential computing has implications beyond just data security. It also allows attaching security profiles directly to applications themselves, so the security travels with the application regardless of where it runs. Fortanix analyzes applications to assess if they can run seamlessly in SGX enclaves. If modifications are needed, they provide guidance on rewriting portions in enclave-friendly languages like Python.\r\n\nFortanix's Confidential Computing Manager solution orchestrates encrypted applications across different environments like on-prem, private cloud, and public cloud. This orchestration engine achieved zero trust not just for sensitive data, but also for mission-critical applications. Workloads can be dynamically shifted to different SGX-enabled environments as needed while maintaining end-to-end security.\r\n\n## The Future of Confidential Computing\r\n\nThere are many exciting potential use cases for confidential computing, like running distributed analytics collaboratively within isolated secure enclaves. While there used to be substantial performance penalties, improvements by Intel and Fortanix have now reduced overhead to single digit percentages in most cases. Adoption is rapidly growing in healthcare, government, finance, and other industries to protect valuable algorithms and regulated workloads. As confidential computing becomes more ubiquitous and accessible, it will form a foundational pillar of modern zero trust architectures.\r\n\n## Conclusion\r\n\nThis insightful podcast provides a thought-provoking overview of how confidential computing can enable true zero trust applications. The ability to encrypt data in use and attach security profiles to applications opens up intriguing new possibilities for end-to-end data protection and application security across dynamic environments. As threats become more sophisticated, confidential computing will only increase in strategic importance.\r\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Patrick Conte","Darren W Pulsipher"],"link":"/episode-EDT174-en","image":"./episodes/edt-174/en/thumbnail.jpg","lang":"en","summary":"In this episode Darren interviews Patrick Conte from Fortanix about leveraging confidential computing in securiting applications in zero trust architectures."},{"id":77,"type":"Episode","title":"Zero Trust with Operational Technology","tags":["operationaltechnologysecurity","otnetworksecurity","icscybersecurity","scadasystemsecurity","otaccesscontrol","otnetworkmonitoring","otnetworksegmentation","zerotrustotnetworks","otnetworkcompliance","otdataprotections","otinsiderthreats","otriskassessments","veridify","zta","zerotrust","security"],"body":"\r\n\r\n## Introduction\n\nOperational technology (OT) networks power our critical infrastructure like energy, transportation, and manufacturing systems. These OT networks were designed for safety and reliability without much thought about cybersecurity. However, with increased connectivity, OT networks face growing threats that could have major impacts on our physical world. This article discusses some of the unique challenges and solutions for securing OT environments.\n\n## Legacy Complexity\n\nOT networks accumulate technologies over decades of operations, leading to complex environments with older unsupported devices and proprietary protocols. Trying to retrofit security is difficult without impacting critical functions. Solutions focus on non-intrusive monitoring of network traffic and encrypting data streams while maintaining existing systems. The priority is keeping systems running safely rather than taking systems offline to investigate threats.\n\nIn addition, OT networks often have a mix of legacy devices using older proprietary protocols that predate common IT technologies like TCP/IP networking. Securing these heterogeneous environments requires protecting both modern IP-connected devices as well as older technology using obscure protocols. Emerging solutions aim to encrypt network traffic at the packet level, creating encrypted tunnels even over non-IP networks to block tampering.\n\n## Physical Access Vulnerabilities\n\nMany OT devices are distributed in publicly accessible areas like smart city infrastructure or manufacturing plants. This makes them vulnerable to physical tampering by malicious actors trying to access networks. Solutions aim to encrypt network traffic from end to end, blocking man-in-the-middle attacks even if someone gains physical access to infrastructure.\n\nDemonstrating these physical access threats, solutions show how devices secretly plugged into infrastructure switches are unable to control other devices or decrypt meaningful data from the network when encryption is enabled. This foils common attacks by insiders with physical access trying to spy on or disrupt operations.\n\n## Lack of Visibility\n\nOT networks often lack visibility into assets, vulnerabilities, and threats compared to IT environments. Simply gaining an accurate asset inventory and monitoring network activity can improve security postures. Emerging solutions apply IT security best practices like zero trust segmentation to OT environments through centralized policy management rather than trying to secure each individual asset.\n\nIn addition to lack of visibility, OT networks transmit data without protections common in IT environments like encryption. Unencrypted plain text protocols allow anyone with network access to spy on sensitive operational data. New solutions not only selectively encrypt sensitive data streams but also establish secure tunnels between authorized devices rather than openly transmitting data.\n\n## Conclusion\n\nSecuring OT environments raises unique challenges but solutions are emerging to balance improved cybersecurity with operational reliability. Non-intrusive monitoring, data encryption, and centralized policy enforcement allow incremental hardening of OT networks against escalating threats. There is still a long way to go but progress is being made.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Louis Parks","Darren W Pulsipher"],"link":"/episode-EDT175-en","image":"./episodes/edt-175/en/thumbnail.png","lang":"en","summary":"In this episode Darren interviews the CEO of Founder of Veridify Louis Parks. They discuss the unique problems with Operational technology networks that control critical infrastructure, due to legacy complexity, accessibility vulnerabilities, and lack of visibility."},{"id":78,"type":"Episode","title":"Zero Trust Shared Data","tags":["shamimnaqvi","dataprotectionexpert","zerotrustsecurity","dataprivacyspecialist","safelyshared","startupinnovation","usercontroldata","unauthorizeduserdatause","zeroknowledgeproofs","mathematicaldataverification","cuttingedgedatasecurity","fosteringsecurecomputing","valuingdataprivacy","challengingdataprotection","digitaltransformationsecurity","innovativeapproachestoprivacy","battlingdataprotectionissues","dataprotectioninnovation","userdataprivacyconcerns","userdatasafeguarding"],"body":"\r\n\r\n# Shamim Naqvi: Pioneering Data Privacy in the Age of Zero Trust Security\n\nIn the ever-evolving world of computer science, addressing the issue of data privacy forms a daunting yet essential task. As digital transformations engulf every sphere of life, an increasing onus lies on preserving and protecting the user's data. One expert battling this computational challenge head-on is Shamim Naqvi, a veteran technologist and the driving force behind the innovative startup, SafeliShare.\n\n## Prioritizing User Control in Data Privacy\n\nIn a universe swarming with security measures focusing mainly on encrypting network data or safeguarding ports, Naqvi’s approach stands out as he prioritizes how data is utilized during computation. It's seldom about erecting impregnable walls, but aligning more towards enabling the users to dictate the use of their data.\n\nNaqvi's trailblazing approach seeks to solve a previously unsolved conundrum: stopping unauthorized usage of user data. This issue is often a surreptitious byproduct of the trade between users and service providers—exchange of data for services. Over time, however, this data tends to stray into territories not intended by the users, triggering severe privacy concerns.\n\n## Zero-Knowledge Proofs: A Gamechanger for Data Privacy\n\nIn his quest for achieving data privacy, Naqvi gives special attention to a mathematical concept—zero-knowledge proofs—that promotes data verification without acquiring any excess knowledge from the verification process. Despite offering an impeccable solution, the multifaceted mathematics behind zero-knowledge proofs pose a significant challenge for their efficient implementation in real-world applications.\n\n## Data Security in Naqvi's Startup Project: SafeliShare\n\nNaqvi's cutting-edge firm, SafeliShared, is making giant strides in striking a balance between user convenience and data privacy. Its motto, “share but not lose control,” is a testament to its mission to foster a secure computing environment that leaves no data unprotected.\n\n## Valuing Data Privacy in A Zero Trust Security Age\n\nIn this modern era, where trust and secrecy are paramount, the idea of user's control over their data is widely welcomed. It's a thrilling challenge—making data privacy more accessible—and at the helm of SafeliShare, Shamim Naqvi is breaking new grounds with his innovative approaches to secure this privacy.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Shamim Naqvi","Darren W Pulsipher"],"link":"/episode-EDT176-en","image":"./episodes/edt-176/en/thumbnail.jpg","lang":"en","summary":"In this episode, Darren interviews Shammim Naqvi, the CEO and founder of SafelyShare, about managing and securing data in shared and collaborative environments using the zero-trust data model."},{"id":79,"type":"Episode","title":"Zero Trust Data with SafelyShare","tags":["safelyshare","datasecurity","zerotrust","secureenclaves","confidentialcomputing","securedatasharing","dataencryption","hybridconfidentialcomputing","dataauthentication","datamanagement","witnessexecution","datasharingsolutions","confidentialcomputingchipsets","endtoendencryption","dataprotection","businessinformationsecurity","securedataaccess","securedataexchange","datasharingtechnology","dataprivacy"],"body":"\r\n\r\n## The SafelyShare Revolution in Data Sharing and Confidentiality \n\nData sharing has always been a key issue when dealing with sensitive and confidential business information. The advanced technological solutions including SafelyShare have been tackling this problem, offering a controlled system for data access without violating data protection. The fundamental basis of this system is \"Zero Trust\", a unique strategy that doesn't assume trust for anyone and keeps control and monitoring at its core. \n\n## Harnessing the Power of Secure Enclaves\n\nA critical aspect of SafelyShare's approach is the use of secure enclaves, or trusted execution environments, ensuring a safe space for data sharing, authentication, and management. These enclaves are created with the help of specific confidential computing chipsets that fully enclose the shared data. With encryption practices implemented outside of these enclaves, data can only be decrypted once it enters the enclave, thereby providing an end-to-end encryption policy. The output exiting the enclave is also encrypted, adding another layer of security to protect the data.\n\nBut challenges exist within this process. Not all online services incorporate a secure enclave in their operation, leading to a high demand for a more flexible, effective solution to confidential computing.\n\n## The Hybrid Approach of Confidential Computing\n\nTo address this issue, SafelyShare offers an approach that is best described as a hybrid model of confidential computing. To compensate for services that don't operate within secure enclaves, this methodology introduces the idea of 'witness execution.' In this scenario, the user places trust in the providers' guarantee of their competency and safe data handling. It's a kind of tacit agreement between the user and the remote service provider, making the confidential computing more feasible in the real world scenarios.\n\nThis hybrid approach redefines the secure sharing paradigm in a world that's continuously evolving. With its elastic foundation, SafelyShare incorporates a profound understanding of the changing security parameters, making confidential computing adaptable and responsive to changing demands and realities.\n\n## Conclusion: Revolutionizing Secure Data Sharing\n\nIn essence, SafelyShare is the leading forerunner in the journey to making sensitive data sharing secure, efficient, and feasible. Navigating around traditional hurdles, it integrates hybrid confidential computing into its framework, achieving a unique blend of trust and practicality. The innovative approach of integrating witnessed computing into the process blurs the lines between full and partial trust, making data security more achievable and delivering a promising narrative for the future of data sharing and security.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Shamim Naqvi","Darren W Pulsipher"],"link":"/episode-EDT177-en","image":"./episodes/edt-177/en/thumbnail.jpg","lang":"en","summary":"During this episode, Darren and SafeLishare CEO Shamim Naqvi discuss how confidential computing can be employed to create managed data-sharing collaborative environments in the cloud."},{"id":80,"type":"Episode","title":"Zero Trust networking with OpenZiti","tags":["zerotrustnetworking","overlaynetworks","networksecurity","digitalage","securitychallenges","networkingconcepts","softwaredevelopers","softwareengineers","defensestrategy","databreaches","virtualnetworks","zerotrustprinciples","openzerotrust","securitymeasures","identitymanagement","secureconnectivity","networkdefense","iotdevices","datatransmission","smartnetworking","serviceintegration","vpnnetworks","wireguardnetworks","datasecurity","itsecurity","digitalinnovations","digitaltransformation","futureproofsecurity"],"body":"\r\n\r\n# Unveiling the Dynamics of Zero Trust Networking and Overlay Networks\n\nAs the digital age progresses, the conversation around network security takes a frontline position. In a rapidly evolving digital landscape, Zero-trust networking and Overlay networks are critical strategies for tackling current security challenges. Here, we delve into these concepts, how they shape our digital systems and provide an understanding of their potential benefits and applications. \n\n## A Closer Look at Zero Trust Networking \n\nZero-trust networking is a mindset that places security as a prime concern in designing and operating digital systems. Its critical aspect is the presumption of potential threats from every part of the network, irrespective of how secure they may appear. This approach moves away from the traditional fortress-style concept in security and leads to more robust networks that do not rely solely on a single firewall's protection. \n\nFirstly, the beauty of zero-trust networks lies in their capacity to work effectively and securely, presenting an advantage for software developers and engineers. Security becomes an enabler rather than a hindrance to the software development process. With zero-trust networking, developers can focus on feature development without worrying about blocked ports or consulting network teams—a significant step towards faster market releases. \n\nNevertheless, zero-trust networking doesn’t eliminate the need for perimeter defenses or firewalls. The zero trust strategy assumes a possible network compromise; therefore, it calls for defense layering instead of solely relying on elementary perimeter defense. \n\n## The Rise of Overlay Networks \n\nAmid the rising security threats and data breaches, overlay networks are emerging as an invaluable tool. These software-defined virtual networks provide an extra layer of security compared to underlay networks such as routers or firewalls. \n\nOverlay networks like VPN and Wireguard allow secure communication between resources even when the underlying network has been compromised. They offer attractive features, like self-reorganization based on conditions, giving them temporary characteristics. These networks also come with options for secure in-application or data system communication—additionally, a clientless endpoint option bolsters user connectivity, requiring no software installation on individual devices. \n\nOverlay networks provide flexibility concerning deployment. There’s no need to rewrite your application code, as the code for the overlay network can be embedded directly into the application code. Alternatively, a virtual appliance can be deployed instead if you want to avoid altering your application. This convenience, combined with added security, sets overlay networks up as future-proof solutions to network security. \n\n## The Power of ZTN and OpenZiti Solutions \n\nZero Trust networking (ZTN) offerings, like Open Zero Trust (Open Ziti), provide competent solutions in zero trust and overlay networking. They deliver robust Zero Trust principles into the field of overlay network solutions. \n\nZTN, for instance, brings its identity system to the table, perfect for edge IoT devices unable to access typical identity services. It offers secure data transmission through mutual tunneling and an intelligent routing fabric that determines the most efficient path from point A to point B. On the other hand, Open Ziti facilitates multiple use cases, managing east-west and north-south connections smoothly and securely. It integrates well with service meshes to provide high-level security. \n\nThus, adopting such holistic security measures becomes necessary as we step into the digital era. ZTN and OpenZiti present practical solutions for those embracing the Zero Trust model, with advantageous features ranging from identity management to secure connectivity. No doubt, these innovations are setting the benchmarks for network security.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Phillip Griffiths","Darren W Pulsipher"],"link":"/episode-EDT178-en","image":"./episodes/edt-178/en/thumbnail.png","lang":"en","summary":"On this episode, Darren interviews Phillip Griffith, a community leader of the open-source project OpenZiti. They discuss the importance of Zero Trust networking in modern IT networks."},{"id":81,"type":"Episode","title":"Leveraging Generative AI in College","tags":["generativeai","academicintegration","aiineducation","digitaltransformation","blogpost","aistudents","byuidaho","chatgpt","aiassistance","academiccheating","distinguishingguidelines","selfassessment","aiinselfassessment","qualitativeimprovement","ongoingdebates","aishortcomings","aiinaugmentation","futureofai","cheatingconcerns","shortcutculture","academicpolicies","aisuccess","aitoolkit","professionalhabitat"],"body":"\r\n\r\n## Navigating the Intricacies of Academic Integration with Generative AI\t\t\n\nIn the fast-paced world defined by the rapid digital transformation, it is increasingly noticeable how AI constructs are becoming inextricable parts of everyday life. One captivating area where their impact can be felt is in the field of academics. This blog post intends to delve into the potential of generative AI with firsthand experiences from a student, Madeline Pulsipher, at BYU Idaho. \n\nApplying generative AI assistance such as ChatGPT in academic work reveals exciting possibilities. When utilized responsibly, this powerful tool can provide a digital advantage in brainstorming ideas, generating essay outlines, and self-assessing your work against grading rubrics.\n\n## Generative AI - Tool or Trick?\n\nThe question of whether or not utilizing AI for academic tasks happens to be cheating presents an intriguing aspect. Madeline rightly points out that using AI to facilitate a process or guide along should not be equated with cheating. Cheating would imply composing an essay solely by the AI and taking credit for the unaided work. \n\nHowever, we must create distinguishing guidelines as we broach newer technological methods. Defining what constitutes responsible use versus cheating when incorporating AI in academics is an essential task that educational institutions must work on and set formally and strenuously.\n\n## The Efficiency of AI in Self-assessment\n\nAn intriguing usage of AI has stopped everyone in their tracks - self-grading her work based on the established marking rubric before submission. Madeline's experiments with this approach bore fruitful results, with her securing As in all her AI-assisted essays. This signifies the newfound potential of AI to assist not just in mechanical tasks but also in the qualitative improvement of work.\n\n## Prospects and Ongoing Debates\n\nThe use of AI in academic contexts has been debated for quite some time. While it can be a valuable tool for enhancing learning outcomes and improving productivity, it's important to remember that AI cannot replace the human intellect. Every new technology has benefits and drawbacks, and AI is no different.\n\nAlthough generative AI can produce content, it lacks the human touch that is essential in communication. It cannot replace human teachers in explaining complex concepts, as it needs the ability to understand the nuances of human conversation. Therefore, while AI can be a valuable asset in certain areas, it must maintain the value of human interaction and expertise.\n\n## Improving Social Interactions\n\nThe COVID-19 pandemic has disrupted the lives of many students beginning their freshman year in college this year. The negative dating trend among teenagers has been further exacerbated during the pandemic. Due to the lack of social interactions, the current generation misses many critical experiences, such as breaking up, first kissing, or asking for another date.\n\nMadeline sought advice from her friends on how to let down a guy who wanted another date but received conflicting advice. Then, she turned to ChapGPT, an impartial and unemotional AI-powered assistant, for advice. She used ChapGPT's suggestions as a guide to develop her approach.\n\nThis ability to use Generative AI as an advisor rather than a definitive authority will be crucial for the next generation to leverage the power of AI in academic and social situations.\n\n## The Future of AI in Academics\n\nVarious concerns continue to hover around integrating AI into academics - worries about cheating, the lack of established institutional policies, and the possibility of fostering a short-cut culture. However, it is undeniable that generative AI is a tool many students are resorting to, and its full potential within academia still needs to be thoroughly explored.\n\nClearly, the stringent line between cheating and appropriate use needs to be carefully charted. But once this line has been established, the success of AI as a tool in academic paradigms looks promising. If wielded correctly, it can become a substantial part of an educational toolkit - shaping competent individuals well-equipped to handle AI in their professional habitats.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Madeline Pulsipher","Darren W Pulsipher"],"link":"/episode-EDT179-en","image":"./episodes/edt-179/en/thumbnail.png","lang":"en","summary":"In this episode, Darren interviews his daughter who recently completed her first semester in college about her experience using generative AI technology in her academic studies. She describes the challenges and successes associated with utilizing this transformational tool."},{"id":82,"type":"Episode","title":"Data Collection and Preparation","tags":["dataarchitecture","datacentric","data"],"body":"\r\n\r\n## We Need Data!  Our Data is a Mess! \n\nThe first thing to think about in this part of the process is the data pipeline. How do we identify what raw data we need, and how do we get it through the pipeline and transform it into insight? There are five key steps in the pipeline: determining the business value of the data, ingesting it, preparing it, analyzing it, and, finally, acting upon the resulting insights.\n\nLet’s look at manufacturing for an example. In determining what data offers business value, you should ask three fundamental questions: What is the demand for my widget? What is the current supply? What is the yield loss? These are seemingly simple questions, but then you have to think about more complex things such as how do I quantify demand, manufacturing capabilities, supply, and yield loss? Where does the data come from? How do I ingest it? How reliable and stable are these data? There are many questions and variables, such as raw product delivery time, projected demand, and unknown yield loss that can create great complexity.\n\nThe pipeline simplifies how all of these components come together. Each type of data goes through the key steps in the pipeline, but each will be different. For example, the ingestion of one type of data will vary from the ingestion of another. The idea, though, is to bring all the data together to create a clear picture.\n\n## We Have Data! What Do We Do With It? \n\nDepending on the type of data and the questions you are trying to answer, you would use different analytic techniques. For example, in answering how many widgets should be manufactured, you could analyze historical supply and demand through analytics and basic business intelligence. To determine which widgets have visual defects, an algorithm that learns to identify defects in images via deep learning might be the best approach. There is no one technique that solves all problems; each is unique to the problem and the data itself.\n\nAdditionally, it’s important to bring in domain experts to help understand the patterns the data yields. The domain expert will understand the data and where it’s coming from, and the data scientist will understand the best approach for the algorithms to gain more insight. If, for example, a decrease in product yield is predicted through a machine learning algorithm, the engineers who need to correct the problem won’t necessarily know where to look without the context of the problem. One of the reasons why organizations aren’t gaining a return on investment to the degree that they should is because they haven’t built their models to be actionable or reflective of the behaviors within the systems they are trying to predict.\n\nHow all of this works together comes down to the business questions you are asking and your challenges. For example, you could have an assortment of algorithms telling you how many widgets to manufacture. You might have a deep learning algorithm that recognizes whether a widget has a defect, and even categorizes the defects. But that doesn’t necessarily help if you don’t know why that defect happened. So you have to tie that information to a few more algorithms to obtain correlations to explain the defects, and you need a plan of action to correct the problem.\n\n## We Need to Create Insights. How do we Train our Data? \n\nHow do we accomplish this? Essentially, you’re bringing all the data together, preparing it, and linking it in order to, for example, quantify supply and predictions in yield loss. You are going to need problem solving and continuous improvement practices over time to meet changing conditions. This is where the culture of the organization comes in. Solving a problem once without a commitment to continuous improvement can cause an organization to miss the real value of doing the analytics in the long run.\n\nWe are seeing a major shift today toward organizations with a data-centric infrastructure. Data is no longer just in the data center, but in the cloud, and on the edge. With the business process at the top, leading to continuous improvement, business and data understanding, and all the way to deployment, organizations built on this infrastructure can see a world of difference.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT18-en","image":"./episodes/edt-18/en/thumbnail.png","lang":"en","summary":"Sarah Kalicin, Lead Data Scientist at Intel and Darren Pulsipher, Chief Solution Architect, Public Sector at Intel talk about the process and benefits of data collection and preparation in becoming a data-centric organization. This is step two in the journey of becoming a data-centric organization."},{"id":83,"type":"Episode","title":"Generative AI in Higher Education (Revisited)","tags":["embracingdigitaltransformation","darrenpulsipher","lauranewey","generativeai","aiineducation","educationtransformation","teachingexperience","aiinclassroom","moderneducationdynamics","criticalthinkingskills","digitaltransformation","resistanceagainstai","traditionalteaching","incoorporatingai","ineducationalcurriculums","innovationandeducation","ethicalusageofai","academicintegrity"],"body":"\r\n\r\n## How Generative A.I. Enhances the Classroom Experience\n\nGenerative AI is rapidly weaving into educational curriculums, impacting how educators approach teaching and fundamentally enhancing the learning experience. According to Newey, this much-debated technology is not merely a form of plagiarism but a brilliant tool that augments and revitalizes educational methodologies. Encouraging students to use A.I. in thinking tasks, she emphasizes fostering and harvesting critical thinking skills in our breakneck digitizing society.\n\nRather than lingering as passive participants, she advocates for students to become active players, analyzing the results generated by AI and considering the quality and substance of their input information. The shift underlines the importance of understanding, research, and analysis over mere result generation.\n\n## Transition From Traditional Teaching \n\nNewey's progressive approach dramatically diverges from the conventional methods that most educators cling onto, especially considering general resistance towards integrating Generative A.I. in educational settings. However, she emphasizes the inevitability and necessity of adopting digitalization for the overall advantage of students.\n\nComparing this transition with the initial resistance to utilizing the internet as a teaching tool indicates where we stand today. Generative AI, like any other evolving technology, necessitates incorporation within the curriculum and demands regular updates for relevance in this fast-paced digital landscape.\n\n## Balancing Innovation and Ethics\n\nWith progression and innovation, Newey also addresses the ethical considerations inherent to this change. She shares several instances where students, unknowingly or subtly, submitted AI-generated essays. Thus, she emphasizes educators' need to vigilantly balance technological embracement and ethical usage.\n\nShe firmly believes that students can use A.I. as a productive tool, but the responsibility also falls upon educators to guide them toward maintaining academic integrity simultaneously.\n\n## Conclusion: Paving the Way Towards an A.I. Enhanced Education System\n\nThe incorporation of Generative AI in education, while met with resistance, is a profound indication of the shifting educational landscape. As Newey illustrates, successful integration of AI in education can significantly enhance learning experiences and the development of essential skills, securing our students' readiness for a future shaped by digital transformation.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Laura Newey","Darren W Pulsipher"],"link":"/episode-EDT180-en","image":"./episodes/edt-180/en/thumbnail.jpg","lang":"en","summary":"In this week's episode of Embracing Digital Transformation, Darren Pulsipher interviews guest speaker Laura Newey about her fascinating journey through the critically emerging world of Generative AI, particularly in the education sector. Covering the transformation of her teaching experience and enriching her students' learning outcomes through AI, she extensively analyzed adapting to modern education dynamics."},{"id":84,"type":"Episode","title":"Zero Trust in 5G","tags":["5gtechnology","zerotrustsecurity","embracingdigitaltransformationpodcast","darrenpulsipher","lelandbrown","yazkrdzalic","kenurquhart","trentonsystems","zscaler","advancedcommunications","operationaltechnology","informationtechnology","lonelyislandapproach","telecominfrastructure","advancedsecuritymodels","commercial5gusage","military5gusage","technicalchallenges","cybersecurity","5gsecurity","integration","solutionarchitecture"],"body":"\r\n\r\n## The Expansive 5G Landscape and The Lonely Island Approach\n\nThe world of 5G technology is rapidly evolving, and as a result, there are a lot of insightful discussions taking place around merging Operational Technology (OT) and Information Technology (IT). Yazz Krdzalic describes the concept of the \"Lonely Island approach.\" This approach refers to the tendency of different entities to focus too heavily on solving their individual problems, which has often led to the stalling of growth in custom hardware in telecom infrastructure. \n\nThe need to break away from this individualistic approach and re-establish a collective architectural framework that can scale and flex with different use cases is becoming increasingly apparent. With the emergence of 5G technology, there is a need for a collaborative approach that can accommodate the various requirements of different entities. The collective approach will help to ensure that the infrastructure is flexible and scalable, making it easier for entities to integrate their technologies and applications into the network. \n\nThe discussions around merging OT and IT are also gaining momentum, and it is becoming clear that the collaboration between these two domains is essential for the success of 5G technology. As the technology continues to evolve, it is expected that there will be more debates and discussions around how to take advantage of the opportunities presented by 5G, while also addressing the challenges posed by the emerging technology. Overall, the future of 5G technology looks bright, and the collaboration between different entities will play a critical role in its success.\n\n## Transitioning to Zero Trust Security\n\nAs technology continues to evolve, security concerns have become a growing issue for individuals and organizations alike. In order to address these concerns and ensure a safe and secure environment, a collective architectural framework is needed. This framework includes the implementation of advanced security models, such as Zero Trust Security. However, transitioning to these models is not always easy. It requires letting go of older methods of operating and ensuring that all technological modules are synchronized and functioning properly. In the past, it was the customers who were burdened with the responsibility of integrating all the pieces. Fortunately, with the adoption of a more evolved approach, the onus of integration has been considerably reduced for the customers, making the implementation of Zero Trust Security and other advanced security models a much smoother process.\n\n## Finding The Common Ground In 5G Usage\n\nThe development of 5G technology has been a game-changer in both commercial and military sectors. However, there are specific requirements that differentiate the commercial and military usage of 5G. Commercial deployments of private 5G networks are largely static, whereas military deployments need to be mobile. \n\nLeland Brown, a prominent expert in the field, has discussed the complexities of finding a common architecture that could cater to both these needs. The challenge was to create a final solution that elegantly fulfilled these requirements. It was important to ensure that the solution was efficient and effective for both commercial and military use cases. \n\nThe development of such solutions is crucial to ensure that 5G technology is utilized to its fullest potential and can cater to the diverse needs of different industries.\n\n## Wrapping up\n\nThe world of technology is constantly evolving and improving, and the advent of 5G technology and Zero Trust security is a testament to this. However, implementing these advancements can be challenging due to technical and cultural obstacles. Thankfully, experts like Leland Brown, Ken Urquhart, and Yaz Krdzalic are working to streamline the integration of 5G technology and Zero Trust security, making the journey towards a safer and more efficient technological future a little easier for everyone. Their insights and expertise are shedding light on the continuous journey of evolution and improvement in the world of technology.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Leland Brown","Yazz Krdzalic","Ken Urquhart","Darren W Pulsipher"],"link":"/episode-EDT181-en","image":"./episodes/edt-181/en/thumbnail.jpg","lang":"en","summary":"In the midst of the growing adoption of 5G technologies worldwide, the experts in the recent episode of Embracing Digital Transformation podcast delved into the integral topic of Zero Trust in 5G security. Host Darren Pulsipher welcomed 5G advanced communications expert Leland Brown, VP of Marketing at Trenton Systems Yazz Krdzalic, and Ken Urquhart, a physicist turned cybersecurity professional from Zscaler, to discuss the integration and advancement of 5G technology, along with its challenges and breakthroughs."},{"id":85,"type":"Episode","title":"Zero Trust Data Assurance","tags":["digitaltransformation","dataintegrity","zerotrust","cybersecurity","strategies","walacor","encryption","loganalysis","geographicaldistribution","externalhackers","organizationalthreats","datamanagement","dataaudits","immutableauditlog","systemchange","legalcompliance","reducerisk","dataintelligence","multilayeredsecuirty","dataprotectionsolution","keyvulnerabilities","improveddataprotection","futuredigitalbusinesses","revolutionizedigitallandscape"],"body":"\r\n\r\n## Unmasking Assumptions About Data Security\n\nIn the past, people have had implicit trust that their data is secure and their privacy is protected. However, this trust is often based on an outdated model that no longer aligns with the current technological landscape. The increasing number of data breaches and cyber attacks has made it evident that data security is more critical than ever, and the precautions that were considered adequate in the past may no longer be sufficient.\n\nToday, data is vulnerable to threats not only from external hackers but also from within organizations. It is essential to understand that a data breach can have significant implications, ranging from financial losses to reputational damage. Therefore, it is crucial to implement a zero-trust approach to data management, which means that every request for access to data must be verified before access is granted. Reliable data audits are also necessary to ensure that the data input matches the output and that there is no unauthorized access to sensitive information.\n\n## Implementing a New Age of Data Security with Walacor\n\nWalacor provides a unique solution to improve our understanding of data security. They offer an automatic and full-proof audit log that is immutable, meaning that once data is entered, it can never be altered or deleted without being detected. This feature makes it incredibly easy to track every change made to the system, which is critical in maintaining a secure environment.\n\nBy providing transparency and traceability, Walacor's solution helps organizations to meet legal compliance requirements and mitigate risks. For instance, in a legal dispute, an immutable audit log can serve as a reliable source of evidence, as it cannot be tampered with. Furthermore, in the event of a data breach, an immutable audit log can help identify the source of the breach and the extent of damage caused.\n\nOverall, Walacor's innovative approach to data security, with its 100% immutable audit log, offers a promising solution for organizations looking to enhance their cybersecurity posture.\n\n## Shaping the Future of Data Intelligence\n\nThe increasing risk of data breaches means that we need to move away from using multiple layers of data security to a more integrated data protection solution. This type of solution lays the foundation for a Zero Trust environment, which significantly reduces the risk of cyber threats and vulnerabilities. By adopting this approach, we can streamline our data protection methods and ensure better data integrity.\n\nThe development of data intelligence in the form of data integrity and security opens up new possibilities for digital businesses. Improved data protection methods, better data integrity, and a reduction in potential cyber threats are just a few of the benefits that are set to transform the digital landscape. Among these, the talk of the town is Walacor's unique approach to data integrity and zero trust, which marks a significant milestone in how we approach data security now and in the future.\n\nCheck out more information from (https://walacor.com)[https://walacor.com]\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Walter Hancock","Darren W Pulsipher"],"link":"/episode-EDT182-en","image":"./episodes/edt-182/en/thumbnail.png","lang":"en","summary":"The need for robust data security strategies has grown exponentially in the digital age, becoming a top priority for businesses around the world. Cybersecurity expert and CTO of Walacor, Walter Hancock, offers keen insight into the importance of data integrity and a zero trust approach in current cybersecurity regimes. "},{"id":86,"type":"Episode","title":"Data Management in Material Science and Manufacturing Industries","tags":["digitaltransformation","materialscience","manufacturingindustry","dataanalytics","machinelearning","artificialintelligence","productdevelopment","structuredmethodologies","projectmanagement","complexdata","unstructureddata","datascience","r&dprocess","newproductdevelopment","datamanagement","technologyinnovation","physicitaldigitalknowledgegap","embracingdigital.org","embracingdigitaltransformationpodcast"],"body":"\r\n\r\n## Bridging the Gap between Physical and Digital in R&D\n\nMaterials Zone is focused on the niche yet significant aspect of material science, specifically in the manufacturing industry. Given the considerable role of materials in product development, effectively managing data becomes crucial. Analogous to a cooking recipe, material science involves a nuanced integration of ingredients (materials) passed through a process to produce the final product.\n\nHowever, this area has historically been ad hoc, relying on trial, error, and intuition. Consequently, the knowledge acquired during this process often gets lost due to insufficient documentation or employee attrition. In our modern, interconnected world, where product development processes often span multiple locations, even countries, establishing structured methodologies to prevent knowledge loss is critical. \n\nOne of the techniques highlighted by Yudilevich is addressing the \"trucking factor,\" which suggests that if the only person who knows how to do a particular task got hit by a truck, it could potentially derail the entire project. Hence, having at least one other person aside from the primary individual who can perform the task could lower the team's vulnerability.\n\n## Capturing Complexities of Material Science Data\n\nThe field of material science generates complex data, often unstructured and difficult to capture using traditional data tables and databases sufficiently. To visualize this, consider data as a graph where raw materials turn into end products. The innumerable interactions between the various constituents give rise to multiple unique dimensions within the data.\n\nMoreover, a seamless translation exists within the manufacturing realm – From the explorative research to the production phase, which demands stabilization and consistency. Collating data from these phases into a unified repository can enhance the R&D process by centralizing information, aiding inter-phase learning, and accelerating new product development.\n\n## Integrating Data Science into Manufacturing\n\nWhile data science has permeated many industries, companies focused mainly on product development in the physical world often find setting up dedicated data departments or integrating analytical tools inefficient and costly. This is where Materials Zone's solution comes into play, making data science, machine learning, and statistical tools accessible to businesses unfamiliar with these areas.\n\nThey offer out-of-the-box tools accompanied by webinars and training sessions for easy adoption, thus reducing the barriers to integrating data science into manufacturing practices. Surprisingly, even Fortune 500 companies who lack the necessary digital skills can benefit significantly from such solutions.\n\n## As We Step Forward\n\nAs the product development process becomes more complex and global, the critical nature of systematic data management combined with technological innovation is coming to the fore. Companies like Materials Zone are paving the path, guiding businesses to bridge their physical-digital knowledge gap, bolster their manufacturing practices, and ensure future success.\n\nFor more information, check out https://materials.zone. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ori Yudilevich"],"link":"/episode-EDT183-en","image":"./episodes/edt-183/en/thumbnail.png","lang":"en","summary":"In a rapidly evolving technological landscape, leaders from diverse sectors apply data analytics, machine learning, and artificial intelligence to their operations. Today, look deeper at a company driving digital transformation in the manufacturing industry – Ori Yudilevich, the CTO of Materials Zone."},{"id":87,"type":"Episode","title":"Using Data as a Strategic Asset","tags":["dataarchitecture","data","datastrategy","people","process"],"body":"\r\n\r\nJust like some people hoard things in their homes, afraid to throw anything away, organizations can hoard data. For example, my email folder is 8 gigabytes. I know it\n\ndoes not need to be that big, but I save things in case I might need them. Now multiply that times the number of employees. At Intel, we’ve got a hundred thousand employees. Imagine how much data we’re storing, just in emails, when we do backups. Add in structured and unstructured databases, presentations, spreadsheets, etc…and it’s clear that just storing it all is not a great strategy.\n\n## Data Statistics\n\nLet’s look at the statistics that show why this is a bad strategy. Approximately 80 percent of data scientists’ time is spent cleaning up data before they can use it. Less than 50% of structured data is being utilized at all, and less than 1% of unstructured data can be analyzed. So, all of this data is being hoarded, but organizations are not, for the most part, using it. Another issue is who has access to all of this stored data. It is alarming that 70% of employees have access to data that they probably shouldn’t. Just as a hoarder’s piles create fire hazards in the home, unorganized piles of data create security risks in an organization.\n\n## Data Explosion\n\nWhat can we do about this problem? First, we need to understand why there is such an explosion of data. With IOT, everything is connected, and we have data processing happening in a multitude of places. The sheer volume of data that is being generated is incredible. This is exacerbated by regulatory issues; it’s hard to know what we can get rid of and what we can’t. We fill up our storage and then buy more. The technology is basically enabling our data hoarding. We need to look at what we can do differently.\n\n## Why We Hoard\n\nExperts say that people hoard because they believe that an item will be useful or valuable in the future, has sentimental value, is unique and irreplaceable, or is too big a bargain to throw away. These same ideas apply to data hoarding. For example, why do I have one of the first presentations I ever gave? It’s stored on a drive and it is in the cloud. I look at it maybe once a year, but I don’t have any real reason beyond sentimental value to keep it. An organization is made up of individuals with these behaviors, and at all different levels of data hoarding, there is fear of getting rid of data.\n\n## Becoming Data Centric\n\nWhat does it look like to become a data-centric company instead of a storage company? Here is a four-step process to becoming a data-centric organization.\n\n## Organizational Foundation\n\nThe first step is creating a data-centric organizational foundation. There are four key players with distinctive roles.\n\nChief Data Officer: The chief data officer’s job is to set strategy and governance over the management of data and the generation of valuable business information. This role is different than that of a chief information officer, who focuses on infrastructure rather than the information itself. This is a difficult job, as the chief data officer is involved in cultural change. They try to keep people from hoarding data, and instead, use it to create real business value.\n\nData Scientist: Data scientists develop models and blueprints by finding patterns in the data and using predictive analytics. The data scientists’ efforts can become simply a one-time science experiment, however, unless the information is operationalized.\n\nData Engineer: This is where data engineers come in. They manage data pipelines and operationalize analysis. As new data comes in, new insight is generated without starting over each time.\n\nData Steward: The data steward manages governance and access to data assets, making sure the right people have the right access at the right time.\n\nWith an organization that includes these four roles, the next big question is whether to centralize or distribute operations.  For example, perhaps distributed matrix management is already working in your large organization, but a smaller organization may need more rigor and would benefit from a more centralized structure.\n\n## Data Collection and Preparation\n\nAn article from the Harvard Business Review effectively uses a sports analogy to describe two strategies to catalog data: defense and offense. In defense, the goal is to protect the data. In offense, the goal is to move forward to score as quickly as possible.\n\n## Data Defense and Offense\n\nWith a defensive strategy, the organization is primarily focused on data security, governance, and compliance. Protecting the data is key. The main data activity will center on extraction, standardization, storage management, and access management. Typically, this strategy will use a more centralized organization and will use a single source of truth.\n\nWith an offensive strategy, the organization is primarily focused on moving quickly to improve its competitive position and be as profitable as possible. Data activities will focus on extraction, modeling, visualization, transformation, and enrichment.\n\nThis strategy will require more flexibility, which means a more distributed organization with multiple versions of truth.\n\nUnderstanding how to use the data based on the strategy is important. Often, organizations will straddle the fence and it can get confusing. Although every organization needs to be able to play defense and offense, organizations must choose a strategy rather than trying to do both, just as professional-level football players don’t play both sides of the ball.\n\n## Analytics Insight\n\nThere is an organizational maturity curve to analytics and building insight from your data. The key is to understand where your organization currently resides and what the next steps are to move up the curve.\n\nIn the descriptive analytics stage, you are just trying to figure out what is going on. In the diagnostic step, you are figuring out why something happened. In the predictive step, you can predict what will happen in the future based on historical data. This is where many organizations strive to be, but the first two steps must be accomplished first. Above predictive is prescriptive, where you can understand why something will happen and guide the organization according to expectations. At the top of the steps is analytic insight, or foresight, where you are making things happen, even progressing beyond prescription.\n\nOne reason it is important to understand where your organization currently sits is because there are specific tools for each phase. For example, you don’t want to be stuck with an AI project that is using prescriptive, or even predictive, algorithms when your organization is still on the descriptive step.\n\n## Operationalize It\n\nTo achieve the goal of operationalization, or making a repeatable process, there are three key elements: a data-centric infrastructure, data pipelines, and business flow.\n\nThe data-centric infrastructure allows you to know where all your data is and what’s in that data through various tools such as a metadata manager like elastic search or meta-data catalogs and repositories. Data pipelines have great tools to enable the process from ingestion to analysis to action. A defensive or offensive strategy will determine which tools you will use in your pipeline. The last element, business flow, is where business understanding of your data and processes will allow deployment of a continuous improvement process to ensure repeatable, valuable insights.\n\n## Call to Action\n\nFirst, develop a data strategy. Get organized and figure out where all of your data is and catalog it. Decide on a defensive or offensive strategy, and then take your analysis insight steps one at a time, using the right tools. Most importantly, operationalize your insights to get the best business value.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT19-en","image":"./episodes/edt-19/en/thumbnail.png","lang":"en","summary":"In this episode, Darren Pulsipher, Intel Chief Solution Architect, Public Sector, explores how organizations can move from simply hoarding data to using it as a strategic asset."},{"id":88,"type":"Episode","title":"Follow the Bit","tags":["iot","dod","edge","technology","data","cybersecurity"],"body":"\r\n\r\n## Internet of Things is the Start\n\nThe widespread implementation of the Internet of Things (IoT) has been taking longer than the industry expected. Many blame the delays in the adoption and roll-out of 5G across the world, but there is another problem that has slowed down the deployment of robust IoT systems: management of complexity. As data moves through the system from the edge, aggregated edge, network, data center, and cloud, securing the data is a major concern since the attack surface increases as it moves outside of the traditional data center. There are point solutions that help improve these problems, but there is not yet a complete solution architecture that solves all of the problems with this data center without walls.\n\n## Complexity of the DoD environment\n\nThe IoT is already complex, and the DOD increases the complexity because of the types of producers and consumers of the data. On the producer side, sensors are attached to satellites, aircraft, ships, and vehicles; even warfighters themselves are basically moving data centers. The amount of data that these edge devices produce can overwhelm a network. The number of heterogeneous devices can make managing them seem nearly impossible, especially when the devices communicate with different protocols and have different levels of classifications. In other words, collecting the data is not the problem; getting it into the hands of those who make the decisions in a useful format quickly is the issue.\n\nIn addition, connectivity can be problematic with edge devices. Hostile environments where network connectivity to a data center is non-existent, or spotty at best can delay the data. This means that a solution in this space must work in connected and disconnected modes of operation. Speed of delivery is a key success factor; lives can be at stake if decisions based on the data are delayed.\n\n## Common Framework for Applications, Data and Security\n\nObviously, we need solutions to these problems. Of paramount importance is a common framework to manage the complexity of these new IT architectures that are outside of the traditional data center walls. The framework needs to address the management of applications, data, and security. We need to be able to deploy portable and reusable applications anywhere within the system, from the edge to the cloud: the “write once, run anywhere” doctrine. This gives us the ability to rapidly develop, test, and deploy applications without having to set up all permutations of hardware configurations in the ecosystem. Using tools in the container ecosystem should help with this. Tools based on Kubernetes (K8s) are a good choice as they have become the defacto standard in the DevOps Community.   \n\nManaging applications in isolation, however, is not enough. All applications need data in some respect, so understanding where the data is, where it is going, and how it is classified is key to successful solutions. We need a common operating environment to manage and govern the different data classes such as domains, security boundaries, governance, data life-cycle management, and data locality. A common operating environment increases the flexibility and velocity of deploying applications.\n\nA common framework of security is also necessary. The critical question is how do you secure your data in all its forms and still share it? There are current hardware and software solutions and continuing progression in this area. Basic security solutions such as encryption should be foundational. Of course, this requires the right underlying engine for storage and capability. Another concern is erroneous or nefarious data entering the system. Establishing a root of trust as a foundation is also necessary in this vast ecosystem.\n\n## Processing at the edge, datacenter and cloud\n\nWhere does Intel come into play in this environment? We can help provide the underlying infrastructure that supports these systems in performance and power. Whether you are processing sensor information on the edge in a low power environment (think Atom and custom ASIC designs), or you are doing Artificial Intelligence training or inference in your data center (Xeon and Neuromorphic computing),  Intel has a processor that can help convert raw data into valuable, actionable information, the key component in this complex, mission-oriented environment.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT2-en","image":"./episodes/edt-2/en/thumbnail.png","lang":"en","summary":"In this episode, Darren interviews Greg Clifton, Director of Department of Defense (DOD) and Intelligence for Intel Corp. They discuss the challenges of data management in a complex system that spans multiple clouds, enterprise data centers, regional data centers, and tactical edge. Listen to Darren and Greg follow a bit of data from its collection and journey through this ecosystem to the production of actionable information for analysts and warfighters. Listen to Darren and Greg discuss some of the obstacles in this large, circular environment and solutions to help get actionable information to analysts and back to warfighters."},{"id":89,"type":"Episode","title":"Destroying the Complexity of Storage Tiers","tags":["data","optane","technology","storage","vastdata"],"body":"\r\n\r\n## Thirty Years of Storage Complexity\n\nStorage may be an old concept, but VAST Data has rocketed to unicorn status in just over a year of revenue shipments. VAST has replaced the old architecture of multi-tiered storage systems with a single, monolithic system that is fast and cost effective. The VAST solutions also eliminate the problem of big, messy storage systems that consist of different file systems and different architectures.\n\n## A Foundation for a New Architecture\n\nThe founder of VAST was looking at storage from a clean slate perspective. He found that customers didn’t necessarily need any more performance from Flash, but instead, they needed support for file and object storage at a lower cost. He took advantage of three technologies that didn’t exist before 2018. First is NVMe over Fabrics, which is used as a kind of hyperscale SAN to connect everything together with low latency. Next is QLC Flash because it is inexpensive and has no moving parts like hard drives do. Third is Intel’s 3D XPoint Optane. Optane has read-write parity and has high endurance at a reasonable price point. Wrapping these three technologies together gives VAST the ability to give customers all-flash performance, but at a price comparable to that of a hard drive. It eliminates the need for other tiers that organizations are buying because they are cheap.\n\n## Introducing Vast Data Universal Storage\n\nThe VAST system is the first disaggregated, shared everything architecture. What this means is that the logic is disaggregated from the state of the file system. Instead, the state of the file system lives in data boxes that contain QLC Flash and Optane. The Optane is used as a big meta data store. It is multi-use, just like the system, which is branded universal storage. With these boxes, there is no single point of failure, so the capacity is, theoretically, unlimited. (VAST has tested the system at about 50 petabytes in a single file system.) You can grow capacity by just adding inexpensive x86 servers into the clusters. Because it’s a parallel file system, any user can access any piece of data from any one of the servers as if it is direct attached, so you can continue to scale.\n\nYou can also scale performance independent of capacity. The only thing that would inhibit the performance of the flash is the CPU, so by having the ability to seamlessly scale the number of CPUs in the cluster, you can grow performance.\n\nOne of the problems that is solved with this structure is latency. Many organizations need low latency to all their data. Because each of these stateless servers has access to everything, you have fast access to all of the data.\n\n## DASE Architecture: Server Pools\n\nAnother great benefit is that it is easy to fine-tune an organization’s storage. The composable nature of the stateless servers, and the absence of cross talk between them, gives you the ability to build a cluster to best suit your needs. For example, you can segment off your control boxes based on different workloads, but they can all access the same data.\n\nIn addition, the system works well with an organization that needs different classification layers to access the data. You have the ability to segregate what users have access to by creating multiple access zones with virtual IP addresses. One of the challenges with NFS is that it broadcasts basically to everything. If you limit the broadcasts to a subset of IP addresses, it gives you the ability to carve out those different architectures into discrete systems.\n\n## Universal Storage Bridges Application Eras\n\nThis is not just a solution geared for HPC clusters; it is not cost-prohibitive. Many companies are using VAST first for backup to establish trust. For example, the National Cancer Institute has a tape library archive, and they wanted to be able to access the information faster. They looked at different platforms, and VAST came in with a lower price point, and for all Flash, which is faster than their production NAS system. So the solution has a good price point, and it is useful for general file sharing and a variety of workloads, such as AI, log analytics, Splunk, etc., not just for HPC. The VAST solutions are simple to manage and truly universal.\n\nVAST is a young company, but they have multiple installs across government agencies such as the National Institute of Health and the Department of Energy’s Tri-labs where all-out performance is needed for these super computers. This is a powerful system in some of the largest HPC environments in the world, supporting mission-critical applications.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Randy Hayes"],"link":"/episode-EDT20-en","image":"./episodes/edt-20/en/thumbnail.jpg","lang":"en","summary":"Randy Hayes from VAST Data and Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel, discuss VAST Data’s innovative storage architecture that eliminates the need for tiers using NVMe over Fabrics, QLC Flash, and 3D XPoint Optane"},{"id":90,"type":"Episode","title":"Big Memory Software Defined Memory Controller","tags":["bigmemory","edge","compute","technology","memverge","optane","pmem","data"],"body":"\r\n\r\nIntel’s 3D XPoint Optane persistent memory technology was a game changer for MemVerge, allowing them to develop software-defined memory, which they describe as expressing the power of Optane PMEM.\n\n## The Best way of Using PMEM\n\nMemVerge developed Big Memory software to meet a growing demand from applications and businesses to process data at an ever-higher volume and velocity. More real-time applications need instant insights and actions from the data. This requires a new, memory-centric infrastructure to fill latency requirements.\n\nApplications that employ AI, machine learning, or real-time big data analytics, for example, typically use DRAM. Although DRAM has a nanosecond-level latency and delivers nice capability and capacity, it does have physical limitations like memory density and how much you can fit in a server. It is also relatively expensive. Most importantly, it is volatile, and as data gets bigger, memory volatility becomes more of a constraint.  PMEM removes these bottlenecks because you can have bigger capacity, lower cost, and persistence.\n\nAn important benefit is that companies do not need to rewrite their applications to use MemVerge’s technology. The company was looking for the best way to use persistence, and the answer was to provide the least disruption to programming paradigms. When Optane PMEM became available as a memory form factor, this was an opportunity to develop valuable software as part of the solution.\n\n## Software Defined Memory \n\nThere is a data transformation that is going on. From a hardware perspective, in the next ten years, there will perhaps be a more heterogeneous world, both in computing and memory elements. A new fabric will emerge, such as CXL, that connects between these elements. The biggest challenge will be getting the application ecosystem to move. A software layer is needed to massage it into consumable, composable pieces that make it easier for the application to digest. MemVerge’s software-defined memory will be an important component in this space.\n\nIn the meantime, MemVerge is building a bridge between the current and future paradigms. MemVerge’s Software-defined memory brings dynamic SLA and QLS, resiliency, application persistence, efficiency, and performance. These are all things you normally get with software-defined storage network, but now can get with memory with lower latency and screaming fast speed.\n\n## Performance on Memory Machine\n\nTwo important MEMVerge intellectual properties are in play. The first is a software virtualization layer that optimizes on the performance of memory – the combination of PMEM and DRAM – that is very close to DRAM performance. Each workload has different performance profiles that can be fine tuned by mixing PMEM and DRAM at different ratios. This fine tuning of PMEM and DRAM gives application developers and It professionals the ability to tune the memory to their applications instead of for the whole machine.   Instead of configuring the existing DRAM and PMEM ratios for the whole machine, you can now dynamically change the ratios of PMEM and DRAM depending on the workload, and even exceed DRAM-level performance.\n\n## ZeroIO Snapshot (Persisting Application Data)\n\nThe second of these inventions is memory snapshots or ZeroIO. It can persist existing transient applications without any rewrites.  It works on top of the software-defined memory, which is a volatile memory service. Even though the underlying PMEM is persistent, volatile memory is necessary to avoid breaking existing applications. The persistence is leveraged through providing the application operator a GUI and CI for managing snapshots. There is a snapshot capability, so you can have an instant capture the state of an entire application. Then, that application can be recovered any time in the future.\n\nThis makes your memory not only persistent, but highly available. After a crash, you can do an instant recovery. If you make a mistake in the database, you can roll back to a previous point. You can also clone on top of the snapshot, so you can create new instances of an application without physically replicating the memory. So you can create multiple independent processes and the logical memory spaces mapping to the same physical memory space. This not only saves memory, but makes the process of creating clones instant. This new technology makes a lot of things that were impossible before, possible.\n\nThere are no architectural changes to your program, but when you need to persist something, you can just snapshot it. This doesn’t change the familiar model of programming, but greatly accelerates the I/O. Another great feature is that snapshots become manageable objects, so they are transportable to anywhere you can restart the application. Live migration can be enabled in certain scenarios as well.\n\n## Future Enabled Use Cases\n\nFor a sneak peak in to the future, MemVerge is planning a 2.0 version in about a year that will have an SDK. In addition to using it as a transparent memory layer, new application developers will have a new way of persisting their data. This will make application development, as well as modification of existing applications, easier.\n\nWith the SDK developers will be able to snapshot segments of the application memory or the complete memory profile, giving the application developer the ability to persist memory with the cumbersome ORM or memory mapping technologies today.\n\nIn partnership with Intel, MemVerge will launch the first version of their product with the software-defined memory and the snapshot capabilities for general availability on September 23, 2020.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Charles Fan"],"link":"/episode-EDT21-en","image":"./episodes/edt-21/en/thumbnail.png","lang":"en","summary":"Charles Fan, CEO of MemVerge, talks to Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel, about their new technology, Big Memory software-defined memory controllers. The technology utilizes Intel 3D XPoint Optane persistent memory to efficiently bridge the gap between current and future architectures, while providing bigger capacity, lower cost, and persistence."},{"id":91,"type":"Episode","title":"An Argument for Multi-Hybrid Cloud","tags":null,"body":"\r\n\r\n## Current Cloud environment\n\nOver the last five years, there has been a fundamental shift in the IT environment. The continual growth of Public Cloud and the emergence of Private cloud options has left many CIO(s) and IT departments playing catchup. In the competitive market of today, many development teams need to move faster than most IT departments can deliver. Development team have found Public Clouds like AWS, GCE, and Azure a viable option for the old style “under the desk” “cottage IT”. The public cloud vendors have made “spinning up” new infrastructure easy and fast. No more waiting around for several levels of technical and business approvals, physical space in the data center, and vendor supply problems. Now in a matter of minutes a development team can have all the infrastructure they need for their new project.\n\nBefore the Public Cloud CIO(s) could easily “walk around” the cubes and count the number of “cottage IT” machines where running under peoples desks. With the physical machines no longer visible to the IT departments, identifying teams and their project’s infrastructures is impossible. Many public cloud have given organizations the ability to consolidate accounting from all of the accounts for specific domains, but visibility into what is running and who is working on the infrastructure is still somewhat of a “snipe hunt”. Many times these “rouge” projects become visible when projects are productized and need to be put into a company secured infrastructure. Security, privacy and regulatory policies can make “productization” of projects near impossible. Especially, if developers have tightly coupled their applications to Cloud infrastructure.\n\nForward thinking IT departments are doing their best to capture “cottage IT” by working with Public Clouds and ISVs to put in a “Company portals” to the Clouds. Putting a pass-thru portal in place is a good start to capturing projects using infrastructure, but many organizations find just a portal is leaving development teams wanting more. Over the last couple of years I have been working with many of these organizations to identify use cases, architectures, and technologies to help develop these augmented portals which we called “Hybrid Multi-Clouds” (MHC). Typically, three major technologies are integrated together to build these MHCs. Cloud Management Platforms (CMP), Automation Frameworks, and Platform as a Service tools (PaaS).\n\n## Cloud Management Platform (CMP)\n\nCloud Management Platforms primary responsibility is managing multiple heterogeneous cloud both public and private. Giving end users the ability to manage multiple clouds and their infrastructure from one common pane of glass.  CMPs are typically opinionated with Cloud Administrators in mind. Although the Cloud Management Platform tools primary focus is managing multiple cloud, many tools have added features from the PAAS and Automation Frameworks or at minimum have a plugin architecture to support it.\n\n### Use Cases covered\n\n* Managing Public Clouds\n\n* Managing Private Clouds\n\n* Managing Cloud identities\n\n* Managing Infrastructure across multiple clouds.\n\n## Automation Frameworks\n\nAutomation Frameworks primary responsibility is to automate the deployment, management and upgrading software stacks on infrastructure. Automation Frameworks came out of the DevOps community and are typically focused on repeatable processes. Many of these tools include scripting languages that i DevOps engineers to repeatability manage and configure software and services. Many DevOps teams are well versed in these tools.\n\n### Use Cases Covered\n\n* Deploy Software on Infrastructure\n\n* Manage Software on Infrastructure\n\n* Upgrade Software and Services\n\n## Platform as a Service (PAAS)\n\nPlatform as a Service is primarily responsible for giving a single portal to re-use platforms and deploy them onto Infrastructure. PaaS tools are typically highly opinionated with the Developer in mind. Which can lead to inflexible infrastructure configurations. Many of these tools have a web portal that give developers the ability to select services and deploy them in the infrastructure. \n\n### Use Cases Covered\n\n* Deploy/Manage Services/Applications\n\n* Manage Service Catalog\n\n* Develop new Services/Applications\n\n## Convergence creates Hybrid Multi-Cloud (MHC)\n\nBecause not one tool set has all of the use cases they need to manage clouds, applications, infrastructure and services, teams spend several “man years” installing, configuring, and integrating these three tool sets together. This has led to an emergence in technologies that integrate these tools including new product offerings, and new features in currently available products. \n\nMany CMP products are including PaaS and Automation Frameworks into their solutions. PaaS tools are now managing multiple clouds. Automation Frameworks are beginning to offer web portals and connectivity to multiple clouds. Many of the tools are moving to the Hybrid Multi-Cloud vision. When looking at which tool(s) to use it is important to remember the roots of the tool. \n\n## Deploying a solution\n\nThe Hybrid Multi-Cloud ecosystem is still fairly new and still requires some heavy integrations between the tools. There are some tools that are starting to deliver complete out of the box solutions, but still with their particular vision of the world. Because the ecosystem is nascent there are many players and choices. Time will tell who will win this space. For now, it will be interesting to watch the tools converge and consolidate while the features mature.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT22-en","image":"./episodes/edt-22/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher describes why a Multi-Hybrid Cloud Architect may already be in your Data Center. Most organizations already all of the ingredients. They just need to know how they fit together."},{"id":92,"type":"Episode","title":"Insight Creation in Data-Centric Organizations","tags":["aiml","dataarchitecture","datacentric","data","insight","technology"],"body":"\r\n\r\nUsing the example of manufacturing widgets, Darren and Sarah previously talked about the data pipeline in an effort to answer a fundamental business question: How many widgets to produce. The complexity becomes apparent when it comes to taking the raw data for customer demand, current supply, and yield loss and converting that into insight.\n\nThe first step in this process, before you can decide on which tools to use, is to prepare the data into usable form. Eighty or ninety percent of a data scientist’s work is in preparing and transforming the data so it can be put into an algorithm, for example, or used for pattern recognition.\n\nThe right tools are complex. AI and machine learning do not use a single algorithm, but a plethora of tools that data scientists use and experiment with in combinations to get the best insight results. In other words, a single algorithm will not tell you how many widgets you should manufacture. There are a lot of overlapping parts, and the tools themselves are complex. In addition, data scientists have different areas of expertise; data science is a team sport. Just as you wouldn’t task a network engineer to design storage architecture, you wouldn’t employ a deep learning engineer who specializes in image recognition to solve your yield analysis problem. You have to hire the right data scientists to design and deploy the right tools to gain insight into your business questions.\n\nLet’s go back to the question of how many widgets you should manufacture. Every situation and model will, of course, be different given the types of questions, data, and dynamics that you have, but we will use this as a starting point. Once this business question is established, the organization will go through an analytic maturity development.\n\nFirst, you will focus on what has happened in the past to see patterns in demand for your widgets. For example, you might look at some time series data to see when demand for widgets goes up and down. How stable is the information over time and how can you use it to forecast the future? Perhaps you could do some machine learning around this to look at different time segments and understand them. You might also want to do some text analysis such as whether people are talking about your widget on social media. The number of likes or shares could be a data source.\n\nYou could do something similar with supply. How stable is the current supply, and how well do you understand the system dynamics? Look at where you need a human to figure out the system dynamics and incorporate that knowledge into the way you do your analytics.  There could be some type of machine learning patterns that give you some ideas of insight, so you might do some unsupervised learning. Unsupervised learning is finding whether there are different categories or segments that you’re not aware of that behave differently from each other. Ask how you can track better, or get a better resolution of what is happening in these groups.\n\nIn short, depending on where the data is coming from and what you’re looking at in that data, you’re going to be using different tools.\n\nYield loss is an example of the complexity of the problems to be solved. The more variation in the manufacturing line, the more waste can occur. Machine learning in this case will be looking at segments and clusters of different types of yields. How do you quantify and predict that?\n\nOne thing data scientists do here is design of experiments to try and estimate causation. By turning knobs and pulling levers in a systematic way, you can see what happens to yield, while adding process controls to avoid drift.\n\nAnother opportunity to run analysis is reliability. For example, with predictive maintenance, your manufacturing tools can be maintained in a timely manner to prevent yield loss. You can also use text analysis in certain situations, such as when you have written records of observations and solutions from technicians over time to use as a collective knowledge base.\n\nDeep learning around image recognition is another strategy to help prevent loss through detection of errors and flaws, and perhaps even categorization of defects.\n\nThe goal of all of this is, of course, to gain valuable business insight for your organization. The key is commitment to a data-centric organization, staying flexible, and having the right tools and the right people to turn your data into actionable insight.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT23-en","image":"./episodes/edt-23/en/thumbnail.png","lang":"en","summary":"In part 5 of a series, Kick-starting your Organizational Transformation to Become Data Centric, Sarah Kalicin, Lead Data Scientist, Intel, and Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel, discuss how to create insight using AI and machine learning in a data-centric organization."},{"id":93,"type":"Episode","title":"Digital Strategy and Data Architecture","tags":["data","dataarchitecture","aiops","secops","devsecops","devops","compute","technology","process"],"body":"\r\n\r\nHaving a plan for your organization’s digital transformation is critical to avoid getting lost in the maze of just going with the latest and greatest technologies and processes. This haphazard strategy will cause your digital transformation to fall flat. A plan, or a roadmap, from where your organization is currently to where you want to end up is the most important part of an effective transformation.\n\nThe three key elements that must be coordinated and balanced in your plan are organizational, procedural, and technological.\n\n## Organizational Divisions\n\nTo understand organizational change, let’s first look at the common groups in most organizations.\n\n### Development\n\nThis group is your day team; they are developing new products. The processes of the development team are well known and mature. They focus on development work, testing, and pushing the product into production. Almost every team will uses some kind of Agile or rapid iteration technique.\n\n### IT\n\nThe goal of the IT team is to optimize infrastructure for cost and efficiency. They make sure the infrastructure is reliable and has built-in control and security. Primarily, they look at compute, storage, network, compliance, and cost.\n\n### Security\n\nThe role of security has become increasingly important in the past few years, more so recently with employees working at home because of COVID-19. The security team primarily focuses on securing intellectual property, data, and infrastructure. Common tools are identity management, protection, detection, and remediation. Understanding how these tools work at a high level is important to your organizational change.\n\n### Data \n\nThis newest group, which previously may have been a statistician or mathematician who did some data mining, is now taking hold with the advent of chief data officers and the organizations built around them. They focus on analyzing, categorizing, and delivering real value from your data. Whether your organization is in manufacturing or services, or whether you’re trying to capture new customers or save money, there are many areas where data scientists can provide value.\n\n## Bringing the Groups Together \n\nTo make an effective digital change, all of these groups need to have a mutual understanding of what each brings to the table and one unified vision. You don’t want your data scientists exploring data that has no value to development or IT. You don’t want your security team locking everything down so tight that the development team can’t get their work done. Among the groups, there must be  common strategies, processes, and architectures.\n\nAlthough common goals and outcomes are the ideal, there are obstacles to overcome. One of the hardest areas in organizational change is communication breakdown at the boundaries. Some organizations have created new groups to smooth the edges between the groups. For example, between security and IT, there could be a Sec Ops or Sec Dev Ops where they automate the policies and procedures that come out of the security team.  Another example would be a Data Dev team, who are developers who work with the data scientists to provide more repeatable processes through application development or integration of tools and applications.  Common architectures in common tool sets that all the groups can rely on make the process and changes much easier.\n\n## The Perfect System \n\nWhat would a common architecture look like? A utopian version doesn’t exist today, but we can look at the elements and perhaps build something toward this ideal.\n\nSelf-service is a must. For example, if a data scientist needs more storage for data, they wouldn’t have to call IT and fill out a bunch of forms etc…, but instead use a time-saving self-service portal that would deliver the storage. Of course, the portal would be policy-driven, so the security team can rest assured that the confidential data the scientist drops into a drive is encrypted and access control is automatic.\n\nAnother ideal feature would be that the system is self-healing and data-driven. If machines became infected, for example, they would automatically be quarantined and the workloads would migrate to another area in the data center or into the public cloud.  Again, IT would have to establish policy and monitor processes, but the system would be mostly automatic. The system should not just be automatic, but intelligent, learning from experience and becoming more efficient.\n\nWe can get some elements of this utopian system today with off-the-shelf products by integrating them and getting everyone to use them. Let’s look at what each organization would want from this architecture.\n\n## IT Architecture (Multi-Hybrid Cloud) \n\nIT is responsible for the underlying infrastructure and data information in the organization. If IT could establish a rock-solid foundation, everyone else could build on top of it. IT needs to move to a multi-hybrid cloud solution so that infrastructure can be easily orchestrated as needed, with flexibility based on policy. There is always a trade-off between cost and reliability, but you have options. A software-defined infrastructure layer easily allows orchestration of compute, storage, network, security, and now even new things like memory and accelerators. The multi-hybrid cloud foundation is a key aspect of your common architecture.\n\n## Security Architecture \n\nThe security team would add to this system and make it as automated as possible.  The first would be the identity aspect. This means that not only can you identify users, but infrastructure, applications, and services so everything has an identity. Those identities can be tied to specific authorizations and accesses to make sure everything is authenticated. On the security side, you want encryption and remediation when there are problems. Ideally, you could establish a root of trust so everything in the ecosystem, both in applications and services and all the way down to firmware and BIOS in the machines, are trusted.\n\n## Development Architecture \n\nDevelopers may worry that all of this process may slow down development, so it needs to happen almost automatically.  Most developers now are focusing on reusable components that can be tested so they know they are safe. They do this through ecosystems on containers such as Kubernetes, Docker, or Mesos. Security can be injected into the development lifecycle at the deployment step before successfully moving into production. On top of the service layer is an application layer where developers can take advantage of workflows. These workflows can be development workflows like CI/CD or business workflows through automation tools like Robotic Process Automation. Having both the service layer and the application layer are key elements in this utopian architecture.\n\n## Data Architecture \n\nWith data scattered across several ecosystems, public clouds, and even out on the edge, we need a better way of managing data for the data scientists and application developers. Extracting data away from storage is one of the important elements here. With this structure, you can orchestrate data across the vast infrastructure and only tie that data to the applications and services where it is needed. The data could be abstracted to land on the infrastructure at the best place during that period of time, whether it’s out on the edge, in the data center, or processed in several different places for application replicas. Security would be required to lock down the data, since the data is the reason for the infrastructure in the first place. Some start-up companies are now in this space, to take control of the data management layer.\n\nThis utopian architecture, with its myriad of moving parts, is called the Edgemere architecture. We are trying to see how all of these parts fit together to help organizations accelerate their digital transformation. We need to understand what each organization needs, what their use cases are, and what commonalities are among the groups to come up with an architecture the whole organization can work in.\n\nYour organization’s part is to break down the barriers between the groups, develop a common vision of where you want to be organizationally, procedurally, and architecturally, and develop a roadmap on how to get there. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT24-en","image":"./episodes/edt-24/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel outlines digital strategy and architecture to effectively transform your organization. He explains how organizational, procedural, and technological elements must be balanced to work efficiently toward a common, ideal architecture to support a unified vision."},{"id":94,"type":"Episode","title":"Operationalizing Data Pipelines","tags":["dataarchitecture","datacentric","data","datamesh","datapipeline","technology","people"],"body":"\r\n\r\nFor the final episode in this series, Darren talks to Sarah Kalicin, Lead Data Scientist at Intel, about operationalizing your data pipeline. They discuss how, once you have your data insights, you can turn them from a one-time science experiment to an ongoing source of information.\n\n## How do we Operationalize Analytics’ Insights?\n\nThe first thing to understand about the data pipeline is that it is not like an enclosed electrical system that you can set up, walk away from, and six months later flip a switch and know the bulb will light up. A data pipeline is different in that the data is variable; it can change or degrade, for example, so you will not necessarily be rewarded with the bulb lighting up at any point, or in this case, the insight you are looking for. You always have to be thinking of what can go wrong in the system and how to correct these short-circuits.\n\nDetecting abnormalities is an integral part of the pipeline. You can’t plan for everything, so you at least need to be able to see when something has happened that is outside the bounds of the original analytics. An example is the COVID crisis, an unpredictable event that caused patterns well outside the norm for many systems. Another example would be a company producing widgets. In order to know how many widgets to produce, the data pipeline contains customer demand, current supply, and yield loss. These may be fairly stable over time, but there could be, for instance, a PR event that causes customer demand to explode. That could have a big impact on the models. Machine learning and deep learning look at familiar patterns, and if they have never seen these patterns before, the models are going to fail or degrade. You have to stay on the edge of discovery.\n\nThe only way to stay on the edge of discovery is to have your data pipelines automated for timely access to information. This is the competitive advantage: current and insightful data that can help you quickly resolve your questions.\n\nIT teams and data teams need to collaborate on automation and determine what should be automated for incoming data, and on managing any changes to the model that the data scientists want to make so it can be easily integrated back into the workflow.\n\n## Deployment Short Circuits \n\nThere are two types of controls that can prevent deployment short circuits: Analytic system controls and organizational controls.\n\nAnalytic system controls is about putting the models that you have trained to work, feeding data through to easily answer your questions. These deployed models must be moderated to check for accuracy of the data. Many things can cause the data to be adversely affected such as environmental change, machine calibration, distribution problems, and so on.\n\nThis isn’t so different from the software development world where changes can affect predictions. The IT department is familiar with the process of running tests to make sure that their models or applications are running according to established guidelines, so dev ops and data scientists should take advantage of these resources and knowledge. There is no need to invent any new process, but instead, the groups should blend resources together to set themselves up for success.\n\nOrganizational controls go back to having an organizational foundation which is committed to being data-centric and providing the right people and resources to work together for common goals. The best chance for obtaining operationalization is when there is collaboration, trust, understanding of needs, and feedback loops among the groups in the organization.\n\nFeedback loops are critical in this process. For example, subject matter specialists can provide information about market dynamics so the data scientists can monitor the model for these changes in data. If a model is going to be used over time, it will constantly need to be iterated and improved.\n\nConsumers of the data should have a dashboard the gives them information and allows them to dig into why something looks a little bit off.  The more they can investigate, or bring up what needs to be investigated, the more empowered your organization is going to be.\n\n## Pipeline \n\nOne key from the IT side to help operationalize the data pipeline is to use a version control such as GitHub so you can have access to previous versions of your model. For auditing purposes, the ability to store the data that created the model and other historical data is important as well. You want to be able to look at the patterns and see how a certain feature changed or impacted the model. You can also plug historical data into your new models to see how much it impacts your current data.\n\nFor example, some systems will have a skewed picture with a great number of people working from home during COVID. A case in point is the Navy. Since COVID, 95% of their IT workers are remote, and their productivity has gone up 35%. From that one data point, you could say, everyone’s going to work from home from now on. Will you continue to get a 35% increase, or if people come back in the office, will you see a 35% decrease? Obviously, that one data point is not necessarily sufficient to predict actual productivity.\n\nAnother tool IT can offer is continuous integration and deployment. Using Jenkins or GitHub Actions or a similar tool when working on a model, you can automatically run tests against your model with your data or generate garbage data on the fly.\n\nThe IT people and data scientists need to collaborate on what and how to monitor the output of the models. IT can monitor output automatically, and they can also monitor how the models are performing in the infrastructure. One example is that IT, with automated continuous integration deployment, can quickly alert the data scientists when a model is taking longer than the norm to keep it from spinning out of control. If IT is invested in data value creation, which has generally been missing in the industry as a whole, the process will be easier and more coherent for everyone.\n\nAnother aspect to think about is design of experiments because interactions among variables and features are important as well. Subject matter experts can help determine what the potential interactions are, and you can model those to help understand what variability can be expected.\n\nIt’s exciting that a data scientist can take raw material and turn it into insight. It does, however, take a team. The more everyone in your organization can learn from each other in a team environment, the more great things can happen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT25-en","image":"./episodes/edt-25/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel, talks to Sarah Kalicin, Lead Data Scientist, Intel about operationalizing your organization’s data pipeline. It takes a team effort to model, monitor, and produce an ongoing source of valuable information. This is the final episode in the series Kick-starting your Organizational Transformation to Become Data-Centric."},{"id":95,"type":"Episode","title":"Multi Cloud Data Solutions with Hammerspace","tags":["hammerspace","data","technology","dataarchitecture","multicloud","compute","multihybridcloud","datamesh"],"body":"\r\n\r\nStorage is ripe for disruption. Currently, managing data is done in a cumbersome, procedural, and often manual and error-prone way. Hammerspace was founded to solve this problem by abstracting data from the storage infrastructure.\n\n## Imagine For a Second….\n\n…if your data were untethered from storage infrastructure. Free from the limitations imposed by current storage systems, users could self-service their data management and protection, change the cost profile instantly, and access data from anywhere on the infrastructure. Software defined storage could scale out performance on demand and deploy modern workloads such as kubernetes on any underlying infrastructure anywhere.\n\n## The Challenge\n\nApplications have become portable, but data is still siloed. The challenge is that performance, reliability, and manageability all suffer at scale because of the silo problem. The solution is to release the data from the limitations of the underlying infrastructure. Hammerspace does this through metadata disaggregation, assimilating the smallest constituent parts to make data portable.\n\n## Untether Data from Storage\n\nWith this technology, your apps have on-demand data wherever you are. You have independent control, data planes, and a global namespace and file system that spans multiple data centers and clouds. Storage is orchestrated; data is fully automated and leverages declarative autonomic data management. In other words, it separates the “how” from the “what,” declaring the desired end state without having to outline every detail of how to get there.\n\n## Hammerspace Architecture\n\nThe system allows you to operate at a file granular level, provides enterprise-class data services such as snapshots and clones, and easily moves to massive scale from data center to data center, from data center to cloud, and back to data center to disperse it in a multi-hybrid cloud scenario.\n\n## Legacy Storage is Unable to Overcome Modern Challenges \n\nLegacy storage does not sufficiently scale, even with horizontal or cloud or planet scale up solutions because even those clusters become silos, sometimes leaving you stuck with data that cannot be correlated and analyzed. Traditional storage also has a hard time scaling capacity and performance independently.\n\nReplication is an ancient technology that leads to copy-data sprawl. Instead of moving the smallest constituent parts, you’re moving the entire payload. Data management is often an afterthought; data management belongs in the front, not the back.\n\nOne problem Hammerspace architecture solves is finding things easily. Versioning can lead to big business problems. For example, both Airbus and Boeing have run into massive problems because some engineers did not have their most updated versions. The solution is orchestrating the data.\n\n## Data Orchestration\n\nOrchestration, first, is the decoupling of all the different silos; the data is treated as a single pool. Hammerspace assimilates the smallest constituent parts, the metadata, to create, essentially, a data anti-gravity system. Then they implement objectives such as durability, availability, and snapshots, or custom defined actions that can be done through hammer script. Finally, data, whether it is kubernetes, NFS, or SMB, becomes portable and ties into the system.\n\nWhat this means in action is that you do not need to go to another silo to service a particular workload. Data is delivered where you want it to be. This data mobility is key because it is live; it is not data migration, which is disruptive and causes down time.\n\nTo minimize expensive egress costs, data is deduplicated and compressed on a file granular basis. Instead of moving an entire fixed volume, you can pick out the data you want to move based on any kind of expression such as folders, metadata tags, or a customer descriptor. This offers flexibility and cost savings.\n\n## Hammerspace Architecture \n\nIn the Hammerspace architecture, the global file system has three components from a high level: the global file system itself, the front end presentation (NFS, CSI Driver, and SMB), and Anvil within the global file system. Anvil is the metadata management component and DSX, which provides metadata management services. These can be implemented as virtual machines, VMware, KBM, or Hyper-V. Anvil is in an A-shaped configuration so there are at least two of them at each location. DSX can be parallel-ized to scale out performance, so you can have a number of these at different locations to make sure you have enough performance. They can be scaled down easily.\n\nOn the back end, the underlying storage can be Hammerspace’s own software defined storage with directly attached disk, assimilated NAS, any cloud, or any combination. This can also scale horizontally, so you can now scale performance and capacity independently. Following the cloud model, it’s also elastic, so should the business change at that particular location, you can scale back performance as well as capacity to make sure applications have just enough of what you need at that location. This all makes for a very flexible architecture to serve any of the application workloads on the front end.\n\nOne great advantage about this flexible architecture is the ability to assimilate data that’s being stored in devices that are not yours, such as NAS or in the cloud. This simplifies moving data. For example, if you have an older NAS and you want to migrate to a newer NAS, it doesn’t matter whether it’s the same vendors or different vendors. Hammerspace assimilates the metadata and moves the data behind and completely transparently to the applications because it’s live data mobility. Another significant advantage is that there is no downtime in moving the data.\n\nIf you would like to try out this technology, go to hammerspace.com and get started with a free trial with a license for up to 10 terabytes deployed in Azure, AWS or Google Cloud. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Johan Ballin"],"link":"/episode-EDT26-en","image":"./episodes/edt-26/en/thumbnail.png","lang":"en","summary":"Johan Ballin, Director of Technical Marketing at Hammerspace and Darren Pulsipher, Chief Solution Architect, Public Sector, at Intel discuss Hammerspace’s hybrid cloud technology that untethers data from storage infrastructure, providing data portability and performance."},{"id":96,"type":"Episode","title":"Hardware Security: Imperative for Transforming Data","tags":["datagovernance","data","sgx","confidentialcomputing","cybersecurity","technology"],"body":"\r\n\r\n## Data Security Landscape\n\nThe world has become incredibly connected with all the devices, and this is driving an exponential growth in the amount of data that we need to manage: the more volume, the more risk. It’s a challenge, and leveraging new computing capabilities like cloud, analytics and edge computing drives additional complexity.\n\nThere are three major trends in security: encrypting everything, workload isolation, and a chain of trust. These three areas are important to help organizations deal with increasing regulatory requirements to keep data protected.\n\nWhen organizations decide to make a digital transformation, or with a catalyst such as COVID 19 that necessitated remote workers, for example, security needs to go hand in hand to keep security controls and compliance in place. If you address security along the way with your deployments and implementations, the more you are going to be able to transform the way you do business in a sustainable fashion.\n\n## Protect Data Throughout its Life Cycle\n\nEncrypting data is important through all its phases. Historically, attackers could get access to data right across the network. After that became encrypted, they started breaking into the data centers and pulling data out of the databases. So then we started encrypting storage. Still, the data arrives at an endpoint and gets pulled into memory and decrypted, so a sophisticated attack that could get root access could potentially grab or manipulate the data at that point of processing. This attack on data in use is the new frontier for attackers and for those defending the space. Intel has some exciting and innovative new capabilities that we are building into our processors to help data owners, application owners, service providers, and basically the whole ecosystem to close those potential vulnerabilities.\n\n## Why Protect Data in Use?\n\nIn many cases, attackers are using privilege escalations at the operating system or Hypervisor layers. They could be coming through a guest OS, a host OS, or even physical hardware access on the server. The attackers could be classic hacker malware-type intrusions, third party competitors, or insiders such as an admin or system admin at a service provider.\n\n## Intel Software Guard Extensions (Intel SGX)\n\nIntel has built a new technology into our processors called Intel Software Guard Extensions, or SGX. This is part of the confidential computing paradigm that’s exploding on the market right now as businesses are trying to transform their data and keep it private. SGX addresses these newer types of attacks by allowing the application to speak directly to the processor in the encrypted memory, bypassing the operating system, the Hypervisor, and essentially everything else on the system. So, even if you had a breach in your operating system, Hypervisor, or other applications, attackers wouldn’t be able to access that data because the operating system itself doesn’t have the visibility to that data. Therefore, you don’t have to trust the provider or the rest of the system stack because you basically operate as if they are compromised already, and your code and data is going to remain confidential and unchanged; it will have integrity.\n\nIntel is trying to make it so you have to trust the fewest number of components possible. SGX goes the farthest down that path for the data center than anything else we’ve seen. All you have to trust is your own application and the processor itself.\n\nSGX offers a powerful capability for businesses such as cloud service providers, who can tell their clients that they, or the government, for example, couldn't get access to your data even if they wanted to do so.\n\n## Intel SGX in Action\n\nSGX is already used broadly by cloud service providers and software vendors, but in some ways, we are just getting started. Although this technology has been out there for a number of years, we’ve built in ecosystems, and we’re bringing new capabilities in our upcoming third generation Xeon Ice Lake processors. This will expand its capabilities, its ability to scale to large enterprise workloads, and it is going to be able to protect much larger chunks of memory with better performance and across a much broader footprint in the mainstream data center.\n\nIntel is one of the founding members of the Confidential Computing Consortium, which is part of the Linux foundation. Most of the big cloud service providers, many software providers, and even our other silicon competitors are working together on these types of trusted execution environment solutions and building standards for handling this type of capability. We are also bringing awareness of the necessity and the business value of confidential computing.\n\n## Intel SGX Software Partner Ecosystem\n\nSGX provides plenty of options for use depending on what the data owner is looking for. If a client wants the most granular level of control, SGX is going to allow them to do that. In fact, they can essentially pare down their application to just the coded data that they want to keep isolated from the rest of the system, or even just a part of that application. This scenario, however, does require them to write the application for that purpose.  In the ecosystem, there are open source resources that are making this type of development a lot easier, and it’s always expanding.\n\nOn the other side is a fast path lift and shift. You can take your application and drop it into a more secure environment. The ecosystem is responding and creating SGX-aware containers. You could drop your unmodified application into that environment where it’s the only thing running, so the application itself thinks it is running in its native environment.\n\nOur ecosystem partners have made a lot of progress in this space with things like Fortanix, Graphene, and Scone. Some are open source and some are proprietary, but come with all the services already baked in. For example, Microsoft Azure Confidential Computing offers the whole range from lift and shift solutions with SGX all the way down to SDKs that allow you to develop your application directly for it and to land it in their environment so you don’t even have to manage the hardware. There is a full set of options, so no one should be afraid of the complexities of SGX. Clients should also have confidence that sensitive material such as machine learning algorithms or encryption keys are going to be handled with a very granular level of protection.\n\n## Confidential Computing: A Security Game Changer\n\nSGX has been out there for some time, and we’ve been working to expand it. It’s been tested and put through the wringer, with hundreds of research papers and hardening over time with updates. It has the advantage of not being the new kid on the block, rather a foundational solution that is being brought into the mainstream with Ice Lake. It’s no longer focused on small, sensitive areas, but ready for the big stuff now.\n\nGovernment, financial services, and health care are some of the industries that saw the appeal of SGX early on because they have a lot of regulatory expectations and privacy requirements, yet they’re trying to share data and do innovative things with multiple parties. Enterprise has similar situations, for example, if they want to move to the public cloud, yet don’t trust it to protect their sensitive data. With SGX, they don’t have to trust the provider.\n\nIntel just had a big announcement this month and we are really blowing the doors open on the things we have coming up. A great place to start is intel.com/sgx for testimonials and a deeper dive into the information. Customers should be looking for the ecosystem partners such as Azure and Fortanix. Another place for information is the Confidential Computing Consortium because of the number of people working in that space.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jesse Schrater"],"link":"/episode-EDT27-en","image":"./episodes/edt-27/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Public Sector, Intel, and Jesse Schrater, Security Manager, Data Platforms Group, Intel, discuss the current security landscape and how Intel’s SGX and partnered ecosystem offers a timely and tested solution for data in use and other security concerns."},{"id":97,"type":"Episode","title":"Legacy Integration with Cloud and RPA","tags":["rpa","bpm","automation","compute","technology"],"body":"\r\n\r\nIt seems like everyone is in the middle of a Digital Transformation. Private Cloud, Public Cloud, Multi-Hybrid Cloud, Data Lakes, Machine Learning, Inference, and Artificial Intelligence are all terms that people are using today to describe their digital transformation, but what about Legacy integration. No one ever talks about Legacy Integration. Why? Because it is challenging to integrate Legacy applications, data, and security into your new pristine Multi-Hybrid Cloud environment. That is one of the last things we want to concern ourselves. If we don’t develop a Legacy strategy, then we have a speedboat with its anchor down. It slows us down and keeps our boat stuck in the harbor.\n\nOne of the growing areas to help with Legacy Integration and automation of integration is the use of automation tools and frameworks. Over the last 3 years, a significant emphasis on the automation of workflows with legacy and new cloud-aware applications for information workers has emerged. These tools sets are called Robotic Process Automation (RPA) tools.\n\n## Robotic Process Automation (RPA)\n\nWhen I first started investigating Robotic Process Automation (RPA) tools, I thought they controlled robots. I was all ready to get my steel-toed boots and a hard hat and visit manufacturing facilities. However, I quickly found out that RPAs mimic the way that information workers work with the different tools they use. Some of the tools are Legacy tools, and some of those tools are new modern applications. RPAs record how the information worker uses the User Interface of the different tools and then allow the recording to be played back, automating the Information Worker workflow.\n\nSurprisingly RPAs are quite a mature technology. Many of the RPA companies have a heritage in the UI Test Tool marketplace. The ability to capture user interaction with multiple applications over time is critical for developing a repeatable User Interface Test. These tools have been around for over 20 years and are quite mature. These UI QA Test Tools have been rebranded and repurposed for Information workers that want to automate their repeatable redundant tasks.\n\n### Current Market Place – 2020 \n\nInvestors see this market as a hot market and have invested heavily in these technologies. Over the last three years, over $2.0 Billion has been invested in the RPA marketplace. Three companies have taken a majority of the investment:\n\n* UiPath - $1 Billion investment on $300 Million in Annual Revenue\n\n* Automation Anywhere - $500 Million investment on $100 Million in Annual Revenue\n\n* BluePrism - $50 Million investment on $30 Million in Annual Revenue\n\n* \n\nMost of the investment has not come from traditional high-tech Silicon Valley, but instead from the Financial centers like New York and London, indicating that financial institutions are looking at RPAs to automate many of their own information workers’ workflows and processes.\n\n## Place where RPA works well\n\nThe first vertical segments to adopt RPAs have been the Financial, Insurance, and Medical industries. These industries have looked for ways to decrease variability, increased reliability, and decreased cost. Due to a large number of information workers in these industries, they have looked to RPAs to automate much of the work their Information Workers are currently doing. To automate these workflows organizations need to understand how these workflows get created.\n\nFirst, let’s understand the information worker. Many of the information workers spend time working with multiple applications, tieing information, and applications together in an ad hoc way. As these workers continue to work with these applications, they organically create workflows coupling data and applications together in an innumerable number of permutations.\n\nSecond, Catalog the workflows as best as you can finding candidates for workflow elimination through duplication and redundancy. Now that workloads are understood the next step is to prioritize and enumerate the workflows. Focusing on the most used workflows with the most significant number of steps tends to be the best way to prioritize workflows.\n\nLastly, figure out how to automate the workflows with RPA bots. The automation can be done through UI recording the workflow from one of the information workers and annotating the workflow with variations based on data entry and security credentials. Once the recording is complete, an RPA bot is created to automate the workflow. Now you need to decide how you want the RPA to run: Attended or Unattended.\n\n## RPA Modes of Operation\n\nRPAs run in two basic modes of execution. Attended and Unattended. Attended means, it runs on the Desktop or Laptop of the information work. It aids the information worker by automating the work that they do day-to-day. Unattended runs in a Virtual Desktop environment and are typically kicked off via an event or trigger and runs without any interaction with the information worker. There are benefits to running in both modes as described below.\n\n### Attended\n\n* Handles tasks for individual employees\n\n* Employees trigger and direct a bot to carry out an activity\n\n* Employees trigger bots to automate tasks as needed at any time\n\n* Increases productivity and customer satisfaction at call centers and other service desk environments\n\n### Unattended\n\n* Automates back-office processes at scale\n\n* Provisioned based on rules-based processes\n\n* Bots complete business processes without human intervention per a predetermined schedule\n\n* Frees employees from rote work, lowering costs, improving compliance, and accelerating processes\n\n## How to integrate RPA in your Enterprise\n\nTo understand how RPAs fit into your Enterprise, you have to first look at the users of the RPAs. Specifically, there are three types of “actors” that use, manage, or influence the RPA tools.\n\n* Information Worker – This is the primary user of the RPA tools. Their manual processes are targets for automation.\n\n* Application Developer – RPA bots change when applications are updated or created. Changes to User Interface require “re-recording” the RPA bots.\n\n* IT Operations – Manage the RPA tools and deploy unattended RPA bots.\n\n* \n\n### Managing Change\n\nManaging the complexity of configurations and security are critical factors to a successful deployment of RPA tools and bots. First, you need an understanding of how the different users of the RPAs interact when changes to applications, workflows, and processes. This understanding critical to managing change in the RPA bots and the toolsets they use.\n\nSmall changes to applications can have a profound effect on Information Workers and how they perform their day-to-day work, which in turn means recording a new or updating an existing RPA bot. Because of the coupling of RPA bots to toolsets and workflows,  creating RPA bots when the workflows or toolsets are immature causes unnecessary churn and fragility. Mature processes and toolsets are great candidates for RPA automation.\n\nAnother thing to consider is where are the tools running that you are automating with your RPA tool. Do they use Legacy applications and infrastructure? Are they using Public or Private Cloud? How are the networks of these systems connected? As the number of environments increases, so does the complexity of maintaining and updating applications and RPA bots. Find ways to decrease the number of environmental boundaries the RPA bot is traversing.\n\n### Managing Security\n\nAnother critical factor to consider is security for the RPA bots. When an Information Worker records their workflow, they need to authenticate (log in) to each tool they are using. Workers authenticate using usernames and passwords, authentication keys, or even Corporate Single Sign-On Tools. Either way, you need to manage the security of these tools in the context of the RPA bot at execution. Any changes to authentication (username, password, auth keys, or credentials) require changes to the RPA bot. Many of the RPA tools consider this and have mechanisms to inject security credentials into the RPA bot and authenticate with the tools at runtime.\n\n## Managing RPA tools and bots with SecDevOps Workflows\n\nThe complexity of RPA tools and bots lends itself very well to well-known patterns in the SecDevOps world. Luckily, many of the problems with managing configurations and dependencies are handled well with a SecDevOps process.  \n\n### RPA Bundling\n\nOne of the tricks is to treat the RPA bot as a complex-service that contains several VMs or containers for each of the tools, a Virtual Desktop, and the bot itself. These services can be bundled together and managed together like one package. A bundle includes not only the services but how the services communicate (network) in a secure manner (Authentication).\n\nPassing a bundle to a service orchestrator allows for greater automation of network firewall management, security, and credential key injection and lifecycle management of the RPA bot and the tools it consumes. There are several tools in the Virtualization space (VMWare, and OpenStack) that allow for the creation and management of these bundles.  The container space has similar scheduling and orchestration tools as well: namely Kubernetes, Mesos,  and Docker Swarm.\n\n### SecDevOps Pipelining\n\nA simple SecDevOps pipeline manages the RPA bot bundle just like any other traditional application bundle.\n\nAn Information Worker builds the RPA bot bundles by recording the User Interface workflow in a development environment. The worker easily records their workflow and then creates a bundle that gets “Checked In” to the pipeline. At that point, the RPA bot bundle moves through a build, test, and production cycle. Checkpoints at each step along the way, help guarantee the quality of the RPA bot. Because the bundle can inject network and security depending on different environments, the RPA bots can be reused by different Information Workers and in different environments.\n\nAnother benefit of putting RPA bots into RPA Bundles is the management of the tools and bots across multiple infrastructure environments like legacy, private, and public clouds. Many of the Service orchestration tools can automatically create connections between these infrastructure environments through creating an overlay network. The pipeline decreases the amount of “hands-on” work done by the IT organization, and in many cases, all of the steps in the pipeline are automated.\n\n## Pitfalls of RPA bots\n\nHere is a list of somethings to watch out for when using RPA bots in your enterprise systems.\n\n* Security can be a gaping hole if you don’t pay attention to it. One of the biggest mistakes is running applications in an RPA bot in privileged mode or with “global” account credentials.\n\n* RPAs bots tightly couple to User Interfaces of multiple applications, any small change to an application means you need to re-record the RPA bot.\n\n* RPA bots cannot hand change very well they are very brittle to change in applications and even configuration of applications.\n\n* Reuse is minimal due to the tight coupling with the application user interfaces. Some tools use tags instead of the absolute position of cursor and clicks.\n\n* Some User Interfaces do not allow themselves to RPAs because they are dynamic. Which means they are hard to record.\n\nThe RPA industry is trying its best to overcome some of these issues inherent with the record/reply aspect of the tools. Some of these pitfalls cannot be overcome because of the generalized approach. Other options, like API gateways and functional automation, should be evaluated.\n\n## AI to the rescue of RPAs \n\nAs mentioned in the pitfalls of the RPA, bots reuse is a big problem that the industry is looking at fixing. One of the techniques they are investigating is the use of AI and Inference to handle dynamic user interfaces and small changes to applications without re-recording RPA bots. Pattern recognition and Optical Character recognition are two areas that are being used to train AI models to be used to identify fields and segments of User Interfaces.\n\nWith these AI models, bots can be more flexible lending themselves to reuse across multiple toolsets, and similar processes/workflows. Another area that RPA vendors are investigating is process optimization using AI and ML.\n\n## Legacy Migration is a journey\n\nThe RPA marketplace has caught new energy as companies are looking to modernize their IT infrastructure and processes. Automating current manual processes through recording is a quick win that many organizations are getting benefits. However, RPA should be considered a stop-gap mechanism instead of the end state. Why? Many of the current information processes require legacy systems and policies. Automating an old process on new infrastructure is similar to automating the creation of buggy whips for an automotive factory. There may be a benefit at first, but in the long term, the process is highly inefficient and antiquated. No matter how fast it runs reliability, it just may not be needed.\n\n## Conclusion\n\nRobotic Process Automation tools are another set of tools that can be used to help organizations with their digital transformation from Legacy to more modern Computing infrastructure and processes. The tools by themselves are not enough, and you need to plan how you are going to use, manage, and eventually replace them. Here are some helpful tips when working with these tools.\n\n* Treat RPAs as Complex Services running in your Multi-Hybrid Cloud\n\n* Run your RPA bots through SecDevOps Workflows like other applications.\n\n* Inject Security and Auth at runtime into the RPA tool.\n\n* Find ways to reuse RPA bots in different parts of your organization.\n\n* Have a plan to replace your RPA bot with a simplified integration\n\n* Look for ways to decrease the Legacy applications (Replace or Remove)\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT28-en","image":"./episodes/edt-28/en/thumbnail.png","lang":"en","summary":"One of the growing areas to help with Legacy Integration and automation of integration is the use of automation tools and frameworks. Over the last 3 years, a significant emphasis on the automation of workflows with legacy and new cloud-aware applications for information workers has emerged. These tools sets are called Robotic Process Automation (RPA) tools. Darren Pulsipher, Chief Solution Architect, Public Sector, Intel, reviews the Robotic Process Automation (RPA) industry and tool offerings."},{"id":98,"type":"Episode","title":"Next-Generation Hyperscale Database - Aerospike","tags":["aerospike","database","ingestion","optane","data","technology","pmem"],"body":"\r\n\r\n## Next-Generation Hyperscale Database\n\nDatabases are showing their age, still taking some time to get results. Aerospike, with the help of Intel technology, breaks through with speed, volume, and low latency. Tim Faulkes, Vice President of Solutions Architecture at Aerospike, joins Darren on this episode to discuss the benefits of their technology.\n\n## NoSQL – An Emerging Market with Multiple Technologies\n\nAerospike was created in 2009, with the first production deployments in 2011. The cofounders were mindful of upcoming challenges. They built the architecture from the ground up with the framework that it would be reliable, enterprise scale, never go down, and be able to handle mass ingests of data. Aerospike’s architecture relied heavily on SSDs, whether NVMe drives in the modern incantations or SATA SAS drives.  Both were in their infancy, so the founders were forward-thinking in where the market was going. They relied on those devices to get the speed, and that’s why Intel technology was so important. Since then, other technologies from Intel such as the persistent memory, have made things a lot easier.\n\n## Indexes in DRAM, Data on SSD\n\nAerospike approaches databases differently. The SSDs are not a faster hard drive. A hard drive has physical sectors and heads and things it has to move to read data. An SSD can quickly read thousands of pieces of data in parallel. It doesn’t run well on rotational drives, but with NVMe/SSD, it runs blazingly fast. No one has the same sort of performance, not even in memory database.\n\nThe unique architecture is designed for high volume, high throughput, and very low latency. For example, some customers are regularly doing 20 million transactions per second. Some are going up to petabytes of data. Typically, customers run on fairly nice hardware but round trip time is critical.  It takes Aerospike about 200 microseconds to look up a piece of data and bring it back to the client, not tens or hundreds of milliseconds. When you need a lot of data fast such as in fraud detection, or if you are ingesting a lot of data, such as in IOT, autonomous vehicles, sensor readings, or medical devices, Aerospike can do those millions of transactions per second across reads and writes. You don’t need to put it into a message bus and then let the database notify downstream systems via the message bus that it’s got the data. It’s already persistent.\n\n## Powering Industry Leading Innovation Around the World\n\nWhat use cases are sweet spots for Aerospike? There are obvious markets such as the previously mentioned IOT, fraud detection, and sensor data, but it’s an emerging market. Aerospike started in advertising technology, where data retrieval has to be done in milliseconds or faster. This is where the good performance was battle tested at a high scale.\n\nAdditional industries where Aerospike can be useful are varied. One example is a large telco in India. The infrastructure there is not always reliable, and sometimes calls are dropped. This company, with Aerospike technology, can tell, in real time, that a call has dropped, and they can immediately reach out and offer the customer credits or other compensation to maintain customer satisfaction.\n\n## Real-time Settlement of Instant Payments\n\nA new area that Aerospike has become heavily involved with is real-time digital payments. This doesn’t have specifically a lot of data, or high throughput requirements, but what these companies need is absolute consistency and absolute availability, even in the face of, say, losing a data center.\n\nThe manufacturing floor is also an emerging market for Aerospike technology in things like semiconductor manufacturing where it’s used to store sensor data to provide real time analytics. Aerospike shines in any industry where there is a lot of data and you need it quickly.\n\n## Performance at Scale – Independent Third Party Test Results\n\nThere are many databases with good technology out there, but there are challenges of scale. Take Redis, for example. Because it stores all of its information in memory, you can’t go up to ten terabytes without significant cost, let alone hundreds of terabytes or petabytes. Since Aerospike stores the information on SSDs, the time difference between looking up data on SSD and memory is about 100 microseconds. Across the scale of petabytes, Aerospike replaces older technology such as Cassandra, which scales really well, but lacks speed.\n\n## Total Cost of Ownership\n\nIn addition, compared to those technologies, people save a lot of money moving to Aerospike because the number of nodes drops down dramatically because of the unique architecture. The savings is in not only in capex, but in opex because there are fewer machines to oversee.\n\nAerospike has collapsed some of the traditional tiers in the architecture. Often legacy systems will have extra cache in front of it to speed it up. This introduces complexity. Aerospike does not need a cache. There isn’t a large amount of DRAM. It relies on the speed of the SSDs and the underlying technology to get the performance of raw storage without a cache. Since the cache and storage tiers are collapsed, the solution is simplified, which means built-in reliability and speed.\n\n## Continental Deployment Example\n\nConsistency across multiple geographic areas is also an important benefit of the technology.  Modern architecture requires low latency, so that typically means there are a bunch of H-based clusters where the data sits close to the user. Otherwise, the speed of light becomes a factor. Aerospike can have all of the H-based clusters talking to each other, so if a record is changed in one cluster, it will automatically propagate to the other clusters, asynchronously. With some uses, however, such as digital payments, there must be strong consistency between the clusters, so they might want to replicate the data synchronously instead. They would rather read from the local copy of the data. The reads become very fast; the writes are affected by the speed of light, but they guarantee consistency across geographical distances. So, this isn’t conflict resolution, it’s conflict avoidance. Being able to spread those strong consistency writes around the world and maybe have multiple systems of records has enormous potential and value.\n\n## Aerospike Connect for Spark\n\nFor example, Aerospike currently interfaces with Spark, an analytic technology that requires its data to be in memory. By loading the data off Aerospike, the Spark data frame can process tens or hundreds of terabytes with enormous cost savings and speed. Integration with other AI tools is one driving force that will open some exciting doors.\n\nGetting data into Aerospike is simple and straightforward. Since applications have an API layer like most databases, Aerospike can ingest from industry standard sources such as Kafka and Janus. The bi-directional ability to talk to message buses makes sense, because not only can Aerospike ingest data from these sources, but egress them. If Aerospike is a source of truth when you write a record, you can push it to downstream systems.\n\n## Aerospike Connect for Kafka\n\nIf you want to use Aerospike for ingestion rather than an ingestion engine on the front end, Aerospike can determine whether it wants to break the data up or keep it in the same format and just put it into Aerospike; With its plugin framework,  it’s an API call. When the data is stored a message bus can be notified for downstream processing. This eliminates one of the ingestion steps and increases total throughput.\n\nScaling to thousands of clients is possible due to the way the Aerospike client is designed as multi-threaded to scale on one process and ingest a lot of data with that.  The client is smart for the sake of speed. Any record goes directly from the client to the node that owns that piece of data. There are no middlemen of kind. That means the client must know about all the nodes, so there are finite connection limits, making hundreds of thousands of clients impractical.\n\n## Continental Deployment Example\n\nIf, however, you’ve got hundreds of thousands of clients, it’s because you have a globally distributed population. If they were talking to one cluster, the speed of light becomes a big issue. There are ways to set up smaller, edge-based clusters that either share information or they talk to a system of record in more of a hub and spoke model, so it can be done.\n\n## Aerospike is a Database for System of Record Applications\n\nAs data moves around the world, different layers of privacy come into play and standards such as GDPR and CCPA, so customers must know the origin of the data. Aerospike has the flexibility to look at the data with a fine grain, and based on the content of the data, know when and where it can be shipped. This is part of Aerospike’s data shipping configuration that is transparent at the application. You don’t have to change anything in the applications, only in the configuration.\n\nAerospike also makes sure the data is secure. Just like all enterprise-strength databases, Aerospike supports encryption at rest and in flight, and security integration of things like Hashicorp Vault. It can be, and is being used, as a system of record.\n\nA foundational goal at Aerospike is using the latest innovations and technology for customers’ success.  For example, when Intel came out with PMem, they used it in app direct mode to use it for its full capabilities. This means for a rolling upgrade on say, a petabyte of information, the time it takes is small. Aerospike holds that petabyte in maybe 50 or 60 nodes instead of thousands of nodes, and when a node is shut down, it doesn’t have to be rebuilt, so the restart time is very fast.\n\nOverall, Aerospike offers security, high availability, speed, low latency, scalability, consistency, and low cost of ownership. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Tim Faulkes"],"link":"/episode-EDT29-en","image":"./episodes/edt-29/en/thumbnail.png","lang":"en","summary":"Databases are showing their age, still taking some time to get results. Aerospike, with the help of Intel technology (Optane), breaks through with speed, volume, and low latency. Tim Faulkes, Vice President of Solutions Architecture at Aerospike, joins Darren Pulsipher, Chief Solutions Architect, Intel, to discuss the benefits of Aerospike technology."},{"id":99,"type":"Episode","title":"Benefits of Multi-Cloud Architecture","tags":["multicloud"],"body":"\r\n\r\n## Agility\n\nAgility is the ability to move quickly to adapt to changing conditions in new technologies, new applications, and new threats such as security and competition. A multi-hybrid model allows the ability to move things among private and public clouds, or even from legacy infrastructure to clouds. It may be tempting for development teams to initially use the public cloud under tight deadlines because they can quickly spin up and spin down infrastructure, but there can be integration problems in the late stages of deploying applications into product. This can result in costly delays. By using the multi-hybrid model, many of these integration points are exposed to the development team early. Application development uses this to fail fast and early in the development process.\n\n## Flexibility\n\nFlexibility in infrastructure is a close relative of agility. Where agility is the ability to move quickly, flexibility is the ability to change. For maximum flexibility, developers need the ability to deploy an application anywhere across private or public clouds or legacy infrastructures, and not be locked into any one cloud’s service or infrastructure.\n\nBy deploying a cloud management platform (CPM) in a multi-hybrid architecture, workloads can be easily redirected to different clouds according to cost, security, and reliability.\n\nAn example of the consequences of lack of flexibility is Netflix’s early decision to use only one cloud service provider. This public cloud had an infrastructure issue that resulted in downtime, and Netflix was unable to stream for several hours on the east coast of the United States. After that disaster, Netflix built in a multi-cloud solution so that they can quickly migrate to another cloud if there are issues. In addition, they are now able to move where it makes the most sense at any time according to cost, security, and reliability.\n\nThe applications of a multi-hybrid cloud are portability and operational flexibility. By not being tied to one cloud’s way of doing things, developers will write code that can easily be transported between clouds. In addition, you will have the operational flexibility to move workloads between clouds to offer your customers what they want in a secure, cost-conscious, and reliable manner.\n\n## Predictive Performance\n\nPublic clouds often come with a price: “noisy neighbors,” which can impact predictive performance or Quality of Service (QoS).\n\nWhen using a public cloud, many times you do not know what or who else is running on the same machine, storage array, or network as you. For some workloads, this is not a problem. If you have an application, however, where you need more predictive performance, noisy neighbors, or those who monopolize bandwidth, disk I/O, CPU, and other resources, can interfere with the QoS results you need.\n\nPrivate clouds can also suffer from noisy neighbors, but since you own the infrastructure and the applications are your own, you have the direct ability to manage them. A noisy neighbor on the public cloud is like living in an apartment building where you have limited options to deal with the partiers next door. On the other hand, noisy neighbors on your private cloud is like living in a house with unruly children that you can immediately manage by restricting resources.\n\nA hybrid cloud strategy gives you the ability to put “sensitive” workloads and applications on private clouds and other workloads and applications on public cloud infrastructure for cost and resource efficiency. Many hybrid tools give you the ability to characterize workloads with QoS requirements to aid in the automatic, optimal placement of workloads on different cloud infrastructures.\n\n## Security and compliance\n\nThere are some dangers with doing things in an automated way in the public and private clouds. However, if security is built into the multi-hybrid architecture, this automation becomes a benefit because it means a security profile can be imposed across all your cloud assets, whether they are private or public, in addition to legacy infrastructure. This common security profile is applied everywhere, and applications are deployed based on those profiles.\n\nFor example, in a private cloud, if you want a specific type of security, such as tying a certain application to a certain machine that only runs in that private cloud, it can easily be a requirement in the profile on a multi-hybrid system.\n\nIn a multi-hybrid system, there are also many great tools for auditing and monitoring your infrastructure. You can not only monitor what is happening in your private cloud but also the public cloud, which will alert you to malicious attacks that could potentially infect your private cloud or legacy infrastructure assets.\n\n## Efficiency\n\nThere are many conflicting ideas about efficiency. If you look at various total cost of ownership (TCO) calculators, you will find different answers about whether private or public clouds are the most cost-efficient. A multi-hybrid cloud solution can solve this dilemma for your organization through heightened visibility and dynamic provisioning.\n\nWith a multi-hybrid cloud architecture and an intelligent orchestrator, your orchestrator can use telemetry from your private and public clouds and legacy infrastructure to make optimal decisions about where the workload should land currently and in the future. Another benefit of this visibility is that you can decide whether it’s efficient for applications to run continuously. An example of this is how we helped the Canadian government’s system to run more efficiently by cutting costs associated with running an application in the public cloud when no one was using it. Instead of running a specific application around the clock, they are now running it 18 hours, 5 days a week, according to actual use.\n\nThis visibility will also help you identify and eliminate end of life (EOL) workloads and applications, which will save real money. For the private cloud, this frees up resources that can be utilized for other workloads, in turn, driving up your efficiency. In addition, cloud brokers in the CMP in the multi-hybrid architecture will basically shop around for the lowest price while still maintaining the QoS for the specific workload. This decreases the overall cost of running the workload and also gives you visibility into your actual cost for using a particular public or private cloud.\n\n* Visibility into costs across Clouds and Legacy infrastructure\n\n* Drive workloads and applications to the lowest costs keeping the same Service level agreements.\n\n* Drive higher utilization of private cloud infrastructure.\n\n## Call to Action\n\nMulti-hybrid cloud architectures are giving CIOs the ability to get in front of the demands of their customers, but there is still some heavy lifting that has to happen. Building a multi-hybrid cloud strategy includes organizational, behavioral, and technical change that cannot happen overnight.  Developing a strong architectural vision and roadmap are key to rolling out a multi-hybrid cloud strategy that can take advantage of multi-hybrid clouds’ strengths and prevent the thrash of the technical industries’ “shiny object” of the month inefficiency.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT3-en","image":"./episodes/edt-3/en/thumbnail.png","lang":"en","summary":"A multi-hybrid cloud architecture allows organizations to take advantage of the benefits of both private and public clouds, optimizing resources and cost efficiency. This model has five main advantages: agility, flexibility, predictive performance, security and compliance, and efficiency."},{"id":100,"type":"Episode","title":"Application Portability with OneAPI","tags":["aiml","devops","compute","technology","process","oneapi","aiops","developer","people"],"body":"\r\n\r\nWith oneAPI, Intel has created a unified software environment for development, geared toward data processing. It is data parallel programming based on an open source C++. Multiple kinds of libraries such as Intel’s MKL, DNN, and other open sources are part of the oneAPI tool kit, along with accessories such as a CUDA translator. You can develop software in oneAPI and then point it to the different libraries depending on what it is you are doing. It has extracted away the complexity of learning a different language for different architectures.\n\n## oneAPI Industry Initiative – Alternative to Single-Vendor Solution\n\nBasically, a software engineer can write code once and it will run on different processors:  CPU, GPU, FPGA, NPU, and VPU. Depending on the architecture and libraries you are using, there could be a recompile, but no code rewrites are required.\n\n## Powerful API Libraries \n\nThis version is just the first step; Intel and others will continue to design with the addition of AI accelerators, for example. The idea is that it will evolve to give developers much more flexibility, and that abstraction will allow many people to be able to more simply design and code, especially from a data science and AI perspective.\n\nPractically speaking, a software engineer could write something on their laptop, try it out there, then use that same code and run it on a cloud fully loaded with neural processors, GPUs, or FPGAs. This could be especially useful in the public sector where engineers are writing special apps that process on the edge, maybe with an FPGA. They won’t have to have a full environment to do their work.\n\nAnother exciting aspect is that once Intel gets the machine learning built in, oneAPI could go through the code and specify which portions would be best on different processors. It would push the code out to the right places for the best speed and performance.\n\n## oneAPI Industry Initiative – Alternative to Single-Vendor Solution\n\nThere are a lot of AI frameworks out there, but oneAPI takes any kind of code migration from something proprietary to an open-source programming language. It is based on SYCL and developed under a whole industry consortium called Khronos group, so that’s a kind of development framework.\n\n## Powerful API Libraries\n\nOneAPI has twenty to thirty libraries such as MKL, libraries for neural networks and machine learning, open CNN or DNN. All are open, part of the larger consortium.\n\nIn addition, Intel is in the process of adding the tensorflow framework and libraries into oneAPI. Many frameworks have already been optimized by Intel and they’re being incorporated or using the same libraries so users can utilize it or build on to it.\n\n## Resources\n\nIntel made oneAPI generally available at the beginning of Nov, 2020, and it was a highlight of the Super Computing virtual convention November 17-19.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT30-en","image":"./episodes/edt-30/en/thumbnail.png","lang":"en","summary":"With oneAPI, Intel has created a unified software environment for development, geared toward data processing. Gretchen Stewart, Chief Data Scientist, Public Sector, Intel, discusses this technology with Darren Pulsipher, Chief Solution Architect, Intel, that eliminates the need for using a different language for different architectures."},{"id":101,"type":"Episode","title":"Improving Employee Value with Catalytic","tags":["rpa","automation","catalytic","compute","process","technology"],"body":"\r\n\r\n## Catalytic is Purpose-built for Operations and Recurring Business Processes \n\nRPA technology lineage comes from test automation and Excel macros, but the business lineage comes from outsourcing. Catalytic, however, views its lineage as software, with its system as the next evolution in creating custom software. Co-founder and CEO Sean Chou’s interest is in back office operations and efficiency.  That narrow focus continues to allow Catalytic to employ AI in a full way and allows them to build something that is accessible in no-code.\n\nCatalytic uses AI in a few different, tactical ways within the application to help reduce the need for expertise.\n\nFor example, one of the hardest parts of learning any platform is understanding its capabilities. One way to use AI to help offset this issue is by using natural language processing. People can just type in what they are trying to accomplish and the application can infer and say what action would be best help you accomplish this step. It’s basically a way to improve the product experience and decrease the level of expertise necessary.\n\nAnother example is that within the workflow, there are six different modules of different actions, each logically named for what it’s intended to solve, such as data processing, document assembly, etc. Within each module, there are different AI actions that people can use within their process, such as optical character recognition (OCR), sentiment analysis (is the person who wrote the email angry?) and other natural language processing actions.\n\nThere are other elements that are less AI and more utility actions such as providing information about a person based on their email address.\n\nA lot of data flows through back office processes. Unfortunately, the system view of the data is often very fragmented. The Catalytic workflow can cut across systems, capturing everything. The data is stored in data tables, and it is easy to build machine learning models based on those tables, so you can predict future outcomes of other workflows.\n\nCatalytic uses AI in these and other specific contexts to drive efficiency and provide insight and visibility.\n\n## Close the Last Mile of Digitization by Building Workflow Solutions around how People Work \n\nFor example, operational intelligence through AI is captured in the platform through a feature called Insights. For every workflow, this tab can tell you how long it takes for all the different steps, how often they break, and whether it’s a human error or integration failure. It gives you opportunities for improvement. At the same time, that machine learning algorithm can start learning to predict the outcomes of field values. So, machine learning can be used in multiple, related ways.\n\n## Despite Investment after Investment, There Still Exists a “Last Mile” Gap that is Filled Manually. \n\nMost customers who have invested in multiple technologies over the years still feel like there is a gap between the investment and the business outcomes they want. That gap is usually filled by people who are doing tasks that software ought to be doing instead, such as copying and pasting something from one system to another. Sometimes adding software to the mix complicates things by adding extra steps and training. The Catalytic solution to these inefficiencies is a system that is reusable and extendable.\n\nIt’s impossible for any off-the-shelf-software to meet the individual needs of each business, and it’s inefficient and time consuming to see the ultimate value. So businesses must meet the software where it is or adapt the software to meet their business. The value in a WordPress-type story is apparent. If the power is actually in the hands of employees, you can have truly personalized software. Catalytic wants to put that power in place for any business process using RPAs combined with actions. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sean Chou","Neil Bahandur"],"link":"/episode-EDT31-en","image":"./episodes/edt-31/en/thumbnail.png","lang":"en","summary":"In part one of two episodes, Sean Chou, Catalytic CEO, and Neil Bahandur, Catalytic Head of Partnerships, join Darren to talk about Catalytic’s technology and how RPAs can help employees become more valuable through automation of back office repeatable processes. "},{"id":102,"type":"Episode","title":"No Code (RPA) Approach to Back Office Efficiency","tags":["rpa","automation","catalytic","compute","process","technology"],"body":"\r\n\r\n## What is Catalytic?\n\nCatalytic is a no-code cloud platform for building workflow solutions that improve and automate your back office operations.\n\nA useful analogy to help people understand what Catalytic can do for the back office workflow is what Wordpress did for web publishing. Prior to Wordpress, creating a website was a complex task that involved multiple people with a variety of specialized skills. With Wordpress and subsequent, more advanced software, creating a website is a relatively simple task. A designer or content expert, for example, can easily create a website on their own.\n\nCatalytic’s goal is to similarly simplify the messy, complex systems of the back office and create efficiency and solutions via robotic process automation (RPA). Someone who has a bit of technical proficiency should now, with Catalytic’s technology, be able to independently create a back office solution.\n\nA common fear that RPAs costs jobs is false; it actually creates new, more valuable opportunities. For example, with Wordpress, there is now the role of Wordpress developer, so this progression has not diminished developers’ skill sets, but added layers of ways people can bring those skills into something more scalable. Back office employees can likewise use their skills to add value or improve customer experience rather than focus on mundane, baseline work inherent in inefficient processes.\n\n## Catalytic’s Evolution and Differentiation\n\nCatalytic differs from its competitors, first, by its roots. The current RPA industry came from two evolutionary lines, one stemming from Excel macro and the other from UI testing. Catalytic comes from the Dev Ops world, scripting back office processes rather than having a screen-oriented view of automation. When Sean Chou co-founded Catalytic, he thought in terms of project management and how to orchestrate business-as-usual processes.\n\n## Catalytic is Purpose-built for Operations and Recurring Business Processes\n\nChou realized that since 85% or more of business is business as usual, many operations could be automated, so the platform could accomplish some of those tasks. It began with a simple, automated notification system that replaced employees sending emails, which then sent Catalytic on a trajectory to create more and more actions. To amplify the capabilities of the platform, Catalytic created an ecosystem with third party partners such as Google to leverage their technologies. Through the ecosystem, there is also the benefit of composable business work where people can create a workflow on the platform and save it, essentially, as an individual unit of work. For example, if there are six standardized steps in capturing a customer record, you can create that once and save it as an action on the platform, then share it with your entire team. This sharing mimics a software developer’s approach to solving hard problems by sharing code, but instead of code, it is actions. Composing, then, becomes much easier.\n\nCatalytic has taken a thoughtful approach to provision, deployment, and management of everything on their platform. The cloud is not an afterthought; they created a cloud platform from scratch. Everything is centered around the cloud, although they also have the ability to work with on-prem systems and hybrid environments.\n\n## Catalytic is Designed to Enable an Enterprise-class Citizen Developer Program\n\nThe system was created in a dev ops environment and works as a build manager for the business, where business folks can actually do it themselves. Old build systems like Clear Case were hard to manage, and Catalytic has simplified things. It not only takes extra steps out, but changes up the division of labor. Catalytic targets people who are closer to the actual process to avoid handoffs. Efficiency is the core concept. An efficient build system that would detect bugs and reject them before trying to hand them off is critical to the success of the automated process.\n\nThe magic of the build systems is that they become more powerful as they pull in more pieces of your infrastructure. For example, it could interact with your SCM system to pull in the code, it could work with your servers to deploy, or it could interact with Rational Robot to do the automated testing. Of course, it all centers around the actions. The more actions are digitized, the more the system can capture, and the more powerful it becomes.\n\nAnother thing that differentiates Catalytic is the emphasis on employees doing high value work. The concept of low value and high value in RPA is common, but instead of using it as a sword to cut groups, Catalytic wants to use it as a shield to protect people to help them make the best use of their time and deliver a competitive advantage.\n\nOn the next podcast, Darren, Sean, and Neil will continue the conversation about the no-code approach to back office efficiency. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sean Chou","Neil Bahandur"],"link":"/episode-EDT32-en","image":"./episodes/edt-32/en/thumbnail.png","lang":"en","summary":"Today’s episode is part 2 of a conversation with Sean Chou, Catalytic CEO, and Neil Bahadur, Catalytic Head of Partnerships. They talk with Darren about their no-code approach to back office efficiency with a platform that utilizes RPA and AI technology."},{"id":103,"type":"Episode","title":"The AWS Outage of Nov 2020","tags":["aws","cloudoutage","compute","csp","cloud","process","cloudreliability","multicloud","reliability"],"body":"\r\n\r\n## What We Learned from the AWS Outage\n\nAmazon Web Services (AWS) suffered a substantial outage at an inopportune time – the day before Thanksgiving in 2020. Since they published their service logs, it’s interesting to review them to see what happened and what we can learn.\n\n## Timeline of the AWS Outage\n\nOn Wednesday, November 25, 2020 around 3:00 am PST, AWS was updating the East region, adding servers to increase the capacity of Kinesis. Kinesis is a popular AI service that does pattern matching on log files and video files. About an hour and a half into the upgrade, the server alarms began firing errors in the Kinesis records. Fairly quickly, by 8 am, they initially identified a couple of candidates on the front-end services as the root cause. Kinesis has several different smaller services, a group of front-end services and a group of back-end services. The way the system is architected, each time a back-end service is running, a thread fires off the front end. One of the problems was that the front end hit the thread threshold as they added servers to the back end during the upgrade. Once they had determined this problem, they made a temporary fix with updates (patches) to the operating system and Kinesis was back online by about 10:30 pm and fully restored by 1:15 am on November 26th.  In the end, Kinesis was not fully operational for 21 hours, and although not everyone uses this AI tool, the impact was widespread.\n\n## Kinesis Impact\n\nSeveral other Amazon services use Kinesis, such as Amazon Cognito and CloudWatch, and they suffered varying degrees of disruption. Cognito was overloaded until about 2 pm; CloudWatch was down until about 10 pm. In a domino effect, services dependent on CloudWatch such as Lambda and EventBridge were also down. Since EventBridge was unavailable, the container services LCS and LKS were affected as well.\n\nThe outages only happened in the East region, and AWS quickly added capacity in the other regions to prevent Kinesis from failing in a similar manner. But during the outage, users in the East region encountered a perplexing problem, as their service dashboard and personal service dashboards were not receiving information and showed false positives. So, there were many other IT organizations investigating the issues since they weren’t getting the correct information. Surprisingly, AWS has been open about the whole incident, so it’s a great learning experience.\n\n## Lessons Learned\n\nOne of the first lessons is that simple operations to infrastructure such as increasing capacity need to be understood and planned. Obviously, AWS didn’t just make the upgrades off the cuff, but they didn’t fully understand the impact. Even if an operation seems rote, it’s always a good idea to run things to failure in a test environment during upgrades, even if it takes more time. This is especially important with services that are critical to other core dependent services.\n\nIn addition, service or micro-service architectures must understand their whole service dependency tree so they can troubleshoot when there are outages. In addition, it’s important to be as loosely coupled to a dependency as possible and include defensive programming with microservices to avoid the domino effect that happened in this case.\n\nAnother issue to watch out for is circular dependencies. If, in a chain of micro services that are dependent on each other, one hiccups, everything can come to a screeching halt, consuming resources and accomplishing nothing.\n\nWhen developing micro services, keep in mind that you will not always have connectivity to the services you are dependent on. Writing programs that can run in a degraded state, or at least indicate that a service is not working, can save time and trouble. In this outage, remember that dashboards were running green even though no new information was coming through.\n\n## Who Handled the Outage Best\n\nMany companies were affected by the AWS outage, including some owned by Amazon itself. Some flew right though the outage, almost unscathed, while others had a harder time recuperating. Those that were able to adjust quickly had a multi hybrid cloud strategy, so they had alternate clouds as backups. Some used a different region of AWS, while others used Google or Azure, and some even ran it back to their own data centers or external websites. At the very least, websites displayed message that they were currently experiencing problems, rather than a 404 error.\n\nCompanies that were not using some of Amazon’s specialized services also did better. For example, the EKS and ECS managed container offerings and Lambda were hit hard and were down for a substantial amount of time, so those dependent on these services were without options.\n\nDuring the outage, auto scaling services were not working properly, so any company that had a lot of traffic at the time had to find out what was going on and scale up services manually. This was a bigger issue than it might normally be since it was the day before Thanksgiving, a time when many consumers are traveling and buying online.  For example, Etsy stayed up, but they could not scale as much as they normally would, leading to decreased sales.\n\nThe companies that have their own external monitoring also fared better. Some even alerted AWS to the outages. They were not dependent only on AWS’s health dashboard, but had their own monitoring running on their servers.\n\nWhat is the main lesson we should learn from this? Organizations should take ownership of their cloud resources, much like any utility. Just like having a backup generator for electricity to assure continuation of business in the event of an outage, companies should use the same best practices for cloud services. This means having a backup cloud that can keep you running, even at diminished capacity, is essential to weathering a storm such as the AWS outage. \n\n## Resources\n\n* https://aws.amazon.com/message/11201/\n\n* https://www.theverge.com/2020/11/25/21719396/amazon-web-services-aws-outage-down-internet\n\n* https://www.zdnet.com/article/amazon-heres-what-caused-major-aws-outage-last-week-apologies\n\n* https://www.wsj.com/articles/amazon-web-services-hit-by-outage-11606326714\n\n* https://www.washingtonpost.com/technology/2020/11/28/amazon-outage-explained\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT33-en","image":"./episodes/edt-33/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Public Sector, at Intel talks about the lessons learned from the AWS outage in November 2020 and preventative solutions to navigating such outages."},{"id":104,"type":"Episode","title":"Embracing Workspace Evolution","tags":["covid","mfa","remoteworker","cybersecurity","people","technology","vdi","vpn"],"body":"\r\n\r\n## Digital Workplaces are Evolving\n\nSince we are past the initial chaos of the transitions necessitated by the pandemic, we need to ask what is coming next and, moving forward, how we can leverage what we learned to conscientiously invest in where we want to be.\n\nOne major lesson was that the organizations that were already agile fared well. They were able to get their remote workforce going quickly. We don’t know exactly what the new normal will be, but we do know it’s going to retain a lot of the same aspects that were accelerated over the last eight months, such as enabling a remote workforce, but at the same time being more collaborative. Intel wants to reach new customers who need more resources to operate out of their own four walls and become more agile.\n\n## Your Workplace Needs New Capabilities Intel Bridges the Gap\n\nIntel engages in creating solutions by helping customers understand the vast Intel ecosystem with different models that can fill the gaps in performance, stability, efficiency, and price. Intel can share the learnings of thousands of customers to help solve problems and provide capabilities that IT organizations sometimes can’t do on their own.\n\n## Business Requirements are Paramount\n\nIn the past, a CIO might have been focused on only three things: security, resiliency, and efficiency. That is not the case anymore. What used to be tantamount to keeping the lights on, staying out of trouble, and running efficiently, is now far more complex. Companies must be forward-thinking to enhance the worker, the workplace, and the transformations they are undergoing in terms of meeting customer and supplier commitments.\n\nIn addition, agility is a requirement since circumstances can rapidly change and businesses must adapt in multiple directions while still meeting these needs.\n\nFinally, more automation via artificial intelligence is an enabler for driving areas such as better collaboration and worker experience enhancement. With a distributed workforce, for example, there is no help desk with a person who can walk over and help with your issue. That might not be the most efficient scenario anyway, so perhaps chat bots or machine learning in a CRM system would be more efficient and allow more shared information.  This type of technical automation with common troubleshooting could produce more connections, insights, and productivity in the end.\n\n## Your Workplace is Evolving\n\nBusiness requirements and strategies are changing, especially around workplace evolution. Some organizations will continue with full remote operations, some will want everyone back in an office when it’s safe, and there will be every configuration in between. Regardless of the situation, organizations must embrace a strategy to make sure they can reach workers any place, any time, and on any device, whether from necessity or choice.\n\nProcess improvement is also key. You don’t want to continue to add processes on top of those you don’t need anymore. Evaluating areas for improvement, whether it’s internal infrastructure or third party capabilities, will add efficiency and value. Rather than building a huge infrastructure that requires management and is burdened by using only its tool sets, organizations should evaluate the incredible amount of ecosystem opportunities that are plugging into as-a-service tools. Outsourcing services that are not your company’s key strategic assets or strengths might make more sense.\n\nAnother area to evaluate is data management. With, for example, all of the data in the collaboration tools, data sprawl becomes an issue.  A clear and effective strategy is necessary.\n\nAlong with data management comes security. Data is spread all over the place now, so organizations must embrace, evaluate, and deploy good security tools and good workflows around data practices.\n\n## Intel Delivers\n\nThere are six main capabilities in the evolving workplace where Intel can help: app and data access, manageability, enhanced security, connectivity, collaboration infrastructure, and multi-cloud atmospheres.  Although Intel only produces silicon, the capabilities, scalability, and security of it meets the smallest to largest needs of organizations.\n\nIntel seeks to exist across multiple environments and offer manageability of those resources. Although customers will not buy assets such as processors, accelerators, memory and storage, and so on directly from Intel, they will leverage the robust, proven ecosystem of products such as hardware players, OEMs, software providers, system integrators, and cloud service providers that Intel makes possible.\n\nOne of Intel’s strengths is its support of this ecosystem. For example, Intel has 15,000 software engineers that develop code, but none of it is sold.  Instead, they help develop the ecosystem by providing new solutions built on top of the silicon. Silicon is the mechanism to provide solutions to help people solve real problems. A good metaphor is that the distance between Intel’s loading dock as a manufacturer and the final customer’s loading dock is too far apart for Intel to drive a truck themselves. The ecosystem delivers across that gap.\n\n## Solution Areas you Might be Evaluating\n\nHow can customers best leverage the capability of the ecosystem? Intel can give recommendations in the six key areas, whether you are struggling with a VDI solution such as deciding whether it should stay on premises or be in a virtual desktop or RDS service, or coming up with a VPN strategy that will assure connectivity.\n\nPart of the solution is in understanding that since Intel works in such a vast ecosystem, they can help meet myriad challenges. Intel is not necessarily going to sell you a processor, for example, but help you get your workload put on the best piece of silicon possible, which could be in of the cloud service providers, or in multiple clouds, in and outside of your own data center.  Intel’s goal is to give you efficiency, portability, and agility in these processes.\n\n## Embrace Workplace Evolution: When or Where do You Want to Start?\n\nIntel has your organization’s best interest at heart because if you are successful in your business, you’re going to come up with new way to use data and new ways to use infrastructure to provide more value to your customers, and in the end, consume more.  Your strategies and growth and development are your own, but Intel is going to give you the foundation to make good choices.\n\nLook for opportunities to engage with your Intel representative. They have a bevy of resources to help you leverage the extensive network of partners that can address your issues and your goals. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Robert Looney"],"link":"/episode-EDT34-en","image":"./episodes/edt-34/en/thumbnail.png","lang":"en","summary":"In this episode, Darren Pulsipher, Chief Solution Architect, Public Sector, and Robert Looney, Americas Data Center Sales Manager for Intel, talk about using a strategic approach to embrace the current workplace evolution. The COVID pandemic created major challenges and transitions in the workplace. Intel is helping customers leverage technologies to best address ongoing challenges in the new normal."},{"id":105,"type":"Episode","title":"Navigating Disruptive Change","tags":["change","culturalchange","organizationalchange","disruption","people","process"],"body":"\r\n\r\nRick recently celebrated his third decade at Intel, and in that time, he helped Intel navigate through tremendous amounts of change and big events. Along with tough competitive situations and industry change beginning with the rise of the internet and the dot.com boom and bust, there were external events such as 9/11, the Great Recession, and now, the COVID-19 pandemic.\n\n## Types of Crisis Situations\n\nThe nature of a modern enterprise in a modern economy is that it will be constantly navigating a high degree of uncertainty, turmoil, change, and disruption. Organizations either wither in those moments or come out better and stronger.\n\nEvery disruption is different in terms of magnitude and speed. Some events unfold over a long period of time, perhaps a technological or structural change in the industry, and then suddenly accelerate. Some, such as COVID, are high impact at unprecedented speed. What’s interesting about COVID, however, is we have been developing the technology to deal with the consequences of the pandemic for a decade, but it took this event to put it into practical use. A good example is telehealth. Its sudden, widespread use has also changed the policy environment, and the healthcare landscape will never be the same.\n\nThis type of fundamental change in policy happens in events with great speed such the Great Recession or 9/11.  Similar things will happen post-COVID. These events, as difficult as they are, provide an opportunity for organizations to take a giant leap forward in how they perform and how they use technology.\n\n## Navigating Disruptive Change\n\nIntel has always stepped forward in responding to big challenges and disruptions. Fundamental parts of the culture are readiness, shared purpose, and trust. These can exist when employees have a sense of psychological safety. For example, Darren felt empowered when Intel’s CEO said that no on would be laid off because of COVID. This allowed him to take risks to meet the crisis without any fear of losing his job. And although the CEO and senior manager set the tone, most of the work of psychological safety is carried out by the frontline managers. This safety and empowerment leads to a built-in readiness. Along with shared purpose and trust, these are the fundamental building blocks of not just an organization ready to respond to a crisis, but the characteristics of a high-performing organization.\n\nA high-performing organization will also have the tools to navigate disruptions that are more of a slow burn rather than a fast, high impact event. With events like COVID or the Great Recession, there’s very little debate about what is happening, and everyone is onboard with the enormity of the issues. If you compare that to, say, a fundamental business shift, an architectural shift, or a technology manifesting on the hype curve that you’re not sure is material to the business yet, there will be more uncertainty and debate about the adjustments.\n\nHow does an organization survive these inflection points? Telemetry, or the input you are assessing, is important. One of the complexities in a big organization is that by the time these inputs get to a senior decision-maker, they may have been through three layers of massaging and positioning, and that can be dangerous. Truth and transparency is a value at Intel. In a company with a high degree of psychological safety, employees can speak the truth about problems.\n\nThe most important input is listening to your customers because they will tend to lead you in the right direction. For example, if someone wants to know about an account, Rick will often bring in the account executive to get the frontline information. It’s also wise leadership to go directly to the experts instead of getting information through three layers of filtering and massaging, especially when operating in a crisis. Meeting the moment boils down to readiness culture, the right telemetry, and decision making.\n\nDecision making can become convoluted in a large organization. One simple solution is that every person walking in to a meeting should be asking, are we here to make a decision? Who is the decision maker? Or, are we simply debating or preparing the telemetry and data for a decision maker? This is just good organizational hygiene.\n\nAndy Grove said, “Let a little chaos reign and then rein in the chaos.” For decisions on the inflection points, the slow burns, sometimes you have to let innovation breathe and percolate a bit, and at the same time you want to manage things in such a way that they don’t careen out of control. Having good processes and guardrails in place helps with this.\n\nIn difficult times, decision makers need to have a deep understanding that individuals are each going to be in a different space, and they have to think through the impact of decisions. Psychological safety is so important, and the frontline and second line managers are critical to the ability of an organization to execute well in time of disruption. Senior leadership is key in setting the tone, but these managers shoulder the work.\n\nFrom predictable future technological changes such as the impact of AI on 5G, to problems like climate change, to unanticipated world events, the only constant is that we will always be navigating disruption, crises, and change. One of the hallmarks of Intel’s culture is its ability to respond, to adapt, and to be resilient to these events. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rick Herrmann"],"link":"/episode-EDT35-en","image":"./episodes/edt-35/en/thumbnail.png","lang":"en","summary":"In this episode, Darren Pulsipher, Chief Solution Architect, Public Sector, and Rick Hermann, Director US Public Sector, Intel, discuss how Intel has been successful in navigating disruptive change over the past three decades."},{"id":106,"type":"Episode","title":"Six Pillars of Cybersecurity","tags":["cybersecurity","prevention","threatdetection","zerotrustarchitecture","zta","technology"],"body":"\r\n\r\nWith cyber-attacks on the rise in all industries, security is more important than ever. In this episode, Darren and Steve Orrin, Federal CTO at Intel, outline the attack vectors, the six pillars of cybersecurity, and how Intel can help.\n\nSteve has seen security evolve in the 25 years he’s worked in the field, both as a science and as an art. At the same time, the level of complexity organizations must deal with to secure their data, systems, and applications has never been more difficult.\n\n## Large Scale Breaches and Cyber Attacks Continue \n\nLarge scale data breaches and deep intrusions are happening up and down the stack in everything from social media platforms to financial services to health care. No type of data is exempt from being targeted with increasingly sophisticated techniques.\n\nWhat is driving these hacks? One answer is that today, a wide scale or deep attack requires fewer resources and less financial investment. The scope and scale of what a hacker can do with a small investment has given adversaries an edge in a complex system.\n\n## Three Forces Impacting Enterprise and Mission Security\n\nAnother answer to what is driving the attacks is that data is a valuable asset: the new oil. Data is vulnerable as the expansion of the attack surface continues to grow.\n\nThere are more integration points, products, vendor operating systems, and devices that are involved in managing, consuming, and transporting the data. The data is further away from the control of the enterprise. Sometimes we’re not even sure where our data exists. For example, perhaps you’ve shared your data with another organization and they shared the analytics that were performed on the data. That metadata often turns into data residue. Your data is flowing through multiple systems, consumable by an attacker after the fact.\n\nThe security industry itself is composed of thousands of security vendors and products that solve a particular piece of the puzzle, so there’s only so much a CIO can do with a limited budget, and only so much they can handle given the complexity. We have to think holistically about how we can secure our data, not just how to secure one transit between point A and point B. Data is compromised at the weakest link, so we have to look at the whole chain.\n\nThe third force impacting enterprises is the locations of the attacks. They include attacks up and down the full stack of hardware, firmware, bios, software, services and applications. With the increase in the sophistication of the attacks we are seeing attacks at multiple layers at asynchronously and independently.\n\n## The Attack Vectors\n\nHackers are not just attacking the hardware or a piece of software, but through multiple vectors: people, process, and technology.\n\nTraditionally, implementing security controls has involved proper training of people and using the right processes and technologies, but in light of recent attacks, we need to remember that the process itself is vulnerable. For example, we need to move to automated patching to reduce the window of exposure from when a vulnerability is discovered to when a patch is released and deployed by an organization. Traditionally, we’ve lived with the risk that attackers may have months to exploit the vulnerability.\n\nThere are other process attacks that we are just now sealing, whether it is in the build process or software development lifecycle. Integrating security early in the development lifecycle is the most important aspect to securing an application. That means developers, QA, and designers all must be involved in the security process. Part of the challenge is the siloed nature of each part of the process, where vulnerabilities can creep in the seams and transitions.\n\n## Six Pillars of Cyber-security\n\n### Supply Chain Security\n\nSupply chain security has been at the forefront only for the past few years. An organization needs to be able to trust the servers, components, and software. A good supply chain with transparency is important in order to validate that everything is coming from legitimate sources. There has been a focus, particularly in government, about the hardware supply chain, but we can’t forget the software supply chain. The software supply chain is a bit more difficult than hardware since there is often a lack of visibility because products can be cobbled together from open source tools, other people’s products, etc.  In a recent attack,  the software supply chain was the problem, and this is just the tip of the iceberg.\n\n### Host & System Security\n\nOnce we have a trusted supply chain, the next step is in hosting system security.  The foundation is secure boot technologies and crypto capabilities to lock down and secure physical devices and systems where the applications work and data will run. This system supports the higher-level stack security features in the hardware.\n\n### Data and Application Security\n\nAbove the host and system security is where you build your application workload security. Data must be protected throughout its lifecycle, at rest, in use, and in transit. We’ve been doing security for data at rest and in transit for a long time using transport encryption, TLS, and IP sec, and other encryption capabilities, and then full disk and file encryption. The missing link has been data “in-use” encrypted memory with hardware isolation. In the last few years, technologies and solution stacks are enabling that last mile of exposure around data protection.\n\n### Network Security\n\nIn parallel with this stack of supply chain hosts and data security, we need network security. Integrity and availability of networks is important to withstand denial of service attacks. The data needs to get to where it needs to go securely. We also must monitor and protect networks from external intrusions, whether that network is enterprise or a distributed network throughout the cloud and the edge. Security here is not about simple firewalling; it’s about active production.\n\n### Identity  & Access Management\n\nIdentity and access management is a foundational capability. We not only need to know who is logging in, but who is logging in to what device. The human is actually a small piece of the puzzle. We need to have identity for all the things, processes, and services that are accessing and managing the data. A person might do their job with just a few keystrokes, but there may be 20 different devices and 100 services and processes that act on the data. We need to have policies and authorization for all of those entities. And as we move more into autonomous processes, there are fewer humans involved, so it becomes even more important to have a strong identity for those processes without a human in the loop.\n\n### Threat detection, Intelligence, and Analytics\n\nThis last pillar is a combination of many things including threat intelligence, analytics, monitoring, and auditing. It’s the overarching visibility into making sure everything is running the way it’s supposed to run, and if something is wrong, the ability to quickly understand where it’s coming from and why. This is the umbrella that drives data security and everything must feed up and feed down. There is a shift from working in a siloed environment, say a vendor who is only concerned with network security, to working across the system as a whole. Successful companies have diverse teams with people from different domains to meet complex security needs.\n\n## Cybersecurity Domains: Achieving Zero Trust with Intel Technologies \n\nIntel provides foundational capabilities in each of the six pillars, whether it’s our compute life cycle assurance initiative to help get the broader OEMs and component providers collaborating on a trusted supply chain, to providing the foundational building blocks of system security, to secure boot starts with the hardware. We have execution technology and boot guard technology with crypto acceleration built in so users can turn it on for data at rest, data in use, and data in transit protection without performance impact.\n\nIn the case of threat detection intelligence, Intel provides primitives such as TPD where an upper-level stack solution can provide visibility and threat detection where we’ve never had it before.\n\nIntel is a technology provider, but we also work in the space of people and process. A good example is the supply chain. We’ve built a process with the ecosystem to enable an enterprise to be able to validate the components and credentials for a given platform and its components. Similarly, there are processes involved with data and use protection through capabilities in the hardware such as SGX being able to encrypt the memory and isolate the data and code for a given application.\n\nIntel is enabling secure processes for leveraging the technologies at scale. Another key part around process is fitting into an organization’s overall risk framework. Intel gives you the evidence and attribution you need within our technologies to map that into your existing risk framework.\n\nThe last piece is people. Dealing with random human behavior is sometimes the hardest part of security, whether it’s phishing scams or engineered attacks on weak passwords. Training is crucial, but it often isn’t enough. Processes and technologies can help augment training by, for example, making passwords stronger or eliminating phishing if a user’s credential can’t be compromised. At the end of the day, however, continuous training and education will always be critical along with mitigating technologies.\n\nSecurity is difficult, but there are lights at the end of the tunnel with all the innovations in the ecosystem and with organizations open to doing things differently. We need to keep our eye on two things: the adoption of risk frameworks, and zero trust. Tying these two worlds together, the cybersecurity domain to policy engines and enforcement can provide a comprehensive approach to security. There is a lot of activity here, and a lot of work still to be done. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT36-en","image":"./episodes/edt-36/en/thumbnail.png","lang":"en","summary":"With cyber-attacks on the rise in all industries, security is more important than ever. In this episode, Darren Pulsipher, Chief Solution Architect, and Steve Orrin, Federal CTO at Intel Federal, outline the attack vectors, the six pillars of cybersecurity, and how Intel can help. The level of complexity organizations must deal with to secure their data, systems, and applications has never been more difficult."},{"id":107,"type":"Episode","title":"2020 in Review","tags":["covid","cybersecurity","remoteworker","people","process","compute","data"],"body":"\r\n\r\n## 2020 – Expectations\n\nPre-COVID, 2020 was looking like the year for new business models, moving toward big digital transformations. AI/ML and analytics were going to play a key role going forward. Using those tools, we were going to start using data more effectively in our organizations. It was going to be the year of 5G, with 5G making a huge splash at the Olympics.\n\nWe were also going to see a major move forward in industry for 4.0 transformations, with Internet of Things and in manufacturing all moving together toward this digital transformation.\n\n## March 13\n\nThen, at least in California, everything came to an abrupt halt on Friday the 13th of March.  Businesses, schools, conferences, meetings, travel, and social events abruptly shut down. Weeks turned into months as optimism about quickly containing the virus waned while cities such as New York City, took big hits and the medical systems were overwhelmed. Everything seemed chaotic and uncertain.\n\nSome interesting things came out of this: Some companies thrived while others had a more difficult time, and people started working remotely.\n\n## Most Looked-at Website in My House\n\nIn my house, the website with the most views was no longer Facebook, but the Center for Systems Science and Engineering at Johns Hopkins, which gives statistics about the virus.  We could see the effects in our own neighborhood and city and all over the world. Our whole perception of what was important and what we had planned for that year changed.\n\n## Shortages\n\nNot only were there immediate shortages of items as diverse as toilet paper, hand sanitizer, and yeast, but a few weeks in, office equipment and technology became high-demand items as people quickly adjusted to distance learning and working from home. This created a shortage of integral tools such as webcams and laptops.\n\nWi Fi 6 routers became important all of a sudden as internet use at home increased. In our family, I moved from working at home periodically to working at home full time and added three teenagers doing distance learning and three adult college students who came home to study and work. Our internet connection was quickly overwhelmed.  I had a hard time finding a Wi Fi 6 router to solve our problem, as many found themselves in similar situations.\n\n## Travel Changed\n\nWith travel virtually halted, many people turned to home improvement projects instead. I used to be at the airport often, with travel comprising half of every week. Now, I found myself at Lowe’s or Home Depot with everyone else. The places were packed. Time that I normally spent traveling was now spent building a shed with my kids. I got to spend more quality time with my kids and improving my home.\n\n## Conferences\n\nA major shift, of course, was the cancellation of in-person conferences. Many moved online instead with great results. For example, the IBM Think conference switched their May in-person event to online instead. Over 100,000 people attended, which was the largest one they’ve ever had.\n\nBesides increased attendance, another bonus is that attendees don’t have to make a weeklong commitment to be at a conference, but can pick and choose sessions and still be at home with family. The downside is the lack of networking and seeing people face-to-face.\n\nIndustry conferences are probably changed forever. They won’t always be completely online, but perhaps a hybrid version makes sense in the future.\n\n## Forced Working from Home\n\nAnother major shift is that we are in each other’s homes virtually conducting business. There are sometimes kids, dogs, or other inevitable distractions in the background. One time, my boss needed to have his daughter sit next to him as he led a staff meeting because she needed help with something. It’s important that we are all flexible during this time.\n\n## Nomadic Workers\n\nSome employees are not working at one home, but have become nomadic since they no longer need to be near their workplace.\n\nFor example, my nephew, who has few home-based responsibilities, was paying exorbitant rent in the Bay Area. When work became remote, he and his roommates dropped their expensive apartment and traveled the world instead, spending several months at a time renting homes in interesting places.\n\nThis is a major shift in the way we think about managing our workers, assets, and data.\n\n## Remote Work\n\nRemote work cranked up quickly. Many companies already had some remote work policies or plans for more remote work. But what was once perhaps an 18-month rollout plan became an 18-day rollout plan.\n\nThe major remote work that happened was VDI (virtual desktop infrastructure). People have been using VDI for decades, but all of a sudden it became a top choice, in part, because of its familiarity.  It is quick, easy, and inexpensive to get people working again with access to the data they need using hardware solutions.\n\nVPNs (virtual private networks) became quickly overloaded as everyone was working from home. We saw companies invest in expanding their VPN, either through buying more licenses or more infrastructure and upgrading as needed.\n\nCompanies also accelerated their adoption of SaaS (software as a service) tools. Office 365 rollouts, for example, happened within a few weeks rather than the planned six to nine months.  Companies with SaaS offerings such as Microsoft and Google rolled up their sleeves and helped organizations get through the migration quickly. For the remote worker, unproductive time was minimized with the effort from the industry as a whole.\n\nIt was remarkable how soon remote workers were up and running. The real heroes here were the frontline IT workers such as the helpdesk, systems engineers, system administrators ect… It was really a Herculean effort.\n\n## Cybersecurity Changes\n\nOne of the things, however, that went by the wayside a little bit with the speed of this change was cybersecurity.  We’ve been feeling the ramifications of this over the last six to eight weeks with several major attacks in cybersecurity.\n\nOne of the reasons is an increased attack surface because data is spread out on laptops throughout the whole organization on unsecured networks in homes.\n\nSome of the data is sitting in the cloud. Now, with SaaS offerings, some of it is sitting in the data center and some on computers that people brought home.\n\n2021 will be a year that we focus on cybersecurity, taking a hard look at the way we’re managing data and securing it through the whole system.\n\n## New Business Dress\n\nOne of the greatest advances this year is the new business dress. The best way to describe it is the business mullet: business dress on the top, pajamas or shorts on the bottom.  I put on pants once in a while, but my kids are probably tired of seeing me in a button-down shirt or even a suit on top paired with shorts.\n\n2020 was a year of change, personally and professionally, but it has led us to place in 2021 where we can move forward with lessons learned and improvements for a better future.  \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT37-en","image":"./episodes/edt-37/en/thumbnail.jpg","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Public Sector, at Intel reflects on the disruption, changes, and adjustments the COVID-19 pandemic brought in 2020."},{"id":108,"type":"Episode","title":"Roaring 20's a Look Forward to 2021","tags":["contactless","covid","remotelearning","remoteworker","people","process"],"body":"\r\n\r\n## COVID-19 An Unprecedented Time of Volatility, Uncertainty, Complexity, and Ambiguity\n\nIn this episode, Darren asks Rachel Mushawar, VP of Sales at Intel, for her insight on what’s ahead for the Roaring 20s after the disruption of COVID-19.\n\n## 2020\n\nThe past twelve months has been marked with unmistakable change and uncertainty, professionally and personally.  Not every year will be like 2020, but the lessons will make us better every year going forward.\n\nDespite individual circumstances, every person has had to pause and figure out what was important. We hold our teams, friends, families, and relationships closer than we ever have.\n\nAnd although we have been physically isolated, connectivity through technology accelerated at an unprecedented rate. Everyone’s sense of possibility in this area has been rejuvenated, and in a way, unlocked and freed for the future.\n\n## The Pandemic Has Created More Responsibilities\n\nOn the other hand, the pandemic has created more responsibilities. Almost half of adults in their forties and fifties have a parent 65 or older and have young children or are financially supporting older kids. This means they are likely juggling their children’s distance learning, working from home, and caring for elderly parents.\n\nThe net job losses in the U.S. in December were all women. Some of this has to do with the increased responsibilities at home during the pandemic. If you have young children at home who are now distance learning, and you’re an hourly worker, how do you do it? You don’t. And although during lockdown fathers nearly doubled their childcare, we have a long way to go in the 20s around gender equality. It can’t just be about technology.\n\n## Remote Education Brings Focus to the Digital Divide\n\nAs we move into the 20s, we’ve got a whole generation of children who have spent the last 12 months homeschooling.\n\nIn fact, one in four American households have at least one child 14 years old or younger. For those students that come from a disadvantaged background, A World Economic Forum study found that 25% do not have a computer. In addition, 33% of students in rural communities have little to no access to the internet. The lack of these two fundamentals of distance learning increased the digital divide.\n\n## We’re Living “The Future of Work”\n\nA lot of old ideas have been shattered in 2020 as companies had to immediately pivot to work from home. Eighty-five percent of organizations expanded or implemented a variety of work-from-home policies that are going to extend past COVID-19. Many companies realized the savings from reducing physical real estate as productivity stayed the same or increased with a stay-at-home or nomadic workforce. Many employees are also realizing time savings from eliminating commutes and personal preparation such as hair, makeup, and wardrobe. A trend going into 2021 will be a more casual, natural look.\n\nAs commutes vanished, connectivity increased, and bedrooms became offices, however, a downside emerged as the average workday lengthened and it became harder to shut down.\n\n## “Contactless” is Driving Everything–as-a-Service\n\nOne of the key things in 2021 is going to be how we continue to leverage technology to keep us connected. For example, there are Zoom-like technologies that take it up a notch and provide social opportunities similar to a water cooler chat in the virtual space. Thirty-two percent of adults had a virtual social gathering in 2020. Intel has done visits to a virtual goat farm and brought in yoga instructors to help fight isolation.\n\n## More Connections, Less Friction, More Virtual, Less Physical Contact\n\nAs we go into the rest of the 20s, technology is going to become a cornerstone for every major transformation regardless of whether it’s in the private or public sector.\n\nHow does this new way of working and increased responsibilities at home all boil down from a CIO’s perspective?\n\nThere are a handful of strategic imperatives for IT. We can break them down in to the traditional categories of applications, network, and data center.\n\nFirst, for applications, CIOs have got to figure out how to enable contactless, meaning, how do you drive everything-as-a-service? This is not just for retailers, but for healthcare, government, and manufacturing. The second part with applications is knowing who your consumer is and how they digest content.\n\nRealizing the importance of your network is next. It is the turbo boost to all things digital. Organizations must have a network that will, for example, enable automating your factories or providing telehealth.  They must be forward thinking for when 5G becomes more of a reality rather than investing in yesterday’s technology. Networks are extended beyond traditional data centers now, so that’s something we need to pay attention to. Of course, security must be a priority here.\n\nJust like networks, security is not an exciting, sexy topic, but it’s a key aspect as we think about all of the endpoints that are now pervasive in our everyday lives. The threat surface is increasingly exponentially with employees working at home on different devices and the implementation of everything-as-a-service to customers. Security is no longer about securing data at rest and in motion anymore. It’s also everything in between.\n\nInstead of the traditional data centers, we should be thinking about them as centers of data, serving certain workloads. The cloud is growing 30 to 40 percent per year, for example, to bring centers of data closer to employees or customers. It may never make sense, however, for some top secret critical data to move to the cloud, but stay on prem. CIOs must understand what their centers of data are, and which would serve which segments of the organization best in terms of recovery, storage, cost efficiency, and performance.\n\nMoving in to the 20s, organizations must make strategic changes, both in hiring practices and in how organizations service their customers, keeping these concepts in mind. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rachel Mushawar"],"link":"/episode-EDT38-en","image":"./episodes/edt-38/en/thumbnail.png","lang":"en","summary":"In this episode, Darren Pulsipher, Chief Solution Architect at Intel, asks Rachel Mushawar, VP of Sales at Intel, for her insight on what’s ahead for the Roaring 20s after the disruption of COVID-19."},{"id":109,"type":"Episode","title":"Watercooler Talk in a Remote Workforce","tags":["collaboration","covid","organizationalchange","people","compute","remoteworker","videoconferencing","signalwire"],"body":"\r\n\r\n## New Collaborative Workspaces\n\nOn this episode, Darren and Sean Heiney, co-founder of SignalWire, Inc., discuss new remote collaborative workspaces that break down the tiresome video meeting conferences that go on all day. Their Cameras on policy and SignalWire Work technology allow for ad hoc communication like never before.\n\n## The Creation of SignalWire Work\n\nSignalWire is the developer of the largest open-source communications platform in the world. For the last three or four years, SignalWire has focused on building the next generation of video and voice in real-time communications applications, which led to SignalWire Work for remote offices.\n\nA predecessor of SignalWire Work was SignalWire’s own tool that they built for themselves since the company has been distributed since inception, working remotely for three years. The existing remote collaboration tools did not satisfy their needs, with more than 60 people spread over a number of countries and continents. Some were already using always-on audio, but since many of their engineers were also sharing Unix screens, it evolved into always-on video. When COVID hit, customers wanted access to this tool, so SignalWire Work was born, along with a version for live events.\n\n## The SignalWire Work Environment\n\nWorking in collaborative remote spaces is not only a technological innovation, but a social experiment as well. For some people, always-on video might seem weird or scary. Once everyone has subscribed to the philosophy, however, it can be more efficient than sitting in a physical office. There are inefficiencies in a world like Zoom where people are only talking in scheduled meetings with specific agendas.  A lot of the important, informal communication is lost.\n\nWith SignalWire tools, coworkers can see inside people’s rooms, as if they have a glass wall in a physical world. You can see how busy they are, what kind of mood they are in, or if they are talking to someone else. You can pop in and ask a question. There is social interaction, water cooler talk, that is not possible with scheduled meetings. When you log into SignalWire Work, it’s like stepping into a physical office.\n\nThere are features that assure privacy. You can fog out your video so people can’t see your face, or go in a heads-down mode where people can’t interrupt you, but they still know you are present and can knock on your virtual door.\n\nAnother benefit is that there can be clearer boundaries between home and work life. When you log off from the office, it’s a clean break.\n\n## The Technology \n\nSignalWire has created the technology that powers everything from the Ring doorbell to pieces of Amazon Connect and Netflix’s customer service engine; they’re basically in every major telco. Now, they are focusing on enabling that technology to get in the hands of regular folks and have them build off of it. One example is the most popular virtual church platform in the world was built about a year ago on SignalWire.\n\nA unique aspect that gives SignalWire a strategic advantage is that video muxing is done in the cloud. A traditional video conference application will encode and transmit the video to every participant in a conference. If there are seven people, there are seven streams. That’s a lot of work on the processor. It’s work on your device to transmit the data and you’re subject to jitter and packet loss on all those individual streams, so you may have a great connection to someone, but the other person looks horrible.  \n\nWith SignalWire, the clients send one feed to the cloud. The cloud takes everyone’s feed, muxes it together, and then sends that one feed back out to everyone so there’s only one transmit and receive. This has many advantages such as better battery life, lower data consumption, and a lower amount of work on local device processors.\n\nFor the user experience, SignalWire can make the audio great when it’s muxed together, or control the layout so everyone sees the same thing in the same orientation. When you point at someone on the screen, for example, everyone can see that. For events such as live exercise sessions, the platform allows users to hear the background music and the host at the same time, along with being able to see, say, thirty other participants. All of this adds up to a more connected feeling and experience.\n\nSignalWire can run on any cloud or platform, down to an individual handset or an atom-based device. Companies can use it on their own infrastructure, which is important for security and controlling data at the highest level. SignalWire can deploy its nodes inside a secured network to protect sensitive data from traversing in the public internet.  \n\nThe platform is fully flexible with users’ tech and applications, allowing even a major broadcast studio to use it to produce and edit one of their shows, bringing in extras for voice-overs on existing recordings because the real-time quality is that high.\n\n## A More Professional Environment in Remote Workspaces\n\nThe key to making this new type of workspace viable is jumping in, and make it company policy. The benefits become quickly apparent. When you come into work, you come into video and are present and interact with your coworkers, just like you did pre-COVID.\n\nWhen people went into the office pre-COVID, they probably dressed nicely to make a professional impression. In this world, that impression is made more through a good audio and video set-up: good microphones, lighting, a high-quality environment. Employees are putting on a their best using tech because this is now a tech world. It’s an evolution from the informal remote work environment of sitting on a beach or at the kitchen table while dinner is being made. More professional conversations can take place in a more professional remote work environment.\n\nThe best way to experience this technology is to try it out. There is a 30-day free trial at https://signalwire.com/products/work\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sean Heiney"],"link":"/episode-EDT39-en","image":"./episodes/edt-39/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Public Sector, Intel, and Sean Heiney, co-founder of SignalWire, Inc., discuss the companies remote work policies of cameras on and their new remote collaborative workspaces technology that fosters ad hoc communication for their completely remote workforce."},{"id":110,"type":"Episode","title":"Information Management Maturity Model","tags":["data","informationmanagement","informationmaturity","datagovernance","datawarehouse","datalake","datamesh","datalineage","technology","process","people"],"body":"\r\n\r\nDeveloping a data strategy can be difficult, especially if the state of your company’s information management or trajectory is unclear.  This Information Management Maturity Model helps CDOs and CIOs identify where they are in their information management journey and their trajectory. This map can help guide organizations as they continuously improve and progress to the ultimate data organization that allows them to derive maximum business value from their data.\n\nThe model represents a series of phases, starting from the least mature to the most mature: Standardized, Managed, Governed, Optimized, and Innovation. Many times, an organization can exist in multiple phases at the same time. You can determine where the majority of your organization operates, and then identify your trail-blazers that should be further along in maturity. Use these trail-blazers to pilot or prototype new processes, technologies, or organizational structures.\n\n## Standardized Phase\n\nThe Standardized phase has three sub-phases: basic, centralized, and simplified. Most organizations find themselves somewhere in this phase of maturity, so look at the behaviors, technology, and processes in your organization to find where your organization might fit.\n\n### Basic\n\nAlmost every organization fits into this phase, at least partially. Here, data is only used reactively and in an ad hoc manner. Additionally, almost all the data collected is stored based on predetermined time frames, often indefinitely. Companies in this basic phase do not erase data for fear of missing out on some critical information in the future. For example, we were working with a company recently that has 30 years of email backups, about 12-10 terabytes, that they are afraid to get rid of. This is not an uncommon practice. \n\nThese attributes best describe this phase:\n\n* Management by reaction\n\n* Uncataloged data\n\n* Store everything everywhere\n\n### Centralized (Data collection Centralized)\n\nAs organizations begin to evaluate their data strategy, they first look at centralizing their storage into big data storage solutions. This approach takes the form of cloud storage or on-prem big data appliances. Once the data is collected in a centralized location, data warehouse technology can enable basic business analytics to derive actionable information. Most of the time, this data is used to fix problems with customers, supply chain, product development, or any other area in your organization where data is generated and collected. \n\nThese attributes best describe this phase:\n\n* Management by reaction\n\n* Basic data collection\n\n* Data warehouses\n\n* Big data storage\n\n* Basic big data analytics (?)\n\n### Simplified\n\nAs the number of data sources increases, companies begin to form organizations that focus on data strategy, organization, and governance. This shift begins with a Chief Data Officer’s (CDO) office. There are debates on whether the CDO fits in the company under the CEO or CIO. Don’t get hung up on where they sit in the organization. The important thing is to establish a data organization focus and implement a plan for data normalization. Normalization gives the ability to correlate different data sources to gather new insight into what is going on across your entire company. Note that without normalization, the data remains siloed and is only partially accessible. Another key attribute of this phase is the need to develop a plan to handle the massive volume of data being collected. Because of the increase in volume and cost of storing this data, tiered storage becomes important. Although in the early stages it is almost impossible to know the optimal way to manage data storage, we recommend using the best information available to develop rational data storage plans, with the caveat that this will need to be reviewed and improved once the data is being used. \n\nThese attributes best describe this phase:\n\n* Predictive data management (beginning of a data-centric organization)\n\n* Data normalization\n\n* Centralized tiered storage\n\n## Managed (Standard Data Profiles)\n\nIn the Managed phase, organizations have formalized their data organization; data scientists, data stewards, and data engineers are now on the team and have defined roles and responsibilities. Meta-data management becomes a key factor in success at this phase, and multiple applications can now take advantage of the data in the company. Movement from a data warehouse to a data lake allows for more agility in development of data-centric applications. Data storage virtualization allows for a more efficient and dynamic storage solution. Data analytics can now run on data sets from multiple sources and departments in the company. \n\nThese attributes best describe this phase:\n\n* Organized data management (Data org in place with key roles identified)\n\n* Meta-data management\n\n* Data lineage\n\n* Data lake\n\n* Big data analytics\n\n* Software-defined storage (storage virtualization)\n\n## Governed\n\nThe Governed phase is primarily reached when an organization has a centralized approach to data and achieved a holistic approach to governing and securing it. The CDO works closely with the CSO (Chief Security Officer) to guarantee that the data and security strategies are working together to protect the company’s valuable data while making it accessible for analytics. Data is classified into different buckets based on criticality, secrecy, or importance. Compliance dictated by regulations is automated and applied to data across the organization. Increased visibility into data usage and security increases with the joint data and security strategies and tactical plans. Basic artificial intelligence is being used widely in the organization, and business decisions are inferred by data. Data can now be gathered and cataloged from all over the company including Internet of Thing (IoT) devices on the company’s physical assets. \n\nThese attributes best describe this phase:\n\n* Data Classification\n\n* Data Compliance\n\n* Data Security\n\n* Basic AI\n\n* Distributed Data Virtualization / IoT\n\n## Optimized\n\nAs organizations’ data collection continues to increase, they need to find efficiencies in automation and continuous process improvement. Automation of data processes is the primary target in the Optimized phase. Specifically, the automation of annotation and meta tagging data decreases the time to derive value from the data. Data has now become too large to move to one centralized place, and a “distributed data lake” architecture emerges as the optimal way to manage data. Machine learning is key in this phase to begin providing information to decision-makers to help optimize business operations and value. Application and data are deployed on network, storage, and compute infrastructure based on historical information and AI models. \n\nThese attributes best describe this phase:\n\n* Automated meta tagging\n\n* Distributed data lake\n\n* Data Inference / ML\n\n* Data-driven infrastructure\n\n## Innovation\n\nThe ultimate organization is in the Innovation phase. It is not just driven by data, but creates new products, offerings, and services based on learnings from data inside and outside their organization. (removed redundant sentence here) This phase is when AI/ML provides invaluable advantages. There are three sub-phases in Innovation: insight, prescriptive, and foresight.\n\n### Insight\n\nInsight is data-driven decision making based on what you can see is actually going on in your ecosystem, for example, in your supply chain, product development, or manufacturing.\n\n### Prescriptive\n\nWhile insight is valuable, it requires human interaction, understanding, and intuition. In the next level, prescriptive, your artificial intelligence is suggesting what you should do based on the insight. This can play an important role in your whole organization, as decisions are data-driven from the supply chain all the way through customer acquisition.\n\n### Foresight\n\nIn this crowning step, the data actually helps create the future. For example, foresight would allow an IT organization to project how much capacity it will need in the future based on historical norms and even factors such as changing conditions with its competitors. Foresight requires a lot of data and training of models, but leads to the ultimate goal of real-time enterprise.\n\nThese attributes best describe this phase:\n\n*\tInsight (data-driven decisions)\n\n*\tPrescriptive (data-driven business)\n\n*\tForesight (create the future)\n\n*\tDeep learning\n\n*\tReal-time enterprise\n\n## Conclusion\n\nIt’s common to feel stuck in one phase and overwhelmed at the amount of change necessary to move into a new phase of maturity. Each step forward, however, is valuable. For example, perhaps you are in a Centralized stage and can look at meta data management. Is there an opportunity to move beyond just cleaning the data and now augment it as well? This type of progressive thinking will move you forward on the chain on maturity in managing your information. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT4-en","image":"./episodes/edt-4/en/thumbnail.png","lang":"en","summary":"In this episode, we will talk about the Information Management Maturity Model and how we can use that model to help our organizations move forward. This model can help you identify where your organization is and where it is going in its information management strategy, ultimately deriving maximum business value from your data"},{"id":111,"type":"Episode","title":"Collaborative Medical Research with Confidential Computing","tags":["confidentialcomputing","cybersecurity","distributedanalytics","sgx","healthcare","data","technology","edge"],"body":"\r\n\r\n## Why Do We Need Confidential Computing?\n\nThe current computer infrastructure is built with a premium on sharing and openness: the internet is free and data should be free. That’s been a problem when it comes to security. We have implemented some solutions that work well; we know to encrypt data when it’s stored and when it’s in transit.  Despite that, data can be attacked in various ways while it’s being read, analyzed, and used.\n\nSensitive data remains vulnerable, whether it’s financial, medical, or location data, both from a visibility standpoint and a data integrity standpoint.\n\nIn dealing with data in healthcare, there are additional layers of complexity. There are many rules and regulations such as HIPAA, and every state has their own regulations around medical data as well. Despite the complexity and the number of governing parties, confidential computing is possible where you have the ability to share data among parties that inherently don’t trust each other.\n\nFor confidentiality, we need to think about a few different factors: data integrity, data confidentiality, and code integrity.\n\n## Critical Data Privacy and Security Problems\n\nAn individual’s healthcare records, especially in the United States, are spread everywhere among doctors, specialists, labs, and hospitals. Most people do not have easy access to their records; it’s next to impossible to create a complete picture of your own health. Confidential computing can help break down these barriers.\n\nFirst of all, it can ensure that whatever data you share, you can trust that it will be protected from a data integrity standpoint; it’s not going to be modified by anyone. It can be confidential, meaning tokenized or encrypted, but it can still be used for computation. For example, if a party wants to do analytics on some data, they don’t need to know information such as names, dates of birth, or social security numbers. So if those portions of the data can be tokenized or encrypted, they can be shared for analysis as long as the other party is trusted.\n\nThis is where security measures such as attestation play a role, so parties can prove their identity. And that attestation can be tied all the way down to the hardware level to the trusted execution environments that the hardware provides. This way, you are not just trusting the transport and endpoint, you are trusting the application and how it will use the data as well.\n\nThere are two basic approaches to this. First is application SDK, which means the developer can decide how to partition their code into trusted and untrusted components. The other approach is to have a runtime encryption system which can be built on top of a trusted execution environment, minimizing the effort required to convert a current application into something that can run in that environment.\n\nWith a hardware-based trusted execution environment protecting applications and data in use, it becomes very difficult for an unauthorized actor, even if they have physical access to the hardware, root privileges or admin rights to the hypervisor, to gain access to the protected application and data. The confidential computing paradigm aims to allow the removal of even the cloud provider from the trusted computing base. That way, only the hardware and the protected application itself is within the attack boundary.\n\nThese computing environments allow the CSPs to leverage the best of what the hardware can offer and the best possible security, over which the end user has absolute control. Each party can determine its own policies and the hierarchies of policies such as state and federal, and each provider of information can determine which policies apply and to whom.\n\n## Real-World Evidence Clinical Study\n\nPulling all the data together and making sense of it is a big challenge in the healthcare industry. The number of privacy settings and data sharing settings that are in place among the different providers, devices, geographical locations, etc. makes it currently impossible.\n\nAI-Vets, Intel, and some partners are working together on this problem. The brilliantly simple architecture allows for use across disparate environments, types of data and policies, yet is able to perform centralized analysis.\n\nAn example of our implementation is one small proof of concept: How do you analyze across multiple parties such as hospitals, research environments, and labs, each with their own data and trials they may be running in a clinical environment? For example, how can we find any correlation among people who take drug X, say for diabetes, and have condition Y, say cancer, when drug X has nothing to do with the treatment of condition Y? These two sets of data would not be in the same place because they are handled by different providers.\n\nIf, however, the providers were part of an ecosystem where they could determine what policies they want to apply at their endpoint, we could have a centralized application, a central research portal, which has connections to these endpoints. There would be third party key management and attestation to verify each other’s credentials and authorizations, so all parties can trust each other.\n\nThat’s one aspect of the trust, but we also need to protect the data that will be pulled out, queried, and transmitted. To accomplish this, we can manage data and applications inside secure, encrypted enclaves. The data is handled using the policies each user sets, such as obfuscating birthdays, social security numbers, etc. This information can be tokenized so it turns into complete garbage in unauthorized hands.\n\nThe central portal can perform a query that spans multiple endpoints and pulls different types of data together into its runtime system and does an analysis on that. So rather than having to pull everything into a data lake and then doing analysis, it’s done in real time. There is no waiting for data to be published or cleansed first by applying all those policies; it happens dynamically and on the fly.\n\nThis allows for tremendous insights. During the pandemic, for example, if we had to wait every day to get the data and run complex analysis on it, that would be hard. If we were able to tap into live data across all these different systems all over the country and around the world, yet be able to share it securely, we could come up with some unique insights that would not be otherwise possible.\n\nWe’ve already seen this in some POCs for clinician sites that we did with our partner Fortanix. They have a product line that makes it easy for different entities to define their policies in a confidential computing environment and verify each other’s identities and manage keys and trusts. The concept of trusted execution environments has been around for some time, and it’s gone mainstream, so it’s become easier to leverage. The use cases for this are fantastic.\n\n## Automated COVID-19 Detection from Chest X-ray Images\n\nIn some use cases, it’s not just the data that needs to be secure, but also the intellectual property associated with some specialized algorithms.  For example, to automatically detect COVID from X-ray images, there would be radiological data, patient data, and there may be a proprietary algorithm to do the analysis. The enclaves can protect both the data and the applications from prying eyes.\n\nThe secure enclave also protects the other machines in the network because if someone pushed something nefarious out onto the end nodes, the key management system would prevent it from being exchanged because it’s not properly attested. Parties choose exactly which data sources the enclave can talk to and it’s locked down, both from what comes in and goes out.\n\n## Electronic Health Records (eHR)\n\nHealth records are a big mess in the United States, with unenforceable paper HIPAA agreements etc. and everything spread across different entities. A use case that may be a good model for us to follow is from the German government. They have mandated that health care data must be stored in electronic health records, and those applications must be deployed in trusted execution environments. The patient is the end user and determines what data is available and to whom.\n\nThat level of granularity in terms of what is available to the end user is tremendous. And not only is all of that data being collected and shared from the different systems, it’s in secure enclaves, so it’s completely secure from the outside world. If someone unauthorized did get access to the data itself, it would be completely meaningless.\n\nThese trusted execution environments are the first step in the direction to controls that are easily understandable and easily enforceable. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Nick Bhadange"],"link":"/episode-EDT40-en","image":"./episodes/edt-40/en/thumbnail.png","lang":"en","summary":"Breakdown the barriers to accelerating medical research for the cure to cancer with confidential computing. Nick Bhadange, Technology Specialist, AI-Vets and Darren Pulsipher, Chief Solution Architect, Public Sector, Intel, discuss the need for confidential computing in healthcare and the potential benefits through use cases."},{"id":112,"type":"Episode","title":"The Black Art of DevOps","tags":["devops","people","technology","compute","devsecops","cybersecurity","multicloud"],"body":"\r\n\r\nLet’s take a look at where DevOps fits into your infrastructure.\n\nAt the bottom of a normal stack, we have a physical layer which could mean a cloud, data center, IOT devices, or legacy infrastructure.\n\nOn top of that, there is normally a software-defined infrastructure that abstracts away the complexity of managing the individual pieces of hardware.\n\nNext is a service management layer, which includes the container ecosystem virtualization and a distributed information management layer, which includes the data plane, data lakes, and everything managing your data.\n\nThen comes the application layer. Application developers use the services inside the application layers. Right at the interface between the application layer and the data management plane and service management are the SecDevOps or DevOps toolkit. These tools include security and identity aspects that provide a secure way of continuously integrating and deploying your products.\n\n## Application / Workload Layer\n\nAt the top of the application and workload layer that feeds SecDevOps are three types of workloads: event-driven workloads, procedural workloads, and a hybrid of the two, which are GUI- or UI-driven workloads.\n\nA simple example of an event-driven workload would be that a purchase order arrives in your system causing other things to happen. There can be sequential or parallel steps, interaction with humans, and automation and interaction with several different applications or subsystems within the company.\n\nMany workload automation tools are available. Some are scripted and some use robotic process automation, which are more GUI- and UI- driven. These tools work on the automation of services underneath, so the workloads drive service interaction.\n\nServices traditionally fall into three major categories: applications, such as off-the-shelf products like Word or an SAP application; complex services, which are built for a specific purpose, such as a MEAN stack with Mongo; and simple services, which do one thing, for example MongoDB which stores the database.  \n\nThere is a new category because of the growth of AI and ML. A lot of services don’t do much without a model attached to it, so we’ve added AI models in to the service layer, which we treat much as we would a simple service.\n\n## Developer Day in the Life\n\nAfter we understand the workloads and services, we can look at what a developer typically does.\n\nA developer will write some code on their workstation and run a few functionality tests. Then they check the code into GitHub, for example, and a continuous integration continuous delivery (CICD) pipeline kicks off. It runs security checks against the code, perhaps lint, static analysis, and dynamic analysis.\n\nOnce it passes those tests, it will usually check into an integration branch where other people on the development team grab the data and develop it and integrate their code with the developer’s code. Then, when it has passed their tests, it is pushed out into a test stage. Once through that stage, it will go into production.\n\nThis is a typical CICD pipeline, which has been around for decades. Over the years, the different ways of describing pipelines has been consolidated and standardized, limiting complexities and errors.\n\n## DevSecOps Stack\n\nThe pipeline is only one element of a SecDevOps stack.\n\nOther necessary elements include a registry and a repository. Think of these as versioned repositories to keep artifacts that are generated during the CICD pipeline so they are easily available to use over and over again.\n\nAnother important element is an automation framework. This helps to alleviate the human labor of running tasks such as security checks or promoting builds from one stage to another. The tools for automation are mature and training is available, so a good automation framework should be fundamental.\n\nAlthough environment management often grows organically over time, it makes sense to manage and architect the environments appropriately to get more reliability and repeatability.\n\nA key element underneath it all is a security profile. You should be able to have the ability to define security profiles, so that they can be used in multiple environments and across multiple application stacks.\n\n## Registries / Repositories\n\nThere are usually at least two different types of repositories. The first is a staging repository, where you can generate images (a gathering of all the code that’s needed in order to spin up a container, for example), and store things like identity and secret keys. This repository contains everything you need to move things into production. Some organizations may have multiple staging repositories as different elements move through different stages of maturity until they reach the production repository. You want to be able to go back to previous versions if necessary.\n\nIn the production, or golden, repository, images are locked down, notarized, and encrypted. Only things in the golden repository every get moved into production.\n\n## Stages\n\nThe best way to think of stages in the CICD pipeline is that each stage works in a single environment. For example, in a build stage, there is a contained build environment with policies. Only when all the steps in this stage are accomplished can things move to the next stage. This avoids chewing up resources with parallel builds and runs that may ultimately fail. At the same time, it’s best not to have so many stages that they hamper progress, so a careful, defined plan is important.\n\n## Steps\n\nInside the stages are steps where the work actually gets done. In building and testing software, steps can be run in parallel or sequence; there are many tools that allow you to define these operations. Although some have a GUI for this, most developers prefer a textual format because it enables version control of the pipeline and steps, allowing security checks against the pipeline.\n\n## Pipeline\n\nWith stages and steps defined, you have a real pipeline. Instead of defining one pipeline for all of your applications, which typically fails because it becomes overly complex with a lot of conditions or too restrictive, I recommend using template pipelines and modifying as necessary, making sure they adhere to compliance standards and regulations. Getting an appropriate pipeline established at the beginning of a project is important, as is flexibility as the project progresses.\n\n## Environments\n\nInstead of creating ad hoc environments, it’s best to create them with intention up front. DevOps or SecDevOps can inject security policies and compliance across all the different projects, ensuring security.\n\n## Service Stack\n\nLet’s look at how developers work, which is on services nowadays. Even if developers are working in a monolithic application, they tend to group their work into a functional units like databases, business logic nodes, or transport layers. For example, using a simple service such as MongoDB. When a developer runs that container on their laptop, it gives them the functionality they expect to store data in a non-SQL way in a document. On the laptop, It may be the only container running.\n\nIn a test or dev environment, there could be multiple instances of that service running, and the developer may deploy a cluster of MongoDB services and connect them together for a test. The service is still a Mongo DB service, but its behavior changes based on the environment that it’s in. The goal for developers is to write code and check it in against the MongoDB service on their laptops to guarantee it will run all right in production.\n\nA simple service like MongoDB is necessary, but by itself, not very useful. Complex services such as LAMP stacks or MEAN stacks are more important. These are multiple services running together, acting as basically one service. Bundled together, this deploys a complex service on a laptop and there are two or three simple service containers that are running, giving developers the necessary functionality to check in their code.\n\nOnce the code is checked in, it kicks off into the development pipeline where the developer is integrating with other people. The same complex service can take on a whole different way of doing things. Many security policies can be attached to that complex service to help make sure it’s secure, reliable, and resilient.\n\n## Service/Application Definitions\n\nIt’s important to understand the concepts of simple and complex services because software developers must define how to get them to work. There are a few definitions. One is called an image definition. These are frequently in the container world, called Docker images. The Docker file defines what is in that image. This is considered a simple container by itself, although people are starting to use containers for complex things.\n\nWithin service definitions, we can include multiple imagine definitions, for example, Docker Compose, Kubernetes Operators, Helm Charts, Terraform, and even CNAB. These are tools that let you define a service. A service is more than just the container; it’s the environment in which the container is running. It could include network definitions, volume connectivity, or even deployment policies.  A complete “service definition” has image, configuration, and provision definitions.\n\n## Putting it all Together\n\nWhen a developer is creating a new service, they’re not just developing the code for the image; they are also defining the environment, or configuration, in which it needs to run. This is where the mesh of your environment and the service definition can come together. At run time, it will produce the environment that is needed in order for the container to run effectively in a repeatable manner, so that you can easily move code from running on a desktop to running in full production as quickly as possible. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT41-en","image":"./episodes/edt-41/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Public Sector, Intel, defines common DevOps terms and explains where DevOps fits into your organization."},{"id":113,"type":"Episode","title":"Securing the DevOps Pipeline = SecDevOps","tags":["cybersecurity","devops","compute","process","technology","multicloud","devsecops"],"body":"\r\n\r\nA study from over 20 years ago on the return of security investment showed the earlier security is built into the development cycle, the cheaper it is than waiting until the end of the cycle. Although we have known this for two decades, it’s still a work in progress. \n\nMost development companies have security as part of their overall development process, so there has been a lot of headway, but it’s a journey, not a sprint. It’s about understanding all of the different exposure points and weaknesses and being able to provide the right security processes to those problems. \n\n## The Attack Vectors\n\nOftentimes people think of attacks as vulnerabilities of employees or packages, for example, and forget the process part of the story. On the operation side is the time it takes to close a vulnerability. On the other side is in the development and delivery of products. There are various break points along that chain, and those have been exploited recently in the latest stages of the build process. As far as customers were concerned, the code that was updated was legitimate because it came right from the source. So we need to think carefully about where to put security into the process. \n\n## Security Built In...\n\nAgile, CI/CD, DevOps,…Where is Security?\n\nWhile we tend to look at needing security at the transitions from developer to QA, and from QA to operations or deployment, security should really be injected into the whole build cycle, not just at a few checkpoints. The approach should be continuous security. \n\n## Security Built In….\n\nMake Security Part of Every Phase\n\nContinuous security is challenging. Most developers and QA aren’t security trained; this is an uphill battle. The industry tried this approach in the early 2000s, but ran into three problems. First, the turnover is too high. Second, the security landscape changes too rapidly to keep them up to date, and third, since it’s not their day job, the right behavior was not incentivized. \n\nHow do we, then, integrate security into the process, automate the key things we want to do, and get out of the way of developers so they can do their jobs, which is to build, test, and deploy the code? The security process can really shine by building it into those automations that you are already doing in DevOps such as automated unit testing, automated quality and regression testing, automated build, and automated deployment. This will not solve all the problems, but it will raise the bar significantly so you can focus on the hard challenges around security.\n\n## Security Built in….\n\nMeans Security is Baked in, not a Single Step or Stage\n\nSome common tools already provide some automated security that points out vulnerabilities. For example, GitHub will run security checks on projects using Node.js code and all the included packages. This can be helpful, but it is too late; the security should be built into the pipeline before it gets checked in. \n\n## Security Built in….?\n\n## How Do We Get There?\n\nCurrent security breaches highlight that security must be injected at every stage of the process, including between build and production, and right before the script runs to build the application. In addition to injecting security into the build process, we need to secure the build process itself; it’s been a gaping hole for a long time. \n\nMany companies that do internal development are now taking a closer look at their build process because of the recent breaches. This is good, but it can’t stop with these knee-jerk reactions to each attack. We need to think holistically and not wait for the next weak link in the chain. \n\nSome practical ways to secure the process are to treat the build server as a critical asset in the overall infrastructure and apply the same rules and controls to that server as you would for your core systems. Proper credentials, firmware secure boot, verifying code, auditing and logging the system, etc… throughout its life is then building into the DevOps process when someone clicks the button.\n\n## Built to Last\n\n### From Solutions to Services and Beyond\n\nMany people don’t think about the script itself as a target. It doesn’t matter how many good modules are included if the script itself isn’t protected. A few ways to protect the script is to run a checksum, and it should be versioned, checked and signed. This adds complexity for DevOps, but there are tools that can help. \n\n### Build Once, Deploy Everywhere\n\nJust like we automate the development process, we can build in the automation for implementing these controls and checks.  Automation prevents another person from potentially messing with your builds, but we do also want to make sure there’s a human receiving results and verifying audits.  \n\nThe tools you are already using can be extended to add security automation and checks such as those to do continuous development integration for the Agile cycle, or automation tools in the Linux world. \n\nOrganizations can also distribute their security people throughout the business development teams so when things go wrong, security people are already embedded in the process. Two places you want to make sure you have security people are in infrastructure to support, for example, your Agile process, and in product management to get security requirements for the product requirement definition phase before it even gets to a developer. \n\nThere is always a shortage of enough trained and capable security people and also funding to hire the right people because of high demand. A few options are to train the people you already have and give them the necessary tools. You do not need a crypto guru at every step of the process. Another possibility is instead of having each coder be responsible for coding authentication, credentials, and protocol in a secure build in an infrastructure library, have a team build modules that are in your languages and your environments that do all the security functions. The coder can pull the module, and it handles the hard work.  That way, you build once and deploy everywhere. \n\nWe are seeing companies provide SaaS security tools, cloud-based services that can be consumed for your application and your runtime environment. This is a great step in the process. There are companies that provide security injection points such as application security in a fast style environment. These application checks such as input sanitation and input validation can be embedded into your functionalist environment, but that’s still waiting to the end. Remember that the earlier in the process you start security, the cheaper and less painful it becomes. \n\nAll of this does, of course, require more integration work. Developers can be wary of the work involved, but if a framework with built-in security exists (and there are prototypes out there such as Ruby on Rails and certain cloud infrastructures), it can save many hours. You still have to make sure, however, that you don’t rely only on the platform for security, as it could be a single point of failure. \n\n## Automation Will Set You Free\n\nThe security breaches in the last six months have been profound. Here are some key points of advice: \n\nSecurity should be integral in the whole lifecycle from requirements forward. Security must be in the DevOps cycle itself, not just in coding and testing, but also in the infrastructure that drives that process. \n\nWhen building security tools and objects through modules, build once, make it modular, and deploy everywhere. \n\nLeverage services that let you rely on someone else’s expertise to augment your own, underfunded, cyber team. \n\nAutomation will set you free. Automate as much as you can to make security easier and faster and reduce friction for your developers and testers. With automation, you can eliminate 80 percent of what we call the stupid stuff so you can spend your limited resources on the hard problems. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT42-en","image":"./episodes/edt-42/en/thumbnail.jpg","lang":"en","summary":"In this episode, Darren Pulsipher, Chief Solution Architect at Intel, and Steve Orrin, CTO of Intel, Federal, discuss why and how the DevOps pipeline must be secured. The only way to deliver solid, resilient, and secure code is if security is built in, and the earlier the better."},{"id":114,"type":"Episode","title":"The Role of the CIO in Cloud Adoption","tags":["cio","cloudadoption","compute","cloud","csp","multicloud"],"body":"\r\n\r\nIn part one of this interview, Darren Pulsipher, Chief Solution Architect, Intel, and Doug Bourgeois, Managing Director, GPS Cloud Strategy Leader, Deloitte, talk about the cloud migration and the role of the CIO.\n\n## CIO Heritage\n\nDoug got involved with the cloud early on while he was working for the federal government running a large shared services organization. He recognized cloud benefits to his service provider organizations, and also realized the value to his organization. First, it would save money at the infrastructure level, and second, it was an opportunity to build cloud while virtualizing to help with the server sprawl problem. For one particular service, the cost cut to the end user was 40 percent.\n\nIn shared services such as cloud, once you reach economies of scale, you can provide the services more economically than most organizations can do on their own.\n\n## The Shift to Cloud\n\nA dozen years ago, organizations, for the most part, were private cloud centric. They were improving their data centers to incorporate a combination of consolidated multi tenant, and with some automation capabilities built in. The pendulum swung about three or four years after that over to the public cloud with the large hyper-scalers (AWS, Azure, Google).\n\nAbout three years ago, organizations moved to an equilibrium in the hybrid cloud. People realized that a wide variety of systems in their portfolio lend themselves to different models, some private, some public, some hybrid. Overall, there is a more holistic approach today to match systems and clouds with specific purposes.\n\n## Putting the Information in CIO\n\nIt’s now more important than ever for CIOs to have a profound knowledge of what’s going on in their organizations, becoming closer to the mission and business goals to best meet its needs. Whereas before, a CIO might be just providing infrastructure, now they need to make educated architectural decisions based on what’s available. There are two reasons for this. The first is the proliferation of data, artificial intelligence, analytics, and machine learning into core business capabilities requires fundamental business understanding. The second is the evolution of cloud has reached a new phase, the digital age, where core systems of the organization must be modernized to improve the service capability for their end users.\n\nThis journey has started to move the CIO back to where they belong, in managing information rather than focusing so much on infrastructure. Many CIOs have been relegated to the infrastructure box, when they could be given the opportunity to do something truly transformative.\n\n## CIO Positioning for Success\n\nSo what does the journey from Chief Infrastructure Officer to Chief Information Officer look like?\n\nOne way is to position yourself to be in charge of something new that the organization is trying to do, maybe a new process or getting in a new market, or even a business unit that isn’t up yet. You have continuity and perspective as you have worked with all the different application owners, and so you are uniquely qualified to take the initiative forward. Another, more common way, is through a negative event, where it becomes obvious change is necessary. A disaster can be the catalyst for a CIO to lead the way to a real transformation.\n\nJoin us for part 2 of the interview ….\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Doug Bourgeois"],"link":"/episode-EDT43-en","image":"./episodes/edt-43/en/thumbnail.png","lang":"en","summary":"In part one of this interview, Darren Pulsipher, Chief Solution Architect, Intel, and Doug Bourgeois, Managing Director, GPS Cloud Strategy Leader, Deloitte, talk about the cloud migration and the role of the CIO."},{"id":115,"type":"Episode","title":"Cloud in the Digital Age","tags":["cloudmigration","cloud","compute","process","deloitte","multicloud"],"body":"\r\n\r\n## The Typical Cloud Migration Journey for an Organization\r\n\r\nOver the last five or six years, the methodologies, tools, and experience in cloud migration have matured to repeatable processes. \r\n\r\nThe first step is to decide on your migration priorities because it’s going to happen in phases, not in one big move. This does not take a lot of time or resources, but it is critically important. An extreme example is that you would not want to pick the mainframe as the first system to move to the cloud, rather a more self-contained system such as email. \r\n\r\nCompanies such as Deloitte have developed and invested in discovery tools that help accelerate the migration processes. These tools will grab a broad set of data, run an algorithm that looks at complexity, and rank all of the systems into different categories. Understanding the configuration and integration points of existing systems and compatibility of software components is fundamental to cloud migration. We also need to look at boundaries and compliance frameworks such as PCI or HIPAA. Building out landing zones for these environments in the cloud is phase two in the process. \r\n\r\n## Value Proposition of Digital Modernization\r\n\r\nAt times there is a substantial amount of prep work for migrations.  The first wave is the easiest with the least amount of modifications, but after that, in phase two, there may be upgrades or changes to operating systems, re-platforming, or moving to a different types of database, for example. The third wave often involves more antiquated client servers or proprietary architectures that require significant re-architecture and may take months to prepare for cloud readiness.\r\n\r\n## Digital Modernization and Cloud Migration\r\n\r\nIt’s important to distinguish that cloud readiness is not the same as cloud optimization; that comes later. \r\n\r\nIn many cases, the driving force behind moving to the cloud is business driven rather than technology driven. For example, a client may not want to continue a lease just to house a data center, or are shifting their physical offices. In those cases, there is a time factor where it makes sense to execute the migration based on readiness rather than optimization. \r\n\r\nOnce in the cloud, you need to optimize because the cost drivers are different in the cloud than in the legacy data center. The cost of a data center, after making the initial investment, is relatively hidden, whereas the cloud is more of a rental agreement that goes on in perpetuity. Many times in the legacy systems, we solve problems by throwing on more memory, more CPU, or more storage because it works to a certain extent, but this creates inefficient systems. If we simply move these inefficient, resource-intensive systems to the cloud, the cost model is much higher than it needs to be, hence the need for optimization. \r\n\r\nSome of the optimization process might be a process change. For example, for an organization in Canada, their cost went through the roof when they did a lift and shift of an SAP instance into the cloud. They realized that they weren’t using this instance at night or on weekends, so they went from a 24/7 model to a 16/5 model. Making this switch saved them a substantial amount of money. So there are ways of making a small effort, high value return with different approaches. \r\n\r\nWe are finally seeing, after being more than a decade into cloud, an emerging trend of finding value in a change of business strategy rather than in infrastructure.  The COVID-19 pandemic was certainly a factor in accelerating this change. A perfect example of this is telemedicine. It already existed, but had been stagnant for five or six years before the pandemic; now this model is the norm. \r\n\r\nTransformative innovations are happening in the cloud. As more systems move to the cloud, industries will continue to try and adopt different models with new, transformative capabilities. \r\n\r\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Doug Bourgeois"],"link":"/episode-EDT44-en","image":"./episodes/edt-44/en/thumbnail.png","lang":"en","summary":"In part two of the interview, Darren Pulsipher, Chief Solution Architect, Intel, and Doug Bourgeois, Managing Director, GPS Cloud Strategy Leader, Deloitte, continue their discussion about the cloud migration."},{"id":116,"type":"Episode","title":"Not All Cores are Equal","tags":["multicloud","compute","optimization","workloadplacement","workload","migration","process","cloudinstance"],"body":"\r\n\r\nIntel has been hiring cloud solution architects to help customers move intelligently to the right cloud instances for their workloads. Stephen has recently come on board at Intel and has a long history in databases in general, with the last seven or eight years focusing on the cloud, leveraging its resources for customers. \n\nSome of the most difficult aspects that Stephen has run into in helping customers move to the cloud are the pricing and the expectations for the move. Cloud service providers often claim customers will save money by moving to the cloud, but this is often not the case. \n\nOne reason is that there is constant change in the competitive cloud service provider market in features, functions, and abilities. Another reason is ignorance, for example, believing that a core VCP is the same across the board as others. Migrating with a lift and shift mindset can also be very expensive. Optimization is key for intelligent cloud migration. \n\nTo decrease costs and potentially save a large amount of money, you must choose the right instances for the right performances. Not all cores, or virtual CPUs, are equal. Within AWS, they have the M4, Intel’s Broadwell, and the M5, which is a mix of Cascade Lake and Skylake, and various workloads can take advantage of those platforms very differently. \n\n## CoreMark Performance\n\nCoreMark is a single threaded application that tests register moves and simple additions for any number of CPUs. Across three generations of Intel processors, Broadwell, Skylake, and Cascade Lake, this test shows very little difference among them. This information has been erroneously used in guiding customer decisions, as the test was is not an effective way of measuring the differences in the processes themselves.  There can be huge differences in performances with different workloads. \n\n## Database WL Performance\n\nStephen performed digital tests on real workloads, focused on open source databases. The Cascade Lake environment was three times better than the Broadwell environment for open source, which makes sense given the advances in speed and on-chip advances that take advantage of low latency access. So going up two generations gives two and three times better performance on these workloads. \n\nThe workload and the core must be considered together. For example, a customer might be tempted to switch to a VCPU that Amazon says is 10% cheaper. AWS might, however, pack cores onto these processors and you are now moving from an environment where you’re one of 48 virtual machines on a box to one where there’s 128 virtual machines; your access to memory is limited, and you might get 60% less performance for a 10% savings. \n\nOrganizations often believe that if they outsource to the cloud, they no longer need a systems architect. It’s true that architects are no longer racking and stacking machines, but they do it now virtually and need to understand what different instances provide for different workloads. \n\nIntel is working on a tool to show which workloads run best on which instances. Meanwhile, Intel has some general guidelines and cloud solution architects to help guide customers.  \n\nIn addition, a lot of testing is important to understand where to place workloads, but it’s also important to test and benchmark to validate that you are getting what you expect from a provider. \n\n## Database WL Performance – Non NVMe\n\nIntel did some testing on workloads on fresh installs on 64 core fixed 64 VCPU instance. After getting the initial number, another instance was instantiated and the same test was run again. The results varied dramatically from the same test each time. This could be because there may be a mix of processes that are offered at a certain tier level, or there could be some distance and additional latencies to block storage attachments, for example, or even noisy neighbors. So it is worthwhile to test to make sure the system meets expectations. \n\nIn short, to get the best value out of the cloud, you need to educate yourself, test the systems, and take advantage of available help. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steven Holt"],"link":"/episode-EDT45-en","image":"./episodes/edt-45/en/thumbnail.png","lang":"en","summary":"In this episode, Darren Pulsipher, Chief Solution Architect, Intel, and Stephen Holt, Cloud Solution Architect, Intel discuss cloud optimization and studies that show that cores perform differently for various workloads."},{"id":117,"type":"Episode","title":"Securing your DevOps Pipeline","tags":["devops","compute","technology","process","devsecops","cybersecurity","zerotrustarchitecture"],"body":"\r\n\r\n## The Attack Vectors\n\nMany attacks happen at the people level. Some of the most recent insidious attacks have focused on phishing and social engineering on individuals inside of DevOps. We need to train people better at all levels. In one case, it was an intern with access to the keys that succumbed to a nefarious scheme. \n\nAnother vector is technology: traditional denial of service attacks, SQL injection attacks, or buffer overflow attacks. The latest incidents use both people and technology to attack the process. They are insidious because they’re happening around the build process and can be very hard to find. They can also propagate malicious code through your customers. To instill customer confidence, organizations must have a strategy to secure the supply pipeline.\n\n## Pipeline Security\n\nNot every build pipeline is the same, but in general, they consist of four stages with environments: development, build, test, and production. These stages are easily broken down into multiple stages depending on the type of product that you are developing, but the environments link to those different stages. \n\nWe need to look at the whole process, which includes software, hardware, and processes, and take a different approach rather than just focusing on infrastructure, which most have done effectively. \n\n## Host and Infrastructure Security\n\nAt the bottom of the stack, you need to secure the hardware in the development and build environment. The test environment will be a little different because you may want to run tests that involve injecting malicious code. Production environments are usually locked down well, especially now that more companies are offering software as a service. In the production and build environments, you need to do all the typical security measures that you would do in production or SaaS environments. \n\nThere are three key elements in these environments. The first is detection. Detection and remediation is a well known safety measure using service logs that use a platform such as Splunk to find anything out of the ordinary. Make sure to do this not just in development environments, but in build and production as well. \n\nKeep in mind that in the test environment, you will need multiple test environments, some more secure than others on the detection side since you want to inject erroneous code into your testing. Don’t just peanut butter security across all the environments; security profiles can be different for each environment. \n\nPrevention is the second key. This means being smart with hardware, making sure things are patched properly, having the right security updates, and doing it in an automated way. This should happen across all the environments, including the test environment, and especially in build and production.\n\nThe third key is part of prevention: hardware root of trust. A chain of trust can be established from the hardware, through the firmware boot sections, all the way into the hypervisors and operating systems. The root of trust can be carried into the development, build, test, and production environments. This can include secure containers and secure virtual machines.  For example, I like to store my encryption and hash keys in hardware like a TPM module and then also with Secure Guard extension from Intel. Even if someone got into the machine, they will not be able to steal those keys. \n\n## Trusted Executables\n\nSetting up trusted executables is the next step. This means that you can run security checks against code that is checked in and built, and then check in the hash with those changes. If there has been anything injected into the code base, you can detect it, since there shouldn’t be any changes to code during the build process. \n\nIn a new stage of security checks, you can run static analysis on the code or dynamic analysis on the code or security violations. There are some great tools that you can easily integrate into your typical DevOps pipeline, whether you are using Jenkins or GitHub workflows, for example. \n\nOnce an executable is built, create the hash immediately, and that hash should be versioned with the executable; it should remain the executable that goes through all the testing and pushed into production. That hash will guarantee that nothing has been tampered with. \n\n## Attested and Secured Images\n\nTypically, there are multiple repositories, or executables, to use for code. The hash that is created at build time is now in the registry, and you can attest these. You can secure those in the images so they can’t be modified.  If someone needs to go back and make a small change, say a label or metadata, it’s important not to make the change and give it the same version number. It’s best to go through the cycle again even if it takes more time than to have manual processes mucking around with your binaries. \n\nNow, you can take that same binary that you’ve run all the tests on and push it to production. At this point, it is a mistake to rebuild the source code. It’s best to push the original build into the production, or golden, repository. This repository is the only place from which images, binaries, or VMs, for example, should be retrieved. All images should be notarized and attested. If you have confidential VMs or applications, or want to make sure that they only land on certain hardware, you can make those kinds of lockdowns. You can encrypt the VMs, containers, or even binaries and lock them with the key that is stored in your build and production systems. \n\n## Injecting Security Tools\n\nSecurity tools must be injected into the build process. Instead of grabbing open source security libraries or recreating ones that already exist, your security engineers should be involved so they can choose tools that can be easily consumed and reused by the development teams. A good example is basic authentication: user login. There should be a common library rather than each application with its own. \n\nIt’s important to treat these security libraries and tools much like you would any other software development program that you are sharing across your organization. Sharing will decrease time and increase security across your whole ecosystem. \n\n## Build Once, Deploy Security Everywhere\n\nOnce you’ve established your development security teams, make sure that you are injecting the policies and tools in all of your products and environments. Great technology exists today that allows you to manage multiple environments. So, when a new application is spun up, it is spun up in a security profile with your own VM or container images as base images that the development teams are using. By configuring security into your VMs or containers into your base images, you get instant security compliance across the different environments. There is also the ability to integrate with security tools, so that if you do find anything unusual in the application, you can notify securitry tools. Don’t try to create security tools that handle one hundred percent of cases, because you will never finish them; go for eighty percent as a strong baseline and create them so application developers can innovate the last twenty percent if necessary, in conjunction with your security team. \n\nA last bit of important advice is to automate everything that you can, especially in the DevOps pipeline to prevent malicious injections. Protect your pipeline; protect your process. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT46-en","image":"./episodes/edt-46/en/thumbnail.png","lang":"en","summary":"In part two of this episode, Darren Pulsipher, Chief Solution Architect, Intel, gives practical tips for securing each stage of the DevOps pipeline, including protecting the hardware and software stacks with hardware root of trust, Security scanning, attested and encrypted containers/VMs. and more."},{"id":118,"type":"Episode","title":"Looking Forward to 2021","tags":["covid","remoteworker"],"body":"\r\n\r\nGreg leads Intel’s US sales and marketing. Intel’s customers are primarily the system OEMs, and Intel’s sales force works daily servicing those accounts and helping them build systems around Intel’s compute and memory technologies. Currently, there is an unprecedented demand for compute.\n\nIntel’s teams work with software companies, cloud providers, Fortune 1000 businesses, schools, and government agencies to make compute accessible and help identify trends and apply use cases the solve business problems in a way that improves society. Intel, then, works as a trendsetter and identifies new trends to make sure customers have the right products.\n\n## COVID as an Accelerant\n\nCurrently, the trends all relate to the COVID pandemic. Organizations spent most of 2020 in crisis mode, with IT deploying technology to keep businesses running and then adapting to the new normal. Now, there is stabilization where IT departments can look forward rather than just responding to the crisis at hand.\n\nOne lesson is that IT can move much faster than we ever thought. For example, it’s amazing how quickly almost every industry was able to switch to remote work. Yet COVID worked as an accelerant rather than a catalyst. Most of the changes were already planned, but COVID compressed timelines. Instead of, say, an 18-month planned rollout of Office 365, it happened in a week and a half because it had to.\n\nOne of the main reasons for the speed of change is that a CIO couldn't possibly make every decision that had to be made during the crisis, so the decision-making was pushed down to empower those at the front lines to work quickly and do what was best for the business.\n\n## Hybrid Work Model\n\nOne change that will take some time is figuring out the new hybrid work model. Intel, for example, is working on expectations for how often employees should come in to the office, after a large majority of employees have spent a year working from home. IT departments are preparing and investing in tools that enable collaboration with some employees in the office and some at home.\n\nOrganizations may not know what their model will look like for many months, so IT departments must be flexible in their approach.\n\n## Employee Experience Apps and Automation\n\nOnline projects that were set aside in 2020 are coming back now in this new hybrid work environment. Employees want to engage with their business in the same way that they do as consumers in software as a service. Consequently, there are many apps being developed around employee experience.\n\nIn addition, there has been much progress around the utilization of bots. For example, Intel just rolled out an HR bot to improve and automate employee experience, accessing all the HR services. This leads to the need for more AI and AI automation, which is driven by machine learning\n\n## Frictionless, Contactless Customer Service\n\nMany of Intel’s customers are setting up for frictionless, contactless customer services in government, retail, and entertainment. One industry that accelerated during the pandemic is telco, which is going to make these frictionless deliveries much easier.\n\nThe capital investment to build out telco infrastructure and 5G is supporting more IOT, remote-type devices now that, pre-pandemic, were going to take years to materialize.\n\n## AI and Federated Learning\n\nIntel has seen a huge spike in working with AI and protected data in areas such as medical discovery and in the financial markets. With Intel’s new third-generation Xeon CPUs, there are secure compute enclaves (SGX), protected memory, that cannot be accessed outside of the system. The use cases that are getting deployed are around AI and federated learning, where users’ and companies’ data can train global models, but the data is not shared in a central repository. With AI, the concept of federated learning, and Intel’s SGX, that data can be protected. Privacy and regulation roadblocks to data can be removed. For example, a hospital’s data, or an individual patient’s data can be protected, but still used to train a more global model with great benefits.\n\n## RPA\n\nMany companies are finding ways to automate tasks, in some cases mundane tasks, to free up their employees to work on higher value projects. This trend has skyrocketed in the last six to eight months, with a lot of growth in the market. In the last five years, RPA markets have received over two billion dollars in VC funding, mostly from the New York City financial markets.  Outside of RPA, there are a lot of automation frameworks that people are using to deploy infrastructure in their data centers and also seamlessly in the cloud.\n\n## Edge Compute\n\nIntel has started to build out reference architectures to help companies build out their edge compute. The important science here is connecting the edge compute all the way back to the cloud infrastructure and building both a hardware and software stack, a control panel, and automation. This is another area of incredible investment.\n\n## Moving Forward\n\n2020 was a year of chaos, unprecedented adaptation, and accelerated change. Now, in a more stable 2021, companies can build on the resulting lessons and trends. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Ernst"],"link":"/episode-EDT47-en","image":"./episodes/edt-47/en/thumbnail.png","lang":"en","summary":"Greg leads Intel’s US sales and marketing. Intel’s customers are primarily the system OEMs, and Intel’s sales force works daily servicing those accounts and helping them build systems around Intel’s compute and memory technologies. Currently, there is an unprecedented demand for compute. Intel’s teams work with software companies, cloud providers, Fortune 1000 businesses, schools, and government agencies to make compute accessible and help identify trends and apply use cases the solve business problems in a way that improves society. Intel, then, works as a trendsetter and identifies new trends to make sure customers have the right products."},{"id":119,"type":"Episode","title":"VAST Data Revisited","tags":["dataarchitecture","data","optane","technology","vastdata"],"body":"\r\n\r\nVAST Data has been growing rapidly in the last six months, with an expanding team and impressive sales such as about 70 petabytes of flash to the federal government. In addition to general file sharing with their all-flash storage systems, they have interesting use cases in areas such as next gen sequencing, confocal microscopy, and wind tunnel simulation workloads. \n\nThe company’s platform allows for solving random read problems because all the data is on flash without the exorbitant cost; the platform handles a high volume of data with low latency. \n\n## VAST Technology and Intel Optane Drives\n\nVAST uses Intel’s Optane Drives to achieve read/write parity, among other benefits. Since Optane is persistent memory, there are no cache coherency issues, and there is no need for DRAM, which is a great architectural advantage in averting a failure or data loss event. The total cost per user is also decreased because you don’t have to buy big machines with lots of memory for file storage if you want more speed. That tier is eliminated. \n\nScalability and flexibility are two other advantages, as you can add more data boxes (D boxes) without affecting performance, and they are completely compatible even with multiple generations of flash.  There is no single point of failure , and there is no theoretical limit; VAST has tested up to 100 petabytes. \n\nYou can improve performance by adding client boxes (C boxes), independent of D boxes. Capacity with flash is basically free, but you have to expose that flash with CPU. The C boxes are completely stateless, so you can grow or shrink them on the fly. VAST guarantees uptime because you have full access to every PCP on the back end.  \n\nDepending on the performance requirements of the customer, there can be different numbers of D boxes and C boxes; for example, a client may not need any more performance, but more capacity, so the cluster may have, say, 11 C boxes and 37 D boxes. \n\nIf you have one C box and one D box, you have about 40 gigabytes a second of bandwidth, which is the base model. Every addition of a D box adds another 40 gigabytes of bandwidth. One C box does not saturate all the IOPS, so if you add another C box you can get 350,000 IOPS out of a box. Since flash is CPU bound, the more CPUs you add, the more it allows you to scale. \n\n## New Storage Business Model\n\nVAST Data’s new product, Gemini, allows customers who need high performance without much capacity license only the amount of hardware that they need from their contract manufacturer, which results in a huge cost savings. Customers can grow and pay for capacity as they need it, rather than incurring an upfront cost for future capacity needs. \n\nFrom an OPEX perspective, this makes sense for many companies. For example, for one customer, a 30 petabyte all-flash system that does a terabyte and a half of bandwidth, the cost is less than S3 infrequently accessed from Amazon. That’s just for one year; moving forward, it’s actually less than Glacier if you look at it from the way Amazon charges, per gig per month. And there is the flexibility to move back to a CAPEX model if that makes more sense later on. \n\nIt’s important to note that VAST is presenting an appliance, not storage as a service, which can be a support nightmare because of all the variations of hardware and firmware, what drives you are supporting, what interconnects, etc….resulting in a crazy amount of complexity. VAST is still shipping the same boxes and servers; it’s just a different business model to allow flexibility in consuming storage.\n\nSome interesting areas where VAST could be useful is in medical research such as digital pathology with an astronomical amount of data that is often difficult for researchers to access because it is on cheap, slow storage. Precision medicine based on a person’s genetic makeup is also a possibility. In addition, running AI against packet capture could be helpful in predicting attacks from advanced adversaries.\n\nAnother promising use case is a customer with all their data in a VAST all-flash storage system rather than in the cloud, but does all their compute in the cloud, perhaps bringing back just the results. The results are typically very small, maybe a couple of bytes of data, and are inexpensive to pull out.  This type of model, for example, could produce value from analytics on old data that is currently just sitting in storage and is too costly to move around. \n\nThere is a lot of flexibility to do a hybrid or multi cloud approach where you have a centralized, on prem storage system that can be accessed via multiple cloud providers. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Randy Hayes"],"link":"/episode-EDT48-en","image":"./episodes/edt-48/en/thumbnail.jpg","lang":"en","summary":"In this episode, Darren checks in with Randy Hayes, now VP of Sales, for the public sector of VAST Data, six months after their last conversation to see how they are doing in the industry, what is new at VAST, and interesting use cases. Their new product, Gemini, offers a different storage business model"},{"id":120,"type":"Episode","title":"Remote Work Before, During and After the Pandemic","tags":["covid","remoteworker"],"body":"\r\n\r\nThey identified four areas that have helped them not only survive but excel during this unusual time: Anticipate your manager's needs, Communicate like your job depends on it (because it for sure does), Wave your flag by waving the flag for others, Block! That! Calendar!\n\n## Anticipate your manager's needs\n\n* Guess what reporting your manager needs and have the information at your fingertips.\n\n* Sales reports, project status, and customer status should be proactively created.\n\n## Communicate like your job depends on it (because it for sure does)\n\n* Do not fill other's email box with useless information\n\n* Put your ask at the beginning of the email. Then add backup information.\n\n* Give your team a summary of what you are doing with helpful information.\n\n## Wave your flag by waving the flag for others\n\n* A key component of success at any company is having visible, tangible accomplishments.\n\n* You cannot always put your head down and work and hope it speaks for itself.\n\n* Recognize others that helped you close the deal or finish the project.\n\n## Block! That! Calendar!\n\n* Set boundaries for family time and when work ends and begins.\n\n* Block off time for \"you\" time. You need time to rest your brain.\n\n* Set 5-10 minutes break between meetings,\n\n* Time to process your inbox\n\n* Time at the end of the day to process your day\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rachel Dreikosen"],"link":"/episode-EDT49-en","image":"./episodes/edt-49/en/thumbnail.png","lang":"en","summary":"In this episode, Darren talks with Rachel Dreikosen, Business Development Manager in Public Sector at Intel, about how COVID-19 has effective her work-life balance and why she started a blog to help other female technical sales professionals."},{"id":121,"type":"Episode","title":"Infrastructure Maturity Model","tags":["infrastructurematurity","compute","technology","sdi","cloud","multicloud"],"body":"\r\n\r\nDigital transformation can be difficult. Many organizations become paralyzed when trying to decipher all of the technologies that are thrown at them every day.  On this journey, like any other, it is important to have a map.  In this episode, Darren explains the  “Infrastructure Maturity Model\" to help organizations find out where they are in their digital transformation, where they can go next, and how to get there.\n\n## Standardized Phase\n\nAlmost every organization is in this Standardized phase or beyond. Understanding this phase gives us a baseline to start talking about the common things we see across organizations.\n\n### Basic\n\nThe basic level is where we find uncoordinated infrastructure and management by reaction.  This is especially common in start-ups where an organization buys services or hardware as needed, in reaction to events rather than as part of a master plan. Here, we also see purpose-built software/hardware stacks. A heavy cottage IT is also part of this phase of maturity.\n\n### Centralized\n\nIn this level, infrastructure is centralized. Organizations buy capacity rather than purpose-built hardware. Purchases are no longer reactive, but part of a longer-term plan. Typically, the organization will have common server configurations so they can purchase in volume for cost efficiency and have higher reliability. IT is more centralized and specialized, for example, in network, storage, compute, and security.\n\n### Simplified\n\nIn the simplified level, infrastructure is more consolidated. Different departments can share equipment such as servers, storage, and networks. Organizations get better performance as they migrate from old ways to new ways of centralizing and simplifying. OpEx costs go down as efficiency increases.\n\n## Virtualized Phase\n\nMost organizations today are in at least some part of the Virtualized phase. This is where we find standard configurations and private clouds.\n\nServer virtualization gives the flexibility to run applications on different machines and to move them easily among machines.\n\nStorage virtualization is a more recent development that brings similar benefits as server virtualization: flexibility for more dynamic decisions.  Storage can now span across the whole ecosystem. Storage can be provisioned for a specific application, and that storage and application can move around within the infrastructure.\n\nNetwork virtualization, the latest addition, has been embraced particularly by Internet Service Providers. They have made a huge investment because of the efficiency of being able to make dynamic changes in the network remotely instead of from the previous purpose-based hardware.\n\nAll of these virtualization technologies are tied together in offerings in the private cloud space, both commercial and open source, often in one easy-to-manage place. Organizations are moving into this phase, and also changing and consolidating their IT from a vertical focus to looking across everything (NOTE: can we call this a horizontal focus?) with systems, applications, and services more so than compute, storage, and network.\n\n## Automated Phase\n\nIn the Automated phase, organizations can automatically provision infrastructure, which saves time by eliminating layers of people and permissions. With the use of self-service portals, an engineer or software developer can get the resources that they need right away. This automatic provisioning also allows for basic automation of patches, VMs, security, and compliance. IT is now less in a reactive mode, but in a proactive mode.  Other valuable parts of this phase are, for example, the automatic starting up of infrastructure for applications in production and self-healing applications.\n\n## Orchestrated Phase\n\nAs automation becomes more complex, an Orchestrated phase becomes necessary. Automation is applying actions onto a machine, whereas orchestration is the coordination of the actions happening across multiple machines or even modalities (storage, compute, network) for an application. In addition, automated application stacks can deploy several applications on several different servers. In this phase, organizations also orchestrate hybrid services, for example, managing resources across public and private clouds. The benefits of this orchestration are decreased OpEx costs, decreased CapEx costs, and decreased time to deployment.\n\n## Real-Time (SLA Managed) Phase\n\nIn a Real-Time phase, organizations are in a service-level agreement infrastructure. Rather than applications, organizations are using services that provide value and tie everything together. Shared services run across multi-hybrid clouds and even legacy infrastructure. Pooling hybrid infrastructure, policy-based orchestration, and service-based orchestration all optimize the infrastructure, data management, and services.  \n\n## Conclusion\n\nIt is common for different parts of your organization to be in different phases. A few groups will be further along than others; that’s healthy.  Small pioneering groups can fail without affecting the whole organization, or they can have a breakthrough and pull the rest of the organization up with them. Very few organizations have moved to the highest phase on the map. The key is to not get discouraged, but to use the map as a guide to find out where you are and what the next steps might be for your organization.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT5-en","image":"./episodes/edt-5/en/thumbnail.png","lang":"en","summary":"Digital transformation can be difficult. Many organizations become paralyzed when trying to decipher all of the technologies that are thrown at them every day. On this journey, like any other, it is important to have a map. In this episode, Darren explains the Infrastructure Maturity Model to help organizations find out where they are in their digital transformation, where they can go next, and how to get there."},{"id":122,"type":"Episode","title":"Understanding Employee Burnout","tags":["employeeburnout","remotelearning","remoteworker","people","process","compute"],"body":"\r\n\r\nUzair’s professional background began in sales, point of sale consumer experience, and evolved into product development. For ten years, he worked on an alarm clock app that worked in different personalized ways to help people wake up motivated and focused. Although that project ultimately failed, Uzair learned a lot about human motivation. A few years ago, when he was working with students in middle and high school, he noticed a great lack of motivation and focus in the first period of the day, which seemed to be getting progressively worse. Due to his professional experience and his own wellness growth, he saw an opportunity to have an impact by creating the District Zero app. \n\n## K-12 Student Wellness\n\nDistrict Zero’s mission is to equip K-12 students with whole learner capabilities. This means helping students manage difficult emotions to recoup their focus and motivation. \n\nThe app uses Agile technology and the power of natural language processing sentiment analysis to work at scale to catch triggers and anxiety via a survey. Once a student completes the survey, the app can suggest content and resources such as a video or game to help with the specific issues. It also has a reporting system for teachers and administrators. \n\nCurrently, teachers and administrators carry a heavy load trying to help their students with wellness, especially in the COVID environment.  They might have a Google Form hooked up to Google Sheets and then use control F to search for negative words and key phrases. The app reduces this load, and avoids the by-product of teacher and administrator negativity. The system uncovers the students’ blockers and pain points and helps resolve them through quick remedies and resources. If students need more support, the system triages them to the correct individual such as a counselor and keeps the teacher informed. \n\nTraditionally, when a student shows anxiety or frustration, for example, they are taken directly to a counselor or social worker when the problem becomes unmanageable. But the student doesn’t need to go from zero to one hundred; there is a middle ground where the problem can be addressed before it comes to fight or flight. The app can help a problem from escalating and help catch issues before it’s too late. \n\nThe timing of the app pilot in Chicago area schools came last August during COVID, so it was particularly suited to help students who had lost that in-person connection with their teachers and struggled with social and emotional learning. A key element for success in social and emotional learning is that everyone in the community needs to be involved and empathetic: teachers, administrators, superintendents, parents, and tax payers; the empathy and connections can’t just happen at school. \n\n## Employee Burnout\n\nDuring COVID, workers showed an increase in productivity, but there is now the risk of employee burnout. Whether it is the stress of being a healthcare worker or a remote worker lacking work/home balance, a multitude of issues makes this a top issue for businesses. The principles of the District Zero K-12 app can be applied to workers to help alleviate burnout. \n\nDistrict Zero began experimenting with the app in their own workplace. and it led to hard conversations about priorities and understanding of what employees’ real needs are. Similar to the K-12 application, the system can identify day-to-day employee struggles and fill in the middle ground by giving support before problems escalate. \n\nDistrict Zero hopes to grow their technology to be beneficial to many different sectors: businesses, corporate healthcare, and even government agencies, for use in areas such as veterans affairs suicide prevention. \n\nThis new tool and insight into supporting student and employee wellness is timely as COVID escalated stress and burnout and we begin a new phase of re-opening and adjustment. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Uzair Hussain"],"link":"/episode-EDT50-en","image":"./episodes/edt-50/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, talks with Uzair Hussain, CEO of District Zero, about how the company’s app that supports K-12 student mental wellness can apply to preventing employee burnout."},{"id":123,"type":"Episode","title":"Practical Optane Persistent Memory Use Cases","tags":["dataarchitecture","memverge","pmem","optane","technology","data"],"body":"\r\n\r\nMemVerge shipped the first version of their software in September 2020, and despite the pandemic, which slowed down educating customers about the new technology, a good pattern of use cases emerged. \n\n## Cloud Service Providers Cost Reduction\n\nThe key metric for a cloud service operator is how many VMs they can deliver to their customers, and at what cost. The size of the memory on the servers becomes the bottleneck of how many VMs they can allocate per server, limiting how low their price per VM can go. \n\nMemVerge software with Optane delivers a larger amount of memory per server, allocating a larger number of VMs, therefore lowering the cost per VM and increasing the competitiveness of cloud service providers. The cost per VM could be three times cheaper. \n\n## Reliability with Large Memory Data Bases\n\nFinancial customers such as stock exchanges, banks, and mutual funds use a lot of memory databases and in-memory applications. In addition to making their memory bigger so they can have more instances per server, MemVerge solves memory database availability challenges. If the data is not being persisted to storage all the time, but just sitting in memory, all the intraday data is lost if there is a crash. This is catastrophic. Even if you have logged all the transactions, you must replay the log to recover the database, which takes many minutes or even hours to recover. \n\nMemVerge offers a new data service that has in-memory snapshot. It persists the  database state to Optane, which is much faster than persisting onto storage. If there is a crash, you have the last snapshot captured on persistent memory and you can recover from that. The recovery only takes a minute or two, so it is a 60 to 100x improvement. \n\n## Genomic Sequencing Reduction Through Memory Snapshot\n\nIn the area of genomics, MemVerge software in combination with Optane increases productivity exponentially. In a multistage data analytics workflow, bigger memory means more parallelism of the pipeline and processing so the whole process is faster. Snapshot is handy here, as well. If an organization is doing, for example, cancer or COVID research, and they need to do DNA or RNA sequences, they have to go through something like 50 stages of processing. Each stage might take hours, and they need to take a checkpoint of the state of intermediate computation results for a few reasons: first, re-running or reproducing results, and second, comparing results if they modify some data. The checkpoints are saved onto storage and this takes from five to 30 minutes. In many cases, this can be more time consuming than the compute itself. So if one job takes 24 hours, they might use eight hours for compute and 16 hours just doing these IO jobs saving those intermediate states. \n\nRather than doing IO, MemVerge uses a snapshot after each stage and captures it on Optane persistent memory. Instead of 16 hours of IO, this process can take one minute. It’s the new way of doing IO; you don’t need to do the serialization or de-serialization to open a file, read, write, etc. All you have to do is take a snapshot. \n\nAlthough this does take a lot of memory, with MemVerge, the memory is bigger than before, and it will continue to improve as Intel innovates. Two other features help with this issue. First, snapshots are taken periodically without creating full copies of the memory state; they are only the change pages so the extra usage of memory is minimized. Second, MemVerge can keep up to 256 layers of snapshots to memory, but at the same time, you can export those snapshots off memory to storage servers or your own storage systems. This is done without interrupting or impacting your running application. \n\nEssentially, you are creating a memory DVR because instead of only running your application forward, you can also run backward almost instantly. It’s a new experience. \n\nGenomics is just the first example of many workloads that could benefit from this technology. \n\nSince MemVerge is a startup, they are narrowly focusing on the three areas of cloud service providers, financial large memory applications, genomics and related data science pipeline jobs, but these use cases all prove the power of the combination of Optane persistent memory and MemVerge software. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT51-en","image":"./episodes/edt-51/en/thumbnail.jpg","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, talks to Charles Fan, CEO of MemVerge, about use cases with their software that utilizes Intel’s Optane persistent memory in an innovative way, removing the bottleneck between memory and storage."},{"id":124,"type":"Episode","title":"Telemetry in the Cloud","tags":["multicloud","optimization","workloadplacement"],"body":"\r\n\r\n## Benefits of Cloud Solution Architects (CSAs)\n\nThe CSA role is hot in the industry right now, and Intel has been hiring a team of CSAs to provide value to their customers. CSAs can help customers avoid the lift and shift mentality that leads to unnecessarily high costs. These new CSAs bring an external perspective and connect with the larger CSA community to problem solve.\n\n## Phases of Telemetry\n\nTelemetry in the cloud should not be relegated to your cloud service provider. For example, Amazon Web Services (AWS) had a major outage on the East Coast last November, and their tools weren’t reporting, so many customers had no idea they were down. Organizations need their own telemetry for monitoring.\n\nThe first phase is no monitoring in the cloud. The second is exposure of telemetry where you understand what you are getting, for example, CPU, memory, and network, but it’s surface level. The next phase is monitoring and action, where you get notifications about slowdowns, transaction rates, response rates etc.  The next phase is further down the stack where the modern intelligent cloud controls and predicts for remediation. Last is complete automation. This is where things are operating on their own, listening, responding, and then informing after the fact.\n\nAn example of complete automation would be a data center that controls an HVAC system, sensing and reducing or increasing temperature in different parts of a building. For workloads, an intelligent cloud can move the workloads around to maximize value in capacity and performance.\n\n## Current Approaches and Limitations\n\nMany times, however, customers of cloud service providers relegate automation at the infrastructure layer and forget the workload layer. Clients need information beyond just the vitals; they need deep, rich telemetry to know what’s really going on. It’s a mistake to rely on the cloud service provider to have your best interest in mind concerning telemetry, and this can manifest in high costs.\n\nSome cloud service providers are opening up more. Currently AWS is a game changer in the telemetry they are providing. Hopefully, others will follow suit.\n\n## Let’s Collect the Right Telemetry…\n\nPicking the right instances matters. Not all cores are created equal; different cores are used for different things, and it’s important to understand what they are to get the best performance and price.\n\nWhat kind of information can you collect? Intel’s product design includes performance monitoring units (PMUs). These are sub-level counters, and they provide information about transactions, delays, latency, and bottlenecks. There are three different camps in PMUs: core, off core, and uncore.  These PMUs collect information on CPI, utilization, frequency, and TMAM. All of this data is available using AWS.\n\nUsing metrics, real telemetry, is a tool to help you optimize your workloads. You could compare how your workload is running in your own data center, for example, to AWS in these instances, and run metrics to find out on which platform the workloads should land.\n\n## IT Relevance\n\nLayering telemetry with benchmarking is an ultimate solution. With benchmarking, you can know your output, and with telemetry, you can look at CPI, utilization, and frequency, and you have the full dashboard of what is happening. You want to do the same thing in the cloud, rather than just dropping workloads onto a seemingly cheaper instance.\n\nIT professionals should not fear losing their jobs because things are moving to the cloud. Instead, they should transfer their skills to learn about benchmarking telemetry rather than having a lift and shift mentality. Becoming proficient in cloud utilization also involves using cloud native features such as Kubernetes and containers. Telemetry works in these areas as well. With Intel’s C advisor, you can get rich telemetry like the core and off core data from your containers.\n\nAlthough it’s a bit daunting when you look at everything that is possible in the cloud, starting small is your best bet. Look at the right applications based on risk. Catalog your apps, look at the tiering of applications, and then start to shift them over to the cloud in sizable chunks of like functions and apps. As you look at new services and learn new applications, consider the architecture behind them and ask the right questions so you are a more informed technical architect. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT52-en","image":"./episodes/edt-52/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, talks to Josh Hilliker, Director of Cloud Solution Architects at Intel about using telemetry in the cloud to maximize value and efficiency."},{"id":125,"type":"Episode","title":"DevOps with Speed While Reducing Risk","tags":["devops","compute","technology","process","devsecops","cybersecurity","multicloud"],"body":"\r\n\r\nThe three key areas to balance in software delivery are velocity, risk, and quality. Most can manage one or two of these things, but adding the third can get difficult.  For example, you might be able to deliver at velocity because you have a great cd system, but once you introduce compliance and policy checks, you are faced with a challenge. Do you stop and check those things, or maybe ingest some results from another tool? Suddenly, your velocity suffers.\n\nOpsMx is working to help people automate those decision points. Automation is the most important thing to keep velocity up while increasing the other two pillars, risk and quality.\n\n## OpsMx Solution: Fully Automated Software Delivery\n\nThe automation works as a data aggregation, by grabbing results from the various tools in the tool chain and then having a mechanism like a policy manager that gives expected results on check marks from tools such as BlackDuck. If it looks right, the pipeline doesn’t stop, as it does currently, for a human check.\n\n## Faster Application Delivery Increases Failure Rates\n\nThe velocity of application delivery has gone from weekly or monthly to hourly, adding pressure to produce quickly. As you try to increase velocity, however, you could start to lose some control over quality, perhaps skipping some of the risk checks. The net result is the faster you try to move, the more apt you are to have a failure.\n\n## Friction Points and Problem Spaces\n\nWith strategic use of automation, you have the ability to increase velocity without swallowing risk. Continuous verification can cut a three or four hour endeavor down to five or ten seconds. Instead of using human labor to parse through logs to look for anomalous behavior that may have passed a binary pass/fail check, continuous verification can do this automatically, allowing people to look at exception cases only.\n\n## What Does Automation Mean in this Context?\n\nThe next piece is using the tools required by the security and compliance team, such as BlackDuck. With automation, you do not have to stop the pipeline to have someone interpret the results before continuing because it will happen automatically.\n\nThe other pieces are policy at a higher level if you are in a regulated industry with more checks, or something as simple as a retailer who, for example, doesn’t want to release new software two weeks before Christmas. Those checks can be automated to eliminate the need for human approval, allowing the pipeline to continue if there are no exceptions. The idea is to remove as many human gates as possible to allow the pipeline to flow.\n\nThe primary set of control and policy automations are reusable across pipelines; you only have to customize or create ones for criteria specific to your release cycle.\n\n## OpsMx Solution: Fully Automated Software Delivery\n\nOpsMx gives you the ability to create those pipelines and the automations of analyzing logs. Spinnaker is the current tool, but OpsMx’s vision is to be as cd agnostic as possible, allowing use of all the cd tools such as Jenkins and Microsoft Azure. OpsMx leverages the cd part for the automation, but is sitting on top of that to help make the automated decisions.\n\nA growing part of the tool is machine learning to help understand what is baseline for a particular application versus what is anomalous behavior. There is also a supervised learning model where a DevOps professional, an engineer, or product owner can specify anomalous behavior as expected in context of the application.\n\nThe system also allows auditability. Any change or exception is documented. There is a complete audit trail of everything that happens, from who approved an exception to which artifact was deployed and what base image was used. Whether it’s running on the cloud or on-prem, you can see the whole pipeline as one entity.\n\nAnother aspect of visibility OpsMx provides is a kind of map of what is deployed where, such as the series of micro services that are currently in the QA, what is in certain stages, or what is in production, and then for any one of those things, you can drill down and get a historical view. You can click any given release and all of the audit information is right at your fingertips.  \n\nIn addition, the next frontier that OpsMx is working on is the efficacy of the artifacts that are being captured so the supply chain, or lineage, is transparent.\n\nOpsMx is now using an agent-based technology that can interact with, for example, resources that sit behind your firewall. The agent acts as a proxy to the intelligence layer so data can be gathered there. There are no worries about opening or exposing firewall ports. It works the same way with the cloud providers: the agent can be deployed inside of the VPC and you no longer have to risk putting any keys and secrets in a cloud-based application. The agent simply acts as a proxy so the authorized piece always remains inside the VPC, securing the way information is gathered.\n\nWith this new direction, the DevOps industry as a whole is in for a shake-up around security and auditability. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT53-en","image":"./episodes/edt-53/en/thumbnail.png","lang":"en","summary":"In this episode, Robert Boule, Head of Solution Engineering at OpsMx, joins Darren to talk about improving speed without increasing risk in the DevOps process. The three key areas to balance in software delivery are velocity, risk, and quality. Most can manage one or two of these things, but adding the third can get difficult. For example, you might be able to deliver at velocity because you have a great cd system, but once you introduce compliance and policy checks, you are faced with a challenge. Do you stop and check those things, or maybe ingest some results from another tool? Suddenly, your velocity suffers."},{"id":126,"type":"Episode","title":"People & Process in Digital Transformation","tags":["multicloud","organizationalchange","change","people","hsbc","cio"],"body":"\r\n\r\nAnn started her career as a programmer, moved into program management, and has experience in acquisitions and sales of companies. She was the Global Head of Data at HSBC and became the Chief Information Officer in September 2016. She delivered major organizational transformation programs throughout her career, including new mortgage systems and new core banking systems.\n\nSome of the challenges of those transformations were asking the right questions during the assessments. In a mortgage transformation program, she first asked questions about the finances: Are we over budget? Are we on plan? Do we need to ask for more funding? For the leadership team, she asked whether the right people were in place with the right skill sets to accomplish the changes. She also delved into the bigger picture: What does the business believe? What are the business objectives? Do they understand clearly why the transformation program is necessary and what we are trying to accomplish? She also looked at the status of the program through metrics and KPIs.\n\nFor that transformation, they already had a program established and started, but Ann came in as almost a kind of internal business consultant. The program was business driven and business led, but they needed to leverage the technology to achieve those business goals and objectives. The technology and the goals needed to be completely aligned.\n\nIn the last transformation Ann headed, almost three years into a five-year transformation program, she took over as the program director for both the business and technology sides.  Four CIOs reported to her, each assigned to a business: retail, commercial, wealth, and investment banking. Ann connected them with their business partners, and they would both come together for meetings to make sure they stayed aligned since they were trying to accomplish the same thing.\n\nAlthough it would seem that these type of partnerships would be regular business practice, Ann found one her biggest challenges as a CIO was getting the company aligned. For something as big and complex as changing a core banking system, a new mobile app, or a new web interface for example, Ann held meetings multiple times a day with the key players because there were probably 15 different work streams, and integration was the key aspect.\n\nComing in from the outside and making organizational changes is a difficult position to be in. People are nervous about change, especially when they trusted their previous leader. With time, however, Ann found that people came along because they knew the objective was to improve the business, and they bought into the journey. One way Ann accomplished this was sitting on the floor with the different teams instead of in an office. She got to know people and it became normal for them to have her around and be able to have conversations and raise issues. She got raw information from the people doing the work instead of filtered information through management.\n\nOne thing leaders often forget is that the people on the teams want to succeed, and a personal touch such as working alongside them goes a long way. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ann Madea"],"link":"/episode-EDT54-en","image":"./episodes/edt-54/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, asks his guest, Ann Madea, former CIO of HSBC, to reflect on the process of big transformational changes she spearheaded in organizations."},{"id":127,"type":"Episode","title":"Communication During Transformation","tags":["communication","organizationalchange","people","process","hsbc","cio"],"body":"\r\n\r\nWhen CIOs deploy solutions, they involve new processes and more than likely organizational change. In Ann’s most recent transformation program, they moved forward with cloud. Ann hired a head of cloud and implemented a multi cloud strategy with Google, AWS, and Azure. This major change caused concern, especially among those who had been working with the infrastructure for a long time.\n\nTo alleviate anxiety and increase engagement, Ann strongly encouraged people to start getting certifications for cloud, Google, AWS, etc.  She took her team on the road to have meetings with Google in New York where they got to see the energy, atmosphere, and collaboration that was happening there. As people began to get certified, excitement for the training and the transformation grew.\n\nThe new head of cloud would have “Google Days” where people came in and presented their use cases and how they were leveraging different products.  They did the same with Amazon and Azure. This helped employees to understand the scope of what they could do, build excitement, and it enabled participation.\n\nAnn’s key for CIO communication in the middle of a transformation is trust but verify. Three or four months into a transformation, Ann was told to focus on assessments in areas such as budgets, review teams, and reestablish relationships with the board and regulators. She decided to do a deeper dive, however, and found that there was really no plan that showed how the program was going to deliver. That began a reevaluation of the leadership team to get the program moving forward.\n\nA CIO implementing a transformation must be willing to get their hands dirty. Trust but verify everything with multiple interviews and streams of information; one or two sources are not enough. Then, Ann believes it’s necessary to start communicating with transparency about the issues in order to get support from the people who are able to provide it.\n\nOne benefit of transparency is that the people who are giving you information, such as the programmers, trust that you are not going to use information against them. Ann’s team knows not to surprise her, and in turn, she does not surprise her bosses. For example, if there is an issue in the data center, she will tell them what she knows and update them as she learns more.\n\nEmployees want to know how they fit into the success of the program, not just that they have to code these five programs. One way Ann accomplished this was by holding many town halls. She also walked the floors and held an informal 30-minute update meeting every week where people could ask questions. Any time Ann held a big meeting where decisions were made, she would pull the whole team together, the business and technology sides, and explain the decisions and the impact. If people know there was a big meeting but are kept in the dark, the rumor mill starts.\n\nWith current teleworking, Ann says it is also important to briefly check in with individuals. Continuous check-ins lead to better relationships, and lessen anxiety about talking to superiors. Bosses need to understand what is motivating the teams, and what people are concerned about. They have families to feed, children in college, or maybe family members with medical issues.\n\nOf course not everything can be discussed with everyone, such as staff reductions or promotions, for example, but Ann likes to call meetings of supervisors called “people meetings” where they can discuss these sensitive issues. The meetings can also help leaders understand their staff and what they are looking to do.  It’s important to hear from the one-downs rather than only leaders, as they are more attuned to the day-to-day happenings. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ann Madea"],"link":"/episode-EDT55-en","image":"./episodes/edt-55/en/thumbnail.png","lang":"en","summary":"In this episode, part two of two, Darren and guest Ann Madea, former CIO of HSBC, talk about communication during organizational transformation. When CIOs deploy solutions, they involve new processes and more than likely organizational change. In Ann’s most recent transformation program, they moved forward with cloud. Ann hired a head of cloud and implemented a multi cloud strategy with Google, AWS, and Azure. This major change caused concern, especially among those who had been working with the infrastructure for a long time."},{"id":128,"type":"Episode","title":"Cloud Broker Organizations","tags":["cloudbroker","cloud","compute","technology","people","process","multicloud","organizationalchange"],"body":"\r\n\r\nDave and Kevin founded Intel’s cloud brokerage team about seven years ago. Intel workers were consuming the public cloud, and it was like the wild wild West with people just swiping their credit cards for access. Security was an issue and spending got out of control. To rein in the chaos, they created a cloud broker organization.\n\nA centralized approach was necessary to control costs, organize and create enterprise contracts with vendors, and set up billing through one organization. Instead of using individual credit cards or creating their own POs, Intel’s business groups use a master PO through IT, which is then billed back to the different groups. Overall, it saves money because now Intel has collective purchasing power, including more advanced cost-saving opportunities, such as purchasing reserved capacity rather than paying on-demand prices.\n\nIt took a bit of time to get everyone on board with the organization, with some still using their credit cards to open accounts. To help negate this, some of the cloud providers gave Intel a report of accounts that were opened with Intel email addresses. Rather than only approaching these “escapes” as a policy violation, it was a chance to educate them about the benefits of using the central Intel account: security standards already in place, enterprise support, training, and cost efficiency.\n\nIn addition to those benefits, Intel also built a cloud Center of Excellence, a community-based forum that they ask people to join when they get their cloud accounts. Its growth has been grassroots, providing information and feedback to members.\n\nMany developers and others who use the cloud just want to use it and not have to think about the security or cost, for example. Having the cloud brokerage team allows them to do that. An analogy is that IT puts the developer in a sandbox with all the toys, but doesn’t allow them to kick sand outside of the sandbox or play with the toys on the outside. This provides the developer community with safe, secure accounts and access whenever they need it.\n\nThere are hundreds of services available across public cloud providers, and they are always releasing new services and capabilities. It’s hard for business groups to have or maintain expertise on all those services. A central cloud broker team that is focused on public cloud and keeps up with the latest services can offer guidance and knowledge about where to land different workloads. The key to the brokerage is that people are coming to a central funnel and are being redirected to the right services.\n\nIt’s not only useful to have a dedicated cloud brokerage team, but people within the team that focus on particular cloud providers. For example, at Intel, as they reached critical mass and the cloud providers matured and started offering a massive amount of services, Kevin focused on AWS and Dave focused on Azure in order to do a deeper dive into each one.\n\nUsing multiple cloud offerings rather than just one was a natural decision as business groups came in with preferences and different workloads do better with different providers. The providers’ differences were more distinct in the past. Now, there is a more level playing field.\n\nA cloud broker is not just someone in a technical position, but a jack of all trades. Dave and Kevin became experts on everything related to cloud, such as security and networking, and they educated those teams as they expanded their scope from on prem to public cloud. By having a central cloud team, Intel’s other organizations were able to become educated, expand, and grow. The team was nicknamed “The Glue” because of their central and varied role. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Kevin Bleckmann","Dave Shrestha"],"link":"/episode-EDT56-en","image":"./episodes/edt-56/en/thumbnail.png","lang":"en","summary":"In this episode, part one of two, Darren and Intel Cloud Solution Architects Dave Shrestha and Kevin Bleckman talk about the importance of a cloud broker organization. Dave and Kevin founded Intel’s cloud brokerage team about seven years ago. Intel workers were consuming the public cloud, and it was like the wild wild West with people just swiping their credit cards for access. Security was an issue and spending got out of control. To rein in the chaos, they created a cloud broker organization."},{"id":129,"type":"Episode","title":"Cloud Broker Organization Part 2","tags":["cloudbroker","change","people","process","compute","organizationalchange","multicloud"],"body":"\r\n\r\n# Considerations Cloud Selection\n\nSeven years ago, when Dave and Kevin first started Intel’s cloud broker team, there were some significant differences in cloud service providers such as regional availability, but now, with maturity, the differences are smaller. Some providers specialize in certain areas like AI and machine learning or rich managed database services, and there are cost factors, but they are more similar than not. Cloud providers, in general, are becoming more of a utility as they mature.\n\nSecurity is good and fairly standard across the cloud providers. Previously, Intel used external third party vendors’ products to manage security with cloud services. The cloud providers have now built much of that into their platforms, and Intel has switched over to using more built-in components. There is still a lot of work to be done in this space. In some cases, you will still have to do security yourself, but the providers are headed in the right direction. Using built-in tooling isn’t the easiest option for new companies because it’s not fully managed, but once you have the skill set and maturity, at least the tooling is provided.\n\nCost management is still an art; many people are shocked at their first cloud bill. Although there are built-in tools that will help, advisors, and public cloud calculators, there are some hidden costs. Ingress and egress charges are probably the number one surprise, followed by over provisioning. Intel has a cloud optimizer that is powered by Densify that will help identify over provisioned resources and recommend through detailed analysis which instance sizes and families are the most optimal.\n\nSome workloads should remain on-prem, especially for large companies like Intel that have a large on-prem data center.  The cloud broker team has an internal tool called the decision framework tool that can help put the right workload in the right place, and sometimes on-prem is the best solution.\n\n# Workload Consideration for Public Cloud Placement\n\nWithout a tool to make the determination, there are five areas to consider.\n\n*\tSecurity: Make sure you know the identity providers, encryption, compliance, and single sign-on options. These are the same items you would normally deal with on prem. Instead of separate tooling, it’s best to have tooling that spans both on-prem and public cloud so your teams don’t have to relearn separate tooling.\n\n*\tPrivacy\n\n*\tWorkload Stability\n\n*\tData affinity/data gravity: Make sure your cloud provider has availability in the regions you need. Some cloud providers have availability zones, or data centers, across all their regions, and some don’t.  In addition, if a workload is connecting to a lot of on-prem systems, it doesn’t make sense to put that workload in the public cloud.\n\n*\tCost: There are still some differences in cost across public cloud providers, especially with large compute instances. Also consider egress costs.\n\n## How to Sell your Cloud Broker Service\n\nBuying into the cloud broker service can be difficult for some, such as developers who are used to having control and spinning up any instance they want. There can also be growing pains. When people at Intel first got their accounts, they felt insecure, and then information security sent messages when they were doing something wrong, and they didn’t necessarily know what they did or how to fix it. As the service evolved and matured, those problems sorted themselves out as the team stepped in to help and then employed auto fixes.\n\nPeople saw that the service was ultimately a benefit because they provided a quick, easy way to get in the public cloud with all the support they needed. After a period, the service began to sell itself.\n\nOne of the key services the team offers is acting as a bridge between Intel’s end customer business partners and information security groups to negate overly aggressive security policies that create hassle and too much ticketing.  The team provides the proper balance that allows developers enough freedom to work within a secured framework and still meet the security requirements. The brokers basically work as mediators between information security and developers.\n\nThe cloud broker team also provides training by bringing in the cloud providers to hold workshops. This benefit grew from the cloud Center of Excellence as well, where people asked for training in specific areas, and then the team would negotiate it with the vendors.\n\nIn addition to external vendors, Intel used internal teams to showcase what they had done with public cloud so other teams could use that knowledge.\n\nCloud broker teams within companies can provide a range of services and benefits such as security and cost efficiency, especially as the public cloud services mature and expand and become more necessary for operations. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Dave Shrestha","Kevin Bleckmann"],"link":"/episode-EDT57-en","image":"./episodes/edt-57/en/thumbnail.png","lang":"en","summary":"In part two of two, Darren Pulsipher, Chief Solution Architect, and Intel Cloud Solution Architects Dave Shrestha and Kevin Bleckman talk about the benefits and services of a cloud broker organization."},{"id":130,"type":"Episode","title":"Understanding Edge to Data Center Problems","tags":["data","edge","compute","edgemere","edgetocloud","cybersecurity","technology","process"],"body":"\r\n\r\n## Mission Integration\n\nThere are many moving parts when putting capabilities out in mission, especially in the Department of Defense, but also in Veterans Affairs hospitals, Homeland Security, FDA, and FEMA, for example, and getting the data on edge devices back to ground stations and regional and enterprise datacenters. The data needs to be usable and reliable for big analytics flows in AI workflows and into the hands of analysts to make decisions based on the raw data.\n\n## Drivers for Edge: Latency, Bandwidth, Security, Connectivity\n\nPart of the puzzle is that edge devices have become more sophisticated and are gathering more data than we could ever throw at 5G. Hopes of 5G conquering all the data and making it seamlessly available to the datacenter never materialized with IoT device advances.\n\nOne of the original architectures around IoT was by Cisco, called a fog. The fog idea was that the datacenter connected the fog to the edge devices, so some of the processing and connectivity was happening there. If network connectivity is reliable and consistent, this works well with enough bandwidth. The amount of data now generated at the edge by these organizations, however, outpaces any amount of available bandwidth.\n\nWith IoT, only a portion of the data is moved to the datacenter, so generally, data value is only happening as it is analyzed there.  The problem is that the datacenter can’t store and ingest all the big data. Even pushing it into the cloud doesn’t resolve the problem, as the cloud can’t consume all the data that is sitting on the edge. So, we want to move away from sending all the data to the datacenter to collect the value and instead, push the data value as close to the edge as possible, decreasing the amount of data volume coming back to the datacenter.\n\nOf course, not all data can be pushed down to the edge; there has to be correlation between different edge devices. The value needs to be in a more centralized place, not necessarily the central datacenter, but perhaps in one of these intermediate fogs or regional datacenters. They key is to move data intelligently and push data value as close to the edge as possible in a repeatable and sustainable manner. In doing so, we can react much more quickly to the edge.\n\n## Common Physical Layer\n\nTo overcome some of these problems, we first need a common physical layer. This means that it’s common from the datacenter through the fog layers down to the edge devices; there’s one way to manage and control the devices and get aid from them in a reliable, common way. This doesn’t necessarily mean the same machine, but a minimal viable device with a common interface. Another benefit of the common physical layer is that if you write code for an application, it can run anywhere in this ecosystem. Intel has some great technology for this such as oneAPI that does a lot of the work so you can write code once, compile the binaries for the different types of devices, push it down to the common physical layer, and it runs appropriately.  In short, the benefits are a common operating model, common security model, and a write-once-run-anywhere mode of operation.\n\n## Software Defined Infrastructure\n\nSDI applies to the datacenter in private and public clouds with their software defined APIs. With SDI in the edge, we get common ways of moving data. We can provision resources in the edge in the datacenter anytime, and we can move data this way in a more seamless manner.\n\n## Distributed Information Management Layer\n\nWe need to be more intelligent about managing and classifying data, moving the data only where it’s going to be processed, whether on the edge, in a regional datacenter, or in the cloud. Important aspects are cataloguing and reusing data and fitting in compliance and security requirements.  The benefit of this distributed information management layer is that you are pushing less data into the datacenter, moving less data, and pushing value down to the edge.\n\n## Service Management Layer\n\nIn order to really push value down to the edge, we need to be able to deploy applications out to the edge. This is where a service management layer, or container ecosystem, comes in. This allows for pushing micro services to the edge, the fog, the datacenter, or the cloud in a repeatable and reliable manner. If a regional datacenter goes down, for example, you don't have to rely on that for the service mesh to continue to operate.\n\n## Application Service Layer\n\nAn application service layer coordinates the different applications so you can create workflows that generate the real business value from the data. Just moving the data around or running it through an Analytics Engine isn’t good enough. The data must move from the Analytics Engine to an analyst workstation. Some tools in this layer would be robotic process automation and DevOps pipelines. This is also where you can enforce security and compliance at the application layer.\n\n## Security and Identity Layers\n\nThe key aspect of the identity layer is to establish trust between entities that are properly identified. We must understand who is accessing what and which devices are accessing what data, at what time, and where. Identity is taken beyond the typical user and into applications identity of data, edge devices, fog, datacenters, and cloud.\n\nThe twin of identity is security. Here we have detection, remediation, encryption, and establishing root of trust. This results in reliability, trusted, data, and compliance. Now, intelligent data can be pushed down to the edge that is then populated up to the datacenter, but you are not moving massive amounts of raw data, only what you need in a secure manner.\n\n## High Level View\n\nTo have a successful edge to cloud architecture that’s repeatable, all of these different elements are necessary. We have seen some organizations build purpose-built edge to cloud architecture, and when they deploy a new capability into that theater, they get stuck. If for example, they hard code the data residing in the edge because they will always process on the edge, or in the datacenter for an application they are always processing in the datacenter, this results in rigidity. It also increases the amount of time it takes deploy new capabilities, perhaps years instead of months. If we take the learnings from application deployments in edge to cloud over and over again and start generalizing, we quickly find that they fall into one of the layers we’ve identified.\n\nFor more information, check out this paper (include link) about the high level view of this architecture of edge to cloud.  We are not prescriptive on what fits in those boxes, but the key is understanding the use cases they encompass. We have ideas on what is in each of the layers, and we’re building out the ecosystems to accommodate your organization’s unique needs within the layers. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT58-en","image":"./episodes/edt-58/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, outlines the common problems throughout edge to data center architectures that he’s observed and discussed with customers in the public sector. He lays out the ideal architecture to resolve these issues"},{"id":131,"type":"Episode","title":"Evolution of Data Privacy","tags":["cybersecurity","data","dataprivacy","privacy","process","people","healthcare","sutterhealth"],"body":"\r\n\r\nAfter graduating from law school, Jeremy went directly in to the JAG Corps for the US Air Force and served as an active duty officer for nine years. One of his responsibilities was as a records manager for two different offices. He switched his focus to healthcare with his last military assignment as counsel for hospitals in the Northeast. After leaving active duty (he is still in the reserves), he went to work as counsel at a hospital and is now at Sutter Health.\n\nAlthough the legal framework is the same for data privacy, there are some differences in government and the private sector: the approach, the goals, and which regulations apply. In government, data privacy means keeping the data protected via the Privacy Act; keep the secret stuff secret. Although there are exceptions as well as the Freedom of Information Act, systems are designed to protect the information, not release it, by default.\n\nIn healthcare, since HIPAA in 1996, HITECH a few years later, and now with the ONC, data is being pushed into homes, into the devices of the patients, and patients can allow access to third parties. “Appropriate access” is probably a better descriptor than “data privacy.”\n\nThe movement in privacy rights is more access and more control from individuals. As a patient, not only do you have a right to your information, you can direct your healthcare provider to give it to a third party: a lawyer, a friend, another medical professional, etc. A patient can also specify a paper or electronic release medium, so there are many rights given to patients.\n\nThese rights for individuals to control their own data are not limited to healthcare. We see this in the current movement with GDPR and recent laws passed in Brazil, Canada, and China, and some US states such as California, Washington, and Virginia.\n\nAs the future of data privacy becomes more about individual rights to access, it will change how organizations can track things. Big companies like Google and Facebook have options now where people can clear out their data or prevent the companies from selling it in various ways. Tracking devices such as traditional cookies, won’t be as relevant, so there will have to be something else that helps targeted advertisers.\n\nA lot of data, of course, such as employment data is already regulated. Individual control of data is not an absolute right; companies need data to function, so they will be able to keep some, but it will become more regulated. In the US, we will have more complexity and more problems before we have standardization. We have 50 states, each with their own regulations.\n\nThere are as many laws as there are definitions of personal information, which can create a conflict. Sutter, for example, has many hospitals in Northern California, and a few auxiliaries in Hawaii, Oregon, and Utah. Sutter must routinely stay on top of those states’ regulations, but if there is a breach, then the state where the affected individuals reside come into play. Sometimes the laws are written so that Sutter has to follow the law in the location of the residence of the patients rather than the business, so that becomes complex.\n\nSometimes it makes sense to outsource these types of problems, and a there is a whole legal industry popping up that helps companies navigate privacy and information security regulations.\n\nFrom the IT side, data security means limiting who has access to things. With data privacy, its opening doors to access. Of course, there is a validation process to who has access, but there is a balancing act to security and privacy, which can create a lot of work for both the legal and the operational sides.\n\nOrganizations that develop any kind of apps that deal with people’s data need to understand that privacy laws that are different in every country and every state and the ramifications of using and storing that data.\n\nJeremy, along with privacy and security teams are engaged with the technical teams, sometimes even from the design phase, to make sure everything meets regulations. For example, he will talk to the team that builds the patient portals to see whether the things they want to do meet regulations. Additionally, he helps answer questions about what kind of database would be best or whether there is a cloud provider that can be set up in compliance. Jeremy finds the more he educates himself and gets training on technical aspects, the more helpful he can be in the process. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jeremy Harris"],"link":"/episode-EDT59-en","image":"./episodes/edt-59/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, discusses what data privacy really means and its future direction with Jeremy Harris, Assistant General Counsel – Privacy/Information Security, at Sutter Health."},{"id":132,"type":"Episode","title":"Managing Telework at Capacity","tags":["telework","remoteworker","vdi","mfa","cybersecurity","people","process"],"body":"\r\n\r\n## Teleworker Modes of Operation\n\nThe modes of operation that we see people working within the Department of Defense and also in the private sector are the following:\n\n* Device as a dumb terminal (VDI - Virtual Desktop Infrastructure)\n\n* Device as part of the internal network (VPN - Virtual Private Network)\n\n* Device as a portal to services\n\nMost workers are utilizing multiple modes of operation, so different types must be supported through the infrastructure.\n\n### Device as a dumb terminal\n\nIn this mode, the worker has their desktop running on a server in the data center and they use their laptop to connect to the “virtual desktop.” They basically use their laptop or another device as a “dumb” terminal. There are some issues with VDI including scalability, network congestion, latency, and redundancy. Because of these issues, a VDI session can be quite expensive. Of the three options, however, VDI is the most secure.\n\n### Device as part of the internal network\n\nThis mode is probably the most common. The worker connects their device to the VPN so they are working on the network as if they were connected in the office. One benefit is mobility because workers can connect any device such as a phone, tablet, or laptop. Another benefit is that there can be different segments of VPN for different classifications of data. One downside is that workers always have to be connected in order to work. And although there is not as much network traffic as with a VDI, there will still be some network congestion as data moves.\n\n### Device as a portal to services\n\nMore people are moving toward this mode. The worker uses their device to connect to services in the cloud, public or private. The main service people are using is Office 365, which allows organizations to use cloud services instead of, or in conjunction with, their own services. Efficiency, performance, and the ability to work disconnected at times are benefits. Downsides include being dependent on a third party…(I’m not sure how to finish this – adoption/migration and security)\n\n## Emerging Bottlenecks\n\nBottlenecks have emerged as the number of people working from home has increased from 15% to targeted over 85%. For the first two modes of operation, VPN scalability can be a major bottleneck. Limited bandwidth is also a big source of bottlenecks. One solution for these problems is to leverage cloud services to ease the strain. On a human level, IT operations and help desks are being overwhelmed as workers are connecting from home for the first time.\n\n### VPN Scalability\n\nThere are several short-term solutions to help alleviate these bottlenecks. Prioritizing user access can be effective, whether it’s based on time schedules or priority of mission. Employee education about adjusting to this environment is necessary.\n\nFor long-term scalability, organizations should be migrating to SaaS solutions using laptops as a portal mode.\n\n### Bandwidth to Sites\n\nOne of the best things organizations can do in the short term is to find out how many VDI users you currently have, and see if you can move them to work on the network or in the collaboration tools mode of operation. This will dramatically reduce the load on bandwidth. You may also have to increase your network capabilities after evaluating how your remote users are working. Education is, again, an essential part of this shift so workers will utilize best practices, for example, disconnecting from VPN when they are not using it, and configuring backup tools to operate during off-hours. In the long term, we suggest a multi-hybrid cloud architecture that gives you the ability to leverage cloud service providers for network bandwidth and burst-ability and optimize for cost and capacity.\n\n### Hosted Services Scalability\n\nTo aid in implementing scalable architectures for short-term purposes, there are several great references, including Outlook Web Access (OWA) and VDI reference architectures.\n\nAgain, for the long term, we recommend moving to a multi-hybrid cloud infrastructure for elasticity, capacity, predictive performance, compliance, and security.\n\n### IT Operations\n\nHow can we scale the help desk team who are likely overwhelmed? One idea is to have online FAQs for workers’ easy reference. User-group community-contributed solutions that are moderated by IT can also be useful. Ideally, organizations should be using a ticket-managed system to identify bottlenecks and streamline processes. In addition, anything that can be automated to avoid repetitive tasks should be automated through, for example, robotic process automation (RPA) or additional scripting. A longer-term solution could be the implementation of AI Chat Bots as a self-service IT help desk. They quickly narrow down online solutions using keywords or recommend contact.\n\n## Conclusion\n\nUltimately, Intel wants to see organizations be successful during this difficult time when workers are shifting from the office to working remotely and dealing with a lot of stress. Intel can help out industry and government and public sectors. We have silicon that works in all of these aspects.  We have partners that deliver hardware and hardware and software solutions, and of course, we sell PCs and client devices that enable remote workers.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Tim Flynn","Pete Schmitz"],"link":"/episode-EDT6-en","image":"./episodes/edt-6/en/thumbnail.png","lang":"en","summary":"In this episode, Darren, Tim Flynn, Retired Rear Admiral of the Navy, and Pete Schmitz, Account Executive for the Navy from Intel, talk about how to manage the explosive growth of teleworkers due to the Covid-19 pandemic. We discuss the different modes that workers can use to work remotely and still be productive: device as a dumb terminal, device as part of the internal network and device as a portal to services. Understanding these modes of operation can help find bottlenecks that can hamper the effectiveness of your team."},{"id":133,"type":"Episode","title":"Managing Risk in the Face of Ransomware","tags":["cybersecurity","ransomware","riskmanagement","process"],"body":"\r\n\r\nOver the past year, there has been a rise in the number and sophistication of cyber-attacks. The three key areas of recurring attacks are ransomware, supply chain attacks, and data breaches.\n\nThe attacks have become more sophisticated for several reasons. First, as security becomes better, the adversary must continue to become better. Organized crime, nation state actors, and other threat agents recognize it takes more sophistication to compromise and thwart security controls. Just like organizations have software development lifecycle processes, so does the malware community. They have tools and frameworks they build from and good processes for building quality into their systems. Different players purchase, sell, and borrow code. They learn from each other and share information on the dark web. They are not just ragtag teams of hackers; they run more like companies.\n\nThese threat agents are in a billion dollar-plus industry. Big money drives the need for maturity. We can’t just bolt on security anymore; it must be built in, and built in everywhere, not just in the products, but in the infrastructure and processes. That was one of the lessons of SolarWinds: Even if you build a good product, the infrastructure that supports it can be vulnerable.\n\nRecent attacks show that no one is immune. Oftentimes organizations will mistakenly assume they are safe since they are not financial services, government, or other high value industries, but recent attacks on companies such as JBS Foods, McDonald’s, and Audi have shown that no one is immune.  Companies, no matter their product, are reliant on their digital infrastructure to be functional; the attack on JBS Foods took down the world’s largest meatpacking industry.\n\nNo one thing serves as a silver bullet for preventing these attacks. There is hope, but it requires a lot of work. An organization must have the diligence to apply the right risk metrics to implement security correctly. If you don’t understand your risk, no amount of security controls will do the job because you don’t know if you are applying them to the right place.\n\nThe key is to start with the right set of policies and risk for your organization. One basic step is that even if your organization hasn’t yet fully figured out how to deploy a zero-trust architecture, denying all access requests until proven worthy is a step in the right direction. What this means is that there is a gate at every door, rather than a master key to everything inside. Default deny is a tenant of zero trust.\n\nIf a company’s strength is not in cybersecurity, or the funding is not available for a sufficient internal team, there are many resources to help. Managed security providers (MSP) are a good option, but there should always be at least one expert on the inside: a Chief Security Officer. This person has the local context of the domain experience to work with the MSP and bring that knowledge in and proliferate it throughout the organization. The MSP is managing your security tools and configurations, but you need someone to impart security wisdom to business and IT units. In light of recent attacks, a security team is not optional.\n\nEvery company should have a plan in place for a ransomware attack. Once it has already happened is not the time to figure it out. One basic is to back up your data regularly. Keep pristine copies of the data, systems, applications, and configurations in an offline, out-of-band storage environment. Six months of clean backup data is important because sometimes ransomware can be sitting in the backups before it is detected. Also, have the platforms or servers you need to run your database available offline so you can spin them up in a fall over or redundancy model.\n\nThis is basically business continuity planning. Just as an organization would have a plan for continuation in the event of a physical disaster such as a flood or power outage, there should also be a plan to continue with critical enterprise applications to get at least partially back up and running while the problem is being solved.\n\nOne step to accomplish this redundancy is to be able to burst to the cloud when necessary, keeping cloud resources in a pristine state and maybe even in a different cloud service provider. Another step is to have a canary in the mine. This means having systems deployed across the enterprise that have sensors turned to 11. To avoid performance, storage, and speed issues, you can deploy them in strategic places rather than system wide to serve as early warnings.\n\nCreating a plan ahead of time will also help with the challenge of what to do in the moment of crisis, whether you pay the ransomware or call the FBI. The plan should be on paper and involve not just your tech people, but your lawyers, CEO, CFO, etc…, and everyone should have access to it. You should know how to buy bitcoin, and you should have the number of the local FBI offices and other information. Run the plan as an exercise to see if it works just as you would a disaster recovery or business continuity plan.\n\nSome industries may think they are safe if they keep their operational technology (OT) and their informational technology (IT) separate, but they are not truly separated. For example, a manufacturing line may be running on computing machines, but much of what drives the supply chain, logistics, and overall organization are IT systems. If those systems go down, nothing is coming in or out. IT systems are mission critical and the learning of recent attacks is that we are reliant on digital technology for all of our businesses.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT60-en","image":"./episodes/edt-60/en/thumbnail.png","lang":"en","summary":"On this episode, part one of two, Darren discusses security trends with frequent guest Steve Orrin, CTO of Intel, Federal. Over the past year, there has been a rise in the number and sophistication of cyber-attacks. The three key areas of recurring attacks are ransomware, supply chain attacks, and data breaches."},{"id":134,"type":"Episode","title":"Data Breaches & Secure Supply Chain","tags":["datbreach","data","process","policy","cybersecurity","supplychain","securesupplychain"],"body":"\r\n\r\nIn addition to high profile attacks in the supply chain such as the incident with SolarWinds, there are other, lesser-known attacks that are equally problematic.\n\n## Secure Supply Chain\n\nOne example is The Great Suspender Chrome extension, a tool for making sure memory is used correctly in Chrome applications, with about two million users. The founders of this open source tool sold their company to an organization for millions of dollars without due diligence. The purchaser turned out to be an organized crime group that then turned the tool into malware and spyware.\n\nThere was no attack in the sense that they didn’t hack into the original company, but they purchased it and did whatever they wanted with the code. What was one day a legitimate product was illegitimate the next day. No amount of security protocol would have fixed that problem. Companies now need to vet their suppliers and the third parties that support them.\n\nOpen source is a blessing and a curse. The blessing is that since it is open source, you have access to the source, and you can review it all you want. The curse is that no one has the time, energy, or expertise to thoroughly vet every piece of open source code that they are using. Malware vulnerable code, even if unintentional, can be introduced if it is not caught by the community, and sometimes that doesn’t happen for a long time.\n\nThere are two things that could mitigate this problem: One is that code could be run through a source code analysis tool and there could be a rating system for code contributors whose code consistently comes with fewer vulnerabilities or bugs. Third party vendors would primarily do this work. Second, there are already some well established startups in the space of verifying open source products to show which objects in a repository are trusted and which are not yet trusted.\n\nCurrent vulnerability scans in open source is one control, but that kind of security alone is not enough. It needs to be combined with additional controls before it runs across your organization.\n\n## Data Breaches\n\nIn addition to ransomware and supply chain attacks, data breaches are a common problem. In 2020, there were 1.8 trillion dollars in data breaches extracted across 7.8 billion data records. A breach at McDonald’s, for example, compromised customer, partner, and internal data.\n\n## Encryption\n\nThe first part of the solution is to have better security tools in data and infrastructure. Encrypting access and incorporating default deny so that even if someone gets in through the front door, they do not have access to everything is critical. Data also needs to be encrypted inside the organization, not just what gets exposed to the cloud or what you send externally. Any data that transverses your network as well as data at rest should be encrypted.\n\nEncryption has a cost, but on modern hardware, there is built-in acceleration that obviates the penalty. You can now turn on encryption throughout your organization without performance impact.\n\n## Segmentation\n\nAnother part is enclaves, or segmentation.  One of the challenges in network corporate environments is that once again if someone gets in the door, it’s free rein if everything is connected. There has been a movement lately to take dev and move it into its own network, and that’s a start, but it’s only the tip of the iceberg. Network segmentation should be across the organization. You can still have transverse, but it’s up against a set of rules and will help limit the impact. For example, if your help desk gets attacked, your HR systems won’t be compromised at the same time.\n\nMicro-segmentation was a buzzword five or so years ago, but it needs to happen now. There are some great tools out there to help with this, such as container ecosystems where you can deploy an application and it’s in its own network with its own firewall.\n\nImplementing proper authentication credentials also needs to happen now. Multi-factor authentication is necessary, as well as entity authentication. Many tools are automated and have automated processes, so entities, not just people, must have proper credentials.\n\n## Zero Trust\n\nZero trust has matured to the point where it should be implemented, and some of the key tenants around default deny and trust no one are critical. The technology has caught up to deployment of those types of concepts.\n\nDevelopers may worry about these security tools slowing down the process, but there are ways to construct the architecture to lessen this problem. For example, if you are a developer and have proper credentials and access, you should be able to access the things you need when you need them, and lose access after you are finished, rather than having a credential that gives you access to everything all the time. The idea of zero trust is not that the company doesn’t trust the developer, but that access is for the right moment, not just blanket access if a bad actor steals the credential.\n\nNo industry can afford to ignore the current risks. Every organization must look at security differently and implement security across the organization and architecture. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT61-en","image":"./episodes/edt-61/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, discusses data breaches & secure supply chain with frequent guest Steve Orrin, CTO of Intel, Federal."},{"id":135,"type":"Episode","title":"Demystifying 5G, Edge and AI","tags":["edge","compute","5g","aiml","iot"],"body":"\r\n\r\nAnna’s background is in the industrial and manufacturing side in information. She has a PhD in chemical engineering, as well as an MBA. She spent 15 years working as a process and design engineer, later doing startup work and engineering management. She has been with Intel for a little over six years, the last two and a half working in the public sector team where she primarily supports Intel’s IoT and edge activities, with increasing involvement in 5G because 5G, along with AI, is dramatically changing that space.\n\n## Network Transformation Foundational to 5G Infrastructure\n\nIn the industrial world, IoT was not anything new; it was just hardwired. With systems now moving to wireless, they have the close coupling of IT plus OT to convert analytics, instead of just having a set of process data specific to that OT world. A different set systems now marry that to the business, so all of this is being pulled into the same space. Although there are some very defined differences in use cases and architectures in the public sector such as military and smart cities, there is a common convergence in the analysis, application, and timeframe to make better business decisions.\n\nOne reason IoT has taken so long to move out of manufacturing and into other areas is primarily hardwiring cost. Security has also been a roadblock.\n\n5G is now unleashing the IoT and edge world because of cost effectiveness, especially on the consumer side. When it comes to critical business infrastructure, however, it’s a different conversation about how to do it in a way that protects the data. 5G will be transformative, but it's not happening just yet on the commercial side. Part of the gap is because of the delay between when the standards are released and the hardware to take advantage of those standards are produced.\n\nFor anyone who has spent a lot of time with 3GPP or other driving standards bodies, this timing is not unexpected. Right now you can stand up a 5G network, but we’re at the stage of doing testbeds where we have to demonstrate the value of 5G. We need to show new use cases that can’t be supported by LTE or by 4G.\n\n## Multiple 5G Deployment Models Deliver Flexibility\n\nWhy not just stay with WiFi 6 instead of using 5G? The answer is complicated. Many of the standards organizations that were driving 5G were also driving WiFi 6, so it is a complementary technology. The differentiator is in the equipment, so you need to know the details of your use cases to determine would be the most cost effective. For example, 5G is amazing, but it wouldn’t be best to put a 5G network on a cruise ship because penetration isn’t there for that to make sense. The environment isn’t friendly to any kind of wireless signal, but WiFi 6 with access points makes more sense.\n\n5G is compelling, however, for several reasons and can do things that 4G and LTE can’t. For augmented reality and virtual reality, you need very low latency and high bandwidth to enable more interactive use cases, for example equipment or machine repair. You need a remote expert that is standing looking at a problem that can do video and audio streaming with overlay of drawings and capabilities that are being managed back from a central location or remote location that is bringing all of that knowledge and expertise directly to the point at which you’re trying to do work. That type of use case can’t be done over an LTE network. Bandwidth plays a part, but latency is the driving force. In order not to get sick while using a headset, you have to have really low latencies with no delay or have things go asynchronous.\n\nThe LTE and 4G world has changed because of 5G due to how they managed spectrum. One area where this is changing in industry is that it is now possible for a company to get a priority license for CBRS spectrum and stand up their own private network, wholly divorced from the federal major telcos. For example, a large manufacturer can cover a huge space more cost effectively with a private LTE than it can with access points. There are also great benefits such as if you want to reconfigure your space and you don’t want all of your workstations to be hardwired, or say you have to move around enormous pieces of metal such as airplane fuselage in your space that could interfere with WiFi signals, you can stand up the infrastructure to be portable and not fixed to hardwire locations.\n\nSecurity is a core concern for any organization. Although 5G was not written with security in mind, 6G will be. Luckily, with the capabilities of 5G, we can do a lot around zero trust networking and other security measures that will instill customers with confidence about how their data is being moved across networks.\n\nThe 5G standards have changed the problem of a few years ago when wireless infrastructure existed in proprietary hardware and proprietary software, with a licensed spectrum only a few companies that could afford.  Software-defined networking enables the ability to host network infrastructure on common, off- the- shelf hardware. There is no need for specialized hardware as in previous generations. This is also happening on the LTE side with, for example, making the CBRS spectrum available and getting away from the proprietary hardware and software.\n\nIntel spends a lot of time working with disruptors that are using our FlexRAN reference architecture. The FlexRAN architecture becomes the basis for helping disruptive technology to proliferate in the new 5G marketplace because it provides a 5G software stack running on common, off-the-shelf hardware where before, proprietary hardware was required.  Now there is a space with much more openness and portability, and the cost of entry is much cheaper than it used to be. It’s no longer just a few companies controlling everything. Intel and others are trying to open everything up and take advantage of open standards to support all these disruptors and change the entire dynamic.\n\n## 5G Spectrum and Regulation for Tomorrow’s Networks\n\nWith improved connectivity, low latency, and high bandwidth, many new use cases will be available. How 5G will be monetized is what is changing across the market. For example, a cloud service provider along with a telco can provide better services for their customers because they are no longer siloed. They are a combined business effort of what really matters: quality and prioritization. Another way to look at this is cloud service providers are buying capabilities that are going to open up network functionality in the same way that telcos are exploring what they can do on the cloud side. Again, this is because the silo is broken down; the data pipe is no longer a set of services.\n\nIt’s not clear how all of this is going to fall out, except that it’s redefining what kind of work you can do because of data accessibility, and where those workloads are going to live. There is a huge value in going from edge to cloud seamlessly and doing it in a way that is based on the need of the customer, which is now possible.\n\n## A New Compute Paradigm Supports New Data Demands\n\n5G is unleashing many different architectural models. For example, it gives two options of architectures for AI, whereas before there was only one with limitations.\n\nWithout the high bandwidth that is provided by 5G, AI was limited to inference on the edge devices, which required pushing AI models out to the edge devices. This cumbersome restriction increased the AI development and deployment cycle and limits the number of AI workloads that can be leveraged at the edge. With an increase in bandwidth, large data streams from cameras or sensors can be sent back to a datacenter which enables multiple AI workloads to be run and for continuous AI learning to occur. This gives organizations the opportunity run both inference on the edge at the same time improve the deep learning required by so many organizations ever-changing demands on their data.\n\nWith AR, for example, 5G means that headsets can be mobile instead of tethered with the same capabilities because 5G allows for the sharing of larger data sets in an untethered world. The data centers traditional walls are being broken down.\n\nIf you don’t have a lot of tech support or detailed knowledge of how to keep your systems running, you can run it all in the cloud. If you don’t want your data on the cloud, you can do a version that’s on prem over a private network that gives you all the type of functionality to aggregate and correlate data to provide a high level understanding of what’s happening in your system in a secure, cost-effective way.\n\nBasically, your data can now reside on the edge, in the cloud, on prem, or in what Cisco calls the fog. It doesn’t matter any longer where your application runs, so you can use the most cost effective model. In industrial spaces, for example, there are massive savings in not having a hard wiring component, or by using a private LTE structure rather than WiFi access points. Getting these types of costs down will lead to the ability to have super rich data. These barriers of cost and physical connectivity are what’s been missing for IoT to take off the way everyone predicted.\n\nAnna predicts that for non-control applications, the next two years will be different because of 5G. A simple example is that in industry, someone could take their regular PC out of the office and onto the factory floor and be able to do everything there. 5G will change what’s possible with respect to controls and doing control of robots and machines over a wireless network in the next five years. The next level of transformation will be that you can do control over a wireless network and do it safely and effectively, putting no one at risk. This will take a lot of validation and stringency of review, but it’s on the horizon.\n\nAlso, it will be exciting what your favorite cloud service provider and telco are going to do together to change what’s possible from a services standpoint. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Anna Scott"],"link":"/episode-EDT62-en","image":"./episodes/edt-62/en/thumbnail.jpg","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, discusses the groundbreaking changes 5G will bring to edge and AI with Dr. Anna Scott, Chief Edge Architect, Intel."},{"id":136,"type":"Episode","title":"Inspiring IT Cultural Change","tags":["organizationalchange","change","people","covid","cio","california"],"body":"\r\n\r\nAmy has served as the state CIO for over five years, appointed by Governor Brown and continuing under Governor Newsom. She worked as a technologist in the public sector for twenty years in a variety of areas from environmental science to CalPERS, to health and human services before being appointed CIO.\n\nOn March 13, 2020, Amy was hosting a meeting discussing, ironically, the need to accelerate the goal of broadband service for all Californians, when the COVID shutdown was ordered. Immediately after that, the state had to transition over 200,000 state employees to telework and implement distance learning. Overnight, the broadband issue became an urgent matter in dealing with day-to-day business. The state started with 5% capability for telework and within a month got up to 92% capability.\n\nThis was the biggest operational hurdle of the pandemic. People had to get used to working remotely and there was a shortage of supplies, which made it even more difficult. The whole change, however, happened relatively smoothly. People were understanding and flexible and allowed continuous improvement to happen.\n\nThis is a process that would normally take years to accomplish. The difference is that people didn’t think twice to say “this is what needs to be done” and got moving instead of overanalyzing everything. Everyone just made things happen, even though it wasn’t perfect.\n\nCybersecurity was also a hurdle, as people working from home had to use the same good practices as if they were in the office, and that took a bit of a cultural change.\n\nOne surprising change was that the desire for using an evidence-based decision making process also accelerated overnight. It sounds a bit like an oxymoron in that everything couldn’t be overanalyzed, but on the other hand, the focus narrowed on those critical decisions that required evidence-based support. The unimportant things got pushed aside because there wasn’t enough bandwidth to do everything.\n\nThe state is still learning what the workforce will look like moving forward. Telework remains for the foreseeable future, and some might become permanent because there has been an increase in productivity. In addition, positions that don’t work well with telework need to come back to a safe environment. Employees’ well being is also a factor in the new dynamic.\n\nGovernment and other organizations need to keep a balance between the highly tactical last 18 months and strategic planning for the future. Amy keeps the strategic view in the state 2023 plan as the “North Star,” but also focuses on current incremental improvements to deliver services. There is an environment of understanding that things aren’t going to be perfect as the state returns to a new normal.\n\nAmy has two cultural goals for her department stemming from the pandemic: People in support positions need to be able to connect their work to the positive changes and impacts to keep up morale, and the environment should become more visionary, reminding workers of the strategic North Stars.\n\nTo maintain a positive cultural shift, leaders must model the way and take into account employees’ well being in their decisions. This includes being flexible with how they work best and giving them the confidence to make decisions to feel fulfilled and empowered. And if things don’t go perfectly, dust it off and move on.\n\nAmy’s advice for other state CIOs would be to do a lot of proof of concept work, rolling up sleeves and trying things out, and be open to different ways of solving a problem. The outcomes of proof of concept better informs leaders in decision making than just trying to analyze things. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Amy Tong"],"link":"/episode-EDT63-en","image":"./episodes/edt-63/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, discusses inspiring cultural change with Amy Tong, CIO of the state of California, in the wake of the COVID pandemic."},{"id":137,"type":"Episode","title":"Follow and Secure the Bit Edge to Cloud","tags":["edge","compute","multicloud","cybersecurity"],"body":"\r\n\r\nIn episode #2, Greg explained the complex questions of collecting, moving, and analyzing data in a Department of Defense (DOD) environment that includes edge devices on ships, planes, and even war-fighters and the need to move and analyze that data quickly for timely, actionable decisions. Intel’s role is to help shorten the process cycles and relieve bottlenecks in the data flow with its technology portfolio. It’s all about matching technology to the mission.\n\nOne recent architecture trend is moving the compute, processing, and intelligence forward to meet the data rather than moving the data to the back end. This adjustment provides the best capability to get real-time responses and intelligence. This approach is a different operational model with challenging questions.\n\n## Heterogeneous Hardware\n\nIn the foundational infrastructure, how do you provide the right compute, storage, memory, and network capabilities to drive analytics at the edge and drive processing where you need it? How do you manage those applications and data? How do you introduce quality and curation of data further up the food chain? Security is a foundational piece of any infrastructure, and now, data must be protected all the way at the edge.\n\nThe industry needs to get to more heterogeneous compute so that the underlying software can run on various hardware platforms so you don’t get locked into a specific software or hardware stack. Part of the beauty of what Intel has done for years is providing software portability: the open framework, x86, and other underlying systems.\n\nAn important factor in how Intel develops capabilities is understanding the workload and data flow rather than focusing on modernization, such as putting things in the cloud. It’s not about how you procure IT (cloud or on-prem) but optimizing the business flow to take advantage of how the data flows and what it is really doing.\n\n## DevOps to the Rescue\n\nAnother critical aspect of these new solutions is deployment time. DevOps has become increasingly important across the DOD, and efforts to develop common DevOps platforms have increased dramatically.  To write code once and run anywhere (cloud, on-prem, or on the edge), organizations use the x86 chipset and Intel’s oneAPI to deploy complex services across the complete ecosystem. For example, oneAPI allows developers to write complex analytics or AI jobs that run on a GPU, a CPU, or even an FPGAs with the same code base. Developers can then test these complex systems in their data center, assuring hit will behave the same way when deployed into the field, decreasing deployment time.\n\nBeing able to leverage a combination of a flexible DevOps environment, which leverages containerization and software frameworks like oneAPI, gives programmers the abstraction they need but with hardware/software optimizations built-in. A key distinction here is that we have figured out a way to take all the power of the hardware and optimize software so that an application can still get performance improvement and still take advantage of specialized hardware.\n\nThe container ecosystem provides abstraction at the system level, so things don’t need to be hard-coded. Providing the ability to scale up or down based on service load and capacity. For example, if a capability runs on the edge, in a portable data center, or back in the cloud, and you lose connectivity or an asset, you can still carry out the mission. This intermittent connectivity is vital in a DOD environment.\n\n## Edge Solutions\n\nMany people think of Intel as a silicon or hardware provider, but the company has developed solutions and reference architectures in the edge to the cloud ecosystem. One example of progress is a naval tactical grid where processing happens on the edge, ships, aircraft, and onshore. The applications have to run across the whole ecosystem, and that can’t be done with only hardware. Intel has leveraged its silicon and security features and, building on top of a container ecosystem, produced complex applications with several microservices that can run when connected and disconnected assets.\n\nAnother example of edge architectures is tracking objects across sensor meshes, even with gaps in the sensor mesh.  By performing sensor fusion, data fusion, and cross-domain analytics, you can track objects across different scenes and across different sensor types, scaling up and down to create a mobile sensor platform that can do edge sensing analytics and fusion. It can then also hand off to a distributed set of nodes that can work in concert to track an object across all those different existing sensors.\n\nFor instance, if you’re tracking an object and it moves between sensors, you will lose it for a moment. What this intelligence allows you to do is to connect those two feeds and track the blind spots. Not only does this work with homogeneous sensor types, this can work with multiple types of sensors such as motion, RF, and IR sensors, so if you lose video but still have an RF and then pick up the video again, you can show the full path of the object. In the past, these types of analytics required moving data to the data center to process these complex interactions; now, we can do this at the sensor mesh edge through microservices at the edge.\n\n## Flexibility of Deployment\n\nAn advantageous approach is building once and using the same architecture and software for different missions with an analytical requirement. Some places where heavy-duty computing is at the edge can scale up and take full advantage of the hardware capabilities. Other platforms may have limited compute capabilities and can run smaller microservices providing only a portion of the complete solution. New mission capabilities and services can be rapidly deployed by building these microservices once for multiple use cases.\n\n## Security at the Edge\n\nComplexity is sometimes the worst enemy of security, but foundational security principles can also secure data and applications out on the edge. Rather than the old approach of protecting the server and hoping everything stays there, it’s essential to understand where the data flows and every place it exists, protecting it no matter what it’s running on. Risk-based approaches and concepts like zero trust have gained traction because they take a systems-independent view of security.\n\nBoiled down, zero trust means default deny. No one gets in unless they need to, and then only for the period for the necessary action. When temporal access controls are married with a risk-based approach of protecting the data throughout its lifecycle, the result is the ability to protect the data regardless of where it is and who is accessing it. This technique is one of the ways to secure these highly complex environments.\n\nThe practical action to take in these ecosystems is to apply a policy that leverages controls that meet the risk of a given system at a given point in time and then continually monitor and update them in real-time to meet the ever-changing cyber-threatening world. Use the technical controls that the hardware and software capabilities already provide, such as Secure Boot, hardware root of trust with TPM modules or SGX storage keys, encryption, etc.\n\nThere is no silver bullet you can buy to provide an end security solution in these complex ecosystems. It’s about creating and enforcing security policies as threats evolve and deploying them at scale, leveraging hardware, software, and the processes necessary to secure the bit as it flows from the edge all the way through the ecosystem.\n\nThe DevOps framework provides effective mechanisms to handle security across all assets in the ecosystem. Containers should be populated with instrumentation to enforce the security controls and policies. Security must be built into the DevOps process itself because if you rely on the developer to implement the security, each will do it slightly differently, increasing complexity and variability in the system. The developer must have the capabilities and the constraints upon which they have to develop.\n\nDevelopers will still have to do security work, such as making sure they use suitable security tools for the particular threat environment, but the heavy lifting, the complexity, should be abstracted into the DevOps architecture.\n\nOne of the critical areas in a complex theater environment is edge device management, such as monitoring and updating firmware.  Making sure those devices are secure in order to be able to support the data security and profiles and policies deployed in the systems at scale is going to require innovation. That’s why the ecosystem is really growing currently: to meet that challenge. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Clifton","Steve Orrin"],"link":"/episode-EDT64-en","image":"./episodes/edt-64/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, discusses following the bit with Greg Clifton, Director, DOD and Intelligence, Intel, in a follow-up to episode #2, along with Steve Orrin, CTO Federal, Intel, who lends his expertise on security in complex edge to cloud environments."},{"id":138,"type":"Episode","title":"Accelerating Cloud Adoption while Decreasing Complexity and Cost","tags":["cloudmigration","cloud","compute","process","multicloud","aiven","cloudcost","technology"],"body":"\r\n\r\nDavid was previously an application developer, and describes himself as a recovering over-engineer-er. That experience, along with his work with AWS and GCP at different companies, gives him the background to understand the nuances of how to run in the cloud, how to scale when it makes financial sense, and how to meet many of the challenges when moving to the cloud.\n\nMany organizations are moving to the cloud, accelerated by consequences of the COVID pandemic. Large and small companies are doing it differently. Smaller companies are moving to the cloud and adopting it as fast as possible, picking up new tools and best practices, but there is still a lot of lift and shift in the data center. During the lift and shift, however, they’re often not picking up any of the benefits of the cloud. So they are moving to be digitally transformed, but they have to ask, what are the next steps?\n\n## Reducing Cost\n\nThere are many things you can do, and should do, for cost reduction and to avoid unpleasant surprises, such as the costs that come with zombie VMs. In the cloud, if you need a server, just click a button and you got it, but then if you forget it’s running, a month later there might be a $10,000 charge on the servers. Another thing to take into consideration is network egress that can add up, especially with region-to-region transfers if you are doing terabytes or petabytes each month.\n\nSpending alerts can help mitigate these issues.\n\nCost projections can be difficult because of issues such as free ingress, but getting the data back out is expensive. Deleting data is also a cost. So organizations need to think about the caveats of how much data to put in, the type of operation, and the usage.\n\nFor cost optimization, something that engineering leadership needs to think about is that you will get to a point where compute and managed service are not that expensive; the most expensive resource are your people. So then how do you optimize the efficiency of your developers? You don’t want them doing remedial and repetitive tasks.\n\nAutomation is key here, especially for any repeated tasks done with high frequency, as well as managed services.\n\nEvery company’s life cycle comes to a point where they have to decide if they are going to invest in and own all of the operations, or hire a DevOps team, or leverage managed service providers. Hiring in-house expertise is expensive. The tipping point is when they decide how well the cloud scales for them.  It’s usually best for companies to consider the total cost of ownership, focus on their competitive advantages, and use managed services for other services.\n\n## Reducing Complexity\n\nOne of the complexities of the cloud is that you need to be able to recreate your environment with reliable and repeatable deployments.  This does not mean that you are going to the website and clicking on it; you are deploying your environment from, for example, a script or YAML file or TerraForm, and you need to be able to spin it up and tear it down quickly.\n\nYou need to be able to kill a server that has become dirty and reliably recreate it in a clean state. If someone moves log files around, tweaks configurations, deletes a database by mistake, or if there is a ransomware attack, you have to be able to reproduce your environments or components of your environment to reduce downtime.\n\nThe critical piece is to have a plan in place based on how quickly you need to recover and how much data you need to continue. It makes sense for some industries to invest in a plan to move to a different region rapidly. You need to weigh how much you are willing to invest in recovery time.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","David Esposito"],"link":"/episode-EDT65-en","image":"./episodes/edt-65/en/thumbnail.jpg","lang":"en","summary":"On this episode, part 1 of 2, Darren talks to David Esposito, Global Solution Architect, from Aiven about accelerating cloud adoption while reducing complexity and cost."},{"id":139,"type":"Episode","title":"Managing Complexity in the Cloud","tags":["data","devops","compute","technology","process","devsecops","cybersecurity","aiven"],"body":"\r\n\r\n## Reducing Complexity\n\nWhat business practices should be put into place when managing cloud assets to decrease complexity? First, you must have infrastructure code and automated deployments in place. Everything that comes next is a conversation to define what risks there are to the company such as outages and downtimes, how to mitigate these risks, and how much to invest in that.\n\nFor some companies, it’s critical to have the highest level of uptime. In healthcare, for example, you might have to have all of the data backup in multiple locations as well as all services available because critical, life-saving decisions might be based on that data. But if you are in ecommerce in April, maybe you can spend some time in that sick region because the consequences are not as dire.\n\nIn healthcare, it makes sense to go multi cloud even though it increases the complexity. Other industries that can benefit from multi cloud are those that are highly regulated such as financial services, oil and gas, or federal agencies. If there is a security incident, they can shut one off and deploy elsewhere.\n\nWhen considering multi cloud, know that are some complexities with differences of APIs. If you are in GCP and you know how to deploy everything and thinking of going to AWS, you have to relearn the platform and deployment process, perhaps even writing some of that code. For this reason, containerization is popular because you can deploy anywhere. It does become more complex with other services; for example, Postgres has different configurations in AWS versus GCP, maybe with different patching versions, different networking setups, and different extensions. So, a service on one cloud is not the same as services on another.\n\nIn addition, containerized environments are not ideal for every situation, and a ideal situation becomes more important at as you scale. If you are doing Kafka and 1,000 messages per second, your own container is fine. But if you are starting to push a million-plus messages per second, a gigabyte, that’s a different conversation.\n\n## Security\n\nSecurity in the cloud is a different beast than security in your own data center. The most important thing to think about is data going outside of your own walls. You must have appropriate security controls in place to ensure data doesn’t leak or have any unauthorized access and make sure compliance requirements are in place. This requires training because the security measures for on premises data centers don’t directly translate to the cloud. For example, S3 containers are encrypted at rest, but not if you have programmatic access through publicly exposed buckets.\n\nIn addition, you need to know what the service providers are doing. For example, Aiven runs databases on GPUs EC2, GCP Compute, or Azure. When they store to disk, there’s one level of encryption that’s done at the cloud level by the cloud provider, so the data is encrypted at rest. Then, Aiven does another layer of encryption with their own managed keys, so data is double encrypted at rest, and any data in transit is encrypted. Anything going to the server is encrypted for that particular server, and when a server is decommissioned, it’s locked with Aiven’s key as well as the cloud provider’s key.\n\n## Aiven\n\nDavid joined Aiven as part of the solutions architecture team when he saw that the company did cloud solutions better than anyone else he had seen, based on his previous experience with managed Kafka and other open source technologies. The mission of Aiven is to make developers’ lives better. They offer a free trial, with production-ready and Kafka going in five minutes. They also have help articles and tutorials to walk you through, so you’re sending a Kafka message in the cloud with cloud-native best practices in less than ten minutes. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","David Esposito"],"link":"/episode-EDT66-en","image":"./episodes/edt-66/en/thumbnail.jpg","lang":"en","summary":"On this episode, part 2 of 2, Darren continues his conversation with David Esposito, Global Solution Architect, from Aiven about accelerating cloud adoption while reducing complexity and cost."},{"id":140,"type":"Episode","title":"5G Past, Present, and Future","tags":["5g","comms","wifi6"],"body":"\r\n\r\nLeeland has been in the telco industry for 20 years. Right out of college, he was recruited to work for the US Department of Defense after they were impressed with his work on applying Bluetooth multiplexing within an automobile, reducing wiring harnessing size. Instead of working with cars as he originally planned, he ended up working with the Department of Defense developing advanced wireless technologies for soldiers.\n\nAt that time, cellular technology was in the transition from 2G to 3G. It was not called WiFi at that point, but wireless LAN, and there was a nascent technology called Bluetooth that they were trying to use to provide the soldiers the ability to gather information within the battle space.\n\nLeeland left the Department of Defense three years later in order to do hands-on work with the technology to gain better understanding. He began working for Sprint and stayed in the telco industry for 13 years designing technology for wireless networks before joining Intel in 2017 to work on 5G.\n\nUntil the mid-90s there was little cellular usage. “Brick” phones from the 80s were huge because of the battery size, and they were expensive. They evolved into “Bag” phones in the early 90s, which were better, but you could only hope they would work to make a phone call.\n\nAs 2G came into play, new features based on customer wants were developed such as text transmission and games. The device could do some low rate data transmission with TDMA technologies, time division multiple access, and GSN, for example. In the mid-90s, companies like Sprint came on the scene, and cell phone usage became more broadly used. In the early 2000s, there was a significant transition with the customer expectation that internet should be available on phones as it was on laptops.\n\n3G brought nominal broadband capabilities where you could use the internet somewhat, but the primary capability was transmitting images. The name “3G” was coined at this time, and this is when the 3GPP body of standards came into play.\n\nIn 2008, telcos began looking at deployment of infrastructure that wasn’t based on large cell towers, but a more distributed system of a hub of baseband units with antennas front hauled via fiber to telephone poles. This began the transition to true broadband, from 3G to 4G.\n\nThis technology should get credit for the economic boom of 2010 to 2020 because with 4G in hand, companies like Amazon and Netflix evolved and flourished.\n\n5G frees services from a monolithic RAN “jail” and opens the field because 5G provides open source architectures with a software-defined base. Software stacks can now be developed and integrated into a whole software solution.  Since it is not tied to a monolithic architecture, 5G can provide services and standalone private networks.\n\nThis massive flexibility is going to allow telcos and their carriers to enhance services and offer various new capabiiities, including bringing access to edge compute.\n\nWhat’s coming next? Leeland sees a movement away from “G,” as it doesn’t give enough credit to the evolution of the technology as there are no real partitions anymore in terms of who can deploy networks. The technology and use cases are broad. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown"],"link":"/episode-EDT67-en","image":"./episodes/edt-67/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, talks to Leeland Brown, Technical Director of 5G, Intel Federal about 5G past, present, and future, with emphasis on its use with the Department of Defense. Part 1 of 2."},{"id":141,"type":"Episode","title":"5G in Defence","tags":["5g","dod","compute","technology","cloud","edge","comms"],"body":"\r\n\r\nAt the turn of the century, the Department of Defense was looking at ways to apply 2G technologies, wireless LAN, and Bluetooth in applications to enable soldiers to capture information in the battlespace. In 2021, those capabilities are still being sought after. The Department of Defense is looking heavily into 5G now since it has moved away from monolithic constructs such as utilizing one RAN architecture and into software defined architectures. There is the flexibility to build new capabilities faster and scale it to different use cases.\n\nThe commercial side of 5G is driven by strategies and stretching developing revenue. That doesn’t necessarily line up to the mission requirements of the federal government or Department of Defense, so the lines between the two worlds are blurring as, for example, AT&T Verizon looks at federal use cases, or Lockheed Martin with their 5G Domino program looks at the commercial space. The difference is who understands the mission and who is driving for the purpose of revenue.\n\nCurrently, ninety percent of commercial 5G deployments are non-standalone networks, meaning 5G is still connected to an existing 4G evolved packet core. The Department of Defense is interested in standalone networks with a complete 5G core, 5G RAN, and 5G devices. There can be a standalone network for a group of soldiers, along with small form factor base stations for vehicular platforms, and multiple protected domains, even drones deployed with some type of 5G access point and space applications.\n\nThis ability to scale across multiple use cases and their various types of workloads is applicable to the commercial side as well. One of the biggest issues, however, is who owns the frequency. Some free spectrum is available with limited range, but these bands, such as ISN bands, are unlicensed and very crowded. The rule is that you must give and accept interference and have your technology work around that, but dynamic system sharing is possible. However, for federal use cases, you can’t be blocked into operations in the U.S. since most soldiers are deployed outside of the U.S.\n\nLeeland predicts that 5G will become integrated into everyone’s life, sometimes when you don’t even know its there. Pattern recognition platforms, for example, facial recognition, and autonomous vehicles will all be connected via 5G. Your broadband access will be integrated into a seamless footprint, connected beyond your cell phone to your car and home. The need for a phone in your hand will dramatically reduce as we begin to see wireless access points in all parts of our lives.\n\nLeeland also predicts that the “G” will go away as the technology expands and evolves.\n\nTo learn more about 5G, the technically inclined can go to the 3GPP standards to see the specificity cases. You can read a spec and understand the difference between release 14 and release 15 and what that means for the industry as a whole.  There is also a lot of information available via the internet such as white papers. Leeland also offers himself as a point of contact.\n\nLeeland would like the next step to be a call to action to make the networks more resilient through the adoption of new technologies. During emergencies, whether natural disasters or terrorist attacks, resiliency is necessary for first responders, as well as for people simply trying to reach their families.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown"],"link":"/episode-EDT68-en","image":"./episodes/edt-68/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, continues his discussion with Leeland Brown, Technical Director of 5G, Intel Federal about 5G past, present, and future, with emphasis on its use with the Department of Defense. Part 2 of 2."},{"id":142,"type":"Episode","title":"Securing the Data Center Through the Edge","tags":["cybersecurity","edge","compute"],"body":"\r\n\r\nA holistic look at security includes the whole infrastructure, from within the cloud environment all the way out to the edge. If you can’t secure the client, you can’t secure the enterprise, so it's essential to have honest conversations in an approachable way, without the jargon, about security.\n\nGone are the days when an employee would get a laptop shipped to an IT cage, then configured and delivered by IT. Sometimes employees never even come to the worksite, so devices need to show up at someone's doorstep. They must be provisioned to make sure they are safe and haven’t been tampered with. From a security standpoint, this is a challenging problem. The supply chain is problematic. There’s less control over the devices and how they get deployed.\n\nThe sudden switch to working from home because of the COVID pandemic required an unprecedented rapid response to this problem.  It may have taken years if it had been a natural progression, but the situation didn’t allow an alternative; IT had to rise to the occasion.\n\nWhen Camille worked in the IoT group, they tried to solve the connectivity and manageability between devices. Complex ecosystems like wind farms, underground mines, implanted medical devices are hard to update when connectivity is sporadic. They were trying to solve these connectivity issues on the edge, and when COVID hit, the intersection between OT and IT suddenly became the center of all enterprise IT departments.\n\nAs some of Darren's customers were battling with how best to bridge OT and IT, COVID hit, and some of it collapsed, resulting in security breaches.\n\nMany people quickly rolled out a work-from-home system and dealt with security later, depending on the organization's maturity level. There was also a massive shift to the cloud. Currently, there is a bit of a pendulum swing back because of breaches such as ransomware attacks. Those breaches happened in the cloud, primarily because people did not understand the shared security responsibility. Now, some organizations are thinking twice about moving their critical data into the cloud. They may move workloads there but are keeping the golden data at home.\n\nAnother recent shift is the importance of client perception. You may be asking the right hardware questions and software layers of protection questions, but you also have to consider your client's perception about where you are keeping the data and why, who is protecting it, and how they are protecting it.\n\nA bigger problem is organizations that cannot answer any of those questions. Sometimes they don't even know where their data is. These organizations should see this as a starting point for what work still needs to be accomplished.\n\nSome new issues exacerbate this problem that the industry has not addressed yet, such as video conferencing. The meeting recording is saved on a laptop, but it's also in the cloud somewhere. Who has access to it? What are the protections? How long will it be there?\n\nOne principle of security is knowledge of whether your device is safe. One of the challenges uncovered with the COVID situation is that many IT shops are hesitant to update systems. They do not want to take a system down, whether it is a server or a client. Not updating for security patches is a mistake.\n\nIntel's job is to work with partners and fellow travelers to make those update a more straightforward, higher trust activity where people will have confidence that it works and something won't go wrong in the process. The industry has made considerable strides in making the update process more systematic and predictable in the last few years.\n\nAnother part of the job is training people to understand that security doesn't stop when a device is shipped from the manufacturer to a customer. Security continues over the life of a device.  What was world-class security at the time of shipping is not world-class security months or years later. Companies should update their machines twice a year to keep them safe.\n\nPeople get nervous about doing updates because unexpected things might happen. Intel validates at scale to prevent issues, whether with thousands of machines in their labs or with OEM partners in labs scattered all over the world. The complete validation makes sure the mitigations work to protect against the vulnerabilities and do no harm to the system. Intel has made a significant investment in partnering and collaborating with its ecosystem partners and driving standards across the industry, and looking to improve the user experience in the future by developing the ability to do the updates without a reboot.\n\nEducating customers about why you are asking them to do an update can go a long way, too. If they understand you have found a vulnerability and may be open to a potential attack, they will likely want to do it.\n\nIn general, people seem to be willing to update their cell phones because they aren't as worried that something won't work afterward, but it's still a challenge on the PC and server sides. Some of that has to do with usage models. Although it's rare for data only exist on a laptop, that mentality is prevalent. When the data exists in the cloud on a cell phone, the perception is it's always going to be there. In addition, people tend to do more immersive, engaging work on laptops than on phones, so they are more sensitive to it. Once the perception shifts and people realize the data on their laptop also exists in the cloud, the updates are more widely accepted. So, in reality, the industry needs to do a combination of technical solutions and mindset changes when it comes to security.\n\nThe way things are evolving is a bit of a hybrid. New learning models like federated learning are rushing in to help address issues like privacy concerns. Models are being pushed to the edge instead of the data moving to the data center. For example, a medical imaging system in a hospital where the data stays put and the model is coming in to look at it. We are starting to see this in industrial applications, where machines are at the edge and become the server. They'll keep data local and do training and updates there. So there will be intelligent devices on the edge, doing things with the raw data, and the question is, how do you secure that?\n\nAnother trend in security, one that didn't start with the COVID pandemic but was undoubtedly accelerated by it, is protecting against physical attack. Historically, security has been focused on something that could happen over the wire such as a network attack or a malicious application. With IoT devices out there with no human attached or watching, we have to protect the data and the devices from being tampered with. That's a difficult challenge.\n\nNowadays, you can't think holistically about security unless you’re also addressing privacy. One complication is that privacy can sometimes be in direct conflict with security. There are no agreed-upon regulations or standards worldwide, so organizations have to figure out how to operate: Hit the highest common denominator, or address every geopolitical requirement? To complicate it further, laws and regulations are constantly changing.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Tom Garrison","Camille Morhardt"],"link":"/episode-EDT69-en","image":"./episodes/edt-69/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, discusses securing the data center through the edge with fellow Intel executives and podcast hosts Tom Garrison, VP of Client Security, and Camille Morhardt, Director of Security Innovation and Communication."},{"id":143,"type":"Episode","title":"Securing the Teleworker","tags":null,"body":"\r\n\r\n## Teleworker Modes of Operation<h2>\n\nThe teleworker modes of operation are not new, whether it is using a device as a terminal (VDI), part of the internal network, or as a portal to services/software (SAAS); what is new is the sheer volume of employees, contractors, and partners all now using existing environments to do their core work, rather than a select, manageable few. Security is a challenge in all of these modes of operation. Organizations need to figure out how best to deploy their existing capabilities to secure and protect access, data, devices, and users.\n\n## Threats of Teleworking<h2>\n\nSome of the security threats are those that have always been present with teleworking: security of the end device and data, the access point, and access to enterprise services. In adapting to the COVID-19 environment, however, new problems have arisen because of the large number of people now teleworking and the mad rush to enable them.\n\nOne issue is workers at home in unsecured environments on networks that were never intended to be used outside of the corporation. This leads to targeted phishing and malware attacks. Another problem is the introduction of new tools, such as those for collaboration and video conferencing that can expose your enterprise and data to attack and malicious use. Basically, there is now an increased attack surface area, much larger than what most organizations initially planned for, and new environments that were never part of the original plan.\n\n## Solutions for Securing the Teleworker<h2>\n\n### Bring your own Device (BYOD)<h3>\n\nOne of the harder issues to deal with is when workers use their own devices. The key challenge is that these are unmanaged devices without all the security agents running and managed by IT; the device is owned and managed by the user. You may have the ability to load a few agents or have some enforcement of policy, but there’s only so much you can do without impeding the worker utilizing it as a personal device. So how can you secure a user with a BYOD?\n\nMake sure these basics are in place: Push the latest patches as a requirement, enforce good access control to your enterprise and services, implement multi-factor authentication, and surround your assets with the right protections such as enterprise rights management for access control of the data and enforce end-point, policy-based access control.\n\nAnother option to reduce your risk may be to limit direct access to enterprise services and have users work in a SASS environment.\n\n### End Points and Unsecure Environments<h3>\n\nThe best solution for many of these problems would be an enterprise rich client, but there is still risk involved. Workers are using a device that is a managed extension of the enterprise network. Security can be in place with a secure boot, full disk encryption, data protection, local firewalls, and enforcing good patching. The risk, however, comes because many organizations already have remote workers in place, such as sales people or field engineers, who may not have the same level of security as workers who were never meant to leave the building, such as finance and human resources.\n\nOrganizations must be vigilant about making sure all workers now have the same security, or even adding extra layers that workers need to work remotely in unsecured networks. Multi-factor authentication should now be a requirement, and end point policy enforcement and enterprise rights management are now more important than ever.\n\nWe are seeing innovations in this area, such as one customer who is giving new employees not just laptops, but also a managed router to avoid unsecured networks.\n\n### VDI Systems & Cloud Services<h3>\n\nImplementing security for both VDI systems and cloud services includes the security basics: data protections, virtualization security for both the enterprise data center and at the access points, application security, secure boots, patching, and network encryption. The key is doing an accurate survey of the enterprise and cloud services that are being deployed to your workers and making sure that they are all secured equally. Each application, even those that are not mission critical, is a potential attack point.\n\n### As-a Service Clients<h3>\n\nSASS clients use services in the cloud and also use applications on their rich client, so there are some additional security issues to worry about. There should be appropriate data protection in enterprise rights management (ERM) for the access to the data through the cloud services and back to the data center. Protection at both sides is critical. Client access to cloud services should be protected through multi-factor authentication and network encryption. The cloud service access to the enterprise data center’s private cloud and enterprise resources should also be protected at the network and data access and application layers. Understanding how clients are using the services and what data they are accessing is where the ERM decisions come into play.\n\n### Misuse and Abuse Insiders<h3>\n\nIT should use a variety of methods to manage the threat and risk of mistakes, misuse, and malicious actions of insiders. Policy-based access control and enforcement from applications to data at both the enterprise and the cloud level is important to thwart misuse and abuse of users who are already authenticated. The major defense that IT has is auditing and monitoring threat intelligence. Managing this information across the enterprise and the cloud over a long period of time can be very effective in detecting aberrant behavior.\n\nThere is no question that we have to think differently now about security issues with teleworking. Our top recommendations are first, deploy the technology you’ve been piloting; second, educate your users; and third, turn on two-factor authentication and protect your data at scale.  If we can do these three things, we can reduce the risk and be better prepared for the future.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT7-en","image":"./episodes/edt-7/en/thumbnail.png","lang":"en","summary":"With the huge shift in people now working from home instead of the office, security is a growing concern for many IT organizations. In this episode, Steve Orrin, CTO of Intel Federal, and Darren discuss the security threats and solutions to help secure your teleworker and enterprise data centers."},{"id":144,"type":"Episode","title":"Modern Data Governance","tags":["datagovernance","immuta","dataaccess","identitymanagement","cybersecurity","data","technology"],"body":"\r\n\r\nAs organizations migrate to the cloud, the way we think about data and the way we think about using data is completely changing; in the past five or six years, the whole infrastructure layer has changed. The necessary performance, scale, rules, and number of users who want to combine data have increased exponentially. To manage that at scale is not trivial, and that’s where Immuta comes in.\n\nOver time, the industry has re-thought the paradigm of warehousing data. Previously, each data team would build data products in a silo, then deliver to a business unit. The business unit would have their appropriate data that delivered a quality service to them. Now, all of a sudden, we want anyone to be able to combine data across the enterprise: business analysts, data scientists, data engineers, etc. The number of users has changed, and it’s not about search applications; it’s about transform and data pipelines.\n\nThat is a difference that requires a different quality of service, time, and level of sophistication that never existed before.  It becomes even more complex when you add in privacy laws, data classification, and the ever-changing rules and regulations.\n\nWhat Immuta is trying to do is make it possible for any user to potentially combine any dataset, internal or external to the organization to get some insight. It’s all about scale. Policy needs to be separated from platform to scale policy for all these new users and all the different combinations of data. Security, privacy, and governance are great, but if you can’t perform, no one’s happy.\n\nOne lesson Immuta learned early on is that proxy won’t work. It is like trying to move petabytes of data through a straw: they become the bottleneck. It’s easy to blame the middleware whenever there is a problem because they are always a bottleneck since they are between the tools and the data. This is a classic problem of middleware.\n\nThe second problem with that design paradigm is that with classic data virtualization, it worked because there was a clear set of data. When there are petabytes of data, however, the give-me-everything-and-we’ll-figure-it-out approach breaks down very quickly because of the sheer size of the data as well as all the rules and policies around it.\n\nIn normal virtualization, you have to natively embed into the cloud compute infrastructure in Snowflake, in Synapse, etc. Instead of putting this abstraction layer on top of these different tools, the abstraction layer is on the policy side.\n\nThe policy is only as good as the ability to audit it. It’s like a chain of evidence. The key is that you can prove that this user complied or did not comply across the computing infrastructure. With modern data governance, it is scaling policy within and it is unifying audits because of the level of complexity of many people are doing so many things with the data. The audit of the policy must be dramatically simplified or it’s impossible to determine compliance.\n\nThere are three types of policy to consider: operational, regulatory, and contractual. An example of how unwieldy all of this can become is to look at a company such as Cummins. If they modernize in the cloud, they have to consider regulations for every market. For one of their engines, what if they have to write a custom rule for each country they are in, but also for every country they are not in, because those countries shouldn’t be able to see it? That doesn’t scale when you’re talking about tens of thousands of data sources for the schemas, which are constantly changing. There are petabytes of telemetry from these engines.\n\nWhat you would want to write is that you can only see the data for the country where you reside once, and then it applies everywhere. But that’s not the way it’s done. So they would be writing something like 700 policies for a data object, when it should be only one. Keeping up with all the changing policies and regulations for each data source would be next to impossible. You would be out of compliance all the time.\n\nIt’s never been easier to globalize a company than today, and Immuta customers expect them to be able to run infrastructure globally across any cloud. They should be able to move their data to any cloud and be in full compliance.\n\nImmuta applies rules to the data through tags versus using the raw data because each domain has their own jargon about how they classify and talk about their data. So they have generically started classifying and tagging in a way to apply general concept templates, such as for HIPAA. These templates, however, are not yet foolproof, and there is a long way to go.\n\nMatt’s vision as CEO of Immuta is to get to a state where domains can share their policies. For example, in healthcare, there is a good reason that Moderna and Pfizer would want to work together. There could be an agreement on how to handle real world data controls with academia. So if there were a consistent policy that could be shared and crowdsourced in a concept policy cloud, that would be the right thing to do.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matthew Carroll"],"link":"/episode-EDT70-en","image":"./episodes/edt-70/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, discusses the reality and future of modern data governance with Matthew Carroll, CEO of Immuta. "},{"id":145,"type":"Episode","title":"Modern Data Governance","tags":["datagovernance","data","immuta","dataaccess","identitymanagement","ceo","technology","process"],"body":"\r\n\r\nIn modern data governance, the first premise is that you have to separate policy from platform.\n\nThe second is that there can be no ontology.  If anyone tries to create a super schema of everything, it’s impossible, but you need a schema to apply policy. So if a user wants to join two columns, you have to have a new policy. Sensitive data must be discovered, both through direct and indirect identifiers. Indirect identifiers are more difficult, and techniques must be applied to mitigate the risk of link attacks.\n\nThe third premise is the introduction of a series of privacy enhancing techniques such as masking, redaction, authorization, etc.\n\nThese tools are automated as part of modern data governance. A parallel example is how you used to have to be a wizard to remove red eye in a photo. Now, you click on a button and the red eye is gone. It’s the same thing in privacy. We need that easy button to automatically find a potentially indirect identifier where there’s greater than very low risk of re-identification.\n\nBesides privacy, there is a myriad of other things with data that should be automated before classification. Although Immuta does not delve into data transformation and those data flows, they provide an API and command-line interface. Engineers who are building these pipelines can do their thing, and Immuta will tell them the rules, and the rules update can be part of the pipeline. They want to be a conduit to that layer.\n\nThere are some new roles in this arena such as data steward and data governance engineer, which are segregated from the data engineer. And then data users are really three separate users with three separate needs.\n\nFirst, there are data scientists who have specialized skills and need data in a specific way. Sometimes they need a specific tools and a specific compute environment somewhere in the world to accomplish their mission.\n\nOn the other hand, data engineers and analytic engineers, one building the pipelines and one sustaining the pipelines, need quick access to an emergent when something breaks. They feed it into a pipeline and deliver it to someone and check to make sure it’s up to date.\n\nFinally, there are the governors who are trying to keep up with regulations.\n\nThese users all have very different views and needs around governance. When applying a new governance, the most important thing is to treat these groups as separate stakeholders.\n\nIf you think of all of these roles in a binary way, as data engineers, what ends up happening is a lot of meetings, so it’s impossible to scale. We need to create a symbiotic relationship among data operations, data science, data analytics, and governance. An example model is Salesforce or ServiceNow, where there is a whole workflow from start to finish and no meetings have to happen. This would be real data ops.\n\nImmuta has, philosophically, an attribute-based approach rather than a role-based approach. The problem with a role-based approach is that you get bloat as you inevitably keep adding roles. One pharmaceutical organization, for example, had over 800,000 roles because roles can never be deleted because of the need to reproduce drug trials. Role bloat can become a scale problem quickly.\n\nAttribute-based access is the key to counter this. Rather than constantly adding roles, users have specific and consistent attributes. For example, an attribute of a user could be that they are tagged so they can only see their own state. With role-based access, every state, whether they can see it or not,  would have to be written in. This modern identity management is very scalable. Attribute access simplifies the number of policies that need to be written and helps with performance.\n\nModern global regulations such as GDPR, however, also require purpose. This is where attribute access becomes important: what purposes can each person operate under? Under a EULA, data must be processed by users only for the stated reason. If not, there needs to be a risk analysis in flight before the data is operationally used for production.\n\nRight now, we are at the beginning of modern data governance. Currently, users make a one-time binary decision on the data, either consent or no consent. The future is somewhere in the middle: limited consent. For example, if a person gives their genomic data to a company such as ancestry.com, down the road, what does that mean for their child? The child did not give consent for their genetic material to be handed over to be possibly examined by, say, a health insurance company to determine risk. In modern data governance, Matt sees consumers providing limited consent, such as allowing a company to only analyze DNA for ancestry and nothing else.\n\nThe future has to be about consent and purpose-based access because ultimately, the derivative data drives insights, as developing machine learning embeds data in the algorithms.\n\nTo learn more about Immuta and building a data governance program, go to Immuta.com \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matthew Carroll"],"link":"/episode-EDT71-en","image":"./episodes/edt-71/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect at Intel, continues his in depth discussion about the reality and future of modern data governance with Matthew Carroll, the CEO of Immuta. In this episode, they discuss Data Classification, Policies and Governance."},{"id":146,"type":"Episode","title":"2021 a Year in Review","tags":["covid","edge","compute","cybersecurity"],"body":"\r\n\r\nOne major expectation for 2021 was that the new vaccine would bring us out of COVID and travel would resume. Darren expected to see his customers face to face rather than via Zoom or web conferencing.\n\nOther expectations were the implementation of hybrid working models, and although remote learning in education was in full swing, there was hope for a return to in-person classrooms. We expected to see an uptick in contactless or frictionless customer service. The gig industry such as UberEats was also kicking off.\n\nIn the industry, we were hoping to see a big push for AI and ML as new technologies become available. We also thought we would see an acceleration in industry 4.0 with new automation, as factories still needed to produce even though they had fewer workers.\n\nWe had great expectations for 2021, and it didn’t disappoint. There were, however, quite a few surprises.\n\n## Surprises in 2021\n\nAlthough the vaccine helped, it didn’t wipe out COVID. We’ve had four waves, and we’re currently in the middle of the Omicron wave at the beginning of 2022. Not everyone was headed back to work, however, and organizations continue to roll out hybrid workplace plans.\n\n2021 also brought the great resignation, with a large number of people changing jobs, perhaps due to burnout, different opportunities, or just being unsettled about when they would have to go back to the office.\n\nIT became the quiet heroes on business continuity, as they became even more flexible with the ability to turn on a dime to support all the different needs for employees and their customers.\n\nAnother big surprise was that major industries outside of high tech were hit with ransomware, such as the meatpacking industry and a pipeline. Security, then, became an important concept.\n\nIntel had a big surprise as well, with the return of Pat Gelsinger as CEO.\n\nCOVID is Still Wreaking Havoc Throughout the World  \n\nCOVID is probably going to be the pivotal or black swan event of the century. The world economies have adjusted to reflect the uncertainty of the four waves of COVID outbreaks, and the peaks and valleys have wreaked havoc with business plans for getting people back into the office.\n\n## Hybrid Work Models Created\n\nOrganizations have come up with some great hybrid work models, but many are still not utilized as the back-to-work dates have been continually pushed back due to outbreaks, such as the current Omicron wave in January 2022.  What we did see is a major culture shift to people settling into remote work and possibly hybrid work. IT organizations have prepared for the back and forth to the office.\n\nDay-to-day work has fundamentally shifted from people storing their work on a machine in the office to storing it in the cloud. Or, things are stored on a portable device and replicated in the cloud. So, we saw a huge shift in cloud-based and SaaS-based offerings like Office 365, as well as in collaboration tools such as Zoom and Teams.  In addition, remote onboarding of new employees has become commonplace. All of this has affected people, process, and technology.\n\n## Combatting the Great Resignation\n\nTo retain employees amidst the great resignation, organizations much first look at the well-being of employees. With the stress and isolation of the pandemic, there has been an uptick of organizations seeking to help employees with mental and emotional health.\n\nFlexibility is another key; with remote work, hours and location can be  flexible. Employees can live in another state or keep non-traditional hours. This flexibility and lack of commute have allowed people to engage with and explore their own communities more.\n\nHR organizations are starting to use artificial intelligence to find out why people are leaving, identify trends, and determine which programs will help boost productivity and help employees to feel like part of the team. Organizations are also turning to automation. There has been an uptick in the robotic process automation industry to deal with fewer employees.\n\n## Relentless IT Pace has not Slowed Down\n\nIT cannot keep up the heroic pace that was required at the beginning of COVID, so although the crazy hours have been tapering, the demand is being met strategically. We saw a massive movement to SaaS offerings so employees were, for example, no longer managing things that weren’t necessarily their strengths. Instead, they could turn to a cloud service provider, or service providers of HR, sales, or ERP systems. A movement to automation and repeatable processes alleviated some of the pressure of the day-to-day office work. There was also greater investment in DevOps and RPA technology to help streamline and secure product development.\n\n## Security, Supply Chain, and Ransomware\n\nSecurity issues caught most industries off guard. Industries such as meatpacking and oil and gas, hospitals, and supply chain and logistics companies were hit with security breaches. Primarily, this was a problem of having to move so quickly to remote work and setting aside careful security. For example, perhaps organizations didn’t do the proper training of remote workers on how to secure their laptops or data. Among other lessons learned, we will see the emergence of zero trust this year.\n\nAnother big problem of this year was supply chain issues, and not just in the silicon supply. Many materials will continue to be in short supply across the board, especially affecting small businesses.\n\nWe have to come up with solutions to combat ransomware. Organizations can make process and cultural changes and utilize new technologies to find these solutions.\n\n## Intel Got a New CEO\n\nThe biggest surprise for Darren, and perhaps for the whole industry, was the return of Pat Gelsinger to Intel, this time as CEO. Everyone at Intel is inspired by the energy he brings. Pat says that he is going to help the industry solve the chip shortage by investing in American manufacturing again.  Darren believes he will move Intel back to the top of manufacturing of chip sales and provide wonderful technology for the whole world.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT73-en","image":"./episodes/edt-73/en/thumbnail.png","lang":"en","summary":"On this episode, Darren reflects on the expectations and surprises of 2021"},{"id":147,"type":"Episode","title":"Ransomware: Prepare and Don't Panic","tags":["cybersecurity","ransomware"],"body":"\r\n\r\nAlthough a ransomware attack begins for most organizations with a ransom request of some sort, the full story has typically been brewing for many months.\n\nThe ransom demand happens after the attackers have encrypted information from compromised systems and locked down whatever they could, or they have stolen sensitive information and now threaten to release it publicly. In either of those cases, they request a sum of money to stop the release or to reinstate the information.\n\nThe amount of money can be targeted because the attackers know how much the company can afford, and they will ask for a massive amount. Less targeted attacks will ask for a random amount and hope they get a hit. More and more, however, the attacks are sophisticated, and the attackers have done their homework. They may have gained significant traction, moving laterally through the environment and compromised multiple domains. They know it will cost the organization a tremendous amount to reinstate their services, so they can ask for more.\n\nBut before the attackers demand the ransom, they have been inside the system for some time, on average just under 300 days, until they fully execute their plan. They try to get as far as they can within the network to make it as impactful as possible. For example, they will blow away the backup exec so that they cripple the organization, leaving corrupted, unusable backup data. They will also spend time compromising credentials for privilege escalation to move laterally throughout the network and ferret out any vulnerability.\n\nEach attack is different, so it’s difficult to name the most common vulnerability. The attackers find any inroad they can, whether it’s a weakness in vulnerability management or even in people, for example, phishing attacks or social engineering tactics. They will make phone calls and impersonate executives, or use executive- type names to glean information and data from employees online.\n\nMany in the industry will say it’s not a matter of if, but when, you will be attacked. Stephanie, however, does not believe this is the case because with due diligence, locking down appropriately, and taking the necessary steps, you won’t be targeted.\n\nDue diligence is what security experts have been preaching for decades: do vulnerability management and patch management, change passwords, and lease privilege if possible. Train your people. Admins should not be on the internet with the administrative accounts posting on forums, for example. It’s all about understanding what the attack surfaces are and managing them.\n\nWhat should you do if you are the victim of a ransomware attack?  Stephanie says that what people should do and what they actually do are two different things. The first knee-jerk reaction of IT ops is often to reboot or patch or change the environment in significant ways. This will only alert the attackers that their time is up. For forensic investigation, it’s extremely important to go easy on those environments. Make as few keystrokes as possible, and definitely don’t reboot.\n\nSome companies will panic and perform denial of services on themselves, going into full lockdown mode, shutting everything down. Rather than panicking, they should depend on their incident response policies and realize that the issue can be worked out even though it’s not going to be pleasant.\n\nObviously, every company should have these incident response policies that can be enacted quickly to manage communications internally and with the media and keep business going if possible.  Security professionals can help set up these response plans and can come in and help during an attack.\n\nTheir first step in preserving the information is to observe some of the questionable activity that is going on in the network. The ransomware attacks don’t begin with encrypting or stealing information; there are many attacks beforehand. It’s important to identify where they are coming from, where they originated, and where they have been. To do that, security professionals need evidence and information and it needs to be properly preserved. A good start is getting the right people in the right place to manage what’s going on.\n\nNext is to properly manage the environment. Unfortunately, once there is ransomware or any kind of breach or incident, the organization is highly vulnerable. One hundred percent of the time, when environments have undergone successful ransomware attacks, publicized or not, they are targeted by the same group or a different group. It’s like a wounded animal with the vultures circling. Attackers know you are wounded and vulnerable. There is another attack coming.\n\nMost often, when security professionals are doing their investigation, they find other indicators of attack and compromise in different parts of the network. They must determine whether it is part of the same attack or a different attack. This investigation is a critical part of recovery from malware because even when you think you have an attack cleaned up and the business is now running again properly, there is still the potential for these other attacks.\n\nA typical attack costs an average of four and a half million dollars to clean up, and that doesn’t include the ransom. The amount can be much larger and is proportional to the size of the organization.\n\nIt is impossible to be certain how many organizations pay the ransom. Many of the ransoms come with threats not to contact law enforcement or disclose the attack. For this reason, the available statistics about how many organizations pay are varied.\n\nSome organizations’ decision-makers respond by saying that they will not pay under any circumstance, even if it costs more to rebuild, hence destroying the ability to negotiate. This is an emotional decision that can cloud judgment. At the end of the day, if the objective is to continue to do business and make money, paying a low ransom of maybe ten or twenty thousand dollars is going to be cheaper than the forensics and the rest of the process. On the other hand, there are documented cases where the ransom was paid, and the data was not fully restored. In addition, the organization has no surety in the security of its environment. There is no guarantee when you pay a ransom; you are asking criminals to act in good faith.\n\nMany governments around the world have made it illegal to pay a ransom because the attackers are considered terrorists, and you aren’t allowed to negotiate with terrorists. Another wrinkle is that the attackers will sometimes refuse to deal with professional negotiators. They will often name someone at the organization as the only person with whom they will negotiate, hoping that person will make emotional decisions.\n\nThe threats are communicated in a variety of ways: email, phone, and even on a desktop background.\n\nThe best strategy to avoid or mitigate an attack is to not wait until your organization is in that situation. Rather, engage in due diligence. Do assessments at least annually to find the gaps in security. The threats and attacks are constantly changing and becoming more sophisticated, so your organization’s security must keep up. Continually monitor and patch. Do vulnerability management, continually change passwords and remind and educate users of the threats. These are not new strategies. Security professionals have been recommending these steps for decades; organizations are just failing to do them properly and evolve.\n\nIt’s also smart to call in the experts to guide the process of incident response plans and exercises. Everyone at the organization should know what to do and who to call in an attack scenario to avoid further damage.\n\nIf your organization is attacked, hopefully the impact will be minimal, or at least contained and manageable if there has been preparation. Whatever time, energy, and money a company invests in preventative measures is a small fraction of the cost of an attack. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Stephanie Sabatini"],"link":"/episode-EDT74-en","image":"./episodes/edt-74/en/thumbnail.jpeg","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, talks to Stephanie Sabatini, Sr. Director, Professional Services, Hitachi Systems Security, about preventing and preparing for ransomware attacks and what to do if your organization is attacked."},{"id":148,"type":"Episode","title":"Cybersecurity Solutions with Hitachi","tags":["cybersecurity","technology"],"body":"\r\n\r\nColin has been with Hitachi for almost 24 years after working for IBM right out of college. He is primarily focused on accounts in the Pacific Northwest and SLED accounts. He had the unusual opportunity of taking his father’s post when his father retired from Hitachi.\n\nDavid has been in IT for over twenty years, starting just before the .com craze. He’s been a hands-on guy for most of his career, dealing with servers, storage, and security.\n\nMany people think of Hitachi as anything from a large equipment or power tool company to one that produces bread makers. Hitachi is, in fact, a one-hundred-year- old engineering and manufacturing company that started making electric motors in 1910 and has morphed into hundreds of operating organizations and wholly-owned subsidiaries, including Hitachi Vantara. Hitachi Vantara is the largest wholly-owned subsidiary, focused primarily on managing data. They build and deliver not just traditional block enterprise storage systems, but they have grown into a digital solutions company embracing the extreme expansion into IoT and data-driven insights.\n\nRobert Mueller said, “There are only two types of companies: those that have been hacked and those that will be. Even that is merging into one category: those that have been hacked and will be again.” This quote sets the stage for how Hitachi can help companies. It’s not a matter of if, but when, you will be hacked, and when you will be hacked again.  This applies not just to companies, but organizations in the public space as well.\n\nHitachi has a whole federal division that works out of Washington D.C., primarily focused on the Department of Defense and federal accounts. Hitachi Vantara does a lot of work with SLED accounts and works together with the federal division to provide solutions for federal, state, and local requirements.\n\nThe best way to visualize a security solution from Hitachi is to imagine your organization as your home, and in that home are four elements of protection that we must deploy.\n\nFirst is physical security.  Hitachi has a smart system, a visual intelligence solution, that can provide video surveillance with intelligence. It can do everything from gunshot detection to license tracking to facial recognition. Hitachi deploys these all across the world. Hitachi cameras picked up on a lot of the events at the Washington Mall last year, and they are used by police organizations all over the country.  This has all grown from Hitachi’s transportation division, which is highly dependent on video technology for security.\n\nPhysical security is not often a topic of discussion, but it is a key aspect. If you can’t control your own building and your own data center, then you do not have security. If someone can get through the door, they can get into your data center.\n\nAfter physical security comes Hitachi ID. This is basically the lock on your front door.  A recent survey showed that only 40 percent of companies have privileged access management and about 74 percent have identity management. This is not enough because that is a lot of exposure, and the SLED space is a high-profile target. Hitachi ID provides password privilege and identity solutions across a single platform; this is a total solution where you can identify not only users but devices and applications, on prem and in the cloud.\n\nThe third aspect is how to protect your data when someone does get through the door.\n\nCybersecurity is a big data problem, and Pentaho is the tool for big data. With Pentaho, you can manage all your sources of data, control your data lake, and produce useful outcomes. Hitachi found that data scientists and cybersecurity analysts spend 80 percent of their time just managing, sourcing, and cleaning the data, and only 20 percent of their time actually analyzing it. With Pentaho, those numbers are flipped, and they can spend 80 percent of their time analyzing the data and getting into real-time threat analysis and threat response.\n\nPentaho is an intrusion detection system, but it is a prevention system as well. For example, one major energy company had done everything ad hoc, writing their own scripts in-house. They re-platformed all of that functionality in Pentaho in one day. Then they were able to do that analysis in real time and keep up with the attackers instead of always coming from behind. What makes Pentaho unique in a crowded field is its flexibility. It can do any kind of data analysis, and it can be based on what your organization needs.\n\nProtecting data from a content perspective is also very important for the SLED space. Hitachi Content Platform Anywhere provides secure file sync and share and a secure dropbox. It’s comparable to Google Drive or Dropbox, for example, but the difference is that HCP Anywhere is controlled by your own security team.\n\nIt is used by the Department of Defense, which renamed it Mill Drive. Field forces on the ground transmit sensitive data to and from various secret places and back to headquarters for reconnaissance and other missions. On a local level, cities utilize it, transmitting video evidence not only through administration, the police force, and other agencies, but also to the DAs office for evidence management.\n\nHCP Anywhere is built on the Hitachi Content Platform, which is their object storage. It has built-in object storage, not just Hitachi’s, but all object storage. Instead of overwriting a file, and therefore allowing an attacker to overwrite your file, you make a new version of the file. Then, if you get attacked, you can roll back to an earlier version before the attack happened. Some rating agencies agree that Hitachi’s object storage is the best on the market. Out of the box, you get 16 versions of every file, 16 pieces of system metadata, and an unlimited amount of custom metadata tags as well.\n\nHitachi also has a console for data policy management in addition to the Hitachi Content Platform Gateway, which puts a NAS in front of the object store that can be either NFS or CIFS. Added on is Hitachi Content Intelligence; it’s a search engine on steroids. You can find any object in your store based on metadata, date, time, etc.  It can extend beyond your own (Or Hitachi’s?) physical hardware across multiple objects stores. The search will work on anything. It is also 100 percent compatible with AWS s3.\n\nFor SLED customers, there’s an opportunity with Hitachi’s partner Flexential to provide Hitachi Content Platform as a service. If a city government, for example, does not want to apply their own object store, they can be in a multi-tenant environment through this partnership.\n\nFinally, Hitachi System Security can help you if you get attacked with ransomware. They can provide analysis on the impact and negotiate with the ransomware attackers. They can start building defensive postures around your organization. The attackers are not solo players, but criminal organizations with developers and management, so it requires a professional response. Hitachi System Security can take an organization from start to finish.\n\nIf you are a member of ISSA, look for Hitachi to sponsor an event in your area.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Colin McLean","David Rowley"],"link":"/episode-EDT75-en","image":"./episodes/edt-75/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, talks to Hitachi’s Colin McLean, Intel Global Team Lead, and David Rowley, Senior Solutions Consultant, about Hitachi’s full circle cyber security offerings."},{"id":149,"type":"Episode","title":"Day in the Life of a Cloud Solution Architect","tags":["csa","cloudsolutionarchitect","multicloud"],"body":"\r\n\r\nThe Intel Cloud Solution Architect team formed less than two years ago with a few people and has grown rapidly to 21 team members. That growth indicates its importance both internally to Intel and externally to customers.\n\nAs customers move from on-prem to the cloud for flexibility and scalability, they need to understand that the underlying infrastructure and features matter for optimization. Since the majority of cloud service providers, including the top three, Google, Amazon, and AWS, are powered by Intel, Intel’s CSAs can help optimize services through a customer’s entire cloud journey.\n\nA typical day for a CSA begins with back-office tasks such as emails, internal meetings, team meetings, staff meetings, and a significant amount of time talking to end-user customers. These can be Intel customers who are currently using Intel CPUs on-prem, and now they are looking at going to a cloud, or maybe they are already in the cloud. CSAs also have partners, one-system integrators, who are interested in how to translate Intel’s stickiness and in some of the features such as ABS firewall or Boost or encryption.\n\nCSAs do not only work in the pre-sales role, but they also design proof of concepts, write use cases, and work with a team to run benchmarks based on a customer’s workload. They also spend time authoring or co-authoring playbooks.\n\nThey also create a playbook to train Intel’s own managers and partner salespeople.\n\nIn addition, CSPs are announcing new services every day, and CSAs must stay on top of the technology supporting those services and understand how the services can benefit customers. This means understanding the users’ workloads and using models as well.\n\nIntel CSAs see themselves as trusted advisors, an extension of the customer’s team, rather than trying to take over decision-making. Many times, for example, a customer will know which cloud they want to use but will need help looking at which tools are available to assist them in analyzing their current workload and then correlating them to what instances are available in the cloud. CSAs will also assist and educate customers with cost analysis. Intel has a portfolio of tools for every state in the cloud journey.\n\nOnce a customer is up and running and finished with the initial project, they will           reach out to CSAs for new initiatives and projects as the cycle continues.\n\nCSAs have a unique set of skills, in that they spend a lot of time writing, communicating, and educating, but also understanding all of the technical aspects and customer needs. For example, a CSA should be able to discern whether an issue is a technical problem or a business problem. The backgrounds of Intel’s CSAs are diverse with different cultures, technical skills, sales experience, and work backgrounds, so they bring expertise in different areas. They are also a deep technical source beyond their own people. If, for example, someone wants to talk in detail about AWS services, they can bring in a peer from AWS. In other words, Intel’s CSAs can be a single point of contact for all the customer’s cloud services needs.\n\nWhy should a customer utilize the Intel CSA team rather than a CSP’s team? Intel is truly agnostic, as their chips and CPUs are running almost every cloud service provider. It doesn’t make any difference to the CSA which service a customer chooses. In addition, a lot of the providers’ CSAs don’t understand the underlying Intel features which are available only in a certain instance type. For example, an Intel CSA would know to choose an instance powered by Ice Lake rather than Cascade Lake to provide significant cost savings and an uptick in performance on their applications. The CSP CSAs would not necessarily be aware of this information.\n\nIntel also has many tools to collect telemetry, whether it’s a cloud instance or bare metal cloud instance. The CSP CSAs do not have access to those tools. They can troubleshoot, but only at the hypervisor level. So if a customer is having a problem, an Intel CSA can go down to the chip level and use troubleshooting tools and telemetry to solve the issue.\n\nThe best way to get in touch with the Intel CSA team is through an account executive. Technically, the CSAs are in the sales and marketing group, so they are actively searching for opportunities as well, such as contacts from the past.\n\nCSA services are not an extra expense. In fact, Intel has a program where they will fund the initial migration to the cloud. CSAs will bring the tools, people, and expertise both from a human resource standpoint and from a monitoring standpoint as well. This will help a customer in a greenfield environment shorten the learning curve. Then, monetization comes when a workload is fully running in the cloud or migration is happening where they are consuming resources.\n\nIntel CSAs have no vested interest in which CSP customers use, but only that they are optimized in a highly secure and reliable environment. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Kiran Agrahara"],"link":"/episode-EDT76-en","image":"./episodes/edt-76/en/thumbnail.png","lang":"en","summary":"On this episode, Darren talks with Kiran Agrahara about what Intel Cloud Solution Architects (CSAs) do in a day to benefit not only cloud service providers (CSPs), but end users."},{"id":150,"type":"Episode","title":"Digital Transformation in 2022","tags":["aiml","comms","data","edge","multicloud","cybersecurity"],"body":"\r\n\r\nAs the Chief Data Scientist for Intel’s public sector, Gretchen spends her days talking to customers about their data challenges, data management, data governance, the ethics around what they are doing, and responsible AI.\n\nAnna’s six years at Intel have been focused on IoT and the edge, with the last three years in the public sector. Both agree that 2022 is going to be an exciting year filled with game-changing possibilities.\n\nDarren, Gretchen, and Anna each represent different parts of digital transformation with customers but came together to provide a common way to talk about the topic, with six pillars of digital transformation: multi-cloud computing, edge computing, artificial intelligence, machine learning, cyber security, data management, and comms.  They agree that these are going to be the areas of big transformations in the coming year. Intel is deeply involved with all of them, as they have built the hardware and software to support each of the pillars. The pillars are interwoven, and Intel plays a key role in leading the industry forward.\n\nIntel’s hardware is the underpinning for the majority of the cloud providers. From a software perspective, Intel has optimized the common frameworks that people use for AI or ML or deep learning to be able to best take advantage of the hardware underneath. In some cases, performance is ten to 100 times better based on Intel’s software.\n\nNot only has Intel provided hardware to edge and comms, especially in 5G, but has augmented those with software reference architectures. In addition, Intel tries to make the ecosystems work for everybody. There is a strong focus on open systems in a non-proprietary way, making it easier for new entrants and existing players to have an expanded footprint in these new markets and offer exciting advances.  Intel is one of, if not the number one, contributor to the open-source software community.\n\n## Cyber Security Pillar\n\nImplicit in Intel’s hardware designs are security features and capabilities to make sure customers in the ecosystem can protect their data in all of its different permutations.\n\nSecurity is never static; it’s always evolving. Intel alone is not going to solve an organization’s security problems. Security is an interplay with what you do with your hardware, how you bring in the right software elements and the boundaries and policies of your organization.\n\nIntel focuses on multiple areas but a foundational role is in hardware root of trust and authentication. Many features can be built in directly to accomplish that. A step further is trusted or transparent supply chains that Intel can share with customers that instill a high degree of confidence. Those capabilities are getting better all the time and Intel is always working to make progress.\n\nCyber attacks are top of mind for many customers because of recent breaches. Intel has security such as trusted execution and secure enclaves. They have a whole set of capabilities for the type of security you want to do such as encryption, without taking a big performance hit. There is a whole matrix of capabilities, including a different set of requirements on the edge because of the loss of the physical security of the data center.\n\nArchitectures with zero trust are becoming the frameworks that are especially prevalent in the public sector. For the Department of Defense, zero trust is a mandate. Intel has many capabilities that feed into zero trust.\n\nOne size does not fit all with security, but it is clear that bolt-on security is a thing of the past. Security must be built in from the beginning and must be continually iterated. Organizations must constantly ask whether they have the right protocols set up, whether they have the right threat detection tools, and whether there is trust all along the supply chain. All of this is integral.\n\n## Edge Computing Pillar\n\nNow that there is so much that can be done with AI, ML, and different algorithms, it’s exciting to see how we can exploit those things at the edge and optimize the architectures to deliver around those use cases.\n\nThere are some really simple models where everything lives in the cloud, and just the data gathering is done at the edge. If the connectivity allows it, then the latencies can match the applications. The bulk of the compute can be cloud-centric. There are, however, countless use cases where this makes no sense because of the sensitivity of the data or latency requirements. So, there are interesting conversations about how to figure out optimal architectures for the edge and what’s happening with the cloud and network.\n\nFor example, could we have a seamless architecture of gathering, using, and processing data immediately to provide intelligence? Can we incorporate that into the next round of trainings so the model will be continuously updated? How fast can we make that loop? Is it viable? Do we need all of our training on the cloud?  If all the training is on the cloud, what is the right interval for dropping those updated models back down?  And can the edge be lightweight enough to use the stuff generated on the cloud? The edge is still very complicated, and there are many possibilities and fascinating questions.\n\n## Artificial Intelligence Pillar\n\nAI and ML enable the edge to do so much more than anyone previously considered. One product is not always the right answer: It’s about fit for purpose. The use of open source is critical and being able to take advantage of microservices to run algorithms at the edge.\n\nFor example, if you have algorithms right at the edge doing the read work, you’re talking about traffic flow. The cadence of red, yellow, and green lights can change on the fly based on how many cars are going through, and at the same time collect data to pass back to a larger data center that could then do retraining. By the end of the week, maybe it makes sense to add some additional microservices or algorithm tweaking. Then that container goes back down to the edge and you can respond better and continue to learn.\n\nAlso, when talking about fit for purpose, bandwidth, latency, and form factor are all considerations.\n\nCrops yields are a good example. A customer collects data that goes to a data center. They are working on the models, but it doesn’t translate into letting them know that they need more fertilizer or that there are current challenges with sun and rain. That means the formula needs to be changed. You need the crop yield data, the information, the algorithm, and the microservices in a much smaller form factor. The latency and bandwidth are different, yet you can have a small unit in the middle of a field collecting that data and responding back to, for example, the flow of water or fertilizer needs to improve crop yields.\n\nHopefully, this year, more of the edge designs will become standardized. With FlexRAM and 5G, there are standards, but everything else is the wild West. Many people are designing interesting things, but they are not designing in a way that makes it easy to have those microservices in a container and ML and AI kinds of algorithms. In some cases, you need multiple algorithms weighted in a different way, changing every week based on new data and the new training. We need to be able to do that in a way that it doesn’t matter who built a device. Creating standards not only in AI and ML data but at the edge will help explode capabilities.\n\n## Comms Pillar\n\nThe commercial side of 5G will lead the way with 5G on phones everywhere. There is still a lag time, however, for having the type of user equipment to do different types of applications for edge or enterprise, for example. Intel is standing up their first 5G networks with partners that are more cutting edge and less on the commercial side. Although their commercial partners have had these going for a long time, private networks that are controlled for a specific purpose are being liberated and applications are being built out. 2022 is the year when these things will become actualized.\n\n## Data Management Pillar\n\nData that used to take hours or days to ingest, prepare, analyze, and act upon can now take minutes or nanoseconds. You can also leverage different models, and when weights change, you can quickly act and consume the data and provide better services. Moving data, managing that data, and operationalizing your AI and ML are part of what data management brings to the world.\n\n## Multi-Cloud Pillar\n\nThe multi-cloud pillar does not mean cloud services providers in this context. It means infrastructure in general and how to abstract that infrastructure away to deploy new capabilities across the edge, across a cloud service provider, or even across your own data center infrastructure. The goal with the multi-cloud architecture is that you know the key users and more importantly, how the data is managed.\n\nDifferent clouds have different capabilities, and depending on use cases, may use different clouds for different purposes. Intel has cloud solution architects that help customers optimize workloads among the cloud offerings.\n\nAll of these pillars are intertwined and work together. Look for future episodes where Darren, Gretchen, and Anna continue this conversation. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Gretchen Stewart","Anna Scott"],"link":"/episode-EDT77-en","image":"./episodes/edt-77/en/thumbnail.png","lang":"en","summary":"Intel’s Darren Pulsipher, Chief Solution Architect, Dr. Anna Scott, Chief Edge Architect, and Gretchen Stewart, Chief Data Scientist discuss the six pillars of digital transformation in 2022: multi-cloud computing, edge computing, artificial intelligence, machine learning, cyber security, data management, and comms. "},{"id":151,"type":"Episode","title":"Understanding the Shared Responsibility Security Model","tags":["multicloud","compute","cybersecurity","sharedresponsibility","cloudsecurity","cloud","technology","process"],"body":"\r\n\r\n## Security in the Cloud\n\nMost people understand the responsibility of security on-prem, but the responsibility becomes murkier in the cloud. If data is protected in the cloud, how is it protected? Who is responsible for that security? What about patching machines?\n\nRecently, cloud service providers have begun offering even more services, so there are multiple models. Sometimes security ends up lost in the middle.\n\n## Security Areas\n\nThere are four major security areas we need to understand.\n\n### Physical\n\nPhysical security is the easy piece to recognize. Cloud service providers are responsible for the physical security of their data centers, and you are responsible for the physical security of your own data centers. In addition, you must secure your physical space. If you are a manufacturer, for example, you must secure the machines inside your area. A recent hack came through the HVAC system that was plugged into an organization’s network.\n\n### Infrastructure\n\nInfrastructure security is not the physical aspect, but the hardware itself. Do your network switches have the right security patches and upgrades? Are the drive and storage devices being upgraded? Are they being protected? The infrastructure can fall into gray areas with cloud services providers, so you must know who is responsible for what and in what circumstances.\n\n### Application\n\nFor application security, you must know who has access to an application and whether it is being updated with the right security patches.\n\n### Data\n\nProtecting your data is one of the most important things you must do. Data can actually be used as a weapon in a ransomware attack where attackers take it or encrypt it. It’s also being used for powerful competitive advantages in different organizations. You must know where your data is and how to protect it.\n\n## Cloud Service Delivery Models\n\nThere are three basic cloud delivery models. Different models are created all the time, but the three big ones are Infrastructure as a Service (IAAS), Platform as a Service (PAAS), and Software as a Service (SAAS). We will categorize everything else as X as a Service (XAAS).\n\n### Infrastructure as a Service\n\nIAAS is when you are leasing from a cloud service provider. This comes in terms of virtual machines and virtual networks, so storage, compute, and network. We are also starting to see cool things with accelerators, such as GPUs, or even neuromorphic processors. IAAS is where you run your applications.\n\n### Platform as a Service\n\nThe next layer up the stack is PAAS. This is where you get a certain framework such as Kubernetes to run things. VMware running on top of IAAS is PAAS. CICD pipelines as a service have many tools that fit into this space. PAAS allows you to build and deploy new services on top of this platform so you can easily deploy and manage large systems that are built on top of IAAS.\n\n### Software as a Service\n\nNext is SAAS. This is specific software that is managed by the software provider or the cloud service provider, or it could be a third party offering SAAS for someone else. The key with SAAS is that they are responsible for application security. They manage the uptime and all the managerial areas such as reliability, security, and integrity. Many of the SAAS programs are built on top of PAAS platforms.\n\n### X as a Service\n\nXAAS can be any new service such as artificial intelligence, device management, or security detection.\n\nUnderstanding these different delivery models is important because shared security models from cloud service providers are based on the delivery model.\n\n## Cyber Domains\n\nEach of the six pillars of cyber security, as identified by Steve Warren, CTO at Intel on an earlier podcast is important whether you are in the cloud, on-prem, or on the edge. The six pillars are threat detection, intelligence, analytics, and orchestration; identity and access management; data and application security; network security; supply chain security; and host and system security. All six of these domains fit into the shared responsibility security model that cloud service providers are espousing.\n\n## Shared Responsibility Security Matrix\n\nThis shared responsibility is illustrated in the matrix. The service model delivery techniques are on the vertical axis: SAAS, PAAS, IAAS, and on-prem. If you are hosting yourself, everything on the far right is your own responsibility.\n\nOn the IAAS side, you are fully responsible for data and application security and half responsible for infrastructure because you are still responsible for network control and operating system. The cloud service provider is responsible for the physical network and host.\n\nOn the PAAS layer, you are still responsible for data security and half responsible for application security and identity and directory infrastructure. There are some tools available to help in these areas. Although you are responsible for applications and their platforms, they are responsible for the frameworks and middleware that they are providing. Although most of the operating system will be taken care of by the PAAS layer and they give you some higher-level tools, you are still responsible for configuring the network controls.\n\nUp the stack on SAAS, even if you are using storage as a service, data as a service, or CRM such as Salesforce, you are still responsible for your data because you still need to design and encrypt your backups and manage accounts and identities.\n\nOne key point is across the models is that you are responsible for your data security; there is never a scenario where you leave all your security to the cloud service providers. You must back it up and ask if you are using object storage so you can roll back from a ransomware attack, if you are correctly maintaining your access management, and if you are using tools that make this easy to do.\n\n## Different Approaches to Security\n\nEach of the top three cloud service providers takes a different approach to security, specifically around network configuration.\n\n### AWS\n\nAWS focuses on prevention. When you spin up a VM, the default is to have no ports open, so you must create security groups. AWS is the most restrictive, using IAM for identity management. AWS is great for mid-sized teams, but it doesn’t do as well for very large organizations.\n\n### Azure\n\nAzure focuses more on ease of use; security is less restrictive. They use the concept of virtual networks for security, so all VMs on the same virtual network can talk to each other on that network. This is the opposite of zero trust, so you have to decide what is more important to you. Azure uses Active Directory, so if you already have a mature, substantial Active Directory, then that is a good way to go for identity management.\n\n### Google Cloud Platform\n\nGoogle Cloud Platform also focuses on ease of use, but they hedge their bets on VMs and network security. You can have profiles that lock down everything on a VM or you can have a profile that opens them up a bit more. They are in the middle of the road as far as restrictiveness. Although not as robust as AWS or Active Directory, GCP has good identity management.\n\nAll three of these cloud service providers office IAAS, PAAS, SAAS, Container as a Service, and a variety of XAAS. You must evaluate the security model and understand the differences in each.\n\nIn some respects, understanding the shared responsibility security model is more difficult than just running things on-prem because now there are more players involved and complexity increases. The key is in understanding the models and using available tools to help you manage security across multiple clouds. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT78-en","image":"./episodes/edt-78/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, explains shared responsibility security models. Who is responsible for security can become murky in the cloud; responsibility depends on cloud service delivery models and other factors.  "},{"id":152,"type":"Episode","title":"Onboarding Remote Workers","tags":["covid","remoteworker","mobilementor","genz","people","process","compute","cloud"],"body":"\r\n\r\nAfter an international career at Nokia, Denis founded Mobile Mentor 17 years ago. The company stemmed from his experience that people were not utilizing the technology in their smartphones. Although smartphones had advanced technology, people used them only for basic functions such as phone calls and text messages. A seminal moment came when Denis was trying to sell a network solution to a CEO, and the CEO asked why his company should buy any more infrastructure when the customers were not utilizing what the company had already purchased.\n\nDenis asked himself why the technology was so far ahead of what consumers actually wanted to do with their devices. He left Nokia and started Mobile Mentor to solve that problem. He hired an army of tech-savvy people who would sit with business customers and get help them get their smartphones working and allow them to be productive, subsequently learning a lot about what drives technology adoption and habits.\n\nToday, remote workers may have two or three devices, plus personal devices. They work from home, the office, and they travel. Most of the work Mobile Mentor does currently is getting those devices to work and, most importantly, making sure they are secure.\n\nSince the number of remote workers skyrocketed with COVID, and many organizations did not implement or prioritize proper security, there was, and continues to be, an avalanche of hacks and ransomware. Ransomware attacks are up 500% since the start of the pandemic, tragically targeting schools, hospitals, and municipal organizations.\n\nIn addition, with the chip shortage, many corporations could not get enough devices for remote workers and had to depend on BYOD (Bring Your Own Device). The use of domestic internet sources also ratcheted up the risk profile.\n\nOne thing that organizations can do to reduce risk is to get rid of using passwords. Passwords were a fantastic idea in 1961, but in 2021, data showed they are the primary reason organizations were getting hacked. Most attacks start with a compromised password from a phishing operation.\n\nKnowledge workers today have a ridiculous number of passwords. Yet only 31% of people use a password management tool. Another 31% write their work passwords in a personal journal, and 24% write their passwords on a Notes application on their smartphones. Alarmingly, according to a BBC study last year, 15% of the British population used their pet’s name as a password and 6% use the word “password.”\n\nThe first step in getting away from passwords is embracing biometrics. Now, an iPhone or a Windows Surface machine scans your face and logs you into the operating system and all of the single sign-on applications and third-party applications where you have that federated identity. That’s a great start. The future of reducing password use will be a combination of biometrics and two-factor authentication everywhere. In the interim, while there are still legacy identity infrastructure and legacy applications where biometrics won’t work, a password management tool makes sense.\n\nMobile Mentor surveyed the industry by generation and found that Generation Z has the most passwords. Many of these people in their early twenties joined the workforce and onboarded remotely during the pandemic. In many cases, they have never met their employer and have not experienced the social connections that happen in a work environment. They’ve got a unique lens in the way they assess their employer.\n\nMobile Mentor’s research shows that people have a preference to work at home, but across all industries, they believe they are more productive in an office setting. This is an interesting dilemma and dichotomy for the employer to try and get these people in the office. The research shows that 67% of Gen Z thinks other companies are doing a better job at providing technology for their employees. So, if an employer pushes for them to come into the office, they may choose a different job. Changing a job nowadays doesn’t mean changing your commute or anything else beyond using a different laptop. This is part of what is happening with the great resignation.\n\nWith this dynamic, the technology experience matters. Research shows that it takes on average three days to get a laptop fully configured for work, compared to two days for an office worker. A remote workers need to raise three service desk tickets on average to get their device up and running, so their experience can be painful. They don’t like the stigma of asking for help.\n\nThe best way to fix this problem is to simplify the process. Zero-touch provisioning is ideal, which is the process of getting the technology configured so a company can drop ship devices to a remote employee, and when they sign in with work credentials, the devices auto configure. Everything is working in less than an hour, and no one in IT has had to manually configure the devices, repackage, and ship them to the employee. There is a lot of work upfront to make this happen, but Mobile Mentor can help clients with this process.\n\nDenis believes that CIOs will learn important concepts by studying Gen Z and remote workers. Gen Z workers bring different attitudes, in particular around security and privacy. They value and prioritize personal privacy over security, almost four to one. For a generation that has grown up with social media, this is hard to understand, but the data is clear. They are hyper-aware of their employer’s privacy policy but almost blind to corporate security initiatives.\n\nDenis’ advice to CIOs for this issue is to position privacy and security as two sides of the same coin. Gen Z can be brought onboard to security if it’s couched in protecting their own, as well as the company’s, data and by extension that of their clients.\n\nMobile Mentor’s research shows that shadow IT is being driven and accelerated by remote workers. Remote workers who live far away from headquarters might be participating in an IT team they’ve never met and are finding applications and storage mechanisms and ways of communicating and collaborating that their companies don’t know about. The lines between personal and work are also blurring. People use personal devices for work, and almost half let their family members play with their work devices. The same number find their company security policies too restrictive and a third say they have found a way to work around the policies.  Two thirds say they find that they are more efficient when they use consumer-grade apps such as Gmail and Dropbox.\n\nDenis advises CIOs to engage remote workers in future product decisions because they are the ones who will pressure test the collaboration tools, storage tools, applications, and authentication process faster than anyone who is based in an office.\n\nFor more information about Mobile Mentor, go to mobile-mentor.com.  There is a separate website, endpointecosystem.com, where they share all of their research for free to educate and inform companies about what’s going on with remote workers to help avoid the next wave of cyber attacks and to improve the onboarded employee technology experience. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Denis O&#39;Shea"],"link":"/episode-EDT79-en","image":"./episodes/edt-79/en/thumbnail.jpg","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, Denis O’Shea, founder of Mobile Mentor about his experience, research, and advice in onboarding remote workers, especially Gen Z workers."},{"id":153,"type":"Episode","title":"Securing the Teleworker Part 2","tags":null,"body":"\r\n\r\n## Client Technologies for Securing the Teleworker\n\nSecurity on the client side starts and stops with being able to verify the client with a secure boot. Intel has provided this technology in our clients’ systems for years with Secure Boot with Trusted Execution Technology (TXT), and more recently, BootGuard (BtG). On the most recent platforms, we have Intel Hardware Shield, a set of technologies that secure the system at its lowest level, at the firmware and BIOS level. This is how something like an enterprise access control system can verify a client as being securely booted and having the correct firmware and the correct security controls in place prior to having access to the enterprise.\n\nIntel has also done a lot of work over the years to help organizations better protect their data. In addition, we have allowed the client to turn on encryption everywhere without a performance impact. We have been implementing new instructions in almost every generation of our products, adding new capabilities to secure the teleworker.\n\nOur other focus has been protecting the applications and their data in use. This is where Intel Software Guard Extensions (SGX) come into play. This gives organizations the ability to put key parts of applications and important data into secure encrypted memory enclaves. With teleworking, this means you can deploy applications into untrusted environments and still maintain a high level of security.\n\nIn addition, new technology that Intel has introduced in the threat detection category allows for organizations to get deep visibility into the operations of the platform to monitor for threats; no malware can hide. These technologies are revolutionizing the way we detect malware using machine learning and artificial intelligence.\n\n## Data center Technologies for Securing the Teleworker\n\nEven if you have secured the clients, you must also secure the data center. Both sides must be protected because your system is only as secure as the weakest link. Many of the techniques for securing the teleworker are similar to securing your enterprise: secure booting, virtualization security, and isolation controls. Again, technologies like Intel TXT and BootGuard enable you to securely boot those platforms and data center and cloud assets. More recently, we’ve introduced technology, Intel Select Solutions for Hardened Security, that integrates a lot of the Intel security technologies into a single platform that is enabled by default.\n\nYou need to be able to protect your data at scale, meaning the ability to use all your security tools without negatively affecting performance. Intel’s Hardware-Accelerated Encryption tools (SHA, AES-2X, VPMADD52) make this possible. Intel’s new instructions and QuickAssist technologies are specifically geared to the enterprise and cloud scale encryption requirements.\n\nFinally, how do you monitor threat intelligence and audit at scale? Intel’s Cyber Intelligence Platform Architecture (FPGA, DCPMM, Optane SSD) uses high performance compute, storage, and memory technologies to scale the cyber intelligence platform, even with the extra burden of more external security with teleworkers.\n\n## Short-term Solutions\n\nEmployee education is the first line for curbing teleworking security threats. Guidance in home security and security training or reinforcement of prior training in areas such as proper data access is crucial. Be pro-active with patches by pushing patches and requiring users to patch their devices. If you have enterprise access control, ERM/DRM and DLP solutions, turn them on and scale them out. Reassess your policies to make sure they meet the new reality of teleworkers. For web connections, turn on TLS and make sure it’s enforced. Two-factor authentication must be leveraged. Most organizations may not think they have the infrastructure to deploy this, but there are a variety of vendors that can help in this area that don’t require you to deploy a large amount of new infrastructure.\n\nMany solutions are simply standard hygiene: Make sure your end point security agents are enabled and up to day. Manage and enforce security policies for the different types of user devices. Enable full disk encryption.\n\nOrganizations must understand that teleworkers are operating in an environment where the likelihood of a device being used by others in a variety of circumstances is high. Good security controls, most importantly employee education, can circumvent problems and allow employees to operate without negative impacts.\n\n## Long-term Solutions\n\nA long-term plan for security in an environment with teleworkers is now necessary, whether it’s for a permanent shift to more remote workers or to deal with another pandemic or similar situation. There are several steps organizations should take now to support this reality in the future.\n\nOne best practice is to implement zero trust policies. This reduces reliance on having to trust every aspect of users and clients who come in. Along with this, multi-factor authentication with users and devices should become standard across the organization. For those who haven’t adopted ERM and policy-based data access control, now is the time to do so to protect the data both offsite and onsite. Implementing deep stack security solutions rather than just at the application or network level is important. This includes secure boot with attestation, virtualization and contain security, and firmware security and monitoring. It is important to extend audit, threat intelligence, and monitoring to teleworker environments, despite pushback from users who don’t want more monitoring on their systems. Also consider extending security beyond the device in teleworker locations whenever possible, such as managed devices and networks.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT8-en","image":"./episodes/edt-8/en/thumbnail.png","lang":"en","summary":"With many employees now working from home, how do you make sure they are working securely yet still give them the flexibility they need to get their jobs done? In this episode, Darren and special Guest Steve Orrin, CTO of Intel Federal, discuss how to leverage Intel technology to effectively help secure the teleworker.  "},{"id":154,"type":"Episode","title":"Meeting Intel's Cloud Solution Architects","tags":["csa","cloudsolutionarchitect","solutionarchitect","people","compute","cloud"],"body":"\r\n\r\nIntel has invested heavily in hiring some of the best CSAs in the industry, with about 80 architects globally and 22 in the United States. These are new roles with a large focus.\n\nStephen Holt is a CSA for the East and manages the other CSAs in that area. Previously at Intel, he specialized in databases, but he came from varied roles in business process, analysis and sales, technical analysis, and technical sales at IBM and an assortment of startups. He brings all that experience to Intel to help the team work together to drive value to customers.\n\nKiran Agrahara is an East Coast CSA, reporting to Stephen. His experience is in data center infrastructure focused on data storage virtualization.  He has worked in the cable and financial industries. In the past five or six years, he worked with startups focusing on areas such as persistent memory and software-defined storage. When customers ask Kiran why they should utilize expertise at Intel, he answers that Intel is everywhere, and he wants to spread that message to end-users.\n\nBefore Sarah Musick came to Intel as a CSA, she was a partner doing software work around cloud migration and optimization that was precipitated by some time in data analytics. She worked for a deep learning textual analytics company before that and found that the work she did in data brought a lot of what she did in her earlier technical sales full circle. She brings the analytics to bear on cloud migration. She came to Intel because of its integrity and its role in the new model of engagement for everyone, including cloud providers.\n\nTodd Christ has been at Intel for 26 years, with 30 years of experience in IT and the product space. He most recently came from Intel’s cloud enterprise solutions group, which is part of the data center platforms group. Intel wants to meet customers where their data resides, whether on-premises or in the cloud. Todd architected Anthos, so the hybrid or multi-cloud models have been important to him, and he’s worked directly with Microsoft and Google.\n\nOne major shift in helping customers move to the cloud is simplicity. Now, customers don’t have to think much about hardware. They are up and doing meaningful work much faster with less overhead. Although the hardware still matters, of course, it’s abstracted away. It’s important, however, not to lose sight of the fundamentals. It’s like having an HVAC system that you never think about until it’s not working right, and then it’s an issue. For example, there are situations where a customer finds that a particular workload they’ve moved to the cloud isn’t working well, so they have to revisit the hardware.\n\nIntel has been with the cloud providers since their inception, and they focus on those workloads. Intel puts a tremendous amount of effort into the cloud ecosystem that helps build those workloads to run best on Intel.\n\nIntel has delivered 2 billion cores to cloud service providers (CSPs) and over 90% of all compute in the cloud runs on Intel.\n\nCustomers want fast scalability and they want the compute resources as quickly as possible; they don’t care what the hardware is. The truth, though, is that low latency workloads work much better on Intel hardware than on any other competitor. The newest third-generation Xeon scalable processor, Icelake, is blazingly fast. Once customers realize that they can save significantly, it sells itself.  So part of the job of a CSA is education.\n\nAlthough the CSPs may seem to sell services as a utility that works out of the box with 100% reliability, you can’t just drop, or lift and shift, critical workloads into the cloud.\n\nIf your applications are designed in a cloud-native format, then you don’t have to think much about deploying it on the cloud. However, if you have a monolithic application designed to run in a data center, for example, you can’t simply lift and shift it on the cloud because it's not optimized to run on specific CPUs. By using Intel optimization or migration tools, customers can make informed decisions before migration.\n\nSome workloads may not be suitable for the cloud. That’s why, especially in the latter part of 2021, there was more buzz about repatriation. The pendulum is swinging back a bit as enterprises in particular learn the right balance. This is where CSAs come in. Not everything should go on the cloud, and Intel can help determine how to optimize things. Some customers are finding that after moving workloads to the cloud because of mandates, they are not saving money and even spending significantly more than keeping things in their own data center. Or perhaps there are security issues because there is data residency in some places and the cloud isn’t in the proper country.\n\nIntel CSAs are agnostic, so they are only interested in what is best for the customers’ particular needs. They help make workloads more mobile so as IT departments become more mature, they can bring workloads back to their own data centers or move it to another CSP in the future, or whatever is most cost effective.  Intel CSAs can help customers in ways CSPs are not currently addressing.\n\nMany customers are concerned about data because it’s expensive to retrieve from a CSP. Intel has a deep bench of people who can help with this issue. They are well informed on not only the constructs of being able to set up hybrid models, but the security, the firewalls, and all those access points. Once your data is behind a firewall there are many layers of security that you need to get out to those services. So the first thing is that you need to be able to send your data back and forth securely.\n\nEven in a multi-cloud scenario, pulling the data out is costly. If you are just moving between Azure and AWS, for example, the data is still migrating, and that is a slow process. If customers have terabytes or even petabytes of data on-prem that they want to move into the cloud, there might be a cloud-like service that runs on-prem where they can get the ease of use and functionality of a cloud. If you think of the cloud as more of a function than a location, there are more possibilities.\n\nIntel is here to free people up to do the work that’s most meaningful to their organization, and analytics is going to be a big part of that. In 2022, there is still a huge gap between how much data people have and how much insight they are producing out of it. Only about 3% of data is actually used to produce insights. So there’s a massive treasure trove and Intel chipsets perform well in situations where there is robust processing work. Crunching data is what is coming next.\n\nArchitecting wisely is part of the future because you don’t have to reinvent the wheel. On the other hand, there are going to be newer solutions that could be a good fit for an organization. Where an organization is in its journey is also key. Older, established companies that have been doing things well for a long time, for example,  may have a lot of technical debt that potentially could be worked through. They need to look at the underlying technology and then eventually take it to a place where they have agility.\n\nThe CSA services at Intel come at no charge because Intel wants to help customers run their workloads most effectively and take advantage of Intel technology that is ubiquitous in the clouds.  The expertise and experience of Intel’s CSAs run deep, and they work as a team to help with any piece of knowledge a customer could need. Customers should ask their Intel account executive or inside sales to get help from a CSA with cloud migration and optimization. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Stephen Holt","Kiran Agrahara","Sarah Musick","Todd Christ"],"link":"/episode-EDT80-en","image":"./episodes/edt-80/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, talks with Intel’s top cloud solution architects Stephen Holt, Kiran Agrahara, Sarah Musick, and Todd Christ about how they can help organizations, at no charge, migrate to the cloud and optimize their workloads.  "},{"id":155,"type":"Episode","title":"Heterogeneous Programming with OneAPI","tags":["cpu","fpga","gpu","heterogeneousprogramming","npu","oneapi","compute","technology","process"],"body":"\r\n\r\nThe objective of oneAPI is to help abstract the enormous diversity that’s coming in hardware so software engineers can take advantage of it higher up in the stack and get the most out of the hardware. James, a software engineer who also works closely with the hardware, loves what oneAPI can contribute in light of trending heterogeneous computing.\n\nThe word heterogeneous here basically means that there are different devices in a system that can do computation, but they don’t run the same instruction set. For example, the CPU has one way of running instructions and the GPU has a different way. FPGA, other ASICs, and specialty devices each have different ways of being programmed. Taking advantage of all of those is the goal.\n\nJohn Hennessy and David Patterson, leaders in the computer architecture field, called this the new golden age of computer architecture. For computer architects, it’s an amazing time to design all sorts of specialty devices to do better things for specific domains, but for programmers, it can be difficult and even scary because of the complexity. This is where the oneAPI initiative and oneAPI products come in.\n\nThe name, oneAPI, is both an initiative with a specification and an implementation. The initiative is a simple concept: software developers should have the freedom to use any device they want with full performance, and their coding should preserve its value; they should not have to rewrite for every new device. These qualities can be applied to compilers, libraries, debuggers, and any type of tool.\n\nA great deal of software to implement certain specifications has been open-sourced. Much of it originated from Intel, but then Intel also makes it available as downloadable toolkits that are already pre-built and ready to go to support Intel hardware. Other vendors creating parts of oneAPI have their own implementations, so everyone comes together on specifications but each gets to create support for their own hardware in open multi-vendor fashion.\n\nThe high-level goal is to write code once and it runs well everywhere.  It is, however, a complex problem that will require some performance tuning. For example, if an algorithm is working well on a GPU, it will run on almost any GPU similarly. If you switch the algorithm to run on an FPGA, you can maximize reuse of your program, but you have the option to recode parts of the algorithm for it to run as well. It’s a complex approach that gives you the ability to take advantage of any hardware with varying degrees of hopefully very isolated rewrites.\n\nYou can build one code for different devices. There are even more sophisticated runtime approaches that allow you to detect what’s there and run different pieces of code so you can actually have a common source code that dynamically decides. You can apply a deployer program and it can figure out what’s on your machine and use it dynamically rather than tell the user to run a different binary depending on what machine you are on.\n\nThis is exciting because, for a long time, engineers used a software stack that was the best for their machine. Nowadays, they want to compile a program that may use devices from multiple vendors. The program should react to that. In order for that to work, the compiler has to be able to spit out code for these various devices in a reliable fashion. This is where openness comes into play.\n\nSome will argue that a closed system will get better performance since the libraries and language are tuned specifically to the hardware capabilities. But the question is, how can you get the best of both worlds? If, for example, there is a vendor-specific implementation for a math library, there can be a common program.  A big part of oneAPI is not trying to reinvent the entire world, but trying to organize it in a way to take advantage of the best on every platform that’s possible.\n\nOneAPI has the capabilities of moving memory and moving data. Different programming models sit on top of oneAPI, and it’s your choice of how much you want to get involved in managing the memory. Moving data around is expensive and consumes power, so you can’t escape that, but oneAPI gives you the tools to manage that by querying the system and letting your program at runtime make the right, dynamic decisions that will get you the best performance.\n\nIntel has decades of experience building tools to help with tuning and migration and has highly optimizing compilers. V2 has helped evolve the industry around hardware counters on processors becoming the norm. There are a variety of other analysis tools to give feedback from the structure of your program to finding deadlock and parallel applications to finding where you need to add some locks. Intel is making all of these tools available in a oneAPI fashion to be more versatile than just being about a CPU.\n\nThe first groups adopting oneAPI include high-performance computing (HPC). With the explosion of new computer architecture ideas, there will be an even greater amount of diversity and innovation in this space. The large codes that can help solve the biggest engineering problems in the world, or for example, solve pharmaceutical problems and evaluate new drugs, demands the latest and greatest hardware. So this concept of performance portability is getting in front of national labs, universities, and research centers.\n\nToday’s HPC is tomorrow’s department servers, so the capability to use different hardware requires software engineers to plan and pay attention to how portable the code is because code does not die quickly; it lasts for decades. Within a few years, heterogeneous systems will touch everyone, and now is the time to get educated about it.\n\nSoftware engineers do not need to parallel program to take advantage of oneAPI. It’s about utilizing things that are open and multi-vendor, multi-architecture capable. Even engineers who are at the top of the stack should understand what is in the stack and what it’s capable of in terms of portability and performance portability.\n\nThe IoT community is another group that has been way ahead in programming across multiple heterogeneous compute devices and using different methods to manage them. Their compute capabilities continue to rise as technology moves on. So oneAPI applies here and can help formalize or standardize things that have been innovated first in the embedded world.\n\nThe idea of one API has moved from being a crazy idea that only a few people were talking about to now more and more recognizing that it makes sense and solves problems in their organization.\n\nThe simplest place to learn about the initiative is at the oneapi.io website. To learn about implementation, click on the implementation tab for a link to follow to find Intel implementations. There, you can download the different toolkits. Search for the Intel dev cloud to try out the tools in the cloud for free, including on different hardware. The oneapi.io website also has a variety of tutorials and resources. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","James Reinders"],"link":"/episode-EDT81-en","image":"./episodes/edt-81/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, discusses the capabilities and future of OneAPI, a cross-industry, open, standards-based unified programming model that delivers a common developer experience across accelerator architectures, with Intel’s OneAPI Chief Evangelist, James Reinders.  "},{"id":156,"type":"Episode","title":"Shifting from Infrastructure to Workloads","tags":["cloudmigration","cloud","compute","process","infrastructure","migration","workload"],"body":"\r\n\r\nSarah’s conversations with customers have shifted from talking about infrastructure to talking about workloads. One obvious reason for this has been the shift from the data center to commoditizing resources to consuming anything as a service. Another reason is the influence of app teams in architectural decisions. The applications are the lifeblood of organizations in a way that they were not even five years ago. The last reason is financial: budgets are now categorized by workloads as opposed generalized IT spend.\n\nOne challenge in this shift from infrastructure to workload is striking a balance between centralized and decentralized processes and workloads.  Some things, such as security governance, are handled by a centralized hub, and others are handled in local, smaller teams. Much of the decision-making is being pushed down into the teams that are creating new applications and services inside IT and also just for customers.\n\nCOVID was an accelerant to promote change and move organizations to the cloud. Some IT organizations are letting their applications move to the cloud without constraints, and would rather clean up on the back end than slow down the innovation that is occurring. This “Black Swan” event is unprecedented and we are still seeing the fallout from the quick paradigm shift.\n\nThe role of the CIO is back, but only if they start thinking about information and workloads again instead of running a data center.  Of course, it depends on the organization. Cloud-native organizations, or those that aspire to be cloud-native, are refactoring their applications in flight because they want to be super agile. The more they do that, the more it drives toward infrastructure and service of the application rather than accepting limits that existed in the data center before and working within those parameters.  Previously, in that situation, it triggered innovation from app teams because when you are dealing with a set of givens, sometimes necessity is the mother of invention, as opposed to the infinite possibilities in the cloud\n\nThat’s one end of the spectrum. On the other end are American heritage corporations, the institutions. Typically, they still have information on the mainframe. It’s an “if it ain’t broke, don’t fix it” situation, especially with static applications. The cloud is enabling CIOs to think beyond the old school way of managing these applications. If CIOs can embrace these new technologies they can now see a path forward.\n\nThe processing that is happening under discrete applications is more relevant than ever. Intel has a huge role in addressing concerns about performance or cost in cloud offerings partly because they have mothered those technologies and also because they are one of the biggest software companies. There is a vast amount of internal knowledge. In other words, not all instances in the cloud are created equal, so Intel has optimized workloads internally to get the most out of the cloud instances they are using.\n\nFor example, many organizations are moving things to Kubernetes clusters and Intel does a ton of optimization around that. They can take things beyond the standard Helm charts with extensions that would look at the health of the node underneath and not just raw availability.  There are many things Intel can do to help customers vastly improve performance and cost, not just 2 or 3 percent, but 30 or 40 percent?\n\nEvery workload does not belong in the cloud. The structure of an organization has an impact on where the workload should land. The key is to be cloud-smart.\n\nA successful multi-cloud strategy is having a primary and a secondary cloud. When many talk about multi-cloud, the motivation behind that is fear of vendor lock-in.  Where most of your data lives has a bearing on multi-cloud strategy, as does where workloads fit best.  \n\nTo develop a strategy, Intel’s cloud solution architects will engage in discovery about what the organization wants to do and where the problems are. Intel often can remediate many of the issues with tools they have at their fingertips. The cloud solution architects will also contextualize offerings to make the process faster and more efficient. Part of their job is to be an educator, so everyone has the information they need to move forward. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Musick"],"link":"/episode-EDT82-en","image":"./episodes/edt-82/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solution Architect, Intel, continues his conversation with Sarah Musick, Cloud Solution Architect, Intel, about the shift from infrastructure to workloads. Please place in the Embracing Digital Transformation playlist."},{"id":157,"type":"Episode","title":"IDC Directions 2022","tags":["data","datacentric","people","process","idc2022"],"body":"\r\n\r\nDavid is based in Maryland, close to the government epicenter. He has over 25 years of background experience in both technical and business development across critical infrastructures such as health care, energy, financial, and industrial. Much of his specialization is on the edge: things out in the hyperedge and embedded space, but also those hard problems in accelerating workloads and trying to get the most out of an architecture that translates up to enterprise and the cloud.\n\nDarren, David, and some of their peers attended the IDC conference partly to see if their proposals and predictions were in line with the trends IDC has been seeing, and they came away feeling validated. They also went to see if there are gaps they need to understand and how they can leverage the ideas and services from IDC.\n\nThe biggest long-term trend, upon which the whole conference was set, is digital-first, and how it is impacting virtually every market. Meredith Whalen, IDC Chief Research Officer, expanded on what digital-first really means. Essentially any organization, business or government, needs to ask how to make digitization into an actual product. By 2024, there is going to be about $10 trillion to spend across all markets in digital products alone. This consumer shift is forcing the government to look at digitization as well, not just in their services, but in how they approach it around world trade, economies, and currencies.\n\nThis is a particular challenge for governments on every level because they grapple with computer networks on many different systems, and in many cases, they are siloed. Effective bridges don’t yet exist to link them together. Government systems need to look at how they can make things streamlined and simple.\n\nThe next big trend is a flip with cloud computing. Each cloud service provider (CSP) has its own infrastructure, and the infrastructure builds are not necessarily portable, so bridges are necessary. Currently, CSPs are non-fungible. It’s true that they only want customers to use their infrastructure, but as software development environments and software applications are built out through their marketplaces, it’s hard to move those across. CEOs and CIOs are not only asking if they are utilizing the software licenses and access they have purchased, but are more focused on whether the use produced a desired outcome.\n\nThe current CSP model tends to be inefficient with business outcomes in mind. Customers need ecosystems of software working together to deliver those outcomes. They are looking for multiple ecosystems that work together, with the ecosystems moving seamlessly across multiple clouds and hybrid clouds. So the trend is that CSPs become fungible, and bridge builders are going to be important here. That’s the flip.\n\nToday’s data is fungible. You can move it around, of course with an associated cost, but you can copy it, mutate it, etc. The trend, however, is moving toward non-fungible data. When that data, or digital asset, has ownership, that creates huge impacts for the future on how data is handled in areas such as security, trust, and business models and ecosystems that revolve around it.  There will be data entities that you will have to accept and attest the validity of who owns them and where they are coming from and all the policy that surrounds that.\n\nIn this evolution, there will be pros and cons. On one hand, consumers have more power over their own privacy if they have rights to their data. The same applies to businesses and organizations. On the other hand, with non-fungible data entities and assets, the ecosystems of software and the data scientists that use the data have to deal with it in a much more concise and structured way. In the long run, everyone will have to manage that. Governments are not in front of this problem, but it’s going to become an increasingly more important part of how they handle and mix trade, not just on physical goods, but digital goods.\n\nHow close is digital first in reality? One example is digital tokens as real currency, such as in the gaming community. Digital tokens are part of this world relative to how apps and ecosystems use non-fungible data. Another example is in healthcare. Today, you go to the doctor or see them virtually, and they can see what is going on and you can describe symptoms. That all shifts through real-time wearables that can monitor glucose levels, heart rate, weight changes, etc. That data belongs to the individual, so it has to be secure and authenticated, but they can also use it for services that aren’t about symptoms, but tailored algorithms and services about what is actually happening in their body to get the best diagnoses.\n\nData points to consider: Meredith Whalen brought up that in 2021  service spending exceeded regular licensed spending in IT for the first time. So, as a service is dominant currently; the trend is that it will move to an outcome-based model.  In 2023, IDC expects the digital spend will be greater than the non-digital spend in companies across the board. Each industry will vary, but on a macro level, 2023 is the tipping point. In 2024, the talent pool follows. IDC predicts that most companies will spend more on technical talent versus non-digital talent. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","David Richard"],"link":"/episode-EDT83-en","image":"./episodes/edt-83/en/thumbnail.png","lang":"en","summary":"On this episode, Darren, Chief Solution Architect, Public Sector, Intel and David Richard, Lead Solution Architect, Department of Defense, Intel reflect on the trends and ideas they gleaned from the 2022 IDC Directions conference."},{"id":158,"type":"Episode","title":"Securing Your Castle with Zero-Trust","tags":["cybersecurity","zerotrust"],"body":"\r\n\r\nThe old model of security could be compared to a castle, with guards along the road and a moat surrounding the castle. All of the crown jewels, in this case, data, would be centrally located and managed inside the castle. The emergence of zero-trust has created a new framework.\n\nThe biggest threat to the data is the end-user, so the number one strategy is a framework that starts on the outer edge with pillars of excellence with interior protections. This updated way of thinking allows organizations to bring their mission and business partners into the conversation in a real way.\n\nThe old style of thinking was more of a hard shell approach, with protections and controls in places of vulnerability. A key part of zero trust architecture is a risk-based approach, which is more dynamic based on two things: what has worked and what has not worked in the past. So if the attackers came in the left door the last time, you will, or course, shore up defenses there, but you will also learn from that attack and shore up other places based on new knowledge of how that happened. A risk-based approach is not just solving for the last attack, but thinking ahead and applying the right controls for current and future threats throughout the enterprise.\n\nPart of the risk-based approach is understanding the ecosystem. Customers, partners, and users are all part of the security calculus. The old hard-shell approach doesn’t work. Just as a castle has people and supplies coming in and out, and the riches might be located in various locations around the kingdom, zero trust takes the security one step further, accounting for all the ingresses and exits for the data or the people who are accessing it.\n\nTraditionally, someone could get access with a single sign-on into the corporate castle. There are several zero-trust principles, but the two foundations are default-deny and continuous monitoring & authorization. Trust is not automatically earned, nor is it permanent. For example, if a guest entered the castle, they are validated at the reception desk and then asked what they are visiting. They might be granted access to visit one person in one room for a certain amount of time, and they will be escorted in transit. They will also be monitored for what they bring in and out on their visit.\n\nZero-trust applies to access in all locations: data centers, clouds, edge devices, business environments, etc….It’s data-centric and access-centric, married with a risk-based approach. There must be more strategy involved. The resulting zero-trust-based approach does not throw out what has worked well before, but combines the good processes, principles, and technologies and adds a temporal element.\n\nThis new element is not as difficult as it has been often portrayed, but it is a process and cultural problem which can be tricky.\n\nMany developers have a fear that a zero-trust architecture will slow them down, but security experts and developers need to have a partnership to overcome that perception.  A real-world example is Log4j. Six months ago, developers could download it with no problem, but now the threat environment has changed. Without a risk-based approach, a developer would be able to download Log4j until someone from security came and shut it down. With a risk-based approach, along with access approaches, Log4j would be unavailable and an alternative would be offered. Another example would be when Log4j is already incorporated in a product, the dynamic trust assessment could put in extra controls rather than locking it down entirely. It’s about both sides of the calculus in play.\n\nThis partnership is similar to the cross-training and information sharing that goes into building security into the development process. As a product is being built and tested, security is also monitoring and assessing risk for both the entities you are working with and the product vulnerabilities in real time. Building a risk-based approach in the process leverages intelligence that gets to the heart of a lot of what we perceive as difficult.\n\nWhat is the first step for CISOs, CIOs, or CTOs to initiate zero trust? Cameron suggests quitting “geekspeak” and communicating in common English. Getting the initiative going can be challenging because typically leaders work with an outcome or objective in mind. Zero-trust does not have a defined objective to work toward other than creating a more highly assured environment for users to operate in. There are, of course, KPIs and other measures to show increased security, but it’s a journey, not a destination. He also emphasizes continuous funding; don’t embed the cyber budget in the IT budget. It needs to be separate and distinct.\n\nThe best place to find high-level information with practical guidance is the NIST publication SP 800-207.  It also lays out the five pillars of trust, which are good starting points.\n\nA primary foundational aspect is to have a good asset inventory of what needs to be protected such as data sources, databases, business processes, and transaction applications. Basically, you need to define the perimeter of your castle. It’s important to not just think about what you own, but what you rely on such as the SAAS environment, the cloud infrastructure, and third-party tools.\n\nThe bigger picture is knowing your value chain. It’s not just what’s in your castle, but it’s how you make money, how that money is distributed, whom you pay, and your providers. Each is a critical piece of the chain. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin","Cameron Chehreh"],"link":"/episode-EDT84-en","image":"./episodes/edt-84/en/thumbnail.png","lang":"en","summary":"On this episode, Darren discusses zero trust security with Intel’s Steve Orrin, CTO Public Sector, and Cameron Chehreh, VP-GM Public Sector."},{"id":159,"type":"Episode","title":"Artificial Intelligence and Security","tags":["aiml","cybersecurity","devops","compute","technology","process"],"body":"\r\n\r\n## AI Failures\n\nRecently, Darren asked a class of high school and college students studying AI to find AI failures. They found examples such as Microsoft Tay, a chatbot that learned casual conversation from Twitter and, within 24 hours, was spewing racist and misogynist comments based on manipulation from Twitter feeds. Another example was a ball tracking system from Inverness Football Club that was meant to track a soccer ball but tracked a bald referee’s head instead.  More serious was a fatality when an Uber autonomous vehicle did not recognize a pedestrian outside of a crosswalk and failed to stop.\n\n## Deploying Solutions\n\nAll of these AI failures had to do with data. When deploying AI solutions, you must ask yourself critical questions: Where am I making my inference?  Is the endpoint secure? If you are making all your inference at the endpoint, maybe object detection with a camera, for example, you need to make sure it is secure; no one should be able to manipulate the data, the camera, or the model.\n\nAnother question is, what is going to get deployed? Am I deploying a neuro network or algorithm out to the edge, or am I just streaming data back from the edge into a data center to make the inference there? In addition, when AI is operationalized, you have to ask how often the models or algorithms will be updated.\n\n## AI Pipeline\n\nThe pipeline for AI development, training, testing, deployment, and inference, needs three things: the application, the model, and the data. Those three things must migrate through the pipeline together and be protected concurrently. You need to make sure the production data is not manipulated even in production.\n\n## AI Security Threats\n\nThe threats for AI are significant, whether they are espionage, sabotage, or fraud because the attack surface is large.\n\n## Attacks\n\nFirst are the models. A model can be manipulated, such as someone putting black and white stickers on stop signs so they are not recognized as such or someone messing with pattern matching detection, so attacks are undetected. Models must not just be protected during training but during testing, deployment, and inference.  Encryption, access control, and model and version control are critical, just as they would be developing an application.\n\nSource training data and production data must also be protected from manipulation.\n\n## Types of Attacks\n\nA paper from the Belfer Center places the threats on a format axis and a visibility axis. The format axis ranges from physical to digital. The visibility axis runs from perceivable to imperceivable.\n\n### Physical Attacks\n\nPhysical attacks can be altering physical items such as the sticker on the stop sign. These attacks were evident early on with autonomous driving and facial recognition. We need better training of the AI algorithms for these attacks, using reinforced learning and negative case learning techniques.\n\n### Digital Attacks\n\nDigital attacks are harder to detect as they are not visible. An attack could be white noise injected into the data stream to throw off the algorithm. These attacks are brutal to combat unless inference is conducted at the edge or pattern detection is deployed.  For this reason, it’s essential to know where the source data is coming from, both in the source training, testing, and production data.\n\n## Identify the Data Sources\n\nData sources must be verified and tested from public data sources. Open-source data is not well guarded. Consider looking at generating data sources, so you have more control. If you use a shared data source, use a version control system such as GitHub or GitLab to check for consistency. Test data also need version control, access control, and other security measures, just as you typically do in a DevOps pipeline.\n\nThe last, most challenging part is protecting the production data. Making the inference as close to the data is a good start. Much speculation can be done right at the edge with neuromorphic processing and even instruction sets in the Intel processors to lessen the risk of data being manipulated during transport.  Instead, you can encrypt the data and send it back to the data center.\n\nProtect and Manage Data / Secure AI Pipeline\n\nOnce you have identified all of your data sources, there are three critical aspects for protection: control, security, and encryption.\n\nThe first is controlling. You should have version control, protected libraries, and backup and restoration in case of corrupted data files. These are standard good security practices that AI should practice as they are in app development.\n\nSecurity should include access authorization, even some zero trust concepts such as giving access to people who need it for only a short time. Please make sure the models are not being manipulated and make sure they are tied to specific applications.\n\nData should be encrypted at rest, in transit, and use. In the past, it has been expensive as far as CPU utilization and time, but now much of the encryption is in silicon and is very fast with minimal to no performance lag.\n\n## Call to Action\n\nData is key to making AI successful and secure, so protect it and use best practices right away around security. Operationalize pipelines to take the humans from the day-to-day grind of deploying and testing AI algorithms. Automate as much as possible and inject security into the AI DevOps pipeline to protect your source data, model, and application. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT85-en","image":"./episodes/edt-85/en/thumbnail.png","lang":"en","summary":"On this episode, Darren discusses the data aspects of artificial intelligence (AI) and the importance of securing that data."},{"id":160,"type":"Episode","title":"The Things I Wish I Knew as a Government CTO","tags":["cto","people","process","change","organizationalchange"],"body":"\r\n\r\nWhat have Jason and Ron learned since joining Intel that they wished they knew when they were in the military?\n\nJason was surprised how much better industry integrated solutions and developed some incredible capabilities missing in the army. He learned that business use cases show that understanding the customer’s problem set is more valuable than simply pushing technology. CTOs need to know the technology and learn how to change the culture of team members to adapt to it. Making the user experience friendlier, building the technology smarter, leaner, and faster, and getting people on board are what can make smoother transitions. He would have liked to have seen more outcomes and solutions in the army instead of simply products.\n\nRon agrees with Jason’s assessment and adds that the challenge he saw from inside the government was how to get industry to come along as mission solution partners so they could better inform military teams on how to apply their technology to those missions. Great technology without understanding how to solve the actual mission problem can be a train wreck.\n\nAnother element both agree is important is user experience. Matching the technology with the people that have to operate it is critical. Everyone using the technology does not need to have a Ph.D. in engineering. The user experience must be built for your people’s level of skills. The outcome is seldom independent of humans. When bringing solutions to defense or industry, you need to understand the mission's technological needs, but the entire space and how to operate in that space. Particularly in the military, you have to plan to execute in moments when things have not gone well and make it work. Technology will not win the day if you aren’t planning for the human element and challenging circumstances.\n\nWhat are the biggest or most exciting surprises for Jason and Ron as they have moved from military to private industry?\n\nJason and Ron say that they didn’t expect a distinct culture of teamwork inherent in military service in private industry at Intel. Both found the onboarding experience and ongoing support encouraging. They appreciate the general attitude of being in it together and the ability to create things with a dynamic team more incredible than you ever could on your own.\n\nThey both appreciate that they are not expected to sell Intel products merely but to help customers solve mission challenges and provide customer feedback to Intel. They are at Intel to solve problems, especially in the public sector, maybe with technology that hasn’t been created yet.\n\nWhat are the technology gaps that Jason and Ron see in the Department of Defense or Intel?\n\nFirst and foremost, Jason says, are cloud operations, and it is a changing dynamic in the DOD. Commanders on the battlefield are risk-averse. There is no room for DDIL. As cloud operations evolve, you have to retrain and relearn all the work into accurate cloud operation and the benefits of edge to cloud capability that offers real-time, accurate information that gets to the right people at the right time. Everyone must have situational awareness and an operational picture of what is happening.\n\nRon believes the next thing on the list is increasing cybersecurity as the vulnerability surface increases.  If the military doesn’t drive in the direction of zero trust as it moves to a competent edge and highly mobile, the results during conflict could be disastrous. The DDIL issue is enormous, but it’s got to be secure against the increased vulnerability surface.\n\nJason believes the technology must move forward despite the risk because the military always wins with the information. Whether it’s FEMA missions, providing nuclear power to a city, setting up field hospitals during COVID, or the battlefield, technological advancement, especially 5G, is critical to operations. DOD leaders must have information and the ability to communicate to their headquarters for guidance, especially in combat operations when leadership could change through casualties down to the lowest corporal.\n\nRon uses the example of the highest priority of the national defense is never to have to fight an adversary on home territory. Since the United States may not have a quantity advantage away from home during a conflict against an adversary who may also be technologically advanced, the military has to be more capable. The military must continue to provide increased capability to national defense forces and do it securely, despite the increased vulnerability exposure. Those problems must be solved so operators can trust the data and use it effectively in an away-game environment.  There is no option but to drive in that direction.\n\nJason adds that another space where the DOD has been slow to adapt is AI due to a lack of people with suitable backgrounds. You don’t come out of Army Ranger School as an AI expert. The same problem existed with cybersecurity for years until the DOD made a significant investment. Some complex resources need to go to AI operations because AI can change everything.\n\nRon, having spent the last few years in the service in the nuclear space, points out that the US will never take the human element out of critical decision loops. AI will be hugely valuable as it can ensure the technology can dynamically adapt. It will reduce the cognitive load and process countless data points so human decision-makers can have a more apparent situational awareness and be better prepared to make informed decisions quickly. That’s the AI space that the DOD must pursue. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jason Dunn-Potter","Ron Fritzemeier"],"link":"/episode-EDT86-en","image":"./episodes/edt-86/en/thumbnail.png","lang":"en","summary":"In this episode, Darren Pulsipher, Chief Solution Architect, Public Sector, Intel, welcomes special guests Jason Dunn-Potter, Ret Chief Warrant Officer, US Army, and Ron Fritzemeier, Ret Rear Admiral, US Navy. Both are now five months into their positions as Solution Architects and Mission Specialists with Intel’s Department of Defense Team.  "},{"id":161,"type":"Episode","title":"Confidential Computing in DevSecOps","tags":["confidentialcomputing","devops","compute","technology","process","devsecops","cybersecurity","sgx"],"body":"\r\n\r\nAnjuna’s software enables applications to run with Intel’s SGX protection and solves the problem of protecting data in use. Anjuna’s mission is to make secure enclaves as simple as possible. With Anjuna’s software, there is no need to change anything in the application; take it, run it in an enclave, and the SGX technology will work out of the box. The software works with any app, in any cloud, at any scale.\n\nThe global software supply chain is under attack. SolarWinds, notably, was an attack on DevOps, and although there have been ideas on how to solve the problem, it hasn’t been locked down. Anjuna technology can be an easy solution. There is no need to re-architect your software or change methodologies. You run them in secure enclaves.\n\nWhen trying out Anjuna’s software, Darren created a stack using Intel SGX on the bottom, Red Hat OpenShift, Anjuna for the confidential computing part, and HashiCorp’s Vault to store a secure ledger. He was shocked at how fast the solution was up and running in less than a week.\n\nDarren calls this process the hardened DevSecOps pipeline, although it’s many moving parts. Ofir agrees with this terminology, as this process is a new DevOps hardened with the SGX hardware technology with Anjuna’s software.\n\nConfidential computing, or secure enclave, solves the problem of protecting data. When you store data in persistent storage, the solution for data at rest is already there. There is also a solution for data in transit with TLS. Securing data in use has not been solved because when data is in use, the application needs to access it from memory in the clear. It can’t be both encrypted and in use at the same time. This has been an endless loop of a problem. If a bad player has access to a machine where the application is running, a hack is as simple as coming through the device, identifying the process, and creating a memory dump. They will get all the secrets and confidential data on file, and it’s not encrypted. This would also include the keys to encryption for data at rest and in transport because the software needs to use them to encrypt.  The lousy player will have the keys to the kingdom.\n\nThe problem is resolved if you run the different applications in secure enclaves. Even if someone got access to the machine, they would not have access to the memory of each application. This doesn’t mean you don’t have to resolve vulnerabilities, but you are much less stressed to fix them as soon as possible. Even if there are kernel vulnerabilities, when something runs in a secure enclave, the kernel cannot access its memory.\n\nAnjuna software runs on other hardware-based technologies as well as SGX. Unlike running encryption in software, where the performance hit would be high, Anjuna can fine-tune the configuration to run your application with a negligible performance hit: less than five percent.\n\nSo you may not want to put everything in a secure enclave just yet, but it is the future for security.\n\nOne of the uses for a secure enclave is to store data that pans different steps in the DevOps pipeline in a secure ledger. The ledger has everything that went into the build, security keys, and hash values used for verification. These verification hash must remain unchanged through the cycle so no one can inject code, libraries, or binaries into the package you deliver. Everything should run in a container in the modern world.\n\nAnother candidate for protection is a signing key. Without secure enclaves, once you have a binary ready, you need to take it to another machine in a dark room that no one has access to. But three people with three different keys sign it there. Secure enclaves enable access to that signing key in your familiar environment, but only the enclave will access it. It will be based on the complex identity of the software running inside the SGX enclave, which is implemented via the attestation quote. In other words, you can attest enclave to enclave. You can also attest to things that run outside of enclaves. It gives you the ability to trust software that runs somewhere else.\n\nThe compilation of binaries is another use. One of the big problems in the Department of Defense, for example, is that they want to be guaranteed that everything that went into the build can be traced back to the developer who wrote it. Especially in embedded systems where software controls multimillion-dollar machines that can kill people or save people’s lives. There must be full traceability to help ensure accountability and secure development has been performed.\n\nIn addition to memory dump attacks, another attack problem that Anjuna solves is making sure that in cases where you need to go to the kernel, it will protect whatever needs to be covered in that interaction between the enclave and the outside world. It also can protect against accessing code and make secrets only available to the enclave. In addition, if someone gets into a machine, they won’t be able to find a TLS certificate in the clear or the key that’s used to encrypt it.\n\nEvery cloud service provider offers secure enclaves, and Anjuna supports them all. They also support on-prem technologies. On top of the primary offering, Anjuna can also enable the ability to encrypt your data at rest and in transit without changing your software, even in legacy applications or new applications that don’t support encrypting every data file.\n\nFor more information, visit anjuna.io, or check out a white paper authored by Darren and Ofir at embracingdigital.com. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ofir Azoulay-Rozanes"],"link":"/episode-EDT87-en","image":"./episodes/edt-87/en/thumbnail.png","lang":"en","summary":"In this episode, Darren Pulsipher, Chief Solutions Architect, Intel, and Ofir Azoulay-Rozanes, Director of Product Management, Anjuna, discuss Anjuna’s solutions for confidential computing in the DevOps lifecycle."},{"id":162,"type":"Episode","title":"Collaborative DevSecOps","tags":["cybersecurity","devsecops","rpa","technology","compute","zerotrust","zta"],"body":"\r\n\r\nMike’s experience as a cybersecurity engineer in the Air Force and then working in development, security, operations, and managed services led him to a goal of creating a product that could be collaborative to build modern automation around what he calls IT as code. He wanted to look at DevSecOps holistically, bringing everyone together.\n\nHis resulting product, Sophos Factory, creates modern solutions around building blocks with the features, functionality, and user experience that can be utilized across the full spectrum of technical talent. This was a complex problem to solve, including visually working people, developers who code, etc...…. It also had to bridge the gap between hardware and software, using an agile process across teams.\n\nSophos Factory is more than just a CI/CD pipeline. That is a small part of the whole system, which works end to end from development, security, operations, and deployment with features like a visual builder, DSL, and support for all content in its native format. It also ties into existing systems. It brings in all the different teams and the different tools they use, so it is significantly beyond simply making a pipeline or automation.\n\nIndividual users are presented with the pieces they are familiar with, but all with the same interface. For example, a set of scripts can be built from a visual format. A security person can consume the same interface with the tools and artifacts they expect. A full-stack developer or a DevOps engineer can pull in and build all of the artifacts in a way the other teams can use.  It’s not the creation of a pipeline for an automation piece but an interconnective fabric between disparate systems.\n\nIntegration means the movement of data, but it also means actions. For example, suppose someone uses Jira on the coding side, ServiceNow on the IT ops side, and an incident response piece on the security side. In that case, you can integrate all those pieces and fire off something to Slack, so everyone has visibility and can respond in near real-time.\n\nSophos Factory packages modules into pipelines for reusability, which become building blocks. These can be built around various use cases, but the goal is that you are creating something that can be used over and over again. For example, suppose you are using ServiceNow and want to create a ticket. In that case, you use that across various other use cases around network automation, infrastructure automation, cloud, native security, etc. It is solution-building rather than just automating these things together. The last piece is future proofing, not just repeatability. You can add or subtract from the overall pipeline that’s not possible with hardware but also very difficult with existing systems such as CI/CD systems that are made for releasing software to production, not for holistically building a solution and maintaining the lifecycle over time.\n\nWith Sophos Factory technology, you can package different tools to help adhere to standards such as CIF or NIST 853 and have them available as low code or no code pipelines. Sophos Factory diverges from other automation technology with its sharing via solution catalogs. You can publish automation building blocks, complete solutions, or consume automation created by other teams. This creates a tremendous amount of flexibility.\n\nVersion control is built into the pipelines and the solution catalogs. If you are using a solution pipeline from a record that somebody else published, you can set it at whatever version you want or pull from the latest version to get any updates. RBAC is also part of the system in case you want only particular users to have, for example, read-only access. With the interoperability of Sophos Factory, you can also integrate scanning tools to maintain visibility in the pipeline. You can also run different channels around policy tools.\n\nSophos Factory weaves together security and IT workflows, creating an excellent integration point among the three-headed monster of Dev, Sec, and Ops.\n\nTo improve security, Sophos Factory has a zero-trust and attestation product, but they also work with other security products such as HashiCorp Console. Zero-trust and attestation capability is the natural evolution to authenticate between different systems. Rather than static credentials, there are now better ways to communicate and share attestation among the others securely.\n\nSophos Factory has a built-in credentialing system for key management, and it supports HashiCorp Vault and cloud-native templates. They can also help critical management services built in the cloud and packaged around a pipeline. There is not just a credential variable at runtime but also a credential step that is only evaluated at runtime. They can layer on top of these security tools, so they naturally become part of your building solution.\n\nSophos Factory is in the RPA space, but it is far beyond a typical RPA runner. They are technically RPA because, although humans are still involved in making things, the machines are leveraged to automate the process. Customers are looking for ways to scale and get value from the IT they purchase securely. Sophos Factory embraces helping technical talent level up and giving them access to toolsets, getting more out of them, and doing it securely. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Mike Fraser","Callen Sapien"],"link":"/episode-EDT88-en","image":"./episodes/edt-88/en/thumbnail.png","lang":"en","summary":"On this episode, Darren talks with Sophos’ Callen Sapien, Director of Product Management, Sophos Factory, and Mike Fraser, VP of DevSecOps about their product that allows for truly collaborative SecDevOps."},{"id":163,"type":"Episode","title":"Future of Big Memory and CXL","tags":["bigmemory","memverge","optane","technology","data","compute","pmem","cxl","ceo"],"body":"\r\n\r\nAn exciting development in memory is CXL (Compute Express Link). A robust ecosystem is being developed from the host side, with Intel and others supporting CXL 1.1 in their next-generation platforms, such as Sapphire Rapids. Those on the memory side, including fourth-generation Intel Optane, will be on CXL. Intel is one driving member of a strong CXL consortium that has been driving the standard. CLX 2.0 has already been defined, with additional capabilities, including the externalization and switching of CXL, and 3.0 is in process, which will standardize how CXL can be provisioned and shared.\n\nThere will be new products coming out of Samsung, Micron, SK-Hynix with CXL capabilities. Additionally, there are new interconnects that are being developed that can potentially connect memory with the fabric so there'll be a fabric-enabled memory that can be shared across multiple servers.\n\nCXL technology is a game-changer. A new memory protocol runs on top of standard PCIe generation five and later. Since it allows you to put memory on PCIe, it can not only be inside the box, but in the future, with PCIe switching, you can have the memory on the fabric become composable and shareable.  The first CXL product will be coming out by the end of the year.\n\nMemVerge already has a software-defined memory controller, and CXL opened up an entirely new world for the software. CXL is to memory as fiber channel is to storage. It’s like a memory area network rather than a storage network. It is possible to go directly into memory with CXL and bypass the CPU altogether.\n\nCXL will be much faster than previous interconnect technologies.  It will have 100 or 200 nanoseconds of latency. This is where the utilization, manageability, and agility increase. There will be higher availability and productivity in the use of memory. In addition, you will be able to provision memory dynamically; you can provision as needed, and it doesn't have to fit into the server box. Theoretically, you will always have enough memory for whatever you need to be active.\n\nWith MemVerge’s snapshot technology, your data is protected and persistent as well. This becomes more important as your memory becomes bigger. If you lose it, it’s harder to reconstruct.\n\nFor CXL to take off, three things need to be in place. On the hardware side, the older hardware leaders need to be on board and embrace the same standards. That has happened this past year, so there is a single standard everyone is supporting. Second, you do not need to change your application to use CXL, just like storage area networks. Third, from a database point of view, you should not have to rewrite. This can happen between what the standards provide and what the operating system supports and MemVerge software that can do auto-tiering between DDR memory and CXL memory.\n\nMemVerge can offer that abstraction layer. It’s essentially memory virtualization. The software-defined memory handles the actual placement of the physical memory.\n\nMemVerge makes big memory transparent to the application so programmers can utilize the higher capacity and never run out of memory. Next is data protection. MemVerge has developed an in-memory snapshot service that can capture an application’s entire state in memory, and that state is immutable. It can be recovered anytime, anywhere. There are many use cases with this, such as ransomware mediation and decreasing cycle times in genomics research.\n\nThe snapshot feature is not only useful in that it can quickly and easily capture a running pipeline allowing you to roll back and recover at any time, but it can save money when using cloud services. The primary service providers have spot instances that are 70-90% off the demand price, but there is a catch: They can take it back at any time with only 30 seconds to two minutes’ notice. This is not enough time to deal with that, especially if you have a lot of data in the memory, so it hasn’t been helpful for many workloads. With MemVerge snapshot capability, you can take periodic snapshots of your running workload in any instance. If the spot instance is taken away, you have an image you can recover and continue running. It’s insurance that allows you to use the low-cost service with protection.\n\nSince you are taking a snapshot of not just an application but a whole container or instance, you can reinstitute it anywhere, on-prem, in the same cloud, or another cloud. This gives you maximum mobility and resiliency in your operations, even in the case of a major cloud service outage. This technology lends itself to many exciting possibilities.\n\nThe CXL revolution and MemVerge Memory Machine software are potent combinations for game-changing possibilities. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Charles Fan"],"link":"/episode-EDT89-en","image":"./episodes/edt-89/en/thumbnail.jpg","lang":"en","summary":"Darren Pulsipher, Chief Solutions Architect, Intel, talks with Charles Fan, CEO of MemVerge, about how the CXL revolution and MemVerge’s software are the future of big memory."},{"id":164,"type":"Episode","title":"Telework Securing Your Home Office","tags":["cybersecurity"],"body":"\r\n\r\n## System Security Tips\n\nThe first thing you want to do to secure your home office is to update all of your systems. This includes both applications and operating systems on all your internet connected devices. You will also want to update end point security software and run regular scans when your device is on. Many people know to do this on their desktop or laptop, but endpoint security should also be deployed on phones and tablets as well. In addition, be sure to turn on your local firewall and enable your router’s firewall.\n\nOne of the major steps you can do to reduce your overall risk while you are on the internet is to reduce the runtime surface area of attack. This means you should close applications that are not in use, close the browser before going to new sites, and log out or close secure sessions before doing activities such as checking email or browsing. You should not be doing different activities simultaneously to avoid cross attacks.\n\nA good education site for learning how to safely browse the internet is Stop. Think. Connect. https://www.stopthinkconnect.org/\n\n## Modem/Router/Wifi Security Tips\n\nChange all default passwords to secure passwords (minimum of 8 – 10 characters, use upper and lower case, numbers and characters). Default passwords come with routers, modems, ISP web portals, and WiFi. Carefully guard who has access to your passwords. It’s also important to change the default network name (SSID) to something without any identifying information.\n\nEnabling two-factor authentication wherever possible will give you another layer of security. Routers and modems need to be updated just like your laptop, so be sure to turn on automatic updates.\n\nOther steps to increase security include turning on WPA and disabling WPS if possible. Enable network address translation (NAT) and DNS filtering on the router and modem. You will also want to disable UPnP.\n\nThese techniques will prevent unauthorized people and your neighbors from “borrowing” your WiFi, which creates a security risk.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT9-en","image":"./episodes/edt-9/en/thumbnail.png","lang":"en","summary":"Securing your device and your data center only gets you so far. With more people working from home you need to help your employee secure their home network and work area. In this episode, Steve Orrin, Federal CTO at Intel helps Darren secure his home network. "},{"id":165,"type":"Episode","title":"Barriers to Future Tech Adoption","tags":["cto","organizationalchange","change","people","process"],"body":"\r\n\r\nGlenn is the Chief Technology Officer at CACI, a six-billion-dollar fortune-500-size company focused on national security. He has been in the industry for 35 years, dedicated to military, intelligence, and cybersecurity for the US government. He is an engineer, so he thinks, acts, and processes questions like an engineer.\n\nExciting new technology in AI, neuromorphic, and security is sometimes not quickly adopted in the public sector.\n\nThe first barrier is complexity. Those in the technical community are outstanding innovators but not always great at making the technology easy to consume, use, or administer. Glenn sees some improvement, however. For example, in the last five years, the complexity of using AI has significantly decreased as more people are leveraging tools and software development kits.\n\nAnother barrier can be the lack of talent to architect, design, deploy, and maintain the technology in the public sector. New talent must not only be attracted to the industry, but they must also be able to get clearances. The other side of the talent equation is many are resistant to adopting new technology. It’s a change management challenge. If not approached correctly, existing talent becomes antibodies to adopting new technology.  Instead of asking how to get to “yes,” they come up with a thousand reasons why not.\n\nThe third is cybersecurity. Glenn does not describe this as a barrier because it is not something that would be removed. Still, the industry could be more proactive in moving the decisions further to the left and involving security earlier to accelerate adoption.\n\nThe public sector could take a lesson from how smartphone technology was adopted so quickly with the advent of the iPhone. Early iPhones were amazingly simple to use, and the company was committed to handing customers something they could immediately utilize. In addition, the development community could instantly create and innovate with the software development toolkits and processes. Kids in the sixth grade can develop with the toolkits for iPhone and Android. Usability was vital, and it ushered in software development efficiently.\n\nThe public sector should try to mimic those qualities and approach their market and customers the same way, especially around analytics. There is a tremendous amount of data, and we are not getting a lot of use out of it.  Just as Steve Jobs moved his technology outside of the average target, the same thing needs to happen in analytics. Instead of targeting data scientists, maybe the target should be a business person or an average person who has a household to run.\n\nGlenn believes the first step in thinking this way about AI and analytics is being fiercely committed to liberating data. So much data is locked in proprietary designs that it is a lousy business model. Customers should renew contracts not because their information is controlled and closed by a provider but because the provider is offering solutions, value, and innovation.\n\nAI should be couched as a digital assistant to customers rather than a murky, magical robot in the background that the customer is nervous about trusting. A digital assistant is simply one step ahead of what you are trying to accomplish by serving up the data and information via machine learning and deep learning that makes your life easier and lets you do the processing. In this way, AI adds instant value and is much less scary.\n\nGlenn believes the technology that the public sector must have today is, first, the cloud and more of it. Cloud takes a lot of human error out of administration. It lowers the attack surface, and it’s a pay-as-you-go consumption model, which can be economical if you develop software for that model.\n\nSecond, are good DevSecOps platforms. Software programmers can spend more time on actual development with the available tools. DevSecOps is still early in action, and the growth will be explosive.\n\nOne technology that was key during COVID remote work is Commercial Solutions for Classified (CSFC). This technology has been around for a decade. Still, it has matured to the point where the complexity has been taken out of its deployment, administration, and security to be taken to the edge quite easily. Someone can work in an unclassified domain and still access secrets, for example, confidence that they won’t have any leakage. This is a new, important capability.\n\nAll of this ties back to attracting and retaining talent. The difficulty in this is not because the public sector does not have exciting, challenging problems but the perception that the government moves very slowly. There is some truth there because of the importance of maintaining a certain sanctity or confidence. Still, in reality, the government is right on the edge of many new technologies such as photonics or light-based communication. Some pockets move swiftly, such as in software development with agile and DevSecOps.\n\nGlenn believes the space domain will explode and be fundamentally different in five years for the future of technology. The ability to put more things in orbit inexpensively with improvements in size, weight, and power, along with the ability to communicate with photonics over thousands of kilometers quickly, allows connectivity and the ability to distribute and use data on those payloads.\n\nAnother area that will be very different is spectrum agility. The domain of the electromagnetic spectrum will be crucial to resilient connectivity in conflicts. The subsequent disputes will heavily emphasize who can communicate and interfere with comms. Spectrum agility is the ability to dynamically know what’s going on around you in the spectrum so you can move quickly. This connectivity is fundamental to joint command and control and the JADC2 vision to work.\n\nA third area is what can be done with Kubernetes and with infrastructure code. Automation will remove labor and ease things in general.\n\nNo conversation about the future of technology would be complete without a mention of Quantum. The quantum programming models being built are entirely different from traditional models. We need to get to a point where we have toolkits for programmers that make the process much more automated. Training someone for two years to be a quantum computing programmer doesn’t scale, so we need the software development toolkits to rise, just as the community abstracted out the complexity and developed toolkits for artificial intelligence.\n\nThe last area is the continued evolution of edge computing. The number of computing processes is phenomenal, engendering creativity with size, weight, and power. Edge computing will continue to transform to be secure and trusted. Resilient communication may not be a dedicated connection but a mesh network where parts of the message are recombined at the other end. This can provide solutions in tactical and deny and disrupted environments.\n\nDarren sees the traditional Von Neumann architecture waning in the future as we can have data persistence without disk drives, and we can have data that lives, moves, and migrates with functions that work with it.  The layers and limitations of the Von Neumann model will be removed.\n\nGlenn thinks this will happen faster than people would generally predict because of experience bias. But it’s a different world when you can get all the other pieces to coexist and remove the latency issues. Imagine what could be done at the speed of inference at the edge, for example, with autonomous vehicles. That technology is happening, and programmers consider abstraction layers from the beginning. So you could, for example, take a neural network model that is already developed and run it through a software development toolkit to place it onto a hardware substrate, a non-Von Neumann architecture, and you don’t have to reprogram. This will accelerate the adoption, and it will be transformational. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Glenn Kurowski"],"link":"/episode-EDT90-en","image":"./episodes/edt-90/en/thumbnail.jpg","lang":"en","summary":"Darren Pulsipher, Chief Solutions Architect, Intel, discusses the barriers to adopting new technologies in the public sector and what is coming in the future with Glenn Kurowski, CTO of CACI."},{"id":166,"type":"Episode","title":"Fighting the Cyber Adversary by Securing your Software Supply Chain","tags":["cybersecurity","devsecops","sbom","securesupplychain","policy","process","compute"],"body":"\r\n\r\nDarren and Eric Greenwald, General Counsel of Finite State, discuss securing the software supply chain in this episode.\n\nFinite State focuses on finding vulnerabilities in firmware, most often software from third parties that may have already had existing vulnerabilities, before folding it into their device. Finite State focuses primarily on industrial IoT devices, medical devices, automobiles, and consumer electronics.\n\nFor seven years before joining Finite State, Eric worked as a lawyer in the private sector, focusing on security testing and identifying vulnerabilities for cybersecurity companies. Before that, he worked in government with cybersecurity and national security, for the FBI and CIA, and as chief counsel for the House Intelligence Committee. His government work culminated when he became the Senior Director for Cybersecurity on the National Security Council at the White House.\n\nEric believes part of the reason there is now an emphasis on securing the supply chain is that the threat has evolved. The increasingly complex nature of software, including that many components have vulnerabilities when they are first created and only discovered over time, makes it more difficult to find devices built through the software supply chain. In addition, recent high-profile attacks via a breach in the software supply chain such as SolarWinds have made people more aware of the danger.\n\nIn the SolarWinds attack, the perpetrator had patience, not immediately causing havoc but waiting a year while roaming through American computer networks, cultivating access and information. That patience is probably the most significant difference between a nation-state and a criminal attack. While sometimes patient criminal attacks are more focused on a financial return on investment, those behind a state attack are willing to take years to develop their intelligence access. With either, however, the attacks are becoming more sophisticated and are much better positioned to take devastating advantage of supply chain complexity.\n\nTo combat these attacks, new regulations are cropping up. The Biden administration issued executive order 14028 in May 2021, which has two main points: secure software development and software bill of materials (SBOM). The executive order is only directed at government procurement, but private industry is likely to follow.\n\nThe details and technical recommendations of secure software development are still being worked out. Still, part of it would be that software suppliers to the government would have to provide an SBOM. The first draft legislation for SBOMS came out in 2014, so the standards for producing them have become more mature and developed. An SBOM is essentially a list of software components that went into a software product, not so different than a list of ingredients on a food product. This offers transparency in the supply chain, which is essential to assess vulnerabilities or to be able to identify a vulnerability that is discovered at a later date.\n\nAn excellent example of this is what happened with Log4j. When that vulnerability was discovered, many companies had no idea if they had it in their stack. It wouldn’t be a magic wand, but an SBOM would allow companies to more easily discover if they have the problem software component in their system and act more quickly to implement a patch.\n\nArguments against publishing SBOMs are that they will provide a roadmap for attackers and give away proprietary information. While these are legitimate concerns and must be discussed, the Commerce Department and the Department of Homeland Security maintain a much more significant benefit to defenders having transparency than any advantage given to attackers. There are bipartisan bills in support SBOMs. There are ways to reduce the risk that SBOMs will fall into the wrong hands, such as secure or non-fungible contracts. The debates over these concerns will continue in the public sector, and more companies will adopt them.\n\nThis legislation is happening because the FDA has suggested that medical device manufacturers incorporate SBOMs as part of the review process, so SBOMs are gaining momentum from these manufacturers. The physical world is becoming more affected by software in medical devices, and embedded systems such as control systems for power plants, HVAC systems, airport controls, etc., so operational systems are at risk, with more significant consequences than business system attacks.\n\nA difficulty for OT professionals is that many of the industrial components are older and have not necessarily been updated. Still, hackers are reluctant to connect to the internet and update because that’s how hackers get in. The best answer to this problem is to try and gain transparency of the components in the stack, scan the system and the devices that are part of the OT network, and do some reverse engineering and decompiling to understand the details. Essentially, you need to create your SBOM and assess where the vulnerabilities are.\n\nThis is the primary area of work for Finite State. They look at the systems and devices and do an analysis. They have a platform that automates the embedded code analysis, provides a read-out of the vulnerabilities and identifies and groups the highest priority vulnerabilities. Sometimes you can knock out a whole category of openness with a single fix. Creating the SBOM by itself, then, is not good enough. It must be tied into a risk management system to wade through and sort the many vulnerabilities. Finding the highest priority risks is a complex process, and Finite State can help security teams prioritize their actions to protect their systems. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Eric Greenwald"],"link":"/episode-EDT91-en","image":"./episodes/edt-91/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solutions Architect, Intel, and Eric Greenwald, General Counsel of Finite State, talk about securing the software supply chain."},{"id":167,"type":"Episode","title":"Protecting the Outer Walls of your Castle with Vulnerability Scans","tags":["cybersecurity","endpointmanagement","vulnerability","cyberattack","iot","edge","censys"],"body":"\r\n\r\nMatt spent eight years as an intelligence officer with the army. In the latter part of his service, he helped build out the cyber mission forces as part of US Cyber Command. After finishing his service, he worked with Army Cyber and a few startups before coming to Censys.\n\nA typical vulnerability assessment requires an organization to provide what they are interested in, say a block list of IPs for a credential vulnerability scan. Then those vulnerabilities are discovered and can be patched. A Censys scan can provide a broad and deep scan on the internet from some basic information such as a domain and a few IPs, discovering everything else the organization owns and continually monitoring for those same aggregations and correlations between the data set. If, for example, the marketing department stands up an exposed website that doesn’t have TLS, Censys will catch it.  Censys takes an external view to know what’s available to an attacker at any given time of day.\n\nEven if, for example, someone deploys an application in a cloud, doesn’t put it on the domain, and uses obfuscation security, Censys will still detect it if it’s in a cloud environment the organization owns. Cloud connectors will discover new cloud instances that pop up. Some connective tissue, whether it’s WHOIS or DNS information, must correlate to some of those instances. Censys is continuing to get better at detecting these types of instances.\n\nThe analogy of a castle works well here. An organization does not just want to rely on what they can see internally with the cameras and sentries. They want a roving security patrol to catch threats before they even get to the castle walls. The patrol can observe the castle the way an attacker would. Censys patrols the internet from an outside perspective, seeing things the way a potential hacker would see them.\n\nOne tool that Censys is looking at is JARM from Salesforce, an active TLS fingerprinting tool.  When things are deployed that don’t match fingerprinting on a specific server, they will stand out as anomalies. It’s essential to look for architectures that should be in a particular configuration but are not.\n\nAn important question is how Censys can avoid helping bad players. First, the bad players are already seeing what Censys can see, perhaps not at that scale, so Censys is assisting organizations to even the playing field. External visibility at scale can help erase the vulnerabilities bad players can exploit. For example, Censys helped Ukraine identify vulnerabilities that the adversary could have used to shape the battlefield. Censys also has a good security team that will make tough calls about who they are comfortable doing business with and who can gain access.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matt Lembright"],"link":"/episode-EDT92-en","image":"./episodes/edt-92/en/thumbnail.png","lang":"en","summary":"On this episode, Darren talks to Matt Lembright, Director of Federal Applications, Censys, about how Censys assesses the attack surface for internet-connected devices, allowing organizations to eliminate vulnerabilities."},{"id":168,"type":"Episode","title":"The Four Superpowers of Digital Transformation","tags":["cybersecurity","aiml","multicloud","comms"],"body":"\r\n\r\nIntel focuses on ubiquitous computing, pervasive connectivity, cloud to the edge, and artificial intelligence (AI). Elements of these four superpowers are mandatory in digital transformation strategies for companies worldwide, so there is great synergy.\n\n## Ubiquitous Compute\n\nUbiquitous computing is a software engineering concept where computing is everywhere. Not only does it refer to smart personal devices such as phones, watches, and appliances, or data center infrastructure and cloud-based apps, but it’s the idea that enterprise infrastructure that has historically been behind a firewall is now coming into everyday life. Enterprise infrastructure is available to employees on campus, and anywhere they have a device. Even more importantly, the infrastructure can extend to customers to improve their experience.\n\nIntel is looking at what foundational elements should be driven through the ecosystem to achieve a goal of every person on earth having access to a petabyte of data or a petaflop of computing within less than a millisecond of access. Intel is working on the laws of physics, economy, and data sovereignty to make that possible. This will give software companies, cloud companies, and all the companies worldwide a framework for software that will create value for their customers.\n\n## Pervasive Connectivity\n\nIn a recent interview, Glenn Kurowski, the CTO of CACI, talked about connectivity and how he sees it unleashing into the space domain. A great example is when Elon Musk flew Starlink over Ukraine to keep them from going into the internet darkness.\n\nWith this tremendous computing power, connectivity is crucial. There is satellite connectivity, 5G, and 4G. Still, even within the internet, between states and countries, the question is, how do you traverse through networks and enable pools of computing that are not interconnected? Without connectivity, the value of computing rapidly declines.\n\nTo protect this interconnected data, Intel and teams of hundreds of technology companies are collaborating and innovating together. Governments must also understand the laws, rules, and concerns. Intel, for example, employs a sizable force to help lawmakers have those conversations to create laws that protect data. Intel’s ecosystem is vast, and people pay attention when Intel says this security is essential.\n\nEstonia is a good model of a country protecting their citizens’ data but also unleashing that data and providing more for their citizens at a lower cost. There will be a natural evolution to this model, as governments actively protect the privacy and think strategically.\n\n## Cloud to Edge\n\nOn the enterprise side is the extension of the surface that businesses can interact with their customers via the cloud to edge. A great example is Omnichannel retail, where a company knows who the customer is, their trends, and what they need. They can recommend additional services with pervasive connectivity from cloud to edge. They could tell the customer where something they are looking for is located when they arrive at the store. Retailers could extend their networks from a data center cloud or on-prem data center to unifying at the edge, creating a mesh network that goes all the way through the store.\n\nNot only would this improve the shopper experience, but the stores could decrease loss by detecting things that are out of stock and the flow within the store. They could place their products in the most advantageous places and monitor, for example, perishable goods to take actions to move products quickly while it’s at their peak.\n\n## Artificial Intelligence\n\nAI is an extension of data analytics and will inevitably grow. Mass amounts of data are created daily, and it is already beyond companies’ abilities to effectively compute. People are only looking at less than five percent of generated data.\n\nWith AI algorithms, it’s possible to find patterns with that data to, for example, cure cancer. It might be sitting there because the information is not yet in a spot where AI can use it. There will have to be a new market surrounding centralized, accessible data sets. An organized data brokerage could make centralized data available to multiple companies via the cloud while protecting data privacy, such as patient identification.\n\n## Security of Data\n\nCOVID quickly made ubiquitous computing important to employees working from home. That, and the subsequent hybrid workforce, exposed security flaws in the industry. There is currently a significant uptick in funding for security to address the problems and keep up with the expansion of the superpowers. Especially with edge-to-cloud architectures and ubiquitous computing, the attack surface has exploded. The industry can keep up but requires tremendous effort and forward-thinking.\n\nIntel has significant innovations in this space with software guard extensions, security features in silicon, and the ecosystem to take advantage of these things. The ecosystem can build new use cases such as confidential distributed analytics for cancer research or multi-domain analytics, which mean across unclassified, classified, and top-secret data. Before, that data could never mingle. Now it can mix securely and solve problems we couldn’t solve before.\n\n## Intel Software\n\nMany might be surprised that Intel has more than 19,000 software engineers. Intel can keep all of these engineers fully occupied as they operate at three levels: foundational software; languages, frameworks, tools, and libraries; and application-level work.\n\nMost of this technology on the application level is given away in the open-source community, where it is accessible, secure, and optimized.\n\nGo to http://embracingdigital.org for Intel resources related to the four superpowers. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Ernst"],"link":"/episode-EDT93-en","image":"./episodes/edt-93/en/thumbnail.jpg","lang":"en","summary":"On this episode, Darren discusses the four superpowers of digital transformation with Intel’s Greg Ernst, Corporate VP of Sales, Americas. Ubiquitous compute, Pervasive Connectivity, Cloud to Edge, Artificial Intelligence."},{"id":169,"type":"Episode","title":"the Rebirth of the Private Cloud","tags":["hybridcloud","microcloud","privatecloud","compute","technology","cloud","vergeio"],"body":"\r\n\r\nGreg started coding on the Commodore 64 when he was a kid and had never quit. He has worked on everything from 3D gaming engines to communications to database engines, web servers, and encryption. He developed interoperable communications software and hardware solutions for first responders after problems surfaced during 9/11. For example, police couldn’t talk to the fire department because everyone was on different systems.\n\nAfter a successful exit from that company, he started a new company to build a vertical search engine from scratch. He quickly realized the hard part was not the search engine or algorithms but infrastructure issues. He found that too much of the code was specific to hardware, and an abstraction layer was missing. This was the basis for what became Yottabyte, his company that started off solving the storage problem by putting together inexpensive drives and presenting it as a simplified platform for which you could write software without having to worry about what was under the hood. His vision then expanded to include computing, memory, and networking. Yottabyte recently became Verge.io.\n\nVerge.io's success is that everything was developed from scratch, even designing their programming language and writing their database engines so the software would do everything. It is not tied to any specific piece of hardware. The goal was simplicity: it’s not just a bunch of pieces stitched together to make a singular private cloud but an entire virtualized data center.\n\nThis lends reliability and security, as there are, for example, seamless updates, no dependency on specific hardware, and fewer vulnerable entry points. In addition, the target users of the system are IT generalists, not SAN experts or programmers.\n\nThe Verge.io platform gives the same experience that a customer might get from a public cloud: simplicity, self-service, and agility, but it has a considerable cost and a data gravity benefit. You are paying for things like IOP or egress with a public cloud. Many little prices start to add up, and you can become accountable to the ecosystem. Conversely, you can run Verge.io software, for example, on bare metal servers for cost savings and the ability to scale up or down quickly.  In a public cloud, you also give up a lot of control. With a private cloud, you keep your data closer to where it is being generated and can operate efficiently at the edge.\n\nPart of the reason service providers choose Verge.io is that they can manage their customers’ workloads without giving up the cost margin and reducing complications of moving them to the public cloud.\n\nOther customers’ needs are met because they can accomplish things that are very difficult to do with other software. For example, the University of Michigan has found vastly improved efficiency and ease of use. They have thousands of researchers, and when they get grant money, they need an environment that is HIPAA or CUI compliant. Before, every request required six to nine months to get the hardware deployed, installed, and certified. Verge.io has built an environment, and getting up and running is as simple as hitting a button. The user is handed a virtual enclave that is fully compliant and very secure because the enclave is encapsulated and isolated.\n\nAnother benefit is that Verge.io’s snapshotting has built-in business continuity and disaster recovery. You can pick up the snapshot and move it to completely different hardware architecture, and it will work the same. The way the snapshotting works allows for a clone copy, even if it’s ten petabytes, in under 30 milliseconds.\n\nBecause the data center can now easily migrate to clouds or a colocation center, business owners have a lot more flexibility in negotiating the price and performance of hardware. In addition, there is no downtime for hardware upgrades or refresh cycles. The system never shuts down.\n\nOne of the following areas for Verge.io is expanding into building out multi-cloud aggregation software with benefits such as a centralized management pane. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Campbell"],"link":"/episode-EDT94-en","image":"./episodes/edt-94/en/thumbnail.png","lang":"en","summary":"Darren Pulsipher, Chief Solutions Architect, Intel, discusses Verge.io’s software-defined data centers that simplify IT and make private cloud easy and efficient with Verge.io’s CTO, Greg Campbell. "},{"id":170,"type":"Episode","title":"Requirements for Edge to Cloud Architectures","tags":["data","edge","compute","cybersecurity"],"body":"\r\n\r\nAnna recently headed an effort to write a white paper on edge to cloud service architectures. Edge to cloud encompasses all the high points of technology that Intel cares about: AI, edge, cloud, and network connectivity. The purpose was to gather experts in these areas to discuss how Intel approaches edge cloud architectures and how these architectures can all connect back to the cloud. The focus was on the elements that matter as opposed to current technology that addresses the issues. This would provide a framework to talk about and look for the disconnects. An interesting revelation was that IT, OT, and network folks speak different technical languages with different taxonomies, among other challenges.\n\nOne lesson from these discussions was that communications are hypercritical, and there must be solid knowledge of your comms, especially at the edge. This dictates how much computing needs to be local and how often you can rely on the cloud. In the public sector, the added complexity of use cases must keep functionality when there are denied, disrupted, intermittent, and limited (DDIL) conditions. We need to rethink these architectures in cloud connectivity based on those limitations.\n\nCommunications and connectivity is the most significant difference between a cloud architecture, service infrastructure, and the edge. Many current tools incorrectly assume constant connectivity; if something is not connected, it is dead. That is not the case on edge. It’s evident in edge organizations such as the military. Still, even in industries such as telehealth, you have to assume there will not be good connectivity, for example, in the telehealth use case where you might be dependent on the patient’s home WiFi. The industrial space has similar requirements. Some can’t be offline because of critical controls for specific machinery or processes.\n\nSo, how can you have edge-centric compute that maintains all critical functionality with connectivity back to the cloud in an essentially intermittent fashion? There are architectures for this, but there is still much more to be done to have seamless operations when the connectivity might not be continuous. What can you keep doing, and what happens when everything is restored? There will be a disconnect with what’s been happening with the data. It becomes complex when you have to synchronize all this data at scale, perhaps with thousands of edge devices.\n\n## Security\n\nSecurity on edge is another area where there is always more work to be done. Traditional security measures such as authentication are still critically important, but devices are a huge attack surface, and their physical security is a different issue. Laptop security tends to be solid, and those measures must be applied to other edge devices. New AI developments will help determine whether the devices are in the location they should be in and detect anomalies in one of, say, ten thousand devices.\n\n## Application Development\n\nIn building applications, developers must understand the unique edge environment and develop them without the need to reprogram or bring in new middleware to run at the edge. Applications must be able to run with computing, power, and connectivity limitations. In addition, the edge can be in a new distributed compute setup, and the application needs to be designed to run off a mesh network with highly distributed compute. Decoupling the application from the hardware is a significant shift and getting more generic and less purpose-built. Still, the whole dialogue is moving toward getting data from anywhere and using it anywhere.\n\n## Data Management\n\nThe volume of data generated and collected at the edge is so great that it doesn’t make sense to send all that data to a data center to be processed. One of the reasons is the cost. In the United States, a private 5G network can be cost-effective for these enormous amounts of data, but the cost would be prohibitive for most organizations without private 5G.\n\nThe other issue is that most of the data is of no use. For example, when monitoring devices or applications, most data indicate that everything is running correctly when you only care about events that suggest things are not OK. AI algorithms are applied at the edge, decreasing the amount of irrelevant data being pushed back to the cloud for processing.\n\nThe traditional mode of operation for data management, copying everything to the data center, and running analytics there, doesn’t work well for the edge. Pushing applications out to the edge doesn’t always work, either. Intel has identified a few other data architectures or data ops. One is called data exchange, where there is a combination of moving data in secure enclaves only after it has been analyzed on edge, like batch processing. The other is called intelligent data streams, where SADE and SABR come into play. Data is only moved based on rules, and it is being streamed. It works in DDIL environments because it can determine the current operating environments and adjust.\n\n## Manageability\n\nSystems must be architected in a way that they can be maintained. You can’t deploy ten thousand devices and then regularly send a small army of people to check on those. IT has been traditionally cautious about not wanting to upgrade a working system. Still, it doesn't make sense to leave systems alone, especially with a fear of ransomware attacks on OT networks. Systems must be architected with a way to keep everything easily updated to have the robustness to deal with the security environment.\n\n## Availability\n\nEspecially in critical fields such as the military or healthcare, it is important to design systems with sufficient redundancy; it’s more of a systems approach. If individual components fail, the end goal still has to be met. That’s very different from what happens in the cloud, where it’s about keeping the infrastructure up.\n\nThe technology is not quite there yet, but it’s on the radar to design for multiple networks. If, for example, you use WiFi 6 preferentially that fails, the system can use 4G or another available network. The network must be solid as well as the compute. A standalone operation without the network side is fragile. If you go with hardwired costs, you get its higher limitations and lose your mobile applications.\n\nYou can find the white paper “Essential Requirements for Edge to Cloud Service Architectures” for more information at embracingdigital.org or intel.org.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Anna Scott"],"link":"/episode-EDT95-en","image":"./episodes/edt-95/en/thumbnail.png","lang":"en","summary":"Intel’s Darren Pulsipher, Chief Solutions Architect, and Dr. Anna Scott, Chief Edge Architect, Public Sector, discuss essential requirements for edge to cloud service architectures.   "},{"id":171,"type":"Episode","title":"Leveraging Virtual Datacenters with Verge.io","tags":["privatecloud","cloud","compute","technology","sdi","virtualdatacenter","vergeio"],"body":"\r\n\r\nAaron has been in IT for over 20 years. He is a pre-sales engineer, meaning he talks to customers about the value and features of Verge.io and helps them define their requirements.\n\nChris is the head of sales at Verge.io with 25 years of experience in tech software companies. He describes Verge.io as having a maniacal focus on customer satisfaction and success with the software.\n\nThe term “virtual data center” is for simplicity. The platform is multi-tenant, so there are standalone nested tenants with all the required resources inside of a data center, from CPU to memory to storage to networking, and have it isolated. Still, at the same time, you can build those on demand. Just like you can create a VM on demand, often with a template, you can do the same thing with a tenant where it encapsulates everything in the virtual data center. You can build through a recipe engine or from scratch. You can easily clone or take a snapshot of one of those tenants.\n\nThese virtual data centers can be large and complex. Verge.io’s largest customer runs over 60 nodes and multiple tenants in their environment. The only restrictions are what’s built in the backend Verge.io cluster. For example, say you have four terabytes of memory and 100 terabytes of storage, and 64 cores in your CPU. You could assign all those resources to that tenant, split it however you want, in half or thirds, and build the tenants that way.\n\nIf, for example, you allocated some nodes to development, some to test, and the rest to production, they would be different tenants, and you can snapshot between them and move into production. In this case, you can even have multiple production environments with blue-green updates.\n\nMany of Verge.io’s customers are managed, service providers. They use multi-tenancy for their end-user customers and, with that, dedicate zero trust secure environments. Customers can have their cloud environment, and they can provision virtual workloads as needed.\n\nThere are also significant use cases for enterprise-type IT customers that want, for example, blue-green dev-test environments or where they may have different environments with different security compliance requirements such as SOX or HIPAA. Verge.io has several large educational institution customers who are doing compliant research. They certify their cluster a single time; then, they can deliver a compliant research environment to one of their researchers in under an hour. Previously, it could take months to get up and running in a compliant environment.\n\nFor updates, although users must use tools at the application level or inside VM workloads, the environment “recipe” for things such as firewall rules or configuration settings on how resources are mapped out to workloads can be seamlessly updated. You can also take a VM in a running environment with new compliance and move it over to a new environment.\n\nVerge.io is also helpful for security. One of the largest quant firms in Europe is a good security use case. They take a picture of their entire environment and then run red and blue team drills against that, looking for security vulnerabilities, checking patches, etc.\n\nVDI can run in the environment. Verge.io partners with a company for VDI support. Verge.io controls the resources, the CPU, and the memory. They also support GPU and GPU passthrough and physical GPU. This is a significant use case for some customers, especially in engineering or oil and gas workloads. The virtual GPU makes for favorable economics because the cost is spread among several users.\n\nA perfect fit for Verge.io is edge use cases. A typical example is the point of sale. If a retail customer has a hundred stores, they might need two or three VM applications of VMs in each store. Since Verge.io has a small hardware footprint, once you have at least two servers, you can put that in the edge case data center and build those VMs. And then, with the snapshot and replications features, those configurations can copy and paste across all the different environments. You can update the latest configurations across all of them, not just OS patches but also firewall rules.\n\nAn exciting future space for Verge.io is in automated driving systems because of the volume of data. Many vendors are testing the vehicles in remote sites and physically shipping the hard drives. Imagine if the data could be processed on-site, fully redundant with the compelling cost associated with it, then the data could be transported in a wide area versus a disk and a truck.\n\nFor more information on Verge.io, visit http://verge.io.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Aaron Reid","Chris Lehman"],"link":"/episode-EDT96-en","image":"./episodes/edt-96/en/thumbnail.png","lang":"en","summary":"Intel’s Darren Pulsipher, Chief Solutions Architect, and https://www.verge.io/ Aaron Reid, Principal Systems Engineer, and Chris Lehman, Senior VP of Sales, discuss use cases for Verge.io’s virtual data center software."},{"id":172,"type":"Episode","title":"The Benefits of Graph Databses","tags":["data","graphdb","katanagraph"],"body":"\r\n\r\nHadi earned his Ph.D. in computer science in 2012 and researched cryptography and network information security. He worked in academia for a few years and then moved into industry, focused on different aspects of security solutions, including identity and access management. He began to learn more about graph modeling in 2015 and realized how graph data modeling could solve some of the exciting and complex problems in his field of study.\n\nIn graph databases, a graph does not mean charts or graphical interfaces but a way to structure data at the storage level so it can be retrieved and processed for some complex problems, especially if the data is interconnected. The graph offers many benefits and can complement existing data structures or solutions, such as relational database models or object storage.\n\nThe main difference between graph and relational databases is that while both are about relationships, relational databases take the relationships to the metadata and schema level, whereas graph databases are data-driven relationships. In other words, you are relating columns of tables in relational databases. To introduce a new relationship, you must change your schema. The graph provides a schema-less infrastructure where you can add more structure around your data but still be flexible so you can ingest any unstructured data.\n\nHalf of the world’s data has been created within the last few years, collected from many different sources, but less than two percent has been analyzed, most of which is structured data. The data is being collected, but the information is insufficient for processing. There must be a way to flexibly add a bit of structure that’s dynamic enough to change if you are uncertain but still benefit from the advanced optimized computation. The graph is an excellent way to do this.\n\nIf you are trying to work with correlated or interconnected data, as opposed to, for example, isolated data with critical values, a graph will offer benefits because of the relationships. Almost every industry can benefit because unstructured data usually comes from various sources and multiple natures.\n\nAn example would be cyber security solutions. There is data from logs and audit trains from the network environments, the cloud infrastructure, the endpoint hosts, etc. The data comes from different sources, such as directories or raw log files. It would be beneficial to correlate the data, for example, because typically, an identity or a user that could be part of a log from the identity management system could be the same user that triggers a process on a laptop such as downloading an attachment from an email. By analyzing those patterns, you can use this correlational linkage to get more insight. In other words, it doesn’t matter how or where the data comes from, but providing this linkage leads to learning about every record by looking at them in context.\n\nOne benefit is that there are no data transformation issues, so this increases speed. This also decreases the storage requirements.\n\nGraphs and relational database structures, in general, are compatible. Here is a simplistic social network example: The node types in the graph look like tables. So you can have one node called “person” and then a table called “person.” You might have another node called “location” and connect the person to a specific location. You can have a table called location and connect them to foreign keys. Then you have this relationship of friends. A friend of a person in a graph model is just a self-loop. That would allow you to model that schema. In a relational database, you would need to create a new table called friends and then connect it. So even at a schema level, you are adding redundancy and some structure on top of it. And if you need to add a new concept of friendship or relationship, you must create new tables, building redundancy and complexity.\n\nOther benefits of graphs over relational databases are graphs on elements, graph AI, and the idea of now model data to find patterns based on how the data is connected. You can decrease the data set that you are searching or analyzing because of the relationships. It’s using the power of data to empower the data even further. The algorithms in a graph database are very different than in relational databases and better optimized to get to large data sets faster.\n\nOne of the drawbacks of a graph database is that it is hard to scale. In a relational database, it’s easy to cut a table and put it on two servers, for example. Earlier graph databases were designed to be a single whole solution, so if you wanted to scale up, you would need to add more memory and CPU.\n\nNow, If you want to work with petabytes of data in graphs, you want to scale vertically as much as possible with technology such as Intel VMs, but you also want to scale horizontally. New technologies, such as Katana’s graph platform, help solve this scaling problem with distributed computing. You can split or divide the problem into pieces and have each work up a small part of the graph for a final solution. Katana has proven that you could use 256 machines or beyond to process data, so you can quickly get tens of terabytes of data in memory.\n\nA graph database requires a similar ecosystem as a relational database. Graph is a bit more dynamic and flexible. If you want to move to a graph analytics platform, which is beyond just the operational databases, you could take advantage of other things such as data warehousing and data lake capabilities. Storage and compute would be separate, meaning that graph processing technologies that do everything in memory don’t need to rely on storage attached to the services so that you can have a different storage service.\n\nKatana uses object storage, and then when they want to compute, they opportunistically load whatever they want from the graph to the distributed memory of all the machines. The data returns immutable to the storage, so if you, say, destroy the whole cluster, you don’t lose anything. All the data is already there and warehoused. Relational databases are a more mature field, but graph databases are becoming more well supported in the ecosystem.\n\nBased on Hadi’s eight years in the field talking to stakeholders and customers, all immediately see the benefit of graph databases. The limitations might be that they can’t keep up with scalability or expense. The work of Katana and other graph technology companies is to make graphs more of a commodity tool that customers can use for various tasks and less of a luxury in the database. For example, Katana is providing customers with graph-based identity and massive data management solutions.\n\nGood use cases of graphs would be the early invention of graph solutions specific to some companies such as LinkedIn and Facebook that have their social graphs. Now, a natural fit is in e-commerce for recommendation engines. Finding connections between customers, accounts, purchases, and other behaviors will enable better recommendations immediately to the shoppers in a way that can’t be done with relational database queries.\n\nTo find out more about Katana or how to contact Hadi, go to embracingdigital.org.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Madi Ahmadi"],"link":"/episode-EDT97-en","image":"./episodes/edt-97/en/thumbnail.jpg","lang":"en","summary":"Darren Pulsipher, Chief Solutions Architect, Intel, Dr. Hadi Ahmadi, Director of Solutions Architecture, Katana Graph discuss the benefits of graph databases. "},{"id":173,"type":"Episode","title":"The Emergence of the Global Data Network","tags":["dataarchitecture","datamanagement","data","technology","cloud","globaldatanetwork","macrometa","multicloud","datamesh"],"body":"\r\n\r\nChetan is an engineer turned operations and start-up guy (Macrometa is his fourth startup). He says he has been working on the same problem of dealing with distributed data and reducing latency for twenty years.\n\nData is no longer in a data center but everywhere: in the cloud, on the edge, and people’s laptops. Effectively managing all of that is a challenge.\n\nAbout ten years ago, Marc Andreessen said software is eating the world. At this point, the software has eaten everything and turned all kinds of constraints and barriers into opportunities. Multithreading computing is one of the barriers that has come down with the cloud. You can build applications that run in different parts of the world simultaneously. A developer movement is happening parallel to make everything as simple as it needs to be for the average computer science person. So on one side is a sophisticated technological evolution and on the other side is a simplicity movement.\n\nArchitectures such as Jamstack allow distributed computing to happen at scale with a great deal of simplicity, but there’s still a vast frontier yet to be discovered and claimed. The extensive land rush opportunity is now at the edge. Distributed data management and edge are two sides of the same coin.\n\nOne big problem is that some software development is moving to function as a service that ignores data. Also, there is a perception that data is ubiquitous, but much of the edge is not always connected. There is no guarantee that an application has access to all the data. Networks are no longer centralized; the notion of stateless microservices came from the cloud movement. This statelessness can become a huge barrier. This is why architectures such as Jamstack and serverless functions treat data as a peripheral issue rather than a core issue.\n\nStateless data structures are simple. You have a specific place where you commit your data, then move on to stateless again. Stateful requires robust infrastructures with more complex data structures because they support the application as it continually emits state. As we move into a real-time streaming data world in which state is constantly emitted from somewhere in the ecosystem, the infrastructure becomes complex and hard to manage because they are not architected. That’s where Macrometa comes in. They have built a new platform for this continuous, real-time active state at the exabyte scale.\n\nDealing with this streaming data in an active and dynamic state is a significant shift for many software developers. Since the first cloud infrastructures came about, then big data platforms, then data as a service, the industry has become efficient at ingesting, processing, and analyzing historical data. But now, we are in a world where data is on a spectrum rather than existing as a monolith. One newly appreciated quality is that data has perishable insight and value. Some data has a brief shelf life. Current time scales are too big to use data efficiently; we need systems within 50 milliseconds to communicate efficiently and reduce cognitive overhead for the people interacting with those systems.\n\nMost people misunderstand latency: it is not something that brings you joy, but the lack of it makes you upset. For example, how long will someone tolerate a choppy YouTube video or a slow-buffering Netflix show? Fifty milliseconds for a machine is an eternity. A machine can do a vast number of things in 50 milliseconds, so latency becomes essential, especially when considering the perishable value of data.\n\nAnother issue is now, because of the cloud, interconnectivity, and the global system, startups are multinational companies, and data becomes location sensitive. Some of the data is regulated, some are PII and can’t be exfiltrated in certain jurisdictions, etc. An excellent example of this problem is how the Europeans don’t want their data leaving their borders, but most of the cloud infrastructure in the U.S. and the applications are built here.\n\nA third issue is that data sits in many places; there are boundaries between systems, physical and logical. Data can be essentially static and rigid, so we need infrastructure that will allow data to connect and flow in real-time with consistency and ordering guarantees. Most importantly, it creates fungibility to be consumed quickly in diverse ways.\n\nAn additional problem is that data has a lot of noise, and it doesn’t make sense to backhaul intercontinental distances, paying transfer fees, only to draw most of it away. Data loses value by the time it gets to its destination. There is also a high refresh rate, so systems often work on stale data.\n\nWe need new ways of solving these types of distributed data problems. Chetan believes the next ten years will belong to this area of data sciences.\n\nThe first generation of distributed data solutions used operational transformation. Google Docs is an excellent example of that. Operational transformation, however, requires centralization of the control, so it doesn’t scale well. Google has figured out a way to scale, but that doesn’t generalize to the average developer. There are only maybe five companies in the world that understand it at that scale, and much of that knowledge is locked up in those companies and proprietary technology.\n\nMacrometa is working with the community and academia to try and create a new body of knowledge, far more efficient than these centralized models in a fully distributive way.\n\nCurrently, there are infrastructures available that are great at solving historical system of record-type problems. They are trying to move toward real-time data, but their architectures aren’t fundamentally meant for it. These new problems with data with time and location sensitivity, actuation value, refresh rates, data gravity, and data noise require a new way, a new infrastructure. Chetan calls this a system of interaction rather than a system of records because systems of interaction are data networks, close to where you originate and consume data, that then filter and rich augment all of it in line and route it to its intended recipients. It’s a networking function.\n\nMacrometa has built network processors that are moving the data around - a global data network. It’s a serverless API system where developers simply consume APIs to solve real-time active and operational data problems. Macrometa is a global data network in the topology of a CDM, but with a data platform like Snowflake that produces rich data primitives to deal with real-time active and operational data values.\n\nYou can integrate analytic tools into the global data network and deploy the analytics near where the data is generated or required. Just as Amazon fundamentally changed retail distribution with edge architecture and algorithms to keep local warehouses optimally stocked for overnight shipments, Macrometa has done the same for data. They are bringing the data and computation on that data much closer and allowing it to happen in milliseconds. This ability to create real-time loops of information is a powerful enabler. For example, small retailers can use local store inventory in their e-commerce without oversubscribing to compete with Amazon.\n\nA great use case for the Macrometa platform is in cybersecurity. Some customers are ripping out their centralized data models to take advantage of the lower latency so they can block threats in real-time.\n\nThe global data network is a transformation layer between your data sources and receivers with the consumers and publishers. It is composed of three technology pieces. The first is the global data mesh, which is the integration layer for data. The second is a global compute fabric that allows you to orchestrate data and business logic in the form of functions and containers globally. The third piece is a global privacy fabric: how to secure data and comply with different data regimes and regulations that affect whether your data is being transmitted or stored.\n\nThe global data mesh is a way to quickly and easily integrate data from different systems across boundaries, whether physical or logical. All of it is incorporated and flows with consistency and ordering guarantees. The most significant value of this mesh is that it makes data fungible and consumable by allowing you to put APIs on data quickly. This can be done in a few hours compared to usually taking months. The global data network is designed for trillions of events per second so that it can move data at vast scales at 90 percent less cost than the cloud.\n\nThe global compute fabric brings business logic and orchestration to move your processing closer to where your data originates or is consumed. This is the anti-cloud pattern. Macrometa will surgically and dynamically move those microservices that need to comply with data regulations, for example, into the right places for execution.\n\nThe last piece is data protection. This is a complex problem and the answers we have today, for example, opening a separate silo for that particular geo to comply with particulars every time you spin up an instance on your app, are not good. Macrometa’s platform has a data network that is already integrating and getting your data to flow across all the boundaries, along with compute functions and ingesting data without boundaries. Now, it can create logical boundaries and pin data to specific regions to protect data. They can set affinities and policies about how data lives and replicates in a region, such as whether it should be anonymized when it’s copied out of the region.\n\nMacrometa’s technology enables use cases that are impossible to do in the cloud because the clouds are too far or too slow. Macrometa has built the infrastructure to solve real-time data problems and turn them into opportunities instead of challenges. For more about Macrometa, go to macrometa.com. \n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Chetan Venkatesh"],"link":"/episode-EDT98-en","image":"./episodes/edt-98/en/thumbnail.jpeg","lang":"en","summary":"In this episode, Darren reminisces with Chetan Venkatesh, CEO of MacroMeta. Venkatesh has a long history of data management from the beginning days of Grid Computing and has started MacroMeta to tackle data management across the globally dispersed edge, data centers, and clouds. "},{"id":174,"type":"Episode","title":"Precog Cyber Attack PAth with XM Cyber","tags":["aiml","cybersecurity","xmcyber","technology","process"],"body":"\r\n\r\nPaul has been in security since the late nineties, getting his start by doing projects for the Department of Homeland Security and the Department of Defense. In 2005, he joined Fishnet Security doing sales engineering and has been tied into sales engineering. He joined XM Cyber to concentrate on breach and attack simulation.\n\nRather than traditional cybersecurity, which is detection, remediation, and prevention, XM Cyber is predictive. A good description is that it is a precog simulation. XM Cyber creates imaginative incidents to give you insight into how your tools might be able to address them and how you could work to remediate specific chokepoints. The idea is to do all of this before pen testing. You can fix things today, see the impact tomorrow, and then continually increase and improve your security.\n\nGoogle maps is a good analogy for how XM Cyber works. When you want to get from one place to the next, Google maps will tell you all the ways to get there, which avoids tolls, which is the most direct, etc…XM Cyber does the same thing but with an attack simulation. For example, suppose you have a compromised active directory user account. In that case, it will show you the six steps to be able to get to an on-prem domain controller and compromise that critical asset. It will also show all the different routes between those two points.\n\nA use case for this is that you can enable a red team to be super-efficient with this information because they don’t have to poke around and try to make discoveries. It can also help the blue section, allowing them to prioritize remediations on chokepoints. For example, if there are 400 attack paths all going to different areas in your DMZ, but all 400 seem to have to leverage this one entity to make that path happen, then you can fix that one problem and destroy all 400 paths. Blue teams can lock down those chokepoints that could enable the attacker.\n\nOne area that XM Cyber analyzes is identity management, not just in the data center but also in the cloud. Sometimes attack paths will be ten steps long, but nine steps will be navigating the identity world. For example, you might have permissions to your admin account, and then that admin account might have additional permissions. Do six or seven adjusting different permissions or resetting passwords and deploying GPOs. You could take nine steps from a standard user account to domain admin by leveraging the active directory.\n\nIn addition to identity, XM Cyber looks at over a hundred entities such as machines, S3 buckets, and SSH keys. These different entities can be combined to create an attack path. Sometimes it gets very complex. For example, an attack could start on-prem, go out to Azure, take advantage of Intune, and then go back over to compromise another machine that allows a pivot over to GCP. Once attackers are in the GCP environment, they can take advantage of trust or permission between AWS and GCP to compromise AWS. XM Cyber looks at all the different types of entities in disparate environments and connects them to assess these paths around how every entity holistically plays together in the risk of all the others.\n\nThere are two ways XM Cyber engages with customers. The first is high-level discovery to assess the environment, expose vulnerabilities, and measure how an attacker can expose new vulnerabilities to put critical assets at risk. The second is a targeted assessment of a specific scenario that the customer is worried about. These engagements are not just static analysis of entities. They are dynamic because they look at traffic and other patterns.\n\nA typical targeted use case is determining if OT is the critical asset or the breach point. XM Cyber plays out scenarios such as if a machine in HR is the breach point, is there any risk to this PLC sitting in the SCADA environment controlling pressure switches that could turn off the electricity for a city municipality. That is an actual use case that XM Cyber can simulate. This type of information is critical in a world where OT is no longer isolated but connected to networks.\n\nXM Cyber is a SaaS solution rather than on-prem, so they can stay dynamic and deliver the best service. It can be scary to think that something in the cloud has all your attack techniques. Still, XM Cyber does a lot of work to ensure that data is completely isolated, SOC 2 compliant, among other certifications, and there is no multi-tenancy. They also do not collect anything sensitive. Sensitive information is hashed, and only a portion is sent to the cloud. They don’t have to have actual data.\n\nOnce XM Cyber has found the problematic pathways, they can also help you remediate them via customer success managers who have weekly or biweekly meetings where the goal is to take the data coming out of the platform and help you use it and find solutions. They also have partners such as managed service partners who bolt the attack simulations onto their platforms to give them insight into offering their services.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Paul Giorgi"],"link":"/episode-EDT99-en","image":"./episodes/edt-99/en/thumbnail.png","lang":"en","summary":"Intel’s Darren Pulsipher, Chief Solutions Architect, and Paul Giorgi, Director of Sales Engineering, XM Cyber, discuss how XM Cyber technology can help organizations uncover attack paths and reduce risk."},{"id":175,"type":"News Brief","title":"2023-2-5","tags":["ai","compute","cybersecurity"],"body":"\n\n## Artificial Intelligence\n\nAccording to Forbes, by 2030, AI will potentially deliver $13 trillion to the global economy, or 16% of the world's \ncurrent GDP. \n\nChatGPT lets scammers craft emails that are so convincing they can get cash from victims without even relying on malware\nSome fear the powerful chatbot will make it far easier for non-coders to create malware and become cybercriminals\n\n[ChatGPT Cybersecurity Social Engineering](https://fortune.com/2023/02/03/chatgpt-cyberattacks-cybersecurity-social-engineering-darktrace-abnormal/)\n\nGoogle has invested almost $400 million in artificial intelligence startup Anthropic, which is testing a rival to OpenAI’s ChatGPT. https://interestingengineering.com/innovation/google-invests-anthropic-battle-chatgpt \nGoogle will be holding an event about how it’s “using the power of AI to reimagine how people search\" A 40-minute event will be streamed on YouTube on February 8th at 8:30AM ET.\n\nAn AI detection tool, that checks for chatGPT written content, says Macbeth written by William Shakespeare was \ngenerated by AI. I guess chatGPT was not trained in Old English. Guess it was not trained on old English (aka Shakespearean English)\n\n[https://venturebeat.com/ai/chatgpt-detection-tool-thinks-macbeth-was-generated-by-ai-what-happens-now/](https://venturebeat.com/ai/chatgpt-detection-tool-thinks-macbeth-was-generated-by-ai-what-happens-now/) \n\n## Ubiquitous Compute\n\nGlobal public cloud revenues continue to increase, with the projected 26% growth to hit $525 billion this year, according to Statista.\nSoftware as a service is projected to be $253 Billion of that revenue. However, as the growth of the cloud \ncontinues, cloud services providers are feeling the pain, as there have been several outages over the last 12 months.\nThis has been such a problem that multiple websites have popped up that monitor and report public cloud availability \nlike cloudharmony.com and thousandeyes.com.  \n\n* [https://cloudharmony.com/status](https://cloudharmony.com/status)\n* [https://www.thousandeyes.com/outages/](https://www.thousandeyes.com/outages/)\n* [https://www.statista.com/outlook/tmo/public-cloud/worldwide](https://www.statista.com/outlook/tmo/public-cloud/worldwide)\n\nOracle and Red Hat have teamed up to provide RHEL-based virtual machine instances in Oracle’s Cloud running on Intel,\nAMD, and Arm processors. This teaming between former competitors is providing customers with a one stop shop for\nworkloads that utilize oracle’s suite of products and traditional linux based workloads. This move continues to push\nOracle into competition with the big Hyper-scalers like Azure, AWS, and Google.\n\n* [https://www.networkworld.com/article/3686513/red-hat-enterprise-linux-arrives-in-oracle-s-cloud.html](https://www.networkworld.com/article/3686513/red-hat-enterprise-linux-arrives-in-oracle-s-cloud.html)\n\n## Cybersecurity\n\nE-commerce in South Korea and the U.S. are being attacked by an ongoing GuLoader malware campaign, cybersecurity firm \nTrellix disclosed late last month. The cyber attackers are shifting away from Microsoft Word document approach to NSIS (Nullsoft Scriptable Install \nSystem) an open source tool for writing installers on the Windows Operating system.\n\n[GuLoader Malware Using Malicious NSIS](https://thehackernews.com/2023/02/guloader-malware-using-malicious-nsis.html)\n\nPro-Russian hackers are using a new DDoS-as-a-Service Platform named Passion to target medical instituitons in the U.\nS., Portugal, Spain, Germany, Poland, Finland, Norway, the Netherlands, and the U.K. This software as a service \noffering is a subscription servivce allowing customers to select their desired attack vectors, duration and intensity.\n\n[Russian Cybercriminals Launch New Passion Attack Platform](https://cyware.com/news/russian-cybercriminals-launch-new-passion-attack-platform-798d8713)\n\nThe US Cybersecurity and Infrastructure Security Agency (CISA) on Thursday announced the release of a Stakeholder-Specific Vulnerability Categorization (SSVC) guide that can help organizations prioritize vulnerability patching using a decision tree model.\nThe SSVC system was created in 2019 by CISA and Carnegie Mellon University’s Software Engineering Institute (SEI), and a year later CISA developed its own customized SSVC decision tree for security flaws relevant to government and critical infrastructure organizations.\n\n[CISA Releases Decsiion Tree Model](https://www.securityweek.com/cisa-releases-decision-tree-model-help-companies-prioritize-vulnerability-patching/)\n\n## Embracing Digital Transformation Podcast\n\nThis week Darren Pulsipher interviews chatGPT. [Check it out here](https://www.embracingdigital.org/episodes-EDT122).\n## Artificial Intelligence\n\nAccording to Forbes, by 2030, AI will potentially deliver $13 trillion to the global economy, or 16% of the world's current GDP. ChatGPT lets scammers craft emails that are so convincing they can get cash from victims without even relying on malware Some fear the powerful chatbot will make it far easier for non-coders to create malware and become cybercriminals\n\n[Read More](https://fortune.com/2023/02/03/chatgpt-cyberattacks-cybersecurity-social-engineering-darktrace-abnormal/)\n\nGoogle has invested almost $400 million in artificial intelligence startup Anthropic, which is testing a rival to OpenAI’s ChatGPT. Google will be holding an event about how it’s “using the power of AI to reimagine how people search\" A 40-minute event will be streamed on YouTube on February 8th at 8:30AM ET.\n\n[Read More](https://interestingengineering.com/innovation/google-invests-anthropic-battle-chatgpt )\n\nAn AI detection tool, that checks for chatGPT written content, says Macbeth written by William Shakespeare was generated by AI. I guess chatGPT was not trained in Old English. Guess it was not trained on old English (aka Shakespearean English)\n\n[Read More](https://venturebeat.com/ai/chatgpt-detection-tool-thinks-macbeth-was-generated-by-ai-what-happens-now/)\n\n## Ubiquitous Compute\n\nGlobal public cloud revenues continue to increase, with the projected 26% growth to hit $525 billion this year, according to Statista. Software as a service is projected to be $253 Billion of that revenue. However, as the growth of the cloud continues, cloud services providers are feeling the pain, as there have been several outages over the last 12 months. This has been such a problem that multiple websites have popped up that monitor and report public cloud availability like cloudharmony.com and thousandeyes.com.  \n\n[Read More](https://cloudharmony.com/status)\n\n[Read More](https://www.thousandeyes.com/outages/)\n\n[Read More](https://www.statista.com/outlook/tmo/public-cloud/worldwide)\n\nOracle and Red Hat have teamed up to provide RHEL-based virtual machine instances in Oracle’s Cloud running on Intel, AMD, and Arm processors. This teaming between former competitors is providing customers with a one stop shop for workloads that utilize oracle’s suite of products and traditional linux based workloads. This move continues to push Oracle into competition with the big Hyper-scalers like Azure, AWS, and Google.\n\n[Read More](https://www.networkworld.com/article/3686513/red-hat-enterprise-linux-arrives-in-oracle-s-cloud.html)\n\n## Cybersecurity\n\nE-commerce in South Korea and the U.S. are being attacked by an ongoing GuLoader malware campaign, cybersecurity firm Trellix disclosed late last month. The cyber attackers are shifting away from Microsoft Word document approach to NSIS (Nullsoft Scriptable Install System) an open source tool for writing installers on the Windows Operating system. \n\n[Read More](https://thehackernews.com/2023/02/guloader-malware-using-malicious-nsis.html)\n\nPro-Russian hackers are using a new DDoS-as-a-Service Platform named Passion to target medical instituitons in the U. S., Portugal, Spain, Germany, Poland, Finland, Norway, the Netherlands, and the U.K. This software as a service offering is a subscription servivce allowing customers to select their desired attack vectors, duration and intensity. \n\n[Read More](https://cyware.com/news/russian-cybercriminals-launch-new-passion-attack-platform-798d8713)\n\nThe US Cybersecurity and Infrastructure Security Agency (CISA) on Thursday announced the release of a Stakeholder-Specific Vulnerability Categorization (SSVC) guide that can help organizations prioritize vulnerability patching using a decision tree model. The SSVC system was created in 2019 by CISA and Carnegie Mellon University’s Software Engineering Institute (SEI), and a year later CISA developed its own customized SSVC decision tree for security flaws relevant to government and critical infrastructure organizations.\n\n[Read More](https://www.securityweek.com/cisa-releases-decision-tree-model-help-companies-prioritize-vulnerability-patching/)\n\n## Embracing Digital Transformation Podcast\n\nThis week Darren Pulsipher interviews chatGPT. \n\n[Read More](https://www.embracingdigital.org/episodes-EDT122)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW1-en","image":"./briefs/edw-1/en/thumbnail.png","lang":"en","summary":"For the week of February 6, 2023. News from around the world of digital transformation in artificial intelligence, cloud computing, and cybersecurity."},{"id":176,"type":"News Brief","title":"2023-4-9","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Artificial Intelligence\n\nYet another thing parents need to be worried about. AI voice cloning! In a recent virtual abduction from a cyber bad actor, the mother of a 15 year old grow was called and spoofed using her daughter's voice to announce her abduction. Followed by ‘I’ve got your daughter’. Quick thinking, the mom texted her husband to check on the location of their daughter who was safe. The FBI is investigating this and several other instances.\n\n[https://www.azfamily.com/2023/04/10/ive-got-your-daughter-scottsdale-mom-warns-close-encounter-with-ai-voice-cloning-scam/](https://www.azfamily.com/2023/04/10/ive-got-your-daughter-scottsdale-mom-warns-close-encounter-with-ai-voice-cloning-scam/)\n\nEugenia Kuyda is the founder of Replika, a US chatbot app that says it offers users an \"AI companion who cares, always here to listen and talk, always on your side\". Launched in 2017, it now has more than two million active users. Each has a chatbot or \"replika\" unique to them, as the AI learns from their conversations. Users can also design their own cartoon avatar for their chatbot.\n\n[https://www.bbc.com/news/business-65110680](https://www.bbc.com/news/business-65110680)\n\nNVIDIA, Getty Images Collaborate on Generative AI. The companies aim to develop two generative AI models using NVIDIA Picasso, part of the new NVIDIA AI Foundations cloud services. Users could employ the models to create a custom image or video in seconds, simply by typing in a concept. Joint effort aims to customize text-to-image and text-to-video foundation models to spawn stunning visuals using fully licensed visual content.\n\n[https://blogs.nvidia.com/blog/2023/03/21/generative-ai-getty-images/?ncid=so-link-748862&=&linkId=100000197541685#cid=gtcs23_so-link_en-us](https://blogs.nvidia.com/blog/2023/03/21/generative-ai-getty-images/?ncid=so-link-748862&=&linkId=100000197541685#cid=gtcs23_so-link_en-us)\n\n## Ubiquitous Compute\n\nIs COBOL finally dead? GFT and Cloud Frame are joining forces to help organizations move their COBOL programs to more efficient platforms. COBOL was developed in 1959 as one of the first multi-vendor-supported languages for mainframe business users.  It has become increasingly expensive to debug and maintain COBOL systems due to the lack of Programmers that still work with COBAL.\n\n[https://www.cloudcomputing-news.net/news/2023/apr/04/gft-and-cloudframe-help-industries-say-cheerio-to-cobol/](https://www.cloudcomputing-news.net/news/2023/apr/04/gft-and-cloudframe-help-industries-say-cheerio-to-cobol/)\n\nLarge language models (LLMs) are all the rage, with ChatGPT leading the way. This is only good for cloud service providers as LLMs require large amounts of computing and data storage for deployment and development. Not to be left behind, cloud service providers Azure, AWS, and Google have all announced their own LMS solutions to compete in this new battleground for Cloud dominance. Let’s ask ChatGPT who the winner will be.\n\n[https://www.infoworld.com/article/3693330/large-language-models-are-the-new-cloud-battleground.html](https://www.infoworld.com/article/3693330/large-language-models-are-the-new-cloud-battleground.html)\n\nGoogle has thrown its hat into the ring of confidential computing with a new offering providing complete privacy of sensitive data by encrypting data at rest in transit and now in use. Azure and AWS have similar product offerings that provide confidential computing to public and private sector customers.\n\n[https://www.wired.com/story/google-cloud-confidential-virtual-machines/](https://www.wired.com/story/google-cloud-confidential-virtual-machines/)\n\n## Cyber Security\n\nIn an interesting DDoS attack, threat actors inundated npm, an open-source package repository for Node.js, by creating malicious websites and publishing empty packages with links to those malicious websites to take advantage of the ecosystem’s good reputation on search engines. Over 1.42 million bogus packages were uploaded.\n\n[https://thehackernews.com/2023/04/hackers-flood-npm-with-bogus-packages.html](https://thehackernews.com/2023/04/hackers-flood-npm-with-bogus-packages.html)\n\nOver 1,000,000 WordPress sites are estimated to be infected by an ongoing campaign to deploy Balada Injector malware. The attacks are known to play out in waves once every few weeks. The attacks redirect random subdomains to various scam sites, including websites with fake tech support. The best way to combat this is to update your plugins on your WordPress sites.\n\n[https://thehackernews.com/2023/04/over-1-million-wordpress-sites-infected.html](https://thehackernews.com/2023/04/over-1-million-wordpress-sites-infected.html)\n\nThe Iranian nation-state group, MuddyWater has been carrying out destructive attacks on hybrid cloud environments under the guise of ransomware operations. The threat actors are masquerading as a standard ransomware campaign but are essentially destroying and disrupting critical IT operations. Findings have shown that MuddyWater has collaborated with DEV-1084 to pull off these attacks.\n\n[https://thehackernews.com/2023/04/iran-based-hackers-caught-carrying-out.html](https://thehackernews.com/2023/04/iran-based-hackers-caught-carrying-out.html)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW10-en","image":"./briefs/edw-10/en/thumbnail.png","lang":"en","summary":"Summary"},{"id":177,"type":"News Brief","title":"2023-4-16","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## CyberSecurity\n\nThe FBI has warned the public against using public phone charging stations due to the risk of cyber-attacks. These charging stations, commonly found in public places such as airports, malls, and hotels, could potentially contain malware that can compromise the security of users' personal information and devices.  Bring your own USB power adapter and you should be good to go.\n\n[https://www.cnbc.com/2023/04/10/fbi-says-you-shouldnt-use-public-phone-charging-stations.html](https://www.cnbc.com/2023/04/10/fbi-says-you-shouldnt-use-public-phone-charging-stations.html)\n\nGoogle has released an urgent update for its Chrome web browser to address critical vulnerability hackers exploit. The vulnerability, identified as a \"use-after-free\" bug, could allow malicious actors to execute arbitrary code and potentially gain control over affected systems. Google has urged users to update their Chrome browsers to the latest version as soon as possible to mitigate the risk of being targeted by cyber-attacks.\n\n[https://thehackernews.com/2023/04/google-releases-urgent-chrome-update-to.html](https://thehackernews.com/2023/04/google-releases-urgent-chrome-update-to.html)\n\nAndroid and Novi survey apps have been found to have severe security vulnerabilities that could potentially expose users' personal information. The flaws could allow attackers to bypass security measures and gain unauthorized access to sensitive data. Users are urged to be cautious and update their apps to the latest versions to protect against potential cyber threats.\n\n[https://thehackernews.com/2023/04/severe-android-and-novi-survey.html](https://thehackernews.com/2023/04/severe-android-and-novi-survey.html)\n\n## Data Management\n\nAccording to OneTrust's Chief Strategy Officer, data privacy and ethics gaps pose an existential threat to organizations. Failure to adequately address these gaps can result in reputational damage, legal and financial liabilities, and loss of customer trust. Organizations must prioritize data privacy and ethics to mitigate risks and protect their businesses in the face of evolving regulatory requirements and increasing public scrutiny.\n\n[https://deloitte.wsj.com/articles/data-privacy-ethics-gaps-an-existential-threat-says-onetrust-cso-01668542698](https://deloitte.wsj.com/articles/data-privacy-ethics-gaps-an-existential-threat-says-onetrust-cso-01668542698)\n\nAfter an in-depth review of recent reports from the Government Accountability Office of the United States, Andrew Kuoh, a principal at Capgemini, has identified three key actions data organizations in the public sector must focus on. Fostering a data-driven culture, establishing data governance frameworks, and leveraging modern technologies such as cloud computing and artificial intelligence (AI) to harness the value of data. (People, Process, and Technology sound familiar)\n\n[https://www.informationweek.com/government/3-actions-to-kickstart-data-ecosystems-in-the-public-sector](https://www.informationweek.com/government/3-actions-to-kickstart-data-ecosystems-in-the-public-sector)\n\nA recent webinar from a DBTA panelist discussed the importance of keeping up with the evolving needs of databases and applications in the cloud. As cloud computing advances, organizations must adapt their database and application strategies to optimize performance, security, and scalability. Emphasizing cloud-native technologies, automation, and best practices can help businesses stay ahead in today's dynamic cloud environment.\n\n[https://www.dbta.com/Editorial/News-Flashes/Keeping-Up-with-the-Evolving-Needs-of-Databases-and-Applications-in-the-Cloud-158166.aspx](https://www.dbta.com/Editorial/News-Flashes/Keeping-Up-with-the-Evolving-Needs-of-Databases-and-Applications-in-the-Cloud-158166.aspx)\n\n## Intelligent Edge\n\nEdge Computing Expo North America, scheduled for 17-18 May 2023, has recently announced a star-studded lineup of technologists and business leaders in the IOT space. The conference will be held at the Santa Clara Convention Center in California. You have got to put this one on your calendar.\n\n[https://www.iot-now.com/2023/04/14/129661-edge-computing-expo-north-america-announces-speaker-line-up-hear-from-leading-experts-in-edge-computing/](https://www.iot-now.com/2023/04/14/129661-edge-computing-expo-north-america-announces-speaker-line-up-hear-from-leading-experts-in-edge-computing/)\n\nIT and OT cybersecurity convergence is becoming real as more cybersecurity solutions from the IT space focus on IoT and industrial infrastructure protection. A recent example is CrowdStrike’s introduction of CloudStrike Falcon Insight for IoT. This platform brings the same tooling across IoT, IT endpoints, and cloud & data center workloads.\n\n[https://www.iot-now.com/2023/04/17/129671-crowdstrike-brings-xdr-for-iot-offering-to-deliver-protection-to-iot-assets/](https://www.iot-now.com/2023/04/17/129671-crowdstrike-brings-xdr-for-iot-offering-to-deliver-protection-to-iot-assets/)\n\nIn a great example of taking on Industry 4.0 deployments. Volvo Group has taken on deploying a preventative maintenance IT infrastructure in its factory in Lyon, France. The Volvo Group is utilizing a Long-Range Wide Area Network (LoRaWAN) to enable real-time monitoring and analysis of data from factory equipment allowing for proactive preventative maintenance.\n\n[https://www.edgecomputing-news.com/2023/04/03/volvo-group-uses-lorawan-for-predictive-maintenance-in-lyon-factory/](https://www.edgecomputing-news.com/2023/04/03/volvo-group-uses-lorawan-for-predictive-maintenance-in-lyon-factory/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW11-en","image":"./briefs/edw-11/en/thumbnail.png","lang":"en","summary":"Summary"},{"id":178,"type":"News Brief","title":"2023-4-23","tags":["ai","edge","cybersecurity"],"body":"\n\n## Artificial Intelligence\n\nAccording to MarketWatch, with an expected annualized growth rate of almost 27%, AI hardware,  a $10 billion industry in 2021, is expected to be an $89 billion industry by 2030.\n\n[https://www.marketwatch.com/story/nvidia-is-ai-hardwares-leader-now-but-intel-amd-and-others-are-closing-fast-8ad6f23f](https://www.marketwatch.com/story/nvidia-is-ai-hardwares-leader-now-but-intel-amd-and-others-are-closing-fast-8ad6f23f)\n\nDefense Advanced Research Projects Agency (DARPA) multi-million dollar GARD (Guaranteeing AI Robustness Against Deception) project, which has three key goals: Develop the algorithms that protect machine learning from vulnerabilities; Develop theories to ensure AI algorithms are defensible against attacks; and share the tools broadly.\n\n[https://www.zdnet.com/in-depth/innovation/these-experts-are-racing-to-protect-ai-from-hackers-time-is-running-out/](https://www.zdnet.com/in-depth/innovation/these-experts-are-racing-to-protect-ai-from-hackers-time-is-running-out/)\n\nTom Brady threatened to sue comedians behind the AI standup video. Using data from interviews with Tom Brady and hundreds of thousands of hours of stand-up comedy footage the Dudsey branh simulated an hour-long stand-up comedy special.\n\n[https://nypost.com/2023/04/20/tom-brady-threatened-to-sue-comedians-over-ai-standup-video/](https://nypost.com/2023/04/20/tom-brady-threatened-to-sue-comedians-over-ai-standup-video/)\n\n## Cybersecurity\n\nA new report from CybelAngel has revealed the top five cybersecurity exposures that pose critical risks to organizations. The report identifies the main threats of cloud misconfigurations, supply chain vulnerabilities, ransomware attacks, phishing scams, and unpatched software. The report urges organizations to mitigate these risks and safeguard their systems and data proactively.\n\n[https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures/](https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures/)\n\nEuropean air traffic control agency Eurocontrol has revealed that it experienced a cyber attack from Russia earlier this month. The attack targeted the agency's systems, but Eurocontrol was able to contain and remediate the incident without any disruption to air traffic. The agency has warned other organizations to be vigilant and strengthen their defenses against cyber threats.\n\n[https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures](https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures)\n\nA critical vulnerability has been discovered in INEA's industrial control system (ICS) product, which could allow remote attackers to take over affected systems and cause disruption or damage to industrial operations. The flaw, which affects all product versions, was discovered by researchers at Claroty. The company has urged organizations using the product to update to the latest version as soon as possible to mitigate the risk of exploitation.\n\n[https://www.securityweek.com/critical-flaw-in-inea-ics-product-exposes-industrial-organizations-to-remote-attacks/](https://www.securityweek.com/critical-flaw-in-inea-ics-product-exposes-industrial-organizations-to-remote-attacks/)\n\n## Intelligent Edge\n\nKneron, a provider of edge artificial intelligence (AI) solutions, has acquired Otus, a maker of imaging solutions for autonomous vehicles. The acquisition will enable Kneron to leverage Otus' expertise in developing compact, power-efficient cameras for edge AI applications. The acquisition is expected to accelerate the adoption of edge AI in the transportation industry.\n\n[https://www.edgecomputing-news.com/2023/04/20/kneron-buys-otus-for-edge-ai-imaging-in-autonomous-vehicles/](https://www.edgecomputing-news.com/2023/04/20/kneron-buys-otus-for-edge-ai-imaging-in-autonomous-vehicles/)\n\nSolo.io has launched Gloo Fabric, a secure multi-cloud service mesh platform for enterprise applications, supporting hybrid and multi-cloud environments, with a centralized dashboard for monitoring, managing and troubleshooting applications.\n\n[https://www.edgeir.com/gloo-fabric-by-solo-io-promises-secure-multi-cloud-discovery-and-connectivity-for-enterprises-20230421](https://www.edgeir.com/gloo-fabric-by-solo-io-promises-secure-multi-cloud-discovery-and-connectivity-for-enterprises-20230421)\n\nCanada's government and Ericsson will invest CA$470m ($376m) in 5G and 6G R&D over five years to create a sustainable and secure telecommunications infrastructure, develop new use cases and applications including smart cities and connected vehicles.\n\n[https://www.edgeir.com/government-of-canada-ericsson-announce-ca470-million-investment-for-5g-6g-rd-20230420](https://www.edgeir.com/government-of-canada-ericsson-announce-ca470-million-investment-for-5g-6g-rd-20230420)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW12-en","image":"./briefs/edw-12/en/thumbnail.png","lang":"en","summary":"Summary"},{"id":179,"type":"News Brief","title":"2023-4-30","tags":["ai","cybersecurity","edge"],"body":"\n\n## Artificial Intelligence\n\nThe AI backlash begins: Fans of Snapchat are expressing their discontent with using ChatGPT-powered bots on the platform, marking a backlash against AI. Users criticize the lack of human-like responses and the inability to differentiate between actual and AI-generated content. The incident highlights growing concerns and the need for a balanced implementation of AI in social media platforms.\n\n[https://www.techradar.com/news/the-ai-backlash-begins-snapchat-fans-revolt-against-chatgpt-powered-bot](https://www.techradar.com/news/the-ai-backlash-begins-snapchat-fans-revolt-against-chatgpt-powered-bot)\n\nAI-equipped eyeglasses read non-verbal/silent speech. Cornell University researchers have developed a silent-speech recognition interface that uses acoustic sensing and artificial intelligence to continuously recognize up to 31 unvocalized commands based on lip and mouth movements. The low-power, wearable interface -- EchoSpeech -- requires just a few minutes of user training data before it recognizes commands and can run on a smartphone. I swear my Grandparents knew how to do this 40 years ago. They communicated well with just a head nod, a smirk, and a smile.\n\n[https://news.cornell.edu/stories/2023/04/ai-equipped-eyeglasses-can-read-silent-speech](https://news.cornell.edu/stories/2023/04/ai-equipped-eyeglasses-can-read-silent-speech)\n\nCan AI chatbots replace bosses? A startup named Aesthetic explores the potential of AI-powered chatbots in taking over managerial tasks, offering guidance, answering questions, and managing workflows. While efficient, some argue that human qualities like empathy are irreplaceable. Is the combination of human and AI interaction the best approach for effective leadership in the workplace?\n\n[https://startup.outlookindia.com/sector/saas/can-your-boss-be-replaced-by-an-ai-chatbot--news-8257](https://startup.outlookindia.com/sector/saas/can-your-boss-be-replaced-by-an-ai-chatbot--news-8257)\n\n## CyberSecurity\n\nThe Cybersecurity and Infrastructure Security Agency (CISA) is seeking public comments on a draft directive requiring federal agencies to attest to software security practices. The directive aims to enhance the security of software supply chains, and the comment period is open until May 3, 2023.\n\n[https://www.cybersecuritydive.com/news/cisa-public-comment-software-security-attestation/648932/](https://www.cybersecuritydive.com/news/cisa-public-comment-software-security-attestation/648932/)\n\nRansomware attackers have evolved tactics, as revealed in a CrowdStrike report, resorting to coercive methods to extort payments from victims. In addition to encrypting data, these tactics involve threatening physical harm, exposing stolen information to the media, and even targeting victims' families. Organizations need to stay vigilant and adapt their defenses to counter these new strategies to mitigate the impact of ransomware attacks and protect their valuable data from exploitation.\n\n[https://cyware.com/news/coercion-in-the-age-of-ransomware-new-tactics-for-extorting-payments-0c31dba6](https://cyware.com/news/coercion-in-the-age-of-ransomware-new-tactics-for-extorting-payments-0c31dba6)\n\nCold storage provider Americold experienced a network breach that led to a widespread outage affecting multiple systems. The company confirmed the incident but did not disclose the nature of the breach or the extent of the impact. As a precautionary measure, Americold temporarily shut down specific systems and engaged external cybersecurity experts to investigate the breach. Customer data is believed to be secure, and the company is working to restore full functionality.\n\n[https://www.bleepingcomputer.com/news/security/cold-storage-giant-americold-outage-caused-by-network-breach/](https://www.bleepingcomputer.com/news/security/cold-storage-giant-americold-outage-caused-by-network-breach/)\n\n## Intelligent Edge\n\nAkamai Technologies has acquired NeoSec, a cybersecurity startup focused on API security, to strengthen its capabilities in detecting and responding to API-based attacks. The acquisition will enhance Akamai's security offerings and provide customers with improved protection against threats targeting application programming interfaces. Akamai aims to address the evolving cybersecurity landscape by expanding its security portfolio and offering enhanced solutions to combat API-related risks.\n\n[https://www.edgecomputing-news.com/2023/04/24/akamai-acquires-neosec-to-bolster-api-detection-and-response/](https://www.edgecomputing-news.com/2023/04/24/akamai-acquires-neosec-to-bolster-api-detection-and-response/)\n\nZadara and Kasten by Veeam have partnered to deliver a comprehensive data protection solution for Kubernetes environments. Combining Zadara's zCompute, zStorage, and Kasten's K10 platform, the solution offers backup, disaster recovery, and application mobility. It enables seamless application movement across Kubernetes clusters, including Zadara's Global Edge Cloud locations. The collaboration addresses the need for agile, cost-effective storage solutions, protecting cloud-native applications in Kubernetes while supporting various databases. Additionally, Zadara recently launched its c9 Flex-N Infrastructure-as-a-Service Platform in Japan in collaboration with BroadBand Tower.\n\n[https://www.edgeir.com/zadara-kasten-by-veeam-unite-to-provide-multi-tier-data-protection-for-kubernetes-20230428](https://www.edgeir.com/zadara-kasten-by-veeam-unite-to-provide-multi-tier-data-protection-for-kubernetes-20230428)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW13-en","image":"./briefs/edw-13/en/thumbnail.png","lang":"en","summary":"News and stories from the Week of May 01, 2023, in Digital Transformation, including cyberattacks and intelligent edge, non-verbal communication AI, and company merges in the IoT space."},{"id":180,"type":"News Brief","title":"2023-5-7","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Data Management\n\nDIQ, a new data trust index tool, promotes data democratization. The tool measures the trustworthiness of data sources, enabling organizations to make informed decisions about data usage. By providing transparency and accountability, DIQ seeks to empower businesses in their data-driven strategies and enhance trust in data-driven decision-making.\n\n[https://tdwi.org/articles/2023/05/04/diq-all-data-trust-index-tool-to-drive-data-democratization.aspx\n](https://tdwi.org/articles/2023/05/04/diq-all-data-trust-index-tool-to-drive-data-democratization.aspx\n)\n\nFIATA and the Global Shippers Forum are advocating for more robust data governance standards in the shipping industry. They highlight the need to address data quality, accessibility, and security challenges to enhance supply chain efficiency. Establishing consistent standards aims to improve data sharing, collaboration, and decision-making among stakeholders, ultimately benefiting the global shipping community.\n\n[https://www.porttechnology.org/news/fiata-global-shippers-forum-push-for-stronger-data-governance-standards/\n](https://www.porttechnology.org/news/fiata-global-shippers-forum-push-for-stronger-data-governance-standards/\n)\n\nDatabricks Ventures has invested in Immuta; a company focused on strengthening lakehouse governance. Immuta's platform provides data access and control solutions, enabling organizations to effectively govern and secure their data lakehouse environments. This investment aims to enhance data governance capabilities within the Databricks ecosystem, promoting data privacy, compliance, and security for enterprises utilizing lakehouse architectures.\n\n[https://www.databricks.com/blog/strengthening-lakehouse-governance-ecosystem-databricks-ventures-invests-immuta\n](https://www.databricks.com/blog/strengthening-lakehouse-governance-ecosystem-databricks-ventures-invests-immuta\n)\n\n## Artificial Intelligence\n\nThe lead engineer behind Google's advanced chatbot, Geoffrey Hinton, has resigned due to concerns about AI technology's potential risks and ethical implications. Hinton believes that the chatbot system he developed can be used to spread misinformation and fake news, emphasizing the need for responsible AI development and deployment to protect society.\n\n[https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\n](https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\n)\n\nTech companies want to be paid for the data powering AI models. Chatbots Are Digesting the Internet, and content providers want to get paid. Artificial intelligence companies are using content created by millions of people without their consent or compensation. Reddit, an invaluable source for OpenAI, recently announced that it would start charging AI companies for access to data. Twitter is also doing the same. OpenAI declined to comment.\n\n[https://www.wsj.com/articles/chatgpt-ai-artificial-intelligence-openai-personal-writing-5328339a\n](https://www.wsj.com/articles/chatgpt-ai-artificial-intelligence-openai-personal-writing-5328339a\n)\n\nAI is starting to displace humans in back office work. IBM CEO announces a temporary halt on hiring for back-office jobs as the company invests in AI technology. The decision reflects IBM's strategic shift towards leveraging artificial intelligence to streamline operations and improve efficiency. The move signals the company's commitment to adapting to technological advancements and the evolving demands of the market.\n\n[https://fortune.com/2023/05/01/ibm-ceo-ai-artificial-intelligence-back-office-jobs-pause-hiring/\n](https://fortune.com/2023/05/01/ibm-ceo-ai-artificial-intelligence-back-office-jobs-pause-hiring/\n)\n\n## Ubiquitous Computing\n\nVMware has introduced Cross-Cloud Managed Services, a new offering to simplify and streamline multi-cloud management. The service provides customers a unified platform to manage various cloud environments, offering enhanced visibility, security, and governance capabilities. With this solution, VMware aims to help organizations overcome the complexities of multi-cloud operations and optimize their cloud strategies for improved efficiency and agility.\n\n[https://www.cloudcomputing-news.net/news/2023/may/04/vmware-unveils-cross-cloud-managed-services/\n](https://www.cloudcomputing-news.net/news/2023/may/04/vmware-unveils-cross-cloud-managed-services/\n)\n\nAWS (Amazon Web Services) has developed a new service that provides secure access to cloud applications without traditional VPNs (Virtual Private Networks). The service, AWS Client VPN, utilizes AWS' global network infrastructure to establish secure connections between users and cloud resources. This approach simplifies remote access while maintaining high levels of security, making it easier for organizations to manage and secure their cloud environments effectively.\n\n[https://www.networkworld.com/article/3695174/aws-secures-access-to-cloud-apps-without-using-vpns.html\n](https://www.networkworld.com/article/3695174/aws-secures-access-to-cloud-apps-without-using-vpns.html\n)\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW14-en","image":"./briefs/edw-14/en/thumbnail.png","lang":"en","summary":"News in Digital Transformation for the week of May 8, 2023 including "},{"id":181,"type":"News Brief","title":"2023-5-14","tags":["ai","edge","cybersecurity"],"body":"\n\n## Artificial Intelligence\r\n\nThe Irish Times fell victim to a hoax when it published an article by an artificial intelligence (AI) program. The AI-generated piece went unnoticed and was posted online, highlighting the challenges of detecting machine-generated content. This incident raises concerns about the potential for misinformation and the need for effective verification processes in the face of AI-generated content.\r\n\n[https://www.lemonde.fr/en/economy/article/2023/05/17/the-irish-times-duped-by-a-hoax-article-written-by-an-artificial-intelligence-program_6026930_19.html\r](https://www.lemonde.fr/en/economy/article/2023/05/17/the-irish-times-duped-by-a-hoax-article-written-by-an-artificial-intelligence-program_6026930_19.html\r)\n\nEurope is leading in establishing guidelines for artificial intelligence (AI) development. The European Union is developing comprehensive regulations to ensure AI is transparent, accountable, and respects human rights. These measures aim to address ethical concerns and potential risks associated with AI, making Europe a global frontrunner in shaping responsible AI implementation.\r\n\n[https://www.nbcnews.com/tech/tech-news/europe-leading-world-building-guardrails-ai-rcna83912\r](https://www.nbcnews.com/tech/tech-news/europe-leading-world-building-guardrails-ai-rcna83912\r)\n\nA new study suggests that generative artificial intelligence (AI) technology has the potential to augment the working hours of healthcare professionals by up to 40%. By automating routine tasks and providing decision support, AI could increase efficiency and allow doctors and nurses to focus on more critical and complex patient care.\r\n\n[https://www.healthcareitnews.com/news/generative-ai-could-augment-40-healthcare-working-hours\r](https://www.healthcareitnews.com/news/generative-ai-could-augment-40-healthcare-working-hours\r)\n\n## Cybersecurity\r\n\nThe Cybersecurity and Infrastructure Security Agency (CISA) has warned about a critical vulnerability in Ruckus wireless access points that is being exploited to infect Wi-Fi networks. The bug allows attackers to gain unauthorized access and execute arbitrary code remotely. CISA advises organizations using Ruckus access points to apply the necessary patches and implement mitigation measures to protect against potential attacks.\r\n\n[https://www.bleepingcomputer.com/news/security/cisa-warns-of-critical-ruckus-bug-used-to-infect-wi-fi-access-points\r](https://www.bleepingcomputer.com/news/security/cisa-warns-of-critical-ruckus-bug-used-to-infect-wi-fi-access-points\r)\n\nSensitive data belonging to 237,000 U.S. government employees have been exposed due to a security breach. The compromised information includes Social Security numbers, contact details, and employment information. The breach was attributed to an unauthorized individual accessing an employee's email account. Authorities are investigating the incident, and affected individuals are being notified.\r\n\n[https://www.yahoo.com/news/data-237-000-us-government-232707971.html\r](https://www.yahoo.com/news/data-237-000-us-government-232707971.html\r)\n\nA severe security flaw has been discovered, exposing over 2 million sensitive records on the WordPress site. The vulnerability in the WordPress plugin \"Essential Addons for Elementor\" allows unauthorized access to personal data, including names, addresses, and financial information. The flaw affects widely used software, posing a significant risk to users' privacy and security. Developers are working on a patch, and users are advised to update their systems promptly.\r\n\n[https://thehackernews.com/2023/05/severe-security-flaw-exposes-over.html\r](https://thehackernews.com/2023/05/severe-security-flaw-exposes-over.html\r)\n\n## Edge Computing\r\n\nA recent report by UL Solutions highlights artificial intelligence (AI) and the Internet of Things (IoT) at the edge as crucial technologies for organizations utilizing 5G networks. The combination of AI and IoT at the edge enables real-time data processing, advanced analytics, and automation, unlocking new possibilities for healthcare, manufacturing, and transportation industries. The report emphasizes the potential of these technologies in driving innovation and efficiency in the 5G era.\r\n\n[https://www.edgeir.com/report-reveals-ai-iot-edge-as-key-technologies-for-organizations-leveraging-5g-20230512\r](https://www.edgeir.com/report-reveals-ai-iot-edge-as-key-technologies-for-organizations-leveraging-5g-20230512\r)\n\nKyndryl, an IT services company, is expanding its offerings by introducing a managed Secure Access Service Edge (SASE) service. SASE combines network security and wide-area networking capabilities into a unified cloud-based solution. By incorporating SASE into its portfolio, Kyndryl aims to provide enhanced security and connectivity for businesses operating in a hybrid or multi-cloud environment. The move aligns with the growing demand for comprehensive and simplified security solutions in the evolving IT landscape.\r\n\n[https://www.sdxcentral.com/articles/analysis/why-kyndryl-is-adding-a-managed-sase-service/2023/05/\r](https://www.sdxcentral.com/articles/analysis/why-kyndryl-is-adding-a-managed-sase-service/2023/05/\r)\n\nThe Edge AI market is predicted to experience substantial growth, with its size expected to increase significantly in the coming years. Factors such as the proliferation of IoT devices, advancements in AI technology, and the need for real-time data processing drive this growth. Edge AI enables intelligent decision-making at the network’s edge, reducing latency and enhancing efficiency. The market's expansion presents opportunities for various industries, including healthcare, manufacturing, and retail.\r\n\n[https://finance.yahoo.com/news/edge-ai-market-size-predicted-190000466.html\r](https://finance.yahoo.com/news/edge-ai-market-size-predicted-190000466.html\r)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW15-en","image":"./briefs/edw-15/en/thumbnail.png","lang":"en","summary":"News from the week of May 15, 2023 in digital transformation including stories from Edge Computing, Cybersecurity, and Artificial Intelligence."},{"id":182,"type":"News Brief","title":"2023-5-21","tags":null,"body":"\n\n## Ubiquitous Computing\r\n\nAmazon Web Services (AWS) plans to invest $12.7 billion in India's cloud infrastructure market by 2030, supporting job creation and renewable energy projects. The investment aims to meet rising customer demand and contribute to India's digital transformation while addressing infrastructure challenges.\r\n\n[https://www.cloudcomputing-news.net/news/2023/may/22/aws-to-put-13-billion-into-india-cloud-infrastructure-by-2030/\r](https://www.cloudcomputing-news.net/news/2023/may/22/aws-to-put-13-billion-into-india-cloud-infrastructure-by-2030/\r)\n\nSnowflake, a cloud data platform provider, has opened a new office in the UK to expand across the EMEA (Europe, Middle East, and Africa) region. The move comes in response to the company's strong momentum in the market and aims better to serve its growing customer base in the area.\r\n\n[https://www.cloudcomputing-news.net/news/2023/may/16/snowflake-opens-uk-office-amid-strong-momentum-across-emea/\r](https://www.cloudcomputing-news.net/news/2023/may/16/snowflake-opens-uk-office-amid-strong-momentum-across-emea/\r)\n\nMicrosoft is preparing to launch its Microsoft Cloud for Sovereignty solution, targeting government and public-sector organizations. The industry cloud solution has undergone private previews to address sector-specific challenges. Microsoft emphasizes transparency, data policies, and security, with plans to offer platform solutions across various industries.\r\n\n[https://www.ciodive.com/news/Microsoft-industry-cloud-data-sovereignty-platform/650214/\r](https://www.ciodive.com/news/Microsoft-industry-cloud-data-sovereignty-platform/650214/\r)\n\n## Advanced Communications\r\n\nIntel has unveiled its Agilex FPGA, a programmable chip designed for smart networking. The Agilex FPGA offers advanced features, including AI acceleration and enhanced security, making it suitable for various applications. The chip is expected to empower network infrastructure with increased flexibility and performance.\r\n\n[https://www.networkworld.com/article/3697156/intel-launches-agilex-fpga-for-smart-networking.html\r](https://www.networkworld.com/article/3697156/intel-launches-agilex-fpga-for-smart-networking.html\r)\n\nEthernet, the pioneering networking technology, celebrates its 50th anniversary. Despite its impressive journey so far, Ethernet's evolution continues. With advancements like faster speeds, greater capacity, and enhanced reliability, Ethernet remains vital in connecting the digital world. As technology advances, Ethernet is poised to play a crucial role in shaping the future of networking.\r\n\n[https://www.networkworld.com/article/3697013/ethernet-turns-50-but-its-voyage-has-only-begun.html\r](https://www.networkworld.com/article/3697013/ethernet-turns-50-but-its-voyage-has-only-begun.html\r)\n\nOpen-source solutions are simplifying the management of complex network fabrics, according to Network Computing. With the increasing complexity of modern networks, open-source tools and frameworks provide flexibility, interoperability, and automation capabilities. These solutions enable organizations to streamline fabric management, reduce costs, and improve efficiency. Open-source-based approaches are becoming crucial for simplifying the complexities of network fabrics in today's digital landscape.\r\n\n[https://www.networkcomputing.com/networking/simplifying-complex-fabrics-open-source-based-solutions\r](https://www.networkcomputing.com/networking/simplifying-complex-fabrics-open-source-based-solutions\r)\n\n## Data Management\r\n\nIDM.net.au suggests exploring data governance automation for five reasons: improved data quality, streamlined compliance, enhanced productivity, increased data visibility, and adaptability to the evolving data landscape. Automation reduces errors, ensures compliance, boosts efficiency, provides control, and keeps organizations competitive.\r\n\n[https://idm.net.au/article/0014302-5-reasons-explore-data-governance-automation-opportunities\r](https://idm.net.au/article/0014302-5-reasons-explore-data-governance-automation-opportunities\r)\n\nTDWI.org discusses how data experts tackle web scraping challenges. They emphasize the significance of selecting appropriate tools and techniques to extract data from websites. Overcoming hurdles like anti-scraping measures and dynamic content necessitates proxies, user agents, and data structuring expertise. Successful web scraping enables valuable data acquisition for analysis and decision-making.\r\n\n[https://tdwi.org/articles/2023/05/18/diq-all-how-data-experts-overcome-web-scraping-challenges.aspx\r](https://tdwi.org/articles/2023/05/18/diq-all-how-data-experts-overcome-web-scraping-challenges.aspx\r)\n\nIn a recent report, Dataversity.net warns about poor security architecture in cloud environments. The article identifies common mistakes, including weak access controls and misconfigurations, that can lead to data breaches. Emphasizing the importance of strong authentication and monitoring, it stresses the need for robust security measures to safeguard sensitive data in cloud architectures.\r\n\n[https://www.dataversity.net/cloud-architecture-mistakes-the-perils-of-poor-security-architecture/\r](https://www.dataversity.net/cloud-architecture-mistakes-the-perils-of-poor-security-architecture/\r)\n\n\n\n","guests":null,"link":"/brief-EDW16-en","image":"./briefs/edw-16/en/thumbnail.png","lang":"en","summary":"Digital Transformation news for the week of May 29, 2023. In this episode."},{"id":183,"type":"News Brief","title":"2023-5-28","tags":null,"body":"\n\n## Artificial Intelligence\n\nOpenAI's CEO, Sam Altman, warns that the company may withdraw its services from the EU due to concerns over the planned AI Act's regulations. This highlights a growing transatlantic rift on AI control. US tech firms, including Google, are preparing for a potential clash with European regulators over AI regulation.\n\n[https://www.ft.com/content/5814b408-8111-49a9-8885-8a8434022352](https://www.ft.com/content/5814b408-8111-49a9-8885-8a8434022352)\n\nMicrosoft President Brad Smith voiced concerns over deep fakes and called for regulations to combat foreign cyber-influence operations. He advocated licensing critical AI, implementing export controls, and holding individuals accountable for AI-related issues. OpenAI CEO Sam Altman emphasized the need for global cooperation and safety compliance.\n\n[http://https//www.reuters.com/technology/microsoft-chief-calls-humans-rule-ai-safeguard-critical-infrastructure-2023-05-25/](http://https//www.reuters.com/technology/microsoft-chief-calls-humans-rule-ai-safeguard-critical-infrastructure-2023-05-25/)\n\nOpenAI is launching a grant program to explore democratic inputs to AI decision-making. They seek diverse perspectives in shaping AI behavior and aim to develop innovative processes for public oversight. Selected teams will receive grants to conduct experiments and publish findings to establish democratic governance for AI.\n\n[https://openai.com/blog/democratic-inputs-to-ai](https://openai.com/blog/democratic-inputs-to-ai)\n\n## Cybersecurity\n\nNorth Korean threat actor, Kimsuky, has enhanced its reconnaissance malware, RandomQuery, according to cybersecurity firm Cyware. The malware targets government, military, and defense organizations, collecting sensitive information. It utilizes new techniques and evades detection to conduct covert espionage operations. Vigilance and strong security measures are crucial to mitigate the threat.\n\n[https://cyware.com/news/north-korea-actor-kimsuky-updates-its-reconnaissance-malware-randomquery-25cb1d1e](https://cyware.com/news/north-korea-actor-kimsuky-updates-its-reconnaissance-malware-randomquery-25cb1d1e)\n\nSecurity researchers have discovered a sophisticated Android spyware named \"Predator\" that has been active since 2021. The spyware targets government and military entities in South Asia, including India, Pakistan, and Afghanistan. It can steal sensitive data, record audio, and video, and perform real-time location tracking. Users are advised to update their devices and be cautious of suspicious apps.\n\n[https://thehackernews.com/2023/05/predator-android-spyware-researchers.html](https://thehackernews.com/2023/05/predator-android-spyware-researchers.html)\n\nMicrosoft caught Chinese government hackers in a campaign called Volt Typhoon, targeting critical infrastructure organizations in Guam, a U.S. territory. The campaign aimed to disrupt the communications infrastructure between the U.S. and Asia. CISA issued a warning, and Microsoft advised on mitigating the threat.\n\n[https://www.securityweek.com/microsoft-catches-chinese-gov-hackers-in-guam-critical-infrastructure-orgs/](https://www.securityweek.com/microsoft-catches-chinese-gov-hackers-in-guam-critical-infrastructure-orgs/)\n\n## Edge Computing\n\nDell Technologies introduces Dell NativeEdge, a software platform simplifying and securing Zero-Trust edge deployments. It enables streamlined edge operations, zero-touch deployment, and multi-cloud application orchestration. The platform aims to enhance efficiency and connectivity at the edge, supporting various industries and use cases.\n\n[https://www.edgecomputing-news.com/2023/05/24/dell-nativeedge-software-transforms-edge-operations/](https://www.edgecomputing-news.com/2023/05/24/dell-nativeedge-software-transforms-edge-operations/)\n\nNTT is strengthening its collaboration with Cisco to expand managed private networks for enterprise IoT, prioritizing sustainability and ease of use. The partnership utilizes Cisco's IoT hardware, enabling actionable insights through AI and ML. The deal builds on their previous collaboration for Cisco's Private 5G solution, offering IoT as a Service, including integrated services and infrastructure management.\n\n[https://www.sdxcentral.com/articles/interview/ntt-doubling-down-on-cisco-to-boost-managed-iot/2023/05/](https://www.sdxcentral.com/articles/interview/ntt-doubling-down-on-cisco-to-boost-managed-iot/2023/05/)\n\nCloudflare, a leading-edge services provider, focuses on artificial intelligence (AI) after reporting strong growth in its edge services during the first quarter of 2023. The company aims to leverage AI to enhance its offerings and improve customer experience, particularly in DDoS mitigation and security. Cloudflare's emphasis on AI aligns with its strategy to deliver innovative edge solutions to a growing customer base.\n\n[https://www.edgeir.com/cloudflare-leaning-into-ai-after-1q23-results-show-good-growth-for-edge-services-20230525](https://www.edgeir.com/cloudflare-leaning-into-ai-after-1q23-results-show-good-growth-for-edge-services-20230525)\n## Artificial Intelligence\n\n\n\n","guests":null,"link":"/brief-EDW17-en","image":"./briefs/edw-17/en/thumbnail.png","lang":"en","summary":"News for Embracing Digital for the week of May 29, 2023, learn about more regulations for AI, increased nation-state cyber attacks, and edge computing investments."},{"id":184,"type":"News Brief","title":"2023-6-4","tags":["ai","compute","cybersecurity"],"body":"\n\n## Artificial Intelligence\n\nAccording to a report, JPMorgan, a banking giant, has advertised over 3,600 AI-related jobs, reflecting Wall Street's increasing interest in revolutionary technology. The move highlights the industry's growing reliance on artificial intelligence for various functions, signaling a shift in the financial sector towards embracing AI and its potential benefits.\n\n[https://www.msn.com/en-us/money/other/banking-giant-jpmorgan-advertised-more-than-3-600-ai-related-jobs-report-says-as-wall-street-starts-to-embrace-the-revolutionary-tech/ar-AA1bYZwL](https://www.msn.com/en-us/money/other/banking-giant-jpmorgan-advertised-more-than-3-600-ai-related-jobs-report-says-as-wall-street-starts-to-embrace-the-revolutionary-tech/ar-AA1bYZwL)\n\nOpenAI has developed a method to enhance the logical reasoning abilities of AI models, reducing instances of \"hallucinations\" or generating incorrect information. By modifying the training process, the research team could produce models that exhibit improved consistency and avoid making up details. This development contributes to creating more reliable and trustworthy AI systems.\n\n[https://www.zdnet.com/article/openai-found-a-way-to-make-ai-models-more-logical-and-avoid-hallucinations/](https://www.zdnet.com/article/openai-found-a-way-to-make-ai-models-more-logical-and-avoid-hallucinations/)\n\nJapan defies copyright laws by allowing AI training with any data, aiming to accelerate its AI progress and compete globally. Concerns from artists are countered by support from academia and businesses, as Japan seeks access to Western data in exchange for its cultural resources. A unique twist to the regulation debate emerges.\n\n[https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/](https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/)\n\n## Ubiquitous Computing\n\nDell strengthens multi-cloud strategies with advancements in its APEX portfolio. The offerings include Dell APEX Cloud Platforms for Azure, Red Hat OpenShift, VMware, and storage solutions for public clouds. Dell APEX expands to provide compute resources and PC-as-a-Service while partnering with Databricks to enable data analysis across on-premises and cloud environments. Not to be left behind, HPE offers their green lake product line multi-cloud offerings.\n\n[https://www.cloudcomputing-news.net/news/2023/jun/01/dell-apex-portfolio-advancements-help-customers-strengthen-multicloud-strategies/](https://www.cloudcomputing-news.net/news/2023/jun/01/dell-apex-portfolio-advancements-help-customers-strengthen-multicloud-strategies/)\n\nA recent report from information week predicts that worldwide spending on cloud services will reach $1.3 trillion by 2025, about 16.9% growth. However, many organizations still need help with the ROI of their investments. The report identifies critical areas to improve ROI in Cloud deployments, including FinOps, adopting multi-hybrid cloud strategies, DevOps and platform engineering, and cloud security. \n\n[https://www.informationweek.com/cloud/leaders-should-pay-attention-to-these-4-major-cloud-trends](https://www.informationweek.com/cloud/leaders-should-pay-attention-to-these-4-major-cloud-trends)\n\nMicrosoft Azure DevOps experienced a ten-hour outage in the South Brazil region due to a simple typo that caused the deletion of seventeen production databases. The error occurred during a code upgrade, and a typo in the pull request led to the deletion of the entire server. The data has been recovered, but the recovery process took more than ten hours due to various complications. Microsoft has implemented fixes and reconfigurations to prevent similar issues in the future.\n\n[https://www.theregister.com/2023/06/03/microsoft_azure_outage_brazil/](https://www.theregister.com/2023/06/03/microsoft_azure_outage_brazil/)\n\n## Cyber Security News\n\nA cybersecurity expert warns that scammers target Gmail users by exploiting a collaboration feature. Users receive fraudulent invitations that redirect them to malicious sites. Vigilance, verification of requests, enabling two-factor authentication, and monitoring account settings are recommended to protect against such scams.\n\n[https://www.wmur.com/article/nh-cybersecurity-expert-gmail-feature-is-being-hacked-by-scammers/44083493](https://www.wmur.com/article/nh-cybersecurity-expert-gmail-feature-is-being-hacked-by-scammers/44083493)\n\nAccording to a survey by the CISO Hall of Fame, cloud security is the top concern for IT professionals. The report highlights the increasing reliance on cloud services and the need to address security challenges such as data breaches and unauthorized access. Key focus areas include identity and access management, encryption, proactive threat detection to ensure robust cloud security measures, and finding cybersecurity talent with cloud service experience.\n\n[https://thehackernews.com/2023/06/cloud-security-tops-concerns-for.html](https://thehackernews.com/2023/06/cloud-security-tops-concerns-for.html)\n\nIn a recent article, Walmart opened up to talk about its best practices, hoping to raise the knowledge of its ecosystem of suppliers and partners. Walmart's security operations center and defense-in-depth approach exemplify best practices, but smaller companies require practical strategies. Aligning security protocols, implementing strong identity and access management, and reviewing access policies can enhance protection. Next-gen security technologies offer cost-effective solutions for mitigating cyber threats.\n\n[https://betanews.com/2023/06/03/walmart-cybersecurity/](https://betanews.com/2023/06/03/walmart-cybersecurity/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW18-en","image":"./briefs/edw-18/en/thumbnail.png","lang":"en","summary":"Digital transformation news for June 6, 2023. This week more jobs in AI, major cyber security breaches, and cloud technology best practices."},{"id":185,"type":"News Brief","title":"2023-6-11","tags":["ai","compute","cybersecurity"],"body":"\n\n## Artificial Intelligence\n\nMicrosoft has made its powerful OpenAI technology available to US government cloud customers. The move allows agencies to utilize advanced AI capabilities in language processing, machine learning, and natural language understanding. The partnership aims to support government entities in their digital transformation efforts.\n\n[https://www.bloomberg.com/news/articles/2023-06-07/microsoft-offers-powerful-openai-technology-to-us-government-cloud-customers](https://www.bloomberg.com/news/articles/2023-06-07/microsoft-offers-powerful-openai-technology-to-us-government-cloud-customers)\n\nRomania's Prime Minister has appointed the world's first AI government adviser. The AI system, named DORA, will assist in decision-making processes, analyze data, and provide recommendations on various policy matters. This groundbreaking move reflects the increasing integration of AI technology in governmental operations and aims to enhance efficiency and effectiveness in governance.\n\n[https://www.euronews.com/next/2023/03/06/romanias-prime-minister-has-hired-the-worlds-first-ai-government-adviser-what-will-it-do](https://www.euronews.com/next/2023/03/06/romanias-prime-minister-has-hired-the-worlds-first-ai-government-adviser-what-will-it-do)\n\nAI outperforms humans in discovering efficient sorting algorithms and integrating them into C++ libraries. AlphaDev, a deep reinforcement learning agent, surpassed human benchmarks by formulating the problem as a game and selecting CPU instructions to create optimal algorithms. The study highlights the potential of AI in algorithm optimization.\n\n[https://www.nature.com/articles/s41586-023-06004-9](https://www.nature.com/articles/s41586-023-06004-9)\n\n## Ubiquitous Computing\n\nMicrosoft 365, the popular suite of productivity tools, suffered widespread outages, causing disruptions for millions of users. The service interruptions affected various components, including Outlook, Teams, and SharePoint. Microsoft acknowledged the issue and said their engineers were working to resolve it.\n\n[https://www.theregister.com/2023/06/06/microsoft_365_outages/](https://www.theregister.com/2023/06/06/microsoft_365_outages/)\n\nCloud service outages are increasing due to geopolitical tensions and internet vulnerabilities. Cross-border data flow restrictions and cyber threats contribute to disruptions. Experts call for cooperation and investment to enhance cloud service resilience and protect global connectivity.\n\n[https://fortune.com/2023/06/07/cloud-outages-on-the-rise-tech-geopolitics-internet/](https://fortune.com/2023/06/07/cloud-outages-on-the-rise-tech-geopolitics-internet/)\n\nThe African cloud market is set for significant growth by 2023, driven by digital transformation and increased adoption of cloud services. Improved internet connectivity and rising demand for cloud-based solutions in various sectors contribute to the expansion. Key players invest in infrastructure and partnerships to seize opportunities in the growing African cloud market.\n\n[https://finance.yahoo.com/news/rise-african-cloud-market-2023-082300194.html](https://finance.yahoo.com/news/rise-african-cloud-market-2023-082300194.html)\n\n## Cybersecurity\n\nTo help combat the shortage of skilled cybersecurity professionals, Accenture introduces \"Skills to Succeed in Cybersecurity,\" a free program to fill one million entry-level cybersecurity jobs. The initiative offers comprehensive training and certification resources, aiming to bridge the skills gap and encourage more individuals to pursue careers in cybersecurity.\n\n[https://fortune.com/education/articles/accenture-launches-free-cybersecurity-upskilling-program-in-effort-to-fill-1-million-entry-level-jobs/](https://fortune.com/education/articles/accenture-launches-free-cybersecurity-upskilling-program-in-effort-to-fill-1-million-entry-level-jobs/)\n\nThe White House extends secure software attestation deadlines and issues clarifying guidance to enhance cybersecurity practices. The extended timelines provide more compliance time, while the direction offers insights into implementation and evaluation processes for federal agencies and contractors. So, for now, are our applications and services unknowingly vulnerable?\n\n[https://federalnewsnetwork.com/cybersecurity/2023/06/white-house-extends-secure-software-attestation-deadlines-offers-clarifying-guidance/](https://federalnewsnetwork.com/cybersecurity/2023/06/white-house-extends-secure-software-attestation-deadlines-offers-clarifying-guidance/)\n\nThe National Cybersecurity Strategy urges substantial changes in safeguarding critical infrastructure. Collaboration between the public and private sectors is emphasized to combat evolving cyber threats. Risk management, resilience, and investment in advanced technologies are highlighted to fortify critical infrastructure and ensure national security.\n\n[https://federalnewsnetwork.com/commentary/2023/06/national-cybersecurity-strategy-calls-for-significant-change-in-critical-infrastructure/](https://federalnewsnetwork.com/commentary/2023/06/national-cybersecurity-strategy-calls-for-significant-change-in-critical-infrastructure/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW19-en","image":"./briefs/edw-19/en/thumbnail.png","lang":"en","summary":"Digital Transformation for the week of June 12, 2023, includes a 1 million cyber force development, many cloud outages, and AI writing code going into the C++ standard library."},{"id":186,"type":"News Brief","title":"2023-2-12","tags":null,"body":"\n\n## Artificial Intelligence\n\nAI mistakes: Blame it on the inaccuracies in the input datasets.\n\n* Alphabet's shares dropped by $100 billion after Google's AI chatbot, Bard, shared inaccurate information in a \npromotional video, and a company event failed to impress. During a live event, Bard stumbled, prompting Google to \n  pull the live stream, which fueled concerns that the tech giant is losing ground to its rival, Microsoft. Reuters \n  reported that Bard credited the James Webb Space Telescope for taking the first pictures of a planet outside the \n  solar system. In reality, the first pictures were taken by the European Southern Observatory's Very Large Telescope. \n  The incident highlights the importance of fact-checking before training AI models.\n\n* Not to be left behind Meta launched BlenderBot, a prototype conversational AI that soon told journalists it had \ndeleted its Facebook account after learning about the company’s privacy scandals. “Since deleting Facebook my life has been much better,” it said. (oh oh)\n\n* In 2016 Microsoft apologized after a Twitter chatbot, Tay, started generating racist and sexist messages. It was \nforced to shut down the bot after users tweeted hateful remarks at Tay, which it then parroted. Its posts included likening feminism to cancer and suggesting the Holocaust did not happen.\n\nMicrosoft’s Satya Nadella told CNBC that AI-powered search is the biggest thing to happen to the company in the nine years he’s been CEO.\n\nOtter.ai from FastCompany can automatically record meetings, take notes, and provide summaries, making it easier to track essential points. It can even perform these tasks over Zoom. https://www.fastcompany.com/90834773/how-to-use-ai-to-improve-employee-performance-and-engagement\n\n## Cybersecurity\n\nNew ESXiArgs Ransomware Variant Emerges After CISA Releases Decryptor Tool.\n\nThe threat actors behind the ESXiArgs ransomware attack have released an updated version that encrypts more data and removes the Bitcoin address from the ransom note, instead asking victims to contact them on Tox to obtain the wallet information. The new variant was reported by a system administrator, who said files larger than 128MB will have 50% of their data encrypted. The Cybersecurity and Infrastructure Security Agency (CISA) had earlier released a decryptor to help affected victims recover from the attack. According to Censys, the attackers likely knew that the original encryption process was easy to circumvent and were aware that researchers were tracking their payments. https://thehackernews.com/2023/02/new-esxiargs-ransomware-variant-emerges.html\n\nMicrosoft, Google, and Apple are looking at replacing traditional passwords with secure passkeys to authenticate users. Passkeys, which offer greater security and resistance to phishing attempts, are becoming more popular as password security continues to be threatened by hackers. Internet security experts suggest that passkeys could become standard within a year, with companies such as Apple, Google, and Microsoft already pushing users towards using them. https://www.cnbc.com/2023/02/11/why-apple-google-microsoft-passkey-should-replace-your-own-password.html\n\n## Intelligent Edge\n\nAT&T is collaborating with Ghost Robotics to use robotic dogs to improve public safety and national defense. The initiative will enhance the FirstNet emergency responder service, and network-connected robotic dogs can deliver a wide range of Internet of Things (IoT) use cases, including those that previously required putting personnel in dangerous situations. Lance Spencer, Client Executive VP - Defense at AT&T, said that this is one way to demonstrate the innovation and transformative possibilities of 5G and IoT.  https://www.iottechnews.com/news/2023/jan/26/att-touts-robotic-dogs-public-safety-national-defense/\n\nA study by Juniper Research predicts that the global number of 5G IoT connections will exceed 100 million by 2026, up 1,100% from 17 million connections in 2023. Healthcare and smart city services are expected to drive this growth. Over 60 million 5G smart city connections are expected globally. 5G will enable more efficient healthcare provision and IoT technologies can address healthcare inefficiencies exposed by the COVID-19 pandemic. Connected emergency services, telemedicine, and real-time remote monitoring will be the most useful applications of 5G IoT.  https://www.iottechnews.com/news/2023/jan/24/5g-iot-connections-exceed-100m-by-2026/\n\n\n","guests":null,"link":"/brief-EDW2-en","image":"./briefs/edw-2/en/thumbnail.png","lang":"en","summary":"Summary"},{"id":187,"type":"News Brief","title":"2023-6-18","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Ubiquitous Compute\n\nInformationWeek explores the AI vs. low-code/no-code debate for developers. While low-code/no-code platforms offer simplicity and speed, AI-powered tools bring advanced automation and intelligence. The article delves into the pros and cons of each approach, highlighting how finding the right balance can empower developers and drive innovation in the DevOps realm.\n\n[https://www.informationweek.com/devops/dos-won-t-hunt-is-ai-better-than-low-code-no-code-for-developers-](https://www.informationweek.com/devops/dos-won-t-hunt-is-ai-better-than-low-code-no-code-for-developers-)\n\nCRN recounts the top 10 cloud outages of 2023 (so far) in a light-hearted manner. From the 'Cloudy with a Chance of Downtime' to 'The Great Data Storm,' these outages served as unexpected opportunities for cloud users to practice their patience and rediscover the joys of offline activities. It is been a rough winter and spring. Let's hope for clearer skies in the cloud ahead!\n\n[https://www.crn.com/news/cloud/the-10-biggest-cloud-outages-of-2023-so-far-](https://www.crn.com/news/cloud/the-10-biggest-cloud-outages-of-2023-so-far-)\n\nOracle experiences impressive cloud revenue growth across industry verticals, fueled by AI adoption. From transforming healthcare with predictive analytics to optimizing supply chains with intelligent automation, Oracle's cloud solutions are empowering businesses across sectors. The future looks bright as AI continues to drive innovation and propel digital transformation.\n\n[https://www.ciodive.com/news/Oracle-cloud-revenue-growth-industry-verticals-AI/652885/](https://www.ciodive.com/news/Oracle-cloud-revenue-growth-industry-verticals-AI/652885/)\n\n## Data Management\n\nIn the quest for responsible AI, TDWI explores key considerations. From data ethics and bias mitigation to interpretability and accountability, responsible AI requires a holistic approach. Organizations are urged to prioritize transparency, fairness, and human oversight to ensure AI systems serve as trusted and beneficial tools in our ever-evolving digital landscape.\n\n[https://tdwi.org/articles/2023/06/08/adv-all-responsible-ai-0608.aspx](https://tdwi.org/articles/2023/06/08/adv-all-responsible-ai-0608.aspx)\n\nDataStax introduces a schema GPT translator to its Apache Pulsar-based Astra Streaming platform. This new addition enhances data management capabilities by enabling seamless integration between schemas and the popular GPT language model. Users can now harness the power of natural language processing in their data streaming workflows, boosting efficiency and insights.\n\n[https://www.infoworld.com/article/3699748/datastax-adds-schema-gpt-translator-to-apache-pulsar-based-astra-streaming.html](https://www.infoworld.com/article/3699748/datastax-adds-schema-gpt-translator-to-apache-pulsar-based-astra-streaming.html)\n\nNot understanding complex privacy laws comes a at price. Microsoft faces a $20 million fine from the FTC for Xbox children's privacy violations. The company allegedly failed to obtain parental consent for data collection and lacked sufficient safeguards. This hefty penalty serves as a reminder for organizations to prioritize and uphold children's privacy rights in the digital realm.\n\n[https://www.cpomagazine.com/data-protection/20-million-fine-issued-to-microsoft-by-ftc-over-xbox-childrens-privacy-violations/](https://www.cpomagazine.com/data-protection/20-million-fine-issued-to-microsoft-by-ftc-over-xbox-childrens-privacy-violations/)\n\n## Artificial Intelligence\n\nMcKinsey's new report highlights the immense economic potential of Generative AI, positioning it as the next frontier for productivity. This transformative technology has the capacity to drive significant growth, revolutionize industries, and unlock unprecedented levels of innovation and creativity. However four sectors are at risk for highly displacement of human workers. Namely, Customer operations, marketing and sales, software engineering, and R&D.\n\n[https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier)\n\nEurope takes a bold step forward in regulating AI, challenging the power of tech giants. New regulations aim to address concerns surrounding transparency, fairness, and accountability. By asserting control over AI applications, Europe seeks to shape the ethical and responsible use of artificial intelligence in the digital landscape. In contrast countries like Japan are embracing AI decreasing regulations on data collection, not wanting to be left out this huge paradigm shift. \n\n[https://www.datacenterknowledge.com/artificial-intelligence/europe-moves-ahead-ai-regulation-challenging-tech-giants-power](https://www.datacenterknowledge.com/artificial-intelligence/europe-moves-ahead-ai-regulation-challenging-tech-giants-power)\n\nBCG just conducted one of the most comprehensive AI studies yet, surveying 13,000 people, from execs to minimum-wagers, in over 18 countries. There findings: Over 80% of leaders are using AI at work while only 20% of frontline workers are. People are more optimistic and are not much concerned about AI. Frontline workers are at risk of replacement if they do not upgrade their skills and begin using AI in their everyday work.\n\n[https://www.bcg.com/publications/2023/what-people-are-saying-about-ai-at-work](https://www.bcg.com/publications/2023/what-people-are-saying-about-ai-at-work)\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW20-en","image":"./briefs/edw-20/en/thumbnail.png","lang":"en","summary":"Digital Transformation news from the week of June 19, 2023 including stories of managers replacing workers with AI, Cloud outages again, and privacy laws taking a bite out of big tech."},{"id":188,"type":"News Brief","title":"2023-6-25","tags":["ai","edge","cybersecurity"],"body":"\n\n## Artificial Intelligence\n\nThe relationship between AI and cryptocurrency is evolving into a complex dynamic as regulators grapple with their coexistence. While AI technologies offer potential for fraud detection and risk assessment in the crypto space, regulatory challenges arise due to the decentralized nature of cryptocurrencies. Striking a balance between innovation and oversight remains a crucial challenge.\n\n[https://www.datacenterknowledge.com/artificial-intelligence/ai-and-crypto-are-becoming-regulatory-frenemies](https://www.datacenterknowledge.com/artificial-intelligence/ai-and-crypto-are-becoming-regulatory-frenemies)\n\nChatGPT, Google's Meena, Bing Chat, and OpenAI's GPT-3 models competed to provide accurate and helpful responses in a real-world test. ChatGPT emerged as the top performer, showcasing its ability to understand and generate coherent answers. However, limitations were observed in all models, highlighting the ongoing challenges in developing chatbots that fully meet user expectations.\n\n[https://www.zdnet.com/article/chatbot-showdown-chatgpt-google-bard-and-bing-chat-put-to-a-real-world-test/](https://www.zdnet.com/article/chatbot-showdown-chatgpt-google-bard-and-bing-chat-put-to-a-real-world-test/)\n\nHPE has announced the launch of an AI supercomputer cloud service called HPE Cray Accelerated Insight. The service aims to give organizations easy access to powerful computing resources for AI workloads. Leveraging Cray's Shasta architecture, the cloud service offers high performance and scalability to accelerate AI research and development.\n\n[https://www.datacenterknowledge.com/cloud/hpe-unveils-ai-supercomputer-cloud-service](https://www.datacenterknowledge.com/cloud/hpe-unveils-ai-supercomputer-cloud-service)\n\n## Cybersecurity\n\nThe cyber war continues!! Hacking group Clop targeted US government agencies, stealing data through a cyberattack on MoveIT software. This sophisticated breach highlights the growing threat of ransomware attacks, with Clop utilizing advanced tactics like double extortion. The incident underscores the need for improved government cybersecurity measures.\n\n[https://www.wired.com/story/clop-moveit-hack-us-agencies-data-theft/](https://www.wired.com/story/clop-moveit-hack-us-agencies-data-theft/)\n\nChinese chipmaker Hualan has been added to the US Entity List due to concerns over national security. Hualan specializes in encryption chips, which play a critical role in safeguarding sensitive information. The move reflects the ongoing tech rivalry between the US and China and could have significant implications for global supply chains and the encryption industry.\n\n[https://www.wired.com/story/hualan-encryption-chips-entity-list-china/](https://www.wired.com/story/hualan-encryption-chips-entity-list-china/)\n\nRemote working has expanded the attack surface. A vulnerability in Microsoft Teams has been discovered, enabling the delivery of malware through external accounts. The bug allows attackers to send malicious messages containing malicious links, potentially compromising users' systems. Microsoft has since released a patch to address the issue and urges users to update their software to ensure security.\n\n[https://www.bleepingcomputer.com/news/security/microsoft-teams-bug-allows-malware-delivery-from-external-accounts/](https://www.bleepingcomputer.com/news/security/microsoft-teams-bug-allows-malware-delivery-from-external-accounts/)\n\n## Edge Computing\n\nSpirent, a leading provider of test and measurement solutions, has introduced an over-the-air performance monitoring solution. The system enables real-time monitoring and analysis of wireless network performance, including latency, throughput, and coverage. This solution aims to support the growing demand for reliable and high-quality wireless connectivity, particularly in the context of emerging technologies like 5G and edge computing.\n\n[https://www.edgecomputing-news.com/2023/06/19/spirent-launches-over-the-air-performance-monitoring-solution/](https://www.edgecomputing-news.com/2023/06/19/spirent-launches-over-the-air-performance-monitoring-solution/)\n\nZscaler has overtaken Cisco in the race for revenue in the Secure Access Service Edge (SASE) market. Zscaler's cloud-native architecture, comprehensive security offerings, and ability to address modern networking challenges have contributed to its success. The increasing demand for secure remote access and cloud-based security solutions has propelled Zscaler's growth, positioning it as a leader in the SASE space.\n\n[https://www.sdxcentral.com/articles/analysis/how-zscaler-finally-topped-cisco-in-the-sase-revenue-race/2023/06/](https://www.sdxcentral.com/articles/analysis/how-zscaler-finally-topped-cisco-in-the-sase-revenue-race/2023/06/)\n\nAT&T, Dell, and VMware are collaborating to simplify 5G edge deployments. Their combined solution aims to streamline the implementation of 5G networks, leveraging AT&T's services, Dell's infrastructure expertise, and VMware's software capabilities for efficient and seamless edge computing applications.\n\n[https://www.networkworld.com/article/3695740/att-dell-and-vmware-team-to-simplify-5g-edge-deployments.html](https://www.networkworld.com/article/3695740/att-dell-and-vmware-team-to-simplify-5g-edge-deployments.html)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW21-en","image":"./briefs/edw-21/en/thumbnail.png","lang":"en","summary":"News this week, June 26, 2023,  in digital transformation, including increased attacks in the cyber war, everyone jumping onto the generative AI bandwagon, and virtualized radio area networks."},{"id":189,"type":"News Brief","title":"2023-7-9","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Artificial Intelligence\n\nAccording to Bank of America's latest discovery, ChatGPT app downloads show signs of a leisurely pace. The AI-powered conversational tool is taking a breather, causing analysts to wonder if chatty bots have run out of things to say or if users crave some old-fashioned human interaction.\n\n[https://www.cnbc.com/2023/07/05/chatgpt-app-downloads-are-slowing-down-bofa-finds-.html](https://www.cnbc.com/2023/07/05/chatgpt-app-downloads-are-slowing-down-bofa-finds-.html)\n\nNew York City declares war on biased algorithms! TechCrunch reports that their anti-bias law for hiring algorithms is now full swing. The algorithms better watch out because they can no longer discriminate in the job market. It's like a superhero movie, but we have lines of code fighting for justice instead of capes!\n\n[https://techcrunch.com/2023/07/05/nycs-anti-bias-law-for-hiring-algorithms-goes-into-effect/](https://techcrunch.com/2023/07/05/nycs-anti-bias-law-for-hiring-algorithms-goes-into-effect/)\n\nIntel unveils its latest creation: a 3D generative AI model poised to revolutionize the virtual world! According to Intel's announcement, this cutting-edge technology brings new dimensions to AI by creating mind-blowing 3D models. With this innovation, Intel is taking \"thinking outside the box\" to a new level. Get ready for a virtual extravaganza!\n\n[https://www.intel.com/content/www/us/en/newsroom/news/intel-introduces-3d-generative-ai-model.html](https://www.intel.com/content/www/us/en/newsroom/news/intel-introduces-3d-generative-ai-model.html)\n\n## Cybersecurity\n\nSolar power is shining a light on a dark vulnerability! SecurityWeek reveals that an exploited solar power product flaw can expose energy organizations to cyber-attacks. It seems even the sun isn't safe from hackers. Let's hope they find a solar solution to secure our energy grids and put hackers in the shade!\n\n[https://www.securityweek.com/exploited-solar-power-product-vulnerability-could-expose-energy-organizations-to-attacks/](https://www.securityweek.com/exploited-solar-power-product-vulnerability-could-expose-energy-organizations-to-attacks/)\n\nUh-oh, the cybersecurity agencies are sounding the alarm! According to The Hacker News, a new threat has emerged, new variants of the TrueBot malware have been found, sending shivers down our digital spines. It's time to buckle up and strengthen our cyber defenses because these sneaky hackers are up to no good. Stay vigilant, folks, and let's thwart this attack by installing updates, use multi-factor-authentication, monitor for signs of infiltration and report incidents!\n\n[https://thehackernews.com/2023/07/cybersecurity-agencies-sound-alarm-on.html](https://thehackernews.com/2023/07/cybersecurity-agencies-sound-alarm-on.html)\n\nCISA detects a cyber-boost in agency reporting! Federal News Network says more agencies automatically report into the CDM (Continuous Diagnostics and Mitigation) dashboard. Our cyber defenses are getting smarter, with agencies stepping up. Big kudos to those keeping an eye on cybersecurity and making the digital world a safer place, one report at a time!\n\n[https://federalnewsnetwork.com/cybersecurity/2023/07/cisa-sees-uptick-in-agencies-automatically-reporting-into-cdm-dashboard/](https://federalnewsnetwork.com/cybersecurity/2023/07/cisa-sees-uptick-in-agencies-automatically-reporting-into-cdm-dashboard/)\n\n## Ubiquitous Computing\n\nIt looks like the US is putting up a digital fence against China! According to Cointelegraph, there are reports of plans to limit China's access to cloud computing services. It's like a virtual game of hide-and-seek but with severe geopolitical implications. Let's see who emerges victorious in this cloud-based tug of war. Grab your popcorn, folks, because the tech battle is heating up!\n\n[https://cointelegraph.com/news/us-reportedly-plans-to-restrict-china-s-access-to-cloud-computing-services](https://cointelegraph.com/news/us-reportedly-plans-to-restrict-china-s-access-to-cloud-computing-services)\n\nRare earth minerals are causing déjà vu in the US-China trade dispute! Intereconomics sheds light on the situation, highlighting the importance of these crucial minerals in various industries. It seems history is repeating itself as rare earths become a focal point in this ongoing trade war. Brace yourselves for a bumpy ride as the US and China again navigate this familiar terrain. Will they find a resolution or spin in an endless loop? Time will tell!\n\n[https://www.intereconomics.eu/contents/year/2019/number/6/article/rare-earths-in-the-trade-dispute-between-the-us-and-china-a-deja-vu.html](https://www.intereconomics.eu/contents/year/2019/number/6/article/rare-earths-in-the-trade-dispute-between-the-us-and-china-a-deja-vu.html)\n\nIBM is turning quantum computing errors into a thing of the past! According to Network World, they're touting error mitigation techniques that promise more significant performance in the quantum realm. It's like a magic spell to minimize those pesky errors and unlock the true potential of quantum computing. Get ready for a quantum leap forward in the world of technology!\n\n[https://www.networkworld.com/article/3699789/ibm-touts-error-mitigation-for-greater-quantum-computing-performance.html](https://www.networkworld.com/article/3699789/ibm-touts-error-mitigation-for-greater-quantum-computing-performance.html)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW23-en","image":"./briefs/edw-23/en/thumbnail.png","lang":"en","summary":"Here&#39;s the latest news in digital transformation for July 10, 2023. The ongoing trade war is starting to impact cloud services. Additionally, there&#39;s speculation that the hype around generative AI may be slowing down. On the cybersecurity front, there are concerns about whether we&#39;re doing enough to stay ahead of malicious actors."},{"id":190,"type":"News Brief","title":"2023-7-16","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Artificial Intelligence\n\nHas AI now been blessed by religion? The Vatican goes digital with divine guidance! Gizmodo reports that Pope Francis and the Vatican have released an AI ethics document, bringing the holy touch to the world of artificial intelligence. It's like a technological sermon on the importance of ethical AI practices. Hallelujah for digital righteousness!\n\n[Read More](https://gizmodo.com/pope-francis-vatican-releases-ai-ethics-1850583076)\n\nElon Musk strikes again, this time with an AI company! The tech mogul has launched a new venture focused on artificial intelligence that will “Seek the truth”. It seems Musk can't resist diving into futuristic endeavors. Will this be another game-changer or just another feather in his futuristic cap? Only time will tell!\n\n[Read More](https://www.cnn.com/2023/07/12/tech/elon-musk-ai-company/index.html)\n\nGenerative AI goes “MAD” after a few rounds of artificial data! Tom's Hardware reports that generative AI starts producing wildly bizarre outcomes when trained on artificial data for over five iterations. The AI’s imagination seems to have run wild, unleashing a quirky and unexpected side. Let's hope it doesn't give us AI-generated unicorns running amok!\n\n[Read More](https://www.tomshardware.com/news/generative-ai-goes-mad-when-trained-on-artificial-data-over-five-times)\n\n## Cybersecurity\n\nIn the realm of cyber-fortresses, Forbes crowns Intel as the reigning champion! The 2023 list of America's most cyber secure companies spotlights Intel's unrivaled defense skills. With its tech prowess and a fortress of innovation, Intel stands tall, ready to face any digital adversary. They don their virtual capes and protect our data from cyber villains with style and finesse. It's no wonder Intel is the talk of the cybersecurity town!\n\n[Read More](https://www.forbes.com/sites/hnewman/2023/06/08/meet-americas-most-cybersecure-companies-2023/?sh=dd8bc202cf60)\n\nThe national cybersecurity battle plan has been unveiled! Help The implementation plan for the National Cybersecurity Strategy has been published. It's a strategic playbook to defend against digital threats, outlining the steps to safeguard our virtual realms. Let's rally behind this plan and fortify our nation's cyber defenses. Onward to a safer digital future!\n\n[Read More](https://www.helpnetsecurity.com/2023/07/13/national-cybersecurity-strategy-implementation-plan-published/)\n\nThe cybersecurity battleground between the US and China is intensifying amid growing tensions. The Hacker News reports on alarming email compromises within US government agencies, further fueling concerns about the ongoing cyber war. It's a high-stakes clash of digital titans, underscoring the pressing need for robust defenses and international cooperation in the face of persistent cyber threats.\n\n[Read More](https://thehackernews.com/2023/07/us-government-agencies-emails.html)\n\n## Edge Computing\n\nInfineon and Edge Impulse join forces to unleash the power of Edge AI! Edge Computing News states this dynamic partnership aims to expand Infineon's edge AI capabilities. It's like a match made in tech heaven, combining Infineon's expertise with Edge Impulse's cutting-edge solutions. Get ready for AI at the edge, revolutionizing how we process data and unlocking new realms of innovation!\n\n[Read More](https://www.edgecomputing-news.com/2023/07/10/infineon-partners-with-edge-impulse-to-extend-its-edge-ai-capabilities/)\n\nHold onto your hats because the edge computing market is set to skyrocket! Edge Computing News reveals that by 2028, this booming industry will be worth a whopping $111.3 billion. It's like an edge computing gold rush, with companies racing to capitalize on the immense potential of decentralized processing power. Prepare for a paradigm shift as the edge takes center stage in the digital revolution!\n\n[Read More](https://www.edgecomputing-news.com/2023/07/10/edge-computing-market-to-be-worth-111-3-billion-by-2028/)\n\nSeoul Robotics is expanding transportation technology with its advanced 3D perception technology. EdgeIR reports on this game-changing innovation that brings enhanced depth perception to transportation systems. As vehicles begin to utilize this cutting-edge technology from Seoul Robotics, we can anticipate safer and smarter journeys in the future.\n\n[Read More](https://www.edgeir.com/seoul-robotics-develops-3d-perception-tech-to-boost-transportation-systems-20230711)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW24-en","image":"./briefs/edw-24/en/thumbnail.png","lang":"en","summary":"In digital transformation news for the week of July 17, 2023, Pope Francis offers guidance on AI ethics, Intel remains America&#39;s cyber fortress, and the edge computing market is set to soar!"},{"id":191,"type":"News Brief","title":"2023-7-23","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Artificial Intelligence\n\n\nMeta, the AI trailblazer, wows the tech community with a game-changing release - introducing Llama 2, the latest open-source LLM model. Unlocking the power of language understanding for all developers, this groundbreaking innovation promises to revolutionize AI projects. Bid farewell to complexity and welcome seamless AI training. Get ready to ride the wave of LLMs open-source models and witness the future of AI unfold! [Read More](https://www.artificialintelligence-news.com/2023/07/19/meta-launches-llama-2-open-source-llm/)\n\n\nForbes Tech Council sounds the alarm as Generative AI sets its sights on personal data. Their riveting article delves into the impending data invasion and its potential consequences. Brace yourself for the revolutionary rise of AI-generated content, posing opportunities and data privacy challenges. Stay informed and vigilant as we navigate the brave new world of #GenerativeAI and safeguard our digital future. [Read More](https://www.forbes.com/sites/forbestechcouncil/2023/07/20/generative-ai-is-coming-for-people-data-are-you-ready/?sh=3e6f95421573)\n\n\nHollywood strike over AI job concerns. Top summer movie 'Mission: Impossible - Dead Reckoning' explores the potential and risks of AI. Is this a cautionary tale or a glimpse into our future? Actors and writers fear their jobs and likenesses will be replaced. Stay tuned for updates. [Read More](https://www.wired.com/story/mission-impossible-dead-reckoning-is-the-perfect-ai-panic-movie/)\n\n## CyberSecurity\n\n\nThe Guardian reports a major cyberattack as Chinese hackers target US officials, including the Ambassador to Beijing. The breach raises concerns over cybersecurity and international relations. Authorities are investigating the extent of the attack and its potential implications. Stay tuned for updates on this significant and concerning development. [Read More](https://www.theguardian.com/us-news/2023/jul/20/ambassador-to-beijing-among-us-officials-hit-by-chinese-hackers)\n\n\nBreaking News: A terrifying cyber assault unfolds! Critical infrastructure under siege as hackers exploit a dangerous Citrix zero-day vulnerability. Panic grips cybersecurity experts, racing against time to thwart the attack's catastrophic consequences. Brace for the fallout and keep your digital defenses ready! #CitrixZeroDay #CybersecurityEmergency [Read More](https://www.securityweek.com/citrix-zero-day-exploited-against-critical-infrastructure-organization/)\n\n\nCyware brings alarming news of \"HotRAT,\" a hidden script lurking in cracked software. Cybercriminals leverage this devious tactic to compromise unsuspecting users. The threat landscape intensifies, prompting caution while downloading cracked programs. Protect your digital haven and avoid falling victim to this insidious ploy. Stay informed and stay safe! #HotRAT #CybersecurityThreat [Read More](https://cyware.com/news/hotrat-as-hidden-script-in-cracked-software-b2baa5b3)\n\n## Data Management\n\n\nTDWI uncovers the dawn of a data management revolution with 'Arch-All Data Fabric.' In this groundbreaking report, experts unveil the blueprint for next-gen data management. Witness the integration of diverse data sources, propelling businesses to unprecedented heights. Stay tuned for insights into the data-driven future! #DataFabric #NextGenDataManagement [Read More](https://tdwi.org/articles/2023/07/20/arch-all-data-fabric-how-to-architect-next-generation-data-management.aspx)\n\n\n\nHammerspace secures a groundbreaking milestone with $56.7M in first institutional funding. This transformative achievement unlocks hidden business opportunities within unstructured data. Discover the power of their innovative solutions, enabling seamless management of unstructured data for businesses. Brace for a new era of data utilization and growth! #Hammerspace #DataInnovation #FundingSuccess [Read More](https://hammerspace.com/hammerspace-raises-56-7m-in-first-institutional-funding-unlocks-business-opportunities-hidden-in-unstructured-data/)\n\n\nLenovo's grand reveal: New data management solutions engineered to supercharge AI workloads! Datanami reports on its cutting-edge offerings, empowering businesses with AI-driven potential. Dive into the latest innovations that promise to revolutionize data handling for AI projects. Embrace a new era of efficiency and intelligence in the tech landscape! #Lenovo #DataManagement #AIWorkloads [Read More](https://www.datanami.com/this-just-in/lenovo-unveils-new-data-management-solutions-to-enable-ai-workloads/)\n\n## Embracing Digital Podcast\n\nIn this episode, Darren revisits an interview he conducted with chatGPT and follows up with a similar interview featuring Google Bard. The comparison draws intrigue, especially when paired with the insights from Episode 122 on Embracing Digital Transformation. [Episode 147](https://www.embracingdigital.org/episode-EDT147) [Episode 122](https://www.embracingdigital.org/episode-EDT122)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW25-en","image":"./briefs/edw-25/en/thumbnail.png","lang":"en","summary":"This week, July 24, 2023, in digital transformation news, there have been developments in Generative AI in the workplace, as well as a China cyber attack on the US Embassy. Additionally, Data Fabrics are beginning to surface as a trend."},{"id":192,"type":"News Brief","title":"2023-7-30","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Advanced Communications\n\n\nIntel and Ericsson are teaming to create better internet connectivity and device experiences. Their collaboration aims to meet the rising demand for high-speed internet and reliable connections, promising exciting technological advancements. With their combined expertise, users can look forward to faster downloads, smoother video calls, and improved overall performance for phones, computers, and other gadgets. Get ready for a more connected and seamless future! [Read More](http://finance.yahoo.com/news/intel-ericsson-expand-collaboration-advance-201000466.html)\n\n\nGet ready for the 5G boom! According to a new study, the global 5G system integration market is set to hit a whopping USD 115.60 billion by 2032. Businesses are racing to adopt this super-fast internet tech for smart cities, healthcare, and more. Manufacturers are also upgrading with cool tech like robots and big data. The demand for 5G integration services is soaring, promising exciting times ahead! [Read More](https://finance.yahoo.com/news/global-5g-system-integration-market-100000051.html)\n\n\nExciting news from T-Mobile! Their revolutionary 5G technology promises super-fast internet, 3.3 GBits/Sec speeds, and seamless connectivity. With this innovation, downloading and streaming will be lightning-fast, and video calls and online gaming will be smoother. Get ready for a new era of connectivity and unique internet experiences! [Read More](https://voip.review/2023/07/26/tmobile-introduces-groundbreaking-5g-tech/)\n\n## Cybersecurity\n\n\nGPT and Cybersecurity: The Future is Exciting and Worrisome! Experts are buzzing about GPT, a powerful AI system that's set to revolutionize cybersecurity. Una-May O'Reilly's talk shed light on the impact of GPT on security, showing it will make defenses more innovative and efficient. But there's a twist: hackers can also use it! AI might boost both sides in a cyber arms race, leaving us in a dash of the unknown. Brace for the thrilling and worrisome future of cybersecurity!  [Read More](https://www.forbes.com/sites/johnwerner/2023/07/28/whos-the-authority-on-gpt-and-cybersecurity-is-it--gpt/)\n\n\nNew rules from the Securities and Exchange Commission (SEC) require public companies to disclose any cybersecurity breaches that could impact their profits within four days. This move aims to protect investors and bring more transparency to cybersecurity risks. Delays are allowed in cases of national security risks. The rule also demands companies share information on their cybersecurity management and expertise. [Read More](https://apnews.com/article/sec-cybersecurity-breach-disclosure-risk-hacking-bb6252463637793bfdc8ace5bfcbe7df)\n\n\nIn breaking news, cybersecurity agencies in Australia and the US have warned about a critical weakness in web applications. Cybercriminals can exploit the Insecure Direct Object Reference vulnerability to gain unauthorized access to confidential information. Stay alert and protect your data with robust authentication and authorization protocols. [Read More](https://thehackernews.com/2023/07/cybersecurity-agencies-warn-against.html)\n\n## Ubiquitous Computing\n\n\nAI has been increasingly used in programming to enhance efficiency and reduce expenses. However, reliance on previous code can lead to mistakes and impede creativity in application development. AI-generated code may not be optimized for the platform, so it's essential to balance the benefits of AI with human expertise for efficient applications. [Read More](https://www.infoworld.com/article/3703611/the-lost-art-of-cloud-application-engineering.html)\n\n\nMicrosoft Leads in Cloud-Based AI Workloads, says Nadella! Microsoft is taking the lead in cloud-based AI workloads, according to its CEO, Satya Nadella. The tech giant's prowess in AI-driven tasks is making waves in the industry. By harnessing the cloud's power, Microsoft delivers cutting-edge AI solutions to businesses and users. With their innovative approach, they are shaping the future of AI technology. [Read More](https://www.cnbc.com/2023/07/25/microsoft-is-in-the-lead-with-cloud-based-ai-workloads-nadella-says.html)\n\n\nAlibaba has teamed up with Meta's Llama AI to develop innovative software. This collaboration between Alibaba Cloud, a division of the Chinese giant, and Llama 2's AI model is setting new trends in the tech industry. This partnership will make China a pioneer in zero-cost programming, bringing exciting AI-powered innovations to the market. Get ready to witness the future of software development! [Read More](https://fagenwasanni.com/news/alibabas-cloud-computing-service-utilizes-metas-ai-model-llama-for-software-development/82096/)\n\n\n## Embracing Digital Transformation Podcast\n\nCheck out this week's episode where Darren engages in an insightful conversation with special guest Jared Shepard, the CEO of Hypori. The interview focuses on the crucial topic of securing remote workers through mobile virtualization. Jared Shepard's unique journey from a high school dropout to a CEO adds an inspiring dimension to the discussion.\n\n[Episode 148](https://www.embracingdigital.org/episode-EDT148)\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW26-en","image":"./briefs/edw-26/en/thumbnail.png","lang":"en","summary":"Here are the latest updates in the field of Digital Transformation for the week of July 31, 2023. This includes news on the partnership between Ericsson and Intel in advancing 5G technology, ChatGPT's efforts in tackling cybersecurity, and the emergence of AI-powered software development."},{"id":193,"type":"News Brief","title":"2023-8-6","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Artificial Intelligence\r\n\n\nThere is growing concern among tech experts about the use of AI-enhanced images. The Guardian has investigated this issue, as there is a worry that AI-altered visuals could impact public opinion and even distort important facts essential to the democratic system. This could seriously affect trust and truth and lead to a debate on how to protect elections from the manipulation of AI.\r [Read More](https://www.theguardian.com/technology/2023/aug/03/ai-enhanced-images-a-threat-to-democratic-processes-experts-warn\r)\n\n\nA recent article by Yahoo Finance reported that Wendy's is using AI to revolutionize their drive-thrus. With automated ordering and personalized menus, the fast food giant is changing the game for quick meals. This technological approach to food service provides a sneak peek into the future of fast-food dining. It will be interesting to see how customers react to this digital transformation.\r [Read More](https://finance.yahoo.com/news/wendys-latest-fast-food-company-210910771.html\r)\n\n\nThe impact of AI on Asian-American professionals is a concerning trend. Job losses are looming large, raising questions about how society will address the complex equation of technology, ethnicity, and workforce dynamics. A closer examination of AI's effects is necessary.\r [Read More](https://www.nbcnews.com/news/asian-america/asian-american-workers-heavily-affected-ai-rcna98179\r)\n\n## Cybersecurity\r\n\n\nNBC News has reported a concerning cyber attack on a hospital network that resulted in the disconnection of its national computer infrastructure. The breach has put patient care at risk, and authorities are working to contain the situation. This incident highlights the importance of healthcare data security in the digital era.\r [Read More](https://www.nbcnews.com/tech/security/hackers-force-hospital-system-take-national-computer-system-offline-rcna98212\r)\n\n\nDevelopers using the widely-used NPM system are being targeted by harmful code packages that can compromise software security. This is a severe threat that needs to be identified and mitigated to prevent cyber attacks. The cybersecurity community is concerned and highlights the need for more secure software supply chains to prevent further infiltration of these packages.\r [Read More](https://thehackernews.com/2023/08/malicious-npm-packages-found.html\r)\n\n\nCISA has released a strategic plan called \"Blueprint for Cybersecurity\" to strengthen national digital infrastructure. It focuses on adaptive defense, incident response, and innovation to create an impenetrable digital shield. This plan is an essential guide for those interested in tech security.\r [Read More](https://www.securitysystemsnews.com/article/cisa-releases-cybersecurity-strategic-plan\r)\n\n## Edge Computing\r\n\n\nThe combination of quantum computing and IoT devices is causing security concerns. Quantum technology's power and IoT devices' potential for hijacking creates a new challenge. Encryption being cracked is a growing fear as quantum computing advances and botnets launch DDoS attacks using hacked IoT devices. Network defenses must be strengthened to cope with this dual threat.\r [Read More](https://www.securitymagazine.com/articles/99604-the-impact-of-quantum-computers-and-iot-devices-on-network-security\r)\n\n\nYahoo Finance has revealed an exciting development in the tech industry as Edge Computing, IoT, and AI merge to create a market predicted to be worth $230 billion by 2025. Edge Computing enables more than 75 billion IoT devices to process data in real time, while AI's cognitive abilities are estimated to boost productivity by up to 40%. This revolution is set to reshape data dynamics and decision-making across various industries.\r [Read More](https://finance.yahoo.com/news/edge-computing-iot-ai-revolutionize-011500739.html\r)\n\n\nThe Asia-Pacific region is undergoing a significant digital transformation driven by the Internet of Things (IoT) and Artificial Intelligence (AI). Mobile Edge Computing (MEC) is leading this change, offering unparalleled levels of connectivity and automation by bringing computation closer to the data source. MEC has immense potential to revolutionize industries, improve lives and drive the region's digital destiny, but it also poses security challenges that demand robust safeguards and updated regulations.\r [Read More](https://fagenwasanni.com/news/innovations-in-iot-and-ai-the-role-of-mobile-edge-computing-in-asia-pacifics-technological-advancements/48663/\r)\n\n## Embracing Digital Transformation Podcast\r\n\n\nIn the latest episode of Embracing Digital Transformation, Darren is joined by Leland Brown, a principal engineer at Capgemini and a former guest on the show. They discuss the upcoming advancements of 5G technology and its impact on Mobile Edge Computing (MEC) in the US Department of Defense.\r [Read More](https://embracingdigital.org/episode-EDT149\r)\n\n## Artificial Intelligence\r\n\n## Artificial Intelligence\r\n\n## Artificial Intelligence\r\n\n\nThere is growing concern among tech experts about the use of AI-enhanced images. The Guardian has investigated this issue, as there is a worry that AI-altered visuals could impact public opinion and even distort important facts essential to the democratic system. This could seriously affect trust and truth and lead to a debate on how to protect elections from the manipulation of AI.\r [Read More](https://www.theguardian.com/technology/2023/aug/03/ai-enhanced-images-a-threat-to-democratic-processes-experts-warn\r)\n\n\nA recent article by Yahoo Finance reported that Wendy's, an American fast food restuarnt, is using AI to revolutionize their drive-thrus. With automated ordering and personalized menus, the fast food giant is changing the game for quick meals. This technological approach to food service provides a sneak peek into the future of fast-food dining. It will be interesting to see how customers react to this digital transformation.\r [Read More](https://finance.yahoo.com/news/wendys-latest-fast-food-company-210910771.html\r)\n\n\nThe impact of AI on Asian-American professionals is a concerning trend. Job losses are looming large, raising questions about how society will address the complex equation of technology, ethnicity, and workforce dynamics. A closer examination of AI's effects is necessary.\r [Read More](https://www.nbcnews.com/news/asian-america/asian-american-workers-heavily-affected-ai-rcna98179\r)\n\n## Cybersecurity\r\n\n\nNBC News has reported a concerning cyber attack on a hospital network that resulted in the disconnection of its national computer infrastructure. The breach has put patient care at risk, and authorities are working to contain the situation. This incident highlights the importance of healthcare data security in the digital era.\r [Read More](https://www.nbcnews.com/tech/security/hackers-force-hospital-system-take-national-computer-system-offline-rcna98212\r)\n\n\nDevelopers using the widely-used NPM system are being targeted by harmful code packages that can compromise software security. This is a severe threat that needs to be identified and mitigated to prevent cyber attacks. The cybersecurity community is concerned and highlights the need for more secure software supply chains to prevent further infiltration of these packages.\r [Read More](https://thehackernews.com/2023/08/malicious-npm-packages-found.html\r)\n\n\nCISA has released a strategic plan called \"Blueprint for Cybersecurity\" to strengthen national digital infrastructure. It focuses on adaptive defense, incident response, and innovation to create an impenetrable digital shield. This plan is an essential guide for those interested in tech security.\r [Read More](https://www.securitysystemsnews.com/article/cisa-releases-cybersecurity-strategic-plan\r)\n\n## Edge Computing\r\n\n\nThe combination of quantum computing and IoT devices is causing security concerns. Quantum technology's power and IoT devices' potential for hijacking creates a new challenge. Encryption being cracked is a growing fear as quantum computing advances and botnets launch DDoS attacks using hacked IoT devices. Network defenses must be strengthened to cope with this dual threat.\r [Read More](https://www.securitymagazine.com/articles/99604-the-impact-of-quantum-computers-and-iot-devices-on-network-security\r)\n\n\nYahoo Finance has revealed an exciting development in the tech industry as Edge Computing, IoT, and AI merge to create a market predicted to be worth $230 billion by 2025. Edge Computing enables more than 75 billion IoT devices to process data in real time, while AI's cognitive abilities are estimated to boost productivity by up to 40%. This revolution is set to reshape data dynamics and decision-making across various industries.\r [Read More](https://finance.yahoo.com/news/edge-computing-iot-ai-revolutionize-011500739.html\r)\n\n\nThe Asia-Pacific region is undergoing a significant digital transformation driven by the Internet of Things (IoT) and Artificial Intelligence (AI). Mobile Edge Computing (MEC) is leading this change, offering unparalleled levels of connectivity and automation by bringing computation closer to the data source. MEC has immense potential to revolutionize industries, improve lives and drive the region's digital destiny, but it also poses security challenges that demand robust safeguards and updated regulations.\r [Read More](https://fagenwasanni.com/news/innovations-in-iot-and-ai-the-role-of-mobile-edge-computing-in-asia-pacifics-technological-advancements/48663/\r)\n\n## Embracing Digital Transformation Podcast\r\n\n\nIn the latest episode of Embracing Digital Transformation, Darren is joined by Leland Brown, a principal engineer at Capgemini and a former guest on the show. They discuss the upcoming advancements of 5G technology and its impact on Mobile Edge Computing (MEC) in the US Department of Defense.\r [Read More](https://embracingdigital.org/episode-EDT149\r)\n\n## Artificial Intelligence\r\n\n## Artificial Intelligence\r\n\n\nThere is growing concern among tech experts about the use of AI-enhanced images. The Guardian has investigated this issue, as there is a worry that AI-altered visuals could impact public opinion and even distort important facts essential to the democratic system. This could seriously affect trust and truth and lead to a debate on how to protect elections from the manipulation of AI.\r [Read More](https://www.theguardian.com/technology/2023/aug/03/ai-enhanced-images-a-threat-to-democratic-processes-experts-warn\r)\n\n\nA recent article by Yahoo Finance reported that Wendy's, an American fast food restuarnt, is using AI to revolutionize their drive-thrus. With automated ordering and personalized menus, the fast food giant is changing the game for quick meals. This technological approach to food service provides a sneak peek into the future of fast-food dining. It will be interesting to see how customers react to this digital transformation.\r [Read More](https://finance.yahoo.com/news/wendys-latest-fast-food-company-210910771.html\r)\n\n\nThe impact of AI on Asian-American professionals is a concerning trend. Job losses are looming large, raising questions about how society will address the complex equation of technology, ethnicity, and workforce dynamics. A closer examination of AI's effects is necessary.\r [Read More](https://www.nbcnews.com/news/asian-america/asian-american-workers-heavily-affected-ai-rcna98179\r)\n\n## Cybersecurity\r\n\n\nNBC News has reported a concerning cyber attack on a hospital network that resulted in the disconnection of its national computer infrastructure. The breach has put patient care at risk, and authorities are working to contain the situation. This incident highlights the importance of healthcare data security in the digital era.\r [Read More](https://www.nbcnews.com/tech/security/hackers-force-hospital-system-take-national-computer-system-offline-rcna98212\r)\n\n\nDevelopers using the widely-used NPM system are being targeted by harmful code packages that can compromise software security. This is a severe threat that needs to be identified and mitigated to prevent cyber attacks. The cybersecurity community is concerned and highlights the need for more secure software supply chains to prevent further infiltration of these packages.\r [Read More](https://thehackernews.com/2023/08/malicious-npm-packages-found.html\r)\n\n\nCISA has released a strategic plan called \"Blueprint for Cybersecurity\" to strengthen national digital infrastructure. It focuses on adaptive defense, incident response, and innovation to create an impenetrable digital shield. This plan is an essential guide for those interested in tech security.\r [Read More](https://www.securitysystemsnews.com/article/cisa-releases-cybersecurity-strategic-plan\r)\n\n## Edge Computing\r\n\n\nThe combination of quantum computing and IoT devices is causing security concerns. Quantum technology's power and IoT devices' potential for hijacking creates a new challenge. Encryption being cracked is a growing fear as quantum computing advances and botnets launch DDoS attacks using hacked IoT devices. Network defenses must be strengthened to cope with this dual threat.\r [Read More](https://www.securitymagazine.com/articles/99604-the-impact-of-quantum-computers-and-iot-devices-on-network-security\r)\n\n\nYahoo Finance has revealed an exciting development in the tech industry as Edge Computing, IoT, and AI merge to create a market predicted to be worth $230 billion by 2025. Edge Computing enables more than 75 billion IoT devices to process data in real time, while AI's cognitive abilities are estimated to boost productivity by up to 40%. This revolution is set to reshape data dynamics and decision-making across various industries.\r [Read More](https://finance.yahoo.com/news/edge-computing-iot-ai-revolutionize-011500739.html\r)\n\n\nThe Asia-Pacific region is undergoing a significant digital transformation driven by the Internet of Things (IoT) and Artificial Intelligence (AI). Mobile Edge Computing (MEC) is leading this change, offering unparalleled levels of connectivity and automation by bringing computation closer to the data source. MEC has immense potential to revolutionize industries, improve lives and drive the region's digital destiny, but it also poses security challenges that demand robust safeguards and updated regulations.\r [Read More](https://fagenwasanni.com/news/innovations-in-iot-and-ai-the-role-of-mobile-edge-computing-in-asia-pacifics-technological-advancements/48663/\r)\n\n## Embracing Digital Transformation Podcast\r\n\n\nIn the latest episode of Embracing Digital Transformation, Darren is joined by Leland Brown, a principal engineer at Capgemini and a former guest on the show. They discuss the upcoming advancements of 5G technology and its impact on Mobile Edge Computing (MEC) in the US Department of Defense.\r [Read More](https://embracingdigital.org/episode-EDT149\r)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW27-en","image":"./briefs/edw-27/en/thumbnail.png","lang":"en","summary":"The week of the August 7, 2023 podcast, there were several significant updates regarding digital transformation. Notably, fast food chains have started implementing AI to tackle workforce shortages, hospitals are facing cyber attacks, and Mobile Edge Computing platforms are coming to light."},{"id":194,"type":"News Brief","title":"2023-8-13","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Ubiquitous Computing\n\n\nDue to a recent breach in Microsoft Azure Cloud, The Department of Homeland Security is reviewing cloud-based identity and authentication systems to prevent malicious attacks on cloud computing environments, following recent security breaches. The goal is to provide recommendations for improving account security and preventing future breaches. [Read More](https://www.reuters.com/technology/us-cyber-safety-review-board-assess-online-intrusion-microsoft-exchange-dhs-2023-08-11/)\n\n\nQuantum computing's potential to revolutionize multiple industries specifically Artificial Intelligence by increasing processing speed is exciting. However, due to the delicate nature of qubits, creating quantum computers is a difficult task. Nonetheless, industry professionals remain hopeful about the potential of quantum computing to bring about major advancements in various sectors. [Read More](https://www.scmp.com/magazines/post-magazine/long-reads/article/3230746/quantum-computing-could-give-ai-rocket-fuel-it-needs-become-transformative-its-not-there-yet)\n\n\nPresident Biden signed an executive order this week that prohibits U.S. investments in Chinese AI, semiconductor, and quantum computing sectors to avoid unintentional backing of China's military and technological advancement. The semiconductor industry, specifically chip development and manufacturing, is the main target. China has expressed discontent with the order, claiming it disrupts economic ties. The order applies to future investments and may have some exceptions. [Read More](https://www.tomshardware.com/news/us-order-bans-future-investments-in-chinese-ai-semiconductor-and-quantum-computing)\n\n## Data Management\n\n\nChina is proposing new rules to limit facial recognition tech and protect user privacy. The Cyberspace Administration of China's draft rules state that facial recognition should only be used when necessary, and that non-biometric solutions should be explored to achieve the same objectives with less intrusion. The move follows concerns in China about the potential misuse of personal information through facial recognition technology. [Read More](https://fagenwasanni.com/news/china-introduces-new-draft-rules-to-regulate-facial-recognition-technology-and-data-management/174996/)\n\n\nIn the shift from in-house servers to cloud technology and data centers for data management, cloud offers scalability and flexibility while eliminating server proximity needs. However, many organizations are shocked at the price of managing data across data centers and clouds as the cloud service providers charge for moving data between modalities.  Developing a data management strategy and hybrid data architecture is becoming critical for organizations to save costs, improve resiliency, and support businesses in these complex operating environments. [Read More](https://devops.com/data-management-cloud-technology-or-data-centers/)\n\n\nGovernments worldwide are taking a more active role in regulating data privacy. Oregon has taken a comprehensive approach, giving consumers more control over their personal data. Companies must now obtain explicit consent for data collection and sharing, and individuals have the right to access, correct, and delete their data. The law also includes transparency obligations and safeguards against discrimination based on data use. Is this another money grab from government or real protection of individual privacy. Time will tell. [Read More](https://www.reuters.com/legal/legalindustry/oregon-passes-comprehensive-privacy-law-2023-08-11/)\n\n## Cyber Security \n\n\nThe National Institute of Standards and Technology (NIST) is taking significant steps in enhancing cybersecurity practices by providing new guidelines and resources. In addition to the previous security framework pillars of identify, protect, detect, respond, and recover, NIST has introduced a new pillar called “govern.” This new addition emphasizes that cybersecurity is a significant source of enterprise risk and helps organizations to better devise and execute decisions to support their security strategy. [Read More](https://www.infosecurity-magazine.com/news/nist-expands-cybersecurity/)\n\n\nThe Black Hat cybersecurity conference in Las Vegas showcased advanced tools for threat detection, incident response, and secure communication. These tools aim to enhance organizations' cybersecurity defenses against emerging threats and challenges. As expected, Generative AI was a central topic, highlighting its potential to thwart cyber attacks. [Read More](https://www.crn.com/news/security/20-hottest-new-cybersecurity-tools-at-black-hat-2023)\n\n\nThe NSA's Codebreaker Challenge helps address the cybersecurity skills shortage by challenging students to solve complex problems and fostering collaboration. It encourages the development of new educational materials and programs, giving the next generation of professionals hands-on experience to strengthen the industry's defense against evolving threats. The program highlights the importance of promoting cybersecurity education and providing practical learning opportunities. [Read More](https://www.darkreading.com/attacks-breaches/nsa-talks-codebreaker-challenge-success-influence-on-education)\n\n## Embracing Digital Transformation Podcast\n\nDarren interviews Sonu Panda, the CEO of Prescriptive Data, in this episode. They discuss how their software helps commercial real estate owners turn their buildings into intelligent and efficient spaces. [Read More](https://embracingdigital.org/episode-EDT150)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW28-en","image":"./briefs/edw-28/en/thumbnail.png","lang":"en","summary":"Digital Transformation news for August 13, 2023. Recent developments in data privacy, security, and technological advancements, including quantum computing and new privacy laws, are worth following. NIST guidelines and Black Hat tools are useful resources. NSA&#39;s Codebreaker Challenge drives innovation in cybersecurity."},{"id":195,"type":"News Brief","title":"2023-8-19","tags":["ai","edge","cybersecurity"],"body":"\n\n## Artificial Intelligence\n\n\nIn a recent ruling, a D.C. court has determined that AI-generated art is not eligible for copyright protection. The decision sets a significant precedent, suggesting that creative works produced solely by artificial intelligence lack the legal rights typically afforded to human-made creations. This verdict may have far-reaching implications for the future of intellectual property in the digital age. [Read More](https://news.bloomberglaw.com/ip-law/ai-generated-art-lacks-copyright-protection-d-c-court-rules)\n\n\nMIT researchers have found that machine learning models used in medical diagnosis and treatment can exacerbate healthcare inequities. These models, when trained on biased data, can lead to unequal access and quality of care for underrepresented groups. This highlights the urgent need for improved data collection and model development to ensure fair and equitable healthcare outcomes. [Read More](https://news.mit.edu/2023/how-machine-learning-models-can-amplify-inequities-medical-diagnosis-treatment-0817)\n\n\nSkyrocketing demand for artificial intelligence (AI) is causing a shortage of high-powered chips, reports Fox Business. As AI applications expand across industries, there is a growing need for specialized chips to power them. This scarcity is affecting various sectors, including automotive, healthcare, and consumer electronics, potentially slowing down innovation. Companies are now racing to ramp up chip production to meet the burgeoning demand. [Read More](https://www.foxbusiness.com/technology/surging-demand-ai-creating-shortage-high-powered-chips)\n\n## Cybersecurity\n\n\nA student-focused app is raising concerns among parents and cybersecurity experts. The app's potential risks to children's safety are under scrutiny, prompting experts to emphasize the importance of parental vigilance and educating young users about online safety. The situation highlights the ongoing need for responsible technology use and proactive measures to protect children in the digital age. [Read More](https://www.live5news.com/2023/08/18/student-geared-app-concerning-lowcountry-parents-cyber-security-experts/)\n\n\nThe fintech industry is being greatly affected by AI and cybersecurity, according to Analytics Insight. AI is helping with detecting fraud, customer service, and assessing risks, but it also brings new security risks. This ongoing interplay between AI and cybersecurity is changing the fintech world and leading to more innovation and increased security measures to protect important financial information. [Read More](https://www.analyticsinsight.net/how-ai-and-cybersecurity-shape-fintech-industry/)\n\n\nA recent Pew Research report reveals that Americans have limited knowledge about AI, cybersecurity, and big tech. Findings indicate that a substantial portion of the population lacks an understanding of these critical topics, underscoring the need for increased public education and awareness efforts in an era dominated by technology and digital concerns. [Read More](https://www.pewresearch.org/internet/2023/08/17/what-americans-know-about-ai-cybersecurity-and-big-tech/)\n\n## Edge Computing\n\n\n\nT-Mobile is partnering with Google Cloud for edge computing, as reported by FierceWireless. This collaboration aims to leverage Google's cloud infrastructure to enhance T-Mobile's 5G network capabilities and provide low-latency services, benefiting consumers and businesses. This strategic move demonstrates the growing importance of edge computing in the telecommunications industry. [Read More](https://www.fiercewireless.com/tech/t-mobile-hooks-google-cloud-edge-compute)\n\n\n\nThe phase-out of 2G and 3G networks may have repercussions for IoT and roaming services. Many IoT devices and international travelers still rely on these older networks. Telecom providers need to consider alternative connectivity options and strategies to prevent disruption in service for these user groups as they transition to more advanced network technologies. It's important to address these issues to avoid accumulating technical debt in the future. [Read More](https://www.lightreading.com/broadband/sunsetting-2g-and-3g-could-leave-iot-and-roamers-in-dark/d/d-id/786146)\n\n\nAt the ET Digital Telco Summit, Airtel's CTO highlighted that the convergence of 5G with cloud and IoT technologies is poised to unlock innovative use cases across various industries. This combination is expected to drive transformative solutions for verticals, showcasing the potential for 5G to revolutionize connectivity and services in India and beyond. [Read More](https://telecom.economictimes.indiatimes.com/news/industry/etdigitaltelcosummit-combination-of-5g-with-cloud-iot-to-drive-new-use-cases-for-verticals-airtel-cto/102827792)\n\n## Embracing Digital Transformation Podcast\n\n\nThis week Darren starts a series on generative AI with interviews from experts in education, healthcare, cybersecurity, and cloud technologies and how generativeAI is playing a role in the future. This weeks stories include interviews from Dr. Jeffrey Lancaster talking about understanding genAI. [Read More](https://embracingdigital.org/episode-EDT151)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW29-en","image":"./briefs/edw-29/en/thumbnail.png","lang":"en","summary":"In the latest Embracing Digital news for August 20, 2023, there are updates regarding AI, cybersecurity, and edge computing. The news includes AI heading to court, cyber bad actors targeting back-to-school season, and technical debt being carried over to the 5G ecosystem by edge computing."},{"id":196,"type":"News Brief","title":"2023-2-19","tags":null,"body":"\n\n## Artificial Intelligence\n\nThe ethics and regulations of using generative AI-enabled Chatbots are starting to come into play as more people are starting to use ChatGPT and other. Chatbots will now need to include a Warning: We may not produce output that is real. Believe at your own risk. You revoke your right to sue us if you reproduce conversations, images, and videos for high-risk use cases that cause hate, harassment, violence, self-harm, illegal activity, deception, discrimination, or spam.\n\n[https://swisscognitive.ch/2023/02/15/honest-lying-why-scaling-generative-ai-responsibly-is-not-a-technology-dilemma-in-as-much-as-a-people-problem/](https://swisscognitive.ch/2023/02/15/honest-lying-why-scaling-generative-ai-responsibly-is-not-a-technology-dilemma-in-as-much-as-a-people-problem/)\n\nAs AI becomes increasingly important in various industries, the role of Chief AI Officer (CAIO) is emerging as a new addition to the C-Suite. A good CAIO should clearly define the desired impact of AI projects and continuously focus the organization on delivering that outcome. A great CAIO should also track the real impact of those projects and update the business on their performance over time. This role therefore escalates the expectations for driving transformation and impact, as well as anticipating and responding to competitors' use of AI.\n\n[https://applieddatascience.medium.com/the-case-for-the-chief-ai-officer-the-newest-c-suite-role-d55cca40c01](https://applieddatascience.medium.com/the-case-for-the-chief-ai-officer-the-newest-c-suite-role-d55cca40c01)\n\nIBM’s latest supercomputer is taking several innovative new directions – it is IBM’s first AI-optimized, cloud-native supercomputer, and it is housed entirely within the IBM Cloud rather than on-premises like traditional supercomputers. According to IBM, “this is the go-to environment for IBM Researchers creating most advanced AI capabilities, including work on foundation models and a place where they collaborate with partners on model creation.\"\n\n[https://futurumresearch.com/research-notes/ibms-latest-supercomputer-is-cloud-based-and-ai-focused/](https://futurumresearch.com/research-notes/ibms-latest-supercomputer-is-cloud-based-and-ai-focused/)\n\n## Ubiquitous Compute\n\nAccording to a study by 451 Research commissioned by Oracle Cloud Infrastructure, almost every cloud journey in enterprises is now becoming a multicloud journey. The study found that 98% of enterprises surveyed are using or plan to use at least two cloud service providers. The top drivers of multicloud strategies are data sovereignty and cost optimization. Enterprises are proactively planning multicloud strategies for the future, with data redundancy being the most anticipated future use case.\n\n[https://www.cloudcomputing-news.net/news/2023/feb/21/98-of-firms-using-public-cloud-adopt-multicloud-infrastructure-provider-strategy/](https://www.cloudcomputing-news.net/news/2023/feb/21/98-of-firms-using-public-cloud-adopt-multicloud-infrastructure-provider-strategy/)\n\nThe \"supercloud,\" a cloud architecture that allows seamless migration of applications across different cloud providers, was first proposed by Cornell University researchers in 2016. The concept has resurfaced as a solution to the challenges of multi-cloud, providing a homogeneous network to tie cloud resources together, resulting in seamless migration, consistent security, and optimal performance. \n\n[https://www.cloudcomputing-news.net/news/2023/feb/22/here-comes-the-supercloud-what-does-it-mean-for-multi-cloud-complexity/](https://www.cloudcomputing-news.net/news/2023/feb/22/here-comes-the-supercloud-what-does-it-mean-for-multi-cloud-complexity/)\n\nDemand for DRAM for servers to exceed that for smartphones due to the growing use of cloud, AI, and HPC apps; DRAM content for servers forecast to increase by 12.1% YoY in 2023 compared to 6.7% for smartphones. Server memory will make up 37.6% of total DRAM bit output, up from 36.8% for mobile DRAM. COVID-19 pandemic has driven demand for cloud services, resulting in sharp increase in server shipments.\n\n## Cybersecurity\n\nCybersecurity company Resecurity has warned of a series of cyberattacks that have targeted data centers globally over the past 18 months, resulting in data exfiltration and publishing of access credentials on the dark web. Although Resecurity did not name the victims, Bloomberg claims that major corporations, including Alibaba, Amazon, Apple, BMW, Goldman Sachs, Huawei Technologies, Microsoft, and Walmart, had their data center credentials stolen. T\n\n[https://www.csoonline.com/article/3688909/cyberattacks-hit-data-centers-to-steal-information-from-global-companies.html#tk.rss_all](https://www.csoonline.com/article/3688909/cyberattacks-hit-data-centers-to-steal-information-from-global-companies.html#tk.rss_all)\n\nCybersecurity firm Menlo Labs has uncovered an unknown threat actor group that is targeting government entities via a Discord-based campaign using the PureCrypter downloader. The campaign uses the domain of a compromised non-profit as a command and control hub to deliver a secondary payload, including the Redline Stealer, AgentTesla, Eternity, Blackmoon, and Philadelphia Ransomware. \n\n[https://www.menlosecurity.com/blog/purecrypter-targets-government-entities-through-discord](https://www.menlosecurity.com/blog/purecrypter-targets-government-entities-through-discord)\n\nThe biggest military cyber-warfare exercise in Western Europe occurred recently in Estonia, with 34 teams from 11 countries participating in a live-fire cyber battle. The seven-day event tested participants’ responses to common and complex cyber scenarios, including attacks on networks and industrial control systems (ICS). One scenario simulates attacks on uncrewed robotic systems. Teams from Italy, Estonia, and the UK were the top performers, judged based on their speed in identifying and responding to cyber threats.\n\n[https://www.securityweek.com/11-countries-take-part-in-military-cyberwarfare-exercise/](https://www.securityweek.com/11-countries-take-part-in-military-cyberwarfare-exercise/)\n\n## Embracing Digital Transformation Podcast\n\nCheck out this weeks full length episode \"Certifying Autonomous Flight\" where Darren interview LuukVan Dijk of Daedalean.\n\n[https://www.embracingdigital.org/episode-EDT126](https://www.embracingdigital.org/episode-EDT126)\n\n\n\n","guests":null,"link":"/brief-EDW3-en","image":"./briefs/edw-3/en/thumbnail.png","lang":"en","summary":"Summary"},{"id":197,"type":"News Brief","title":"2023-8-26","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Cybersecurity\n\n\nA new threat has emerged. The \"Flax Typhoon\" cyber group, believed to be linked to China, has been making waves. They are suspected of launching sophisticated cyberattacks on global organizations. Experts closely monitor this situation as it could have significant implications for digital security. Stay tuned for updates on this developing story. [Read More](https://thehackernews.com/2023/08/china-linked-flax-typhoon-cyber.html)\n\n\nCybersecurity companies are sounding the alarm as ransomware attacks surged more than 153% compared to one year ago and over 16% last month. Experts warn that organizations must bolster their defenses and educate employees about these threats to safeguard against costly data breaches. [Read More](https://www.securityweek.com/cybersecurity-companies-report-surge-in-ransomware-attacks/)\n\n\nThe cybersecurity sector is grappling with a severe talent shortage, raising expectations for solutions. The demand for skilled professionals in this ever-evolving field is soaring, yet the gap persists. To address this, companies are exploring strategies like upskilling current employees and promoting diversity in the industry. It might be time to go back to school to get cybersecurity certified. [Read More](https://www.helpnetsecurity.com/2023/08/25/cybersecurity-talent-shortage-expectations/)\n\n## Artificial Intelligence\n\n\nArtificial intelligence has given a paralyzed woman her voice back in an incredible development. Researchers at UCSF have employed AI technology to decode the woman's brain signals and translate them into speech. This groundbreaking achievement could help numerous individuals regain their ability to communicate. For more on this inspiring story, check out an in-depth article on ucsf.edu. [Read More](https://www.ucsf.edu/news/2023/08/425986/how-artificial-intelligence-gave-paralyzed-woman-her-voice-back)\n\n\nHigh schoolers are getting ready for the rise of artificial intelligence (AI). MIT researchers have launched a new initiative to help prepare students for the AI-driven future. This program aims to equip young minds with the skills and knowledge needed to thrive in a world increasingly influenced by AI technology. Check out Darren's interview with Pete Schmitz on the Embracing Digital Transformation Podcast episode, Training the Next Generation in AI, to get a close-up look at what is happening today. [Read More](https://news.mit.edu/2023/how-to-help-high-schoolers-prepare-rise-of-artificial-intelligence-0824)\n\n\nIn a thought-provoking DeZeen in-depth analysis, the environmental cost of artificial intelligence (AI) is scrutinized. As AI applications expand, so does their energy consumption, potentially exacerbating climate concerns. This critical examination explores the sustainability challenges posed by AI and the urgent need for eco-friendly solutions in developing and deploying AI technology. Stay tuned for deeper insights into this pressing environmental issue. [Read More](https://www.dezeen.com/2023/08/26/dezeen-in-depth-examines-the-environmental-cost-of-artificial-intelligence/)\n\n## Ubiquitous Computing\n\n\nAs we move closer to the quantum computing era, safeguarding critical infrastructure becomes even more crucial. C4ISRNET delves into the challenges and strategies for protecting vital systems in this new technological landscape. The potential for quantum computing to break current encryption methods raises concerns, highlighting the need for innovative cybersecurity solutions. [Read More](https://www.c4isrnet.com/it-networks/2023/08/21/how-to-protect-critical-infrastructure-in-the-quantum-computing-era/)\n\n\nAs reported by XDA developers, Dropbox is ending its unlimited cloud storage policy. This decision comes in response to instances of abuse by some users. As a result, Dropbox is implementing new storage limitations, which marks a substantial change for its user base. This development underscores cloud storage providers' ongoing challenges in balancing user demands with sustainable business models. [Read More](https://www.xda-developers.com/dropbox-ends-unlimited-cloud-storage-policy-abused/)\n\n\nThe global Pandemic in 2020 has revitalized a tech sector suffering stagnate growth, Virtual Desktop Infrastructure. Microsoft and VMware are reshaping business VDI and cloud technology. SiliconANGLE discusses how these technology giants collaborate to redefine VDI and enhance cloud solutions. This partnership will offer businesses more flexibility, scalability, and performance in IT environments. [Read More](https://siliconangle.com/2023/08/24/microsoft-vmware-redefining-business-vdi-cloud-technology-vmwareexplore/)\n\n## Embracing Digital Transformation Podcast\n\nThis week, the Embracing Generative AI series continues, featuring interviews with high school teachers who have adopted ChatGPT and exploring new use cases for generative AI. Subscribe to Embracing Digital Transformation for in-depth discussions on current hot topics. [Read More](http://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW30-en","image":"./briefs/edw-30/en/thumbnail.png","lang":"en","summary":"Here are the latest updates on digital transformation news for the week of August 27, 2023. This week's stories cover Cybersecurity, Artificial Intelligence, and Ubiquitous Computing. In this episode, you'll find stories on a significant increase of 150% in Ransomware attacks, how Quantum Computing has exposed IoT cybersecurity vulnerabilities, and how AI is helping to restore the voice of those who are paralyzed."},{"id":198,"type":"News Brief","title":"2023-9-2","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\nWe are in the throws of the digital revolution. During this time, it can be challenging to sift through all of the hype and find what works, what is still viable two years from now, and what contributes to your organization. Many IT organizations need help with confusing messages and conflicting technologies. We help organizations sort through the chatter and embrace digital transformation. The world of digital transformation is constantly changing, and you need to know what is viable today.  Listen to the podcast weekly for the latest news in cybersecurity, advanced communications, data management, artificial intelligence, edge, and cloud computing.\n\n<iframe width=\"100%\" height=\"400\" frameborder=\"no\" scrolling=\"no\" seamless src=\"https://share.transistor.fm/e/embracing-digital-this-week/playlist\"></iframe>\n\n<iframe width=\"560\" height=\"400\" src=\"https://www.youtube.com/embed/videoseries?list=PLj-81kG3zG5Ztc-Qinloap5jcSY8n6x-X\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n\n## Cybersecurity\n\n\n\nA recent cyber attack, believed to be backed by China, has caused concerns for national security in Guam. The Volt Typhoon hacker group targeted Guam's telecommunication and power grid to establish a stealthy presence. Guam is considered a vital logistics hub for the US in any conflict with China. The hackers took advantage of a vulnerability in the Fortinet Fortigate firewall, a leading cybersecurity protection product. This attack highlights a new threat as nation-state hackers increasingly target cybersecurity tools crucial for protecting systems. [Read More](https://www.msn.com/en-us/news/other/cybersecurity-tools-are-new-targets-for-nation-state-hackers/ar-AA1g5mib)\n\n\nA cyber attack caused a week-long technology outage at Hospital Sisters Health System, including St. Elizabeth's in O'Fallon, Illinois. CEO Damond Boatright confirmed the incident disrupted internet service, websites, telephones, and computer applications. While progress is being made to restore critical systems, the HSHS website remains down, and patient access to online charts and doctor communication has been disrupted. This is the second attack on hospital groups in the United States in two months. [Read More](https://www.bnd.com/news/local/article278879724.html)\n\n\n\nCommunity colleges are facing a growing cybersecurity threat: \"ghost students.\" These fictitious identities exploit enrollment processes to access academic resources, potentially compromising student data and institutional integrity. With hackers becoming increasingly sophisticated, community colleges invest in cybersecurity measures to combat this menace. The battle against \"ghost students\" highlights the need for robust digital security in educational institutions to protect sensitive information and ensure a safe learning environment. [Read More](https://www.chronicle.com/article/meet-the-cybersecurity-threat-haunting-community-colleges-ghost-students)\n\n## Artificial Intelligence\n\n\nThe use of AI in higher education has sparked debate between those who see its potential for personalization and efficiency and those who worry about ethics and the loss of human connection. Careful consideration is necessary for integrating AI in college classrooms. College professor Laura Torres Newey shared her approach to tackling this issue in an Embracing Digital Transformation podcast episode. Her insights reveal the challenges and opportunities of incorporating AI in higher education. [Read More](https://theconversation.com/should-ai-be-permitted-in-college-classrooms-4-scholars-weigh-in-212176)\n\n\nThe entertainment industry faces labor disputes, with artificial intelligence (AI) at the heart of the conflict. The Screen Actors Guild strike has entered its 8th week, with workers in Hollywood demanding fair compensation and creative control in the use of AI-driven technology. This dispute reflects the industry's struggle to balance the benefits of AI, like cost-efficiency and innovation, with concerns about job displacement and artistic autonomy. As the entertainment landscape evolves, resolving these AI-related labor issues will be crucial for workers and the industry's future. The world is watching! [Read More](https://www.pbs.org/newshour/show/why-artificial-intelligence-is-a-central-dispute-in-the-hollywood-strikes)\n\n\nThe regulation of artificial intelligence (AI) is a global concern, and countries are taking diverse approaches to address it. China, Israel, and the European Union (EU) are at the forefront of AI governance. China focuses on AI ethics and data security, Israel promotes innovation through regulatory sandboxes, and the EU proposes strict AI rules to ensure accountability. As AI continues to shape various industries and aspects of daily life, finding the right balance between innovation and regulation remains a crucial challenge on the global stage. [Read More](https://www.washingtonpost.com/world/2023/09/03/ai-regulation-law-china-israel-eu/)\n\n## Edge Computing\n\n\nIntel is fortifying edge computing security by introducing a groundbreaking layer of protection. The company's new technology promises to safeguard edge devices and data, addressing critical cybersecurity concerns. With edge computing's growing importance in various industries, this development represents a significant stride in ensuring the integrity and safety of distributed computing environments. [Read More](https://www.fool.com/investing/2023/09/01/intel-adds-a-layer-of-protection-to-edge-computing/)\n\n\nEnsuring the security of the Internet of Things (IoT) is crucial to safeguarding critical infrastructure such as power grids and water treatment plants. As IoT devices become increasingly integrated into these systems, protecting them from evolving threats, including potential risks associated with certain countries, is a pressing issue for the United States. The challenges and implications of Chinese technology in IoT are currently under close examination, highlighting the need for proactive measures to safeguard critical infrastructure in the country. [Read More](https://www.forbes.com/sites/davealtavilla/2023/09/03/securing-the-iot-from-the-threat-china-poses-to-us-infrastructure/?sh=4325a3f112c0)\n\n\nEdge computing is not just for manufacturing and critical infrastructure management. Edge computing is revolutionizing healthcare by enabling real-time data processing, which leads to faster response times for essential patient data. It supports remote monitoring and telemedicine, making healthcare more accessible and reducing the strain on healthcare facilities. Additionally, edge computing enhances data privacy and security by processing sensitive patient information locally, reducing the risk of data breaches and ensuring compliance with healthcare regulations. [Read More](https://www.ft.com/partnercontent/ntt-ltd/edge-computing-delivers-healthcare-beyond-the-clinic.html?blaid=3846770)\n\n## Embracing Digital Transformation\n\n\n\nThe series embracing generative AI continues with interviews about becoming data-ready in the GenAI revolution and how higher education deals with generative AI in the classroom.  [Read More](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW31-en","image":"./briefs/edw-31/en/thumbnail.png","lang":"en","summary":"News for the week of September 3, 2023 on Embracing Digital This week, there are updates on cybersecurity, artificial intelligence, and edge computing. China&#39;s cyber-attacks on military bases and critical infrastructure, Intel&#39;s efforts to enhance security in the IoT space, and governments&#39; challenges in regulating AI."},{"id":199,"type":"News Brief","title":"2023-9-9","tags":["ai","gpt4","openai","microsoft","generativeai","environmentalimpact","irs","taxevasion","education","cybersecurity","iosspyware","zerodayvulnerability","northkoreanhackers","quantumrandomness","cloudfirst","cloudsmart","embracingdigital","generativeaipolicy","highereducation"],"body":"\n\n## Artificial Intelligence\n\n\nThe birthplace of GPT-4 has been uncovered! Des Moines, Iowa’s corn fields have been the center of the latest from OpenAI, but at a significant cost to the environment. Microsoft's latest environmental report revealed a 34% increase in worldwide water consumption from 2021 to 2022, amounting to almost 1.7 billion gallons. This surge is primarily due to the company's substantial investment in generative AI and partnership with OpenAI. The impact of generative AI on the environment is significant, and major organizations like Microsoft, Google, and OpenAI are taking steps to address these concerns. [Read More](https://apnews.com/article/chatgpt-gpt4-iowa-ai-water-consumption-microsoft-f551fde98083d17a7e8d904f8be822c4)\n\n\n\nThe Internal Revenue Service in the United States plans to implement AI technology to improve the supervision of large law firms, hedge funds, private equity firms, and real estate investors. The initiative aims to detect intricate tax evasion cases and recover owed federal revenue from affluent taxpayers. The IRS has allocated $80 billion from the Inflation Reduction Act to tackle patterns and trends in conducting significant audits and enforcing penalties against tax evaders in these industries.  [Read More](https://www.nytimes.com/2023/09/08/us/politics/irs-deploys-artificial-intelligence-to-target-rich-partnerships.html)\n\n\nEducators and scholars are at the forefront of addressing AI's role in education. As students head back to school this fall, several articles from journals worldwide cover AI applications, ethical concerns, augmentation over automation, preparing students for AI-integrated work, detecting deepfakes, preserving human common sense, and leveraging generative AI for teaching. These insights provide valuable guidance for shaping the future of AI in education. Also, check out Darren’s interview with the New York Times and professor of English Laura Torres Newey about how she is integrating AI in her classes this Fall. [Read More](https://daily.jstor.org/artificial-intelligence-and-education-a-reading-list/)\n\n## Cybersecurity\n\n\nChina is getting serious about Cybersecurity. China's new cybersecurity regulations aim to enhance digital security for the internet, but small businesses could face challenges. The full impact on businesses of all sizes remains unclear, and there may be repercussions for online privacy. Nonetheless, these measures are necessary to safeguard online data and protect against cyber threats. Individuals and businesses must stay up-to-date with these changes and take necessary precautions to secure their online presence. [Read More](https://www.scmp.com/news/china/politics/article/3233931/china-steps-cybersecurity-enforcement-smaller-businesses-are-feeling-heat)\n\n\nThe once immune iPhone System has come under attack this last week with spyware detected in the popular iOS that runs iPhones. Apple has taken swift action after discovering spyware threats. They've released essential software updates to keep your devices safe. It's like having a superhero protect your tech world! We'll dive into the details of these updates, how they thwarted potential security breaches, and what this means for your digital safety. Stay tuned as we unravel the latest developments in technology and cybersecurity. [Read More](https://www.washingtonpost.com/politics/2023/09/08/apple-issues-software-updates-after-spyware-discoveries/)\n\n\nNorth Korean hackers have reportedly exploited a zero-day vulnerability, which gives them access to a fortress of digital defenses. This cyberattack poses a significant threat to targeted systems, data breaches, and the overall cybersecurity landscape. Efforts are underway to counteract the threat, including patches and safeguards to protect against similar attacks in the future. [Read More](https://thehackernews.com/2023/09/north-korean-hackers-exploit-zero-day.html)\n\n## Ubiquitous Computing\n\n\nIntegrating generative AI systems is changing cloud architecture, imbuing it with an intelligent brain capable of learning and creating independently. This transformation reshapes cloud infrastructure, bolstering automation, optimizing resource allocation, and enhancing security measures. The infusion of generative AI systems into the cloud presents promising opportunities and challenges, and we will embark on an in-depth journey to unravel them. [Read More](https://www.infoworld.com/article/3706094/adding-generative-ai-systems-may-change-your-cloud-architecture.html)\n\n\nIn a groundbreaking achievement, MIT has taken control of quantum randomness for the first time. It's like taming the wild and unpredictable! We'll delve into the details of this remarkable feat, which could have profound implications for quantum computing and cryptography. Join us as we explore how MIT's breakthrough allows us to harness the inherent randomness of the quantum world, opening up new frontiers in technology and security. Time will tell what this impact will have on computing. [Read More](https://scitechdaily.com/harnessing-the-void-mit-controls-quantum-randomness-for-the-first-time/)\n\n\n'Cloud-first' is out, and 'Cloud-smart' is the new buzzword in tech. Think of it as moving from 'the cloud is the answer' to 'the cloud as a strategic ally.' We'll delve into the details of this shift, exploring what 'cloud-smart' means for businesses and IT strategies. Join us as we discuss how organizations evolve their approaches to leverage the cloud more intelligently, making it a central part of their digital transformation journeys. [Read More](https://www.infoworld.com/article/3705615/cloud-first-is-dead-cloud-smart-is-whats-happening-now.html)\n\n## Embracing Digital Transformation \n\n\n\nThis Week continues the series Embracing Generative AI in the podcast with a fascinating interview about developing a Generative AI Policy for your workplace and how Higher education is tackling generative AI in the classroom. Tune in to the latest episodes on Tuesdays and Thursdays. [Read More](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW32-en","image":"./briefs/edw-32/en/thumbnail.png","lang":"en","summary":"Get the latest digital news for the week of September 10, 2023 covering Artificial Intelligence, Cybersecurity, and Ubiquitous Computing. This week, learn about a new iPhone cyber threat, the birthplace of GPT-4, and MIT's approach to tackling quantum computing uncertainty."},{"id":200,"type":"News Brief","title":"2023-9-16","tags":["aisummit","rishisunak","chineseofficialsban","nationalsecurity","ukchinatensions","amazonselfpublishers","aigeneratedcontent","digitalpublishing","columbiauniversityresearch","aireliability","languagemodels","chatbotperformance","cognitivescience","webassembly(wasm)","edgecomputingrevolution","edgedevices","latencyreduction","secureexecution","iotapplications","mobilecomputing","ai","security","edge","embracingdigital"],"body":"\n\nWe are in the throws of the digital revolution. During this time, it can be challenging to sift through all of the hype and find what works, what is still viable two years from now, and what contributes to your organization. Many IT organizations need help with confusing messages and conflicting technologies. We help organizations sort through the chatter and embrace digital transformation. The world of digital transformation is constantly changing, and you need to know what is viable today.  Listen to the podcast weekly for the latest news in cybersecurity, advanced communications, data management, artificial intelligence, edge, and cloud computing.\n\n<iframe width=\"100%\" height=\"400\" frameborder=\"no\" scrolling=\"no\" seamless src=\"https://share.transistor.fm/e/embracing-digital-this-week/playlist\"></iframe>\n\n<iframe width=\"560\" height=\"400\" src=\"https://www.youtube.com/embed/videoseries?list=PLj-81kG3zG5Ztc-Qinloap5jcSY8n6x-X\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n\n## Artificial Intelligence\n\n\nUK Chancellor Rishi Sunak is mulling a ban on Chinese officials attending 50% of the upcoming AI Summit due to national security concerns. The move reflects growing tensions between the UK and China over technology and intellectual property. Details of the potential ban are yet to be finalized. The AI Summit is a prominent event for global tech collaboration. [Read More](https://www.theguardian.com/technology/2023/sep/15/rishi-sunak-considers-banning-chinese-officials-from-half-of-ai-summit)\n\n\nAmazon now requires self-publishers to disclose any involvement of AI in their works to increase transparency in the digital publishing world. This move comes in response to concerns about the market being flooded with AI-generated content. These new rules will impact the self-publishing industry on Amazon. [Read More](https://www.theguardian.com/books/2023/sep/11/self-publishers-must-declare-if-content-sold-on-amazons-site-is-ai-generated)\n\n\n\nResearch from Columbia University highlights concerns over the reliability of AI in decision-making. The study found that even advanced language models can mistake nonsense for natural language, raising questions about accuracy. This offers an opportunity to improve chatbot performance and understand human language processing. By bridging the gap between AI and cognitive science, we can create more helpful and effective AI-powered assistants that better serve their users. [Read More](https://www.sciencedaily.com/releases/2023/09/230914114704.htm)\n\n## Edge Computing\n\n\nWebAssembly (Wasm) is poised to revolutionize edge computing. This technology allows running code from different languages on edge devices, enhancing flexibility and performance. By reducing latency and enabling secure, efficient execution, Wasm can transform how applications are deployed and executed at the edge. Developers can leverage Wasm's potential to create more dynamic and responsive edge computing solutions, enhancing various industries like IoT and mobile computing. [Read More](https://www.infoworld.com/article/3703052/how-webassembly-will-transform-edge-computing.html)\n\n\n\nThe race to move AI to the edge is heating up! Cadence Design Systems has unveiled new AI intellectual property (AI IP) software tools that offer offloading capabilities from host processors. These tools aim to optimize AI workloads, improving performance and energy efficiency in edge devices. The technology enables more efficient execution of AI tasks, making it suitable for various applications, including IoT and edge computing, where resource optimization is crucial. Cadence's AI IP software tools can help accelerate AI adoption in edge devices. [Read More](https://www.edgeir.com/cadences-new-ai-ip-software-tools-promises-offloading-abilities-from-any-host-processor-20230914)\n\n\n\nEdge Computing conferences are in full swing this fall, with no less than seven major conferences this fall worldwide. Starting the last week in September in Amsterdam, traveling through Toronto, Dubai, Paris, and London, to name a few places. Dust off your travel bags this fall will be busy for the IoT companies. You can find the complete list at edgier.com. [Read More](https://www.edgeir.com/edge-computing-events)\n\n## Cybersecurity\n\n\nMicrosoft's Security Response Center has identified critical vulnerabilities in the open-source software library NCurses, used in Unix-like systems (Linux and iOS). These flaws could allow attackers to execute malicious code or cause denial-of-service attacks. Users are urged to update their NCurses installations to the latest version to mitigate potential risks. Vulnerabilities in widely used libraries like NCurses underscore the importance of regular security updates and patch management in the software ecosystem. [Read More](https://thehackernews.com/2023/09/microsoft-uncovers-flaws-in-ncurses.html)\n\n\nThe Free Download Manager (FDM) website, a very popular site for free and open-source software,  has been compromised, exposing users to potential risks. Attackers injected malicious code into the site, potentially affecting users who downloaded the software between September 11 and 12, 2023. This breach emphasizes the importance of downloading software only from trusted sources and regularly updating security software to protect against potential threats. FDM has taken steps to address the issue, but users should remain vigilant. [Read More](https://thehackernews.com/2023/09/free-download-manager-site-compromised.html)\n\n\nIranian nation-state actors have reportedly been utilizing sophisticated spear-phishing attacks to target individuals in the United States, the Middle East, and Asia. The attacks involve deceptive emails impersonating reputable organizations aimed at delivering malware and stealing sensitive information. Security experts emphasize the need for robust email security measures and user awareness to defend against such cyber threats orchestrated by nation-state actors. [Read More](https://thehackernews.com/2023/09/iranian-nation-state-actors-employ.html)\n\n## Embracing Digital Transformation\n\n\n\nThis week continues the Embracing Generative AI series with guests talking about operationalizing generative AI in the workplace, including using it to enhance security and generate natural language reports. [Read More](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW33-en","image":"./briefs/edw-33/en/thumbnail.png","lang":"en","summary":"Stay up-to-date with the latest news in digital transformation for the week of September 17, 2023. Get the latest updates on artificial intelligence, edge computing, and cybersecurity. This week&#39;s highlights include Microsoft uncovering vulnerabilities in their competitors&#39; software, Amazon implementing AI disclosure requirements, and a comprehensive list of upcoming IoT conferences this fall."},{"id":201,"type":"News Brief","title":"2023-9-23","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Artificial Intelligence\n\n\nPhysicists are developing generative models for AI using well-understood physical equations. The Poisson flow generative model (PFGM), representing data with charged particles, can produce high-quality images 10 to 20 times faster than diffusion models. Researchers aim to explore more physical processes and refine the PFGM by adjusting its dimensionality. [Read More](https://www.quantamagazine.org/new-physics-inspired-generative-ai-exceeds-expectations-20230919/ )\n\n\nHow about running a chatbot on your PC? Intel has announced a new chip, expected in December, that will enable laptops to run generative artificial intelligence chatbots without relying on cloud data centers. This capability, demonstrated at their software developer conference, allows businesses and consumers to test AI technologies like ChatGPT without sending sensitive data to the cloud. [Read More](https://www.reuters.com/technology/intel-says-newest-laptop-chips-software-will-handle-generative-ai-2023-09-19/)\n\n\nThe open-source community's growth is making waves in AI development. Not new to innovation, Open-source projects like Hadoop and Spark have enabled developers to advance AI development since the 1970s. AI generative models like ChatGPT and Llama 2 are built upon open-source foundations, challenging the proprietary status quo of AI programs. Developers will continue to push the edge of AI outside of the bounds of commercial development. [Read More](https://www.zdnet.com/article/why-open-source-is-the-cradle-of-artificial-intelligence/)\n\n## Ubiquitous Compute\n\n\nIntel has launched its 'Developer Cloud' platform for developers to test and deploy AI and high-performance computing applications using the latest hardware platforms. It supports AI training, model optimization, and inference workloads and is based on open software with oneAPI, allowing hardware choice and code portability. Other AI-related technologies were also introduced during the announcement. Anyone can sign up at cloud.intel.com. [Read More](https://ummid.com/news/2023/september/20-09-2023/intel-developer-cloud-reaches-general-availability.html)\n\n\nTo help curb the mystical and costly data management expenses, WiMi Hologram Cloud has developed a new holographic data compression algorithm that enhances cloud computing efficiency by reducing data transmission and storage needs. These advancements and a distributed image storage protocol are meeting the growing demand for advanced holographic technology. This should help organizations that are suffering from data egress costs in the cloud. [Read More](https://beststocks.com/wimi-hologram-cloud-revolutionizing-cloud-com/ )\n\n\nAt Intel's Innovation 2023 event, the company unveiled a 288-core CPU as part of its 'Sierra Forest' lineup for high-density servers. This CPU will have 144 cores on its dual chiplets, totaling 288 cores and threads. Intel also confirmed that its 5th-gen Xeon family, 'Emerald Rapids,' will launch on December 14, offering performance improvements while maintaining power consumption. Intel emphasized its focus on AI applications, with a supercomputer for AI tasks using Xeon processors and Gaudi2 AI accelerators.  [Read More](https://www.techspot.com/news/100221-intel-announces-288-core-sierra-forest-xeon-processor.html)\n\n## Cyber Security\n\n\nScattered Spider, a skilled hacking group, is gaining notoriety for ransomware attacks on companies. Analysts suggest the group is mostly 17-22-year-olds from Western countries. They use advanced social engineering techniques, like SIM swapping, and tactics such as \"SWATing\" to identify privileged access points. While their motive may not be monetary, their attacks have disrupted various sectors and prompted law enforcement investigations. [Read More](https://www.reuters.com/technology/power-influence-notoriety-gen-z-hackers-who-struck-mgm-caesars-2023-09-22/)\n\n\nA report on cybercrime trends in India reveals a surge in cyberattacks, including phishing, malware, and financial fraud, with COVID-19 exacerbating the situation. The findings underscore the urgency for heightened cybersecurity measures, public awareness, and collaboration between law enforcement and private sectors to combat cyber threats effectively. Education on cybersecurity best practices is also emphasized to protect individuals from such attacks. [Read More](https://www.theregister.com/2023/09/21/india_cybercrime_trends_report/)\n\n\nTo help combat the cyber security threat, Microsoft Azure is trying to increase cyber security through education. Azure has recently announced over 60 hours of free courses on cybersecurity, including hybrid cloud security, firewall configuration, patch management, Active directory management, and encryption technologies, to name a few. You can find out more at learn.microsoft.com. [Read More](https://www.helpnetsecurity.com/2023/09/20/free-microsoft-azure-cybersecurity-resources/)\n\n## Embracing Digital Transformation\n\nThis week starts a new series on Building a Cloud Strategy with cloud solutions architects from Intel. Darren interviews Intel’s experts in cloud migration, cloud cost management, and workload evaluation. Check out the latest on the Embracing Digital Transformation podcast. [Read More](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW34-en","image":"./briefs/edw-34/en/thumbnail.png","lang":"en","summary":"Digital Transformation news for September 24, 2023, includes artificial intelligence, ubiquitous computing, and cybersecurity. Listen about Intel’s new CPUs changing AI and high-density computing and how a hacker group of teenagers took down Las Vegas’ MGM casinos."},{"id":202,"type":"News Brief","title":"2023-9-30","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","datasciencesolution","polystores","dataarchitecture","aichatbots","cybersecurityawareness","aisecuritycenter","googlechrome","mozillafirefox","digitaltransformationpodcast","intelcloudsolutionarchitects"],"body":"\n\n## Data Management \n\nBMLL Vantage, a data science solution, was honored with the 'Best Data Science Solution' at the Data Management Insight Awards. This accolade celebrates BMLL Vantage's exceptional work in data science, acknowledging its significant contributions to the industry. [Read More](https://mondovisione.com/media-and-resources/news/bmll-vantage-wins-best-data-science-solution-at-the-data-management-insight-aw/)\n\n\n\nPolystores integrate diverse data sources seamlessly, breaking down silos and enabling cross-functional analysis. They optimize performance by leveraging various database strengths and adapt to evolving technologies. Organizations should use polystores to unlock data potential for informed decision-making in the era of big data. [Read More](https://thenewstack.io/polystores-the-data-management-game-changer/)\n\n\nWhen deciding on a data architecture, it's essential to consider factors such as data size, frequency, source location, type, privacy, access, and urgency. Understanding the different types of architectures and their uses can help you adapt to changes in business drivers and environmental conditions and choose the best solution for your company. Considering these factors has been shown to improve systems architects' decision-making abilities. [Read More](https://www.intel.com/content/www/us/en/content-details/789953/content-details.html)\n\n## Artificial Intelligence\n\n\nMark Zuckerberg revealed a set of AI chatbots at Meta Connect 2023, including one with rapper Snoop Dogg as a Dungeon Master in the role-playing game D&D. These bots, modeled after celebrities, aren't available to the public. Meta AI is the leading chatbot assistant, built on a custom-made language model similar to OpenAI's ChatGPT, prioritizing visual flair over mimicking conversation patterns. Plans include adding voice capabilities. [Read More](https://kotaku.com/meta-quest-3-ai-chatgpt-snoop-dogg-facebook-chatbot-1850882666)\n\n\nIsraeli Prime Minister Netanyahu warns of the potential dangers of the AI revolution, such as disruptions to democracy, mind manipulation, job losses, and AI-driven wars. He urges nations to address these concerns and prevent self-taught machines from controlling humans. However, he also acknowledges the positive aspects of AI, such as robotic assistance for the elderly and improved transportation. Israel aims to be a global leader in AI. [Read More](https://www.foxnews.com/world/netanyahu-warns-potential-eruption-ai-driven-wars-lead-unimaginable-consquences)\n\n\nAn OpenAI employee, Lilian Weng, shared her emotional experience using ChatGPT's voice mode for a personal conversation about stress and work-life balance. While Weng found the interaction comforting, it raises concerns about AI's role in providing therapy. This reflects a trend where AI aims to appear more human but has faced challenges, as seen with previous AI therapy experiments receiving mixed responses and, in some cases, causing harm. Ethical considerations are crucial when integrating AI into mental health contexts. [Read More](https://fortune.com/2023/09/28/generative-ai-cfos-company-strategy/)\n\n## Cybersecurity\n\n\nOctober marks the 20th Cybersecurity Awareness Month to promote online safety. The theme this year is \"Secure Our World.” It focuses on four critical cybersecurity best practices: using a password manager, implementing multifactor authentication, recognizing and reporting phishing attempts, and regularly installing updates. The initiative aims to provide information to help individuals stay safer online. [Read More](https://www.ktsm.com/local/octobers-cybersecurity-awareness-month-2023/)\n\n\nThe US National Security Agency has created an AI security center to oversee AI in defense and intelligence services. The director, General Paul Nakasone, stressed the importance of keeping the US ahead in AI and preventing foreign actors from stealing US innovations. The center will promote secure AI adoption in national security and defense industries. AI will have a significant role in national security, diplomacy, technology, and the economy. [Read More](https://www.aljazeera.com/news/2023/9/29/us-national-security-agency-unveils-artificial-intelligence-security-centre)\n\n\nA new security issue has been found in Google Chrome and Mozilla Firefox browsers. This could affect other programs, too. It's called \"StrangeU.\" Hackers could use it to control your computer. The companies have fixed the problem, but other programs could still be at risk. Keep all your software updated and watch for similar issues in other apps. Stay safe! [Read More](https://arstechnica.com/security/2023/09/new-0-day-in-chrome-and-firefox-is-likely-to-plague-other-software/)\n\n## Embracing Digital Transformation Podcast\n\nThis week continues the series on multi-hybrid cloud architectures with additional interviews from Intel's cloud solution architects. To listen to these 30-minute interviews, you can find embracing digital transformation on your favorite podcasting sites.  [Read More](https://www.embracingdigital.org/episode-EDT163)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW35-en","image":"./briefs/edw-35/en/thumbnail.png","lang":"en","summary":"Digital transformation news for the week of October 1st including stories in data management, artificial intelligence, and cybersecurity. Come listen to the celebration of cybersecurity awareness month, an AI that makes Snoop Dogg a dungeon master for D&amp;D, and learn about a new data architecture Polystore."},{"id":203,"type":"News Brief","title":"2023-10-7","tags":["ai","edge","cybersecurity","precrime","aihealcare","mgmbreach","scatteredspider","aplhav","aiethics","convergeditot","iot","criticalinfrastructure"],"body":"\n\n## Cybersecurity\n\n\nOn October 5th, MGM Resorts International announced that a data breach and ransomware attack had caused disruptions, resulting in a $100 million loss during the third quarter. The hacking groups AlphaV and Scattered Spider claimed responsibility for the breach, in which they allegedly stole data from the MGM system and held it for extortion. [Read More](https://www.reuters.com/business/mgm-expects-cybersecurity-issue-negatively-impact-third-quarter-earnings-2023-10-05/)\n\n\nSecurityWeek has reported the discovery of backdoored firmware in Android devices used by U.S. schools. Researchers have identified vulnerabilities in laptops and tablets distributed to educational institutions, which could potentially expose sensitive student data to cyber threats. The compromised firmware could allow cybercriminals to access and manipulate the devices remotely if exploited.  [Read More](https://www.securityweek.com/android-devices-with-backdoored-firmware-found-in-us-schools/)\n\n\nIn the context of Cybersecurity Awareness Month 2023, it is emphasized in SC Media that more than just promoting cybersecurity awareness is required. SC Media calls on organizations to increase the intelligence sharing on cyber attacks and cyber security best practices and debunks the myths of sharing valuable information across the Cybersecurity industry. [Read More](https://www.scmagazine.com/perspective/cybersecurity-awareness-month-2023-why-we-need-more-than-just-cybersecurity-awareness)\n\n## Artificial Intelligence\n\n\nAccording to Freedom House's 2023 \"Freedom on the Net\" report, governments worldwide are using AI for surveillance and censorship. China is leading the way in AI-powered censorship. This highlights the need to balance technological advancement with protecting individual rights and digital freedoms. The report calls out the need for Global efforts to safeguard individuals' digital freedom. [Read More](https://gizmodo.com/freedom-house-2023-freedom-on-the-net-report-ai-1850887842)\n\n\nA recent article published in Nature discusses research into utilizing AI algorithms to predict and manage epileptic seizures. The study presents promising results in accurately forecasting seizures in a group of patients, offering the potential for more effective seizure management and improved patient outcomes. AI-driven seizure prediction could significantly enhance the quality of life for individuals living with epilepsy and reduce the associated risks. [Read More](https://www.nature.com/articles/s41746-023-00931-7)\n\n\nAccording to an NBC Bay area report, a UC Berkeley professor is pioneering using artificial intelligence (AI) to combat domestic violence. The professor is developing AI algorithms to analyze social media posts and text messages for signs of domestic abuse, aiming to provide early intervention and support to victims. This innovative application of AI technology has the potential to make a significant impact in identifying and addressing domestic violence issues in the digital age. Sounds like the beginning of Pre-Crime in the Bay Area! [Read More](https://www.nbcbayarea.com/news/local/uc-berkeley-professor-artificial-intelligence-domestic-violence/3336053/)\n\n## Edge Computing\n\n\nResearchers are issuing a warning about the exposure of approximately 100,000 industrial control systems (ICS) on the internet, according to BleepingComputer. This alarming finding raises concerns about potential cyberattacks on critical infrastructure and manufacturing facilities. The exposed ICS devices pose a significant security risk, highlighting the urgent need for improved cybersecurity measures to safeguard critical infrastructure from potential threats and attacks. [Read More](https://www.bleepingcomputer.com/news/security/researchers-warn-of-100-000-industrial-control-systems-exposed-online/)\n\n\nAs discussed in a report on Game Is Hard, edge computing is emerging as a notable alternative to NVIDIA's dominance in the AI chip market. With the rise of edge computing, more companies are exploring AI chip solutions tailored for local processing and real-time data analysis. This trend reflects the evolving landscape of AI technology and competition in the market, potentially offering new options and innovations for various industries beyond NVIDIA's traditional stronghold. [Read More](https://gameishard.gg/news/the-rise-of-edge-computing-an-alternative-to-nvidias-dominance-in-the-ai-chip-market/282141/)\n\n\nForbes Tech Council's article explores how Generative AI addresses skills gaps in the convergence of industrial IT and operational technology (IoT). By automating tasks, analyzing data, and optimizing processes, Generative AI is helping bridge the divide between these traditionally separate domains. This innovative application of AI technology has the potential to enhance efficiency and productivity in industrial settings, enabling smoother integration and operation of IT and IoT systems. [Read More](https://www.forbes.com/sites/forbestechcouncil/2023/10/04/how-generative-ai-fills-skills-gaps-for-industrial-itot-convergence/)\n\n## Embracing Digital Transformation News\n\nThis week Darren releases the last episode of Embracing Multi-Hybrid Cloud series focused on continuous improvement in operationalizing a Cloud strategy. Listen on any podcasting site at Embracing Digital Transformation. [Read More](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW36-en","image":"./briefs/edw-36/en/thumbnail.png","lang":"en","summary":"Get the latest news on Digital Transformation for the week of October 8, 2023. This week&#39;s highlights include a report on MGM Resorts losing $100 million due to a cyber attack, how AI is being used to assist victims of domestic violence, and the impact of Edge computing on AI data centers. Tune in to stay informed."},{"id":204,"type":"News Brief","title":"2023-10-14","tags":["compute","edge","israelcyberattacks","hamasconflict","redalertsystemapp","powerplantcyberthreats","generativeai","cloudmigration","cmauk","publiccloudinvestigation","avoslocker","ransomeware","criticalinfrastructure","ecybersecurity","embracingdigital","edw37","zerotrustarchitecture"],"body":"\n\n## Cybersecurity\n\n\nIsrael battles cyberattacks during the Hamas conflict. The 'Red Alert System' app was compromised to send false alerts and political messages. Israeli power plants were also hit. CEO of Secure Cyber Defense, Shawn Waldman, warns of ongoing cyber threats, highlighting recent intelligence on foreign actors targeting critical U.S. infrastructure. He emphasizes the need for continued progress in cybersecurity. [Read More](https://www.wdtn.com/as-seen-on-2-news/israeli-cyberwarfare-cyberattacks-infrastructure/)\n\n\nIsrael and Palestine saw a surge in internet traffic after Israel declared war on Hamas on October 7, 2023. Cloudflare's data reveals that cyberattacks targeting Israel increased, including DDoS attacks on Israeli newspapers. In Palestine, internet disruptions were observed, potentially related to power outages. Cloudflare is monitoring these trends and offers tools for tracking internet traffic patterns. [Read More](https://blog.cloudflare.com/internet-traffic-patterns-in-israel-and-palestine-following-the-october-2023-attacks/)\n\n\nRansomware attacks are becoming more sophisticated, with networking devices being increasingly exploited for delivery. Protected Health Information in the healthcare sector is a prime target. High-income organizations, especially in the US, are preferred targets. Newer groups are emerging, and languages like Rust and GoLang are being adopted. Organizations are enhancing cybersecurity measures, and Cyble Vision is recommended to stay ahead of ransomware threats. [Read More](https://thehackernews.com/2023/10/ransomware-attacks-doubled-year-on-year.html)\n\n## Edge Computing\n\n\nThe AvosLocker ransomware gang has been implicated in attacks against critical infrastructure sectors in the U.S. Their tactics include using legitimate software and open-source remote system administration tools to compromise networks, followed by data extortion threats. AvosLocker emerged in mid-2021 and employs techniques to disable antivirus protection, affecting Windows, Linux, and VMware ESXi environments. The group is known for using open-source tools and living-off-the-land (LotL) tactics to avoid attribution. CISA and the FBI recommend mitigation measures for critical infrastructure organizations, including application controls, limiting remote desktop services, restricting PowerShell use, and maintaining offline backups. Ransomware attacks have surged in 2023, with attackers deploying ransomware rapidly after initial access. [Read More](https://thehackernews.com/2023/10/fbi-cisa-warn-of-rising-avoslocker.html)\n\n\nThe NSA has released ELITEWOLF, a repository of signatures and analytics on its GitHub, to enhance the security of Operational Technology (OT) and counter malicious cyber activity targeting critical infrastructure. Given the increasing threat to OT systems, the NSA recommends OT infrastructure owners and operators use ELITEWOLF as part of vigilant system monitoring. This initiative follows the Protect Operational Technologies and Control Systems against Cyber Attacks Cybersecurity Advisory. [Read More](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3554537/nsa-releases-a-repository-of-signatures-and-analytics-to-secure-operational-tec/)\n\n\nNATO has vowed to respond firmly to any confirmed deliberate attack on critical Baltic Sea infrastructure. The pledge comes in the wake of damage to underwater infrastructure in the region. Finland is investigating the incident, which may have involved external forces, including Russia. If proven to be a deliberate attack on NATO’s critical infrastructure, the organization will respond with a united and determined effort. [Read More](https://news.yahoo.com/attack-alliances-critical-infrastructure-deliberate-073708273.html)\n\n## Cloud Technology\n\n\nGenerative AI reduces cloud migration efforts by 30-50%, leveraging large language models (LLMs). LLMs can assess infrastructure, move workloads, and verify migration effectiveness. McKinsey's Bhargs Srivathsan noted that generative AI and the cloud are mutually beneficial, as the cloud enables generative AI, which in turn accelerates cloud migration. LLMs can be utilized for content generation, customer engagement, synthetic data creation, and coding. [Read More](https://www.theregister.com/2023/10/11/generative_ai_cloud_migration/)\n\n\nEnterprises with on-premises technology solutions face disruption while moving to full cloud. To address this challenge, the Hybrid Cloud approach allows companies to maintain core services on-premises. This approach adds new cloud capabilities to legacy infrastructure, reducing disruption and preserving tried-and-tested technology. For instance, a Hybrid Cloud can be used in contact center operations to layer chat and social digital channels on top of existing infrastructure, allowing organizations to handle fluctuations in call volume without significant disruption. [Read More](https://www.techradar.com/pro/why-businesses-should-stop-wondering-about-cloud-and-go-hybrid)\n\n\nThe UK's Competition and Markets Authority (CMA) investigates the public cloud market to ensure fair competition for businesses and individuals. The probe will scrutinize the operations of providers like AWS, Microsoft Azure, and Google Cloud to address concerns about anti-competitive behavior, data use, and barriers to entry. The CMA aims to encourage innovation, offer competitive prices, and provide customer choice. [Read More](https://www.techrepublic.com/article/cma-investigates-uk-public-cloud-market/)\n\n## Embracing Digital Transformation Podcast\n\n\n\nIn this week's episode of the Embracing Digital Transformation Podcast, Darren interviews Rachel Driekosen about how AI is being used to safeguard children online and bring perpetrators to justice. Stay tuned for an upcoming series on Zero Trust Architecture in the next few weeks. [Read More](https://www.embracingdigital.org/episode-EDT167-en)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW37-en","image":"./briefs/edw-37/en/thumbnail.png","lang":"en","summary":"Digital Transformation this Week for October 15, 2023, reports ongoing cyberattacks during the Israel-Hamas conflict and the compromising of the &#39;Red Alert System&#39; app. Additionally, generative AI has been shown to reduce cloud migration efforts while the UK&#39;s CMA investigates the public cloud market."},{"id":205,"type":"News Brief","title":"2023-10-21","tags":["keywords","ai","compute","cybersecurity","iranianhackers","cybersecuritybreach","ironnetclosure","secretmicrosoft365","cloudtrends","healthcareai","chinachipexports","openai","digitalnews","ubiquitouscomputing","digitaltransformation","dataencryption","aiethics","techindustry","emergingtech","intelinterview","aistrategies","quantumcomputing","datasecurity","techtrends"],"body":"\n\n## Cybersecurity\n\n\nIranian hackers were able to infiltrate a Middle Eastern government network and stay hidden for eight months before being discovered. The attack impacted various agencies, and the extent of the damage is still being assessed. Experts are investigating the incident and emphasizing the need for strong cybersecurity measures to protect against future attacks. The breach serves as a reminder of the importance of prioritizing cybersecurity in our increasingly digital world. [Read More](https://www.securityweek.com/iranian-hackers-lurked-for-8-months-in-government-network/)\n\n\nClark County School District in Nevada had to resort to pen-and-paper assignments after a ransomware attack disrupted their digital resources. The district is working on fixing the issue and highlighting the need for cybersecurity to protect educational infrastructure from cyber threats. [Read More](https://thenevadaindependent.com/article/clark-county-students-back-to-pen-and-paper-assignments-after-cybersecurity-breach)\n\n\nIronNet, a former unicorn cybersecurity company, has shut down after facing challenges in funding and market competition. The cybersecurity industry is still in the growing phase and needs to adapt to changing threats and economic pressures, emphasizing the need for resilience and innovation. [Read More](https://www.scmagazine.com/news/more-cybersecurity-firm-closures-expected-after-ironnet-shutters)\n\n## Ubiquitous Computing\n\n\nThe Pentagon is implementing a secret-level version of Microsoft 365, called DOD365-Secret, to strengthen national defense capabilities. The new service will provide secure communication and collaboration tools, and is being piloted by the Defense Information Systems Agency (DISA) for the past year. This move highlights the government's commitment to enhancing cybersecurity and data protection, particularly in sensitive military environments. [Read More](https://federalnewsnetwork.com/on-dod/2023/10/secret-level-version-of-microsoft-365-rolls-out-to-top-pentagon-offices-this-month/)\n\n\nForbes predicts ten major cloud computing trends that will shape the industry in 2024. These trends include edge computing, cybersecurity, serverless computing, AI integration, hybrid and multi-cloud adoption, sustainable cloud practices, quantum computing, data analytics, and cloud-native development. It's important for businesses and IT professionals to stay updated with these trends to navigate the evolving cloud landscape. [Read More](https://www.forbes.com/sites/bernardmarr/2023/10/09/the-10-biggest-cloud-computing-trends-in-2024-everyone-must-be-ready-for-now/?sh=2093527266d6)\n\n## Artificial Intelligence\n\n\nQ-Day is on its way. Quantum computers could soon become powerful enough to crack the encryption algorithms that protect our digital information today. This threatens data security, so governments and tech companies are investing in quantum-resistant encryption and technology to safeguard against this.The World Health Organization (WHO) has outlined considerations for regulating artificial intelligence (AI) in the healthcare sector, per their announcement on October 19, 2023. This guidance emphasizes the importance of ethics, transparency, data privacy, and accountability in AI applications for health. It addresses the growing role of AI in healthcare and the need for a framework that ensures its responsible and ethical use. [Read More](https://www.who.int/news/item/19-10-2023-who-outlines-considerations-for-regulation-of-artificial-intelligence-for-health)\n\nThe New York Times reports on China's restrictions on the export of AI chips. These restrictions aim to preserve national security and technological independence, impacting global tech supply chains. As AI plays an increasingly pivotal role in various sectors, this move reflects China's efforts to assert control over advanced technology and raises concerns about global technology trade dynamics. [Read More](https://www.nytimes.com/2023/10/17/business/economy/ai-chips-china-restrictions.html)\n\n\nAccording to The Information online journal, OpenAI has reportedly halted development on a new AI model called \"Arrakis,\" marking a rare setback for the organization. The decision suggests the complexities and challenges of developing advanced AI models and underscores the importance of responsible and ethical AI research and development. [Read More](https://www.theinformation.com/articles/openai-dropped-work-on-new-arrakis-ai-model-in-rare-setback)\n\n## Embracing Digital Transformation Podcast News\n\nIn his Embracing Generative AI series, Darren features an interview with Andy Morris, Intel's Lead AI strategist. The interview dives into the topic of everyday generative AI and explores various tools that can be helpful for people in their daily lives. It's an informative conversation that sheds light on the impact of AI in our daily lives and how we can leverage it for our benefit. [Read More](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW38-en","image":"./briefs/edw-38/en/thumbnail.png","lang":"en","summary":"Embracing Digital news for the week of October 22, 2023 including news in cyber security, Ubiquitous computing, and artificial intelligence. This week’s highlights include going back to pen and paper in school distributes, watching for Q-Day, and AI in healthcare guidelines."},{"id":206,"type":"News Brief","title":"2023-10-28","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n\n## Cybersecurity\n\n\nKaspersky reports on a new malware, dubbed \"Elegant,\" which features advanced espionage capabilities and has similarities to NSA-linked hacking tools. The cybersecurity firm has not attributed the malware to a specific threat actor, but its discovery highlights the need for robust cybersecurity measures in the face of state-sponsored cyberattacks.  [Read More](https://cyberscoop.com/kaspersky-reveals-elegant-malware-resembling-nsa-code/)\n\n\n\nCISA and HHS have launched a cybersecurity toolkit for healthcare organizations. The toolkit aims to enhance the industry's cybersecurity posture by offering valuable resources and guidance to mitigate cyber threats. The goal is to strengthen critical healthcare infrastructure security, given the rising number of cyber risks, and ransomware attacks on hospitals recently. Learn more at the CISA news page. [Read More](https://www.cisa.gov/news-events/news/cisa-hhs-release-collaborative-cybersecurity-healthcare-toolkit)\n\n\nCalifornia has implemented new cybersecurity regulations including risk assessments, incident response plans, data encryption, and multi-factor authentication. It's critical for businesses in California to comply, as non-compliance may result in penalties and legal ramifications. Stay informed by checking for updates on the California Cybersecurity website. [Read More](https://www.armstrongteasdale.com/thought-leadership/californias-newest-cybersecurity-rule-what-you-need-to-know/)\n\n## Edge Computing\n\n\nThe UK Parliament is investigating the cybersecurity of critical infrastructure, including energy, transportation, and healthcare. The inquiry aims to assess the readiness and resilience of these sectors against potential cyber threats. This move comes amid growing concerns over the vulnerability of essential services to cyberattacks. It highlights the importance of protecting vital services from digital threats in an interconnected world. The recent Israel-Hamas conflict highlights the need to safeguard critical infrastructure from cyber threats. [Read More](https://www.bankinfosecurity.com/uk-parliament-probes-critical-infrastructure-cybersecurity-a-23400)\n\n\nTenable and Siemens Energy are partnering to improve industrial cybersecurity. The collaboration will merge Tenable's expertise in cybersecurity with Siemens Energy's domain knowledge to provide advanced security solutions for critical infrastructure. The aim is to bolster protection for essential energy systems against evolving cyber threats. The significance of safeguarding critical infrastructure in an increasingly interconnected world cannot be overstated.  [Read More](https://finance.yahoo.com/news/tenable-siemens-energy-expand-collaboration-130000076.html)\n\n\nSingaporean banks DBS and Citibank suffered a service outage due to a cooling system failure in their shared data center. The incident disrupted banking services and caused inconvenience to customers. Such incidents highlight the significance of robust data center infrastructure for uninterrupted financial services. The banks have since restored their services, emphasizing the critical role of technology in modern banking operations. [Read More](https://www.channelnewsasia.com/singapore/dbs-citibank-outage-data-centre-cooling-system-down-3861076)\n\n## Ubiquitous Computing\n\n\nAmazon has launched an independent cloud service for Europe to meet the increasing data privacy concerns and regulatory requirements. The new cloud service aims to provide enhanced data sovereignty and security to European customers while enabling organizations to leverage cloud computing in a more localized and compliant manner. [Read More](https://www.euronews.com/next/2023/10/25/amazon-rolls-out-new-independent-cloud-for-europe)\n\n\nThe United States and Australia are collaborating on the development of advanced quantum chips. This partnership highlights the increasing importance of quantum computing in national security and technological advancement. As countries strive to maintain their leadership in this critical area of research and development, joint efforts like these are becoming more common. The collaboration demonstrates the deepening cooperation between the two allies in science and technology. [Read More](https://foreignpolicy.com/2023/10/25/quantum-computing-united-states-australia-cooperation-allies-science-technology-chips/)\n\n\nA recent report found that 40% of businesses are losing revenue due to technology downtime and managing cloud services. This highlights the importance of robust IT infrastructure and streamlined cloud management. Addressing downtime and simplifying cloud operations is critical to ensuring uninterrupted business operations and maximizing revenue. Read more in the report at cloud-computing news. [Read More](https://www.cloudcomputing-news.net/news/2023/oct/24/40-of-firms-lose-revenue-from-technology-downtime-and-cloud-complexity/)\n\n## Embracing Digital Transformation Podcast\n\nA new series called \"Embracing Zero Trust\" begins this week. Dr. Anna Scott and Dave Marcus, who are special guests, will be describing the six pillars of Zero Trust and how they can be utilized in existing infrastructure. Meanwhile, Darren's \"Embracing Generative AI\" continues with Sunny Stueve, a Human Factors expert from Leidos. [Read More](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW39-en","image":"./briefs/edw-39/en/thumbnail.png","lang":"en","summary":"In the latest edition of our Embracing Digital This Week, we bring you the latest developments and stories in the world of cybersecurity, edge computing, and ubiquitous computing. In this episode, we cover a new malware called &#34;Elegant,&#34; cybersecurity regulations in California, the UK Parliament&#39;s inquiry into critical infrastructure cybersecurity, and other interesting news from the tech industry."},{"id":207,"type":"News Brief","title":"2023-2-26","tags":null,"body":"\n\n## Artificial Intelligence\n\nOpenAI and Bain & Company formed Global AI Services Alliance to transform business potential. Bain will combine its deep digital implementation capabilities and strategic expertise with OpenAI’s AI tools and platforms, including ChatGPT, to help its clients worldwide identify and implement AI's value to maximizing business potential. The Coca-Cola Company is the first company to engage with the new alliance.\n\nAmazon’s cloud unit has partnered with Hugging Face, the maker of ChatGPT rival. Hugging Face will build the next version of its language model, called BLOOM, on AWS.\n\n[https://www.seattletimes.com/business/amazons-cloud-unit-partners-with-hugging-face-maker-of-chatgpt-rival/](https://www.seattletimes.com/business/amazons-cloud-unit-partners-with-hugging-face-maker-of-chatgpt-rival/)\n\nAI can also be used for actual good in the world too. AI is dreaming up drugs that no one has ever seen. Now we’ve got to see if they work. The first drugs designed with the help of AI are now in clinical trials, the rigorous tests done on human volunteers to see if a treatment is safe—and works—before regulators clear them for widespread use.\n\n[https://www.technologyreview.com/2023/02/15/1067904/ai-automation-drug-development](https://www.technologyreview.com/2023/02/15/1067904/ai-automation-drug-development)\n\n## Intelligent Edge\n\nSecurity vulnerabilities have been found in wireless industrial IoT devices from four vendors, creating a significant attack surface for threat actors looking to exploit operational technology (OT) environments. The flaws offer remote attack entry points, allowing unauthenticated adversaries to gain a foothold and use it as leverage to spread to other hosts, thereby causing serious damage.\n\n[https://thehackernews.com/2023/02/critical-infrastructure-at-risk-from.html](https://thehackernews.com/2023/02/critical-infrastructure-at-risk-from.html)\n\nThe Connectivity Standards Alliance (CSA) has formed a working group dedicated to promoting the adoption of \"Matter\" for healthcare purposes. Matter is an interoperable standard for smart home devices released by the CSA last year. The Health and Wellness Working Group will align major contributors in the healthcare industry around the Matter standard to ensure secure and interoperable health and wellness devices. \n\n[https://www.iottechnews.com/news/2023/feb/10/matter-creator-csa-announces-healthcare-working-group/](https://www.iottechnews.com/news/2023/feb/10/matter-creator-csa-announces-healthcare-working-group/)\n\nRobo-taxis from Amazon’s self-driving vehicle biz Zoox are now on the streets of Foster City, California. Zoox’s vehicles are purpose-built for autonomous driving and do not feature any manual controls. The company’s full-time employees are the first to use the autonomous shuttle service between their campuses. This requires absolute trust in the quality of your team’s work!\n\n[https://www.iottechnews.com/news/2023/feb/14/amazon-robotaxis-hit-the-streets-of-california/](https://www.iottechnews.com/news/2023/feb/14/amazon-robotaxis-hit-the-streets-of-california/)\n\n## Data Management\n\nIn a recent study from Informatica on data sprawl, Informatica found that dispersed data and the need for unified management are widespread pain points for many organizations. Additionally, data quality and governance are top priorities. This is leading to increased investment in data technologies this year. The survey-based research found that 78% of CDOs said they must strategically align with revenue-generating organizations to justify the additional spending to improve data analytics and governance. \n\n[https://accelerationeconomy.com/data/informatica-research-highlights-data-sprawl-why-management-needs-to-be-unified/](https://accelerationeconomy.com/data/informatica-research-highlights-data-sprawl-why-management-needs-to-be-unified/)\n\nAlation Inc., an enterprise data intelligence solutions provider, has released Alation Marketplaces. This new product allows third-party datasets to augment existing data in the Alation Data Catalog, enabling users to explore external third-party data and supplement existing data sets. In addition, the company has expanded its Alation Anywhere to Microsoft Teams and Alation Connected Sheets to Microsoft Excel to allow data users to access contextual information from the catalog directly within their tool of choice.\n\n[https://www.alation.com/press-releases/alation-launches-data-marketplaces/](https://www.alation.com/press-releases/alation-launches-data-marketplaces/)\n\n\n\n","guests":null,"link":"/brief-EDW4-en","image":"./briefs/edw-4/en/thumbnail.png","lang":"en","summary":"Summary"},{"id":208,"type":"News Brief","title":"2023-11-11","tags":["homelawncaretools","ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Cybersecurity\n\n\n\nAustralian ports are grappling with a cyberattack, affecting DP World operations. The attack on the shipping giant has led to disruptions in container movements. While the extent and origin of the breach are under investigation, it raises concerns about the vulnerability of critical infrastructure to cyber threats. Authorities are working to restore normalcy as the incident underscores the growing significance of cybersecurity in safeguarding essential services. [Read More](https://www.bloomberg.com/news/articles/2023-11-11/australian-authorities-meet-as-dp-world-shuts-ports-on-cyber-act)\n<hr>\n<br>\n\n\nICBC, the world's largest bank, has fallen victim to a ransomware cyberattack. The breach has disrupted operations, impacting customer services. While the extent of the attack is being assessed, it underscores the escalating threat of cybercrime to financial institutions. ICBC is working to contain the breach and restore normalcy, emphasizing the need for robust cybersecurity measures in the global banking sector. [Read More](https://www.cnbc.com/2023/11/10/icbc-the-worlds-biggest-bank-hit-by-ransomware-cyberattack.html)\n\n\nCyberattacks are supporting kinetic attacks! Russian hacking group SandWorm is implicated in a cyberattack that disrupted power supply in Ukraine. The sophisticated attack targeted energy infrastructure, causing widespread outages. Ukrainian authorities are investigating the incident, highlighting the ongoing threat of state-sponsored cyberattacks to critical infrastructure. The situation underscores the need for enhanced cybersecurity measures to safeguard essential services. [Read More](https://thehackernews.com/2023/11/russian-hackers-sandworm-cause-power.html)\n\n## Artificial Intelligence\n\n\nAI experts are warning of a potential data shortage, emphasizing the world's diminishing data resources to fuel artificial intelligence. The exponential growth of AI applications and data-hungry models is outpacing data generation. This scarcity could hinder further AI advancements, prompting the need for innovative approaches to data collection and synthesis. The challenge underscores the importance of responsible and sustainable AI development. [Read More](https://www.sciencealert.com/the-world-is-running-out-of-data-to-feed-ai-experts-warn)\n\n\nAI proliferation threatens democracy! caution experts at Reuters Next conference. The rapid advancement of artificial intelligence raises concerns about its potential misuse for surveillance, manipulation, and erosion of democratic values. Experts call for ethical guidelines, regulatory frameworks, and public discourse to mitigate risks associated with AI's growing influence on politics and society. [Read More](https://www.reuters.com/technology/reuters-next-rapid-ai-proliferation-is-threat-democracy-experts-say-2023-11-08/)\n\n\nIn an interview, Barack Obama expressed concern over the impact of artificial intelligence (AI) on jobs. He acknowledges the potential for job displacement due to automation and emphasizes the need for policies to address these challenges. Obama suggests investing in education and retraining programs to prepare the workforce for evolving job markets shaped by AI and automation. [Read More](https://www.independent.co.uk/tv/news/barack-obama-biden-ai-jobs-b2445354.html)\n\n## Ubiquitous Computing\n\n\nA breakthrough in quantum computing integrates AI and machine learning for robust qubit error correction. Scientists have developed an AI-enhanced quantum computing approach that improves error correction for qubits, a crucial challenge in quantum computing. This innovative combination of AI and quantum technology holds promise for advancing the reliability and performance of quantum computers. [Read More](https://scitechdaily.com/ai-enhanced-quantum-computing-machine-learning-powers-robust-qubit-error-correction/)\n\n\nCloud Software Group, the parent company of Citrix, is discontinuing new commercial transactions in China, including Hong Kong and Macau, effective December 3. Citing the \"increasing cost\" of operating in the region, the decision follows similar moves by other American tech firms amid a weaker economic outlook and stricter data-security regulations in China. As more US firms exit China, Beijing has proposed relaxing cross-border data-security controls to attract foreign investors. [Read More](https://finance.yahoo.com/news/citrix-owner-cloud-software-becomes-093000327.html)\n\n\nA recent article by Analytics Insights forecasts cloud computing trends for 2024. These include the importance of edge computing for faster data processing, the rise of multi-cloud strategies for flexibility, and the integration of AI/ML for optimized data analysis. Security measures such as zero-trust architecture will be a priority in addressing the evolving threat landscape. [Read More](https://www.analyticsinsight.net/cloud-trends-for-2024-whats-on-for-cloud-computing/)\n\n## Embracing Digital Transformation Podcast\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW41-en","image":"./briefs/edw-41/en/thumbnail.png","lang":"en","summary":"Embracing Digital This week of November 12, 2023, including news in cybersecurity. Artificial intelligence and ubiquitous computing. Find out about several major cyber-attacks on critical infrastructure, AI running out of data, and breakthroughs in quantum computing."},{"id":209,"type":"News Brief","title":"2023-11-18","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Cybersecurity\n\n\nThe Cybersecurity and Infrastructure Security Agency (CISA) plans to enforce executive accountability in software security for government procurement. The initiative aims to make executives sign off on the security of software sold to the government. This move is part of CISA's efforts to enhance cybersecurity measures and ensure the integrity of software used in critical government systems. [Read More](https://federalnewsnetwork.com/cybersecurity/2023/11/cisa-aims-to-make-executives-sign-off-on-security-of-software-sold-to-government/)\n\n\nU.S. cybersecurity agencies issue a warning about a new ransomware threat targeting critical infrastructure. The alert highlights the potential risks and urges organizations to enhance their cybersecurity protocols. The agencies emphasize the importance of proactive measures, including regularly updating software and employing robust security practices to mitigate the risk of ransomware attacks. [Read More](https://thehackernews.com/2023/11/us-cybersecurity-agencies-warn-of.html)\n\n\n\nSolarWinds faces a lawsuit from the Securities and Exchange Commission (SEC) following the 2020 cyberattack. The SEC alleges the company failed to disclose vulnerabilities, misleading investors. SolarWinds, known for its IT management software, suffered a high-profile supply chain attack. The lawsuit underscores the increasing focus on cybersecurity disclosure obligations and the potential legal consequences for companies that inadequately address such incidents. [Read More](https://www.nytimes.com/2023/11/18/business/dealbook/solarwinds-sec-lawsuit.html)\n\n## Artificial Intelligence \n\n\nOpenAI CEO Sam Altman resigns amid increasing concerns over the company’s direction. Altman's departure follows internal tensions and resignations over the company's focus on commercial applications rather than its initial commitment to broad AI benefits. The move raises questions about the balance between corporate interests and ethical considerations in AI development. [Read More](https://www.washingtonpost.com/technology/2023/11/17/openai-ceo-resigns/)\n\n\nMicrosoft's recent AI event highlights three key takeaways. First, the company emphasizes its commitment to responsible AI development, focusing on fairness, transparency, and accountability. Second, Microsoft aims to empower developers with tools like OpenAI's GPT, which offers advanced natural language capabilities. Lastly, the event underscores Microsoft's dedication to AI-driven innovation across industries, showcasing healthcare, finance, and climate science applications.  [Read More](https://www.cnbc.com/2023/11/17/here-are-three-key-takeaways-from-microsofts-bullish-ai-event-.html)\n\n\nYouTube confronts the challenge of deepfake content as AI creators produce increasingly convincing and misleading videos. The platform struggles to distinguish between harmless and malicious uses of AI-generated content. YouTube acknowledges the need for vigilance and a comprehensive approach to addressing deepfakes to maintain the integrity of its content. Content creators are now asked to verify the if the content is generated by an AI or not. [Read More](https://apnews.com/article/youtube-artitifical-intelligence-deep-fake-ai-creaters-0513fd9fddbd93af327f0411dd29ff3d)\n\n##  Ubiquitous Computing\n\n\n\nAmazon and Microsoft engage in fierce competition for lucrative government cloud computing contracts as federal agencies increasingly rely on cloud services. The companies are vying for the Department of Defense's Joint Warfighter Cloud Capability (JWCC) contract, a deal worth potentially $10 billion. This battle reflects the broader trend of tech giants competing for government cloud contracts and underscores the strategic importance of cloud services in modernizing government infrastructure. [Read More](https://www.bloomberg.com/news/articles/2023-11-16/amazon-amzn-microsoft-msft-fight-for-cloud-computing-government-contracts)\n\n\nThe U.S. Navy is enhancing its torpedo systems by leveraging cloud computing for submarines. This upgrade aims to improve the torpedoes' performance and capabilities. The Navy seeks to enhance data processing and analysis by utilizing cloud computing, allowing for more sophisticated and efficient torpedo operations. The move reflects the military's ongoing efforts to integrate advanced technologies, such as cloud computing, to bolster the capabilities of its naval systems. [Read More](https://www.defensenews.com/naval/2023/11/16/us-navy-upgrading-torpedoes-leveraging-cloud-computing-for-submarines/)\n\n\nThe U.S. government flags Alibaba over concerns related to the potential misuse of American-made chips in its surveillance technology. The move is part of the broader effort to restrict exports of sensitive technologies amid national security concerns. This development underscores the scrutiny of Chinese tech companies. It highlights the U.S. government's focus on preventing the unintended use of American technology in ways that could compromise security or human rights. [Read More](https://finance.yahoo.com/news/alibaba-flagged-us-chip-curbs-192349433.html)\n\n## Embracing Digital Transformation Podcast News\n\n\n\nCheck out the three series titled \"Embracing Generative AI,\" \"Embracing Zero Trust,\" and \"Embracing Multi-Hybrid Cloud.\" In these series, Darren explores the world of AI with guests from industry, education, and everyday people, brings on cybersecurity experts to talk about protecting the digital world, and searches the sky for the latest cloud strategies and technologies. [Read More](https://www.embracingdigital.org/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW42-en","image":"./briefs/edw-42/en/thumbnail.png","lang":"en","summary":"Digital Transformation for Cybersecurity, AI, and Computing on the week of November 19, 2023. Topics include legal implications of cybersecurity, Sam Altman&#39;s departure from OpenAI, and Cloud providers competing for government contracts."},{"id":210,"type":"News Brief","title":"2023-11-25","tags":["uniquehandmadewoodfurniture.","ai","compute","cybersecurity","samaltman","openai","aibreakthrough","neuralchat7b","intel","responsibleai","meta","hacking","insiderthreats","securitymeasures","eucybersecurityregulations","clearfake","malware","ubiquitouscomputing","macstadium","m2prochip","alibabacloud","cloudcomputing"],"body":"\n\n## Artificial Intelligence\n\n\nSam Altman left OpenAI after a letter expressing concerns about an undisclosed AI breakthrough. The letter, sent by an anonymous employee, led to Altman's removal as CEO. The breakthrough details remain confidential, raising questions about OpenAI's internal dynamics. Altman was reinstated when 700 of the 770 employees followed him to Microsoft. [Read More](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/)\n\n\nIntel has developed the NeuralChat 7B, a chatbot model that prioritizes user data protection. The model is trained using Differential Privacy Optimization (DPO) to enhance privacy safeguards. By implementing DPO, Intel aims to ensure secure interactions within the chatbot and mitigate privacy risks. These efforts align with evolving standards for responsible AI development. [Read More](https://medium.com/@bnjmn_marie/neuralchat-7b-intels-chat-model-trained-with-dpo-e691dfd52591)\n\n\nMeta, the parent company of Facebook, reportedly disbanded its Responsible AI team, raising concerns about its commitment to ethical AI practices. Critics argue that dismantling the team may undermine efforts to address ethical concerns and potential biases in Meta's AI technologies, emphasizing the ongoing debate around responsible AI development. [Read More](https://www.theverge.com/2023/11/18/23966980/meta-disbanded-responsible-ai-team-artificial-intelligence)\n\n## Cybersecurity \n\n\nA cybersecurity firm executive has pleaded guilty to hacking hospitals in a shocking turn of events. The executive, previously entrusted with safeguarding systems, admitted to exploiting vulnerabilities for personal gain. The breach raises serious questions about insider threats and the need for robust security measures within the cybersecurity sector itself. The guilty plea underscores the imperative for stringent cybersecurity protocols and ethical conduct, especially when entrusted with protecting critical infrastructure like healthcare institutions. [Read More](https://www.bleepingcomputer.com/news/security/cybersecurity-firm-executive-pleads-guilty-to-hacking-hospitals/)\n\n\nThe EU plans to expand cybersecurity regulations to cover critical sectors such as energy, transport, and digital service providers like Amazon, Google, and Microsoft. This is to strengthen cybersecurity across diverse industries due to increasing cyber threats. However, some see the new regulations as a way to capture revenue from Big Tech. [Read More](https://www.finextra.com/newsarticle/43338/eu-considers-widening-scope-of-cybersecurity-regulation)\n\n\nA new malware called ClearFake has been discovered targeting Mac users. It disguises itself as a legitimate antivirus app, deceiving users into downloading malicious software. Experts warn of the potential for data theft and system compromise. It is essential to be cautious and use reputable security software to counter evolving threats targeting macOS platforms. [Read More](https://cybersecuritynews.com/clearfake-new-malware-mac/)\n\n## Ubiquitous Computing\n\n\nMacStadium's new Mac Cloud, powered by the M2 Pro chip, offers advanced performance and capabilities for macOS developers and businesses. It caters to the growing demand for Kubernetes-based Mac development in cloud environments, meeting the unique needs of the Apple development community. The M2 Pro chip significantly advances Mac hosting capabilities, providing improved speed and efficiency. [Read More](https://finance.yahoo.com/news/macstadium-unveils-powerful-next-generation-120000257.html)\n\n\n\nAlibaba has added three new executives to its cloud computing unit, indicating its commitment to boosting its cloud business. The executives bring diverse expertise to the competitive cloud computing market and will drive innovation and growth. Alibaba's focus on cloud services aligns with the digital landscape's evolving demands, reinforcing its resolve to maintain a leading position in the global cloud computing sector. [Read More](https://www.proactiveinvestors.com.au/companies/news/1034236/alibaba-taps-three-new-cloud-computing-executives-after-nixing-spinoff-ipo-plans-1034236.html)\n\n\nAccording to reports, Europe is lagging in the global AI race and must shift focus to quantum computing to stay competitive. Investing in transformative quantum technology is imperative for Europe to regain its technological leadership and address the evolving global innovation landscape. [Read More](https://www.euronews.com/next/2023/11/23/europe-has-lost-the-ai-race-it-cant-ignore-the-quantum-computing-one)\n\n## Embracing Digital Transformation Podcast\n\n\n\nThis week, Darren returned after a week off for Thanksgiving. He interviewed Louis Parks, the CEO of Veridify, where he revealed the vulnerabilities of Operational Technology's Critical Infrastructure. Additionally, check out the new branding and logos of Embracing Digital Transformation. [Read More](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW43-en","image":"./briefs/edw-43/en/thumbnail.png","lang":"en","summary":"Digital Transformation news for the week of November 26, 2023. From the transformational shifts in artificial intelligence leadership to the imperative focus on cybersecurity and the unveiling of advanced computing solutions, this compilation encapsulates pivotal moments in the ever-evolving technology landscape. Join us as we delve into the intricacies of recent developments, illuminating the profound impacts they carry across sectors, and guiding the trajectory of the digital future."},{"id":211,"type":"News Brief","title":"2023-12-2","tags":["ai","compute","datamanagement","aws","siemens","industrialdge","cloudconfiguration","multiaccessedgecomputing","mec","networkarchitecture","iot","realtimeapplications","edgecomputinginhealthcare","healthcaremarket","polarismarketresearch","usda","datastrategy","digitalgovernance","vastdata","aiworkflows"],"body":"\n\n## Edge computing\n\n\nAmazon Web Services (AWS) and Siemens have joined forces to streamline industrial edge-to-cloud configuration. The collaboration aims to simplify the integration of edge devices with cloud services, enhancing efficiency in industrial settings. The partnership combines AWS IoT Greengrass and Siemens' Industrial Edge platform, providing a seamless solution for configuring, managing, and optimizing edge devices in the cloud. This collaboration is set to facilitate smoother and more accessible industrial edge-to-cloud connectivity. [Read More](https://www.edgecomputing-news.com/2023/12/01/aws-and-siemens-team-up-for-easier-industrial-edge-to-cloud-configuration/)\n\n\n\nMulti-Access Edge Computing (MEC) is a network architecture that brings computing capabilities closer to the edge of the network, reducing latency and improving efficiency. By deploying computing resources at the edge, MEC enables faster processing of data generated by devices like IoT sensors and mobile devices. This decentralized approach enhances real-time applications and services like augmented reality and autonomous vehicles. MEC leverages edge servers in close proximity to end-users, optimizing data processing for low-latency, high-performance computing. [Read More](https://www.techopedia.com/what-is-multi-access-edge-computing)\n\n\nAccording to Polaris Market Research, the Edge Computing in Healthcare market is projected to surpass USD 43.29 billion by 2032, exhibiting a robust CAGR growth of 26.3%. The increasing adoption of edge computing solutions in healthcare, driven by advancements in IoT and AI technologies, is expected to enhance data processing efficiency, enable real-time applications, and improve patient care. The report suggests a significant market expansion as healthcare embraces edge computing for enhanced performance and decentralized data processing. [Read More](https://www.prnewswire.co.uk/news-releases/edge-computing-in-healthcare-market-expected-to-cross-usd-43-29-billion-by-2032--driving-26-3-cagr-growth-polaris-market-research-301998143.html)\n\n## Data Management\n\n\n\nThe USDA has revealed a new data strategy to enhance digital governance. The initiative leverages data to improve services, decision-making, and accountability. The plan emphasizes data-driven innovation, collaboration, and modernization of data infrastructure. Key components include prioritizing privacy and security, promoting data accessibility, and fostering a culture of responsible data use. The USDA's comprehensive approach aims to maximize the value of data in supporting the agency's mission and serving the public more effectively. [Read More](https://www.nextgov.com/digital-government/2023/11/usda-unveils-new-data-strategy/392382/)\n\n\nVAST Data has released a platform update designed to simplify AI workflows and hybrid cloud operations on AWS. The update focuses on enhancing the efficiency of data-intensive processes, offering seamless integration with AWS services. VAST Data's platform aims to streamline data management and accelerate AI workloads, providing a more user-friendly experience for organizations leveraging AWS for their hybrid cloud operations. The update underscores VAST Data's commitment to optimizing performance and simplifying complex AI and hybrid cloud workflows. [Read More](https://www.datanami.com/this-just-in/vast-datas-new-platform-update-aims-to-simplify-ai-workflows-and-hybrid-cloud-operations-on-aws/)\n\n\nIntel has partnered with Granulate to optimize data management operations on Databricks, a platform used for big data analytics. Granulate's real-time continuous optimization technology, powered by Intel hardware, is designed to improve performance and resource utilization for Databricks users. This collaboration reflects the industry's efforts to enhance data analytics capabilities, enabling organizations to take full advantage of the power of big data for increased efficiency and reduced costs. [Read More](https://www.valdostadailytimes.com/ap/business/intel-granulate-optimizes-databricks-data-management-operations/article_ef0e550d-dbd3-5e5d-9a1d-75ffc7d0be4d.html)\n\n## Artificial Intelligence\n\n\nCalifornians are urging their lawmakers to take measures to safeguard elections and ensure electoral integrity. The growing concerns about election security have prompted citizens to call for legislative action to protect the democratic process. This demand reflects a rising awareness of securing elections from potential threats. It reinforces the call for lawmakers to enact policies that enhance transparency, cybersecurity, and overall confidence in the electoral system. [Read More](https://news.yahoo.com/californians-want-lawmakers-safeguard-elections-150026166.html)\n\n\nNATO has announced a comprehensive strategy for artificial intelligence (AI) that recognizes its crucial role in modern warfare and security. The strategy uses AI to enhance decision-making, cybersecurity, and operational efficiency. NATO's decision reflects the increasing significance of AI in military domains and the need to remain at the forefront of technological advancements. The strategy highlights ethical guidelines and emphasizes the importance of collaboration with allies and industry partners. [Read More](https://news.yahoo.com/nato-artificial-intelligence-strategy-amid-143228193.html)\n\n\nIntel recently announced \"Vision AI Everywhere,\" a new technology integrating AI into data centers and devices. This integration aims to enhance performance and efficiency, optimize workloads, boost data center capabilities, and enable real-time decision-making. During the HPE Discover event, Intel showcased various applications such as energy-efficient data centers and advanced medical imaging. \"Vision AI Everywhere\" is aligned with Intel's commitment to promoting AI adoption in different industries, ushering in a new era of intelligent computing. [Read More](https://siliconangle.com/2023/11/29/data-centers-devices-intels-vision-ai-everywhere-hpediscover-hpediscover/)\n\n## Embracing Digital Transformation \n\n\n\nEmbracingdigital.org has recently undergone a makeover, sporting a new look and logo. Check out the updated branding of the show, along with a new shop that offers the latest gifts for digital transformation enthusiasts. Also, on this week's show, there is a two-part interview with Shamim Naqvi, CEO of SafeLiShare, where they delve into Zero Trust and data sharing. [Read More](http://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW44-en","image":"./briefs/edw-44/en/thumbnail.png","lang":"en","summary":"Please check out the latest news in the world of Digital Transformation for the week of December 3, 2023. You&#39;ll find a variety of interesting stories related to edge computing, data management, and artificial intelligence. This week, AWS and Siemens have collaborated to simplify edge computing, while Intel is helping to improve cloud-based data management. Additionally, several governments are developing new strategies for AI."},{"id":212,"type":"News Brief","title":"2023-12-9","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","usarmy","uno","cybersec","iot","aijobs","gemini","nano","pro","ultra","meta","ibm","openai","altman","aws","google","microsoft","mcdonalds","vergeio","nutanix","embracingdt"],"body":"\n\n## Cybersecurity\n\n\nThe US Army is planning to improve its connectivity and cybersecurity capabilities by introducing a new unified network called UNO. This initiative aims to provide the military with fast and secure communication, which can help overcome challenges in the digital battlefield. The cooperative network is expected to simplify operations and enhance resilience against cyber threats, thus strengthening the Army's technological infrastructure. [Read More](https://www.c4isrnet.com/battlefield-tech/it-networks/2023/12/04/unified-network-promises-us-army-rapid-connectivity-cybersecurity/)\n\n\nAs the Internet of Things (IoT) and artificial intelligence (AI) continue to advance, the risk to critical infrastructure from cyber threats has increased significantly. Cybersecurity experts are emphasizing the need for better security measures to protect against these threats. They are relying on the integration of AI and machine learning to improve the detection and prevention of new cyber attacks. This constantly evolving landscape of cybersecurity in the IoT and cloud computing age underscores the ongoing need to adapt and secure digital ecosystems. [Read More](https://readwrite.com/the-evolution-of-cybersecurity-in-the-age-of-iot-and-cloud-computing/)\n\n\nAround 50% of surveyed organizations have plans to trim down their cybersecurity staff, which raises concerns in light of the ever-evolving cyber threats. This trend underscores the significance of strategic workforce management for keeping digital solid defenses. Meanwhile, some other companies need help finding suitable candidates to fill their vacant cybersecurity positions. [Read More](https://www.csoonline.com/article/1251369/almost-50-organizations-plan-to-reduce-cybersecurity-headcounts-survey.html)\n\n## Artificial Intelligence\n\n\nGoogle has launched a new AI platform called Gemini, which aims to simplify the training and deployment of machine learning models. This will enable developers to create more scalable and efficient AI applications. Gemini offers user-friendly tools and resources to promote innovation and advancements in AI technology. It currently targets three offerings: nano for phones, pro for on-prem data centers, and ultra for a cloud-based public cloud fronted by Bard. [Read More](https://apnews.com/article/google-gemini-artificial-intelligence-launch-95d05d02051e75e20b574614ae720b8b)\n\n\n\nMeta and IBM are working together to develop an open standard for artificial intelligence, aiming to promote interoperability and counter the influence of big tech players. By fostering a more collaborative approach, they hope to create a competitive landscape encouraging innovation and diversity in AI tech. [Read More](https://www.thestreet.com/technology/meta-and-ibm-team-up-against-dominant-big-tech-players)\n\n\nOpenAI's CEO, Sam Altman, emphasized the importance of responsible AI development after an employee was fired due to concerns about the misuse of AI technology. The incident highlights the ongoing challenges in the tech industry regarding ethics and governance in artificial intelligence. [Read More](https://www.foxbusiness.com/technology/openais-sam-altman-opens-up-shock-firing)\n\n## Ubiquitous computing\n\n\nAWS and Google criticize Microsoft's restrictive cloud practices, accusing them of limiting customer choice and interoperability, which stifles competition in the market. This highlights ongoing tensions among cloud providers and concerns about fair business practices and market dynamics. [Read More](https://www.itpro.com/cloud/cloud-computing/aws-joins-google-in-calling-out-restrictive-microsoft-cloud-practices)\n\n\nMcDonald's and Google Cloud partner to use cloud tech and generative AI solutions in McDonald's restaurants globally. The objective is to enhance customer experience, optimize operations, and drive innovation in the fast-food industry. By integrating Google Cloud's advanced technologies, McDonald's aims to stay at the forefront of digital transformation, delivering improved services and operational efficiencies across its worldwide network of restaurants. [Read More](https://www.prnewswire.com/news-releases/mcdonalds-and-google-cloud-announce-strategic-partnership-to-connect-latest-cloud-technology-and-apply-generative-ai-solutions-across-its-restaurants-worldwide-302006915.html)\n\n\n\nIt has been reported that the acquisition of VMware by Broadcom has caused changes that led to 20% of its users looking for other virtualization solutions. This opportunity trend has created a for other players in the market to establish themselves as viable alternatives. Despite VMware's dominance in the virtualization market, competition from companies like Nunatix and VergeIO is causing customers to shift away from the established platform. [Read More](https://www.channelfutures.com/mergers-acquisitions/20-of-users-looking-to-escape-to-vmware-alternatives)\n\n## Embracing Digital Transformation Podcast\n\n\n\nIn this episode of his series on Embracing Zero Trust, Darren interviews the CEO of SafeLiShare, Shamim Naqvi, to discuss Zero Trust security and data collaboration using Confidential computing. Additionally, Darren would like to thank his listeners for their support.  Thanks to you, the podcast has over 3000 subscribers and 4000 weekly listeners. [Read More](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW45-en","image":"./briefs/edw-45/en/thumbnail.png","lang":"en","summary":"Digital Transformation news for the week of Dec 10, 2023, contains stories about cybersecurity, artificial intelligence, and ubiquitous computing. This week listen to the US Army&#39;s plan to improve cybersecurity through unification, the cloud wars going to court in the UK, and the drama of openAI and Sam Altman."},{"id":213,"type":"News Brief","title":"2023-12-16","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","innovation","intel","pc","aiworkforce","snowflake","aws","solix","aiintegration","britain","nationalgrid","chineserisks","hackers","infrastructure","vulnerabilities","nsa","russianthreats","digitaltransformation","openziti","zerotrust"],"body":"\n\n## Artificial Intelligence \n\n\nIntel's latest announcement marks a pivotal moment in the birth of AI-powered PCs. The Core Ultra H and U series processors launch introduces the revolutionary Meteor Lake architecture, embedding advanced AI capabilities into ultra-thin notebooks. This release enhances performance and raises the Arc graphics platform, elevating the user experience to new heights. Intel's commitment to innovation signals a transformative era in the laptop market, where AI becomes integral to everyday computing.  [Read More](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\nNew York City Councilwoman-elect Susan Zhuang has admitted to quietly employing AI to communicate with the public and respond to media inquiries. Finally a politician speaking the truth! Zhuang's use of AI marks a unique approach to engaging with constituents, and the implications of this tech-driven strategy may reshape the landscape of political communication in the digital age.  [Read More](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\nIntel's creation of the personalized AI PC has brought a potential massive labor disruption to businesses as they continue integrating artificial intelligence (AI) into their operations. As AI technologies are adopted, job functions and workdays are expected to change, raising concerns about potential workforce challenges. Companies are preparing for this transformative shift and recognizing the need for proactive strategies to navigate potential disruptions and ensure a smooth transition to an AI-driven future.  [Read More](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Data Management\n\n\nSnowflake, a leading cloud data platform, has achieved FedRAMP High Authorization on AWS GovCloud US West and US East. This significant milestone highlights Snowflake's commitment to meeting stringent government security standards, enabling federal agencies to leverage its advanced data management capabilities in a secure cloud environment. The FedRAMP High Authorization underscores Snowflake's position as a trusted partner in providing fast and scalable solutions for government entities, fostering innovation and efficiency in data management. [Read More](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\nSolix has introduced the Common Data Platform (CDP) 3.0, which includes advanced multi-cloud data management and enterprise AI capabilities. This release reflects Solix's commitment to providing state-of-the-art solutions for managing and deriving insights from large datasets. The CDP 3.0's improved machine learning and AI features enable organizations to make more informed data-driven decisions. The trend of multi-cloud data management is driving the development of many new commercial offerings, and Solix's latest release is poised to be a significant contributor in this space.  [Read More](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\nIn a recent report, Datanami explores crucial considerations for ensuring your data management strategy is ready for AI with five key indicators. Prioritize data quality, scalability, integration capabilities, robust security, and flexibility. These elements are crucial for optimizing systems as artificial intelligence continues to shape data processes. Stay ahead in the AI era by evaluating and enhancing these aspects to meet the demands of evolving technologies. Read more at datanami.com for comprehensive insights. [Read More](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Cybersecurity\n\n\n\nBritain's National Grid has reportedly severed ties with a China-based supplier due to cybersecurity concerns, as Reuters reported on December 17, 2023. The decision reflects heightened global scrutiny over potential risks associated with foreign technology partnerships, particularly in critical infrastructure sectors. This move by the National Grid underscores the increasing emphasis on bolstering cybersecurity measures to safeguard essential services against potential threats.  [Read More](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\nHackers have targeted the U.S. water supply, raising alarms about the vulnerability of critical infrastructure. Axios reported on December 16, 2023, that this breach poses significant national security and public safety concerns. The incident underscores the urgent need for robust cybersecurity measures to safeguard essential services. Authorities are investigating the breach, highlighting the ongoing challenges in defending critical infrastructure from cyber threats.  [Read More](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\nThe NSA has warned about Russian cyber actors exploiting a known vulnerability with global implications. The identified vulnerability has a significant impact worldwide, emphasizing the importance of mitigating cybersecurity risks. The NSA advises organizations to promptly address and patch this vulnerability to enhance their resilience against potential cyber threats.  [Read More](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Embracing Digital Transformation\n\n\n\nDarren looks to open source for zero trust network architecture with openZiti with one of the open-source community organizers Philip Griffiths.  Also, a special thanks to the Embracing Digital Transformation listeners for spreading the news, we have added over 4000 subscribers to our YouTube channel and had over 14,000 downloads or views of our podcast last week. Thanks again for your support. [Read More](http://www.embracingdigital.org)\n\n## Artificial Intelligence \n\n## Artificial Intelligence \n\n## Artificial Intelligence \n\n\n\nIntel's latest announcement marks a pivotal moment in the birth of AI-powered PCs. The Core Ultra H and U series processors launch introduces the revolutionary Meteor Lake architecture, embedding advanced AI capabilities into ultra-thin notebooks. This release enhances performance and raises the Arc graphics platform, elevating the user experience to new heights. Intel's commitment to innovation signals a transformative era in the laptop market, where AI becomes integral to everyday computing.  [Read More](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nNew York City Councilwoman-elect Susan Zhuang has admitted to quietly employing AI to communicate with the public and respond to media inquiries. Finally a politician speaking the truth! Zhuang's use of AI marks a unique approach to engaging with constituents, and the implications of this tech-driven strategy may reshape the landscape of political communication in the digital age.  [Read More](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nIntel's creation of the personalized AI PC has brought a potential massive labor disruption to businesses as they continue integrating artificial intelligence (AI) into their operations. As AI technologies are adopted, job functions and workdays are expected to change, raising concerns about potential workforce challenges. Companies are preparing for this transformative shift and recognizing the need for proactive strategies to navigate potential disruptions and ensure a smooth transition to an AI-driven future.  [Read More](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Data Management\n\n\n\nSnowflake, a leading cloud data platform, has achieved FedRAMP High Authorization on AWS GovCloud US West and US East. This significant milestone highlights Snowflake's commitment to meeting stringent government security standards, enabling federal agencies to leverage its advanced data management capabilities in a secure cloud environment. The FedRAMP High Authorization underscores Snowflake's position as a trusted partner in providing fast and scalable solutions for government entities, fostering innovation and efficiency in data management. [Read More](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix has introduced the Common Data Platform (CDP) 3.0, which includes advanced multi-cloud data management and enterprise AI capabilities. This release reflects Solix's commitment to providing state-of-the-art solutions for managing and deriving insights from large datasets. The CDP 3.0's improved machine learning and AI features enable organizations to make more informed data-driven decisions. The trend of multi-cloud data management is driving the development of many new commercial offerings, and Solix's latest release is poised to be a significant contributor in this space.  [Read More](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\n\nIn a recent report, Datanami explores crucial considerations for ensuring your data management strategy is ready for AI with five key indicators. Prioritize data quality, scalability, integration capabilities, robust security, and flexibility. These elements are crucial for optimizing systems as artificial intelligence continues to shape data processes. Stay ahead in the AI era by evaluating and enhancing these aspects to meet the demands of evolving technologies. Read more at datanami.com for comprehensive insights. [Read More](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Cybersecurity\n\n\n\nBritain's National Grid has reportedly severed ties with a China-based supplier due to cybersecurity concerns, as Reuters reported on December 17, 2023. The decision reflects heightened global scrutiny over potential risks associated with foreign technology partnerships, particularly in critical infrastructure sectors. This move by the National Grid underscores the increasing emphasis on bolstering cybersecurity measures to safeguard essential services against potential threats.  [Read More](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\n\nHackers have targeted the U.S. water supply, raising alarms about the vulnerability of critical infrastructure. Axios reported on December 16, 2023, that this breach poses significant national security and public safety concerns. The incident underscores the urgent need for robust cybersecurity measures to safeguard essential services. Authorities are investigating the breach, highlighting the ongoing challenges in defending critical infrastructure from cyber threats.  [Read More](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\n\nThe NSA has warned about Russian cyber actors exploiting a known vulnerability with global implications. The identified vulnerability has a significant impact worldwide, emphasizing the importance of mitigating cybersecurity risks. The NSA advises organizations to promptly address and patch this vulnerability to enhance their resilience against potential cyber threats.  [Read More](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Embracing Digital Transformation\n\n\n\nDarren looks to open source for zero trust network architecture with openZiti with one of the open-source community organizers Philip Griffiths.  Also, a special thanks to the Embracing Digital Transformation listeners for spreading the news, we have added over 4000 subscribers to our YouTube channel and had over 14,000 downloads or views of our podcast last week. Thanks again for your support. [Read More](http://www.embracingdigital.org)\n\n## Artificial Intelligence \n\n## Artificial Intelligence \n\n\n\nIntel's latest announcement marks a pivotal moment in the birth of AI-powered PCs. The Core Ultra H and U series processors launch introduces the revolutionary Meteor Lake architecture, embedding advanced AI capabilities into ultra-thin notebooks. This release enhances performance and raises the Arc graphics platform, elevating the user experience to new heights. Intel's commitment to innovation signals a transformative era in the laptop market, where AI becomes integral to everyday computing.  [Read More](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nNew York City Councilwoman-elect Susan Zhuang has admitted to quietly employing AI to communicate with the public and respond to media inquiries. Finally a politician speaking the truth! Zhuang's use of AI marks a unique approach to engaging with constituents, and the implications of this tech-driven strategy may reshape the landscape of political communication in the digital age.  [Read More](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nIntel's creation of the personalized AI PC has brought a potential massive labor disruption to businesses as they continue integrating artificial intelligence (AI) into their operations. As AI technologies are adopted, job functions and workdays are expected to change, raising concerns about potential workforce challenges. Companies are preparing for this transformative shift and recognizing the need for proactive strategies to navigate potential disruptions and ensure a smooth transition to an AI-driven future.  [Read More](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Data Management\n\n\n\nSnowflake, a leading cloud data platform, has achieved FedRAMP High Authorization on AWS GovCloud US West and US East. This significant milestone highlights Snowflake's commitment to meeting stringent government security standards, enabling federal agencies to leverage its advanced data management capabilities in a secure cloud environment. The FedRAMP High Authorization underscores Snowflake's position as a trusted partner in providing fast and scalable solutions for government entities, fostering innovation and efficiency in data management. [Read More](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix has introduced the Common Data Platform (CDP) 3.0, which includes advanced multi-cloud data management and enterprise AI capabilities. This release reflects Solix's commitment to providing state-of-the-art solutions for managing and deriving insights from large datasets. The CDP 3.0's improved machine learning and AI features enable organizations to make more informed data-driven decisions. The trend of multi-cloud data management is driving the development of many new commercial offerings, and Solix's latest release is poised to be a significant contributor in this space.  [Read More](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\n\nIn a recent report, Datanami explores crucial considerations for ensuring your data management strategy is ready for AI with five key indicators. Prioritize data quality, scalability, integration capabilities, robust security, and flexibility. These elements are crucial for optimizing systems as artificial intelligence continues to shape data processes. Stay ahead in the AI era by evaluating and enhancing these aspects to meet the demands of evolving technologies. Read more at datanami.com for comprehensive insights. [Read More](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Cybersecurity\n\n\n\nBritain's National Grid has reportedly severed ties with a China-based supplier due to cybersecurity concerns, as Reuters reported on December 17, 2023. The decision reflects heightened global scrutiny over potential risks associated with foreign technology partnerships, particularly in critical infrastructure sectors. This move by the National Grid underscores the increasing emphasis on bolstering cybersecurity measures to safeguard essential services against potential threats.  [Read More](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\n\nHackers have targeted the U.S. water supply, raising alarms about the vulnerability of critical infrastructure. Axios reported on December 16, 2023, that this breach poses significant national security and public safety concerns. The incident underscores the urgent need for robust cybersecurity measures to safeguard essential services. Authorities are investigating the breach, highlighting the ongoing challenges in defending critical infrastructure from cyber threats.  [Read More](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\n\nThe NSA has warned about Russian cyber actors exploiting a known vulnerability with global implications. The identified vulnerability has a significant impact worldwide, emphasizing the importance of mitigating cybersecurity risks. The NSA advises organizations to promptly address and patch this vulnerability to enhance their resilience against potential cyber threats.  [Read More](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Embracing Digital Transformation\n\n\n\nDarren looks to open source for zero trust network architecture with openZiti with one of the open-source community organizers Philip Griffiths.  Also, a special thanks to the Embracing Digital Transformation listeners for spreading the news, we have added over 4000 subscribers to our YouTube channel and had over 14,000 downloads or views of our podcast last week. Thanks again for your support. [Read More](http://www.embracingdigital.org)\n\n## Artificial Intelligence \n\n\n\nIntel's latest announcement marks a pivotal moment in the birth of AI-powered PCs. The Core Ultra H and U series processors launch introduces the revolutionary Meteor Lake architecture, embedding advanced AI capabilities into ultra-thin notebooks. This release enhances performance and raises the Arc graphics platform, elevating the user experience to new heights. Intel's commitment to innovation signals a transformative era in the laptop market, where AI becomes integral to everyday computing.  [Read More](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nNew York City Councilwoman-elect Susan Zhuang has admitted to quietly employing AI to communicate with the public and respond to media inquiries. Finally a politician speaking the truth! Zhuang's use of AI marks a unique approach to engaging with constituents, and the implications of this tech-driven strategy may reshape the landscape of political communication in the digital age.  [Read More](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nIntel's creation of the personalized AI PC has brought a potential massive labor disruption to businesses as they continue integrating artificial intelligence (AI) into their operations. As AI technologies are adopted, job functions and workdays are expected to change, raising concerns about potential workforce challenges. Companies are preparing for this transformative shift and recognizing the need for proactive strategies to navigate potential disruptions and ensure a smooth transition to an AI-driven future.  [Read More](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Data Management\n\n\n\nSnowflake, a leading cloud data platform, has achieved FedRAMP High Authorization on AWS GovCloud US West and US East. This significant milestone highlights Snowflake's commitment to meeting stringent government security standards, enabling federal agencies to leverage its advanced data management capabilities in a secure cloud environment. The FedRAMP High Authorization underscores Snowflake's position as a trusted partner in providing fast and scalable solutions for government entities, fostering innovation and efficiency in data management. [Read More](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix has introduced the Common Data Platform (CDP) 3.0, which includes advanced multi-cloud data management and enterprise AI capabilities. This release reflects Solix's commitment to providing state-of-the-art solutions for managing and deriving insights from large datasets. The CDP 3.0's improved machine learning and AI features enable organizations to make more informed data-driven decisions. The trend of multi-cloud data management is driving the development of many new commercial offerings, and Solix's latest release is poised to be a significant contributor in this space.  [Read More](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\n\nIn a recent report, Datanami explores crucial considerations for ensuring your data management strategy is ready for AI with five key indicators. Prioritize data quality, scalability, integration capabilities, robust security, and flexibility. These elements are crucial for optimizing systems as artificial intelligence continues to shape data processes. Stay ahead in the AI era by evaluating and enhancing these aspects to meet the demands of evolving technologies. Read more at datanami.com for comprehensive insights. [Read More](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Cybersecurity\n\n\n\nBritain's National Grid has reportedly severed ties with a China-based supplier due to cybersecurity concerns, as Reuters reported on December 17, 2023. The decision reflects heightened global scrutiny over potential risks associated with foreign technology partnerships, particularly in critical infrastructure sectors. This move by the National Grid underscores the increasing emphasis on bolstering cybersecurity measures to safeguard essential services against potential threats.  [Read More](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\n\nHackers have targeted the U.S. water supply, raising alarms about the vulnerability of critical infrastructure. Axios reported on December 16, 2023, that this breach poses significant national security and public safety concerns. The incident underscores the urgent need for robust cybersecurity measures to safeguard essential services. Authorities are investigating the breach, highlighting the ongoing challenges in defending critical infrastructure from cyber threats.  [Read More](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\n\nThe NSA has warned about Russian cyber actors exploiting a known vulnerability with global implications. The identified vulnerability has a significant impact worldwide, emphasizing the importance of mitigating cybersecurity risks. The NSA advises organizations to promptly address and patch this vulnerability to enhance their resilience against potential cyber threats.  [Read More](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Embracing Digital Transformation\n\n\n\nDarren looks to open source for zero trust network architecture with openZiti with one of the open-source community organizers Philip Griffiths.  Also, a special thanks to the Embracing Digital Transformation listeners for spreading the news, we have added over 4000 subscribers to our YouTube channel and had over 14,000 downloads or views of our podcast last week. Thanks again for your support. [Read More](http://www.embracingdigital.org)\n\n## Artificial Intelligence \n\n## Artificial Intelligence \n\n\n\nIntel's latest announcement marks a pivotal moment in the birth of AI-powered PCs. The Core Ultra H and U series processors launch introduces the revolutionary Meteor Lake architecture, embedding advanced AI capabilities into ultra-thin notebooks. This release enhances performance and raises the Arc graphics platform, elevating the user experience to new heights. Intel's commitment to innovation signals a transformative era in the laptop market, where AI becomes integral to everyday computing.  [Read More](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nNew York City Councilwoman-elect Susan Zhuang has admitted to quietly employing AI to communicate with the public and respond to media inquiries. Finally a politician speaking the truth! Zhuang's use of AI marks a unique approach to engaging with constituents, and the implications of this tech-driven strategy may reshape the landscape of political communication in the digital age.  [Read More](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nIntel's creation of the personalized AI PC has brought a potential massive labor disruption to businesses as they continue integrating artificial intelligence (AI) into their operations. As AI technologies are adopted, job functions and workdays are expected to change, raising concerns about potential workforce challenges. Companies are preparing for this transformative shift and recognizing the need for proactive strategies to navigate potential disruptions and ensure a smooth transition to an AI-driven future.  [Read More](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Data Management\n\n\n\nSnowflake, a leading cloud data platform, has achieved FedRAMP High Authorization on AWS GovCloud US West and US East. This significant milestone highlights Snowflake's commitment to meeting stringent government security standards, enabling federal agencies to leverage its advanced data management capabilities in a secure cloud environment. The FedRAMP High Authorization underscores Snowflake's position as a trusted partner in providing fast and scalable solutions for government entities, fostering innovation and efficiency in data management. [Read More](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix has introduced the Common Data Platform (CDP) 3.0, which includes advanced multi-cloud data management and enterprise AI capabilities. This release reflects Solix's commitment to providing state-of-the-art solutions for managing and deriving insights from large datasets. The CDP 3.0's improved machine learning and AI features enable organizations to make more informed data-driven decisions. The trend of multi-cloud data management is driving the development of many new commercial offerings, and Solix's latest release is poised to be a significant contributor in this space.  [Read More](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\n\nIn a recent report, Datanami explores crucial considerations for ensuring your data management strategy is ready for AI with five key indicators. Prioritize data quality, scalability, integration capabilities, robust security, and flexibility. These elements are crucial for optimizing systems as artificial intelligence continues to shape data processes. Stay ahead in the AI era by evaluating and enhancing these aspects to meet the demands of evolving technologies. Read more at datanami.com for comprehensive insights. [Read More](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Cybersecurity\n\n\n\nBritain's National Grid has reportedly severed ties with a China-based supplier due to cybersecurity concerns, as Reuters reported on December 17, 2023. The decision reflects heightened global scrutiny over potential risks associated with foreign technology partnerships, particularly in critical infrastructure sectors. This move by the National Grid underscores the increasing emphasis on bolstering cybersecurity measures to safeguard essential services against potential threats.  [Read More](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\n\nHackers have targeted the U.S. water supply, raising alarms about the vulnerability of critical infrastructure. Axios reported on December 16, 2023, that this breach poses significant national security and public safety concerns. The incident underscores the urgent need for robust cybersecurity measures to safeguard essential services. Authorities are investigating the breach, highlighting the ongoing challenges in defending critical infrastructure from cyber threats.  [Read More](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\n\nThe NSA has warned about Russian cyber actors exploiting a known vulnerability with global implications. The identified vulnerability has a significant impact worldwide, emphasizing the importance of mitigating cybersecurity risks. The NSA advises organizations to promptly address and patch this vulnerability to enhance their resilience against potential cyber threats.  [Read More](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Embracing Digital Transformation\n\n\n\nDarren looks to open source for zero trust network architecture with openZiti with one of the open-source community organizers Philip Griffiths.  Also, a special thanks to the Embracing Digital Transformation listeners for spreading the news, we have added over 4000 subscribers to our YouTube channel and had over 14,000 downloads or views of our podcast last week. Thanks again for your support. [Read More](http://www.embracingdigital.org)\n\n## Artificial Intelligence \n\n## Artificial Intelligence \n\n\n\nIntel's latest announcement marks a pivotal moment in the birth of AI-powered PCs. The Core Ultra H and U series processors launch introduces the revolutionary Meteor Lake architecture, embedding advanced AI capabilities into ultra-thin notebooks. This release enhances performance and raises the Arc graphics platform, elevating the user experience to new heights. Intel's commitment to innovation signals a transformative era in the laptop market, where AI becomes integral to everyday computing.  [Read More](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nNew York City Councilwoman-elect Susan Zhuang has admitted to quietly employing AI to communicate with the public and respond to media inquiries. Finally a politician speaking the truth! Zhuang's use of AI marks a unique approach to engaging with constituents, and the implications of this tech-driven strategy may reshape the landscape of political communication in the digital age.  [Read More](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nIntel's creation of the personalized AI PC has brought a potential massive labor disruption to businesses as they continue integrating artificial intelligence (AI) into their operations. As AI technologies are adopted, job functions and workdays are expected to change, raising concerns about potential workforce challenges. Companies are preparing for this transformative shift and recognizing the need for proactive strategies to navigate potential disruptions and ensure a smooth transition to an AI-driven future.  [Read More](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Data Management\n\n\n\nSnowflake, a leading cloud data platform, has achieved FedRAMP High Authorization on AWS GovCloud US West and US East. This significant milestone highlights Snowflake's commitment to meeting stringent government security standards, enabling federal agencies to leverage its advanced data management capabilities in a secure cloud environment. The FedRAMP High Authorization underscores Snowflake's position as a trusted partner in providing fast and scalable solutions for government entities, fostering innovation and efficiency in data management. [Read More](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix has introduced the Common Data Platform (CDP) 3.0, which includes advanced multi-cloud data management and enterprise AI capabilities. This release reflects Solix's commitment to providing state-of-the-art solutions for managing and deriving insights from large datasets. The CDP 3.0's improved machine learning and AI features enable organizations to make more informed data-driven decisions. The trend of multi-cloud data management is driving the development of many new commercial offerings, and Solix's latest release is poised to be a significant contributor in this space.  [Read More](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\n\nIn a recent report, Datanami explores crucial considerations for ensuring your data management strategy is ready for AI with five key indicators. Prioritize data quality, scalability, integration capabilities, robust security, and flexibility. These elements are crucial for optimizing systems as artificial intelligence continues to shape data processes. Stay ahead in the AI era by evaluating and enhancing these aspects to meet the demands of evolving technologies. Read more at datanami.com for comprehensive insights. [Read More](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Cybersecurity\n\n\n\nBritain's National Grid has reportedly severed ties with a China-based supplier due to cybersecurity concerns, as Reuters reported on December 17, 2023. The decision reflects heightened global scrutiny over potential risks associated with foreign technology partnerships, particularly in critical infrastructure sectors. This move by the National Grid underscores the increasing emphasis on bolstering cybersecurity measures to safeguard essential services against potential threats.  [Read More](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\n\nHackers have targeted the U.S. water supply, raising alarms about the vulnerability of critical infrastructure. Axios reported on December 16, 2023, that this breach poses significant national security and public safety concerns. The incident underscores the urgent need for robust cybersecurity measures to safeguard essential services. Authorities are investigating the breach, highlighting the ongoing challenges in defending critical infrastructure from cyber threats.  [Read More](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\n\nThe NSA has warned about Russian cyber actors exploiting a known vulnerability with global implications. The identified vulnerability has a significant impact worldwide, emphasizing the importance of mitigating cybersecurity risks. The NSA advises organizations to promptly address and patch this vulnerability to enhance their resilience against potential cyber threats.  [Read More](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Embracing Digital Transformation\n\n\n\nDarren looks to open source for zero trust network architecture with openZiti with one of the open-source community organizers Philip Griffiths.  Also, a special thanks to the Embracing Digital Transformation listeners for spreading the news, we have added over 4000 subscribers to our YouTube channel and had over 14,000 downloads or views of our podcast last week. Thanks again for your support. [Read More](http://www.embracingdigital.org)\n\n## Artificial Intelligence \n\n\n\nIntel's latest announcement marks a pivotal moment in the birth of AI-powered PCs. The Core Ultra H and U series processors launch introduces the revolutionary Meteor Lake architecture, embedding advanced AI capabilities into ultra-thin notebooks. This release enhances performance and raises the Arc graphics platform, elevating the user experience to new heights. Intel's commitment to innovation signals a transformative era in the laptop market, where AI becomes integral to everyday computing.  [Read More](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nNew York City Councilwoman-elect Susan Zhuang has admitted to quietly employing AI to communicate with the public and respond to media inquiries. Finally a politician speaking the truth! Zhuang's use of AI marks a unique approach to engaging with constituents, and the implications of this tech-driven strategy may reshape the landscape of political communication in the digital age.  [Read More](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nIntel's creation of the personalized AI PC has brought a potential massive labor disruption to businesses as they continue integrating artificial intelligence (AI) into their operations. As AI technologies are adopted, job functions and workdays are expected to change, raising concerns about potential workforce challenges. Companies are preparing for this transformative shift and recognizing the need for proactive strategies to navigate potential disruptions and ensure a smooth transition to an AI-driven future.  [Read More](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Data Management\n\n\n\nSnowflake, a leading cloud data platform, has achieved FedRAMP High Authorization on AWS GovCloud US West and US East. This significant milestone highlights Snowflake's commitment to meeting stringent government security standards, enabling federal agencies to leverage its advanced data management capabilities in a secure cloud environment. The FedRAMP High Authorization underscores Snowflake's position as a trusted partner in providing fast and scalable solutions for government entities, fostering innovation and efficiency in data management. [Read More](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix has introduced the Common Data Platform (CDP) 3.0, which includes advanced multi-cloud data management and enterprise AI capabilities. This release reflects Solix's commitment to providing state-of-the-art solutions for managing and deriving insights from large datasets. The CDP 3.0's improved machine learning and AI features enable organizations to make more informed data-driven decisions. The trend of multi-cloud data management is driving the development of many new commercial offerings, and Solix's latest release is poised to be a significant contributor in this space.  [Read More](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\n\nIn a recent report, Datanami explores crucial considerations for ensuring your data management strategy is ready for AI with five key indicators. Prioritize data quality, scalability, integration capabilities, robust security, and flexibility. These elements are crucial for optimizing systems as artificial intelligence continues to shape data processes. Stay ahead in the AI era by evaluating and enhancing these aspects to meet the demands of evolving technologies. Read more at datanami.com for comprehensive insights. [Read More](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Cybersecurity\n\n\n\nBritain's National Grid has reportedly severed ties with a China-based supplier due to cybersecurity concerns, as Reuters reported on December 17, 2023. The decision reflects heightened global scrutiny over potential risks associated with foreign technology partnerships, particularly in critical infrastructure sectors. This move by the National Grid underscores the increasing emphasis on bolstering cybersecurity measures to safeguard essential services against potential threats.  [Read More](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\n\nHackers have targeted the U.S. water supply, raising alarms about the vulnerability of critical infrastructure. Axios reported on December 16, 2023, that this breach poses significant national security and public safety concerns. The incident underscores the urgent need for robust cybersecurity measures to safeguard essential services. Authorities are investigating the breach, highlighting the ongoing challenges in defending critical infrastructure from cyber threats.  [Read More](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\n\nThe NSA has warned about Russian cyber actors exploiting a known vulnerability with global implications. The identified vulnerability has a significant impact worldwide, emphasizing the importance of mitigating cybersecurity risks. The NSA advises organizations to promptly address and patch this vulnerability to enhance their resilience against potential cyber threats.  [Read More](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Embracing Digital Transformation\n\n\n\nDarren looks to open source for zero trust network architecture with openZiti with one of the open-source community organizers Philip Griffiths.  Also, a special thanks to the Embracing Digital Transformation listeners for spreading the news, we have added over 4000 subscribers to our YouTube channel and had over 14,000 downloads or views of our podcast last week. Thanks again for your support. [Read More](http://www.embracingdigital.org)\n\n## Artificial Intelligence \n\n\n\nIntel's latest announcement marks a pivotal moment in the birth of AI-powered PCs. The Core Ultra H and U series processors launch introduces the revolutionary Meteor Lake architecture, embedding advanced AI capabilities into ultra-thin notebooks. This release enhances performance and raises the Arc graphics platform, elevating the user experience to new heights. Intel's commitment to innovation signals a transformative era in the laptop market, where AI becomes integral to everyday computing.  [Read More](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nNew York City Councilwoman-elect Susan Zhuang has admitted to quietly employing AI to communicate with the public and respond to media inquiries. Finally a politician speaking the truth! Zhuang's use of AI marks a unique approach to engaging with constituents, and the implications of this tech-driven strategy may reshape the landscape of political communication in the digital age.  [Read More](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nIntel's creation of the personalized AI PC has brought a potential massive labor disruption to businesses as they continue integrating artificial intelligence (AI) into their operations. As AI technologies are adopted, job functions and workdays are expected to change, raising concerns about potential workforce challenges. Companies are preparing for this transformative shift and recognizing the need for proactive strategies to navigate potential disruptions and ensure a smooth transition to an AI-driven future.  [Read More](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Data Management\n\n\n\nSnowflake, a leading cloud data platform, has achieved FedRAMP High Authorization on AWS GovCloud US West and US East. This significant milestone highlights Snowflake's commitment to meeting stringent government security standards, enabling federal agencies to leverage its advanced data management capabilities in a secure cloud environment. The FedRAMP High Authorization underscores Snowflake's position as a trusted partner in providing fast and scalable solutions for government entities, fostering innovation and efficiency in data management. [Read More](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix has introduced the Common Data Platform (CDP) 3.0, which includes advanced multi-cloud data management and enterprise AI capabilities. This release reflects Solix's commitment to providing state-of-the-art solutions for managing and deriving insights from large datasets. The CDP 3.0's improved machine learning and AI features enable organizations to make more informed data-driven decisions. The trend of multi-cloud data management is driving the development of many new commercial offerings, and Solix's latest release is poised to be a significant contributor in this space.  [Read More](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\n\nIn a recent report, Datanami explores crucial considerations for ensuring your data management strategy is ready for AI with five key indicators. Prioritize data quality, scalability, integration capabilities, robust security, and flexibility. These elements are crucial for optimizing systems as artificial intelligence continues to shape data processes. Stay ahead in the AI era by evaluating and enhancing these aspects to meet the demands of evolving technologies. Read more at datanami.com for comprehensive insights. [Read More](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Cybersecurity\n\n\n\nBritain's National Grid has reportedly severed ties with a China-based supplier due to cybersecurity concerns, as Reuters reported on December 17, 2023. The decision reflects heightened global scrutiny over potential risks associated with foreign technology partnerships, particularly in critical infrastructure sectors. This move by the National Grid underscores the increasing emphasis on bolstering cybersecurity measures to safeguard essential services against potential threats.  [Read More](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\n\nHackers have targeted the U.S. water supply, raising alarms about the vulnerability of critical infrastructure. Axios reported on December 16, 2023, that this breach poses significant national security and public safety concerns. The incident underscores the urgent need for robust cybersecurity measures to safeguard essential services. Authorities are investigating the breach, highlighting the ongoing challenges in defending critical infrastructure from cyber threats.  [Read More](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\n\nThe NSA has warned about Russian cyber actors exploiting a known vulnerability with global implications. The identified vulnerability has a significant impact worldwide, emphasizing the importance of mitigating cybersecurity risks. The NSA advises organizations to promptly address and patch this vulnerability to enhance their resilience against potential cyber threats.  [Read More](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Embracing Digital Transformation\n\n\n\nDarren looks to open source for zero trust network architecture with openZiti with one of the open-source community organizers Philip Griffiths.  Also, a special thanks to the Embracing Digital Transformation listeners for spreading the news, we have added over 4000 subscribers to our YouTube channel and had over 14,000 downloads or views of our podcast last week. Thanks again for your support. [Read More](http://www.embracingdigital.org)\n\n## Artificial Intelligence \n\n\n\nIntel's latest announcement marks a pivotal moment in the birth of AI-powered PCs. The Core Ultra H and U series processors launch introduces the revolutionary Meteor Lake architecture, embedding advanced AI capabilities into ultra-thin notebooks. This release enhances performance and raises the Arc graphics platform, elevating the user experience to new heights. Intel's commitment to innovation signals a transformative era in the laptop market, where AI becomes integral to everyday computing.  [Read More](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nNew York City Councilwoman-elect Susan Zhuang has admitted to quietly employing AI to communicate with the public and respond to media inquiries. Finally a politician speaking the truth! Zhuang's use of AI marks a unique approach to engaging with constituents, and the implications of this tech-driven strategy may reshape the landscape of political communication in the digital age.  [Read More](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nIntel's creation of the personalized AI PC has brought a potential massive labor disruption to businesses as they continue integrating artificial intelligence (AI) into their operations. As AI technologies are adopted, job functions and workdays are expected to change, raising concerns about potential workforce challenges. Companies are preparing for this transformative shift and recognizing the need for proactive strategies to navigate potential disruptions and ensure a smooth transition to an AI-driven future.  [Read More](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Data Management\n\n\n\nSnowflake, a leading cloud data platform, has achieved FedRAMP High Authorization on AWS GovCloud US West and US East. This significant milestone highlights Snowflake's commitment to meeting stringent government security standards, enabling federal agencies to leverage its advanced data management capabilities in a secure cloud environment. The FedRAMP High Authorization underscores Snowflake's position as a trusted partner in providing fast and scalable solutions for government entities, fostering innovation and efficiency in data management. [Read More](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix has introduced the Common Data Platform (CDP) 3.0, which includes advanced multi-cloud data management and enterprise AI capabilities. This release reflects Solix's commitment to providing state-of-the-art solutions for managing and deriving insights from large datasets. The CDP 3.0's improved machine learning and AI features enable organizations to make more informed data-driven decisions. The trend of multi-cloud data management is driving the development of many new commercial offerings, and Solix's latest release is poised to be a significant contributor in this space.  [Read More](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\n\nIn a recent report, Datanami explores crucial considerations for ensuring your data management strategy is ready for AI with five key indicators. Prioritize data quality, scalability, integration capabilities, robust security, and flexibility. These elements are crucial for optimizing systems as artificial intelligence continues to shape data processes. Stay ahead in the AI era by evaluating and enhancing these aspects to meet the demands of evolving technologies. Read more at datanami.com for comprehensive insights. [Read More](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Cybersecurity\n\n\n\nBritain's National Grid has reportedly severed ties with a China-based supplier due to cybersecurity concerns, as Reuters reported on December 17, 2023. The decision reflects heightened global scrutiny over potential risks associated with foreign technology partnerships, particularly in critical infrastructure sectors. This move by the National Grid underscores the increasing emphasis on bolstering cybersecurity measures to safeguard essential services against potential threats.  [Read More](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\n\nHackers have targeted the U.S. water supply, raising alarms about the vulnerability of critical infrastructure. Axios reported on December 16, 2023, that this breach poses significant national security and public safety concerns. The incident underscores the urgent need for robust cybersecurity measures to safeguard essential services. Authorities are investigating the breach, highlighting the ongoing challenges in defending critical infrastructure from cyber threats.  [Read More](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\n\nThe NSA has warned about Russian cyber actors exploiting a known vulnerability with global implications. The identified vulnerability has a significant impact worldwide, emphasizing the importance of mitigating cybersecurity risks. The NSA advises organizations to promptly address and patch this vulnerability to enhance their resilience against potential cyber threats.  [Read More](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Embracing Digital Transformation\n\n\n\nDarren looks to open source for zero trust network architecture with openZiti with one of the open-source community organizers Philip Griffiths.  Also, a special thanks to the Embracing Digital Transformation listeners for spreading the news, we have added over 4000 subscribers to our YouTube channel and had over 14,000 downloads or views of our podcast last week. Thanks again for your support. [Read More](http://www.embracingdigital.org)\n\n## Artificial Intelligence \n\n\n\nIntel's latest announcement marks a pivotal moment in the birth of AI-powered PCs. The Core Ultra H and U series processors launch introduces the revolutionary Meteor Lake architecture, embedding advanced AI capabilities into ultra-thin notebooks. This release enhances performance and raises the Arc graphics platform, elevating the user experience to new heights. Intel's commitment to innovation signals a transformative era in the laptop market, where AI becomes integral to everyday computing.  [Read More](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nNew York City Councilwoman-elect Susan Zhuang has admitted to quietly employing AI to communicate with the public and respond to media inquiries. Finally a politician speaking the truth! Zhuang's use of AI marks a unique approach to engaging with constituents, and the implications of this tech-driven strategy may reshape the landscape of political communication in the digital age.  [Read More](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nIntel's creation of the personalized AI PC has brought a potential massive labor disruption to businesses as they continue integrating artificial intelligence (AI) into their operations. As AI technologies are adopted, job functions and workdays are expected to change, raising concerns about potential workforce challenges. Companies are preparing for this transformative shift and recognizing the need for proactive strategies to navigate potential disruptions and ensure a smooth transition to an AI-driven future.  [Read More](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Data Management\n\n\n\nSnowflake, a leading cloud data platform, has achieved FedRAMP High Authorization on AWS GovCloud US West and US East. This significant milestone highlights Snowflake's commitment to meeting stringent government security standards, enabling federal agencies to leverage its advanced data management capabilities in a secure cloud environment. The FedRAMP High Authorization underscores Snowflake's position as a trusted partner in providing fast and scalable solutions for government entities, fostering innovation and efficiency in data management. [Read More](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix has introduced the Common Data Platform (CDP) 3.0, which includes advanced multi-cloud data management and enterprise AI capabilities. This release reflects Solix's commitment to providing state-of-the-art solutions for managing and deriving insights from large datasets. The CDP 3.0's improved machine learning and AI features enable organizations to make more informed data-driven decisions. The trend of multi-cloud data management is driving the development of many new commercial offerings, and Solix's latest release is poised to be a significant contributor in this space.  [Read More](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\n\nIn a recent report, Datanami explores crucial considerations for ensuring your data management strategy is ready for AI with five key indicators. Prioritize data quality, scalability, integration capabilities, robust security, and flexibility. These elements are crucial for optimizing systems as artificial intelligence continues to shape data processes. Stay ahead in the AI era by evaluating and enhancing these aspects to meet the demands of evolving technologies. Read more at datanami.com for comprehensive insights. [Read More](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Cybersecurity\n\n\n\nBritain's National Grid has reportedly severed ties with a China-based supplier due to cybersecurity concerns, as Reuters reported on December 17, 2023. The decision reflects heightened global scrutiny over potential risks associated with foreign technology partnerships, particularly in critical infrastructure sectors. This move by the National Grid underscores the increasing emphasis on bolstering cybersecurity measures to safeguard essential services against potential threats.  [Read More](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\n\nHackers have targeted the U.S. water supply, raising alarms about the vulnerability of critical infrastructure. Axios reported on December 16, 2023, that this breach poses significant national security and public safety concerns. The incident underscores the urgent need for robust cybersecurity measures to safeguard essential services. Authorities are investigating the breach, highlighting the ongoing challenges in defending critical infrastructure from cyber threats.  [Read More](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\n\nThe NSA has warned about Russian cyber actors exploiting a known vulnerability with global implications. The identified vulnerability has a significant impact worldwide, emphasizing the importance of mitigating cybersecurity risks. The NSA advises organizations to promptly address and patch this vulnerability to enhance their resilience against potential cyber threats.  [Read More](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Embracing Digital Transformation\n\n\n\nDarren looks to open source for zero trust network architecture with openZiti with one of the open-source community organizers Philip Griffiths.  Also, a special thanks to the Embracing Digital Transformation listeners for spreading the news, we have added over 4000 subscribers to our YouTube channel and had over 14,000 downloads or views of our podcast last week. Thanks again for your support. [Read More](http://www.embracingdigital.org)\n\n## Artificial Intelligence \n\n\n\nIntel's latest announcement marks a pivotal moment in the birth of AI-powered PCs. The Core Ultra H and U series processors launch introduces the revolutionary Meteor Lake architecture, embedding advanced AI capabilities into ultra-thin notebooks. This release enhances performance and raises the Arc graphics platform, elevating the user experience to new heights. Intel's commitment to innovation signals a transformative era in the laptop market, where AI becomes integral to everyday computing.  [Read More](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nNew York City Councilwoman-elect Susan Zhuang has admitted to quietly employing AI to communicate with the public and respond to media inquiries. Finally a politician speaking the truth! Zhuang's use of AI marks a unique approach to engaging with constituents, and the implications of this tech-driven strategy may reshape the landscape of political communication in the digital age.  [Read More](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nIntel's creation of the personalized AI PC has brought a potential massive labor disruption to businesses as they continue integrating artificial intelligence (AI) into their operations. As AI technologies are adopted, job functions and workdays are expected to change, raising concerns about potential workforce challenges. Companies are preparing for this transformative shift and recognizing the need for proactive strategies to navigate potential disruptions and ensure a smooth transition to an AI-driven future.  [Read More](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Data Management\n\n\n\nSnowflake, a leading cloud data platform, has achieved FedRAMP High Authorization on AWS GovCloud US West and US East. This significant milestone highlights Snowflake's commitment to meeting stringent government security standards, enabling federal agencies to leverage its advanced data management capabilities in a secure cloud environment. The FedRAMP High Authorization underscores Snowflake's position as a trusted partner in providing fast and scalable solutions for government entities, fostering innovation and efficiency in data management. [Read More](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix has introduced the Common Data Platform (CDP) 3.0, which includes advanced multi-cloud data management and enterprise AI capabilities. This release reflects Solix's commitment to providing state-of-the-art solutions for managing and deriving insights from large datasets. The CDP 3.0's improved machine learning and AI features enable organizations to make more informed data-driven decisions. The trend of multi-cloud data management is driving the development of many new commercial offerings, and Solix's latest release is poised to be a significant contributor in this space.  [Read More](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n## Artificial Intelligence \n\n\n\nIntel's latest announcement marks a pivotal moment in the birth of AI-powered PCs. The Core Ultra H and U series processors launch introduces the revolutionary Meteor Lake architecture, embedding advanced AI capabilities into ultra-thin notebooks. This release enhances performance and raises the Arc graphics platform, elevating the user experience to new heights. Intel's commitment to innovation signals a transformative era in the laptop market, where AI becomes integral to everyday computing.  [Read More](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nNew York City Councilwoman-elect Susan Zhuang has admitted to quietly employing AI to communicate with the public and respond to media inquiries. Finally a politician speaking the truth! Zhuang's use of AI marks a unique approach to engaging with constituents, and the implications of this tech-driven strategy may reshape the landscape of political communication in the digital age.  [Read More](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nIntel's creation of the personalized AI PC has brought a potential massive labor disruption to businesses as they continue integrating artificial intelligence (AI) into their operations. As AI technologies are adopted, job functions and workdays are expected to change, raising concerns about potential workforce challenges. Companies are preparing for this transformative shift and recognizing the need for proactive strategies to navigate potential disruptions and ensure a smooth transition to an AI-driven future.  [Read More](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Data Management\n\n\n\nSnowflake, a leading cloud data platform, has achieved FedRAMP High Authorization on AWS GovCloud US West and US East. This significant milestone highlights Snowflake's commitment to meeting stringent government security standards, enabling federal agencies to leverage its advanced data management capabilities in a secure cloud environment. The FedRAMP High Authorization underscores Snowflake's position as a trusted partner in providing fast and scalable solutions for government entities, fostering innovation and efficiency in data management. [Read More](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix has introduced the Common Data Platform (CDP) 3.0, which includes advanced multi-cloud data management and enterprise AI capabilities. This release reflects Solix's commitment to providing state-of-the-art solutions for managing and deriving insights from large datasets. The CDP 3.0's improved machine learning and AI features enable organizations to make more informed data-driven decisions. The trend of multi-cloud data management is driving the development of many new commercial offerings, and Solix's latest release is poised to be a significant contributor in this space.  [Read More](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\n\nIn a recent report, Datanami explores crucial considerations for ensuring your data management strategy is ready for AI with five key indicators. Prioritize data quality, scalability, integration capabilities, robust security, and flexibility. These elements are crucial for optimizing systems as artificial intelligence continues to shape data processes. Stay ahead in the AI era by evaluating and enhancing these aspects to meet the demands of evolving technologies. Read more at datanami.com for comprehensive insights. [Read More](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Cybersecurity\n\n\n\nBritain's National Grid has reportedly severed ties with a China-based supplier due to cybersecurity concerns, as Reuters reported on December 17, 2023. The decision reflects heightened global scrutiny over potential risks associated with foreign technology partnerships, particularly in critical infrastructure sectors. This move by the National Grid underscores the increasing emphasis on bolstering cybersecurity measures to safeguard essential services against potential threats.  [Read More](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\n\nHackers have targeted the U.S. water supply, raising alarms about the vulnerability of critical infrastructure. Axios reported on December 16, 2023, that this breach poses significant national security and public safety concerns. The incident underscores the urgent need for robust cybersecurity measures to safeguard essential services. Authorities are investigating the breach, highlighting the ongoing challenges in defending critical infrastructure from cyber threats.  [Read More](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\n\nThe NSA has warned about Russian cyber actors exploiting a known vulnerability with global implications. The identified vulnerability has a significant impact worldwide, emphasizing the importance of mitigating cybersecurity risks. The NSA advises organizations to promptly address and patch this vulnerability to enhance their resilience against potential cyber threats.  [Read More](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Embracing Digital Transformation\n\n\n\nDarren looks to open source for zero trust network architecture with openZiti with one of the open-source community organizers Philip Griffiths.  Also, a special thanks to the Embracing Digital Transformation listeners for spreading the news, we have added over 4000 subscribers to our YouTube channel and had over 14,000 downloads or views of our podcast last week. Thanks again for your support. [Read More](http://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW46-en","image":"./briefs/edw-46/en/thumbnail.png","lang":"en","summary":"The week of December 17, 2023, in Digital Transformation news includes stories about AI, data management, and cybersecurity. Check out the birth of the AI PC, Multi-cloud data management for government clouds, and major warnings for critical infrastructure cybersecurity. "},{"id":214,"type":"News Brief","title":"2023-12-23","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","wordpress","plugindata","xwpspamshield","ontariohack","healthcarecyber","holidayhacks","ontarioincidents","artificialintel","generativeai","aiinschool","intelnervana","cloudcomputing","cloudcost","pentagoncloud","cisco","isovalent","openziti","podcast","digitaltransf"],"body":"\n\n## Cybersecurity\n\n\n\nA rogue WordPress plugin has jeopardized users' data by exposing email addresses, usernames, and plaintext passwords. Security researchers discovered the breach, urging affected users to reset their credentials immediately. The plugin, named \"X-WP-SPAM-SHIELD-PRO,\" is known for malicious activities, emphasizing the importance of regular security audits for WordPress sites. Website administrators are advised to remove the compromised plugin and enhance security measures to prevent unauthorized access and data exposure. [Read More](https://thehackernews.com/2023/12/rogue-wordpress-plugin-exposes-e.html)\n\n\n\nThe CBC reports a surge in cyberattacks targeting Ontario, Canada, in 2023. These incidents involve a range of sectors, including healthcare, municipalities, and education. Hackers exploit vulnerabilities, causing disruptions and exposing sensitive data. Experts emphasize improving cybersecurity measures, training, and collaboration to safeguard against evolving cyber threats and protect critical infrastructure. [Read More](https://www.cbc.ca/news/canada/toronto/cybersecurity-ontario-incidents-2023-1.7048495)\n\n\n\nCyber attacks during the Christmas holidays are rising for several reasons. Firstly, online shopping activity is heightened, creating opportunities for cybercriminals. Secondly, phishing attempts have increased, where attackers trick victims using email or social media. Thirdly, remote working employees present a security risk. Fourthly, social engineering attacks can be more successful during this season. Finally, potential security negligence due to festivities can also increase the risk of attacks. It is vital to be aware of these risks and take measures to mitigate them. [Read More](https://bit-sentinel.com/5-reasons-why-cyber-attacks-increase-during-the-christmas-holidays/)\n\n## Artificial Intelligence\n\n\n\nIn 2023, generative AI made significant strides across diverse sectors, including healthcare, finance, and entertainment. CNBC's Jim Cramer recaps the year-end by emphasizing AI's contributions to drug discovery, financial modeling, and content creation. The transformative potential of generative AI is highlighted, marking its continued influence on innovation and technological advancements. [Read More](https://www.cnbc.com/2023/12/21/jim-cramer-recaps-the-year-in-generative-artificial-intelligence.html)\n\n\n\nFuture-Ed's recent article explores the potential of AI integration in schools. It emphasizes the importance of ethical considerations and responsible implementation for successful integration. The article provides valuable insights for educators, policymakers, and stakeholders navigating this evolving landscape. The Embracing Digital Transformation podcast will also feature interviews with college students and their professors, discussing their thoughts. [Read More](https://www.future-ed.org/navigating-the-artificial-intelligence-revolution-in-schools/)\n\n\n\nIntel has unveiled a new chip, the Intel Nervana NCS-1, to accelerate artificial intelligence (AI) applications. The NCS-1 is designed to enhance AI performance, offering high throughput and efficiency. Intel aims to advance AI innovation by providing developers with tools to efficiently deploy and scale AI models. The chip is part of Intel's commitment to empowering AI applications across various industries and represents a significant step in the company's AI strategy. [Read More](https://www.intc.com/news-events/press-releases/detail/1663/intel-accelerates-ai-everywhere-with-launch-of-powerful)\n\n## Ubiquitous Computing\n\n\n\nFinancial institutions, including Capital One and Arvest Bank, are prioritizing cloud cost control amid challenges. The focus is optimizing expenses through strategic alignment with business goals, employing cloud cost management tools, and implementing governance practices. As cloud computing evolves, effective cost management becomes crucial for these organizations. [Read More](https://www.ciodive.com/news/cloud-cost-control-capital-one-arvest-bank/703025/)\n\n\n\nThe Washington Post reports on the Pentagon's reconsideration of its cloud computing contracts, suggesting potential shifts in strategy. The article explores the implications of these changes and their impact on major technology companies bidding for lucrative defense contracts. As the Pentagon navigates the complex landscape of cloud computing, the article highlights the evolving dynamics and competitive landscape in the defense sector's pursuit of advanced technological capabilities. [Read More](https://www.washingtonpost.com/technology/2023/12/21/pentagon-cloud-computing-contracts/)\n\n\n\nCisco is making a strategic move in cloud security by acquiring Isovalent, an innovator in cloud-native networking and security. This acquisition reflects Cisco's commitment to advancing its artificial intelligence (AI) and cloud security capabilities. Isovalent's expertise in eBPF (extended Berkeley Packet Filter) technology is expected to enhance Cisco's ability to secure cloud environments, providing advanced solutions for the evolving cybersecurity landscape in cloud-based infrastructures. [Read More](https://www.informationweek.com/it-infrastructure/cloud-computing#close-modal)\n\n## Embracing Digital Transformation\n\n\n\nFirstly, I'd like to wish all my listeners a Merry Christmas and a Happy New Year. In this week's podcast, Darren explores OpenZiti, an open-source zero-trust networking solution. Also, thank you to everyone spreading the word about the podcast: we've now surpassed 10,000 weekly listeners, and your support is much appreciated. [Read More](https://www.embracingdigital.org)\n\n## Cybersecurity\n\n\n\nA rogue WordPress plugin has jeopardized users' data by exposing email addresses, usernames, and plaintext passwords. Security researchers discovered the breach, urging affected users to reset their credentials immediately. The plugin, named \"X-WP-SPAM-SHIELD-PRO,\" is known for malicious activities, emphasizing the importance of regular security audits for WordPress sites. Website administrators are advised to remove the compromised plugin and enhance security measures to prevent unauthorized access and data exposure. [Read More](https://thehackernews.com/2023/12/rogue-wordpress-plugin-exposes-e.html)\n\n\n\nThe CBC reports a surge in cyberattacks targeting Ontario, Canada, in 2023. These incidents involve a range of sectors, including healthcare, municipalities, and education. Hackers exploit vulnerabilities, causing disruptions and exposing sensitive data. Experts emphasize improving cybersecurity measures, training, and collaboration to safeguard against evolving cyber threats and protect critical infrastructure. [Read More](https://www.cbc.ca/news/canada/toronto/cybersecurity-ontario-incidents-2023-1.7048495)\n\n\n\nCyber attacks during the Christmas holidays are rising for several reasons. Firstly, online shopping activity is heightened, creating opportunities for cybercriminals. Secondly, phishing attempts have increased, where attackers trick victims using email or social media. Thirdly, remote working employees present a security risk. Fourthly, social engineering attacks can be more successful during this season. Finally, potential security negligence due to festivities can also increase the risk of attacks. It is vital to be aware of these risks and take measures to mitigate them. [Read More](https://bit-sentinel.com/5-reasons-why-cyber-attacks-increase-during-the-christmas-holidays/)\n\n## Artificial Intelligence\n\n\n\nIn 2023, generative AI made significant strides across diverse sectors, including healthcare, finance, and entertainment. CNBC's Jim Cramer recaps the year-end by emphasizing AI's contributions to drug discovery, financial modeling, and content creation. The transformative potential of generative AI is highlighted, marking its continued influence on innovation and technological advancements. [Read More](https://www.cnbc.com/2023/12/21/jim-cramer-recaps-the-year-in-generative-artificial-intelligence.html)\n\n\n\nFuture-Ed's recent article explores the potential of AI integration in schools. It emphasizes the importance of ethical considerations and responsible implementation for successful integration. The article provides valuable insights for educators, policymakers, and stakeholders navigating this evolving landscape. The Embracing Digital Transformation podcast will also feature interviews with college students and their professors, discussing their thoughts. [Read More](https://www.future-ed.org/navigating-the-artificial-intelligence-revolution-in-schools/)\n\n\n\nIntel has unveiled a new chip, the Intel Nervana NCS-1, to accelerate artificial intelligence (AI) applications. The NCS-1 is designed to enhance AI performance, offering high throughput and efficiency. Intel aims to advance AI innovation by providing developers with tools to efficiently deploy and scale AI models. The chip is part of Intel's commitment to empowering AI applications across various industries and represents a significant step in the company's AI strategy. [Read More](https://www.intc.com/news-events/press-releases/detail/1663/intel-accelerates-ai-everywhere-with-launch-of-powerful)\n\n## Ubiquitous Computing\n\n\n\nFinancial institutions, including Capital One and Arvest Bank, are prioritizing cloud cost control amid challenges. The focus is optimizing expenses through strategic alignment with business goals, employing cloud cost management tools, and implementing governance practices. As cloud computing evolves, effective cost management becomes crucial for these organizations. [Read More](https://www.ciodive.com/news/cloud-cost-control-capital-one-arvest-bank/703025/)\n\n\n\nThe Washington Post reports on the Pentagon's reconsideration of its cloud computing contracts, suggesting potential shifts in strategy. The article explores the implications of these changes and their impact on major technology companies bidding for lucrative defense contracts. As the Pentagon navigates the complex landscape of cloud computing, the article highlights the evolving dynamics and competitive landscape in the defense sector's pursuit of advanced technological capabilities. [Read More](https://www.washingtonpost.com/technology/2023/12/21/pentagon-cloud-computing-contracts/)\n\n\n\nCisco is making a strategic move in cloud security by acquiring Isovalent, an innovator in cloud-native networking and security. This acquisition reflects Cisco's commitment to advancing its artificial intelligence (AI) and cloud security capabilities. Isovalent's expertise in eBPF (extended Berkeley Packet Filter) technology is expected to enhance Cisco's ability to secure cloud environments, providing advanced solutions for the evolving cybersecurity landscape in cloud-based infrastructures. [Read More](https://www.informationweek.com/it-infrastructure/cloud-computing#close-modal)\n\n## Embracing Digital Transformation\n\n\n\nFirstly, I'd like to wish all my listeners a Merry Christmas and a Happy New Year. In this week's podcast, Darren explores OpenZiti, an open-source zero-trust networking solution. Also, thank you to everyone spreading the word about the podcast: we've now surpassed 10,000 weekly listeners, and your support is much appreciated. [Read More](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW47-en","image":"./briefs/edw-47/en/thumbnail.png","lang":"en","summary":"During the week of December 24th, 2023 there were several news stories about digital transformation. These stories covered topics such as cybersecurity, artificial intelligence, and computing. Some of the highlights of the week included an increase in cyberattacks during the holiday season, Intel&#39;s effort to further develop AI technology, and the trend toward cost controls in cloud computing."},{"id":215,"type":"News Brief","title":"2023-12-31","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","edgecompute","digitaltransform","nsacyber","aics","tomtom","microsoft","mitresearch","aiethics","iot","healthcare","malwaredetect","patientdata","aiagriculture","predictive","genai","automation","navigation","malware"],"body":"\n\n## Cybersecurity\n\n\n\nCheckpoint's AI-powered cybersecurity solution offers a game-changing platform with a 98% accuracy rate in detecting malicious activity. The system uses advanced algorithms and predictive analytics to provide businesses with a powerful defense against evolving cyber risks, reinforcing digital security during heightened threats. [Read More](https://fortune.com/2023/12/29/ai-cybersecurity-checkpoint/)\n\n\n\nCyber attacks on healthcare facilities have surged, causing concerns about patient safety at Liberty Hospital, which has been fighting off an attack for the last two weeks. Robust cybersecurity measures are needed to protect sensitive medical information and ensure uninterrupted healthcare services. This highlights the broader vulnerabilities of critical infrastructure in the face of escalating cyber threats. [Read More](https://www.kctv5.com/2023/12/30/liberty-hospital-staff-worries-patients-are-jeopardy-if-cyber-security-incident-drags/)\n\n\n\nThe NSA's 2023 Cybersecurity Year in Review report offers insights into cyber threats, successful defenses, and emerging trends. The publication emphasizes the agency's commitment to enhancing national cyber resilience and is a valuable resource for industry professionals, policymakers, and the public. It provides a retrospective analysis of the cybersecurity landscape and guides future efforts to strengthen digital security. [Read More](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3621654/nsa-publishes-2023-cybersecurity-year-in-review/)\n\n## Artificial Intelligence\n\n\n\nMIT researchers have developed algorithms that enable machines to understand contextual nuances in language, promoting more accurate interactions. This breakthrough in natural language processing has potential applications in various fields, bridging the gap between human communication and machine comprehension. MIT's research paves the way for more advanced and context-aware AI systems. [Read More](https://news.mit.edu/2023/leveraging-language-understand-machines-1222)\n\n\n\nAI and automation are transforming governments by improving efficiency and agility. This shift towards a digital-first approach reflects a commitment to adapt to the modern era. The impact of these technologies is reshaping governance and providing a more advanced approach to meeting digital challenges. [Read More](https://federalnewsnetwork.com/commentary/2023/12/navigating-the-era-of-innovation-how-artificial-intelligence-and-automation-are-driving-a-digital-first-government/)\n\n\n\nMichael Cohen admits to submitting fake legal documents generated by AI while working for Donald Trump, raising concerns about AI ethics and oversight in the legal system. This highlights the need for vigilant oversight and safeguards to maintain the integrity of legal proceedings. The incident underscores the evolving challenges of AI ethics and integrity in legal processes. [Read More](https://www.nbcnews.com/politics/politics-news/michael-cohen-says-unknowingly-submitted-fake-ai-generated-legal-cases-rcna131631)\n\n## Edge Computing\n\n\n\nTomTom and Microsoft have partnered to introduce an innovative, generative AI solution for connected vehicles. The technology harnesses advanced algorithms to transform navigation, promising a more intelligent and responsive driving experience. This development signifies a significant step forward in integrating AI into automobiles. [Read More](https://www.iottechnews.com/news/2023/dec/19/tomtom-microsoft-unveil-generative-ai-connected-vehicles/)\n\n\n\nPanasonic fights IoT malware by deploying honeypots to detect cyber threats. The company gains valuable insights into evolving malware tactics by creating simulated targets to attract malicious activity, enhancing cybersecurity measures. This proactive approach reflects the growing emphasis on innovative techniques to safeguard connected devices and networks in the face of escalating security challenges. [Read More](https://www.wired.com/story/panasonic-iot-malware-honeypots/)\n\n\n\nIntegrating Artificial Intelligence (AI) and Internet of Things (IoT) technologies is rapidly transforming the agriculture industry. The precision agriculture market is forecasted to reach €5.2 billion by 2027, thanks to the innovative use of IoT, data analytics, and automation in farming. This technology optimizes crop yields, reduces resource consumption, and promotes sustainable and efficient agriculture worldwide. [Read More](https://iotbusinessnews.com/2023/12/22/53545-the-precision-agriculture-market-to-reach-e-5-2-billion-worldwide-in-2027/)\n\n## Embracing Digital Transformation\n\n\n\nThis week, the Embracing Digital Transformation continues its Embracing Generative AI series featuring interviews with a college student, Madeline Pulsipher, who shares her journey with GenAI, followed by a professor's perspective, Laura Newey, on using GenAI in the classroom. Don't miss these insightful episodes! [Read More](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW48-en","image":"./briefs/edw-48/en/thumbnail.png","lang":"en","summary":"Digital Transformation news for the week of January 1, 2024, covering cybersecurity, artificial intelligence, and edge computing. Topics include IoT and AI on the farm, politicians deceived by AI-generated hallucinations, and healthcare-facing cyberattacks."},{"id":216,"type":"News Brief","title":"2024-1-7","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","intel","articul8","digitalbridge","genai","ethics","china","itot","upskilling","airbus","atos","solix","dataplatform","microsoft","oracle","azure","database","digitaltransformation","generativeai"],"body":"\n\n## Artificial Intelligence\n\n\n\nIntel recently announced the spin-off of AI software company Articul8, backed by DigitalBridge. The move is aimed at improving enterprise AI capabilities. Articul8 will focus on developing advanced AI solutions, leveraging Intel's expertise in the field. This strategic move signifies Intel's commitment to advancing artificial intelligence technologies for broader industry applications. [Read More](https://www.reuters.com/technology/intel-spins-out-ai-software-firm-with-backing-digitalbridge-2024-01-03/)\n\n\n\nA report on GenAI from Foreign Affairs discusses concerns about the impact of artificial intelligence on the democratic values of the United States. The report highlights the potential misuse of AI in activities like disinformation and surveillance, and stresses the need for ethical guidelines and regulatory frameworks to address these concerns and protect the foundational principles of democracy. [Read More](https://www.foreignaffairs.com/united-states/artificial-intelligences-threat-democracy)\n\n\n\nChina has recently released guidelines for AI researchers, preventing them from using certain funds for military-related purposes. The guidelines aim to guarantee that AI technology is utilized for peaceful and ethical purposes, with a focus on avoiding its misuse in areas that could potentially harm international security. This move highlights China's determination to shape the ethical development and application of AI, emphasizing its commitment to the responsible use of AI technology. [Read More](https://www.scmp.com/news/china/science/article/3247420/china-unveils-new-artificial-intelligence-guidelines-scientists-and-bans-use-funding-applications)\n\n## Cybersecurity\n\n\n\nSecurity Intelligence's analysis emphasizes the importance of integrating IT and OT in cybersecurity. This convergence is critical to safeguarding industrial systems against cyber threats. The analysis explores challenges and advantages and highlights the need for a comprehensive security strategy. [Read More](https://securityintelligence.com/posts/it-and-ot-cybersecurity-integration/)\n\n\n\nContinuous learning in cybersecurity is crucial to address the skills gap. A recent report advocates for upskilling existing talent and investing in training programs to meet the demand for skilled professionals. Organizations need to foster a culture of ongoing skill enhancement to enhance cyber resilience. [Read More](https://www.informationweek.com/cyber-resilience/upskilling-is-the-secret-to-closing-the-cybersecurity-skills-gap-)\n\n\n\nAirbus is acquiring Atos' cybersecurity unit for $2 billion, to strengthen its cybersecurity capabilities in the ICS-OT sectors. The move emphasizes the need for robust cybersecurity measures in critical infrastructure, especially in industries where operational technology plays a pivotal role. The deal implies a strategic step towards enhancing cybersecurity within Airbus's operational framework. [Read More](https://www.darkreading.com/ics-ot-security/airbus-acquire-atos-cybersecurity-unit-2-billion)\n\n## Data Management\n\n\n\nSolix has revealed an Enterprise Data Platform for the \"Gen AI\" era. It aims to offer advanced data management solutions to meet the evolving needs of AI technologies. Solix's platform is focused on optimizing data processing and storage capabilities to support the demands of AI-driven applications and analytics. This move signifies a strategic step towards enabling efficient data utilization in the rapidly advancing field of artificial intelligence. [Read More](https://venturebeat.com/data-infrastructure/solix-launches-new-enterprise-data-platform-for-the-gen-ai-era/)\n\n\n\nMicrosoft and Oracle have teamed up to enhance compatibility between their databases, Microsoft SQL Server and Oracle Database, on Microsoft's Azure cloud platform. This partnership seeks to provide a seamless experience for users running these databases on Azure, with improved performance and flexibility. The collaboration aims to meet the evolving demands of cloud-based database solutions, supporting interoperability and ease of use for businesses utilizing both Microsoft and Oracle technologies on Azure. [Read More](https://www.infoq.com/news/2024/01/microsoft-oracle-database-azure/)\n\n\n\nIn 2024, enterprise data and AI technologies are undergoing significant shifts. Trends include the expanding role of AI in data management, heightened emphasis on data ethics and privacy, the ascent of augmented analytics, and the influence of edge computing on data processing. Organizations are urged to adapt to these evolving dynamics to remain competitive and harness the complete potential of data and AI technologies this year. [Read More](https://tdwi.org/articles/2024/01/05/ta-all-shifting-sands-in-enterprise-data-and-ai-technologies-in-2024.aspx)\n\n## Embracing Digital Transformation\n\n\n\nThis week on the podcast, Darren continues his series on Embracing Generative AI when he- interviews Laura Newey after a semester of teaching English in the university with Chat GPT and Generative AI as a tool. You have to check this episode and share with your college students. [Read More](https://www.embracingdigital.org/en)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW49-en","image":"./briefs/edw-49/en/thumbnail.png","lang":"en","summary":"Intel recently announced the spin-off of AI software company Articul8, backed by DigitalBridge. The move is aimed at improving enterprise AI capabilities. Articul8 will focus on developing advanced AI solutions, leveraging Intel's expertise in the field. This strategic move signifies Intel's commitment to advancing artificial intelligence technologies for broader industry applications."},{"id":217,"type":"News Brief","title":"2023-3-5","tags":null,"body":"\n\n## Artificial Intelligence\n\nGenerative AI based Chatbots are far from perfect. ChatGPT, Microsoft's Bing chatbot, Meta's Galactica (a generative AI designed to help scientists with tasks like annotating proteins or writing code), and other systems have been taken down or reined in after they've been found to generate unreliable or incorrect information or spiraled into emotional-seeming reactions and even threats.\n\nThe US and European governments are collaborating on an AI research study to develop strategies for regulating and fostering AI innovation. The study will cover five areas: extreme weather and climate forecasting, emergency response management, health and medicine improvements, electric grid optimization, and agriculture optimization. Participants include the US Energy Department, the Department of Agriculture, the Pentagon, the National Oceanic and Atmospheric Administration, health and science agencies, and their European counterparts. \n\n[https://www.axios.com/2023/03/01/ai-research-us-eu](https://www.axios.com/2023/03/01/ai-research-us-eu)\n\nAirdot Deploy: Automatically deploy ML models and scale them. Automatically identifies the required packages, understands module dependency, re-factors the code, builds REST APIs around the ML model, packages it in a container, spins up the infra, automatically scales it up and down, and sets up automated alerts and monitoring, all with just one line of code. So much for you DevOps team. \n\n[https://www.airdot.io/blog/announcing-airdot-deploy-ml](https://www.airdot.io/blog/announcing-airdot-deploy-ml)\n\n## Cybersecurity\n\nA report by Tenable reveals that cyberattacks are primarily carried out using known vulnerabilities for which patches are already available. Threat actors continue to exploit vulnerabilities that organizations have failed to patch or remediate, including high-severity flaws in Microsoft Exchange and virtual private network solutions. \n\n[https://www.helpnetsecurity.com/2023/03/03/known-exploitable-vulnerabilities/?web_view=true](https://www.helpnetsecurity.com/2023/03/03/known-exploitable-vulnerabilities/?web_view=true)\n\nMunicipal CISOs face challenges as cyber threats increase, with ransomware attacks being the largest concern. Oakland, California, recently declared a state of emergency following a ransomware attack by the Playgroup that disrupted phone systems and non-emergency services. This attack is one in a series of ransomware attacks on local governments in the US, including those in Baltimore, New Orleans, Pensacola, and Atlanta. \n\n[https://www.csoonline.com/article/3688958/municipal-cisos-grapple-with-challenges-as-cyber-threats-soar.html](https://www.csoonline.com/article/3688958/municipal-cisos-grapple-with-challenges-as-cyber-threats-soar.html)\n\nPresident Biden signed the Quantum Computing Cybersecurity Preparedness Act, requiring federal agencies to prioritize using quantum-resistant technology. The law mandates guidance for assessing critical systems with post-quantum cryptography standards. This is in response to fears of quantum technology making existing encryption vulnerable to being cracked quickly. \n\n[https://fedscoop.com/biden-signs-quantum-computing-cybersecurity-act-into-law/](https://fedscoop.com/biden-signs-quantum-computing-cybersecurity-act-into-law/)\n\n## Ubiquitous Compute\n\nAccenture's annual banking cloud report found that banks targeting core systems for cloud migration face a major risk factor in the difficulty of sourcing cloud talent. Banks perceive core migration as high-risk due to a lack of staff with cloud expertise and the difficulty of recruiting the required skills in the current environment. Public cloud adoption represents both a security upgrade and a risk factor, reflected in the banking industry's lack of cloud adoption. \n\n[https://www.ciodive.com/news/banks-finance-cloud-migration-skils-gap-cybersecurity/644169/](https://www.ciodive.com/news/banks-finance-cloud-migration-skils-gap-cybersecurity/644169/)\n\nCloud-based data warehouse company Snowflake plans to add over 1,000 employees in the current fiscal year, according to CFO Mike Scarpelli during the company's Q4 earnings call. This follows the addition of 1,900 people last year, in contrast to the downsizing trend among other big tech companies. Snowflake plans to prioritize hiring in product, engineering, and sales.  \n\n[https://www.ciodive.com/news/Snowflake-expands-workforce-extends-AWS-partnership/644034/](https://www.ciodive.com/news/Snowflake-expands-workforce-extends-AWS-partnership/644034/)\n\nAccording to industry experts, CIOs must restructure their IT departments and strategies to take advantage of cloud transformation fully. Neil Holden, CIO at Halfords Group, believes IT departments must operate differently because of the cloud and what it means to the business. Holden reorganized his team to ensure they could maximize the capabilities and business opportunities provided by the cloud.   \n\n[https://www.cio.com/article/463595/transforming-it-for-cloud-success.html](https://www.cio.com/article/463595/transforming-it-for-cloud-success.html)\n\n## Embracing Digital Transformation Podcast\n\nCheck out this weeks full length episode \"Innovation as a Service\" where Darren interview Andrew Cohen Managing Director at Netsurit.\n\n[https://www.embracingdigital.org/episode-EDT127](https://www.embracingdigital.org/episode-EDT127)\n\n\n\n","guests":null,"link":"/brief-EDW5-en","image":"./briefs/edw-5/en/thumbnail.png","lang":"en","summary":"Summary"},{"id":218,"type":"News Brief","title":"2024-1-14","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","sandworm","hacking","zyxelsecurity","miraibotnet","sechack","bitcoinhack","cryptojacking","openai","aijobs","airegulation","ainetworks","mitai","aiwarfare","taiwanai","israelai","edgecomputing","iotsecurity","5ginorbit","iridiumnetwork","digitaltransformation","zero5gtrust","dataassurance"],"body":"\n\n## Cybersecurity\n\n\nNew research by Forescout challenges cyber attribution in hacking incidents, making it difficult to identify culprits accurately. The Sandworm hacking group may not have been involved in the cyber attacks targeting 22 Danish energy organizations in May 2023, which exploited a security flaw in the Zyxel firewall and deployed Mirai botnet variants on infected hosts via an unknown initial access vector. The findings highlight the challenges in attributing cyber threats. [Read More](https://thehackernews.com/2024/01/new-findings-challenge-attribution-in.html)\n<hr>\n\n\nThe US Securities and Exchange Commission's (SEC) Twitter account was hacked, with false information posted about Bitcoin exchange-traded funds. The FBI is investigating the hack, and the SEC has confirmed that it resulted from an individual gaining control of the account through a third party. The incident raises concerns over the platform's vulnerability to disinformation. Security analysts express concern over the lack of two-factor authentication, and lawmakers have criticized the SEC for potentially undermining markets. [Read More](https://www.wired.com/story/sec-x-account-hack-investigation/)\n\n<hr>\n\nUkrainian authorities, with the help of Europol and a cloud provider, have arrested a 29-year-old man in Mykolaiv for orchestrating a sophisticated cryptojacking scheme. The suspect infiltrated 1,500 accounts belonging to a well-known US company using custom brute-force tools. He created over a million virtual computers to ensure malware operation, earning over $2m in illicit profits. Cryptojacking exploits compromised credentials on cloud platforms, allowing unauthorized use of computing resources to mine cryptocurrencies. The suspect's arrest took place on January 9th. [Read More](https://thehackernews.com/2024/01/29-year-old-ukrainian-cryptojacking.html)\n\n<hr>\n\n## Artificial Intelligence\n\n\nOpenAI CEO Sam Altman has expressed concern about the rapid penetration of artificial intelligence (AI) in society. He describes it as the fastest technological revolution, highlighting the speed at which society needs to adapt. Altman believes that progress in AI will lead to changes in jobs, but he also thinks it will create new and better opportunities. Altman discusses the need for a global regulatory body to oversee robust AI systems, considering their potential impact on society and geopolitical balance. [Read More](https://www.livemint.com/technology/gadgets/amazon-republic-day-sale-top-5-smartwatches-with-up-to-77-discounts-11705312172154.html)\n<hr>\n\n\nOne of the challenges with artificial intelligence (AI) and neural networks is comprehending how they function. MIT researchers have developed a new AI system that studies and explains neural network behavior to address this issue. This system uses pre-trained language models and enables a better comprehension of neural network computations. Additionally, the researchers have introduced the FIND benchmark to evaluate the accuracy of interpretation techniques. Despite some limitations, the FIND benchmark is a valuable tool for assessing the effectiveness of interpretability procedures. [Read More](https://www.marktechpost.com/2024/01/13/mit-researchers-developed-a-new-method-that-uses-artificial-intelligence-to-automate-the-explanation-of-complex-neural-networks/)\n<hr>\n\n\nTaiwan and Israel are using AI to tackle different challenges. Taiwan employed innovative strategies to counter foreign meddling in their elections, using AI tools to flag misleading content, establishing anti-disinformation initiatives, and monitoring the internet for information manipulation. Meanwhile, Israel's military uses an AI system called \"the Gospel\" to locate targets faster, reducing civilian casualties and identifying Hamas tunnels and missile launchers. Despite concerns about algorithmic errors and the death toll of Palestinian citizens, Israel's use of AI is seen as a potential game-changer in tactical warfare. [Read More](https://lynnwoodtimes.com/2024/01/14/artificial-intelligence-240114/)\n<hr>\n\n## Edge Computing\n\n\nThe US and EU have agreed to a joint roadmap for a consumer labeling program for smart home products and connected devices. The program will feature a cyber trust mark on device packaging to indicate compliance with security standards. The Federal Communications Commission is leading the initiative and aims to finalize the policy and standards by the end of 2024. The program will inform consumers about the cybersecurity of IoT devices, promoting awareness and safety. [Read More](https://www.nextgov.com/cybersecurity/2024/01/eu-signs-iot-safety-label-plan/393328/)\n<hr>\n\nIoT 5G connectivity from space is on the horizon, expanding edge computing beyond traditional brick-and-mortar manufacturing. Iridium Communications has recently launched Project Stardust, a 5G network service leveraging its low-Earth orbit satellite constellation. The service is designed to support 5G services, IoT devices, messaging, emergency services, and asset tracking. Iridium began testing the service in 2023 and plans to launch it commercially in 2026. [Read More](https://www.sdxcentral.com/articles/news/iridiums-project-stardust-satellite-strategy-supports-5g-iot-from-space/2024/01/)\n<hr>\n\n\nIn 2023, IoT saw significant developments such as the EU's NIS2 Cybersecurity Directive, layoffs by major tech players affecting IoT, 5G in space, and sustainability-driven initiatives. Renesas acquired cellular IoT chipmaker Sequans for $249 million, while IoT cloud wars intensified with Google shutting down its IoT Core service. Pragmatic Semiconductor raised $389.3 million, and Samsara became the best-performing IoT stock. India embarked on a national smart meter roll-out, and generative AI and IoT breakthroughs combined to provide guided repair and teach robots with vision systems. Wow! What a year! [Read More](https://iot-analytics.com/iot-2023-in-review/)\n\n<hr>\n\n## Embracing Digital Transformation\n\nThis week, Darren will be hosting his podcast with two interviews. The first interview will focus on Zero Trust security in 5G, and the second will focus on improving Data Assurance in cloud architectures, focusing on Embracing Zero Trust. By tuning in, you can learn about the latest digital transformations. We had a great month last month, with over 60,000 listens to the podcast. Thank you for sharing with your friends. [Read More](https://www.embracingdigital.org/en)\n<hr>\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW50-en","image":"./briefs/edw-50/en/thumbnail.png","lang":"en","summary":"Digital Transformation news for the week of January 15, 2024, including updates on Cybersecurity, artificial intelligence, and edge computing. This week, the SEC hack raises alarms, AI is used to explain AI, and 5G is deployed in space."},{"id":219,"type":"News Brief","title":"2024-1-21","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","tapestorage","filestorage","cloudstorage","sql","nosql","cpra","dataprivacy","cisa","openai","pentagon","ibm","quantumcomputing","nuclearecurity","csuite","deepfake","mcafee","digitaltransformation"],"body":"\n\n## Data Management\n\n\nHammerspace has recently integrated tape storage into its global file system to improve data management capabilities. This integration enables users to efficiently access and manage data across different storage mediums, promoting flexibility in file storage solutions. The current solution offered by Hammerspace establishes a global file system that covers multiple data centers, clouds, and traditional edge boundaries. [Read More](https://www.techtarget.com/searchstorage/news/366566738/Hammerspaces-global-file-system-now-includes-tape)\n\n<hr>\n\n\nHeterogeneous data management is essential for modern businesses. Toad Data Studio supports SQL, NoSQL, and cloud databases, providing comprehensive database management capabilities. Cross-platform compatibility streamlines workflows, query optimization enhances database performance, and collaboration tools enable efficient teamwork. Therefore, Toad Data Studio is an indispensable resource for managing and administering databases across various environments. [Read More](https://www.infoworld.com/article/3712250/toad-data-studio-manages-sql-nosql-and-cloud-databases.html)\n\n<hr>\n\n\nIf you operate a business in California, it is crucial to prepare for the upcoming California Privacy Rights Act (CPRA) by refining your data privacy practices. By enhancing data management, consent mechanisms, and compliance strategies, you can meet the CPRA requirements and stay ahead in protecting consumer privacy. It is recommended that proactive measures be implemented to ensure your business fully complies with the CPRA. [Read More](https://www.jdsupra.com/legalnews/prepare-for-the-cpra-by-improving-your-7537199/)\n\n<hr>\n\n## Cybersecurity\n\n\nCISA has released a comprehensive cybersecurity incident response guide tailored to water utilities. The guide is designed to help water utilities respond effectively to cybersecurity incidents and mitigate potential cyber threats. It provides valuable insights and serves as a resource for water utilities to bolster their incident response capabilities. This move is a significant step in the fight against cyber attacks on critical infrastructure in the water sector. [Read More](https://www.techtarget.com/searchsecurity/news/366566740/CISA-posts-incident-response-guide-for-water-utilities)\n\n<hr>\n\n\nOpenAI has partnered with the Pentagon on a cybersecurity initiative, representing a shift from its previous stance. By leveraging OpenAI's expertise, the partnership aims to advance cybersecurity capabilities for defense. This development highlights the evolving role of AI in national security efforts, with OpenAI actively contributing to cybersecurity projects in collaboration with the Pentagon. [Read More](https://www.semafor.com/article/01/16/2024/openai-is-working-with-the-pentagon-on-cybersecurity-projects)\n\n<hr>\n\n\n\nIBM has warned that the emergence of quantum computing could result in a severe threat to cybersecurity. The concern is that quantum computers could break widely used encryption methods, necessitating the development of quantum-resistant security measures. Organizations are being urged to take proactive steps to prepare for the future challenges posed by quantum computing in cybersecurity. [Read More](https://www.bloomberg.com/news/articles/2024-01-17/quantum-computing-to-spark-cybersecurity-armageddon-ibm-says)\n\n<hr>\n\n## Artificial Intelligence\n\n\nIntegrating artificial intelligence (AI) into decision-making processes within atomic security raises thought-provoking questions about how AI technologies might influence stability, trust dynamics, and the overall strategic balance in the context of nuclear capabilities. It is essential to ensure responsible AI implementation to avoid unintended consequences that might threaten global security and stability. The critical need for reliable AI integration emphasizes the importance of providing a secure and stable international environment in the context of nuclear capabilities. [Read More](https://warontherocks.com/2024/01/artificial-intelligence-and-nuclear-stability/)\n\n<hr>\n\n\nArtificial intelligence (AI) is on track to disrupt the C-suite in three significant ways. Executives can expect changes in decision-making processes, workforce dynamics, and the overall role of leaders. As AI continues evolving, its transformative impact reshapes traditional leadership paradigms. Executives are encouraged to adapt to these changes and strategically leverage AI for a competitive advantage in the dynamic business environment. [Read More](https://www.cio.com/article/1293438/3-ways-ai-is-set-to-disrupt-the-c-suite.html)\n\n<hr>\n\n\nMcAfee has launched an AI-powered deepfake audio detection system to address the growing threat of manipulated audio content. The technology is designed to identify and mitigate risks associated with deepfake audio, emphasizing the need for advanced detection and prevention solutions. This reflects the increasing importance of AI in countering emerging threats in manipulated media and enhancing cybersecurity measures. [Read More](https://www.artificialintelligence-news.com/2024/01/08/mcafee-unveils-ai-powered-deepfake-audio-detection/)\n\n<hr>\n\n## Embracing Digital Transformation\n\n\n\nDarren's latest episode on the Embracing Digital Transformation podcast delves into the R&D manufacturing data management world. He looks at the intricacies of collecting and analyzing data, from innovative pure research to the more predictable manufacturing process. It's an insightful exploration of the challenges and opportunities of digital transformation in this field. [Read More](https://www.embracingdigital.org)\n\n<hr>\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW51-en","image":"./briefs/edw-51/en/thumbnail.png","lang":"en","summary":"Get ready for this week&#39;s Digital Transformation News for January 22, 2024, which features stories on data management, cybersecurity, and artificial intelligence. This week, we have news on data privacy rights in California, tips on protecting your organization from cyber-attacks related to AI and quantum computing, and a look at how AI is being used to assist the government in making wartime decisions."},{"id":220,"type":"News Brief","title":"2023-3-12","tags":null,"body":"\n\n## Artificial Intelligence\n\nMicrosoft confirms new version of its popular AI chatbot will be able to turn text into VIDEO. GPT-4 and future developments will forever change how we think about artificial intelligence. This powerful AI can process text and numbers, videos, images, and more. Microsoft explained that GPT-4 would be “multimodal.” Holger Kenn, Director of Business Strategy at Microsoft Germany, explained that this would allow the company’s AI to translate a user’s text into images, music, and video.\n\nAI to replace actors? Imagine using AI to create a film without hiring real actors or writing a fully illustrated 200-page book in just one day. Yes, you read that right, GPT-4 makes it possible to use artificial intelligence for almost anything you can imagine.\n\nSchrodinger: A ChatGPT-Integrated Fully Interactive Multi-Agent Financial Market Simulator Citadel, one of the world's most successful hedge funds, is in talks to secure an enterprise-wide ChatGPT license.  ChatGPT can produce correct code with a (simple) trade execution strategy. \n\n[https://www.youtube.com/watch?v=tvzO79V9uq4](https://www.youtube.com/watch?v=tvzO79V9uq4)\n\n## Data Management\n\n“Zero” is the new word of the day, with the “Data Alliance” coining the word Zero-Copy for new distributed data architectures. Zero-copy integration is a concept that enables data to be shared between different systems without copying it. The traditional approach of copying data from one system to another can lead to inefficiencies, data consistency issues, and security vulnerabilities. \n\n[http://tdan.com/the-data-centric-revolution-zero-copy-integration/30462](http://tdan.com/the-data-centric-revolution-zero-copy-integration/30462)\n\nThe COVID-19 pandemic accelerated the vision of data everywhere, in the cloud, the data center, laptops, and IoT devices. This data sprawl has caused IT organizations to re-evaluate their data strategies. New distributed data management strategies and solutions need to be developed that are more comprehensive in managing heterogenous data types, including structured, semi-structured, and unstructured data.  \n\n[https://www.engineeringnews.co.za/article/modern-data-management-platforms-are-vital-for-solving-modern-data-management-problems-2023-03-14/rep_id:4136](https://www.engineeringnews.co.za/article/modern-data-management-platforms-are-vital-for-solving-modern-data-management-problems-2023-03-14/rep_id:4136)\n\nThe complexity of data governance is driving organizational changes, including the emergence of the data steward role in organizations. Data stewards are beginning to manage data access, privacy concerns, and data lifecycle management. Placing data stewards in the organization is still up for a healthy debate as organizations mature their data-centric organizations. \n\n[https://www.techtarget.com/searchdatamanagement/tip/Data-stewardship-Essential-to-data-governance-strategies](https://www.techtarget.com/searchdatamanagement/tip/Data-stewardship-Essential-to-data-governance-strategies)\n\n## Cybersecurity\n\nAn open-source adversary-in-the-middle (AiTM) phishing kit has found several takers in the cybercrime world for its ability to orchestrate attacks at scale. Microsoft Threat Intelligence tracks the threat actor behind the kit’s development under its emerging moniker DEV-1101. An AiTM phishing attack typically involves a threat actor attempting to steal and intercept a target's password and session cookies by deploying a proxy server between the user and the website.   \n\n[https://thehackernews.com/2023/03/microsoft-warns-of-large-scale-use-of.html](https://thehackernews.com/2023/03/microsoft-warns-of-large-scale-use-of.html)\n\nThe Cybersecurity and Infrastructure Security Agency (CISA) has warned organizations that operate critical infrastructure about ransomware vulnerabilities in their devices. The advisory includes a list of devices that attackers may target and recommends that organizations assess their networks for possible risks. CISA has a newly established Ransomware Vulnerability Warning Pilot (RVWP) program with two goals: to scan critical infrastructure entities' networks and help vulnerable organizations fix the flaws before they get hacked. \n\n[https://www.bleepingcomputer.com/news/security/cisa-now-warns-critical-infrastructure-of-ransomware-vulnerable-devices/](https://www.bleepingcomputer.com/news/security/cisa-now-warns-critical-infrastructure-of-ransomware-vulnerable-devices/)\n\nA report by Drata highlights key compliance trends for 2023. The report outlines the shift towards continuous compliance monitoring and the need for automation tools to achieve compliance. It also highlights the increasing focus on privacy regulations and the growing importance of vendor risk management. The report recommends that companies prioritize their compliance efforts, utilize automation tools, and stay current with evolving regulatory requirements to avoid potential fines and reputational damage. \n\n[https://drata.com/resources/2023-compliance-trends](https://drata.com/resources/2023-compliance-trends)\n\n## Embracing Digital Transformation Podcast\n\nCheck out this weeks full length episode \"Closing the Digital Skills Gap\" where Darren interviews Jon Gottfried from Major League Hacking.\n\n[https://www.embracingdigital.org/episode-EDT128](https://www.embracingdigital.org/episode-EDT128)\n\n\n\n","guests":null,"link":"/brief-EDW6-en","image":"./briefs/edw-6/en/thumbnail.png","lang":"en","summary":"Summary"},{"id":221,"type":"News Brief","title":"2023-3-19","tags":["ai","compute","datamanagement"],"body":"\n\n## Artificial Intelligence\n\nIn the “Potential for Risky Emergent Behaviors” section of the company’s technical report, OpenAI partnered with the Alignment Research Center to test GPT-4's skills. The Center used the AI to convince a human to send the solution to a CAPTCHA code via text message—and it worked.\n\n[https://gizmodo.com/gpt4-open-ai-chatbot-task-rabbit-chatgpt-1850227471](https://gizmodo.com/gpt4-open-ai-chatbot-task-rabbit-chatgpt-1850227471)\n\nCopilot features, Microsoft executives brought up the software’s tendency to produce inaccurate responses but pitched that as something that could be useful. If people realize that Copilot’s responses could be sloppy with the facts, they can edit the inaccuracies and more quickly send their emails or finish their presentation slides.\n\n[https://www.cnbc.com/2023/03/16/microsoft-justifies-ais-usefully-wrong-answers.html?__source=sharebar|linkedin&par=sharebar](https://www.cnbc.com/2023/03/16/microsoft-justifies-ais-usefully-wrong-answers.html?__source=sharebar|linkedin&par=sharebar)\n\nGoogle Health announced Med-PaLM 2, a new groundbreaking version of its large medical language model. Med-PaLM 2 consistently performs at an expert level on medical exam questions and reaches an 85% accuracy, scoring 18% higher than its predecessor.\n\n## Ubiquitous Compute\n\nYou may soon be swimming on top of a data center 😊 A British tech firm, Deep Green, plans to set up small data centers at public pools across the UK. An innovative solution for data center cooling and energy efficiency, this approach uses the excess heat generated by servers installed beneath a public pool to warm the water via a heat exchanger.\n\nThe US government has launched a new initiative to help organizations detect security flaws in Microsoft's cloud services. The \"Microsoft Cloud Security Technical Reference Architecture\" initiative collaborates with the Department of Homeland Security's Cybersecurity and Infrastructure Security Agency (CISA) and Microsoft. The program aims to provide organizations with a comprehensive set of guidelines, tools, and best practices to improve the security of their Microsoft cloud services. The initiative is part of a more significant effort by the US government to enhance critical systems and infrastructure cybersecurity.\n\n[https://www.techradar.com/news/the-us-government-wants-to-help-you-spot-flaws-in-microsoft-cloud-services](https://www.techradar.com/news/the-us-government-wants-to-help-you-spot-flaws-in-microsoft-cloud-services)\n\nAccording to a recent survey, 83% of Chief Information Officers (CIOs) feel they must do more with less in 2023. The survey, which polled 500 CIOs in the US and the UK, found that budget constraints are the main factor driving this sentiment, with 74% of respondents citing it as a significant challenge. Additionally, CIOs expressed concerns about the difficulty of finding qualified personnel (55% of respondents) and the need to meet constantly changing business requirements (51%). The survey also found that 87% of CIOs plan to accelerate their adoption of cloud computing this year to help meet their business needs.\n\n[https://www.cloudcomputing-news.net/news/2023/mar/13/83-of-cios-must-do-more-with-less-in-2023/](https://www.cloudcomputing-news.net/news/2023/mar/13/83-of-cios-must-do-more-with-less-in-2023/)\n\n## Data Management\n\nA survey by TDWI found that organizations are adopting a data management strategy to improve data quality and reduce silos. 71% of respondents have implemented or planned to implement a data management strategy, but challenges such as data governance and lack of skilled personnel still need to be solved. Organizations are investing in cloud-based data management solutions, master data management, and data integration tools.\n\n[https://tdwi.org/articles/2023/03/23/diq-all-data-management-0323.aspx](https://tdwi.org/articles/2023/03/23/diq-all-data-management-0323.aspx)\n\nArtificial intelligence (AI) has the potential to help solve the problem of IT data overload. IT departments need help managing the sheer volume of data. AI can benefit by analyzing and identifying patterns in large data sets, providing insights that would be difficult to uncover manually. However, AI is not a silver bullet, and organizations must ensure that their data is accurate and high-quality to achieve the full benefits of AI.\n\n[https://www.infoworld.com/article/3689668/can-ai-solve-it-s-eternal-data-problem.html](https://www.infoworld.com/article/3689668/can-ai-solve-it-s-eternal-data-problem.html)\n\nA new report reveals the importance of data management to enterprises. The report highlights how effective data management can help organizations improve decision-making, reduce costs, and comply with regulations. The report recommends that organizations establish a data management strategy, prioritize data quality, and ensure they have the right tools and technologies. The report also stresses the importance of data governance and the need for organizations to foster a culture that values data as a strategic asset.\n\n[https://solutionsreview.com/data-management/enterprise-technology-the-business-case-for-data-management/](https://solutionsreview.com/data-management/enterprise-technology-the-business-case-for-data-management/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW7-en","image":"./briefs/edw-7/en/thumbnail.png","lang":"en","summary":"Summary"},{"id":222,"type":"News Brief","title":"2023-3-26","tags":["ai","compute","cybersecurity"],"body":"\n\n## Ubiquitous Compute\n\nGordon Moore, the co-founder of Intel Corporation and inventor of Moore's Law, passed away on March 24, 2023, at 92. Born in 1929, Moore co-founded Intel in 1968 and served as its CEO from 1975 to 1987. He is best known for his prediction, Moore's Law, that the number of transistors on a microchip would double roughly every two years, leading to exponential growth in computing power. This prediction has held for over 50 years and has played a vital role in driving the advancement of the technology industry.\n\nThe 4004, Intel's first microprocessor, debuted in 1971 as the first commercially available microprocessor. Initially designed for calculators, it quickly found applications in other areas, such as traffic light controllers and electronic cash registers. The 4004 contained 2,300 transistors on a single chip and helped establish Intel as a major player in the semiconductor industry. Today, microprocessors are used in various devices, from smartphones to supercomputers.\n\nIntel has announced the availability of its latest Xeon Max Scalable processor series, based on its Sapphire Rapids architecture. The Xeon Max series features 100 Billion, 10nm SuperFin transistors, AI and encryption acceleration, and PCIe 5.0 support, providing enhanced performance and security for enterprise computing workloads.  This accounts for 40 Million times more transistors in the latest processor than their first processor over 50 years ago.\n\n[https://www.datacenterdynamics.com/en/news/intel-announces-xeon-max-sapphire-rapids-cpus/](https://www.datacenterdynamics.com/en/news/intel-announces-xeon-max-sapphire-rapids-cpus/)\n\n## Artificial Intelligence\n\nChatGPT, a large language model trained by OpenAI, experienced a technical issue that caused it to go down for several hours. The problem, which was caused by a bug in the system, affected the history function of the model, making it unable to access its previous conversations. OpenAI has since resolved the issue, and ChatGPT is now fully operational. The incident serves as a reminder of the potential risks of relying on AI and the importance of having robust systems to address technical issues.\n\n[https://www.independent.co.uk/tech/chatgpt-down-bug-issue-history-b2306269.html](https://www.independent.co.uk/tech/chatgpt-down-bug-issue-history-b2306269.html)\n\nBard, an artificial intelligence language model developed by Google as a competitor to OpenAI's GPT, is now available for public use. Bard, which uses a different approach to language modeling than GPT, aims to generate more creative and diverse text by allowing users to input their prompts and constraints. The release of Bard for public use is expected to accelerate the development of new natural language processing applications and advance the field of AI language modeling.\n\n[https://www.bloomberg.com/news/articles/2023-03-21/google-chatgpt-rival-bard-now-open-to-public-use](https://www.bloomberg.com/news/articles/2023-03-21/google-chatgpt-rival-bard-now-open-to-public-use)\n\nA new farming method, Synecoculture, involves planting multiple plant species together in high density. However, it can be a complex process due to varying growth speeds and seasons. Researchers have developed a robot to sow, prune, and harvest plants in dense vegetation growth to address this issue. The robot's small and flexible body will be helpful in large-scale Synecoculture, making it an essential step toward achieving sustainable farming and carbon neutrality.\n\n[https://www.sciencedaily.com/releases/2023/03/230320102001.htm](https://www.sciencedaily.com/releases/2023/03/230320102001.htm)\n\n## Cybersecurity\n\nThe US Cybersecurity and Infrastructure Security Agency (CISA) has released a new tool to detect malicious activity in Microsoft cloud services. The Sparrow tool can scan Azure and Microsoft 365 environments for signs of hacking and other unauthorized activities. Sparrow uses open-source data and artificial intelligence algorithms to identify potential threats and provides users with actionable recommendations to prevent further damage. The release of Sparrow is part of CISA's ongoing efforts to enhance the security of cloud-based systems and protect against cyber attacks.\n\n[https://www.bleepingcomputer.com/news/security/new-cisa-tool-detects-hacking-activity-in-microsoft-cloud-services/](https://www.bleepingcomputer.com/news/security/new-cisa-tool-detects-hacking-activity-in-microsoft-cloud-services/)\n\nTesla's electric vehicles were hacked twice at the annual Pwn2Own exploit contest held by the Zero Day Initiative. A team of researchers was able to exploit a vulnerability in the infotainment system of a Model 3 to take control of the vehicle's headlights, speakers, and other methods. Another team used a bug in the same approach to execute code and gain access to the car's data. Tesla has since released an over-the-air patch to fix the vulnerabilities.\n\n[https://www.securityweek.com/tesla-hacked-twice-at-pwn2own-exploit-contest/](https://www.securityweek.com/tesla-hacked-twice-at-pwn2own-exploit-contest/)\n\nIntel has announced its new 13th-gen Core vPro platform, which features hardware-based security features designed to reduce the platform's attack surface. The new platform includes Intel Hardware Shield, which uses CPU-level threat detection to provide a secure boot process, and Intel Control-Flow Enforcement Technology, which helps prevent return-oriented programming (ROP) attacks. The platform also includes Intel's Threat Detection Technology, which uses machine learning to identify potential security threats. The 13th-gen Core vPro platform is aimed at business customers looking for advanced security features to protect against increasingly sophisticated cyber attacks.\n\n[https://www.securityweek.com/intel-boasts-attack-surface-reduction-with-new-13th-gen-core-vpro-platform/](https://www.securityweek.com/intel-boasts-attack-surface-reduction-with-new-13th-gen-core-vpro-platform/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW8-en","image":"./briefs/edw-8/en/thumbnail.png","lang":"en","summary":"Summary"},{"id":223,"type":"News Brief","title":"2023-4-2","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Cybersecurity\n\nNew MacStealer macOS Malware Steals iCloud Keychain Data and Passwords. A new information-stealing malware has set its sights on Apple's macOS operating system to siphon sensitive information from compromised devices. MacStealer is the latest example of a threat that uses Telegram as a command-and-control (C2) platform to exfiltrate data.\n\nMicrosoft Issues Patch for aCropalypse Privacy Flaw in Windows Screenshot Tools. Microsoft released an out-of-band update to address a privacy-defeating flaw in its screenshot editing tool for Windows 10 and 11. The issue, dubbed aCropalypse, could enable malicious actors to recover edited portions of screenshots, potentially revealing sensitive information that may have been cropped out.\n\nOpenAI Reveals Redis Bug Behind ChatGPT User Data Exposure Incident. OpenAI disclosed that a bug in the Redis open-source library was responsible for exposing other users' personal information and chat titles in the upstart's ChatGPT service earlier this week. The glitch, which came to light on March 20, 2023, enabled certain users to view brief descriptions of other users' conversations from the chat history sidebar, prompting the company.\n\n## Artificial Intelligence\n\nThis is BIG: Elon Musk, Bill Gates, and other tech leaders call for a pause in the ‘out of control’ AI race. 1,100+ notable signatories just signed an open letter asking “all AI labs to pause for at least 6 months immediately”. The letter, which was also signed by the CEO of OpenAI, said the pause should apply to AI systems “more powerful than GPT-4.” It also said independent experts should use the proposed pause to jointly develop and implement a set of shared protocols for AI tools that are safe “beyond a reasonable doubt.”\n\n[https://techcrunch.com/2023/03/28/1100-notable-signatories-just-signed-an-open-letter-asking-all-ai-labs-to-immediately-pause-for-at-least-6-months/](https://techcrunch.com/2023/03/28/1100-notable-signatories-just-signed-an-open-letter-asking-all-ai-labs-to-immediately-pause-for-at-least-6-months/)\n\nCerebras Systems Releases Seven New GPT Models Trained on CS-2 Wafer-Scale Systems. First time a company has used non-GPU-based AI systems to train LLMs up to 13 billion parameters and is sharing the models, weights, and training recipe via the industry standard Apache 2.0 license. \tA series of seven GPT models with 111M to 13B parameters. Typically a multi-month undertaking, this work was completed in a few weeks. All seven Cerebras-GPT models are immediately available on Hugging Face and Cerebras Model Zoo on GitHub.\n\n[https://www.marketwatch.com/press-release/cerebras-systems-releases-seven-new-gpt-models-trained-on-cs-2-wafer-scale-systems-2023-03-28](https://www.marketwatch.com/press-release/cerebras-systems-releases-seven-new-gpt-models-trained-on-cs-2-wafer-scale-systems-2023-03-28)\n\nGoogle Bard with Wordle – it could have gone better. After correcting the bot and reminding it only to guess five-letter words, it apologized and returned with SLANTS, then carried on with different variations of the word SLANT ☹\n\n[https://www.techradar.com/news/i-tried-to-use-google-bard-to-help-me-with-wordle-but-it-didnt-go-well](https://www.techradar.com/news/i-tried-to-use-google-bard-to-help-me-with-wordle-but-it-didnt-go-well)\n\n## Intelligent Edge\n\nBT has partnered with Amazon Web Services (AWS) to test 5G edge computing in Manchester using AWS Wavelength. The trial will investigate how edge computing can enhance the performance of BT's 5G network in three key areas: augmented reality, immersive gaming and industrial automation. The aim is to improve customer experience by providing faster and more reliable services. The trial is part of a larger collaboration between the two companies to explore edge computing's potential in the UK.\n\n[https://www.edgecomputing-news.com/2023/03/29/bt-uses-aws-wavelength-for-5g-edge-trial-in-manchester/](https://www.edgecomputing-news.com/2023/03/29/bt-uses-aws-wavelength-for-5g-edge-trial-in-manchester/)\n\nComcast has announced expanded partnerships aimed at improving connectivity in smart buildings and environments. The partnerships include agreements with advanced metering infrastructure provider Sensus and BuildingIQ, an artificial intelligence-based energy management solutions provider. The goal is to provide smarter energy management, better water management, and improved building automation. These partnerships are part of Comcast's efforts to improve connectivity for commercial customers through its network of Wi-Fi hotspots, sensors, and other IoT devices.\n\n[https://www.edgeir.com/comcast-focuses-connectivity-on-smarter-buildings-environments-with-expanded-partnerships-20230331](https://www.edgeir.com/comcast-focuses-connectivity-on-smarter-buildings-environments-with-expanded-partnerships-20230331)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW9-en","image":"./briefs/edw-9/en/thumbnail.png","lang":"en","summary":"Summary"}]