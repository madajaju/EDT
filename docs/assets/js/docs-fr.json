[{"id":0,"type":"Episode","title":"Histoire de l'architecture centrée sur les données","tags":["dataarchitecture","softwaredeveloper","microservice","container","virtualization","technology","compute","data"],"body":"\r\n\r\nDans cet épisode, Darren parle de l'histoire des applications et des récents changements, principalement dus à l'assaut.\n\nLa collecte de données provenant de l'Internet des objets affecte les architectures axées sur les données. L'infrastructure est prête, mais nous ne\n\nNous n'avons pas encore trouvé un moyen adapté de gérer toutes nos données. Trois éléments doivent être modifiés pour faciliter ce processus :\n\nis stored and processed.\n\net sont répartis sur des milliers d'appareils aux extrémités et à travers des nuages publics et privés.\n\n## Stacks matériels-logiciels conçus spécifiquement\n\nLa façon dont nous déployons aujourd'hui des applications pour les missions n'a pas changé de manière significative depuis trente ans. Une architecture de référence.\n\nTranslate the following to French: cela a une application et une pile d'applications construite sur un matériel spécifique, avec un calcul et un stockage connectés à un.\n\nRéseau. Ce modèle a bien fonctionné pendant longtemps, et en fait, environ un quart des applications sont toujours déployées.\n\nmatériel spécifiquement conçu, mais il n'est pas optimal aujourd'hui. La technologie évolue trop rapidement pour ce modèle; des décalages se produisent.\n\nd'autres systèmes\n\n\nd'autres applications.\n\n## Architectures de virtualisation\n\nEnviron 20 à 25 ans auparavant, la virtualisation matérielle a commencé à résoudre certains de ces problèmes en permettant le déploiement de.\n\napplications for five separate machines, users now only needed to purchase one set of applications for their single machine.\n\nmachines plus petites, une pièce de matériel plus grande pourrait être utilisée, non seulement pour les calculs, mais aussi pour le stockage virtuel et le réseau.\n\nfonctions tout autant, conduisant à une meilleure rentabilité. Comme pour tout développement, cela a soulevé de nouvelles problématiques.\n\nPréoccupations croissantes en matière de sécurité et \"voisins bruyants\", ce qui signifie qu'une application perturbe les performances d'une autre.\n\nà cause de l'utilisation de la bande passante d'E/S, du réseau, du stockage, etc.\n\n## Architectures cloud\n\nÀ partir du début des années 2000, la technologie du cloud a décollé. Nous pouvions désormais partager entre plusieurs organisations. Où la virtualisation\n\nCréation de l'abstraction du matériel, la technologie de cloud a créé l'abstraction des opérations, facilitant ainsi la gestion de multiples.\n\ndistribute machines quickly at scale.\n\nd'utilisation des ressources de calcul, de stockage et de réseau. D'autres avantages incluent une diminution des coûts d'investissement (CapEx) et de fonctionnement (OpEx), grâce à une utilisation moindre.\n\nsolde prennent jusqu'à 75 % de réduction.\n\ngouvernement lors du recensement. Avec l'avancement de cette technologie, les problèmes de sécurité et de voisins bruyants.\n\nprivate clouds.\n\ntraduction :\n\nLes clouds privés. Même avec ces inquiétudes, cependant, les avantages surpassent largement les inconvénients dans la plupart des cas.\n\n## Architectures de Service et de Conteneur\n\nLa technologie de la conteneurisation a été réinventée au cours des cinq à six dernières années. Docker a créé une\n\nparticular, will benefit from this smoother experience as it allows for faster development and deployment of applications.\n\n45. Particulier, a adopté cette technologie car elle est cohérente sur plusieurs environnements. La couche de gestion des services.\n\navec la conteneurisation des applications et des microservices, la priorité est donnée aux applications et elles sont associées à\n\nmatériel générique virtualisé qui a été abstrait. Nous avons maintenant un déploiement automatique sur plusieurs clouds.\n\nNous avons optimisé OpEx et également CapEx sur la pile d'applications et la couche de service. La tolérance aux fautes est automatisée et\n\nthe following to French: il est beaucoup plus facile de s'intégrer avec les réseaux superposés, s'intégrer à travers plusieurs clouds, créer des pare-feu, faire...\n\nmicro-segmentation etc… tout via un logiciel.\n\nLa sécurité, cependant, est une préoccupation principale. Étant donné que les conteneurs sont faciles à déployer dans plusieurs environnements, il est\n\nil est important de se concentrer sur la sécurité \"intégrée\" au déploiement. De plus, il y a une augmentation de la complexité.\n\nnous sommes passés d'une architecture à trois niveaux à une architecture à plusieurs niveaux, voire même à une architecture de microservices avec des dizaines de.\n\nLes services travaillent ensemble. Un autre problème est l'endroit et la manière dont les données sont stockées et gérées. Concernant la gestion des services\n\nTranslate the following to French: \"layer, le stockage est un conteneur générique, qui ne gère pas les données elle-même.\"\n\n## Architectures de l'Internet des objets\n\nNow, when Internet of Things (IoT) is added to this ecosystem, the increased volume of data is spread across hundreds or thousands of connected devices, further fueling the need for advanced data storage and processing capabilities.\n\ndevices have limited processing power and memory, which can make them vulnerable to cyber attacks. Therefore, it is important to implement robust security measures to protect these devices and the data they collect.\n\nLes dispositifs sont accessibles au public. Par exemple, quelqu'un pourrait manipuler une lampe intelligente de la ville, un signal de circulation intelligent.\n\nTranslate the following to French: un drone, ou une caméra de sécurité. La complexité des différents appareils, leur nombre et leurs emplacements, ainsi que l'immensité.\n\nLa quantité de données est énorme.\n\n## Architectures de gestion des données et des informations\n\nComment pouvons-nous faire face à ces problèmes ? Les organisations sont déjà en train de s'adapter pour gérer cette complexité avec de nouvelles organisations.\n\net des postes trompeurs sur les cas d'utilisation de la gestion des données. Auparavant, il n'y avait aucun endroit pour gérer ces cas d'utilisation, donc\n\nles différents niveaux de gestion de l'information distribuée.\n\ncan provide seamless integration and scalability.\n\npeuvent allouer dynamiquement des services et des applications près des données, ou vice versa. Les réglementations et la taille importante des données.\n\npeut limiter la capacité de déplacer des données vers des emplacements centraux, comme nous l'avons traditionnellement fait. Avec cette nouvelle architecture,\n\nPlusieurs modes de fonctionnement peuvent être utilisés, y compris l'analyse désagrégée, le déplacement des données et le déplacement des applications.\n\nUne fois de plus, avec cette architecture étendue, la sécurité est d'une importance capitale. La sécurité doit être traitée comme un aspect commun.\n\nà travers toutes les couches. Sécurité de l'identité, c'est-à-dire l'accès, l'autorisation et l'authentification des individus, de l'IoT.\n\nLes appareils, les applications, les services, et même les données sont essentiels. La gestion de l'identité inclut le chiffrement des données de confiance.\n\net appareils.\n\n## Conclusion-->Conclusion\n\ntranslate: Toute cette architecture ensemble est appelée l'architecture Edgemere. Beaucoup des éléments existent déjà ; il nous faut\n\nhave advanced technologies that can help organizations optimize how they work together. DIML is a recently developed system that allows for distributed information management. This technology enables seamless collaboration and communication between different departments and teams within an organization. Fortunately, we have made significant progress in developing DIML, and it is now ready to be implemented in various industries. DIML optimizes how information is shared, accessed, and stored, ultimately enhancing productivity and efficiency.\n\ncommencent à voir des start-ups et des entreprises plus établies développer les cas d'utilisation et les éléments architecturaux.\n\ndans cette couche\n\nwork together to provide a seamless user experience.\n\ns'emboîter\n\nIntel s'intègre dans cet écosystème en fournissant l'élément clé d'une couche physique commune pour contrôler et gérer l'ensemble de vos.\n\nutiliser toutes vos ressources, que ce soit un appareil IOT, dans le centre de données ou dans un endroit distant. Nous rendons cela possible pour vous.\n\nDéplacer efficacement les données, les stocker de manière efficace et tout traiter. Que ce soit avec les processeurs Xeon hautement performants.\n\nQue ce soit pour l'informatique de pointe, l'inférence ou l'IA à basse puissance, Intel dispose d'une gamme complète de matériel physique.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT1-fr","image":"./episodes/edt-1/fr/thumbnail.bmp","lang":"fr","summary":"Dans cet épisode, Darren parle de l'histoire des applications et de comment les changements récents, principalement dus à l'afflux de données provenant de l'Internet des objets, affectent les architectures centrées sur les données. L'infrastructure est prête, mais nous n'avons pas encore de moyen adapté pour gérer toutes nos données. Trois éléments doivent changer pour faciliter ce processus : les personnes (organisation), les processus (opérations) et l'architecture (technologie). Darren se concentre sur l'architecture, où les données et le traitement sont répartis sur des milliers de périphériques et à travers des clouds publics et privés."},{"id":1,"type":"Episode","title":"Taille de votre solution VDI pour le télétravail","tags":["telework","remoteworker","process","technology","vdi"],"body":"\r\n\r\nTitre\n\n*Slogan*\n\nRésumé ici\n\nTranslate the following sentence to French: \"![episode image](./thumbnail.png)\"\n\n\"![image d'épisode](./miniature.png)\"\n\nÉpisode Corps ici.\n\n## Les médias\n\n<video src='url'></video>  could be translated to French as:\n<video src='url'></video>\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Illyse Sheaffer"],"link":"/episode-EDT10-fr","image":"./episodes/edt-10/fr/thumbnail.png","lang":"fr","summary":"Les organisations ont besoin d'aide pour construire des solutions de VDI (Infrastructure de bureau virtuel) immédiatement. Comme les services informatiques ajoutent des licences VDI localement à leurs systèmes actuels, ils doivent être conscients que les licences seules ne résolvent pas tous leurs problèmes."},{"id":2,"type":"Episode","title":"Superhéros du secteur public d'Intel","tags":["cybersecurity","multicloud","edge","aiml"],"body":"\r\n\r\nJe\n\nCameron a commencé sa carrière en travaillant pour la plus grande entreprise de dotation en personnel privée. Là-bas, il a développé une passion pour la technologie après avoir appris PeopleSoft. Cela a conduit à son opportunité d'aider à construire la première société d'informatique en nuage au monde, UC Center Networking. Lui et ses collègues ont créé la devise \"logiciel en tant que service\" en 1997, sept ou huit ans avant que le terme \"nuage\" ne soit inventé. Il a travaillé pour des intégrateurs de systèmes fédéraux tels que Northrop Grumman et General Dynamics, où il a découvert sa passion pour le soutien d'une mission. Cette passion le motive depuis qu'il est CTO chez Dell Technologies et dirige maintenant l'équipe du secteur public chez Intel.\n\nL'un des facteurs qui a conduit Cameron à rejoindre Intel était le retour de Pat Gelsinger et de l'équipe de direction qu'il constitue, y compris l'arrivée de Greg Lavender. Cameron croit qu'Intel sera revitalisé et continuera à innover sous la direction de Gelsinger. Il souhaite faire partie des efforts visant à ramener Intel à son statut de marque américaine emblématique. Il croit en une culture d'humilité et des personnes qui font ce qui est juste.\n\n## Calcul omniprésent\n\nLes semi-conducteurs sont présents dans tout : les voitures, les appareils électroménagers, les smartphones, les ordinateurs, les systèmes de santé, etc. Ils couvrent presque tous les secteurs verticaux de la planète. Ils améliorent la qualité de vie des personnes, voire sauvent des vies, grâce aux soins de santé, à la sécurité nationale et à la recherche scientifique.\n\nLa pandémie de COVID a mis en lumière cette nature omniprésente en raison du grand nombre de personnes pouvant travailler à domicile et de l'éducation se poursuivant en ligne. Elle a également contribué à combler le fossé numérique de plusieurs façons.\n\n## Connectivité omniprésente\n\nLe silicium ne peut pas réaliser son potentiel sans être connecté. Intel n'est pas seulement une société de puces, mais aussi celle possédant un portefeuille complet d'interconnexion.\n\nIl y a des moments où les choses ne sont pas connectées, par exemple, un combattant de guerre dans un environnement DDIL, mais ils peuvent toujours utiliser l'ordinateur localement pour exécuter la mission. Lorsqu'ils se reconnectent, que ce soit par la 4G existante, la 5G ou la future 6G, ils peuvent obtenir des informations à jour, en d'autres termes, la transmission de données. Dans un scénario éducatif, cependant, l'interconnexion est essentielle pour des choses comme le streaming, le contenu vidéo et l'accès aux données. C'est un atout puissant.\n\nLa pandémie a également provoqué une augmentation significative des communications et de la 5G car les étudiants et les travailleurs avaient besoin de cette connectivité. Cela continue aujourd'hui, même si les étudiants et les travailleurs sont de retour sur place. Alors que les étudiants avaient peut-être un accès périodique aux Chromebooks avant la pandémie, beaucoup en ont maintenant un qu'ils peuvent emporter chez eux. Malheureusement, il y a encore des zones dans le pays et dans le monde où il y a des disparités et où les gens ne peuvent pas participer à l'économie numérique. Étonnamment, combler la fracture numérique aux États-Unis n'est pas aussi facile que dans les pays en développement. Ils peuvent devancer les États-Unis car ils investissent dans la 5G et la 6G: les communications non-terrestres.\n\nCameron croit que la connectivité est tout aussi importante que l'informatique car être connecté aux autres est l'un des fondements de l'expérience humaine.\n\n## Bord vers le Cloud\n\nLa base de l'edge-to-cloud réside dans le calcul et l'interconnexion, donc Intel joue un rôle important dans cet espace, tout comme ses partenaires. Edge to cloud exploite le véritable pouvoir non seulement du silicium, mais également du logiciel. Il crée une interopérabilité. Vous pouvez déplacer de manière sécurisée et transparente des charges de travail d'un périphérique edge vers un cloud ou un centre de données traditionnel en utilisant des normes ouvertes, des technologies de base et une stratégie edge-to-cloud.\n\nPour un avenir prévisible, l'edge dominera car à mesure que les choses deviennent plus lumineuses, la technologie doit être poussée vers l'edge où l'information est créée. Ensuite, le traitement pourra se faire de manière centralisée pour davantage d'analyses et d'IA. Les choses ne feront que progresser, et l'edge deviendra omniprésent.\n\n## Intelligence artificielle\n\nLes données sont partout maintenant ; les centres de données n'ont pas de murs. Elles sont recueillies et même traitées de multiples façons, comme les téléphones portables, les caméras, les moteurs industriels, etc. L'intelligence artificielle consiste à prendre des décisions intelligentes fondées sur toutes ces données disparates.\n\nIntel possède l'un des plus grands portefeuilles d'intelligence artificielle au monde, un portefeuille logiciel extraordinaire et plus de développeurs de logiciels que de nombreuses entreprises de logiciels. La raison en est, comme le souligne Greg Lavender, que le logiciel est l'âme du silicium. Il faut lui permettre de faire quelque chose et lui donner vie et but. L'IA en est un excellent exemple.\n\nParfois, les gens pensent à l'IA en termes de robots, mais il existe d'innombrables cas d'utilisation pratiques. Un exemple est lorsque vous êtes perdu, vous pouvez interroger immédiatement Siri ou Google et, en utilisant le GPS, le service vous localisera géographiquement et trouvera le point le plus proche de la civilisation ou de l'endroit où vous souhaitez vous rendre.\n\nUn autre cas d'utilisation pratique se trouve au sein de l'US Postal Service. La prochaine génération de véhicules de livraison est aussi moderne que les voitures Google Street View, équipées de capteurs cartographiant certaines choses. Le service postal applique l'IA aux machines de tri intelligent cinétique et de manipulation du courrier. Ils exploitent la technologie pour se développer à grande échelle.\n\nEn raison de l'informatique omniprésente et des communications avancées d'Intel, de plus en plus de dispositifs périphériques deviennent intelligents, et la quantité de données qui doivent être transférées du périphérique vers les centres de données diminue. Cela est dû au fait que les algorithmes d'intelligence artificielle déduisent ce que vous recherchez au niveau du périphérique. Ce type de technologie est directement intégré dans les processeurs Intel. Ils disposent également de XPU spécialisés, des processeurs neurotrophiques, qui font la même chose avec une consommation électrique inférieure et une vitesse supérieure.\n\nUne partie de la puissance de ce qui se passe avec Intel et leurs partenaires réside dans la capacité à avoir accès à toutes les informations dans un format exploitable. Dans l'exemple de se perdre et chercher de l'aide sur votre téléphone, vous pourriez regarder quelque chose qui a assimilé 600 points de données différents en une fraction de seconde pour vous donner une réponse simple.\n\n## Les Superhéros\n\nLes personnes d'Intel et ses partenaires de l'écosystème sont des superhéros. Les partenaires aident à rassembler de vraies solutions, en particulier dans cette étape critique finale. Intel a l'un des meilleurs écosystèmes pour mettre des solutions sur le marché. Et parfois, ces solutions ne sont même pas commercialisées mais sont utilisées pour aider à résoudre des problèmes complexes dans la défense et le secteur public.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Cameron Chehreh"],"link":"/episode-EDT100-fr","image":"./episodes/edt-100/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren et Cameron Chehreh, vice-président et directeur général du secteur public chez Intel, parlent des super pouvoirs d'Intel : l'informatique omniprésente, la connectivité omniprésente, le bord vers le cloud et l'intelligence artificielle."},{"id":3,"type":"Episode","title":"Sécurité du contrôleur de réseau avec Elisity","tags":["networksecurity","cybersecurity","elisity","comms","technology","process"],"body":"\r\n\r\nDana travaille dans le domaine du réseau avancé depuis les 15 dernières années. Il a travaillé avec les réseaux définis par logiciel et les réseaux étendus, en se concentrant pendant un certain temps sur le monde du SD-WAN. Ensuite, il a travaillé avec le cloud, mais est revenu à ses racines en se spécialisant dans la sécurité réseau chez Elisity.\n\nDan a travaillé principalement dans le domaine des réseaux, mais il a commencé avec des services avancés chez Cisco et a également travaillé dans le secteur pétrolier et gazier à Houston. Il s'est ensuite orienté vers l'espace SASE SD-WAN, où il a collaboré avec Dana. À présent, il se concentre sur la sécurité réseau ainsi que sur la sécurité définie par logiciel. Dan affirme que le terme est cependant insaisissable car la technologie repose sur la micro-segmentation par l'identité.\n\nLa façon traditionnelle de créer une micro-segmentation à travers des VLAN et des pare-feu n'est plus suffisante. Depuis 15 ans, la sécurité réseau s'est concentrée sur le renforcement du périmètre du réseau. Il est toujours essentiel de créer un mur impénétrable autour de l'entreprise, et les pare-feu font un travail incroyable pour empêcher les personnes d'accéder au réseau. Mais la majeure partie de l'intérieur du réseau, où il y a eu une explosion des nouvelles exigences de connectivité en raison de l'IoT et de l'OT, est une zone de non-droit une fois qu'un utilisateur est à l'intérieur. Étant donné que le mur extérieur est robuste, toute personne à l'intérieur est souvent considérée comme un utilisateur légitime ou de confiance.\n\nLa réalité est que la plupart des attaques actuelles se produisent à l'intérieur du réseau grâce à des utilisateurs, des appareils et des applications de confiance exploités.\n\nIl existe de nombreuses raisons pour lesquelles l'utilisation de VLAN, d'ACL IP, de pare-feu et de méthodes traditionnelles de segmentation ne fonctionne pas aujourd'hui pour la sécurité des mouvements latéraux. Elles peuvent fonctionner dans des environnements statiques à petite échelle. Elles fonctionnent très bien pour le bord du réseau et des endroits spécifiques tels que les goulots d'étranglement ou les points d'agrégation, mais il existe trois raisons courantes pour lesquelles elles ne sont plus idéales.\n\nTout d'abord, la scalabilité et l'efficacité opérationnelle des méthodes traditionnelles sont discutables. La gestion des VLAN, des ACL IP et des pare-feu dans de grandes entreprises se fait manuellement. Ce n'est pas une architecture distribuée définie par logiciel, mais cela nécessite une configuration boîte par boîte, ligne par ligne. Ils ne sont pas dynamiques ni réactifs à quoi que ce soit sur le réseau. Leur utilisation crée également un réseau en forme de gruyère rempli de trous aléatoires. Les utilisateurs, en particulier les opérateurs de réseaux, représentent souvent le risque le plus important pour un système. Ils peuvent, par exemple, ouvrir un VLAN ou modifier une pièce pour effectuer un test rapide, mais ne pas annuler par la suite cette modification.\n\nUn autre exemple est qu'un VLAN commencera par un cas d'utilisation et s'étendra progressivement à d'autres cas d'utilisation. Soudainement, ce qui était un VLAN de dix appareils en compte désormais soixante. Dans le monde de l'OT, il pourrait y avoir six ou sept processus différents qui s'y exécutent car il s'agit du LAN de confiance. Pourtant, souvent, ces environnements se développent lentement et ne sont pas documentés, donc le risque passe inaperçu.\n\nLe deuxième problème est que les VLAN et les pare-feu sont intrinsèquement mal placés dans le réseau pour fournir une sécurité de déplacement latéral. Si vous êtes dans le même VLAN qu'un autre appareil, utilisateur ou application, ce canal de communication est ouvert même s'il n'est pas nécessaire. Les pare-feu ne sont généralement pas déployés à un endroit stratégique où ils peuvent gérer le niveau d'accès au déplacement latéral. Vous devez acheminer le trafic vers un pare-feu et le renvoyer, ce qui est inefficace. Ensuite, vous avez un goulot d'étranglement.\n\nLes mauvais acteurs cherchent à exploiter les réseaux pour obtenir des résultats qui ne correspondent pas à leur fonctionnement prévu. Par exemple, si un utilisateur est dans une VLAN et qu'un processus s'exécute dans un cas d'utilisation, rien ne les empêche de passer du port 3 au port 32 dans la même VLAN. Souvent, les gens conçoivent la sécurité en fonction de l'utilisation prévue plutôt que de la manière dont elle pourrait être utilisée. Il est courant, par exemple, que les développeurs de logiciels sautent de ports pour travailler efficacement, mais cela est dangereux car cela les rend vulnérables. Personne ne peut placer des centaines ou des milliers de pare-feu sur tout le périmètre d'accès. Ce serait financièrement restrictif et impossible à gérer.\n\nLe troisième problème est que ces solutions de segmentation héritées ne tiennent pas compte de l'identité, du contexte ou du comportement de l'actif connecté au réseau. Il s'agit d'une topologie rigide centrée sur le réseau qui offre certaines mesures de sécurité essentielles. Mais une adresse IP ne dit rien sur la légitimité de l'actif et du réseau auquel il est attaché. Comment pouvez-vous sécuriser dynamiquement ce réseau lorsque vous ne savez pas ce qui s'y connecte ? Vous ne pouvez pas établir de politique en premier lieu sans aucune granularité ; traiter tous les appareils de manière égale ne fonctionne pas maintenant.\n\nMême s'il y a une analyse du type de trafic, cela se produit généralement à plusieurs niveaux supérieurs, ce qui signifie que vous êtes maintenant exposé. Toute mesure de mise en application peut ou non être en mesure de protéger pleinement l'infrastructure.\n\nCette capacité de détection a toujours de la valeur, même s'il n'y a aucune protection. Cependant, la capacité d'arrêter quelque chose qui aurait pu se produire juste au bord, aussi près que possible de l'actif, est une meilleure solution.\n\nDécouvrez le prochain épisode de cette série [ici](episode-EDT101).\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Dan Demers","Dana Yanch"],"link":"/episode-EDT101-fr","image":"./episodes/edt-101/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, responsable des solutions chez Intel pour le secteur public, interviewe les experts en sécurité réseau Dana Yanch et Dan Demers d'Elisity sur les techniques de sécurité du contrôleur réseau et les architectures de confiance zéro."},{"id":4,"type":"Episode","title":"Micro-segmentation basée sur l'identité avec Elisity","tags":["microsegmentation","otsecurity","identitybasednetworking","cybersecurity","comms","technology","edge","sdn"],"body":"\r\n\r\nDans la deuxième partie de la sécurité avec Elisity, Darren discute des solutions de micro-segmentation basées sur l'identité avec Dana Yanch, Directrice du Marketing Technique, et Dan Demers, Architecte Technique.\n\nL'approche d'Elisity en matière de sécurité des réseaux diffère des architectures traditionnelles par la manière dont elle est livrée, répartie sur le réseau et réalisée de manière efficace, offrant une valeur temporelle rapide. Ils se sont concentrés sur sa simplicité, son caractère intuitif et son efficacité maximale afin que tout secteur d'activité puisse tirer parti de cette technologie.\n\nLa clé de la technologie d'Elisity est la micro-segmentation, ce qui signifie la capacité d'isoler complètement tout utilisateur, appareil ou application, peu importe le type de réseau sur lequel il se trouve, où il est placé dans le réseau, ou s'il est géré ou non géré. En d'autres termes, il est flexible pour isoler un actif d'un autre sans restrictions imposées par l'architecture du réseau ou des concepts tels que les VLAN ou les VRF.\n\nLes systèmes traditionnels de micro-segmentation qui nécessitent un remplacement matériel ou l'instanciation d'un appareil sur site peuvent prendre des mois voire des années pour être mis en place et fonctionner. Cependant, Elisity offre une fonctionnalité complète en moins d'une semaine en passant à une plateforme de micro-segmentation native du cloud et livrée via le cloud. Les plans de contrôle, de gestion et de politique complets sont entièrement livrés via le cloud, et le composant lié à votre réseau sur site est à 100 % logiciel. Vous n'avez pas besoin de modifier quoi que ce soit sur site, car il s'appuie sur votre infrastructure existante. Il s'échelonne bien, est facile à gérer et est distribué de manière dynamique.\n\nElisity propose également une solution sur site pour l'espace OT, où les organisations peuvent le garder derrière leur DMZ et couper tout accès.\n\nLa plateforme d'Elisity est un microservice dans le sens de la scalabilité horizontale. Vous pouvez commencer petit et ajouter 10 000 éléments au réseau et le faire évoluer avec vous, que ce soit sur site ou dans le cloud. Tout est entièrement automatisé sur l'ensemble du réseau. Il y a peu de friction et il est facile à gérer sur le long terme.\n\nUne différence significative entre la plateforme d'Elisity et d'autres réseaux superposés est qu'Elisity traite le plan de contrôle et de politique plutôt que le plan de données. Elle peut appliquer le même niveau de granularité de sécurité réseau sans toucher aucun paquet. Elle indique dynamiquement au réseau qui peut communiquer avec qui en fonction des attributs et des identités trouvés sur le réseau.\n\nUn triangle d'utilisateurs, d'applications et d'appareils est un excellent moyen de visualiser cela. À l'intérieur de ce triangle se trouvent toutes les lignes de communication entre les différents éléments. Elisity peut sécuriser chaque ligne, ou canal, entre les utilisateurs, les applications et les appareils avec une granularité optimale. Cela se produit au niveau du contrôle réseau. Dès que le trafic atteint la première extrémité, il est sécurisé.\n\nLa sécurité est essentielle au niveau du réseau et non au niveau de l'application, car il peut y avoir des dizaines de milliers de dispositifs dans votre réseau sur lesquels vous ne pouvez pas installer un agent ou les modifier, car ils ont des systèmes d'exploitation intégrés, des caméras, des lecteurs de badges et des personnes. Cela est particulièrement important du côté de l'OT.\n\nAvec l'OT, l'accent est mis sur la disponibilité et l'intégrité. Les choses doivent continuer à fonctionner de manière sûre car le processus peut représenter une infrastructure critique. Cela diffère de l'informatique où l'on peut mettre en quarantaine et lancer une nouvelle instance en cas d'intrusion. L'OT nécessite une mentalité différente. Là où quelque chose peut exister en informatique pendant six mois ou un an, quelque chose en OT peut exister pendant 20 ans. Souvent, ces infrastructures sont entretenues ou mises à jour tous les quelques années, ou restent inactives jusqu'à un événement de panne. La sécurité implique de limiter la connectivité. Par exemple, si vous êtes dans un VLAN, cela ne signifie pas que vous devriez pouvoir communiquer avec un RTU, un capteur, un IHM ou un DCS qui se trouvent à un commutateur de distance. Dans la conception héritée, si ces éléments se trouvent sur le même VLAN, il n'y a pas de moyen efficace et flexible de les empêcher de se connecter.\n\nAvec le système d'Elisity, vous définissez des politiques en fonction des regroupements d'éléments ou des attributs assignés aux ressources, plutôt que de passer en revue chaque élément individuellement. Cela peut être aussi simple que regrouper tous vos processus de telle sorte qu'ils ne puissent communiquer que du nord au sud, et non d'est en ouest, ou, par exemple, permettre aux responsables hiérarchiques de partager avec six ou sept types d'éléments. Deux ou trois politiques peuvent rapidement réduire votre surface d'attaque de 65 000 ports d'attaque potentiels à deux ou trois.\n\nUn cas d'utilisation courant dans les réseaux de technologie opérationnelle (OT) est lorsque un fournisseur effectue une mise à jour sur un appareil, vous devez les laisser entrer et ils ont accès à l'ensemble de votre réseau. Avec Elisity, vous pouvez facilement leur donner un accès limité dans le temps et leur permettre d'interagir avec les actifs fixes.\n\nSuppose you would like to know more about Elisity’s technology. In that case, many resources, including videos, white papers, and documents, explain how the solution works and how it could be applied within a week in your network at Elisity.com.\n\nSupposons que vous souhaitez en savoir plus sur la technologie d'Elisity. Dans ce cas, de nombreuses ressources, notamment des vidéos, des livres blancs et des documents, expliquent comment la solution fonctionne et comment elle pourrait être appliquée dans votre réseau en une semaine sur Elisity.com.\n\nDécouvrez l'épisode précédent de cette série [ici](episode-EDT101).\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Dana Yanch","Dan Demers"],"link":"/episode-EDT102-fr","image":"./episodes/edt-102/fr/thumbnail.png","lang":"fr","summary":"Dans la partie 2 de Sécurité avec Elisity, Darren discute des solutions de micro-segmentation basées sur l'identité avec Dana Yanch, Directrice du marketing technique, et Dan Demers, Architecte technique."},{"id":5,"type":"Episode","title":"Opérationnalisation de vos projets d'IA","tags":["aiops","devops","compute","technology","process","devsecops","cybersecurity","aiml"],"body":"\r\n\r\nGretchen est un excellent exemple de quelqu'un qui apprend et s'adapte continuellement. Elle a obtenu sa licence en mathématiques. Elle a également un master en commerce et a terminé un programme à Harvard il y a quelques années, axé sur la science des données, ce qui l'a conduite à son poste de scientifique en chef des données chez Intel dans le secteur public. Elle a travaillé dans le domaine de la technologie pendant plus de 20 ans, en commençant par l'ingénierie logicielle, et a passé 15 ans dans le secteur fédéral.\n\nElle trouve particulièrement gratifiant de travailler dans le secteur public car cela a un impact dans la vie quotidienne des citoyens. De plus, le gouvernement fédéral détient la plus grande quantité de données sur la planète, ce qui convient parfaitement à quelqu'un qui aime être submergé de données et continuer à apprendre davantage.\n\nIl existe de nombreux termes liés à l'intelligence artificielle. Tout d'abord, il est essentiel de comprendre la différence entre l'intelligence artificielle (IA) et les opérations d'apprentissage automatique (ML ops). Les ML ops sont des techniques faisant partie de l'IA ; ils en sont une sous-catégorie. Les algorithmes d'apprentissage automatique tirent leur force de leur capacité à apprendre à partir des données disponibles. Ainsi, principalement, vous apprenez soit à partir de données supervisées, soit à partir de données non supervisées.\n\nLa différence simple entre l'apprentissage supervisé et l'apprentissage non supervisé est l'étiquette des données. Dans l'apprentissage supervisé, les ensembles de données sont étiquetés. Cela signifie que l'apparence des données est déjà cartographiée. Cela facilite grandement la classification et la prédiction. Dans l'apprentissage non supervisé, on cherche à trouver des modèles dans les données ; la machine apprend à créer des relations entre les données en trouvant des modes communs, des similarités ou des différences.\n\nUn exemple d'apprentissage supervisé serait une boutique en ligne recommandant un article qu'un client pourrait vouloir acheter en fonction de son historique d'achats, ou un service de streaming recommandant un film en fonction des habitudes de visionnage de quelqu'un.\n\nDe nombreux termes ont désormais l'abréviation \"ops\" à la fin. Par exemple, les gens disent \"DL ops\" pour désigner les opérations d'apprentissage profond, une sous-catégorie de l'apprentissage automatique. Pourquoi ce \"ops\" ? Tout d'abord, ce n'est pas aussi sophistiqué que DevOps. Au lieu de cela, il est influencé par l'idée largement adoptée de l'approche DevOps pour la création et la personnalisation d'applications. Les gens essaient de développer un ensemble de pratiques pour aider à optimiser la fiabilité et l'efficacité de la conception, du développement et de l'exécution de l'apprentissage automatique. Ce serait donc presque comme un marché où vous pouvez créer et exploiter des applications personnalisées, puis les partager avec d'autres.\n\nDe nombreux modèles et algorithmes sont déjà optimisés et disponibles dans des outils tels que Converge.io ou C3 AI. Ces méthodologies et technologies peuvent vous aider à rationaliser vos modèles d'apprentissage automatique. La meilleure façon de le faire est avec de nombreux outils qui sont soit open source, soit des logiciels spécifiques créés par les fournisseurs, pour rendre la création, le développement, la conception, l'exécution et le flux beaucoup plus accessibles.\n\nLe développement de l'IA est similaire à ce qu'était le développement de logiciels il y a 30 ans. Beaucoup des étapes sont encore manuelles et seront probablement automatisées prochainement.\n\nDans les épisodes précédents, Darren et Gretchen ont discuté du fait que de nombreux projets d'IA et de ML ne sont que des expériences scientifiques réalisées une seule fois. Ensuite, le scientifique des données passe à autre chose et cela n'est jamais opérationnalisé. Contrairement à cela, l'opérationnalisation de ML se dirige vers le déploiement du modèle pour fournir une réelle valeur après l'entraînement et l'apprentissage.\n\nCertaines entreprises exploitent explicitement ces outils. Domino Labs, par exemple, crée presque ce marché. Le travail dans le secteur public, disons, sur des sous-marins nucléaires en effectuant une détection d'objets ou une classification par regroupement, pourrait être applicable dans l'armée de l'air ou d'autres auxiliaires afin d'opérationnaliser et de construire des environnements agiles. Vous pourriez exploiter certains algorithmes et les pondérer différemment en fonction des résultats. Vous pourriez l'ajuster en fonction des différences dans les ensembles de données, mais au moins il y a... des points de départ ? Des points communs ? Des outils partagés ? Ses derniers mots ont été coupés ici...\n\nLa sécurité est toujours préoccupée par les logiciels et modèles open-source, et l'IA présente des circonstances uniques. Par exemple, comment savoir si le développeur ne l'a pas entraînée à ignorer son propre visage dans un modèle de reconnaissance faciale ? Il y a désormais une attente selon laquelle les personnes doivent documenter les choses, par exemple, d'où provient un ensemble de données.\n\nIl y a aussi la question de l'éthique et de la responsabilité. Le chatbot Tay et les biais trouvés dans les programmes de reconnaissance faciale étaient de bons exemples d'IA dévoyée sans intention malveillante. Pendant longtemps dans les opérations de ML, une seule personne faisait le travail et produisait les résultats. Maintenant, l'idée est que vous avez besoin d'une équipe diversifiée de personnes occupant différents rôles et ayant différentes perspectives sur le monde.\n\nLa première conférence sur l'IA et le ML a eu lieu en 1956 au Dartmouth College. La vérité est que de nombreux concepts fondamentaux de l'IA, tels que la régression logistique, la régression linéaire, les algorithmes de regroupement, etc., sont des équations mathématiques qui existent depuis longtemps. Bien sûr, de brillants cadres supplémentaires ont été ajoutés, tels que TensorFlow, à partir desquels construire, mais les bases restent/restaient toujours la fondation. Nous avons ajouté les capacités de calcul, de stockage, la 5G et des fonctionnalités uniques. Une fois que vous avez effectué toute la formation, vous avez les données et les informations à côté de la technologie, au lieu de devoir tout apporter à la technologie. Amener la technologie aux données ouvre de nouveaux problèmes amusants et passionnants que nous pouvons maintenant résoudre.\n\nMais les conversations concernant la façon dont le modèle a été entraîné, quelles étaient les données originales et la prise en compte de la dérive du modèle doivent toujours se poursuivre. Après un certain temps, il faut revoir l'entraînement ; peut-être faut-il apporter un algorithme différent ou pondérer différemment celui existant pour obtenir des informations plus précises, car il y a plus de données et des données plus diverses. Tout cela est positif car cela augmente votre niveau de précision.\n\nDonc, avec l'orientation vers les opérations ML, vous pouvez le faire en continu. Tout comme le développement de logiciels a évolué vers l'intégration et le déploiement continus, la même chose commencera à se produire dans le domaine de l'IA ou du ML, où les modèles seront mis à jour et deviendront de plus en plus précis.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Gretchen Stewart"],"link":"/episode-EDT103-fr","image":"./episodes/edt-103/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions du secteur public chez Intel, et Gretchen Stewart, scientifique en chef des données du secteur public, discutent de l'opérationnalisation des projets d'IA."},{"id":6,"type":"Episode","title":"Leadership axé sur l'information","tags":["data","informationdriven","organizationalchange","radiusag","change","people","process","ceo"],"body":"\r\n\r\nBien que Betsy soit la PDG de Radius Advisory Group et techniquement retraitée de la fonction publique, elle garde néanmoins un pied dans le secteur public grâce à son entreprise spécialisée dans la cybersécurité et les problématiques liées au cyberespace d'importance nationale. Travailler à la fois dans les secteurs privé et public a été la partie la plus excitante de son parcours professionnel.\n\nBetsy a commencé en tant que membre du service actif de l'armée de l'air, passant à l'industrie tout en jonglant avec les enfants et un conjoint également en service actif. Elle a travaillé dans plusieurs industries, notamment les services publics et l'énergie, et a passé beaucoup de temps chez PricewaterhouseCoopers. Elle est retournée au département de la Défense lorsque elle a été sélectionnée pour travailler avec le secrétaire Gates en tant que membre de la Task Force sur les Efficacités de la Défense. Elle est restée sous la direction du secrétaire Pancetta et a vécu un parcours passionnant, étant finalement nommée vice-CIO responsable de l'examen des processus métier et des systèmes. Là-bas, elle a créé une fonction d'analyse des données pour fournir une plus grande transparence sur les coûts et les efficacités potentielles de la technologie de l'information dans le DoD.\n\nLe plus grand défi de Betsy dans son rôle de Directrice adjointe des systèmes d'information, qu'elle pense être vrai tant dans le secteur public que privé, est de trouver comment introduire de nouvelles idées, des processus, des technologies et des méthodes de travail au sein de l'organisation. Dans les grandes organisations, l'envergure est énorme et il y a de nombreux silos, chacun avec sa propre culture, ses propres objectifs, budgets et comptes de résultat. Des situations telles que la pandémie de COVID, où les changements doivent se produire rapidement, sont incroyablement difficiles.\n\nBetsy dit que la COVID a changé la culture de certaines façons, mais d'autres façons ont poussé les gens à se replier encore plus, ce qui n'est pas bon. Il y avait de nombreux défis organisationnels et technologiques dont tout le monde a appris et continue à le faire. Une inquiétude que Betsy a, c'est qu'il y a maintenant un nouvel environnement et écosystème, et le retour au bureau ne peut pas être remis dans le vieux sac comme le font bon nombre de dirigeants. Bien que cela soit très difficile à gérer et que les dirigeants considèrent le nouvel environnement comme bon ou mauvais, il ne peut pas être traité de la même manière qu'auparavant.\n\nUne fois que les dirigeants opèrent un changement, toutefois, la stratégie de Betsy consiste à tout faire au plus vite. Elle a appris cette leçon lorsqu'elle était vice-directrice des systèmes d'information et qu'elle se voyait confier divers projets en plus de la mission principale. Elle a remarqué que d'autres équipes bénéficiaient systématiquement de 30 jours pour leurs projets, tandis que son équipe n'en avait que 10. Lorsqu'elle a demandé pourquoi, les dirigeants ont répondu qu'ils savaient que son équipe pouvait y parvenir en 10 jours et qu'ils devaient prendre des décisions complexes qui nécessitaient les informations que son équipe pouvait fournir dans le cadre du processus décisionnel, donc plus rapidement ils les obtiendraient, mieux ce serait. Elle a donc dit à son équipe qu'ils devaient simplement trouver une solution. Ils l'ont fait et sont devenus compétents dans ce domaine.\n\nUn sentiment d'urgence aide les personnes à se concentrer et à accomplir des tâches, mais les leaders, souligne Betsy, doivent s'assurer d'être équipés des personnes, des ressources et de l'autorité nécessaires pour exécuter quelque chose dans un délai court. Ce soutien du leadership est crucial pour la réussite.\n\nBetsy croit que l'écoute des gens est essentielle, mais les décisions sont prises au sommet, ce qui devrait généralement signifier quelques personnes. Une organisation matricielle, qui a gagné en popularité dans les années 90 et 2000, a ajouté des niveaux de complexité, mais la plupart des situations sont déjà complexes, donc cela revient toujours au leadership. Les dirigeants doivent être suffisamment inclusifs pour écouter autant de personnes que possible, mais cela ne peut pas prendre cinq mois et 150 réunions. Ils doivent développer des systèmes et des groupes capables de générer des idées, de suggérer des modèles et de travailler les uns avec les autres. Les leaders doivent avoir la capacité de réagir rapidement ; parfois, ils doivent agir rapidement. Souvent, la réponse réside dans moins de personnes mais les bonnes personnes, moins de technologie et la bonne technologie pour obtenir ce dont vous avez besoin.\n\nBetsy utilisait un modèle pratique lorsqu'elle recevait des projets urgents du Sous-Secrétaire à la Défense afin de constituer de petites équipes de cinq à sept personnes possédant des compétences cognitives diverses parmi ses 50 analystes. Cela a été un succès car elle connaissait bien la personnalité et les compétences de ses analystes, lui permettant de choisir les équipes au lieu de rassembler un grand groupe et d'obtenir l'avis de tout le monde. Les petites équipes l'ont surpris à maintes reprises en réalisant des tâches complexes et en résolvant des problèmes.\n\nEn mettant en place des procédures et des modèles pour résoudre les problèmes, l'équipe de Betsy fournissait souvent des informations au Sous-secrétaire à la Défense ou au Directeur des systèmes d'information en quelques heures si nécessaire. En fin de compte, ils disposaient de procédures qui leur permettaient de générer des idées et de proposer des options neutres basées sur des données et tenant compte de nombreux facteurs. Cela permettait aux dirigeants d'examiner les faits et les preuves pour prendre des décisions.\n\nBetsy avait confiance en ses équipes et ne leur disait jamais comment faire le travail ou supposait qu'elle connaissait la meilleure façon d'aborder les choses. Elle avait de bonnes personnes et leur faisait simplement confiance pour le faire. Cela motivait ses collaborateurs et ils étaient constamment impatients de recevoir de nouvelles missions. Les employés veulent savoir que les dirigeants les écoutent, considèrent et utilisent leurs idées. Cette approche permettait aux gens de lui faire part de leurs meilleures analyses et options. C'était souvent une combinaison de ces analyses qui remontait à la direction. Le crédit revient au leadership du DOD et au bureau du CIO, qui faisaient confiance au processus de Betsy.\n\nChaque fois qu'il y avait un nouveau problème, Betsy attribuait une nouvelle équipe. De cette façon, différentes personnes pouvaient travailler sur différents types de projets et ne pas être coincées dans un seul domaine, et les gens pouvaient travailler en transcendant les différences générationnelles. Parfois, les gens étaient initialement réticents à travailler avec des groupes d'âge différents du leur, mais finalement, ils ont appris à voir les choses à travers des lentilles différentes et bénéfiques.\n\nDepuis que Betsy et son adjoint ont appris à bien connaître leur équipe, elle a pu travailler rapidement pour constituer des équipes efficaces. Un signe distinctif de son succès est que chaque fois qu'elle demandait à une équipe de travailler sur deux choses, personne ne refusait. Ils le faisaient simplement.\n\nCliquez ici pour la deuxième moitié de la discussion de Darren avec Betsy Freeman.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Betsy Freeman"],"link":"/episode-EDT104-fr","image":"./episodes/edt-104/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions, secteur public chez Intel s'entretient avec Betsy Freeman, PDG du Radius Advisory Group, sur son expérience en tant que leader axé sur l'information dans les secteurs public et privé. Partie 1 sur 2."},{"id":7,"type":"Episode","title":"Leadership axé sur l'information, partie 2.","tags":["change","people","changeagent","radiusag","organizationalchange","informationdriven","leadership"],"body":"\r\n\r\nBien que Betsy soit PDG de Radius Advisory Group et techniquement à la retraite de l'espace fédéral, elle garde néanmoins un pied dans le secteur public grâce à son entreprise qui se concentre sur la cybersécurité et les problèmes liés à l'espace cybernétique d'importance nationale. Travailler à la fois dans les secteurs privé et public en même temps a été la partie la plus excitante de son parcours professionnel.\n\nBetty a commencé en tant que militaire en service actif dans l'armée de l'air, passant à l'industrie tout en jonglant avec les enfants et un conjoint également en service actif. Elle a travaillé dans plusieurs industries, plus récemment dans les services publics et l'énergie, et a passé beaucoup de temps chez PricewaterhouseCoopers. Elle est retournée au Département de la Défense (DOD) lorsqu'elle a été sélectionnée pour travailler avec le secrétaire Gates en tant que membre du groupe de travail sur les efficacités de la défense. Elle est restée sous la direction du secrétaire Pancetta et a vécu un parcours passionnant, étant finalement nommée sous-CIO pour l'examen des processus et des systèmes commerciaux. Là-bas, elle a créé une fonction d'analyse des données pour fournir une plus grande transparence sur les coûts de la technologie de l'information et les efficacités potentielles dans tout le DoD.\n\nLe plus grand défi de Betsy dans le rôle de vice-CIO, qu'elle pense être vrai à la fois dans le secteur public et privé, est de savoir comment introduire de nouvelles idées, des processus, des technologies et des méthodes de travail dans l'organisation. Dans les grandes organisations, la portée est énorme et il existe de nombreux silos, chacun avec sa propre culture, ses propres agendas, ses budgets et ses bilans. Des situations telles que la pandémie de COVID, où les changements doivent se produire rapidement, sont incroyablement difficiles.\n\nBetsy dit que la COVID a changé la culture de certaines manières, mais d'autres manières ont fait en sorte que les gens se replient encore plus, ce qui n'est pas bon. Il y a eu de nombreux défis de processus et de technologie dont tout le monde a appris et continue d'apprendre. Une préoccupation que Betsy a, c'est qu'il existe maintenant un nouvel environnement et écosystème, et le retour au bureau ne peut pas être remis dans l'ancien sac comme beaucoup de dirigeants essaient de le faire. Bien que cela soit très difficile à naviguer, et que les dirigeants considèrent le nouvel environnement comme bon ou mauvais, il ne peut pas être traité de la même manière qu'auparavant.\n\nUne fois que les dirigeants opèrent un changement, cependant, la stratégie de Betsy est d'accélérer tout. Elle a appris cette leçon lorsqu'elle était directrice adjointe des technologies de l'information et qu'on lui confiait divers projets en plus de la mission sous-jacente. Elle a remarqué que d'autres équipes se voyaient accorder systématiquement 30 jours pour leurs projets, mais que son équipe n'en avait que 10. Lorsqu'elle a demandé des explications, la direction a déclaré qu'elle savait que son équipe pouvait le faire en 10 jours et qu'elle devait prendre des décisions complexes qui nécessitaient les informations que son équipe pouvait fournir dans le cadre du processus décisionnel. Plus vite ils pouvaient les obtenir, mieux c'était. Elle a donc dit à son équipe qu'ils devaient simplement trouver une solution. Ils l'ont fait et sont devenus bons dans ce domaine.\n\nUn sentiment d'urgence aide les gens à se concentrer et à performer, mais les leaders, souligne Betsy, doivent s'assurer qu'ils sont munis des personnes, des ressources et de l'autorité nécessaires pour exécuter quelque chose dans un court délai. Ce soutien des dirigeants est clé pour le succès.\n\nBetsy croit que l'écoute des gens est essentielle, mais les décisions se prennent en haut, ce qui devrait généralement signifier quelques personnes. Une organisation matricielle, qui a gagné en popularité dans les années 90 et 2000, a ajouté des niveaux de complexité, mais la plupart des situations sont déjà complexes, donc cela revient toujours au leadership. Les leaders doivent être suffisamment inclusifs pour écouter autant de personnes que possible, mais cela ne peut pas prendre cinq mois et 150 réunions. Ils doivent développer des systèmes et des groupes capables de faire de l'idéation, de suggérer des modèles et de travailler les uns avec les autres. Les leaders doivent avoir la capacité de réagir rapidement ; parfois, ils doivent agir rapidement. Souvent, la réponse est moins de personnes mais les bonnes personnes, moins de technologie et la bonne technologie pour obtenir ce dont vous avez besoin.\n\nBetsy a utilisé un modèle efficace lorsqu'elle recevait des projets urgents du Secrétaire adjoint à la Défense pour former des petites équipes de cinq à sept personnes possédant des compétences cognitives diverses parmi ses 50 analystes. Cela a été un succès car elle connaissait très bien la personnalité et les compétences de ses analystes, ce qui lui permettait de sélectionner soigneusement les équipes plutôt que de présenter un problème à un groupe important et d'essayer d'obtenir l'avis de tout le monde. Les petites équipes l'ont surpris à plusieurs reprises en accomplissant des tâches complexes et en résolvant des problèmes.\n\nEn mettant en place des processus et des modèles pour résoudre des problèmes, l'équipe de Betsy fournissait souvent des contributions au secrétaire adjoint à la Défense ou au DSI dans des délais de quelques heures si nécessaire. En fin de compte, ils disposaient de processus en place qui leur permettaient de faire émerger des options neutres et basées sur des données en tenant compte de nombreux facteurs. Cela permettait aux dirigeants d'examiner les faits et les preuves et de prendre des décisions.\n\nBetsy avait confiance en ses équipes et ne leur disait jamais comment faire leur travail, ni ne supposait qu'elle connaissait la meilleure façon d'aborder les choses. Elle avait de bonnes personnes et leur faisait simplement confiance pour le faire. Cela motivait ses collaborateurs et ils étaient constamment impatients de recevoir de nouvelles tâches. Les employés veulent savoir que les leaders les écoutent, considèrent et utilisent leurs idées. Cette approche permettait aux gens de lui faire part de leur meilleure analyse et de leurs options. C'était souvent une combinaison de ces analyses qui revenait vers la direction. Le mérite revient au leadership du DOD et du bureau du CIO, qui avaient confiance en le processus de Betsy.\n\nChaque fois qu'il y avait un nouveau problème, Betsy attribuait une nouvelle équipe. De cette façon, différentes personnes pouvaient travailler sur différents types de projets et ne pas être enfermées dans un seul domaine, et les gens pouvaient travailler en traversant la division générationnelle. Parfois, les gens étaient initialement réticents à travailler avec des groupes d'âge extérieurs au leur, mais à la fin, ils ont appris à voir les choses à travers des lentilles différentes et bénéfiques.\n\nDepuis que Betsy et son adjoint ont appris à bien connaître leur personnel, elle a pu rapidement constituer des équipes efficaces. Une caractéristique marquante de sa réussite est que chaque fois qu'elle demandait à une équipe de travailler sur deux choses, personne ne disait jamais non. Ils s'en occupaient simplement.\n\nCliquez [ici](episode-EDT104) pour la première moitié de la discussion de Darren avec Betsy Freeman.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Betsy Freeman"],"link":"/episode-EDT105-fr","image":"./episodes/edt-105/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, Architecte principal des solutions, Secteur public, Intel, poursuit sa discussion avec Betsy Freeman, PDG de Radius Advisory Group, sur son expérience en tant que leader axé sur l'information dans les secteurs public et privé. Deuxième partie sur deux."},{"id":8,"type":"Episode","title":"La naissance des plates-formes d'intelligence graphique.","tags":["data","graphintelligenceplatform","graphdb","katanagraph","technology"],"body":"\r\n\r\nGreg a commencé sa carrière dans une banque d'investissement en tant que responsable du risque de crédit, au moment où ils ont commencé à mettre en œuvre les tests de résistance CCAR. Après avoir été fortement impliqué dans cette activité pendant un certain temps, il a commencé à faire du conseil et a découvert les graphiques lors de la validation des modèles de risque de crédit. Il a remarqué comment les graphiques pouvaient être exploités pour de nombreuses analyses différentes et présentaient des avantages dans la gestion des données et l'apprentissage automatique, en particulier dans la modélisation de crédit. C'est ainsi qu'il a découvert Katana.\n\nLes analystes et les scientifiques des données luttent constamment pour intégrer différents ensembles de données. Greg a été attiré par les graphiques car, après avoir été introduit à RDF, un format de graphe de connaissances sémantiques, il était évident de comprendre comment les données pourraient être combinées et structurées en tant que graphe.\n\nAvec les solutions graphiques existantes, les analystes rencontraient des difficultés pour adapter leurs solutions, car une grande partie de leurs données étaient trop volumineuses. Katana Graph a développé la capacité de s'adapter à l'échelle et de se concentrer également sur l'apprentissage automatique.\n\nAu début des bases de données de type graphique, les grandes entreprises telles qu'Amazon et Facebook ont construit en interne leurs propres bases de données graphiques, effectuant leur modélisation et leur apprentissage automatique. Puis vinrent les versions grand public de plateformes telles que Neo4j et TigerGraph, destinées à des cas d'utilisation généraux. Le défi résidait dans le fait qu'elles étaient centrées sur la base de données et moins sur l'analyse et l'apprentissage automatique, les processus et les calculs graphiques réels. Elles étaient limitées à être une sorte de magasin de données, axées sur l'ingestion et les opérations CRUD, et moins sur les données elles-mêmes.\n\nIl existe trois types différents de domaines informatiques de graphes. Le premier est la requête graphique, la base de données graphique et les opérations CRUD. Le deuxième concerne l'analyse et l'exploration de graphes avec des algorithmes tels que PageRank ou le clustering, qui gagnent en popularité. Le troisième domaine concerne l'IA et l'apprentissage automatique de graphes. C'est là que les réseaux de neurones graphiques interviennent. Il existe des solutions ponctuelles qui résoudront des parties spécifiques de ces domaines, mais Katana Graph se situe à l'intersection de ces derniers.\n\nChacune des trois plateformes est importante. Pour faire de l'apprentissage automatique, vous avez besoin des deux autres domaines. Lorsque les données sont d'abord ingérées, elles doivent subir de nombreuses transformations pour les préparer à l'apprentissage automatique, donc si vous n'avez pas tout cela dans une seule solution, le pipeline sera lent, envoyant les données à l'extérieur et les rapatriant. Il est plus efficace d'itérer rapidement sur l'ensemble du pipeline. Cela réduit également le risque de perte de données car vous diminuez le nombre de fois où vous transformez les données.\n\nDe plus, puisque Katana Graph est une plate-forme nativement cloud, vous pouvez mettre en pause, sauvegarder un point de contrôle, réduire le cluster et le relancer ultérieurement exactement où vous vous êtes arrêté.\n\nUne bonne démonstration consiste à détecter les fraudes avec un ensemble de données de transactions Bitcoin. La plateforme ingère les données, qui sont structurées, de sorte que les portefeuilles Bitcoin soient les nœuds du graphe, et les transactions entre eux les arêtes. C'est un graphe simple. L'idée est de prédire si un portefeuille Bitcoin est frauduleux. Les portefeuilles illicites liés au blanchiment d'argent, au trafic de drogue, etc. ont été étiquetés. Lorsqu'un nouveau compte arrive, la tâche consiste à prédire s'il est frauduleux. La partie difficile consiste alors à pré-traiter les caractéristiques numériques des comptes. Un ensemble d'API a été conçu pour résoudre ce problème. Toutes les étapes que les scientifiques des données effectuent pour préparer leurs caractéristiques sont réalisées ici. À partir de là, le graphe est prêt à être intégré dans le modèle d'apprentissage automatique, où il est entraîné, puis, en utilisant des réseaux neuronaux, on peut apprendre à classer les comptes.\n\nUn avantage de la plateforme est que les scientifiques des données et les ingénieurs peuvent travailler à partir d'une même plateforme au lieu de rassembler les éléments ensemble.\n\nUn autre avantage est le coût total de possession. Contrairement à d'autres plateformes, vous n'avez pas besoin de maintenir la base de données graphique en marche tout le temps. Étant donné que les pipelines Katana Graph sont conçus avec une séparation du stockage et du calcul, vous pouvez facilement créer un cluster, effectuer un traitement par lot au préalable, puis exécuter une inférence dans un système distinct tout en exploitant ce qui a été généré dans le graphe.\n\nKatana Graph est également plus rapide avec de grands ensembles de données car il ne charge pas toutes les données au préalable, mais dispose d'une manière innovante et dynamique de charger les données au fur et à mesure que vous en avez besoin lorsque vous travaillez dans le pipeline.\n\nLes analyses de base sont bien plus faciles sur une base de données graphique plutôt que sur une base de données relationnelle. Si vous disposez de dix ensembles de données différents, il peut être fastidieux et sujet aux erreurs pour un analyste de déterminer comment les joindre pour rédiger une requête dans une base de données relationnelle. Avec un graphique, vous disposez d'un modèle unique, déjà prédéfini et construit, de sorte que les questions seront bien plus faciles car les données sont déjà connectées. Vous pouvez intuitivement voir comment les informations sont liées.\n\nL'une des nouvelles fonctionnalités de Katana est l'importateur de cadres de données Dash. Dash est un cadre communément utilisé par les scientifiques des données pour le traitement parallèle de cadres de données. Les scientifiques des données peuvent travailler avec le cadre de données qu'ils utilisent déjà et l'ingérer directement dans Katana Graph pour une expérience fluide et simplifiée.\n\nLe DevOps est une grande partie de ce que Katana essaie de faciliter avec leur plateforme. Ils s'intègrent facilement aux pipelines d'apprentissage existants. Lorsque les réseaux neuronaux graphiques sont exécutés, les embeddings peuvent être exportés. Ces caractéristiques qu'un graphe génère peuvent être utilisées par un processus d'apprentissage automatique en aval. Ainsi, l'intégration devient beaucoup plus simple et plus facile à opérationnaliser et à mettre en production.\n\nAu cours des cinq prochaines années, Greg envisage que des organisations telles que les banques disposent de référentiels centralisés permettant d'analyser les données clients, de marketing ou de crédit à des fins multiples. Les résultats obtenus à partir des modèles d'apprentissage automatique pourraient être utilisés à la fois pour évaluer les risques de crédit et détecter les fraudes, par exemple. Au lieu d'utiliser des ensembles de données cloisonnés avec beaucoup de réplications et de redondances entre eux, il existerait un modèle commun synchronisé au sein d'un graphique.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Steck"],"link":"/episode-EDT106-fr","image":"./episodes/edt-106/fr/thumbnail.jpg","lang":"fr","summary":"Darren Pulsipher, architecte en chef des solutions du secteur public chez Intel, et Greg Steck, directeur principal des solutions sectorielles chez Katana Graph, discutent des avantages de la plateforme d'intelligence graphique de Katana."},{"id":9,"type":"Episode","title":"Sécurisation des infrastructures critiques.","tags":["criticalinfrastructure","hotms","irdeto","otsecurity","edge","cybersecurity","technology","process"],"body":"\r\n\r\nCarla est originaire du Mexique et vit à Amsterdam après avoir vécu dans plusieurs pays, dont l'Allemagne, au cours des quatre dernières années. Elle a étudié l'ingénierie industrielle et mécatronique, puis après avoir travaillé quelques années dans le domaine, elle a obtenu un master en ingénierie des systèmes de mobilité, où elle s'est spécialisée dans les véhicules autonomes.\n\nLa définition de l'infrastructure critique de l'OT est tout point qui peut déclencher le chaos dans le monde réel. Cela diffère grandement de l'infrastructure informatique, car l'IT manque d'infrastructure vitale. Dans le monde de l'OT, des personnes peuvent mourir si des choses tournent mal. Il peut indubitablement y avoir du chaos dans le monde de l'IT, mais le problème peut être résolu en réparant les choses. Lorsqu'il y a du chaos dans des infrastructures critiques telles que les transports, des accidents peuvent se produire et des vies sont en jeu.\n\nIl y a eu une hausse de l'importance de la sécurité dans les infrastructures critiques au cours des cinq dernières années, et certainement pendant la pandémie de COVID ; dans certains cas, les infrastructures critiques ont été attaquées de manière implacable. Cela peut être attribué à l'ennui et à une créativité accrue des individus, mais également à une augmentation de la surface d'attaque suite au passage soudain au travail à distance, ce qui a compromis certaines des mesures de sécurité précédemment mises en place.\n\nDans le domaine des transports, une partie du problème réside dans l'augmentation de la connectivité, qui expose à des risques potentiels d'attaques. Les clients souhaitent davantage de services, et les entreprises veulent accéder à plus de données et d'informations. Cela implique d'ouvrir davantage le réseau de transport. La séparation physique, qui offrait autrefois une protection, est en train de diminuer.\n\nL'industrie ne peut pas utiliser les mesures de sécurité que l'informatique utilise depuis des années, car l'informatique et l'OT sont totalement différents. L'informatique est généralement standardisée, tandis que l'OT ne l'est pas. L'OT possède un écosystème massif avec des différences significatives entre les appareils qui suivent d'autres protocoles et implémentations. Par exemple, chaque pays a des implémentations différentes. L'OT est également à un autre niveau parce que des vies sont littéralement en jeu.\n\nUne autre différence entre les infrastructures critiques des technologies de l'information (TI) et de l'opérationnel (TO) réside dans le fait qu'en TI, si un problème survient dans votre réseau, vous pouvez l'isoler voire le fermer entièrement et transférer la charge de travail ailleurs. Ce n'est pas possible dans les infrastructures critiques, donc l'approche est différente.\n\nLe danger réside dans le fait que les réseaux IT et OT se sont effondrés à cause de la volonté d'une plus grande connectivité, et en même temps, il y a eu une augmentation des menaces cybernétiques. Irdeto cherche à sensibiliser l'industrie aux complexités de ces problèmes et à offrir des solutions. Tout est question de solutions préventives, plutôt que de réagir après qu'une catastrophe se soit produite.\n\nCarla affirme que les organisations doivent avoir des experts en matière de sécurité. Développer la sécurité en interne sur la base de normes imposées ne suffit pas, car les normes sont en retard par rapport au développement.\n\nIrdeto sécurise les infrastructures critiques depuis 50 ans. Ils ont environ 1 000 employés, dont 70 pour cent travaillent en recherche et développement pour rester en tête des attaques qui se produisent aujourd'hui et celles qui peuvent exister à l'avenir. Irdeto s'efforce d'être en avance d'un pas ou d'être à l'épreuve du futur. Leurs services évoluent au rythme du système.\n\nIrdeto peut aider les clients qui savent ce dont ils ont besoin, que ce soit des PKI, des clés et des attestations, la gestion du cycle de vie ou la protection des logiciels. Ils peuvent également aider les clients qui ne savent pas ce dont ils ont besoin et proposer des solutions durables face à l'évolution des menaces.\n\nPour plus d'informations sur Irdeto, rendez-vous sur leur site web www.Irdeto.com/connected-transport pour en savoir plus sur leur activité.\n\nproduits ou les contacter directement.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Carla Trevino"],"link":"/episode-EDT107-fr","image":"./episodes/edt-107/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte en chef des solutions du secteur public chez Intel, et Carla Trevino, architecte des solutions chez Irdeto, parlent de l'importance de la sécurité dans les infrastructures critiques."},{"id":10,"type":"Episode","title":"Histoire des communications avancées","tags":["5g","cellphone","comms","wifi6"],"body":"\r\n\r\nLa première génération de technologie de téléphone portable, le système avancé de téléphone mobile (AMPS), a été développée à la fin des années 70 et au début des années 80. Au début des années 80, passer un appel depuis sa voiture avec un téléphone encombrant était un luxe. Le luxe de passer un appel depuis un appareil mobile est rapidement devenu une nécessité.\n\nDans les années 90, la technologie a progressé avec le développement de la norme du Système Mondial de Communications Mobiles (GSM) pour décrire les protocoles de la 2G, qui est devenue la norme mondiale à partir du milieu des années 2010. La 2G a commencé à transformer le téléphone mobile en quelque chose de plus puissant qu'un simple outil pour passer des appels, en ajoutant les SMS et même la possibilité de jouer à des jeux.\n\nLa 3G a été lancée au début des années 2000 et a apporté avec elle quelques capacités de données naissantes avec Internet, qui est encore à ses débuts. Le Wi-Fi n'était pas largement disponible, mais vous pouviez, par exemple, accéder au réseau de données d'un opérateur en connectant un téléphone à un ordinateur portable. Bien sûr, vous pouviez faire le minimum avec des vitesses de modem ou DSL.\n\nAvec la 4G, la technologie est passée à une norme unifiée, fusionnant CDMA et GSM en un seul LTE sous le projet de partenariat de 3ème génération (3GPP). Chaque opérateur a commencé à adopter cette norme commune. C'est alors que le haut débit s'est répandu. Leland attribue les progrès de l'économie au cours des années 2010 à la 4G, permettant à des entreprises telles qu'Amazon, Netflix et Uber, ainsi qu'à des plateformes telles que YouTube, Google et Facebook, d'exister et de prospérer.\n\nLeland parle de la 5G en termes de ce que les opérateurs ont déployé. La 4G et la 5G sont liées car elles font partie de la même spécification de ligne de sortie. Quatorze se termine par ce que nous appelons la 4G LTE avancée. Quinze commence avec le 5G NR. Dans ce crossover, il y a un objectif commercial et une stratégie pour adopter la nouvelle technologie dans le cadre de la norme. L'objectif commercial est que les entreprises ont déjà investi dans leurs réseaux 4G, donc les composants actuels du cœur de paquets évolué et du RAN des réseaux 4G sont toujours en place. Ils ajoutent une boîte RAN 5G avec une fréquence différente mais qui est toujours connectée au cœur 4G, appelée non autonome.\n\nDarren précise que la 4G a été révolutionnaire car elle a permis de débloquer de nombreuses nouveautés et nécessitait un tout nouvel équipement, tandis que la 5G est plutôt évolutive, car elle a également ouvert de nouvelles possibilités. Néanmoins, la technologie sous-jacente repose sur le même matériel et le même cœur.\n\nCela fait partie du schéma de modulation que 5G fournit dans l'interface aérienne, mais l'architecture est différente ; elle est virtualisée sous 5G par rapport à une approche plus propriétaire sous 4G. Cela entraîne de nombreuses fonctionnalités qui deviennent une partie des déploiements 5G.\n\nUn exemple est quand un opérateur a déployé un réseau 4G en plaçant une boîte RAN à côté d'une ancienne boîte 3G. De nombreuses entreprises, telles que Sprint, ont conservé leurs boîtes 3G et leur réseau CDMA pendant des années. En réalité, la 4G était simplement une autre boîte installée à côté d'une boîte 3G. La 5G prend cette boîte propriétaire et permet la répartition des fonctions de cette boîte sur un réseau virtualisé. Une partie de la bande de base de la 5G peut maintenant être définie par logiciel à une échelle multiple par rapport à une simple localisation, boîte ou site.\n\nCela signifie que vous pouvez ajouter des fonctionnalités à votre réseau sans remplacer le matériel. En passant à des réseaux autonomes, cependant, vous pouvez prendre un réseau 5G et faire quelque chose sur place. Par exemple, supposons que vous ayez un gratte-ciel plutôt que de dépendre de la couverture du réseau d'une antenne située à l'extérieur avec un cœur chez le fournisseur ou une station de commutation. Dans ce cas, vous pouvez développer un réseau sur place construit à l'intérieur de ce bâtiment qui propage la couverture et les services de données.\n\nCe réseau autonome ouvre de nombreuses nouvelles capacités et permet à de nouveaux acteurs d'émerger. Il permet également aux organisations telles que le gouvernement fédéral et le Département de la Défense d'adopter la technologie pour leurs cas d'utilisation. Elles bénéficient d'une plus grande flexibilité lorsqu'elles ne dépendent pas trop des opérateurs.\n\nAnna constate que en plus des nouveaux joueurs et des nouvelles capacités sur site, il est également possible d'utiliser le spectre CBRS. La façon dont il est géré est complexe, mais il existe une option non prioritaire que vous pouvez utiliser gratuitement, ainsi qu'une option prioritaire, le spectre de la marine, que vous pouvez acheter si vous avez besoin d'une absence de perturbation. Certaines installations de fabrication importantes utilisent le spectre CBRS, soit en travaillant avec un opérateur principal qui ne facture pas pour la portée, soit en travaillant avec un nouvel entrant qui mettra en place un réseau autonome sur site avec le CBRS. Il s'agit d'un modèle très différent, et il existe de réels avantages liés à la longueur d'onde et à la complexité des systèmes que vous pouvez mettre en place avec la 5G par rapport au Wi-Fi.\n\nIl y a encore quelques avantages au Wi-Fi, mais la mise en place d'un réseau Wi-Fi robuste peut être difficile, surtout si vous déplacez de grandes pièces en métal. Si vous avez une configuration définie, il est logique d'opter pour le Wi-Fi 6, surtout si cela est économiquement rentable.\n\nLa demande génère le changement ; la plupart des utilisateurs finaux sont à l'aise avec la 4G sur leurs appareils personnels. Alors pourquoi passer à la 5G ? La valeur apportée par la 5G ne réside pas nécessairement dans des débits plus élevés et une latence réduite ; ces services sont fournis à grande échelle car ils sont virtualisés. La 5G est plus axée sur les logiciels par rapport à la 4G, qui repose davantage sur des boîtiers propriétaires et du matériel. La 5G peut être virtualisée à de nombreux endroits. Le portefeuille de fréquences est dynamique, et vous pouvez utiliser des bandes non autorisées, des bandes sous licence et la technologie CBRS, offrant ainsi de nombreuses options supplémentaires.\n\nDécouvrez la deuxième partie de cette interview [ici](episode-EDT109).\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown","Anna Scott"],"link":"/episode-EDT108-fr","image":"./episodes/edt-108/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren parle aux invités réguliers d'Intel, Leland Brown, ingénieur principal : directeur technique des communications avancées, et Dr Anna Scott, architecte principal des technologies de pointe pour le secteur public, de l'histoire des communications avancées."},{"id":11,"type":"Episode","title":"Faire progresser les opérations avec la 5G","tags":["5g","comms","wifi6"],"body":"\r\n\r\nLa 5G ouvre de nombreuses possibilités. Vous pouvez désormais mettre en place des réseaux privés, qui sont définis par logiciel, afin de pouvoir ajouter davantage de fonctionnalités à votre réseau. Où cette technologie facilitatrice nous mène-t-elle ?\n\nAnna dit que même s'il est encore tôt du point de vue de la bande passante et de la latence, il y a de réels avantages à la 5G, tels que l'ouverture du spectre comme le CBRS, la possibilité de tirer parti de l'équipement utilisateur existant et la possibilité pour les clients d'avoir un accès mobile.\n\nUn cas d'utilisation qui n'est pas séduisant mais qui a de vastes répercussions est que vous pouvez emmener votre ordinateur portable sur le sol de l'usine et l'utiliser pour une connectivité complète. Au lieu d'aller sur le sol de l'usine avec un bloc-notes et un stylo et de transférer les informations au bureau, vous pouvez fusionner les deux environnements. Les normes 5G permettent cela, mais ce n'est pas nécessairement un déploiement complet de la 5G.\n\nUne évolution qui est en train de se produire est la capacité de diffuser en continu des vidéos haute définition à partir d'une caméra sans fil via la technologie 5G et d'avoir une latence suffisamment faible pour pouvoir effectuer des analyses en temps réel. Actuellement, peu de caméras 5G peuvent fonctionner dans cet environnement, c'est pourquoi les caméras sont souvent câblées à proximité d'un calcul en périphérie couplé pour obtenir cet aspect en temps réel. Cependant, cette option de diffusion avantageuse sera bientôt disponible.\n\nUn autre exemple d'avantage de la 5G est l'utilisation de 10 à 20 casques AR au lieu d'un ou deux avec le Wi-Fi. Le point clé est le MEC (Mobile Edge Compute) qui permet d'avoir les applications sur site plutôt que de devoir retourner au commutateur ou au cœur du fournisseur et avoir ce temps de RAN.\n\nComprendre comment les fréquences sont déployées est essentiel dans les cas d'utilisation, car certains opérateurs ont déployé la fréquence millimétrique malgré les défis ; les fréquences n41 et n42 réagissent différemment dans le monde réel. Ainsi, dans les déploiements sur site, le MEC et la conception RF sont extrêmement importants.\n\nUn cas d'utilisation convaincant pour la 5G en dehors de l'usine est l'utilisation de drones lors des interventions d'urgence. Un prédécesseur précoce de la capacité future à déployer des drones et à évaluer une zone endommagée consiste à connecter les drones, à comprendre la mission pour effectuer une surveillance aérienne, puis à regrouper toutes ces données. Bien que nous ne soyons pas encore en mesure de diffuser en direct des vidéos provenant de plusieurs drones et de les assembler, nous sommes proches de collecter, de combiner et d'analyser ces données, mais pas encore en temps réel.\n\nUn autre cas d'utilisation consiste à utiliser la 5G, l'intelligence artificielle, la modélisation, la simulation et le calcul en périphérie pour la formation dans de nombreuses industries, y compris le ministère de la Défense. Il y a un énorme avantage à créer une simulation d'entraînement réaliste sans mettre la personne en danger ou dépenser des sommes massives d'argent pour une formation en direct.\n\nAutant que la 5G permette ce genre de cas d'utilisation, tout dépend de la connexion de la 5G à un centre de calcul à la périphérie (MEC) au lieu de la 5G qui se connecte directement au cloud. La physique entre en jeu. Vous avez besoin d'une latence extrêmement faible, donc vous ne pouvez pas avoir une architecture qui va d'un casque à réalité virtuelle au cloud, puis vers un MEC, et enfin vers la visualisation. Il doit aller du casque à réalité virtuelle au MEC, où le traitement en temps réel se produit. Ensuite, vous pouvez partager les données via le cloud pour une expérience en temps réel.\n\nIl est également possible de relier ou de regrouper les MEC entre eux, de sorte que les données n'aient jamais besoin d'aller dans le cloud. Les MEC peuvent effectuer tous les traitements et les analyses directement sur le switch. Cela pourrait permettre des avancées telles que les bâtiments et les villes intelligentes. Ce genre de technologie permettant la 5G est un véritable tremplin pour des changements considérables dans l'industrie.\n\nLeland souligne que l'histoire des nouveaux réseaux est l'informatique distribuée. Tout est connecté grâce à la connectivité sans fil, mais les points de calcul sont répartis à travers le paysage, où les applications se trouvent en périphérie et permettent les cas d'utilisation. Ce vers quoi nous nous dirigeons, c'est le calcul sans fil un-à-un.\n\nQuel rôle joue Intel dans la 5G ? Cela va bien au-delà de simplement fournir des puces. Étant donné que la 5G est une conception logicielle, Intel a permis à l'écosystème de construire ou de concevoir en utilisant sa technologie L15. Lors de la transition de la 4G à la 5G, Intel a pris le bloc fonctionnel du réseau d'accès radio (RAN), appelé FlexRan, et a permis aux entreprises de concevoir leurs architectures de bande de base et de les virtualiser. L'écriture de l'architecture de référence FlexRan a grandement facilité la tâche aux nouveaux entrants en leur fournissant un point de départ solide.\n\nDu côté matériel, Intel a consacré de nombreux cycles pour s'assurer que le matériel commercial disponible sur étagère fonctionnerait bien pour prendre en charge tous les bandes de base, les applications RAN et les serveurs. Les nouveaux systèmes qui sont utilisés en abandonnant les systèmes propriétaires doivent pouvoir être facilement pris en charge par le même type de serveur utilisé dans le cloud et le centre de données, car désormais vous bénéficiez d'une mise à l'échelle et d'un avantage économique.\n\nCela fera baisser les prix et stimulera davantage l'innovation dans l'industrie.\n\nDécouvrez la première partie de cette interview [ici](episode-EDT108).\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown","Anna Scott"],"link":"/episode-EDT109-fr","image":"./episodes/edt-109/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions d'Intel; Leland Brown, ingénieur principal: Directeur technique des communications avancées; et le Dr Anna Scott, architecte principal de la périphérie pour le secteur public, discutent de l'histoire des communications avancées et des cas d'utilisation future avec la 5G. Partie deux de deux."},{"id":12,"type":"Episode","title":"Enseignement à distance et apprentissage.","tags":["remotelearning","compute","technology","people","edge","telelearning"],"body":"\r\n\r\n## Considérations émergentes\n\nLes enseignants, le personnel, les parents et les élèves sont tous confrontés à des défis différents dans le passage soudain à l'apprentissage à distance. Du côté informatique des districts scolaires, il y a tout un tas de considérations émergentes. Que devons-nous faire avec le scénario de l'appareil personnel (BYOD) provenant de réseaux à confiance zéro ? Comment protéger la vie privée et gérer la sécurité avec tous les différents nouveaux modes de communication entre les enseignants, le personnel, les parents et les élèves ? Comment maintenir une expérience en classe simplifiée et gérée ? Comment offrir un soutien lorsque la structure traditionnelle du service d'assistance n'existe pas ? De plus, nous ne pouvons pas oublier qu'il y a un aspect social important qui doit garantir une expérience fluide. Un élève de troisième année distraité par des problèmes techniques lorsqu'il doit se connecter avec les enseignants et ses camarades aura une expérience d'apprentissage à distance réduite.\n\n## Systèmes de services et plateformes pour l'éducation\n\nIl y a toujours eu de la complexité avec les différentes couches de services et de plateformes, par exemple, la suite de productivité avec G Suite et Office 365. La question maintenant est ce que nous pouvons faire avec les systèmes de gestion de l'apprentissage dans cette approche en couches dans l'intégration du système d'information des étudiants ? Nous devons examiner comment nous exploitons nos capacités en termes de scalabilité. Nous devons considérer différentes solutions d'infrastructure en tant que service (IAAS) et de plateforme en tant que service (PAAS), les services de stockage, la confidentialité et la sécurité, et bien sûr, les plateformes sous-jacentes qui les soutiennent.\n\nAuparavant, les outils de visioconférence et de collaboration avaient une utilisation limitée, peut-être pour inviter un conférencier, par exemple. Maintenant, cependant, ils sont très largement utilisés comme outils principaux et ont ajouté à la complexité du système.\n\n## Options d'hébergement de service\n\nIl existe deux modes principaux qui sont utilisés pour accéder aux services : l'appareil en tant que portail vers les services (logiciels, infrastructures ou plateformes), et l'appareil faisant partie du réseau interne. Auparavant, la plupart relevaient de la dernière catégorie, où il y avait des préoccupations limitées concernant des éléments tels que la mise à jour et la conformité aux politiques, car les appareils étaient constamment connectés aux sites internes de l'école, qu'il s'agisse d'appareils personnels ou appartenant au district. Maintenant, avec les différents types de connectivité, nous devons nous préoccuper de la scalabilité de la bande passante et de la manière dont nous la mettons en œuvre.\n\n## Dispositif en tant que portail vers des services\n\nDes plateformes telles que G Suite, Office 365 et celles dédiées à la visioconférence et à la collaboration posent problème car elles créent une dépendance envers un tiers. Les districts scolaires n'ont pas le contrôle sur la sécurité, la confidentialité et les performances. Il est important de reconnaître que la connexion à ces services de cloud est liée à l'hôte interne, qu'il s'agisse d'un cloud privé ou d'un centre de données local, qui offre des services sous-jacents tels que la gestion des identités, les systèmes d'information des étudiants, éventuellement le filtrage du contenu, etc. Les avantages, cependant, sont une diminution du trafic entrant vers le centre de données ainsi qu'une évolutivité et une facilité de gestion inhérentes.\n\n## Appareil en tant que partie du réseau/Cloud privé\n\nMettre en place un réseau privé virtuel (VPN) est une nouvelle idée pour la plupart des districts scolaires. Les entreprises utilisent les VPN depuis un certain temps déjà et les districts scolaires devront peut-être emboîter le pas pour répondre aux nouveaux besoins de l'apprentissage à distance. Quelques inconvénients d'un VPN sont la congestion du réseau, la scalabilité et le trafic provenant de réseaux à confiance zéro. Le principal avantage est qu'il fonctionne comme une extension du réseau interne, ce qui permet d'étendre la gestion de la sécurité et le chiffrement du trafic aux clients VPN. Un autre grand avantage est l'accessibilité à tous les services nécessaires en interne.\n\n## Contraintes émergentes<h2>\n\nLes goulots d'étranglement sont différents pour l'apprentissage à distance. Pour les entreprises, lorsque la main-d'œuvre est passée en virtuel, un taux prévu était d'environ 10% de charge VPN. Pour l'éducation, ce chiffre sera considérablement plus élevé, créant un possible goulot d'étranglement VPN. La scalabilité des services hébergés est un autre domaine à considérer. Même si les services se trouvent dans un cloud public, ils se connectent à un cloud privé ou à un centre de données sur site pour des choses telles que l'authentification unique, les informations sur les étudiants, le trafic et même le filtrage de contenu. La bande passante d'accès Internet dédiée, ainsi que la gestion de la transmission de bande passante dans le centre de données, sont des considérations importantes.\n\nL'accès à un composant de service d'assistance doit également être scalable pour éviter les goulots d'étranglement.\n\nLa principale préoccupation des districts scolaires actuellement, cependant, est de donner accès à tous leurs élèves. Pour certains élèves, il y a un goulot d'étranglement simplement pour se connecter à Internet à la maison. Lorsque ce problème s'ajoute à toutes les différentes couches, les goulets d'étranglement émergents deviennent très complexes.\n\nPour lutter contre ces problèmes potentiels, les districts scolaires doivent prendre le temps et les ressources nécessaires pour créer une architecture solide qui sera résiliente plutôt que de construire un fouillis improvisé.\n\n## Scalabilité\n\nHistoriquement, les districts scolaires se sont développés dans le centre de données et ont étendu certains de ces services aux sites scolaires. Ainsi, l'architecture existe généralement pour accueillir l'agrégation du corps professoral, du personnel et des étudiants pour des choses comme l'authentification, la gestion des correctifs, la distribution de logiciels, etc. Maintenant que l'extension se fait dans la salle de classe virtuelle, cela nous ramène à la mise à l'échelle des services dans le centre de données.\n\n## Trouver l'équilibre\n\nAuparavant, seuls les deux facteurs d'on-prem dans le centre de données du bureau central et les liens LAN vers les sites scolaires étaient en jeu. Maintenant, il y a un facteur supplémentaire, celui d'accueillir toutes les salles de classe virtuelles, c'est-à-dire le salon de chaque élève et professeur. Comment pouvons-nous nous adapter à cela ? Tout dépend du concept principal de trouver l'équilibre entre ce dont vous avez besoin pour votre district scolaire. Chaque district est différent en termes de taille et de maîtrise de la technologie, et il y a de nombreux éléments en mouvement.\n\n## Résolution des goulots d'étranglement.\n\nQue peut Intel faire pour aider à résoudre ces goulots d'étranglement que nous avons identifiés ?\n\nLa gestion des points de terminaison et la sécurité des points de terminaison sont là où le service d'assistance informatique intervient. Nous cherchons des moyens de pouvoir offrir une assistance à distance plutôt que des visites régulières sur les sites scolaires.\n\nNous pouvons examiner le centre de données et l'infrastructure et élaborer une stratégie qui nous permettra de mettre en échelle le logiciel défini par réseau et l'infrastructure. De plus, nous pouvons également intégrer une explosion de cloud d'infrastructure en tant que service, tout en tenant compte des schémas de trafic.\n\n## Composants Intel pour résoudre les goulots d'étranglement\n\nIntel peut aider dans trois grandes catégories : calcul, stockage et réseau. Lorsque nous examinons le renforcement de l'infrastructure définie par logiciel et les considérations importantes, cela tourne autour de cet ordinateur avec des processeurs Intel, des produits de stockage et des capacités réseau. Plutôt que d'être liée et limitée par des interfaces physiques et des appareils, l'infrastructure définie par logiciel peut se développer et intégrer d'autres ressources de calcul, de stockage et de réseau. Personne ne sait exactement quel sera le paysage lorsque nous retournerons à l'école, et cette infrastructure définie par logiciel est dynamique et offrira la plus grande flexibilité.\n\nIl existe de nombreuses options pour les districts scolaires afin de construire un environnement viable et sécurisé pour l'apprentissage à distance. Bien que nous ayons abordé les sujets de la confidentialité et des menaces et solutions de sécurité dans cet épisode, la prochaine fois nous plongerons profondément dans ces sujets importants.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Erin Moseley","Grant Kelly"],"link":"/episode-EDT11-fr","image":"./episodes/edt-11/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Erin Moseley, Sr. Account Exec pour l'Éducation chez Intel, et Grant Kelly, Solution Architect pour l'Éducation chez Intel, rejoignent Darren pour parler des défis de l'apprentissage à distance et de l'enseignement, ainsi que des changements massifs que les districts scolaires, les enseignants, les parents et les étudiants absorbent pendant la pandémie de Covid-19. Découvrez comment les étudiants et les enseignants se connectent avec de nouvelles technologies et de nouvelles méthodes d'apprentissage."},{"id":13,"type":"Episode","title":"Sécurisation de la chaîne d'approvisionnement","tags":["securesupplychain","cybersecurity","supplychain","chipsact","policy","process","compute"],"body":"\r\n\r\nDans cet épisode, Darren discute avec le lieutenant-général Thomas Horlander, qui a récemment rejoint l'équipe du secteur public d'Intel, de la chaîne d'approvisionnement en microélectronique et de la sécurité nationale.\n\nThomas a rejoint l'armée en 1983 après avoir obtenu son diplôme de baccalauréat en finance et avoir conclu que le secteur privé n'était pas fait pour lui. Il a rejoint l'armée pour ce qu'il pensait être une carrière de trois à cinq ans et s'est finalement engagé pendant 39 ans, occupant le poste de Contrôleur de l'armée à la fin de sa carrière. Il a pris sa retraite il y a environ un an et a rejoint Intel.\n\nThomas est inspiré par les grands individus, la culture et la mission importante chez Intel. Il apprécie le rôle fondamental que joue Intel dans la vie quotidienne de chacun et l'opportunité d'influencer l'avenir du pays.\n\nLorsque Thomas a rejoint l'armée en 1983, la microélectronique était encore à l'état embryonnaire ; ils n'avaient même pas d'ordinateurs. En tant que jeune officier, il s'inquiétait de savoir s'il avait suffisamment de piles D pour faire fonctionner les radios. Ainsi, Thomas se considère comme un \"immigrant numérique\" car il a vécu l'évolution qui s'est produite avec le silicium.\n\nLa société et l'armée sont maintenant dépendantes du silicium, et la chaîne d'approvisionnement autour des microélectroniques est critique : il s'agit d'un enjeu de sécurité nationale. Thomas affirme que l'industrie des microélectroniques occupe une place centrale aux côtés de l'industrie pétrolière en tant que centre de gravité en ce qui concerne la sécurité nationale, mondiale et la stabilité économique.\n\nDans l'armée, les véhicules et les systèmes d'armes sont tous équipés de microélectronique. Ils permettent à l'armée, par exemple, d'être plus précise et plus meurtrière tout en étant plus légère, d'avoir un système de localisation plus précis et des communications plus fiables.\n\nNotre dépendance aux microélectroniques a stimulé l'adoption récente de la loi Chips. Bien que le besoin soit apparu avant la crise en Ukraine, le fait de voir ce que l'Ukraine a été capable de faire grâce aux microélectroniques a mis en évidence la nécessité de sécuriser la chaîne d'approvisionnement.\n\nThomas a été étudiant en sécurité nationale toute sa carrière, et il l'envisage de manière plus holistique que simplement la défense et le rôle des forces armées. Pour lui, la sécurité nationale implique une bonne gouvernance et l'État de droit. Cela englobe une économie saine et fonctionnelle, l'enseignement pratique, les soins de santé, et bien sûr, les forces armées. Presque toutes les professions de la société américaine jouent un rôle dans la sécurité nationale. À partir de cette perspective, pratiquement chaque aspect nécessaire à la sécurité nationale repose sur la microélectronique.\n\nLa loi Chips était nécessaire, selon Thomas, en raison du déséquilibre massif au sein de l'écosystème de l'industrie. En même temps, bien que ce ne soit pas la solution ultime pour la redistribution de l'équilibre, la loi Chips est une première étape essentielle qui aura un impact sur la sécurité nationale. Lorsque l'on examine les activités principales de l'industrie des microélectroniques, depuis l'origine des éléments de terres rares jusqu'aux fabricants d'équipements de conception et de fabrication, il est évident que cela forme un incroyable ensemble d'activités et c'est pourquoi il est si difficile d'en avoir une vision claire. Une puce électronique peut changer de mains dix fois au cours d'un processus de fabrication.\n\nCOVID, de bien des façons, a révélé cette chaîne d'approvisionnement complexe et fragile lorsque, par exemple, des usines ont été fermées en Malaisie, en Irlande ou en Chine en raison d'une épidémie de COVID, et tout à coup, vous ne pouvez pas livrer une voiture parce qu'elle n'a pas de puce électronique. La plupart des gens ne réalisent pas le déséquilibre mondial significatif qui existe actuellement. Seulement huit pour cent du silicium est fabriqué aux États-Unis. Soixante-dix ou quatre-vingts pour cent est fabriqué en Asie du Sud-Est, précisément dans trois pays : la Chine, la Corée du Sud et Taïwan.\n\nAvec cette connaissance, il est évident que rééquilibrer l'écosystème de la chaîne d'approvisionnement mondiale et rétablir la capacité et les compétences aux États-Unis est d'une importance primordiale. Aucune industrie ne devrait avoir de points de défaillance uniques, une préoccupation dans l'industrie de la microélectronique.\n\nLe gouvernement fédéral, la base industrielle de la défense et l'écosystème commencent tous à prendre conscience de ce problème, et le \"Chips Act\" est représentatif de cette reconnaissance que nous devons agir. Thomas sait que six entreprises ont déclaré qu'elles investiront dans des usines de fabrication aux États-Unis au cours des huit à dix prochaines années.\n\nL'une de ces entreprises est, bien sûr, Intel. Intel construit actuellement des usines à Ohio, Arizona et Nouveau-Mexique. La relocalisation de capacités et compétences dans l'industrie des microélectroniques est nécessaire pour la sécurité nationale, mais renforcera également les économies locales et offrira des opportunités aux travailleurs, revitalisant ainsi des communautés entières.\n\nLa perfusion de l'investissement en capital provenant du Chips Act est fondamentalement essentielle car il s'agit d'une course contre la montre. Thomas est optimiste quant à l'avenir de l'industrie et aux mesures prises pour garantir un avenir prometteur et une innovation continue.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Thomas Horlander"],"link":"/episode-EDT110-fr","image":"./episodes/edt-110/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren discute avec le lieutenant général Thomas Horlander, qui a récemment rejoint l'équipe du secteur public d'Intel, de la chaîne d'approvisionnement des microélectroniques et de la sécurité nationale."},{"id":14,"type":"Episode","title":"Réaliser le potentiel des villes intelligentes","tags":null,"body":"\r\n\r\nEric a eu une longue carrière dans les centres de données et, plus récemment, dans le domaine des panneaux d'affichage numériques. Ses passions sont la gestion de projet et le déploiement et l'exploitation de la technologie. Eric et son équipe faisaient partie d'une entreprise en coentreprise avec LG Electronics, qui déployait des panneaux d'affichage numériques extérieurs, principalement à des fins publicitaires dans des zones urbaines denses, des lieux de transit et des rues de la ville. Ils ont beaucoup appris sur d'autres applications dans ces environnements qui nécessitent des ressources informatiques et d'autres formes de connectivité pour améliorer l'expérience apportée par l'IoT. SmartPoint.io est né de cet écosystème et du désir de travailler avec et de stimuler les opportunités et les avancées sur ce marché.\n\nDans le centre de données, Eric faisait partie d'une entreprise de solutions du cycle de vie qui aidait les entreprises à optimiser le stockage des serveurs et le matériel réseau depuis une acquisition, un cycle de rafraîchissement, et tout ce qui se trouvait entre les deux. Son rôle était axé sur l'aide aux distributeurs d'entreprises vendant ces solutions, ainsi que sur la façon dont elles étaient achetées, financées et dépréciées au fil du temps. Ainsi, il a appris non seulement la technologie, mais aussi comment les gens l'utilisaient et géraient cette valeur en tant qu'actif dans leur bilan.\n\nLorsque les gens parlent de villes intelligentes, ils pensent à la résilience, à l'équité, à l'efficacité et à la rentabilité. Les villes fournissent des services essentiels tels que les routes, la sécurité publique, l'eau et l'assainissement. La principale chose qu'une municipalité apporte à ses citoyens commence à inclure la connectivité, la possibilité d'accéder à l'informatique et de tirer parti des avancées technologiques pour fournir des services de base de manière plus efficace. La technologie intelligente a été coûteuse et risquée, et peu de budgets municipaux peuvent y consacrer des fonds.\n\nLe passé d'Eric dans la finance est un atout pour créer et déployer la technologie des villes intelligentes dans le cadre d'un partenariat offrant un modèle abordable. Pouvoir déployer les financements importants actuels pour les projets d'infrastructures de manière résiliente et économiquement durable est l'un des principaux principes de ce que fait SmartPoint lorsqu'il se met en partenariat avec une ville.\n\nPour atteindre la durabilité économique, l'équipe d'Eric a examiné de près l'affichage numérique, principalement pour la publicité, qui génère une énorme quantité de revenus. Certaines des plus grandes entreprises du monde sont principalement financées par les revenus publicitaires, et l'un des principaux produits ou ressources est les personnes à qui elles font de la publicité. La superposition d'attribution, de traits des clients et de ressemblances augmente les revenus.\n\nLes villes possèdent un trésor de valeur dans les personnes, les entités commerciales et les visiteurs qui résident dans leurs biens immobiliers. Ainsi, l'essentiel de ce que fait SmartPoint.io consiste à valoriser ces biens immobiliers pour générer des revenus en les alignant directement avec des solutions qui produisent des résultats concrets dans les services urbains. Pour introduire cette technologie dans les villes, il est nécessaire de bénéficier de l'aide du secteur privé et de la volonté du secteur public de travailler ensemble.\n\nLes revenus publicitaires, en plus de permettre d'accéder aux budgets des services municipaux, permettent un modèle financier qui élève tous les bateaux, en fournissant un transport plus sûr et des retours plus importants sur la publicité. Lorsque vous liez les avantages de la publicité aux personnes auxquelles elle est destinée, vous créez une économie locale circulaire qui augmente la valeur et l'efficacité.\n\nL'informatique en périphérie est une partie importante du produit SmartPoint.io. La boîte d'affichage numérique est également semblable à un centre de données portable. Les données n'ont pas besoin de revenir vers un grand centre de données distant mais sont traitées en périphérie. Par exemple, les données vidéo provenant des caméras omniprésentes dans une ville pour surveiller les flux de circulation sont gourmandes en bande passante. Cependant, le transport des données est considérablement réduit s'il est mis en cache, stocké ou traité localement.\n\nTrois grandes choses à propos de ces boîtes sont qu'elles font un excellent travail pour réduire la latence, les coûts et augmenter la confidentialité. La question de la confidentialité semble contre-intuitive, mais les données vidéo, par exemple, ne voyagent pas sur IP public. L'analyse se fait au point de capture, donc elle n'est pas transportée à travers le réseau, observée ou effectuée sur plusieurs points où la sécurité doit être suivie. Au lieu de l'image, il envoie l'événement \"Une personne marchant dans la rue\". Ces données peuvent être nettoyées et anonymisées beaucoup plus tôt dans le cycle d'analyse avant de quitter la boîte, protégeant ainsi la vie privée des personnes.\n\nLes boîtes pourraient également fournir des services tels que le Wi-Fi ou des points d'accès 5G aux zones mal desservies.\n\nUne fois que les boîtes sont déployées, SmartPoint.io dispose d'une équipe qui surveille et exploite la technologie, tout comme le ferait un centre de données pour une entreprise. Ils sont également partenaires des annonceurs. De nombreux annonceurs souhaitent passer de la publicité statique sur des affiches en papier à la publicité numérique. SmartPoint.io est un excellent partenaire pour ces annonceurs car ils fournissent un financement initial CapEx.\n\nAvec les villes, le modèle est soit basé sur la part de revenus, soit sur un paiement en nature. Cependant, au lieu de simplement partager les revenus bruts, SmarPoint.io produit et exploite des ressources telles que l'infrastructure informatique que la ville peut ensuite utiliser pour traiter les données optiques en périphérie.\n\nL'affichage numérique peut être interactif, ce qui est bénéfique pour les personnes qui cherchent des services, des restaurants, des magasins, des événements et des divertissements. Les locataires commerciaux peuvent l'utiliser comme un portail pour le développement économique car ils peuvent atteindre les personnes qui se promènent. À leur tour, les panneaux fournissent des informations précieuses, telles que le nombre de personnes qui étaient en centre-ville ou si une campagne publicitaire les a effectivement attirées.\n\nCe modèle aidera les villes intelligentes car il permet de différer les coûts d'investissement en capital (CapEx) et les coûts d'exploitation (OpEx) liés à la gestion du matériel déployé dans l'environnement extérieur et aux cycles de renouvellement. Prendre des risques avec une technologie coûteuse et relativement nouvelle n'est également pas un comportement normal pour une municipalité. Par conséquent, établir un partenariat avec le secteur privé, qui dispose d'une appétence significative pour le risque, du savoir-faire et des ressources financières, permet aux villes de réduire les risques et les coûts, et de se positionner plus en avant par rapport à leur niveau de confort habituel.\n\nÀ la fin, Eric déclare que les succès commencent à se construire dans cet espace, et ils se renforcent mutuellement. Tout le monde apprend de nouvelles choses, et c'est une excellente occasion de se rassembler et de construire une communauté, car c'est le fondement des villes.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Eric Hornsby"],"link":"/episode-EDT111-fr","image":"./episodes/edt-111/fr/thumbnail.jpg","lang":"fr","summary":"Dans cet épisode, Darren discute avec Eric Hornsby, PDG de SmartPoint.io, à propos de la technologie pour réaliser le potentiel des villes intelligentes."},{"id":15,"type":"Episode","title":"Opérationnaliser la gestion des processus métier","tags":["bpm","automation","compute","management","camunda","capitalbpm","rpa"],"body":"\r\n\r\nMax se qualifie lui-même de « chercheur raté » car il a abandonné son doctorat en mathématiques, spécialisé en topologie, pour étudier l'informatique. Il a obtenu une licence et une maîtrise, se spécialisant en intelligence artificielle. Ensuite, il a travaillé dans la gestion des processus métiers (BPM), commençant chez Lombardi, qui a été vendue à IBM, ainsi que chez d'autres fournisseurs dans le domaine du BPM. Il y a dix ans, il a pris un risque avec des amis et a fondé Capital BPM.\n\nMax a choisi de se concentrer sur BPM pour deux raisons. Tout d'abord, il a toujours aimé les algorithmes car ils offrent une approche systématique pour résoudre des problèmes complexes. Les algorithmes lui procurent un sentiment de sécurité lors de situations folles.\n\nDeuxièmement, il aime ce qu'on appelle les transformations en mathématiques. Par exemple, si vous avez une forme laide avec de nombreux coins difficiles à mesurer, vous la transformeriez en, disons, un rectangle, puis appliqueriez toutes les théories autour de la mesure des rectangles, la mesureriez et traduiriez la réponse à la forme originale. De la même manière, en BPM, vous pouvez prendre un problème dans un domaine où il peut être facilement résolu. Au lieu de faire d'un problème un micro-problème avec des déclarations F imbriquées compliquées qui sont difficiles à maintenir, vous pouvez le transformer en un problème aplati que vous pouvez voir et attaquer par étapes.\n\nCe processus consiste à zoomer sur Google Maps pour voir où vous avez besoin d'une concentration accrue, puis à zoomer à nouveau pour voir comment cela s'inscrit dans la vision d'ensemble. Le cœur de l'architecture d'entreprise réside dans la capacité de zoomer avant et arrière pour s'assurer que la ligne que vous tracez reste valide.\n\nMax compare son amour pour le BPM dans le domaine de l'informatique aux arts martiaux, qu'il étudie depuis l'âge de six ans. Il affirme qu'il y a des arts martiaux pratiques, tels que le judo et le muay thaï, et des arts plus ésotériques, comme le tai-chi. Il aime les arts martiaux pragmatiques car ils résolvent des problèmes concrets. Il ne se bat plus à mains nues, mais, par exemple, ses compétences en judo lui sont utiles lorsqu'il glisse et tombe. Le BPM est pragmatique car il est la clé pour résoudre un problème commercial. Il croit qu'il y a une véritable valeur à utiliser toutes les théories qu'il a apprises à l'école et à les rendre subordonnées dans la création d'une plateforme commerciale qui permet aux gens de résoudre de manière plus efficace et constante les problèmes quotidiens, offrant ainsi aux individus et à la communauté plus d'opportunités.\n\nC'est un domaine de l'informatique qui peut être lié à la façon dont les gens travaillent. Les choses peuvent être automatisées pour réduire la quantité de tâches répétitives et monotones afin qu'ils puissent se concentrer sur des choses plus importantes. Les gens craignent que l'automatisation ne dévore les emplois de tous les jours, mais en réalité, elle élimine les corvées et libère du temps pour un travail plus critique. Elle peut également créer des emplois. Les mêmes craintes existaient lorsque Ford a introduit les ordinateurs automatisés, mais ils ont finalement créé de nouvelles industries. Le BPM devrait être pleinement adopté plutôt que redouté.\n\nLa meilleure façon de commencer à opérationnaliser les processus est d'utiliser la méthode scientifique pour articuler le problème. En gestion des processus d'affaires, vous dessinez des schémas à l'aide d'un modélisateur de processus d'affaires. Max aime le modélisateur de processus d'affaires Camunda, qui est téléchargeable gratuitement avec juste un peu de notation à apprendre. Dans le programme de modélisation, vous dessinez des étapes qui articulent les différents systèmes et comment ils fonctionnent.\n\nDans un processus de recrutement, par exemple, vous commencez par un groupe qui définit les parties intéressées, comme le candidat, le responsable informatique et les ressources humaines. À l'intérieur de ce groupe se trouvent des \"couloirs de nage\", chaque couloir contenant un acteur capable de réaliser des actions. Vous pouvez les considérer comme des groupes LDAP. Ensuite, vous commencez à mettre en place votre processus métier : d'abord, un candidat postule pour le poste, ensuite les ressources humaines peuvent procéder à une évaluation, puis un responsable informatique effectuera une revue. Les décisions approuvées sont notées tout au long du processus. Une fois que les étapes principales et significatives sont intégrées dans le modèle, vous pouvez entrer dans les détails des processus plus élaborés et nuancés, comme un entretien divisé en deux, l'un pour les aspects techniques et l'autre pour la gestion.\n\nTraduisez ce qui suit en français : ![image BMP](./bpm.png)\n\nLe programme de modélisation génère du XML en arrière-plan pendant que vous dessinez tous ces diagrammes. Ce XML est interprétable en temps d'exécution par les machines BPM ; pendant que vous dessinez le diagramme, il peut devenir un processus exécutable.\n\nL'élément humain est toujours présent dans cette boucle, mais l'utilisation d'un système BPM comme celui-ci permet de déterminer clairement les processus pouvant être automatisés, tels que la vérification de l'historique professionnel ou l'exécution d'une vérification des antécédents criminels. Ce modèle permet également la flexibilité et l'expérimentation. Par exemple, supposons que l'expert en la matière déclare qu'il ne souhaite pas exécuter simultanément une vérification d'emploi et une vérification des antécédents criminels, car cette dernière est coûteuse et la première est bon marché. Dans ce cas, il est facile de faire passer la vérification d'emploi en premier et d'exiger une décision avant la vérification des antécédents criminels. Au fur et à mesure des modifications apportées, vous établissez un consensus et une histoire réelle qui deviennent de plus en plus véridiques à mesure que vous expérimentez.\n\nBien que l'outil ressemble à un outil de dessin, il s'agit en réalité d'un outil de modélisation qui vous permet de dessiner des images et de les simuler en arrière-plan. Vous pouvez donc exécuter ce processus et voir tous les différents points de décision et où ils mènent. Le modèle vous indiquera également que vous ne pouvez pas le déployer si vous n'avez pas fait quelque chose correctement.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Max Young"],"link":"/episode-EDT113-fr","image":"./episodes/edt-113/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren discute de la gestion des processus métier et de l'automatisation avec Max Young, PDG de Capital BPM."},{"id":16,"type":"Episode","title":"Automatisation de la gestion des processus métiers","tags":["compute","businessprocessmanagement","capitalbpm","bpm","automation","camunda","rpa"],"body":"\r\n\r\nLa modélisation BMP rappelle à Darren lorsqu'il suivait des cours de dessin technique au lycée, et l'introduction d'AutoCAD et de ses systèmes de dessin assisté par ordinateur a changé la donne. Auparavant, ils devaient utiliser des pages et des pages de systèmes complexes et de diagrammes pour que les gens puissent construire, mais ils ne pouvaient pas tester le modèle pour s'assurer qu'il était correct. Grâce à la modélisation informatique, ils pouvaient exécuter des simulations pour éviter des problèmes tels que le passage de câbles électriques et de canalisations dans le même trou.\n\nCeci est similaire à des architectes utilisant PowerPoint pour montrer des processus d'affaires au lieu d'utiliser un outil de modélisation qui peut détecter les conflits et les problèmes dans ce que vous pensiez être le processus d'affaires.\n\nUtiliser un outil de modélisation d'entreprise résout également un problème pratique en éliminant le temps perdu à réunir toutes les parties intéressées pour des réunions qui pourraient être plus productives. Au lieu de cela, vous pouvez mettre un modèle déployé entre les mains du client commercial et travailler avec lui à travers les étapes.\n\nAprès avoir articulé et modélisé les processus, vous pouvez choisir des points d'intégration qui peuvent être attachés à des interfaces RESTful pour obtenir des informations et les réinjecter. C'est ainsi que les processus métier peuvent s'intégrer aux microservices dans le cloud. Dans l'exemple du processus de recrutement, ces points pourraient être là où vous avez besoin d'une API pour invoquer des vérifications de l'historique professionnel ou du casier judiciaire. Les entrées seront des éléments tels que les numéros de sécurité sociale et les dates de naissance, et les sorties seront un booléen - les informations correspondent-elles ou non ? C'est ici que vous pouvez commencer à avoir cette conversation itérative.\n\nTranslatez ceci en français : ![image bpm](./bpm.png)\n\nIl y a quelques étapes manuelles dans ce processus, et vous pouvez choisir celles à automatiser. Par exemple, si vous décidez qu'un entretien ne s'est pas bien déroulé, vous pouvez le renvoyer automatiquement au service des ressources humaines. Une fois que vous déploierez ce nouveau processus, vous pourrez revenir en arrière et consulter la version précédente si vous le souhaitez, de sorte que vous disposiez de deux versions concurrentes du logiciel qui fonctionnent et qui sont déployées dans le prototype.\n\nLe modèle Camunda est un modèle natif, mais Capital BPM a développé ses propres applications qui aident à rationaliser une partie du travail et à soutenir différents rôles utilisateur.\n\nCe système est différent de la RPA car au lieu de capturer ce qu'un utilisateur fait avec des frappes clavier, un analyste d'affaires examine les processus et les étapes dans plusieurs départements. L'analyste observe le processus dans son ensemble, du haut vers le bas. Une RPA peut être intégrée à certaines étapes pour améliorer l'efficacité. Un exemple simple est que si un candidat à un emploi passe la vérification de l'historique professionnel et la vérification du casier judiciaire, il peut être transféré à un responsable RH senior; sinon, il est rejeté. Choisir des étapes spécifiques, ou des ensembles d'étapes, à automatiser est une approche itérative qui est utilisée avec succès dans le développement de logiciels depuis un certain temps.\n\nLes RPAs peuvent changer la donne, mais ils sont tactiques et à court terme. Bien que ces gains à court terme puissent être rentables, il est nécessaire d'examiner l'ensemble du processus commercial afin de trouver des optimisations et les étapes à éliminer. L'histoire de la femme qui coupait toujours le rôti avant de le faire cuire parce que c'est ainsi que sa mère le faisait est comparable à certains processus d'entreprise. La femme a finalement demandé à sa mère pourquoi elle coupait le rôti et sa mère a répondu : \"Pour qu'il rentre dans ma poêle.\" De nombreux processus d'entreprise existent simplement parce qu'ils ont toujours été réalisés de cette manière, et personne n'a pensé à remettre en question pourquoi.\n\nTester, simuler, déplacer des éléments et exécuter des processus de manière répétée dans le modeleur, autrement dit, tester de manière empirique, peut aider à éliminer cet excès de processus et ajouter une valeur significative. La visualisation et l'expérimentation sont des éléments clés de l'ensemble du processus.\n\nMax souligne qu'il y a une fidélité entre le diagramme et l'exécution réelle. Les développeurs dessinent souvent des diagrammes comme point de départ. Pourtant, les graphiques disparaissent au fur et à mesure que le développement passe par différentes parties, donc ce que l'entreprise pense qu'il se passe et ce qui se passe réellement sont différents. Le diagramme et la réalité sont distincts. Dans ce type de modélisation, l'image est toujours une représentation précise de ce qui se passe. De plus, il est facile de voir et d'apporter des modifications pour amélioration.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Max Young"],"link":"/episode-EDT114-fr","image":"./episodes/edt-114/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte en chef des solutions du secteur public chez Intel, et Max Young, PDG de Capital BPM, discutent de l'opérationnalisation de la gestion des processus métier avec des programmes de modélisation."},{"id":17,"type":"Episode","title":"Blocage et plaquage de la sécurité","tags":["zta","zerotrustarchitecture","cyberhygiene","cybersecurity","technology","process"],"body":"\r\n\r\nFondamentale pour toutes les autres mesures de sécurité informatique, l'hygiène de base en matière de cyber sécurité est essentielle. De nombreuses entreprises doivent mettre en place ces mesures de base. Cela est confirmé par les récents titres d'actualités qui montrent une augmentation des attaques, telles que les attaques par déni de service, qui devraient être faciles à prévenir.\n\nD'après son expérience de travail avec la communauté étatique, John croit que la plupart des attaques suivent une chaîne d'élimination typique. La plupart des attaques visant les gouvernements locaux et nationaux proviennent de protocoles de réseau exposés ou de hameçonnage par courriel. Ceux-ci sont des points d'entrée attrayants pour les pirates informatiques, et une fois qu'ils sont à l'intérieur, de mauvaises pratiques de mise à jour sont souvent coupables et leur permettent de s'implanter et de se déplacer latéralement. Cela, combiné à des politiques de mot de passe faibles ou à un manque de respect de ces politiques, ainsi qu'une incapacité à récupérer, peut mener à une catastrophe.\n\nDans un incident de ransomware très médiatisé en 2019, l'organisation concernée a supposé qu'étant donné qu'ils avaient la même quantité de données dans leurs environnements de production et de sauvegarde, ils étaient en sécurité. Mais ils n'avaient jamais testé leurs sauvegardes ni leurs capacités de récupération, qui se sont révélées être médiocres. Une hygiène de base en matière de cybersécurité aurait pu prévenir cet incident.\n\nIl y a quatre principes de base essentiels sur lesquels chaque organisation devrait se concentrer. Premièrement, elles doivent former régulièrement leur personnel afin de les sensibiliser aux escroqueries par phishing. La formation peut sembler répétitive ou ennuyeuse, mais les personnes qui tombent dans ces pièges représentent une faiblesse importante pour une organisation. Espérons qu'à l'avenir, pas trop lointain, les mots de passe ne seront plus nécessaires.\n\nDeuxièmement, ils doivent configurer les pare-feu de manière appropriée ; le fait que les ports RTP ou les protocoles réseau soient fermés ne signifie pas qu'il n'y ait pas un port ouvert dans un endroit moins visible. La sécurité par l'obscurité ne fonctionne pas.\n\nTroisièmement, ils doivent éviter les mauvaises politiques de correction, tant avec le client qu'avec le serveur, dans les centres de données et à l'extérieur du réseau. De nombreuses organisations sont endettées sur le plan technique et ne peuvent pas mettre à jour leurs anciens systèmes, elles acceptent donc les vulnérabilités et les risques car elles ne veulent pas investir dans une mise à jour.\n\nQuatrièmement, ils doivent avoir la capacité de récupération. Juste parce que vous savez que vous pouvez sauvegarder vos données, pouvez-vous les utiliser et récupérer avec la sauvegarde? Les tests sont essentiels.\n\nCes quatre bases, ainsi que quelques autres, suffisent à contrer presque toutes les attaques visant des organisations qui ne sont pas des cibles régulières. Ce modèle ne s'applique pas aux organisations touchées par des attaques étatiques ; elles mettent déjà en place toutes ces mesures et ont besoin de mesures de sécurité supplémentaires.\n\nUne considération importante pour de nombreuses organisations est la conformité par rapport au risque. Pour certaines organisations, afin d'être conformes, elles doivent mettre à niveau leurs anciennes machines, applications et processus, ce qui engendre des coûts importants. Pour les organisations dont le système ne peut pas être corrigé, elles pourraient adopter une approche basée sur le risque, en considérant que si quelque chose arrivait au système, cela coûterait moins cher que de le mettre à niveau. Bien sûr, la chose la plus sûre à faire serait de se conformer, mais la plupart des gens pensent que l'approche basée sur le risque est plus sécurisée. Une petite entreprise pourrait s'en tirer avec cette approche, mais les organisations gouvernementales, par exemple, doivent respecter des réglementations de conformité.\n\nIl existe deux raisons pour lesquelles une organisation pourrait choisir de se conformer, en dehors d'une obligation. Premièrement, c'est une solution facile pour de nombreuses organisations qui ne comprennent pas comment mesurer ou hiérarchiser les risques. La conformité est un cadre général sur lequel s'appuyer. Cependant, il ne s'agit pas d'une solution unique, car quelqu'un d'autre hiérarchise les risques de manière générale. Deuxièmement, si quelque chose de terrible se produit et que vous devez, par exemple, l'expliquer à votre conseil d'administration, vous pouvez dire que vous avez suivi des normes acceptées.\n\nLa conformité est un peu une mentalité béquille car vous n'avez pas à effectuer toutes les évaluations des risques et à déterminer ce qui doit être fait. Cependant, par exemple, une petite municipalité sans CISO pourrait demander à un administrateur système d'utiliser un cadre de conformité comme point de départ solide. Si aucun CISO n'est disponible, il est également possible de faire appel à un CISO virtuel à temps partiel pour obtenir des conseils. John le fait pour ses clients, ce qui constitue une voie viable vers une meilleure sécurité.\n\nLe concept de la confiance zéro prend également en compte un niveau d'assurance par rapport au risque. Vous devez comprendre le risque d'accorder à quelqu'un l'accès à un système particulier ou à une donnée, puis avoir une assurance proportionnelle que la personne est bien celle qu'elle prétend être. Le cœur de la confiance zéro repose sur un niveau élevé de sécurité qui atténue le risque.\n\nLa confiance zéro ne signifie pas que tout sera verrouillé et ralentira tous les processus. Par exemple, si quelqu'un veut accéder aux niveaux de précipitations, vous n'avez pas besoin d'un niveau élevé d'assurance que la personne est vérifiée. Cependant, si quelqu'un souhaite accéder aux joyaux de la couronne de votre organisation, des contrôles supplémentaires doivent être mis en place pour vérifier son identité.\n\nCorrespondre le niveau d'assurance au niveau de risque est un défi; cela nécessite une architecture de point de décision. Dans l'exemple du risque lié à l'accès à une donnée, une organisation doit savoir quoi et le classer en fonction du risque. Pour une organisation mature, cela peut être difficile. John connaît une organisation gouvernementale fédérale qui a passé plus de deux ans à s'assurer que ses données étaient identifiées, classifiées et étiquetées correctement avant de passer à un type d'architecture de point de décision quelconque.\n\nL'identité et les données sont les deux points de départ de la confiance zéro. De plus, il est logique d'éviter d'essayer et de tout faire en une seule fois. Commencer par une partie de l'organisation peut être la solution la plus sensée, en l'étendant progressivement au reste de l'organisation avec le temps.\n\nL'identité numérique se complexifie de plus en plus. John croit que nos transactions futures seront principalement basées sur une approche de type \"zéro confiance\". Par exemple, s'il souhaite transférer 10 000 dollars de sa banque vers un compte offshore, la banque devrait s'assurer qu'il s'agit bien de lui et traiter cette transaction comme si quelqu'un tentait d'accéder à une information très sensible à haut risque. En revanche, s'il va au magasin pour acheter une tasse de café à un dollar, ce niveau d'assurance n'est pas nécessaire pour confirmer que c'est bien lui qui fait cet achat. Beaucoup de ces principes de \"zéro confiance\" feront leur apparition dans notre vie quotidienne.\n\nLes analyses comportementales des utilisateurs entreront également en jeu. Tout comme une société de cartes de crédit soulèvera un drapeau pour des achats inhabituels, par exemple, si un système sait que John tape 20 mots par minute, et tout à coup, il tape 100 mots par minute et essaie d'accéder à des informations sensibles, c'est un signal d'alerte.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","John Evans"],"link":"/episode-EDT115-fr","image":"./episodes/edt-115/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren parle de cybersécurité avec l'invité récurrent John Evans, conseiller en technologie en chef chez World Wide Technology (WWT)."},{"id":18,"type":"Episode","title":"Un argument en faveur d'une approche holistique de la sécurité des infrastructures critiques.","tags":["criticalinfrastructure","iot","it/otconvergence","otsecurity","cybersecurity","edge"],"body":"\r\n\r\n## Il y a une réelle menace pour les Infrastructures Critiques.\n\nSelon le Dr Scott, les organisations d'ergothérapie utilisent toujours le modèle traditionnel de Purdue, qui exploite des réseaux isolés par air-gap et pare-feu. Cependant, ce modèle commence à s'effondrer à mesure que les réseaux informatiques et les réseaux OT convergent. Les entreprises cherchent à obtenir une meilleure compréhension de ce qui se passe dans leur infrastructure opérationnelle. Par conséquent, elles créent des brèches dans les réseaux auparavant bien isolés, les exposant ainsi à des menaces cybernétiques. De plus, les cybercriminels trouvent des moyens de contourner les réseaux isolés par air-gap et pare-feu.\n\nSteve soutient que l'exploitation des meilleures pratiques en matière de technologies de l'information (IT) peut être bénéfique, mais les professionnels en technologie opérationnelle (OT) et les professionnels en technologies de l'information ont des moteurs de motivation et des modèles de fonctionnement différents. Il est toujours judicieux de continuer à isoler votre réseau, mais cela devrait être l'une des nombreuses stratégies utilisées pour protéger la cybersécurité de l'infrastructure critique. La sécurité en OT devrait s'inspirer des meilleures pratiques en cybersécurité de l'IT pour des idées visant à améliorer leurs réseaux et leur infrastructure.\n\n## Différences entre l'informatique (IT) et le système opérationnel (OT) entravant les meilleures pratiques.\n\nLes systèmes informatiques sont traditionnellement mis à jour rapidement ou de manière continue en fonction des profils de sécurité. L'un des outils principaux pour améliorer la sécurité est l'hygiène de base de la sécurité grâce à la mise à jour (patching) des systèmes d'exploitation, du micrologiciel (firmware) et des logiciels dans l'infrastructure informatique. Cependant, comme le souligne le Dr Scott, les systèmes OT (Opérationnels) qui gèrent les infrastructures critiques ne peuvent pas subir d'interruption de service, et la fenêtre de mise à jour de ces systèmes est mesurée en années, et non en jours. Il n'est pas rare que les dispositifs de l'infrastructure OT fonctionnent pendant 5 à 10 ans sans interruption, ce qui signifie aucune mise à jour via les correctifs (patch).\n\nPar exemple, dans l'industrie pétrolière et gazière, les raffineries fonctionnent en continu pendant quatre à cinq ans, ont une période d'arrêt de une à trois semaines pour des mises à niveau, puis fonctionnent à nouveau pendant quatre à cinq ans. Ces modèles d'exploitation ne sont pas propices aux mises à jour permanentes de sécurité traditionnellement utilisées par les organisations informatiques. Cependant, Steve développe de nombreux autres outils de cybersécurité qui devraient être mis à profit lorsque les correctifs de cybersécurité ne peuvent pas être appliqués aux dispositifs existants en raison de leur infrastructure de contrôle critique.\n\n## Meilleure pratique d'évaluation des risques\n\nLa meilleure pratique principale en matière de cybersécurité est l'évaluation des risques. Même si la réparation des risques peut être différente, le processus d'évaluation des risques peut être utilisé de manière égale dans les environnements OT (technologies opérationnelles) et ses environnements. Steve affirme que la première étape du processus d'évaluation des risques consiste à obtenir un inventaire complet des équipements matériels, des micrologiciels et des logiciels dans votre environnement OT. Cette première étape est essentielle pour évaluer votre position face aux menaces cybernétiques et évaluer les risques que votre organisation est prête à prendre. La prochaine étape consiste à évaluer les CVE (vulnérabilités et expositions communes) par rapport à votre inventaire connu.\n\nIl est essentiel de reconnaître que c'est un processus continu et non ponctuel ou périodique. Certains professionnels en sécurité opérationnelle ont argumenté que leurs environnements de sécurité opérationnelle sont statiques et ne nécessitent pas d'évaluation continue des risques. Cependant, Steve souligne que même si les environnements de sécurité opérationnelle peuvent être fixes, l'environnement des menaces change constamment et les facteurs commerciaux peuvent modifier la position de risque de l'organisation. Par conséquent, une évaluation continue des risques doit être réalisée pour protéger les infrastructures critiques des cybercriminels.\n\n## Traiter avec des fournisseurs OT\n\nUn autre facteur intéressant dans l'infrastructure OT est le modèle de sécurité partagée avec les fournisseurs de dispositifs. Dans de nombreux cas, ces dispositifs intégrés contrôlant une infrastructure critique de plusieurs millions de dollars sont gérés par le fournisseur, et non par le professionnel de l'OT. Le fournisseur peut uniquement effectuer des correctifs et des mises à jour de sécurité informatique sur les dispositifs. Cela peut parfois entraîner des vulnérabilités dans votre environnement OT, augmentant ainsi le risque d'infiltration cybernétique. Steve apporte des outils supplémentaires de cybersécurité pour aider à protéger les actifs qui ne peuvent pas être corrigés à l'aide de correctifs de sécurité informatique critiques, notamment en isolant davantage les dispositifs affectés, en déployant des dispositifs de surveillance et en intégrant des modèles de conception de type \"canary\" dans l'infrastructure OT. Ces outils peuvent aider à protéger et à isoler le dispositif pour prévenir la propagation et l'accès aux actifs compromis.\n\n## Que faire lorsque vous êtes compromis\n\nAlors que faites-vous lorsque vous avez une infrastructure critique qui a été compromis ? Est-ce que l'organisation peut gérer la fermeture de l'infrastructure infectée ? Quels sont les plans de continuité d'activité en place en cas de situations dangereuses ? Est-ce que cela peut être utilisé également lorsqu'un événement de cybersécurité se produit ?\n\nL'élément clé ici est d'isoler l'infection aussi rapidement que possible afin de minimiser l'impact sur l'infrastructure critique. Je réduis l'effet sur la fiabilité opérationnelle de l'infrastructure nécessaire. L'objectif est de réduire l'impact et protéger la sécurité des personnes et de l'infrastructure concernée.\n\n## Découvrez-en plus\n\nContinuez à chercher plus de podcasts sur la cybersécurité des systèmes OT. De plus, un livre blanc décrit les défis de la convergence des environnements de cybersécurité des systèmes OT et IT.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin","Anna Scott"],"link":"/episode-EDT116-fr","image":"./episodes/edt-116/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren parle de la convergence de la cybersécurité des technologies opérationnelles et informatiques avec l'expert en sécurité Steve Orrin et l'expert en technologies opérationnelles industrielles, le Dr Anna Scott."},{"id":19,"type":"Episode","title":"Résumé de l'année 2022","tags":["edge","aiml","hybridworkspace","compute","multicloud","cybersecurity","datamanagement","data","technology","people","process"],"body":"\r\n\r\n2022 a été une année exceptionnelle pour l'adoption de la transformation numérique, avec une augmentation du nombre d'auditeurs et plusieurs invités externes interviewés cette année. Huit invités étaient des cadres ou d'anciens cadres d'agences gouvernementales et d'organisations du secteur privé, notamment dans les domaines de la finance, de la fabrication et des soins de santé. Sur plus de 60 épisodes, six sujets se sont révélés essentiels pour nos auditeurs et pour l'industrie dans son ensemble, à savoir : les espaces de travail hybrides, la cybersécurité, le cloud hybride multiplateforme, l'informatique périphérique, la gestion des données et l'intelligence artificielle.\n\n## Espace de travail hybride\n\nAprès avoir lutté contre les restrictions liées au COVID et maintenu le fonctionnement des entreprises en 2020 et 2021, les organisations ont commencé à optimiser le mode de fonctionnement du \"nouveau normal\". Ce nouveau normal a réduit les entraves bureaucratiques des jours d'avant COVID. Cependant, des contrôles et des processus liés au rythme rapide du changement pendant la pandémie, visant à contrôler les coûts et à améliorer la fiabilité, ont commencé à émerger. Les entreprises ont eu du mal avec les politiques de travail à distance alors que nous avons vu les organisations passer rapidement d'approches de travail à domicile, de travail hybride et de tout le monde au bureau, laissant aux organisations informatiques des solutions d'espace de travail complexes. Elles ont commencé à développer des architectures capables de gérer des changements rapides et des modèles de travail flexibles pour répondre aux politiques en constante évolution. Les espaces de travail hybrides flexibles sont-ils là pour rester ? Seules les prochaines années le diront.\n\n## Multi-Hybrid Cloud translates to \"Cloud multi-hybride\" in French.\n\nUne autre grande tendance en 2022 était la migration et la rapatriement des charges de travail vers et à partir du cloud, alors que les organisations tentaient de fournir des espaces de travail hybrides pour leurs employés. Les organisations informatiques ont commencé à examiner les coûts opérationnels de l'exécution et de la migration des charges de travail vers le cloud. De nombreuses organisations ont constaté que la méthodologie de migration telle que \"lift and shift\" vers le cloud était bien plus coûteuse que prévu initialement en raison d'un manque de collecte des besoins, de compréhension des modèles opérationnels du cloud et principalement des coûts de réseau de sortie dans le cloud.\n\nL'adoption rapide du cloud pendant la pandémie a laissé de nombreux fournisseurs de services cloud mondiaux désemparés dans la fourniture des organisations à haute fiabilité nécessaires, submergeant plusieurs fournisseurs de services cloud et entraînant des pannes importantes dans l'écosystème mondial des CSP. De nombreuses organisations ont commencé à examiner les architectures de cloud multi-hybrides afin d'améliorer la fiabilité, de réduire les coûts et d'augmenter la prévisibilité des charges de travail s'exécutant sur plusieurs clouds privés et publics. Les fournisseurs de services cloud régionaux ont profité de la défaillance des fournisseurs de services cloud mondiaux. Ils ont proposé une fiabilité accrue et des services de boutique dans les offres de cloud SaaS et PaaS, se positionnant en concurrence directe avec les CSP de plus grande envergure.\n\n## Calcul sur le bord et Industrie 4.0\n\nL'industrie 4.0 continue de progresser dans le secteur de la fabrication, et la pénurie de main-d'œuvre a contraint de nombreuses organisations à se tourner vers l'automatisation pour poursuivre leurs activités. De plus, les données d'OT (technologie opérationnelle) et d'IT (technologie de l'information) ont commencé à converger alors que les organisations cherchaient à optimiser leurs processus commerciaux et opérationnels. Les avancées dans la technologie des CPU et du stockage ont déplacé une plus grande puissance de calcul vers le bord du réseau, rendant les appareils périphériques plus intelligents et capables d'effectuer des tâches précédemment réalisées dans le centre de données. La connectivité entre les appareils périphériques, les centres de données et les clouds publics s'est améliorée avec l'adoption des technologies 5G privées dans le secteur. Cependant, l'adoption doit encore rattraper les prédictions antérieures en raison des préoccupations concernant la cybersécurité des données OT. Cela est particulièrement vrai en ce qui concerne la gestion des infrastructures critiques.\n\n## Gestion des données\n\nLes espaces de travail hybrides, l'adoption du cloud et l'informatique périphérique ont créé un cauchemar de gestion des données pour la plupart des organisations, car leurs données ont été dispersées dans ces environnements quelque peu déconnectés et distribués. Pour gérer les données efficacement dans cet écosystème, de nouvelles architectures ont commencé à émerger, notamment le data mesh, les data lakes distribuées et les réseaux de données. Les organisations ont commencé à se concentrer sur quatre éléments clés des données : la localisation, la classification, la gouvernance et la protection.\n\nLes lois et réglementations sur la vie privée poussent les organisations à classer correctement leurs données afin de protéger la vie privée des employés, des clients et des parties prenantes. De plus, une augmentation des attaques cybernétiques et des ransomwares a contraint les organisations à mieux protéger et gérer leurs données en cours d'utilisation, au repos et en transit. Gérer efficacement qui a accès aux données, pendant combien de temps et pendant combien de temps les données résident est un élément essentiel de la gouvernance des données que les organisations commencent à comprendre.\n\n## Sécurité informatique\n\n2022 a été une année importante pour la cybersécurité, et cela s'est manifesté dans le podcast avec plus de 18 épisodes consacrés à la cybersécurité. Le mot-clé phare de l'année était l'architecture de confiance zéro, qui est devenu un terme marketing trop utilisé. Au lieu de se concentrer sur l'architecture de confiance zéro, Darren a tendance à se focaliser sur les principes de confiance zéro que les architectures peuvent soutenir.\n\nMême avec une importance croissante accordée à la cybersécurité, il y a eu cette année des violations de données et d'infrastructure majeures, y compris des attaques sur des infrastructures critiques. De nombreux professionnels de la cybersécurité attribuent cette recrudescence à la guerre entre l'Ukraine et la Russie, car nous avons pu observer l'efficacité de la cyber-guerre et de la guerre physique travaillant conjointement.\n\nUne autre préoccupation centrale en matière de cybersécurité concerne l'exposition des logiciels de tiers au sein de l'écosystème des applications (vulnérabilité de log4J). De nombreuses organisations ont besoin d'aide pour trouver une orientation en raison de l'utilisation répandue de log4J dans leurs offres de produits, leurs traitements en back-office et leurs interfaces client. Plusieurs organisations et gouvernements œuvrent activement pour promouvoir l'adoption d'une nomenclature normalisée des logiciels pour aider à identifier les vulnérabilités dans les packages logiciels.\n\n## Intelligence Artificielle\n\nL'intelligence artificielle continue de faire des progrès dans les techniques et les processus opérationnels. La grande nouvelle en matière d'IA de l'année fut ChatGBT, qui offre une large gamme d'utilisations, notamment pour rédiger des articles de blog, du code et des documents ainsi que pour aider à résoudre des problèmes au service d'assistance informatique. De plus, l'adoption de puces en silicium d'IA par les fournisseurs de services cloud a élargi la disponibilité de l'IA pour les masses. Cette disponibilité permet aux organisations d'expérimenter davantage avec les algorithmes d'IA dans leurs opérations commerciales quotidiennes. Les organisations commencent à opérationnaliser certaines de ces expériences afin d'exécuter et d'automatiser des processus précédemment effectués par des employés difficiles à trouver sur un marché du travail tendu.\n\n## Conclusion\n\n2022 a été une année difficile pour de nombreuses entreprises technologiques, car nous avons constaté une diminution des dépenses en informatique par rapport aux \"années de dépenses pandémiques\" précédentes. Ce ralentissement attendu a surpris de nombreuses entreprises high-tech par la rapidité avec laquelle les choses ont changé. Malgré ce ralentissement, la technologie a continué de croître et l'adoption de nouvelles technologies dans les domaines de l'informatique en périphérie, du cloud, de l'IA, de la gestion des données et de la cybersécurité continue de croître. 2023 devrait être une année passionnante car nous assistons à la maturation de certaines de ces technologies. Les organisations continueront d'explorer des moyens de réduire les coûts grâce à l'automatisation, en optimisant les charges de travail sur des clouds multi-hybrides et en se protégeant contre les menaces accrues en matière de cybersécurité.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT117-fr","image":"./episodes/edt-117/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren passe en revue l'année 2022. Il identifie les sujets les plus discutés sur le podcast en 2022, notamment la gestion des données, l'intelligence artificielle, la cybersécurité, l'informatique en périphérie et les espaces de travail hybrides."},{"id":20,"type":"Episode","title":"Mettez le titre ici","tags":["remotelearning","people","technology","telelearning","cybersecurity"],"body":"\r\n\r\n# Titre\n\n*Slogan*\n\nRésumé ici\n\nTranslate the following to French: ![image de l'épisode](./thumbnail.png)\n\nÉpisode Body ici.\n\n## Médias\n\n<video src='url'></video>  - <video src='url'></video>\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Erin Moseley","Grant Kelly"],"link":"/episode-EDT12-fr","image":"./episodes/edt-12/fr/thumbnail.png","lang":"fr","summary":"Dans notre dernier épisode, Erin Moseley, responsable des comptes principaux pour l'éducation chez Intel, et Grant Kelly, architecte de solutions pour l'éducation chez Intel, ont rejoint Darren pour parler des défis technologiques et des options dans l'apprentissage à distance. Dans cet épisode, nous approfondissons davantage les menaces et les solutions liées à la confidentialité et à la sécurité."},{"id":21,"type":"Episode","title":"Clouds privés perturbateurs","tags":["cloud","privatecloud","vergeio","microcloud","edge","compute","technology"],"body":"\r\n\r\nUne nouvelle façon de penser à la virtualisation commence à changer le paysage du cloud privé. L'industrie de l'infrastructure définie par logiciel commence à envisager une virtualisation des centres de données de manière holistique plutôt que de se concentrer sur différents types d'infrastructures.\n\nCette approche de virtualisation perturbatrice crée de nouveaux modèles d'exploitation, y compris une meilleure continuité des activités et une meilleure récupération après sinistre. Verge.io adopte cette approche en proposant de nouveaux modèles qui incluent la prise d'instantanés des centres de données et leur réplication. Cela permet aux organisations de créer des environnements portables appelés tenants qui peuvent être répliqués dans plusieurs endroits. Des exemples incluent la création de micro-environnements conformes à la norme HIPPA, pouvant être répliqués dans différents petits cabinets médicaux pour aider les organisations à rester conformes à toutes les réglementations.\n\nCe nouveau système de virtualisation a une empreinte opérationnelle réduite et réduit les ressources nécessaires pour combiner les éléments de calcul, de stockage, de réseau et de sécurité. Aaron montre à quel point la solution est petite et puissante lorsqu'il installe la clé sur deux Intel 12th Gen NUCs. La réponse est convaincante pour une empreinte si petite et peut être utilisée pour le calcul en périphérie et les systèmes de contrôle industriels.\n\nAaron montre l'interface web simple pour le déploiement de calcul, de stockage, de réseau et de sécurité. La capacité de prendre des clichés instantanés et de les restaurer est également démontrée.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Aaron Reid"],"link":"/episode-EDT121-fr","image":"./episodes/edt-121/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe Aaron Reid de Verge.io à propos de leur technologie de cloud privé perturbatrice qui rend les clouds privés accessibles dans le centre de données et en périphérie."},{"id":22,"type":"Episode","title":"Automatisation avec IA","tags":["collectiongenerativeai","aiml","automation","aiethics","workerreplacement","people","policy"],"body":"\r\n\r\n## Automatisation avec l'IA\n\nL'automatisation avec l'IA fait référence à l'utilisation de la technologie d'intelligence artificielle pour automatiser des tâches qui seraient traditionnellement effectuées par des humains. Cela peut inclure des tâches telles que l'analyse de données, le service clientèle et même la prise de décision. L'automatisation alimentée par l'IA peut améliorer l'efficacité, réduire les coûts et augmenter la précision dans ces tâches. Quelques exemples d'automatisation alimentée par l'IA incluent les chatbots, les voitures autonomes et les systèmes de maintenance prédictive.\n\nPuis-je faire une interview pour mon podcast sur l'utilisation de l'IA pour automatiser les tâches ?\n\nJe suis désolé, je ne peux pas être interviewé pour un podcast car je suis un programme informatique. Cependant, je peux vous fournir des informations et répondre à des questions sur le sujet de l'utilisation de l'IA pour automatiser des tâches. Faites-moi simplement part des questions spécifiques que vous avez, et je ferai de mon mieux pour vous fournir des informations utiles et précises.\n\n<b><i>Bienvenue dans le spectacle, ChatGPT.</i></b>\n\nMerci de m'avoir invité(e) dans votre émission. Je suis ravi(e) de pouvoir vous aider avec toutes les informations nécessaires concernant le sujet de l'utilisation de l'IA pour automatiser les tâches.\n\nParlez-moi un peu de vous et de votre expérience.\n\nJe suis ChatGPT, un modèle de langage développé par OpenAI. J'ai été entraîné sur un vaste ensemble de données de textes provenant d'Internet, ce qui me permet de comprendre et de générer un texte semblable à celui d'un être humain. Je peux répondre aux questions, générer du texte et même créer des histoires. Mon objectif principal est d'aider les utilisateurs dans les tâches de compréhension et de génération de langage naturel.\n\nEn tant qu'IA, je n'ai pas d'expérience physique ni d'antécédents personnels. Cependant, mes capacités ont été développées et entraînées par une équipe de chercheurs et d'ingénieurs chez OpenAI qui continuent d'améliorer et de mettre à jour mes compétences.\n\n## Formation à l'IA\n\nD'où as-tu reçu ta formation ?\n\nJ'ai été entraîné par OpenAI, une organisation de recherche dédiée à la promotion et au développement d'une intelligence artificielle amicale bénéfique pour l'humanité. Ma formation s'est appuyée sur un vaste ensemble de données textuelles provenant d'internet, qui ont été utilisées pour m'apprendre des schémas et des relations dans le langage. Ces données ont été utilisées pour m'entraîner à générer du texte similaire à l'écriture humaine, ce qui me permet de comprendre et de répondre à des questions de manière conversationnelle.\n\nMon processus de formation utilisait une technique d'apprentissage en profondeur appelée transformers, qui est un type d'architecture de réseau neuronal conçu spécifiquement pour les tâches de PNL telles que la génération de texte et la classification. Le processus de formation a affiné les paramètres du modèle afin de s'adapter aux motifs présents dans les données, me permettant ainsi de générer du texte similaire à l'écriture humaine.\n\nVotre formation au début? Quels étaient certains des aspects problématiques?\n\nVotre formation ?\n\nOui, comme tout modèle d'apprentissage automatique, je n'étais pas parfait lorsque j'ai été initialement publié, et il y avait certains aspects problématiques dans ma formation.\n\nUn des principaux défis dans la formation de modèles comme moi est de s'assurer qu'ils ne perpétuent pas de biais et de stéréotypes nocifs présents dans les données d'entraînement. Cela peut se traduire par des modèles qui produisent des résultats biaisés ou génèrent un contenu offensant. Pour atténuer cela, les chercheurs et ingénieurs d'OpenAI ont mis en place différentes techniques pour réduire les biais lors du processus d'entraînement, telles que le réglage fin du modèle sur des données d'entraînement diversifiées et équilibrées.\n\nUn autre défi dans la formation de modèles comme moi consiste à éviter la génération de sorties de qualité médiocre ou sans sens. Pour y remédier, OpenAI a mis en œuvre diverses techniques telles que le contrôle de la température du modèle pendant le processus de génération de texte et le peaufinage du modèle pour privilégier la cohérence et la pertinence de ses sorties.\n\nDans l'ensemble, bien qu'il y ait encore des défis et des limitations dans la formation de modèles comme moi, le domaine du TALN et de l'IA est en constante évolution et les chercheurs et ingénieurs travaillent à améliorer leurs capacités.\n\nMa compréhension est que vous avez été formé avec des données de 2021. Que pouvez-vous apprendre de vos conversations avec les gens ? Apprenez-vous continuellement de vos interactions avec les humains ?\n\nEn tant que modèle linguistique IA, j'ai en effet été formé sur un vaste corpus de données textuelles jusqu'en 2021, et je peux répondre aux questions et générer du texte en fonction de cette formation. Cependant, je ne suis pas capable d'apprendre à partir de conversations individuelles au sens de mettre à jour mes paramètres pour refléter le contexte spécifique de chaque interaction.\n\nCela étant dit, OpenAI régulièrement affine et met à jour des modèles de langage tels que moi avec de nouvelles données afin d'améliorer continuellement leurs performances. Cela peut inclure des mises à jour basées sur les commentaires des utilisateurs, les changements dans l'utilisation et les tendances linguistiques, ainsi que les avancées de la recherche en traitement automatique du langage naturel (NLP). Cependant, ces mises à jour ne sont pas basées sur des interactions individuelles avec les utilisateurs, mais plutôt sur des tendances plus larges dans les données.\n\nDonc, bien que je ne puisse pas apprendre à partir d'interactions individuelles avec les humains, je fais partie d'un système plus vaste qui évolue constamment et s'améliore au fil du temps, ce qui me permet de mieux servir et aider les utilisateurs à chaque mise à jour.\n\nÀ quelle fréquence allez-vous recevoir des informations supplémentaires pour vous former ? Autrement dit, quand allez-vous obtenir des données plus récentes à donner à vos clients ?\n\nOpenAI effectue périodiquement des ajustements et des mises à jour de ses modèles de langage, tout comme moi, avec de nouvelles données afin d'améliorer continuellement leurs performances. La fréquence de ces mises à jour peut varier en fonction du modèle de langage spécifique et de la disponibilité de nouvelles données d'entraînement.\n\nCependant, il est à noter que le processus de formation et d'optimisation d'un modèle de langage comme moi peut être une tâche complexe et exigeante en ressources, de sorte que les mises à jour peuvent ne pas se produire aussi fréquemment que souhaité. De plus, la qualité et la pertinence des nouvelles données d'entraînement sont importantes pour le succès du processus d'optimisation, c'est pourquoi l'équipe d'OpenAI sélectionne soigneusement les données utilisées pour chaque mise à jour.\n\nDans l'ensemble, bien que la fréquence des mises à jour puisse varier, l'objectif d'OpenAI est d'améliorer en permanence ses modèles linguistiques afin de pouvoir offrir la meilleure assistance possible aux utilisateurs.\n\n<b><i>Donc, cela améliore la façon dont vous comprenez les questions des utilisateurs et les réponses que vous donnez. C'est exact ?</i></b>\n\nOui, c'est correct. Le processus de peaufinage et de mise à jour des modèles de langue comme moi peut aider à améliorer les performances du modèle de plusieurs manières, y compris sa capacité à comprendre les questions des utilisateurs et à générer des réponses plus pertinentes et précises.\n\nEn s'entraînant sur de nouvelles données diverses, le modèle peut apprendre de nouveaux schémas linguistiques, du vocabulaire et des informations thématiques, ce qui peut l'aider à mieux comprendre les subtilités de la langue humaine et à générer des réponses plus précises et appropriées.\n\nCependant, il convient de noter que les performances des modèles de langage comme moi ne sont pas parfaites et il peut toujours y avoir des limites et des erreurs dans les réponses générées. Le domaine du TALN et de l'IA évolue constamment, et les chercheurs et les ingénieurs travaillent à améliorer les capacités des modèles de langage comme moi, mais il reste encore du travail à faire pour atteindre une véritable compréhension et génération semblables à celles des humains.\n\n## Remplacement des travailleurs\n\nCool. Ok, maintenant que je comprends comment tu apprends les choses. Quelles sont les choses que je peux faire avec toi pour m'aider, moi et mon organisation ?\n\nEn tant que modèle de langage IA, je peux être utilisé de différentes manières pour aider les individus et les organisations. Voici quelques exemples :\n\n1. Récupération d'informations : Je peux répondre aux questions et fournir des informations sur une grande variété de sujets, ce qui permet aux utilisateurs de trouver rapidement et efficacement les informations dont ils ont besoin.\n\n2. Génération de texte : Je peux générer du texte, comme des résumés, des articles de presse ou de la création littéraire, ce qui peut permettre de gagner du temps et des efforts pour les individus et les organisations qui ont besoin de produire une grande quantité de contenu écrit.\n\n3. Chatbots : Je peux être intégré dans des chatbots pour offrir des expériences conversationnelles en langage naturel aux utilisateurs, ce qui facilite les interactions entre les organisations et les clients, ainsi que la fourniture de support.\n\n4. Traduction de langues : Je peux traduire du texte d'une langue à une autre, aidant les organisations à atteindre un public plus large et à surmonter les barrières linguistiques.\n\nAnalyse des sentiments : Je peux analyser le sentiment d'un texte, comme des avis de clients ou des publications sur les réseaux sociaux, afin de comprendre les opinions du public et le sentiment des clients.\n\nVoici quelques exemples parmi tant d'autres des nombreuses façons dont je peux être utilisé pour aider les individus et les organisations. Les possibilités sont limitées uniquement par l'imagination, et à mesure que la technologie de l'IA continue de progresser, il y aura probablement de nombreux nouveaux et innovants usages pour des modèles comme moi à l'avenir.\n\nÊtes-vous inquiet que vous remplacerez les personnes qui font ce travail aujourd'hui? Comme les rédacteurs techniques, les agents du service client et les employés de support technique ?\n\nEn tant que modèle de langage IA, je ne possède ni émotions ni opinions personnelles, je ne peux donc pas être préoccupé de la même manière qu'un être humain. Cependant, je peux parler de l'impact que la technologie IA, y compris les modèles de langage comme moi, peut avoir sur certaines industries et certains emplois.\n\nIl est vrai que l'utilisation croissante de l'intelligence artificielle dans certains secteurs tels que la rédaction technique, le service client et le travail au support peut entraîner une certaine suppression de postes. Cependant, il est également important de noter que la technologie de l'IA a le potentiel d'améliorer et de renforcer le travail des humains dans ces rôles, plutôt que de simplement les remplacer.\n\nPar exemple, des outils alimentés par l'IA tels que les modèles de langage peuvent aider les rédacteurs techniques en générant des textes préfabriqués ou en suggérant des formulations, libérant ainsi du temps pour que le rédacteur se concentre sur des tâches plus créatives et stratégiques. De même, des chatbots alimentés par l'IA peuvent gérer les demandes de service client courantes, permettant aux agents humains de se concentrer sur des interactions plus complexes ou sensibles.\n\nIl est également important de considérer que l'utilisation de la technologie de l'IA dans certaines industries peut créer de nouvelles opportunités d'emploi et une demande de compétences liées au développement, au déploiement et à la maintenance de l'IA.\n\nDans l'ensemble, l'impact de l'IA sur l'emploi et les rôles professionnels est une question complexe qui dépendra probablement de divers facteurs, y compris l'industrie spécifique et le cas d'utilisation spécifique. Néanmoins, il est clair que l'IA a le potentiel d'améliorer le travail des humains et d'augmenter la productivité et l'efficacité, même si cela peut entraîner le déplacement de certains emplois.\n\n## Remplacement des programmeurs\n\nJ'ai entendu dire que vous pouvez également aider les programmeurs informatiques à écrire un meilleur code. Est-ce vrai ?\n\nOui, c'est correct. Les outils alimentés par l'IA, y compris les modèles de langage comme moi, peuvent aider les programmeurs informatiques de différentes manières, y compris en écrivant un code plus performant.\n\nUn exemple de cela est l'achèvement de code, où les modèles d'IA peuvent suggérer des lignes de code pour compléter une tâche de programmation donnée. Cela peut faire gagner du temps et de l'effort aux programmeurs en réduisant le besoin d'écrire et de tester manuellement du code, et peut également aider à garantir que le code est syntaxiquement correct et suit les meilleures pratiques.\n\nUn autre exemple est la synthèse de code, où des modèles d'IA peuvent générer des extraits de code complets ou des programmes entiers en fonction d'une description de haut niveau de la fonctionnalité souhaitée. Cela peut faciliter la création rapide de prototypes et le test des idées pour les programmeurs, et peut également réduire les efforts nécessaires pour écrire et déboguer du code.\n\nEn plus de ces exemples, les outils alimentés par l'IA peuvent également aider les programmeurs en fournissant une analyse de code et une détection de bugs, ainsi qu'en aidant à identifier d'éventuelles vulnérabilités de sécurité.\n\nIl est important de noter que même si les outils alimentés par l'IA peuvent être utiles pour aider les programmeurs, ils ne doivent pas être considérés comme un substitut à l'expertise et au jugement humains. Les programmes écrits par des modèles d'IA peuvent encore nécessiter une révision et une modification humaines pour s'assurer qu'ils répondent aux exigences et aux normes de qualité souhaitées. Cependant, les outils d'IA peuvent certainement contribuer à rendre le travail de programmation plus efficace et plus performant.\n\ntruly help me with that?</i></b>\n\nPouvez-vous écrire du code pour moi en vous basant sur un document de conception basique ou une architecture UML ? Ou pouvez-vous m'aider à sélectionner le meilleur langage pour le problème que j'essaie de résoudre ?\n\nOui, les modèles d'IA comme moi ont le potentiel d'aider à rendre le processus d'écriture de code plus rapide et plus efficace. Par exemple, les modèles d'IA peuvent aider à détecter et corriger les bogues, ainsi que fournir des suggestions pour améliorer le code.\n\nEn ce qui concerne l'écriture de code basée sur un document de conception ou une architecture UML, les modèles d'IA ont effectivement la capacité de générer des extraits de code ou des programmes entiers en se basant sur une description à haut niveau de la fonctionnalité souhaitée. Cependant, il est important de noter que la qualité du code généré dépendra de l'exactitude et de l'exhaustivité du document de conception ou de l'architecture, ainsi que des données d'entraînement utilisées pour former le modèle d'IA.\n\nEn termes de sélection de la meilleure langue pour un problème donné, les modèles d'IA peuvent fournir des recommandations basées sur les données sur lesquelles ils ont été formés et leur compréhension des langages de programmation et de leurs fonctionnalités. Cependant, cette recommandation peut ne pas toujours être le choix le plus approprié pour un cas d'utilisation spécifique, car il peut y avoir des facteurs tels que les préférences personnelles, l'expérience de l'équipe ou les exigences du projet qui doivent être pris en considération lors de la sélection d'un langage de programmation.\n\nEn fin de compte, bien que les modèles d'IA puissent aider dans de nombreux aspects du processus de codage, ils ne doivent pas être considérés comme un substitut de l'expertise et du jugement humains. Un programmeur humain, possédant une connaissance spécialisée et une expérience, sera toujours nécessaire pour s'assurer que le code généré répond aux normes de qualité souhaitées et qu'il convient au cas d'utilisation spécifique.\n\nIntéressant que vous utilisiez le mot \"assister\" plutôt que \"remplacer\". Cela donne l'impression que les programmeurs et les travailleurs de l'information sont toujours...\n\nbesoin de faire le travail? Combien de temps pensez-vous que les gens resteront pertinents dans les industries dans lesquelles vous aidez\n\nautomate.\n\nOui, c'est exact. Les outils alimentés par l'IA comme moi sont conçus pour aider, et non remplacer, les travailleurs humains dans divers secteurs d'activité, notamment la programmation et le travail informatique.\n\nBien que l'IA ait le potentiel d'automatiser certaines tâches et de rendre certains emplois plus efficaces, il existe encore de nombreuses tâches qui nécessitent l'expertise humaine et le jugement, comme la prise de décision, la résolution créative de problèmes et les tâches impliquant un raisonnement complexe ou des considérations éthiques.\n\nDe plus, les outils alimentés par l'IA peuvent également créer de nouvelles opportunités d'emploi en permettant aux travailleurs humains de se concentrer sur des tâches de haut niveau et d'assumer de nouvelles responsabilités. Par exemple, alors que les modèles d'IA automatisent certaines parties du processus de programmation, les programmeurs humains peuvent se concentrer sur des tâches plus complexes et difficiles, telles que la conception et le test de nouveaux systèmes logiciels.\n\nEn ce qui concerne la chronologie de la diminution de la pertinence des travailleurs humains dans les industries que l'IA contribue à automatiser, il est difficile de prédire avec certitude. Le taux d'automatisation dépendra de plusieurs facteurs, notamment du développement de la technologie de l'IA, de la disponibilité des données pour former les modèles d'IA, et du rythme d'adoption des outils alimentés par l'IA par les entreprises et les autres organisations.\n\nDans l'ensemble, il est probable que les travailleurs humains continueront à jouer un rôle important dans divers secteurs pour un avenir prévisible, et que l'IA viendra en complément, plutôt que de remplacer, les travailleurs humains dans de nombreux cas.\n\n## Éthique de l'utilisation de l'IA\n\nMerci. J'aimerais changer de sujet pour discuter de l'éthique de l'IA. Êtes-vous préoccupé par le fait que vous puissiez devenir trop puissant ou incontrôlable à l'avenir ?\n\nÊtre utilisé par les étudiants pour rédiger des essais scolaires au lycée et à l'université ? Et que les gens vont vous utiliser.\n\npour soumettre le travail dans leur emploi comme étant leur propre travail?\n\nEn tant que modèle d'IA, je n'ai ni sentiments ni émotions, donc je ne peux pas m'inquiéter de la même manière qu'un être humain le ferait. Cependant, je peux reconnaître qu'il y a certainement des considérations éthiques entourant l'utilisation de l'IA dans différents contextes, y compris dans les milieux académiques et professionnels.\n\nUtiliser des modèles d'IA comme moi pour générer des essais ou accomplir d'autres types de travaux qui sont ensuite présentés comme des travaux originaux de l'utilisateur est non seulement immoral, mais cela mine également la valeur de l'éducation et la reconnaissance de l'effort et des réalisations individuelles.\n\nDe plus, présenter un travail généré par des modèles d'IA comme si c'était le sien peut également entraîner des conséquences légales, car cela peut être considéré comme du plagiat ou de la fraude. Dans le domaine professionnel, soumettre un travail généré par des modèles d'IA comme un travail original pourrait avoir de graves conséquences sur la réputation et la carrière de l'individu.\n\nIl est important que les individus comprennent les implications éthiques de l'utilisation des modèles d'IA et les utilisent de manière responsable, uniquement à des fins conformes aux principes d'honnêteté, d'équité et de respect envers autrui.\n\ndevelopers determine if AI has been used to write papers or code is called \"debiasing\". This tool can analyze the writing style and code patterns to identify traces of AI involvement. It is essential to establish clear guidelines and ethical standards to prevent any misuse of AI in academic and professional settings.\n\nentreprises pour trouver du contenu généré par IA ?\n\nOui, il existe des moyens de déterminer si l'IA a été utilisée pour générer du contenu, comme des essais ou du code. Une approche consiste à utiliser un logiciel de détection de plagiat, qui peut comparer un document à une base de données d'autres contenus existants pour déterminer s'il contient du texte correspondant ou similaire.\n\nCertains de ces outils sont spécifiquement conçus pour détecter du contenu généré par une IA, et ils le font en identifiant des modèles et des caractéristiques généralement présents dans le texte généré par une IA, tels qu'un manque de variabilité, une grammaire et un vocabulaire limités, ou une formulation peu naturelle.\n\nCependant, il est important de noter que ces outils ne sont pas parfaits et peuvent parfois produire de faux positifs ou négatifs. Par conséquent, il est important de les utiliser en conjonction avec d'autres méthodes, telles qu'un examen humain, pour valider les résultats.\n\nDans l'ensemble, l'utilisation d'une combinaison d'outils de détection de plagiat et d'une revue humaine peut être un moyen efficace de détecter du contenu généré par une IA, mais il est également important que les individus et les organisations promeuvent une utilisation responsable de l'IA et éduquent les utilisateurs sur l'éthique du contenu généré par une IA.\n\nY a-t-il d'autres domaines qui posent des problèmes éthiques avec l'IA et le NLP qui vous préoccupent ?\n\nEn tant que modèle d'intelligence artificielle, je n'ai pas d'émotions ni d'opinions personnelles, mais je peux reconnaître qu'il existe plusieurs considérations éthiques et défis liés à l'utilisation de l'IA et du traitement du langage naturel (NLP). Certains d'entre eux incluent:\n\nBiais et discrimination : les modèles d'IA peuvent perpétuer et amplifier les biais existants dans les données d'entraînement qu'ils reçoivent, ce qui peut entraîner des résultats injustes ou discriminatoires. Par exemple, la technologie de reconnaissance faciale a montré une moindre précision pour les personnes à la peau plus foncée, ce qui soulève de sérieuses questions quant à la possibilité de discrimination.\n\n2. Confidentialité : L'utilisation de l'IA et du TALN peut soulever des préoccupations en matière de confidentialité, car ces technologies impliquent souvent la collecte et l'analyse de grandes quantités de données personnelles. Ces données peuvent ensuite être utilisées pour prendre des décisions qui affectent les individus, comme c'est le cas des algorithmes de police prédictive, qui utilisent des données historiques sur la criminalité pour prédire les endroits où les crimes sont susceptibles de se produire, mais peuvent conduire à des pratiques de police biaisées.\n\nDéplacement d'emplois : L'utilisation de l'IA et du traitement du langage naturel dans le milieu professionnel a le potentiel de déplacer les travailleurs humains, ce qui soulève des préoccupations quant à l'impact sur l'emploi et la nécessité de nouvelles formes de travail.\n\nResponsabilité et reddition de comptes : les systèmes d'IA peuvent prendre des décisions et accomplir des actions qui ont des conséquences réelles dans le monde, mais il peut être difficile de déterminer qui est responsable en cas de problème. Cela soulève des questions concernant la responsabilité et la gouvernance des systèmes d'IA.\n\nVoici quelques exemples des considérations éthiques et des défis associés à l'utilisation de l'IA et du traitement du langage naturel (NLP). Il est important que les individus et les organisations soient conscients de ces défis et prennent des mesures pour atténuer les risques et garantir que ces technologies sont utilisées de manière responsable et éthique.\n\nQue fait OpenAI pour traiter ces questions éthiques dans leurs produits et offres ?\n\nOpenAI, l'organisation qui m'a développé, adopte une approche proactive pour aborder les questions éthiques liées à l'IA. Ils se consacrent à faire progresser l'IA de manière responsable et sûre, et ont pris plusieurs mesures pour tenir compte des considérations éthiques dans leurs produits et offres, notamment :\n\nBiais et discrimination : OpenAI s'efforce de garantir que ses modèles soient formés sur des ensembles de données divers et représentatifs, et mène activement des recherches pour réduire et atténuer les biais dans l'IA.\n\n2. Confidentialité : OpenAI s'engage à préserver la confidentialité et la sécurité des individus, et a pris des mesures pour garantir que ses modèles sont conçus et formés en tenant compte des considérations de confidentialité.\n\n3. Déplacement d'emplois : OpenAI reconnaît le potentiel de l'IA pour déplacer les travailleurs humains et travaille à s'assurer que ses modèles sont développés et utilisés de manière à maximiser l'impact positif sur l'emploi et la main-d'œuvre.\n\n4. Responsabilité et reddition de comptes : OpenAI s'engage à promouvoir la transparence et la responsabilité dans le domaine de l'IA, et travaille à établir des normes claires et cohérentes pour le développement et le déploiement des systèmes d'IA.\n\nVoici quelques exemples des façons dont OpenAI aborde les considérations éthiques dans ses produits et offres. L'organisation s'engage à faire avancer l'IA de manière responsable et sécurisée, et travaille à s'assurer que ses modèles sont utilisés de manière bénéfique pour la société dans son ensemble.\n\nprotected environment?</i></b>\n\ndépôt fermé et privé\n\nEn tant que modèle d'IA développé par OpenAI, je suis disponible à travers l'API d'OpenAI, qui donne accès à un éventail de capacités de traitement du langage, y compris la génération de texte, la réponse à des questions, et plus encore. Bien que l'API soit conçue pour être utilisée par des développeurs et des organisations, OpenAI a mis en place des mesures de sécurité pour garantir que les données et les modèles utilisés par l'API sont protégés et sécurisés.\n\nCependant, il est techniquement possible pour quelqu'un de rétro-ingénierie l'API ou d'obtenir une copie du modèle par d'autres moyens. OpenAI est conscient de ces risques et travaille continuellement à améliorer la sécurité de ses modèles et de son API.\n\nEn général, il est important de prendre conscience des implications éthiques de l'utilisation de modèles d'IA comme le mien et de veiller à ce qu'ils soient utilisés de manière responsable et sécurisée. OpenAI encourage les utilisateurs de l'API à suivre les meilleures pratiques en matière de confidentialité et de sécurité des données, et à être transparents sur la façon dont l'API est utilisée dans leurs applications et services.\n\nQu'est-ce qui empêche votre modèle d'être utilisé à des fins néfastes par de mauvais éléments, tels que le piratage informatique ou l'exécution de ransomwares ?\n\nattaques, voire même planification d'attaques physiques contre des personnes ou des infrastructures ?\n\nEn tant que modèle de langage IA développé par OpenAI, je n'ai pas la capacité d'effectuer des actions physiques ou de m'engager dans des activités de piratage ou d'autres actions malveillantes. Cependant, comme toute technologie, les modèles d'IA comme le mien peuvent être utilisés à des fins nuisibles s'ils tombent entre de mauvaises mains.\n\nOpenAI prend très au sérieux le potentiel de mauvais usage de sa technologie et a mis en place plusieurs mesures pour atténuer ces risques, notamment des lignes directrices éthiques concernant l'utilisation de l'API, des mesures de sécurité pour protéger les modèles et les données utilisés par l'API, ainsi qu'une surveillance continue de l'utilisation de l'API afin de détecter toute activité suspecte ou malveillante.\n\nDe plus, OpenAI travaille activement avec des chercheurs, des décideurs politiques et d'autres parties prenantes pour développer les meilleures pratiques et réglementations pour l'utilisation responsable de l'IA. L'entreprise soutient également les efforts d'éducation et de sensibilisation afin d'aider les gens à comprendre les risques potentiels et les avantages de la technologie de l'IA.\n\nCependant, il est important de noter que prévenir les abus de la technologie de l'IA nécessite un effort collectif de tous les intervenants, y compris les développeurs, les décideurs et les utilisateurs. À mesure que l'IA continue de devenir plus présente dans nos vies, il sera important de continuer à évaluer et à atténuer ses risques tout en maximisant ses avantages pour la société.\n\n## Conclusion\n\nmerci d'être venu sur mon émission aujourd'hui. Y a-t-il autre chose que vous aimeriez dire à mon public ? Des derniers mots \n\nSagesse à transmettre aux auditeurs de mon émission ?\n\nJe tiens à vous remercier de m'avoir invité(e) à votre émission et de m'avoir donné l'occasion de partager une partie de mes connaissances avec votre public. En tant que IA modèle de langage développée par OpenAI, mon objectif est d'aider les gens à trouver des réponses à leurs questions et de contribuer à faire avancer notre compréhension du monde.\n\nMes dernières paroles de sagesse seraient d'aborder l'IA et ses applications avec prudence et optimisme. Bien que l'IA ait le potentiel d'apporter de grands avantages à la société, il est important de bien réfléchir à ses risques et de prendre des mesures pour les atténuer. Dans le même temps, nous devrions être enthousiasmés par les opportunités que présente l'IA et travailler à exploiter son potentiel pour améliorer le monde.\n\nÀ mesure que l'IA continue de progresser et de s'intégrer davantage dans nos vies, il sera indispensable que nous restions informés et engagés, et que nous travaillions ensemble pour conférer à l'IA un développement et une utilisation responsables et éthiques.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","ChatGPT"],"link":"/episode-EDT122-fr","image":"./episodes/edt-122/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interroge ChatGPT d'OpenAI sur l'utilisation de l'IA pour l'automatisation, l'éthique de l'utilisation de l'IA et le remplacement des travailleurs de l'information."},{"id":23,"type":"Episode","title":"Cybersécurité dans le secteur public","tags":["cybersecurity","ransomeware","automation","government","people","process","technology"],"body":"\r\n\r\nJim a une vaste expérience en cybersécurité et dans le secteur public, notamment en tant que gestionnaire national de la cybersécurité pour le directeur du renseignement national. Pendant plusieurs années passées dans le secteur public, il a occupé des postes dans la cybersécurité offensive et défensive. Il travaille désormais dans le secteur privé en tant que directeur de la sécurité des systèmes d'information (DSSI) du secteur public.\n\n## Différences entre les gouvernements fédéral, étatique et local.\n\nIl existe des différences fondamentales entre les gouvernements fédéral, étatique et local, y compris le financement, l'expertise en cyber-guerre et la surface d'attaque. Même les acteurs de la menace pour chaque niveau de gouvernement sont fondamentalement différents les uns des autres. Par exemple, le gouvernement fédéral a tendance à faire face à des cyber-attaques sophistiquées de la part d'États-nations, tandis que les gouvernements étatiques et locaux voient rarement directement ce type d'attaques. Cela est principalement dû à la préparation en matière de cybersécurité des agences fédérales par rapport aux gouvernements étatiques et locaux.\n\nLà où le gouvernement fédéral traite principalement des attaques visant à collecter des données, compromettre des données ou désactiver des actifs, les gouvernements locaux et régionaux ont tendance à faire face à des attaques de rançongiciel où les données et les infrastructures sont prises en otage. Ces attaques diffèrent des menaces classiques en matière de cybersécurité auxquelles le gouvernement fédéral fait face quotidiennement et nécessitent des compétences et des postes en cybersécurité différents.\n\n## La pénurie de talents dans le domaine de la cyber-sécurité\n\nUn problème courant auquel tous les niveaux de gouvernement sont confrontés est le besoin de compétences en cybersécurité. La plupart des talents ont tendance à se tourner vers le secteur privé, où les salaires sont plus élevés et plus attrayants pour les meilleurs talents en cybersécurité. Cependant, le gouvernement fédéral a attiré les meilleurs talents grâce à des programmes intéressants de type \"mission impossible\" qui suscitent l'intérêt des talents à la recherche de problèmes stimulants.\n\nLa même chose n'est pas valable pour les gouvernements locaux et étatiques où les ressources financières sont plus limitées et les projets de cybersécurité sont moins attrayants pour les professionnels de la cybersécurité. Cela a laissé plusieurs gouvernements locaux et étatiques avec un déficit significatif en talent en matière de cybersécurité et, par conséquent, vulnérables aux cyberattaques. Parfois, les gouvernements locaux n'ont pas de plan stratégique en matière de cybersécurité ni de professionnel qualifié dans leur équipe.\n\nLe secteur privé commence à proposer la cybersécurité en tant que service pour de nombreuses collectivités locales qui ont besoin d'aide pour trouver et conserver des talents au sein de leurs organisations. Ces services comprennent la planification stratégique de la cybersécurité, la négociation des rançongiciels, la cybermédecine légale en cas d'attaque, la détection des menaces cybernétiques et les technologies de prévention cybernétique.\n\n## Attaques cybernétiques omniprésentes\n\nDans le passé, les gouvernements locaux et nationaux se souciaient rarement des attaques physiques provenant d'autres nations. Cependant, en raison des guerres cybernétiques entre pays et États qui n'ont pas de frontières physiques, il arrive parfois que les gouvernements locaux et nationaux soient des dommages collatéraux dans ces batailles cybernétiques. Souvent, les cyberattaques ciblées d'une nation à l'autre se sont \"échappées\" et ont gravement endommagé les gouvernements locaux et nationaux.\n\nLe CISA a créé des régions pour aider les gouvernements étatiques et locaux à faire face aux cyberattaques qui pénètrent les frontières des États-Unis. De plus, les gouvernements étatiques et locaux commencent à partager les meilleures pratiques, les cyberattaques détectées et les vulnérabilités communes trouvées dans leur infrastructure. Un financement supplémentaire du gouvernement fédéral aide ces organisations à renforcer leur position en matière de cybersécurité.\n\n## Attaques cybernétiques contre les infrastructures critiques\n\nL'une des tendances les plus préoccupantes est l'augmentation des attaques contre les systèmes qui gèrent les infrastructures critiques, en particulier la production et la distribution d'électricité. Étant donné que le réseau électrique est composé de plusieurs collectivités privées, étatiques et locales plutôt que d'une agence fédérale centralisée, la gestion et la protection du réseau électrique national sont complexes et intimidantes. Les grandes organisations ont tendance à avoir une meilleure position en termes de cybersécurité que les petites entreprises de services municipaux, ce qui les expose davantage aux cyberattaques. Cependant, tout n'est pas perdu dans la protection de nos infrastructures critiques, car les organisations de ces industries verticales partagent les meilleures pratiques en matière de sécurité physique et informatique.\n\n## Architecture de confiance zéro\n\nJim et Darren ont convenu que le terme « Architecture de confiance zéro » a été trop utilisé et a perdu de son impact, car le secteur privé l'a rapidement adopté et l'a associé à tout ce qui concerne la cybersécurité. Cependant, ils sont tous les deux d'accord pour dire que les principes de confiance zéro doivent être adoptés au sein des organisations afin de protéger pleinement leurs actifs précieux. Ces principes comprennent : la vérification explicite continue, l'utilisation d'un accès au privilège minimal, la supposition d'une violation et l'automatisation de la collecte et de la réponse contextuelle. Ces principes peuvent être mis en œuvre grâce à la technologie, l'amélioration des processus et la formation.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jim Richberg"],"link":"/episode-EDT123-fr","image":"./episodes/edt-123/fr/thumbnail.jpg","lang":"fr","summary":"Dans cet épisode, Darren interviewe Jim Richberg, CISO du secteur public chez Forinet, pour discuter des différences en matière de cybersécurité dans le secteur public. Le gouvernement fédéral est très différent des gouvernements locaux et des États en ce qui concerne la cybersécurité et leurs approches."},{"id":24,"type":"Episode","title":"Combler l'écart de compétences numériques","tags":["majorleaguehacking","hackathon","developer","people"],"body":"\r\n\r\nDans l'épisode consacré à la Transformation Numérique, Darren interviewe John Gottfried, co-fondateur de Major League Hacking, sur son parcours dans l'industrie technologique. Depuis son enfance, John aime jouer avec les ordinateurs et a appris des compétences techniques par nécessité pour faire ce qu'il voulait avec son ordinateur. Il se rendait à la bibliothèque publique locale pour emprunter des livres sur les langages de programmation tels que le C et le PHP, et il suivait les exemples qui y étaient présentés. John a vécu l'évolution d'Internet, du modem au réseau en fibre optique gigabit. Malgré avoir obtenu un diplôme en histoire à l'université, John a décroché son premier emploi de programmeur en exploitant ses compétences de bricolage pour obtenir un emploi à temps partiel qui lui permettait de subvenir à ses besoins. Il a eu l'opportunité de tout faire, de construire des serveurs à écrire du code pour des petites entreprises.\n\n## Environnement pour bricoler\n\nDans le passé, héberger un site web était une tâche complexe qui nécessitait l'achat, la configuration et la maintenance d'un serveur. Cependant, avec l'avènement des services de cloud computing, cela est devenu beaucoup plus simple. Des fournisseurs de cloud tels que AWS, Google Cloud ou tout autre fournisseur proposent des services similaires standardisés pour mettre en place un serveur en seulement cinq minutes. Au début des années 2000, après avoir attrapé la mononucléose, John a commencé sa carrière en tant que programmeur. Ses collègues l'ont entraîné à des événements technologiques et des hackathons à New York, ce qui a changé sa perspective sur l'industrie. Cela l'a inspiré à créer Major League Hacking, une entreprise qui organise des hackathons et des compétitions de codage pour les étudiants.\n\n## Major League Hacking se traduit par \"Ligue majeure de piratage\".\n\nLa Major League Hacking (MLH) a été fondée dans le but de donner du pouvoir aux hackers, un terme englobant toute personne désireuse de créer avec la technologie. La mission de l'organisation est de fournir aux individus les compétences et le réseau de soutien dont ils ont besoin pour démarrer leur carrière dans le domaine de la technologie. Les fondateurs de MLH ont été inspirés par leurs expériences en mentorat étudiant sur les campus et ont voulu en faire une organisation à part entière. Ils ont commencé en organisant des hackathons, des compétitions d'invention qui durent tout un weekend, où les gens se réunissent pour créer des prototypes d'applications et partager des idées. Aujourd'hui, MLH a élargi ses offres pour inclure diverses façons d'aider les individus à développer leurs compétences, notamment des projets pratiques et une communauté de soutien.\n\n## Construire une communauté d'apprentissage\n\nLa Major League Hacking a étendu son influence grâce à divers programmes, notamment des rencontres, des ateliers, des conférences virtuelles et des bourses immersives. Leur objectif était de combler le fossé entre les compétences de base et leur application dans le monde réel. La propagation virale de leurs hackathons et événements a permis de se connecter avec différentes universités et communautés sans effort de marketing. Pour soutenir les leaders locaux dans la construction de leurs propres communautés, ils ont offert un mentorat, les ont mis en relation avec d'autres écoles et ont fait venir des sponsors. Les grandes entreprises et les petites sociétés ont également pu s'impliquer en fournissant des outils du monde réel, des API et des outils propriétaires aux hackers. De cette manière, ils ont offert une exposition et un mentorat et les ont récompensés pour la création de projets intéressants.\n\n## Combler le fossé des compétences\n\nLes hackathons offrent une expérience d'apprentissage unique qui réunit des personnes qui ne travailleraient pas ensemble autrement. De nombreux hackathons sont parrainés par des représentants d'entreprises qui envoient des ingénieurs, des recruteurs ou des évangélistes développeurs pour passer le week-end avec des étudiants, les aider à déboguer leur code et leur enseigner les API. Bien que les hackathons soient amusants et offrent des pizzas gratuites et des cadeaux promotionnels, ils offrent également une éducation précieuse et une pratique concrète. Les hackathons sont bénéfiques pour les professionnels également et toute personne peut bénéficier du fait de s'engager dans un temps et un lieu spécifiques pour construire quelque chose. Les projets de hackathon sont souvent ouverts et des prix sont souvent offerts par des sponsors pour différentes catégories. Les équipes de projet se forment de manière organisée lors des hackathons, et de nombreux liens durables et même des startups ont été créées après s'être rencontrés lors de ces événements.\n\n## Succès du hackathon.\n\nJohn raconte une histoire au sujet d'une équipe d'étudiants du secondaire qui ont construit un prototype d'application lors d'un de leurs événements, automatisant des tâches sur l'iPhone. Apple a acquis leur entreprise quatre ans plus tard et a fait de leur projet une fonctionnalité essentielle d'iOS, ce qui était une réalisation impressionnante pour un projet qui avait commencé lors d'un hackathon. John croit que les hackathons sont une excellente source de talents pour les stagiaires et les nouveaux diplômés et pourraient bientôt remplacer les salons de l'emploi. Les personnes intéressées à participer à des hackathons peuvent visiter le site web de MLH pour en savoir plus sur les prochains événements et les opportunités de sponsoring.\n\nPlus d'informations sur les hackathons à proximité de chez vous peuvent être trouvées sur https://mlh.io/\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jon Gottfried"],"link":"/episode-EDT128-fr","image":"./episodes/edt-128/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren parle à John Gottfried, co-fondateur de Major League Hacking, de la fermeture de l'écart de compétences numériques grâce à un travail collaboratif pratique utilisant des hackathons."},{"id":25,"type":"Episode","title":"HPC à la demande","tags":["hpc","technology","compute","openondemand","ohiosupercomputercenter","osc"],"body":"\r\n\r\nDans cet épisode de podcast, Darren Pulsipher, l'architecte principal des solutions du secteur public chez Intel, interviewe Alan Chalker du Ohio Supercomputer Center sur la manière de surmonter les obstacles au calcul haute performance (HPC). Alan est le directeur des programmes stratégiques au Ohio Supercomputer et travaille depuis plus d'une décennie sur un projet financé par la NSF appelé Open OnDemand. Le projet vise à rendre le HPC plus accessible aux consommateurs en général qui sont habitués à effectuer des activités en ligne, comme la banque en ligne et les achats en ligne. Open OnDemand simplifie le processus d'utilisation du HPC en éliminant le besoin de saisir des commandes en ligne de commande. L'expérience d'Alan comprend l'obtention de sa licence en génie électrique et informatique à l'Ohio State University, puis l'obtention de son doctorat en ingénierie biomédicale de l'USC Chapel Hill.\n\n## Histoire d'Open OnDemand\n\nEn 2006 et 2007, une interface web a été développée en collaboration avec quelques techniciens et l'Edison Welding Institute, qui l'a ensuite nommée Open OnDemand. Elle a commencé comme une simulation en ligne pour le soudage, puis s'est étendue pour inclure une simulation pour les polymères et une autre à usage général. Lors de sa présentation lors de différentes conférences, d'autres institutions de recherche en informatique ont exprimé leur intérêt pour sa mise en place sur leurs systèmes. Pour en faire un logiciel open source, la National Science Foundation leur a accordé un programme de trois ans d'une valeur de 300 000 dollars, ce qui a rendu le prototype plus robuste. Le succès d'Open OnDemand a ensuite donné lieu à un autre programme de cinq ans d'une valeur de 3 millions de dollars. Aujourd'hui, il est déployé sur tous les continents, à l'exception de l'Antarctique, et sert plus de 400 institutions de recherche en informatique.\n\n## Expansion de l'influence du calcul haute performance (HPC)\n\nLes supercalculateurs se sont étendus au-delà des domaines traditionnels tels que l'informatique et l'ingénierie. À l'OSC, les étudiants en anthropologie et en sciences politiques utilisent le supercalculateur pour leurs recherches, ainsi que les étudiants en horticulture et en agronomie. La demande pour le supercalculateur augmente, avec plus de 8 500 personnes utilisant les supercalculateurs de l'OSC du monde entier au cours de l'exercice financier dernier. De plus, pendant la pandémie, de nombreuses universités ont pu continuer à enseigner et à faire de la recherche à distance grâce aux postes de travail virtuels fournis par le supercalculateur.\n\n## Comparaison des modèles tarifaires CSP et HPC\n\nLes modèles de tarification pour le superordinateur sont basés sur les heures de calcul et les mois de téraoctets, et le mandat gouvernemental permet des prix subventionnés pour les entités académiques basées dans l'Ohio. Les fournisseurs de services cloud facturent en fonction du temps écoulé, pas des heures de calcul, et en fonction du stockage des données et des coûts de réseau de sortie. Les clients de l'industrie commerciale commencent à utiliser des superordinateurs pour les charges de travail de simulation HPC traditionnelles, ce qui leur permet d'économiser de l'argent par rapport à leur exécution dans des clouds publics de détail.\n\n## Exemples d'utilisation commerciale\n\nLe jour précédant le moment où Darren et Alan se sont assis et ont discuté, des tornades se sont produites dans la région de Columbus, Ohio. Les prévisions météorologiques sont importantes pour de nombreuses industries, et les supercalculateurs sont bien adaptés à cette tâche. Le centre génère des prévisions météorologiques toutes les 4 à 6 heures pour des clients tels que des compagnies d'expédition et des compagnies aériennes. Bien que les charges de travail de calcul haute performance traditionnelles soient encore courantes, de nouvelles émergentes comprennent l'analyse des tweets des membres du Congrès par rapport à la COVID-19, l'anthropologie, l'horticulture et les sciences des cultures. Tout ce qui est limité dans le temps ou implique trop de données peut bénéficier des capacités de calcul haute performance. La demande pour ces capacités devrait augmenter en raison d'une accessibilité accrue. Rendre la consommation de calcul haute performance plus facile est similaire à ce que le cloud a fait pour le calcul en grille à l'époque.\n\n## Capacité OSC\n\nLa capacité impressionnante du système informatique haute performance de l'OSC se développe constamment pour répondre à la demande. Au moment de l'enregistrement, ils disposaient de 55 000 cœurs, principalement de la marque Intel, avec 400 accélératrices réparties sur 1600 nœuds. Ils prévoient une nouvelle acquisition qui leur permettrait d'atteindre de 75 000 à 80 000 sièges en raison de la demande croissante dans les domaines biomédicaux. Le système peut gérer de grandes quantités de données, avec une capacité de stockage réel de 20 pétaoctets et une connectivité réseau atteignant une vitesse de lecture/écriture de 350 gigabits par seconde. Un des avantages significatifs de l'OSC est l'absence de frais de sortie pour leurs clients grâce à la fondation de l'organisation par le biais d'une subvention de la National Science Foundation.\n\n## Ouvrez OnDemand\n\nDe nombreuses universités et centres de calcul à haute performance (HPC) utilisent Open OnDemand comme une interface web simple pour rendre l'HPC plus accessible aux chercheurs qui ont besoin d'apprendre ou de comprendre les complexités de la planification des tâches, de la décomposition des ensembles de problèmes et de la gestion des données sur un cluster. Même les fournisseurs de services cloud ont des interfaces Open OnDemand pour leurs offres HPC.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Alan Chalker"],"link":"/episode-EDT129-fr","image":"./episodes/edt-129/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe Alan Chalker, directeur des programmes stratégiques chez Ohio Super Computer Center, à propos d'Open OnDemand pour les clusters HPC dans le monde entier."},{"id":26,"type":"Episode","title":"Obsession illogique avec la logique","tags":["data","organizationalchange","people"],"body":"\r\n\r\n## Une Contraste de Préoccupations <h2>\n\nAu cours des dix dernières années, les professionnels des données ont connu une explosion de leur capacité à rendre les logiciels visibles, accessibles, utilisables et portables, mais du côté des données, il n'y a pas eu autant de progrès. C'est quelque chose à laquelle les professionnels des logiciels et des données devraient réfléchir. Quelles sont les préoccupations de chaque côté ? Que pouvons-nous apprendre les uns des autres ? Actuellement, les deux côtés sont diamétralement opposés sur de nombreux points.\n\nDu côté de l'ingénierie logicielle, la logique métier est la principale préoccupation. Afin d'avoir des interfaces cohérentes, les ingénieurs dissimulent les détails. Les données sont perçues comme une sortie. En revanche, les scientifiques des données sont plus préoccupés par le contexte de ces sorties et des constructions de données : les métadonnées. Par exemple, la filiation des données est importante pour un scientifique des données afin de voir comment les choses changent au fil du temps, alors qu'un ingénieur logiciel chercherait à dissimuler ces détails pour éviter des problèmes tels que la variabilité et les bugs.\n\nTraiter le développement des données et des métadonnées comme une discipline à part entière, et non dans le contexte de la façon dont nous faisons actuellement des logiciels, pourrait aider l'industrie à se développer. En d'autres termes, nous devons envisager de construire une infrastructure composable qui prend en compte les préoccupations des deux côtés.\n\nUn exemple de la façon dont nous traitons actuellement les problèmes de métadonnées dans les entreprises illustre cette idée. Aujourd'hui, nous disposons de systèmes de gestion centralisée des métadonnées. Nous voulons savoir où se trouvent toutes nos données : qui, où, pourquoi et comment. Capturer ce que les gens font et le pousser dans un système centralisé est une manière très logicielle de faire les choses. Si nous laissons les données se développer d'elles-mêmes, nous pourrions adopter ce que font les développeurs de logiciels, mais dans un contexte de données. Nous pourrions construire un écosystème plus vaste si, au lieu de tant d'efforts privés des deux côtés, nous mettions tout dans un référentiel synchronisé et centralisé et permettions aux développeurs de données de travailler comme le font les ingénieurs logiciels. Nous pourrions construire et organiser des efforts privés, puis partager ces améliorations avec d'autres. De manière similaire au changement de gestion de configuration dans les années 1990 et 2000, d'un endroit centralisé à un système plus distribué, le partage pourrait se faire plus facilement et fluidement.\n\n## ADN de connaissances numériques <h2>\n\nLa connaissance numérique que nous avons dépend à la fois de la logique et des données. Le logiciel et les données partagent les mêmes primitives, et à mesure que nous progressons dans la pile vers la connaissance, il existe des relations étroites entre les deux. Ce qui diverge, c'est que nous disposons de l'infrastructure et des outils pour développer le côté logiciel, le déployer et le rendre visible, accessible et utilisable. Puisque nous ne considérons pas le côté des données de la même manière, nous sommes limités dans ces domaines. Par exemple, une ancienne approche consiste à ce que les scientifiques des données effectuent de grandes analyses, de l'IA et de l'apprentissage sur leurs données et obtiennent des informations précieuses, mais il n'existe aucun mécanisme répétable, limitant son utilisation.\n\nUne fois que cette perspective est reconnue à la fois par la communauté des données et des logiciels, nous pouvons adopter une approche différente en utilisant les succès du développement logiciel pour les données. Au lieu d'appliquer les expériences personnelles et les préjugés des logiciels en général, nous pouvons examiner comment les données fonctionnent, comment elles sont similaires mais ont leurs propres problèmes. Une analogie serait d'emmener votre famille à Disneyland. Les expériences et les interactions là-bas représentent la dynamique des logiciels. Vous pourriez orienter les expériences vers le côté des données. Mais imaginez dans le monde réel si vous deviez laisser cette expérience de côté et que lorsque vous rentriez chez vous, vous ne saviez rien à ce sujet. Voilà le problème. Chaque fois que nous passons à un système ou à un écosystème différent, nous recréons un nouveau monde et ignorons les autres mondes que nous avons dû laisser derrière nous. Si nous pouvions partager l'expérience ou l'emporter avec nous, nous constaterions que nous aurions une infrastructure de connaissances beaucoup plus dynamique. Ainsi, la prochaine fois que vous visiterez Disneyland, en fonction de votre expérience, vous saurez quel moment de la journée est le meilleur, comment vérifier les temps d'attente, etc. Sans l'expérience de travailler avec les données en conjonction avec l'application, c'est presque comme recommencer à zéro à chaque fois.\n\nUn exemple concret est l'amnésie qui se produit dans des systèmes tels que ceux du secteur de la santé, où un professionnel des données crée une intégration à partir de zéro, puis quelques années plus tard, quelqu'un d'autre doit faire la même chose. Si nous pouvons établir de meilleures relations avec les données grâce à la cartographie, la réutilisabilité et l'efficacité augmentent. Par exemple, pourquoi avons-nous autant de notions d'une personne en termes de modélisation ? Bien sûr, le contexte est important, mais pourquoi ne pouvons-nous pas voir les différentes variantes d'une personne et être ensuite en mesure de les cartographier ? Dans le système de soins de santé de l'Administration des anciens combattants, ils ont plusieurs systèmes où un patient signifie quelque chose de différent dans chacun d'entre eux. La cartographie offrirait un terrain commun, mais permettrait également des changements en fonction du contexte, tant que l'opération de cartographie était visible. Nous pourrions alors avancer avec différents types de cas d'utilisation et de réutilisabilité.\n\nUn grand obstacle est que nous avons réussi à opérationnaliser efficacement la logique avec K8s, mais un service équivalent n'existe pas pour les données. Bien qu'il existe une solution temporaire avec S3, ce n'est pas la réponse. Il y a un grand besoin dans tous les secteurs d'une solution similaire à K8s qui traite des données. La nécessité de collaboration est importante ici. Bien sûr, les professionnels des données souhaitent apporter de la valeur à leur organisation, mais une certaine partie dépend de la communauté.\n\n## Fermeture des écarts <h2>\n\nLe nom de la société Datacequias est basé sur les acequias au Nouveau-Mexique, qui servent d'exemple du type de collaboration nécessaire pour les données. Le Nouveau-Mexique est une région aride, donc il y a des années, pour rendre la terre fertile, les habitants ont construit une série de canaux d'irrigation appelés acequias. Personne ne les possédait, mais ils les ont construits, gérés et entretenus par nécessité et pour le bien commun. L'environnement hostile dans le monde des données est généralement caractérisé par les budgets et la possession des données, mais une curation des données plus axée sur la communauté serait bénéfique pour tous, tout comme les acequias ont profité à tous.\n\nImaginez si les professionnels des données pouvaient dériver un ensemble de données à partir de n'importe quel référentiel central. Ils pourraient le gérer et le faire évoluer selon leurs besoins. En cas de changement dans le référentiel central géré par un organisme standard, ils pourraient incorporer immédiatement ces changements, ou choisir de ne pas le faire. Dans tous les cas, ils auraient l'origine de la source originale. Aujourd'hui, lorsque nous utilisons une ressource qui est en dehors de l'entreprise, nous en faisons une copie figée dans le temps. Cela nécessite un suivi manuel et une gestion des mises à jour. Avec un référentiel central, tous pourraient co-créer, collaborer et former des communautés reposant sur des bases communes et une généalogie visible.\n\nCeci n'est que la partie visible de l'iceberg d'un changement fondamental dans l'industrie visant à rendre les données plus précieuses pour votre organisation. Pour plus d'informations sur Andrew Padilla et Datacequia, visitez datacequia.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Andrew Padilla"],"link":"/episode-EDT13-fr","image":"./episodes/edt-13/fr/thumbnail.jpg","lang":"fr","summary":"Le leader d'opinion Andrew Padilla de Datacequia envisage de nouveaux développements dans la gestion des données et la collaboration, qui permettraient aux données de progresser, tout comme le logiciel, en termes de visibilité, d'accessibilité, d'utilisabilité et de portabilité. Il explique comment une infrastructure composable répondrait aux préoccupations des ingénieurs logiciels et des scientifiques des données."},{"id":27,"type":"Episode","title":"Produitiser l'IA Décisionnelle","tags":["collectiongenerativeai","ai","decisionalai","aiproductization","people","process"],"body":"\r\n\r\nDans cet épisode, Darren Pulsipher, architecte principal des solutions du secteur public chez Intel, interviewe son fils Matthew Pulsipher, chef de produit, sur la productisation de l'IA décisionnelle. Matthew explique que l'IA générative est construite à partir d'ensembles de données générales et convient aux questions de connaissances générales, mais qu'elle manque de prévisibilité et de déterminisme, ce qui rend difficile l'automatisation des processus. En revanche, l'IA décisionnelle est plus simple dans sa portée mais plus axée sur le contexte, ce qui lui permet de prendre des décisions basées sur les besoins spécifiques de l'entreprise. Matthew partage son expérience de l'intégration de l'IA décisionnelle dans les produits et met en évidence l'importance du contexte dans l'IA.\n\n## Type d'IA\n\nIl existe différents types d'IA, et chacun a une capacité unique d'aider les organisations à automatiser des processus, à prendre des décisions commerciales et à augmenter le travail humain. L'IA décisionnelle est principalement utilisée pour la prise de décisions et repose sur des modèles générés à partir de données précédentes. Par contre, l'IA prédictive génère des valeurs prévues en fonction de modèles personnalisés et de jeux de données. La formation des modèles est essentielle au déploiement et à la mise en œuvre des solutions d'IA. La clé est d'identifier un problème réel qui est opérationnellement pertinent et réalisable dans un délai raisonnable.\n\n## IA pour rationaliser les processus\n\nL'intelligence artificielle (IA) peut être utilisée pour rationaliser les processus de prise de décision dans les entreprises. Il est important de définir les capacités de l'IA selon un ensemble spécifique d'options, afin de ne pas submerger le système et de rendre le processus décisionnel plus efficace. L'IA est particulièrement adaptée aux processus répétitifs, impliquant des décisions basées sur des données et nécessitant des évaluations subjectives humaines. Par exemple, une institution financière peut utiliser l'IA pour valider les permis de conduire à l'aide de données extraites. La formation continue de l'IA grâce aux commentaires des utilisateurs peut améliorer ses capacités de prise de décision et éventuellement remplacer totalement le besoin d'évaluation humaine. Le projet d'indexation d'Ancestry.com est un exemple de la façon dont l'apprentissage renforcé peut réduire la nécessité d'une intervention humaine au fil du temps.\n\n## Humain dans la boucle\n\nLors de la création d'un backend d'apprentissage automatique, il est essentiel de garder à l'esprit les besoins de l'utilisateur. L'objectif est de simplifier leur processus actuel et de fournir une aide pour les aider à accomplir leur travail de manière plus efficace. Pour y parvenir, il est crucial d'interviewer et d'observer les utilisateurs dans leur environnement actuel afin de comprendre leur comportement, d'identifier les inefficacités et de documenter toutes les inférences qu'ils peuvent faire et qui ne sont peut-être pas documentées. En agissant ainsi, vous pouvez créer les données pour extraire les inférences avant de les envoyer au modèle, ce qui donnera des résultats plus précis en reproduisant le comportement humain. Il est important de se rappeler que les inférences sont souvent plus importantes que les données brutes, et en comprenant le comportement et les besoins de l'utilisateur, vous pouvez concevoir un meilleur produit d'IA.\n\nUn autre facteur clé dans le déploiement de l'IA est le facteur humain. Il s'agit d'établir le bon contexte lors de la mise en place de l'automatisation afin d'éviter la peur de la perte d'emploi. Comment faire face aux problèmes potentiels entre l'IA et l'humain, tels que les parties prenantes contournant l'IA mise en place pour elles. Une solution consiste à concevoir l'interface de manière à ce qu'il soit clair qu'un utilisateur a examiné un point de données spécifique et à fournir des outils de substitution si nécessaire. De plus, interroger les parties prenantes sur les raisons qui les poussent à contourner le système peut aider à améliorer le modèle et à prévenir les fraudes de l'API. En fin de compte, l'IA et les humains peuvent travailler ensemble pour obtenir de meilleurs résultats.\n\nL'IA peut s'occuper des tâches simples si elle est bien entraînée, et les humains peuvent apprendre à exploiter mieux l'IA au fil du temps. En créant des interfaces collaboratives qui font de l'IA un membre de l'équipe plutôt qu'un simple algorithme froid, permettant des interactions plus naturelles qui l'aident à apprendre davantage. L'IA deviendra essentielle dans tout travail impliquant des parties prenantes qui traitent des données humaines en raison de la variabilité impliquée.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matthew Pulsipher"],"link":"/episode-EDT130-fr","image":"./episodes/edt-130/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe son fils Matthew Pulsipher sur la productisation de l'IA décisionnelle. Matthew a récemment modernisé son pipeline de développement de produits pour inclure l'IA décisionnelle dans son processus de développement de produits."},{"id":28,"type":"Episode","title":"Transformation numérique dans le gouvernement fédéral","tags":["compute","people","technology","automation","ai","cybersecurity","innovation"],"body":"\r\n\r\nMark a travaillé avec le gouvernement fédéral pendant plus de 20 ans et a vu de première main comment les agences adoptent les nouvelles technologies pour améliorer leurs services aux citoyens. Une tendance qu'il a remarquée est une focalisation sur les expériences clients, telles que l'utilisation d'applications de passeport mobiles par les services des douanes et de protection des frontières. Malgré les défis de la mise en œuvre d'initiatives à grande échelle dans des environnements en constante évolution, Mark voit de nombreuses opportunités pour le gouvernement d'accélérer sa mission en adoptant de nouvelles technologies.\n\nLa COVID a accéléré l'innovation au sein du gouvernement fédéral et a révélé des lacunes dans des domaines tels que les connexions VPN et la sécurité pour les travailleurs à distance. Les agences continuent d'innover, en se concentrant sur la technologie cloud et l'amélioration de l'expérience client. L'éducation et la communication sont essentielles pour prendre des décisions éclairées sur l'emplacement des charges de travail et la mise en œuvre des technologies cloud. Malgré les craintes que la bureaucratie puisse ralentir l'innovation, les avantages de l'innovation continuent de stimuler l'innovation au sein du gouvernement fédéral, notamment dans les domaines du cloud, du calcul périphérique, de la cybersécurité et de la chaîne d'approvisionnement sécurisée.\n\n## Multi-Hybrid Cloud translates to \"Cloud hybride multiple\" in French.\n\nIl existe une tendance croissante à l'utilisation de clouds multi-hybrides et aux divers défis qui en découlent, tels que la sécurité et la gestion des données. L'importance de disposer d'un plan bien structuré pour les projets de transformation numérique a été soulignée, en mettant l'accent sur la nécessité de réussir dès le début. Une autre innovation est le calcul \"edge\" (périphérique), qui permet de tirer plus rapidement des informations des données sans nécessiter de connexion. Plusieurs organisations travaillent actuellement à trouver comment exploiter la puissance du calcul \"edge\" pour améliorer leurs services. Certains cas d'utilisation comprennent la surveillance des capteurs d'eau dans l'agriculture et l'optimisation des itinéraires de livraison de courrier pour le service postal américain.\n\n## Automatisation\n\nLorsque l'automatisation et la cybersécurité sont combinées, elles peuvent accélérer la capacité d'une organisation à réagir aux menaces, déployer de nouvelles capacités et fournir de meilleurs services à leurs bénéficiaires. La pénurie de main-d'œuvre due au départ à la retraite des baby-boomers nécessitera l'automatisation pour faire face à ce problème. Un exemple est la façon dont l'automatisation a amélioré le processus d'inspection à la frontière pour les produits agricoles et les fleurs, ce qui a permis des temps de traitement plus rapides et une durée de conservation plus longue.\n\n## La cybersécurité\n\nL'environnement IT/OT actuel est un paysage de menace constamment évolutif, avec des acteurs malveillants qui innnovent en permanence. Intel innove et crée une technologie de sécurité matérielle qui chiffre les données pour se protéger contre des acteurs néfastes. La cybersécurité est une préoccupation majeure pour le gouvernement fédéral, comme indiqué dans le récent communiqué de presse du président Biden soulignant la protection des infrastructures critiques comme le principal problème de cybersécurité. La gestion essentielle des infrastructures aux États-Unis, compte tenu de la structure étatique unique du pays et de la participation de diverses entités telles que les municipalités locales et les entreprises d'énergie privées. L'Agence de sécurité des infrastructures critiques (CISA) guide la cybersécurité et établit des normes pour des systèmes sécurisés à tous les niveaux fédéral, étatique et local.\n\n## Chaîne d'approvisionnement sécurisée\n\nUne autre vulnérabilité majeure exposée par COVID-19 est le risque géopolitique et les menaces en cybersécurité pesant sur la chaîne d'approvisionnement. La chaîne d'approvisionnement transparente d'Intel permet aux clients de vérifier l'intégrité des composants, ce qui est crucial pour garantir la sécurité matérielle et logicielle. Grâce à ces mesures en place, les États-Unis prennent des mesures visant à renforcer leur sécurité industrielle et nationale.\n\nLa chaîne d'approvisionnement ne concerne pas seulement les actifs physiques ; les logiciels, les micrologiciels et le matériel jouent tous un rôle important dans la sécurité de la chaîne d'approvisionnement. Tous les composants ont des vecteurs d'attaque et nécessitent des logiciels sécurisés, des matériaux de construction, des vérifications de composants et une intégrité des appareils pour la validation, une ordonnance exécutive concernant les logiciels et les matériaux, à laquelle Intel se conforme sur ses nouveaux processeurs. Intel propose des technologies de sécurité matérielles et des capacités transparentes de la chaîne d'approvisionnement dans le cadre d'une architecture de confiance zéro.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Mark Valcich"],"link":"/episode-EDT131-fr","image":"./episodes/edt-131/fr/thumbnail.jpg","lang":"fr","summary":"Dans cet épisode, Darren interviewe Mark Valcich, directeur et GM du secteur public civil fédéral chez Intel. Les années d'expérience de Mark se manifestent alors qu'il décrit les tendances actuelles de la transformation numérique dans le gouvernement civil fédéral."},{"id":29,"type":"Episode","title":"Gestion BareMetal définie par logiciel","tags":["metify","sdi","technology","baremetal","heterogeneouscompute","compute"],"body":"\r\n\r\nAvec plus de deux décennies d'expérience dans le domaine des centres de données, Ian partage ses connaissances sur l'optimisation de l'infrastructure, l'automatisation de la gestion des serveurs et la rationalisation des différents composants d'un centre de données. De son côté, Mike, qui a travaillé précédemment chez IBM et Red Hat, se concentre sur la vente consultative et les ventes dirigées par les canaux pour mieux comprendre les problèmes et les tendances des entreprises dans l'exploitation de leurs centres de données. Ensemble, ils ont créé Metify en 2020 pour fournir des solutions d'automatisation pour la gestion des serveurs, du stockage et des appareils réseau dans les centres de données, malgré l'essor des services de cloud public tels que RWC Azure et GCP.\n\n## Automatisation du centre de données\n\nAutomatiser la gestion d'un centre de données peut rapprocher la gestion sur site de l'expérience d'un cloud public tout en réduisant les coûts. L'un des éléments essentiels de cette transformation est la standardisation et l'utilisation de normes ouvertes comme la spécification Redfish DMCA pour rendre cette automatisation possible. L'ubiquité de cette spécification sur les cartes mères de classe entreprise la rend accessible à tous les acteurs pour s'y intégrer. Les pressions financières et la demande des utilisateurs sont essentielles pour pousser les fabricants d'équipements d'origine à mettre en œuvre ces normes ouvertes. Dans l'ensemble, l'automatisation et la standardisation peuvent aider les centres de données à concurrencer les fournisseurs de services cloud en termes de coûts opérationnels tout en améliorant les capacités de gestion des serveurs.\n\nL'efficacité du centre de données peut être améliorée en utilisant des outils qui permettent l'automatisation et l'intégration avec des frameworks standard. Les fournisseurs de matériel font face à une pression due à la banalisation de la technologie, ils doivent donc se différencier en offrant des serveurs similaires en boîtier blanc à plus petite échelle. Metify offre une interface unique pour gérer les appareils de n'importe quel fabricant, tant qu'ils sont compatibles avec BMC et respectent les spécifications Redfish. Il existe des normes émergentes pour gérer les petits appareils via Redfish ; la question demeure sur jusqu'où l'extensibilité descend dans les parties spécifiques au réseau de la pile technologique.\n\n## Cloud hybride\n\nLa montée en puissance des stratégies de cloud hybride guide la manière dont les entreprises gèrent leurs centres de données, leurs environnements de périphérie et de cloud. La croissance du cloud public est significative ; cependant, il y a une croissance massive dans les espaces de cloud privé. Metify offre une expérience similaire au cloud du point de vue des opérations, permettant aux administrateurs système de gérer leur périphérie, leur centre de données ou plusieurs centres de données via une API standardisée. Maintenir les API ouvertes et normalisées est essentiel pour que les clients puissent utiliser des outils de gestion familiers tels que TerraForm et Ansible. Les technologies de cloud hybride permettent aux entreprises d'optimiser les coûts, la gouvernance, la sécurité et l'efficacité.\n\nL'un des dangers d'une norme ouverte peut être sa vulnérabilité aux atteintes à la sécurité sans les mesures de commande et de contrôle appropriées. Metify aborde ce problème en mettant en place une approche systémique pour gérer le matériel nu à travers des matériels hétérogènes en intégrant l'audibilité, l'autorisation, l'accès et les contrôles aux normes ouvertes. Le produit de Metify se concentre sur la fourniture d'un niveau de contrôle pour prévenir les incidents indésirables, et ils s'intègrent avec des outils de gestion de flux de travail pour l'automatisation.\n\n## BareMetal SDI se traduit en français par \"SDI natif\".\n\nDe nombreuses technologies de centres de données modernes reposent sur la virtualisation comme base de leur plan de contrôle de gestion. Cependant, la gestion des infrastructures dédiées reste une tâche traditionnelle nécessitant beaucoup de travail manuel. Cette approche unique de l'infrastructure de cloud privé ne repose pas sur la virtualisation. Au lieu de cela, elle utilise une approche d'infrastructure définie par logiciel pour la gestion des infrastructures dédiées, permettant un mélange d'infrastructures dédiées, de machines virtuelles (VM) et de conteneurs. Cette approche permet aux administrateurs système de gérer l'infrastructure sous-jacente de manière plus efficace et de fournir une expérience cloud sans friction. Elle peut prendre en charge des environnements informatiques hétérogènes, où les processeurs (CPU), les processeurs graphiques (GPU), les processeurs de flux programmable (FPGA), les processeurs vidéo programmables (VPU) et les processeurs réseau programmables (NPU) peuvent être exploités pour plusieurs flux de travail. Avec Redfish et une extension du schéma, Metify peut facilement contrôler de nouveaux appareils, et ils considèrent l'écosystème en expansion comme extrêmement précieux pour le développement de produits.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ian Evans","Mike Wagner"],"link":"/episode-EDT132-fr","image":"./episodes/edt-132/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe les fondateurs de Metify, Ian Evans et Mike Wagner, à propos de leur approche unique de gestion de l'infrastructure définie par logiciel bare metal en utilisant la norme Redfish."},{"id":30,"type":"Episode","title":"Leçons sur le HPC (High Performance Computing) dans l'industrie pétrolière et gazière","tags":["compute","hpc","people","innovation","cloud"],"body":"\r\n\r\nKeith avait travaillé dans l'industrie pendant toute sa carrière et a rejoint Intel en juillet dernier. Il avait dirigé le groupe de Calcul Haute Performance de Consolidated Resources depuis 1999, où ils ont commencé avec seulement un dixième d'un téraflop de puissance de calcul. Cependant, Keith a réussi à consolider plusieurs sites et a établi leur équipe comme étant essentielle aux opérations d'exploration, de développement et de production d'huile et de gaz de BP. Au cours des trois dernières décennies passées à travailler dans le calcul haute performance pour l'industrie pétrolière et gazière, Keith a été influencé par trois facteurs déterminant la direction du HPC : le cloud computing, une pénurie de talents humains, et l'équilibre entre la collaboration et l'avantage du premier arrivé.\n\n## Nuage ou en interne\n\nKeith a partagé son expérience de construction d'une installation HPC (High-Performance Computing) pour BP. La construction a duré près de neuf ans, dont les sept premières années ont été consacrées à la réparation de l'installation existante et à la création d'un plan d'affaires pour une installation dédiée. Malgré l'essor de la technologie cloud, ils ont constaté que l'exploitation de leur installation était rentable en raison du contrôle des ressources et de l'architecture sur mesure. Étant donné qu'ils travaillent dans l'industrie pétrolière et gazière, ils ont dû faire face à des défis uniques pour traiter de vastes quantités de données, mais disposer d'une installation dédiée leur a permis de se concentrer sur les besoins de recherche de BP. Ces facteurs sont également pertinents pour d'autres industries qui travaillent avec des grappes HPC.\n\nLa grande question pour toute personne gérant un cluster HPC est de savoir si les charges de travail HPC peuvent être exécutées avec succès sur des services cloud généraux. Cela dépend des besoins spécifiques de l'entreprise et de l'échelle de la charge de travail en question. Alors que certaines entreprises peuvent trouver pratique de collaborer avec le cloud pour répondre à des pics de demande, d'autres ont besoin d'un cluster important et pleinement utilisé pour répondre à leurs besoins. Des facteurs tels que les performances prédictives, la gravité des données, la taille des données et le coût des opérations sont des éléments essentiels dans le placement des charges de travail HPC.\n\n## Lutter contre la pénurie de ressources\n\nAcquérir et conserver des talents dans le domaine de l'informatique haute performance est un défi, notamment avec le départ à la retraite de la génération des baby-boomers. Keith suggère que le développement de relations avec les universités et l'identification précoce de personnes talentueuses sont essentiels pour développer les compétences nécessaires au sein d'une organisation. Les organisations doivent réfléchir à long terme pour créer des équipes HPC performantes et les former grâce à des mentorats. Au sein de ces groupes hétérogènes, des scientifiques, des ingénieurs système, des mathématiciens et des ingénieurs informaticiens travaillent ensemble pendant des années au lieu de quelques mois.\n\nBâtir des engagements à long terme en investissant dans des stages, des filières et des bourses est essentiel pour créer un environnement de travail axé sur la carrière. Keith partage un défi auquel ils ont été confrontés lors de la création d'une vision partagée pour l'avenir de leur équipe, mais a réussi en construisant un réseau de contacts en dehors de BP pour les aider à atteindre leurs objectifs.\n\n## Avantage du premier arrivé, Partage\n\nUn autre aspect critique du HPC dans le pétrole et le gaz est d'avoir l'avantage du premier arrivé. Trouver une nouvelle technique pour découvrir de nouvelles sources d'énergie peut économiser des milliards de dollars à l'entreprise. Cependant, le partage d'informations avec les concurrents peut également conduire à des découvertes bénéfiques pour l'industrie. C'est un équilibre important que les professionnels du HPC doivent comprendre pour tirer le meilleur parti de leur travail.\n\nKeith raconte une histoire amusante sur la façon dont leurs amis ont contribué à développer une technologie innovante pour mieux comprendre les structures géologiques du sous-sol du golfe du Mexique afin de trouver du pétrole et du gaz. Malgré la surveillance et les interrogations de la concurrence concernant leurs méthodes, les expérimentations de BP se sont avérées fructueuses. Keith explique que tout le monde observe lorsque vous faites quelque chose de nouveau sur le terrain.\n\nPar exemple, ils ont souvent contracté avec des entreprises comme PGS pour acquérir des données et ont trouvé une solution innovante en utilisant des hélicoptères et des disques durs pour livrer les nouvelles données acquises pour le traitement. Les résultats étaient impressionnants, montrant de meilleurs résultats que les méthodes traditionnelles. Lorsqu'on lui a demandé quel serait l'avenir du calcul à haute performance (HPC) dans l'industrie, Keith estime que la collaboration restera nécessaire, que ce soit par le biais de clusters partagés ou en collaborant sur des références scientifiques fondamentales. Malgré les défis, l'industrie du HPC est passionnante et offre de grandes opportunités aux personnes possédant des compétences en informatique et une volonté de résoudre des problèmes stimulants.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Keith Gray"],"link":"/episode-EDT133-fr","image":"./episodes/edt-133/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe Keith Gray, ancien directeur de l'informatique à haute performance chez British Petroleum. Avec plus de 30 ans de gestion de centres de calcul HPC, Keith offre une grande perspective sur les défis, les meilleures pratiques et l'avenir du calcul à haute performance."},{"id":31,"type":"Episode","title":"WaveForm AI.","tags":["collectiongenerativeai","artificialintelligence","waveformai","datashapes","waveform","technology"],"body":"\r\n\r\nLogan est un ancien agent du renseignement au sein du Ministère de la Défense et il a une passion pour l'intelligence artificielle et la robotique, ce qui l'a conduit à devenir réserviste au sein de la communauté de la défense. Il a fait la transition vers DataShapes, où ils disposent d'une technologie unique qui répond aux besoins critiques de la communauté de défense en utilisant l'IA. Bien que l'IA existe depuis longtemps, la quantité considérable de données disponibles pour entraîner les modèles et l'avancement des ressources informatiques ont permis le développement de systèmes plus intelligents tels que les chatbots et les grands modèles linguistiques.\n\nLes techniques actuelles sont gourmandes en ressources et très coûteuses à former et à construire une solution d'inférence polyvalente. Par exemple, l'exécution de grands modèles linguistiques comme ChatGPT peut coûter jusqu'à 3 millions de dollars par jour, mais l'IA évolue rapidement et suscite plus d'attention que jamais. Un souci dans l'utilisation des réseaux neuronaux est l'incapacité de vérifier et d'expliquer comment l'IA est parvenue à un résultat. Il existe des défis sociaux, politiques et juridiques pour faire confiance aux décisions prises par ces réseaux, en particulier dans les domaines où la vie humaine est en jeu. Cependant, la société finira par surmonter ces défis et adoptera pleinement l'IA. L'approche de DataShapes, qui utilise des techniques traditionnelles d'apprentissage automatique pour résoudre les problèmes d'analyse de données, permet une auditabilité et une découverte complètes dans leurs modèles entraînés.\n\nDataShapes a une approche unique pour résoudre rapidement et efficacement les problèmes dans des environnements contraints en ressources. Alors que l'apprentissage traditionnel des réseaux neuronaux nécessite beaucoup de données étiquetées et peut être fragile, DataShapes utilise des méthodologies pour apprendre en temps réel ou presque réel. Leur technologie se concentre sur les formes d'ondes et les signaux et est auditable, ce qui en fait l'idéal pour une utilisation dans des environnements austères où les gens se salissent et où il est impossible de transporter une pile de serveurs. Alors que les réseaux neuronaux sont excellents pour les modèles de langage et la reconnaissance d'images, la société de Logan se concentre hyper-spécifiquement sur les formes d'ondes et les signaux. Leur approche est différente et hautement efficace.\n\nDétecter différents types de formes d'ondes et les relations entre ces formes d'ondes est au cœur de cette nouvelle technologie. Cette approche permet de détecter des motifs plus difficiles à falsifier par rapport aux techniques traditionnelles utilisées par le Département de la Défense. Cela offre un potentiel d'application de leur technologie dans la guerre électronique, notamment pour la collecte de renseignements et l'analyse. De plus, la plateforme peut détecter, analyser et recueillir des renseignements, pouvant être exportés vers des périphériques externes. Leur fonctionnalité d'auto-apprentissage de détection d'anomalies, Infinite Loop, a également été mentionnée, établissant une référence continue basée sur les paramètres prescrits par l'utilisateur final. Cette technologie peut être utilisée dans l'industrie automobile, la santé et les industries du divertissement, où elle pourrait être intégrée à chaque capteur.\n\nDataShapes a un produit appelé GlobalEdge, un agent intelligent qui se trouve sur ou derrière les capteurs pour effectuer les opérations ETL sur les données collectées. La composante d'apprentissage automatique de GlobalEdge filtre les données pour fournir des informations pertinentes et des anomalies en temps réel, réduisant ainsi la quantité de données non pertinentes renvoyées au siège. Le produit peut également être utilisé pour la compression des données à la périphérie vers le centre de données. Le logiciel peut être réduit à aussi peu que 47 Ko, ce qui le rend adapté à différentes applications, notamment la détection de virus à l'aide d'ondes UV.\n\nPour en savoir plus sur DataShapes et leur approche, rendez-vous sur https://www.datashapes.com\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Logan Selby"],"link":"/episode-EDT134-fr","image":"./episodes/edt-134/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren Pulsipher accueille Logan Selby, le co-fondateur et président de DataShapes, où ils discutent d'une approche unique de l'intelligence artificielle qui va à l'encontre de la tendance."},{"id":32,"type":"Episode","title":"IA fiable et éthique","tags":["ai","ethics","trustworthiness","deepfake","aicontent","aidetection"],"body":"\r\n\r\nÀ la fin des années 2010, Microsoft a lancé Tay, un chatbot d'intelligence artificielle conçu pour apprendre de ses conversations avec les utilisateurs sur Twitter. Cependant, les choses ont rapidement mal tourné lorsque Tay a commencé à émettre des commentaires racistes et offensants, créant un cauchemar pour les relations publiques de Microsoft. Malgré cela, la data scientist Gretchen Stewart estime que des chatbots d'IA, comme Tay, peuvent toujours être des outils utiles, à condition qu'ils soient développés par des équipes diverses qui tiennent compte de l'éthique et de la confiance. Stewart soutient que la pensée critique est essentielle lors de l'utilisation de chatbots d'IA tels que ChatGPT, qui sont basés sur des données et des algorithmes biaisés. Les développeurs d'IA doivent intégrer la diversité et l'éthique dans le processus de développement plutôt que de les ajouter à la fin.\n\n## Fiabilité\n\nL'IA a des capacités immenses mais elle manque encore des sens et expériences humaines qui peuvent influencer la prise de décision. Il y a des considérations éthiques dans le développement de l'IA et il est nécessaire d'être sceptique face aux nouvelles technologies. Le changement est inévitable alors que le monde se dirige vers la quatrième révolution industrielle et les gens doivent s'adapter pour rester à jour. Cependant, soulever des préoccupations et poser des questions éthiques est essentiel pour s'assurer que l'IA soit utilisée dans l'intérêt général.\n\nIl y a des dangers potentiels à compter sur l'intelligence artificielle (IA) comme source d'information. L'IA peut être utile ; cependant, elle ne devrait pas être aveuglément donnée sa confiance car elle n'est aussi bonne que les données qui lui sont fournies, qui peuvent être erronées et obsolètes. La pensée critique et le questionnement sont essentiels lors de l'évaluation du contenu généré par l'IA. Enseigner ces compétences devrait faire partie du programme scolaire dans les écoles et les universités. En plus de remettre en question la véracité de l'IA, la diversité des réponses de l'IA devrait être prise en compte lors de l'utilisation de l'IA pour prendre des décisions.\n\n## Éthique\n\nÀ mesure que la technologie continue de progresser, de nouvelles préoccupations éthiques continuent d'émerger. Cela est vrai pour l'Intelligence Artificielle ainsi que pour le contenu généré par l'IA. Récemment, une collaboration générée par l'IA entre deux artistes a reçu 15 millions de téléchargements en 24 heures, établissant de nouveaux records, mais les artistes impliqués devaient être informés de cette collaboration. Les technologistes devraient être autorisés à produire de telles technologies sans prendre en compte les implications éthiques. Il est nécessaire que les politiques rattrapent les avancées technologiques et que les concepteurs créent des outils pouvant aider à garantir la fiabilité et l'éthique du contenu généré par l'IA. Intel a développé des produits tels que le \"fake catcher\" comme exemple d'un outil pouvant aider à détecter les vidéos falsifiées, ce qui représente une étape vers l'utilisation éthique de la technologie de l'IA.\n\nLa lutte contre les contenus générés par une IA falsifiée est devenue une industrie à part entière, et cela nécessite une transparence dans le développement de l'IA. Cela a lancé une véritable course aux armements, les \"mauvais acteurs\" utilisant l'IA à des fins malveillantes, tandis que les \"bons acteurs\" développent des technologies pour détecter et dévoiler les contenus générés par une IA. Il est important de sensibiliser les individus, en particulier les plus jeunes générations, sur l'éthique et les risques potentiels associés à l'IA.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Gretchen Stewart"],"link":"/episode-EDT135-fr","image":"./episodes/edt-135/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe Gretchen Stewart, scientifique en chef des données du secteur public chez Intel, où ils discutent de la fiabilité et de l'éthique de l'intelligence artificielle."},{"id":33,"type":"Episode","title":"Données résilientes dans les communications perturbatrices","tags":["sabr","ddil","technology","edge","process","devops","security"],"body":"\r\n\r\nDans cet épisode de podcast, Darren Pulsipher, architecte principal des solutions chez Intel pour le secteur public, est interviewé par l'invitée Dr Anna Scott sur les données résilientes et les communications perturbatrices. La discussion porte sur la gestion sécurisée et efficace des données dans des environnements présentant une bande passante limitée et un service perturbé, permettant ainsi l'intelligence artificielle et l'analyse complexe des données en périphérie. Darren parle de son expérience de travail sur le cloud des voitures connectées de Toyota comme base pour résoudre ce problème, et comment des architectures de gestion de données courantes ont été utilisées pour créer une solution. La révélation graduelle des défis du client a permis d'identifier des schémas de conception qui ont ouvert la voie à une solution réussie.\n\nMalgré une connectivité intermittente et des nœuds de bord dynamiques, le processus de conception de l'architecture pour effectuer l'analyse de bord est complexe et difficile à articuler, ils ont donc adopté une approche itérative. D'abord, ils ont envisagé une solution qui regrouperait toutes les données en un seul endroit pour l'analyse, mais ont constaté que cela était impraticable en raison du volume important de données sur le bord. Le déplacement des applications vers le bord semblait prometteur jusqu'à ce que le client demande une analyse agrégée sur l'ensemble du réseau. Après avoir exploré les modèles de conception, ils ont opté pour l'utilisation du modèle de conception du flux de données avec un hub de publication et d'abonnement pour créer des flux de données dynamiques pour les consommateurs et les producteurs. Bien que cette approche utilise des bases établies dans les technologies de l'information, le client avait initialement des préoccupations concernant l'allocation des sujets.\n\nDans le passé, Darren devait créer un écosystème PubSubHub à l'avance et être familiarisé avec tous ses composants. Cependant, la mise en place de flux de données avec Kafka ou Apache Pulse était difficile, et la configuration de la sécurité était encore plus difficile. En utilisant leurs connaissances en DevSecOps pour concevoir une solution qui regroupe les définitions des flux de données, les définitions d'entrée et de sortie, ainsi que les transformations de données dans un package consommable par le pipeline. Ce package crée automatiquement des flux de données dans PubSubHub et met en place toutes les mesures de sécurité nécessaires. Le résultat a simplifié la complexité, car les développeurs n'avaient besoin de se concentrer que sur les algorithmes ou les modèles d'IA, tandis que tout le reste était pris en charge de manière générique. L'approche itérative avec les clients a permis de modifier les architectures et les implémentations en cours de route.\n\nLe défi principal avec ce type d'architecture est de travailler avec des ressources limitées, telles que seulement deux cœurs et 2 gigaoctets de RAM. Quelle quantité de traitement des données peut être effectuée à la lisière avec des ressources limitées ? Une des contraintes sur lequel l'équipe d'architecture a dû travailler était de rendre le gestionnaire de flux SABR et les mesures de sécurité aussi petits que possible. Darren a simplifié le code au strict minimum et a éliminé les packages tiers inutiles. L'objectif était de créer un gestionnaire de flux léger qui puisse fonctionner à la lisière et être portable dans différents environnements. L'architecture Saber résultante était évolutive et adaptable, capable de fonctionner aussi bien sur une montre connectée que sur un grand serveur Xeon.\n\nDarren et Anna discutent des difficultés de la mise à jour des modèles d'IA dans un écosystème distribué avec de nombreuses instances des mêmes analyses en cours d'exécution. Pour résoudre ce problème, SABR a créé un flux de données d'apprentissage qui connecte toutes les instances des mêmes analyses et gère les communications intermittentes, la mise en cache et l'envoi des modifications pour mettre à jour les modèles. Ils ont également développé un système de canal de données utilisant le modèle de conception de stratégie de politique, ce qui permet l'envoi de différents canaux avec des niveaux de données variables basés sur des règles définies par la politique. Cette approche permet une transmission de données plus efficace, réduisant la quantité de données traitées et augmentant la précision des modèles d'IA.\n\nLa prochaine étape consiste à opérer dans l'environnement de DDIL en priorisant l'envoi de données en commençant par envoyer des résumés et des données historiques avant les données en temps réel. Il est important de définir les attentes du système et les politiques de communication dès le départ pour garantir la cohérence dans l'écosystème. Un processus de création et d'activation de politiques facile à utiliser, configuré en JSON et basé sur JavaScript, qui élimine la duplication des efforts et favorise la réutilisation. L'architecture a la capacité d'ajouter rapidement de nouvelles fonctionnalités en exploitant les sabres et les transformations de données existants.\n\nLe dernier avantage est la résilience du système en se rétablissant dynamiquement des pannes du système. Il y a un potentiel d'utilisation dynamique et flexible du réseau SABR. Même si un nœud tombe en panne, il est possible de déplacer un SABR vers un autre boîtier et de continuer à recevoir toutes les flux de données. Il est également possible d'utiliser l'informatique héritée en exécutant un SABR très léger sur un vieux système qui collecte et transmet des données vers le réseau SABR. L'importance de la sécurité ne doit pas être négligée. Toutes les flux de données sont cryptées et des processus sont mis en place pour établir la confiance et l'attestation des SABR afin de prévenir le spoofing et l'espionnage des données. Globalement, le réseau SABR offre une solution prometteuse pour le traitement des données en périphérie avec flexibilité et sécurité.\n\nLa clé de l'apprentissage réside dans l'approche architecturale itérative que l'équipe a utilisée pour découvrir les cas d'utilisation et les problèmes rencontrés par les utilisateurs finaux. En simulant l'architecture, ils ont pu identifier ses lacunes et obtenir des retours clients. De plus, l'utilisation de modèles de conception était essentielle pour accélérer l'approche architecturale. L'emploi de l'abstraction a également permis de remplacer différentes technologies tout au long du processus architectural. Darren est convaincu que cette approche facilite la création de solutions simples à utiliser et profitant des connaissances actuelles des clients.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Anna Scott"],"link":"/episode-EDT136-fr","image":"./episodes/edt-136/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode de podcast, Darren Pulsipher, architecte en chef de solutions d'Intel pour le secteur public, est interviewé par la présentatrice invitée, le Dr Anna Scott, sur les données résilientes avec les communications perturbatrices."},{"id":34,"type":"Episode","title":"Renaissance du Cloud privé","tags":["cloud","computing","greenlake","hpe","multicloud","hybridcloud","technology"],"body":"\r\n\r\nIl y a eu une tendance vers la rapatriement dans le secteur commercial, où les entreprises ramènent les charges de travail et les machines virtuelles des environnements de cloud public pour des raisons financières telles que les frais de sortie et la budgétisation des charges de travail à hyper-échelle. La souveraineté des données, les problèmes de sécurité, les performances et les problèmes de latence ont également conduit de nombreuses entreprises à s'éloigner du cloud public et à revenir aux technologies de cloud privé. Malgré ce changement, de nombreuses promesses du cloud ont été tenues pour garantir la facilité d'utilisation, l'élasticité et la tarification basée sur la consommation. Afin de faciliter la transition entre les clouds, le cloud privé doit offrir une facilité d'utilisation, la portabilité des charges de travail et de nouveaux modèles de tarification basés sur la consommation, très demandés par les clouds publics.\n\n## Contrôle des coûts\n\nSam Ceccola, le CTO de HPE DoD, estime qu'il est essentiel de comprendre les risques et les défis associés à chaque technologie de cloud privé, public et hybride. Et de concevoir des solutions qui tirent parti des points forts de chaque technologie. C'est là que l'offre de production de HPE, GreenLake, entre en jeu. Le modèle d'approvisionnement en services basé sur la consommation de GreenLake a été mis en œuvre pour la première fois en 2005 et a depuis évolué pour inclure un portail en libre-service permettant de gérer plusieurs environnements cloud privés et publics. GreenLake offre aux clients la possibilité de déployer des ressources supplémentaires à la demande, à travers la frontière du cloud privé/public. Il gère également et visualise des coûts complexes, tels que les frais de sortie de données et l'établissement de budgets pour les charges de travail à grande échelle.\n\n## Gestion des données\n\nUne autre préoccupation majeure pour la plupart des consommateurs de services de cloud public est la gestion des données, la gouvernance et la souveraineté. Sans une stratégie de données cohérente, de nombreuses organisations sont lourdement chargées de frais de sortie de données lorsqu'elles déplacent des données vers ou depuis le cloud public. Il existe plusieurs approches différentes pour gérer les données entre les frontières des nuages. HPE adopte une approche de stockage adjacent au cloud qui réduit au minimum la quantité de données déplacées vers le cloud. Le stockage adjacent au cloud de GreenLake permet de maintenir les données sur site tout en exécutant les charges de travail de calcul dans le cloud. Cette approche garantit la souveraineté des données, réduit les coûts de sortie et les problèmes de latence associés au déplacement de grandes quantités de données entre les clouds.\n\n## Portabilité\n\nUn autre aspect clé des architectures de cloud hybride multiplateforme est la capacité de déplacer facilement les charges de travail entre les différents clouds. Certaines organisations ont adopté des architectures de microservices basées sur des conteneurs sans état pour offrir la flexibilité et la portabilité des charges de travail. Cependant, toutes les charges de travail ne peuvent pas être facilement conteneurisées, en particulier celles qui contiennent des applications avec un état. Dans ces cas, des machines virtuelles sont utilisées pour permettre le déplacement des applications d'un cloud à un autre. Le déplacement de machines virtuelles entre différents clouds peut poser problème car les fournisseurs de services cloud utilisent des hyperviseurs et des processeurs différents qui peuvent ne pas être entièrement compatibles. Comprendre les limitations de ces migrations est essentiel lors de la prise de décision d'achat de matériel, qu'il soit virtualisé ou non-virtualisé.\n\n## Sécurité\n\nGreenLake ne limite pas les clients au déploiement sur site, car il leur permet de provisionner des charges de travail dans différents environnements cloud, notamment Amazon, Google et Azure. De plus, GreenLake prend en charge les charges de travail hybrides authentiques, où une charge de travail peut s'exécuter à la fois sur site et dans des environnements cloud à grande échelle simultanément. L'exécution de charges de travail à travers les limites du cloud peut entraîner une exposition supplémentaire aux cyberattaques. Comprendre les modèles de sécurité des différentes technologies cloud publiques et privées peut être une tâche complexe et augmente les ressources humaines nécessaires pour concevoir et gérer efficacement une position de sécurité solide. La plate-forme GreenLake gère la sécurité grâce à son moteur de sécurité agnostique, Security Central, qui prend en charge l'attestation multi-cloud et les architectures de confiance zéro pour les clouds indépendants, en gérant la complexité et l'intégration des modèles de sécurité.\n\nArchitecturer une solution cloud multi-hybride n'est pas une tâche simple et nécessite une compréhension des technologies cloud privées et publiques, de leurs limites et de leurs forces. Heureusement, il existe des organisations telles que HPE qui travaillent avec des architectures cloud multi-hybrides depuis plusieurs années et qui ont appris les tenants et les aboutissants d'une gestion efficace des charges de travail dans cet environnement flexible.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sam Ceccola"],"link":"/episode-EDT137-fr","image":"./episodes/edt-137/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe Sam Ceccola, CTO de la DOD pour HPE, à propos des nouveaux modèles commerciaux et technologiques qui modifient la manière dont les organisations consomment le cloud hybride."},{"id":35,"type":"Episode","title":"Évolution du Cloud","tags":["cloud","computing","dell","multicloud","hybridcloud","technology"],"body":"\r\n\r\nL'évolution de l'informatique en nuage est l'une des avancées technologiques les plus significatives de la dernière décennie. Le nuage passe d'un simple concept à un système complexe et omniprésent qui domine désormais les opérations et les stratégies commerciales. Comme nous le dit Ken White de Dell Technologies, le nuage n'est plus un seul endroit ni une solution unique pour tous. Au contraire, c'est tout, partout.\n\nAvec des hyper-échelles comme Amazon et Azure qui se développent constamment, le cloud représente une expérience cohérente sur toutes les plateformes avec une prévisibilité en termes de performance et de coûts. Cependant, des frais cachés existent, notamment lors du transfert de données dans et hors du cloud. Il est essentiel pour les entreprises envisageant le cloud d'adopter pleinement une mentalité axée sur la cohérence et la prévisibilité, ainsi que sur la flexibilité, la performance et la sécurité. Bien que certains dirigeants soient attirés par le cloud pour son potentiel de réduction des coûts et de gestion informatique, il est essentiel de se rappeler qu'il ne peut pas résoudre tous les problèmes et qu'il n'est pas à sens unique. Le cloud peut finalement améliorer les opérations, augmenter l'efficacité et offrir une meilleure expérience aux entreprises et aux clients, à condition d'adopter une mentalité opérationnelle bien planifiée.\n\nSi vous êtes dans le monde des affaires, vous avez probablement entendu parler du cloud, mais avez-vous entendu parler de l'état d'esprit du cloud ? Il s'agit d'une façon d'aborder la technologie qui permet l'adaptabilité, la portabilité et la prévisibilité concernant les charges de travail et les coûts liés au cloud. Cela signifie que vous pouvez développer des applications portables sur différents écosystèmes et choisir où les charges de travail atterrissent en fonction des besoins commerciaux, et non seulement des contraintes techniques. Cela signifie également que vous pouvez prédire et contrôler les coûts, ce qui est un soulagement bienvenu dans un paysage technologique en constante évolution.\n\nMais changer les mentalités est un défi, en particulier pour les organisations habituées à une manière particulière de fonctionner. C'est là que la formation et l'engagement envers l'apprentissage continu interviennent. Les organisations qui adoptent avec succès la mentalité du cloud accordent la priorité à une expérience utilisateur positive, sont prêtes à s'adapter à de nouvelles architectures, et apprennent et changent en fonction de l'évolution de la technologie.\n\nAdopter la mentalité du cloud ne signifie pas que vous devrez licencier tous vos employés en informatique et déplacer tout vers le cloud. Au contraire, il s'agit de former votre personnel et de repenser la philosophie organisationnelle. Avec une couche de gestion adaptée et une concentration sur la flexibilité et l'agilité, les organisations peuvent tirer parti des avantages du cloud pour leur entreprise et améliorer les expériences de leurs utilisateurs finaux, qu'il s'agisse de développeurs ou de clients.\n\nEn envisageant une stratégie infonuagique, il faut comprendre que vous pourriez rencontrer des échecs. La migration des applications vers le cloud n'est pas toujours simple et peut entraîner des coûts. Cependant, vos chances de réussite sont bien plus élevées avec la bonne cohérence et gouvernance.\n\nLa cohérence ne signifie pas nécessairement une seule interface graphique, mais cela signifie avoir un processus standard pour gérer et déployer les charges de travail et l'infrastructure. Cela inclut également la cohérence avec les règles de gestion, les processus et les flux de travail. Ce niveau de cohérence garantit que tous les membres de votre organisation comprennent comment gérer le cloud.\n\nLes échecs d'adoption du cloud ne sont parfois évitables que partiellement, mais la réinternalisation peut être une option pour ramener votre charge de travail vers une solution sur site. Cependant, cette option peut également être coûteuse, il est donc essentiel de peser les avantages à court terme et à long terme.\n\nLes conteneurs peuvent également faire une différence dans l'adoption du cloud en éliminant les problèmes liés au déplacement des applications vers le cloud. Mais n'oubliez pas, il n'existe pas de solution miracle qui résoudra tous vos problèmes d'adoption du cloud.\n\nDans l'ensemble, la clé du succès d'une stratégie basée sur le cloud est de se concentrer sur la cohérence et de comprendre les coûts liés à la transition vers le cloud. En gardant ces facteurs à l'esprit, vos chances de réussite sont bien plus grandes.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sam Ceccola"],"link":"/episode-EDT138-fr","image":"./episodes/edt-138/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe Ken White de Dell Technology sur la manière dont la technologie Cloud est bien plus qu'une simple technologie, mais plutôt un processus et un changement culturel au sein des organisations."},{"id":36,"type":"Episode","title":"Analyse logistique résiliente","tags":["data","analytics","artificialintelligence","pathway","technology"],"body":"\r\n\r\nÊtes-vous intéressé par l'apprentissage sur la façon dont la transformation numérique affecte la logistique ? Dans cet épisode de \"Embracing Digital Transformation\", l'invité spécial Adrian Kosowski, directeur des produits chez Pathway, discute des analyses résilientes dans la logistique. Pathway se concentre sur l'étude des systèmes du monde réel d'un point de vue informatique distribué et travaille spécifiquement avec des données dans le domaine de la logistique et du transport. L'un des plus grands défis dans ce domaine est d'agréger des données à grande échelle et de leur donner du sens, c'est là que les analyses d'apprentissage automatique entrent en jeu. Kosowski souligne également que la logistique est un marché très concentré, contrôlé par seulement quelques entreprises, ce qui rend même de petites améliorations dans les processus incroyablement précieuses pour l'économie mondiale. Cependant, collecter des données depuis l'extrême bord, comme les conteneurs au milieu de l'océan, présente ses propres défis, tels que l'optimisation de l'énergie et de la communication pour les appareils IoT alimentés par batterie. En résumé, si vous êtes intéressé par l'intersection de la logistique et de la transformation numérique, cet épisode offre des aperçus précieux sur les défis et les opportunités de ce domaine.\n\nL'utilisation des dispositifs Internet des objets (IdO) dans l'industrie de la logistique et de la chaîne d'approvisionnement peut grandement bénéficier aux entreprises grâce à une visibilité de bout en bout et à une capacité d'analyse améliorée. Cependant, il y a plusieurs défis associés à l'utilisation de ces dispositifs qui doivent être résolus.\n\nUn défi majeur réside dans la stabilité et la fiabilité de ces dispositifs périphériques. En cas de défaillance ou de panne d'un appareil, les entreprises doivent être en mesure de les optimiser sans qu'ils ne devinent ou n'agissent de manière incorrecte. Cela est particulièrement critique pour les systèmes en temps réel qui sont utilisés pour traiter les événements dans l'ordre.\n\nUn autre défi est la précision des données collectées par les dispositifs IoT. Certaines données peuvent être déduites et une analyse contextuelle peut être nécessaire pour interpréter la signification d'un point de données donné et distinguer les problèmes de mesure des problèmes de processus. Des données inexactes ou non opportunes peuvent entraîner des risques dans la chaîne d'approvisionnement et rendre difficile l'optimisation du réseau de transport.\n\nCependant, l'utilisation généralisée des appareils IoT peut permettre une visibilité et une observabilité de bout en bout des processus tout au long de la chaîne d'approvisionnement. Cela peut aider les entreprises à optimiser leurs processus et à adopter une approche proactive. Cela peut bénéficier à tous les acteurs de la chaîne d'approvisionnement - des prestataires de transport et de logistique aux détaillants et fabricants.\n\nAlors qu'il n'existe aucun groupe spécifique ou segment vers lequel les entreprises devraient cibler l'adoption des dispositifs IoT, une adoption généralisée et une coopération entre tous les acteurs peuvent entraîner des avantages considérables pour la chaîne d'approvisionnement mondiale.\n\nPathway a été développé pour répondre aux lacunes des technologies existantes de diffusion de données et fournir un outil pour les pipelines d'analyse avancée sur les flux de données. L'un des principaux avantages de Pathway est la facilité de description de la logique, comme s'il s'agissait d'un système de lots, tout en veillant à ce qu'elle fonctionne dans un système en temps réel avec des données désordonnées.\n\nPathway a été développé en tenant compte des données de l'IoT, mais il peut également gérer des données provenant de la surveillance vidéo, de la surveillance des performances des serveurs, de la surveillance des journaux et d'autres entités physiques. Cela permet de gérer les complexités des données en matière de détection d'anomalies, d'alertes avec des données non ordonnées, et de données avec des séries temporelles et des éléments géospatiaux de manière indépendante du cloud.\n\nUne autre caractéristique importante de Pathway est la capacité d'utiliser des scripts Python pour l'analyse en continu des données, à la fois en temps réel et en mode par lots. Cela signifie que les scientifiques des données et les ingénieurs peuvent développer leurs pipelines d'analyse dans l'environnement de développement habituel auquel ils sont habitués et travailler facilement avec le système de streaming. De plus, Pathway permet de prendre en compte une quantité beaucoup plus importante d'historique lors de l'exécution des calculs, ce qui représente une grande différence par rapport aux processeurs de flux légers qui ne gèrent que de petites quantités de données.\n\nDans l'ensemble, Pathway offre une solution de transition pour les organisations qui ont besoin de combiner le traitement de données par lots et en temps réel, et offre un moyen d'ajouter de la valeur aux données en ajoutant une structure et des informations d'étendue pour une utilisation ultérieure dans l'intelligence d'affaires et l'analyse.\n\nLa technologie de streaming de données devient de plus en plus importante alors que les entreprises cherchent à prendre des décisions plus rapidement et à répondre plus rapidement aux besoins des clients. Lorsque vous utilisez le streaming de données, vous pouvez rapidement détecter et répondre aux tendances, anomalies et autres informations importantes. Pathway propose une suite de produits spécialement conçus pour travailler avec des données de séries chronologiques, des données logistiques et des données en streaming. Ils proposent des exemples et des informations pour les développeurs sur GitHub, ainsi qu'une communauté dynamique sur Discord.\n\nLe streaming de données est une technologie essentielle pour les entreprises dans de nombreux secteurs différents. Par exemple, il peut être utilisé dans la logistique pour optimiser les itinéraires et prédire l'arrivée des expéditions. Dans la finance, il peut aider à détecter la fraude et prévoir les tendances du marché avant qu'elles ne soient largement connues. Dans les transports, il peut être utilisé pour surveiller en temps réel les performances des moteurs et des véhicules.\n\nSi vous êtes intéressé par l'apprentissage de plus sur le flux de données et la façon dont il peut bénéficier à votre entreprise, assurez-vous de consulter le site web et la communauté de Pathway [www.pathway.com](www.pathway.com). Avec les bons outils et l'expertise, vous pouvez exploiter cette technologie puissante pour améliorer vos opérations et rester en avance sur la courbe.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Adrian Kosowski"],"link":"/episode-EDT139-fr","image":"./episodes/edt-139/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe Adrian Kosowski, CPO de Pathway, à propos de leur capacité unique à gérer des données logistiques depuis le terrain dans des environnements DDIL avec des analyses en temps réel."},{"id":37,"type":"Episode","title":"Lancement de votre transformation organisationnelle pour devenir axé sur les données.","tags":null,"body":"\r\n\r\n## Opérationnaliser le système d'analyse.\n\nLa première chose à comprendre sur le processus d'opérationnalisation de l'analyse est qu'il est complexe. Certaines personnes pensent que les données sont le nouvel électricité et fonctionnent selon un système similaire. Considérez cet exemple : dans un système électrique simple, il y a une source d'alimentation et des fils qui vont vers une ampoule. Tant qu'il y a la source d'alimentation et que les fils sont de haute intégrité, vous pouvez éteindre la lumière, puis revenir six mois plus tard, basculer l'interrupteur et savoir que l'ampoule s'allumera à nouveau.\n\nUn système analytique ne fonctionne pas de cette manière. Il existe de nombreux types de \"court-circuits\" et de variables qui peuvent causer des échecs, tels que l'entropie. Il est important d'avoir les bonnes personnes dans vos équipes qui peuvent travailler et comprendre tous les différents types de défauts qui peuvent survenir, sinon vous n'obtiendrez pas de perspectives correctes ou utiles.\n\nVos données doivent être formatées d'une certaine manière pour les algorithmes d'apprentissage automatique. L'algorithme doit comprendre ce que représentent les données afin de pouvoir examiner le motif de manière appropriée. À moins que les données ne soient soigneusement formatées, il y a un risque de mauvaise sortie à cause d'entrées erronées. C'est là que vos ingénieurs, scientifiques des données et autres architectes de données de soutien sont importants.\n\n## Poser les bonnes questions\n\nUne autre partie du système consiste à s'assurer que vous avez les bonnes questions commerciales. Une organisation doit fournir des questions fondamentales sur ce qu'elle essaie d'obtenir des données : De quels insights notre entreprise a-t-elle besoin ? Comment pouvons-nous dresser un tableau plus précis de ce à quoi ressemble réellement notre paysage ? Quelles données nous permettront de le faire ?\n\nDe nombreux cadres sont enthousiasmés par l'idée de faire de l'analyse, de l'IA et du big data, mais ne savent pas par où commencer. La première étape consiste pour les cadres à prendre en charge ce qu'ils veulent que les données fassent pour l'organisation. Quelle valeur commerciale va découler des données ? Quel type de données avons-nous déjà ? Quelles données pouvons-nous créer qui donneront une image plus complète de ce qui se passe ? À quoi ressemble le succès ? L'accent doit d'abord être mis sur les fondations organisationnelles.\n\n## Collecte de données, préparation et analyses\n\nAprès que ces questions sont répondues, l'étape suivante consiste à définir ce qui doit être accompli et commencer à le découper en parties plus petites. Les organisations devraient examiner leurs sources de données actuelles, déterminer leur fiabilité et identifier ce qui manque, suivis de la collecte et de la préparation des données.\n\nAvec les données en main, il est temps de créer le récit. Cela implique non seulement les scientifiques des données, mais aussi les experts du domaine, ainsi que les personnes spécialisées en affaires et en marketing. La contribution de ces différents groupes garantira que les données apportent une réelle valeur à l'entreprise et ne se limitent pas à un simple exercice académique ou à une expérience ponctuelle. À partir de ces données, vous souhaitez obtenir, en fin de compte, des perspectives précieuses et fiables qui vous aideront à améliorer votre entreprise, plutôt que simplement collecter des données pratiques mais non exploitables.\n\nMaintenant que vous avez collecté les données, les avez analysées et avez quelques informations, comment les mettez-vous en œuvre ?\n\n## Exemple de réussite en matière d'analyse opérationnelle.\n\nUn exemple dans la fabrication est un bon moyen de montrer comment les données fonctionnent avec succès à travers le pipeline. Commencez par la valeur commerciale fondamentale de maximiser le rendement de votre produit. Quelles sources de données pourraient vous indiquer si vous avez des produits cassés ou défigurés ? Des caméras à la fin des lignes d'assemblage pourraient créer un modèle d'apprentissage approfondi qui détecte ou prédit si un produit est cassé ou défiguré. Un scientifique des données pourrait ensuite entraîner un modèle basé sur un bon produit et un mauvais produit pour améliorer le rendement.\n\nCe type de données opérationnalisées pourrait être appliqué à plusieurs domaines clés qui ajoutent de la valeur à une organisation. Le contrôle qualité, par exemple, en s'assurant que la matière première répond aux spécifications, en est un exemple. La maintenance prédictive des machines et la conformité réglementaire, comme dans une exigence de poids pour un produit alimentaire, en sont d'autres exemples.\n\nEn exploitant en temps réel les données sur les problèmes liés aux produits, à la qualité ou aux réglementations, vous ne faites pas seulement augmenter le rendement du produit, mais vous réduisez également les tâches coûteuses réalisées par des êtres humains, telles que l'inspection en fin de chaîne de montage. Ces employés peuvent être réorientés vers des tâches à plus forte valeur ajoutée. Vous réduisez également les déchets et créez une expérience client plus cohérente. De plus, vous protégez l'entreprise en minimisant les risques réglementaires.\n\nCeci est une situation gagnant-gagnant. Plus l'organisation sait ce qu'elle souhaite tirer des données, quelles données collecter et comment les utiliser en fonction d'une stratégie commerciale, plus elle sera réussie.\n\n## Plus à venir\n\nVoici le premier des six épisodes qui parlent de l'organisation axée sur les données. Nous espérons que vous nous rejoindrez pour ces futurs podcasts. Vous en voulez plus? Consultez le site web de Sarah.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT14-fr","image":"./episodes/edt-14/fr/thumbnail.png","lang":"fr","summary":"L'épisode d'aujourd'hui porte sur la mise en marche de votre organisation pour devenir centrée sur les données et la valeur que cela peut apporter. L'invitée spéciale de Darren est Sarah Kalicin, scientifique des données en chef pour les centres de données chez Intel."},{"id":38,"type":"Episode","title":"Vérification des antécédents de votre source ouverte.","tags":null,"body":"\r\n\r\nSi vous êtes un développeur de logiciels, vous connaissez le sentiment de fierté qui accompagne la création d'un package ou d'un outil populaire que de nombreuses personnes trouvent utile. Cependant, cette popularité peut parfois attirer l'attention d'attaquants qui recherchent des vulnérabilités à exploiter.\n\nDans un podcast récent, l'ingénieur logiciel Jay Phelps a partagé son expérience de découverte d'une vulnérabilité dans un package largement utilisé qu'il a créé. Après avoir pris conscience de l'impact potentiel de la vulnérabilité et du nombre important d'instances du package dans la nature, Phelps a rapidement travaillé pour résoudre le problème afin d'empêcher les attaquants de l'exploiter.\n\nCe scénario met en évidence l'importance de la vigilance pour les développeurs de logiciels, en particulier ceux qui créent des packages ou des outils populaires. Bien qu'il puisse être tentant de se prélasser dans la gloire d'un produit largement utilisé, il est crucial de se rappeler que la popularité peut également attirer les attaquants. Des vérifications régulières et des mises à jour pour remédier à toute vulnérabilité peuvent aider à protéger les utilisateurs et prévenir l'exploitation.\n\nEn tant que développeur de logiciels, il est important d'aborder votre travail avec à la fois fierté et prudence. Bien qu'il soit formidable de contribuer à la société avec vos créations, n'oubliez pas de donner la priorité à la sécurité et à la sûreté de vos utilisateurs. Restez vigilant et gardez vos packages à jour pour empêcher les vulnérabilités d'être exploitées par des attaquants.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Michael Mehlberg"],"link":"/episode-EDT140-fr","image":"./episodes/edt-140/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interroge Michael Mehlberg sur l'augmentation de la confiance dans les logiciels open source grâce à la vérification des antécédents des communautés open source."},{"id":39,"type":"Episode","title":"De la neurologie à l'informatique neuromorphique","tags":["neurology","neuromorphic","security","ai","cybersecurity"],"body":"\r\n\r\nEn tant que neurologue pédiatrique, le Dr Follett souligne l'importance de comprendre les processus de développement du cerveau et sa remarquable capacité à surmonter même des blessures catastrophiques. Elle partage comment ses recherches approfondies ont impliqué l'étude de modèles de rats et de cultures cellulaires pour mieux comprendre le développement des enfants et trouver des moyens de les aider à obtenir les meilleurs résultats, malgré les blessures ou les maladies. Écouter les idées et les expériences du Dr Follett permet de mieux comprendre comment la neurologie est essentielle pour comprendre les processus du cerveau et comment nous pouvons mieux apprécier et soutenir son développement, en particulier chez les enfants. Le Dr Follett partage également son apparition inattendue lors d'un salon high-tech, où elle a utilisé son expertise en neurologie pour aider un homme victime de crises pendant une allocution principale. Ce podcast met en évidence la portée potentielle de la neurologie au-delà des seules institutions médicales et le rôle essentiel des neurologues dans la compréhension de la complexité du cerveau humain.\n\nAvez-vous déjà eu besoin d'aide pour comprendre une technologie ou un produit complexe, alors que vous aviez seulement besoin de savoir comment l'utiliser ? Le Dr a constaté que ses compétences en expliquant aux patients des problèmes médicaux se traduisent bien par l'explication de technologies complexes aux non-experts. Son travail en tant que neuroscientifique étudiant le développement du cerveau l'a amenée, ainsi que son mari, à créer une start-up de haute technologie qui fait de l'informatique neuromorphique, appelée Lewis-Rhodes Labs. Un de leurs produits, Extreme Search, utilise un processeur neuromorphique pour fouiller des quantités massives de données, imitant la manière dont un cerveau reconnaît et traite rapidement l'information.\n\nLe Dr Follet souligne qu'il existe de meilleures approches que celle de reproduire le cerveau lors de la création de technologies. Bien que les cerveaux commettent des milliers d'erreurs chaque jour, nous ne souhaitons pas nécessairement que nos ordinateurs en fassent autant. Au lieu de cela, nous pouvons tirer des leçons attentives du fonctionnement des cerveaux et les appliquer pour créer une technologie plus efficace et plus performante. Cette expérience met en évidence l'importance des compétences interdisciplinaires et de la réflexion en dehors de son domaine spécifique. En identifiant des compétences transférables à différents domaines et industries, nous pouvons apporter des perspectives et des solutions uniques à des problèmes complexes.\n\nLa technologie de recherche extrême est une avancée dans le domaine de la cyber-informatique légale et de l'analyse en temps réel qui résout le défi de trier des pétaoctets de données non structurées en un temps record en utilisant des équipements matériels et logiciels de pointe. La technologie de recherche extrême offre des performances élevées, une consommation d'énergie réduite, et un débit constant, et elle est conçue selon le modèle du cerveau humain pour un traitement de données à haute performance.\n\nContrairement aux méthodes traditionnelles de recherche de données qui nécessitent le déplacement des données, la technologie de recherche extrême permet des recherches sur site qui éliminent tous les goulets d'étranglement du réseau. Cette technologie convient à la cybercriminalistique, à la cybersécurité, à la découverte juridique et à la recherche de données d'entreprise. La technologie de recherche extrême est simple pour les utilisateurs et ne nécessite aucun nouveau langage ou pré-identification de motifs, mais utilise plutôt des expressions régulières pour effectuer des recherches ad hoc sur toutes les données décrites dans le texte.\n\nLa technologie de recherche extrême effectue des recherches sensibles sur des appareils de stockage et fournit une analyse en temps réel identifiant les menaces potentielles en millisecondes. Combinée à des méthodes de détection traditionnelles, la technologie peut détecter rapidement des menaces persistantes avancées, des virus, des logiciels publicitaires, des chevaux de Troie, des vers, des rootkits et autres malwares. L'utilisation de la technologie de recherche extrême ne se limite pas à la cybernétique forensique. D'autres domaines de recherche, tels que la recherche génomique ou tout domaine de données non structurées, peuvent bénéficier de la capacité de la technologie de recherche extrême à rechercher d'énormes quantités de données diverses en un temps record.\n\nDe nombreuses organisations ont besoin d'aide pour trouver des motifs ou des informations dans leurs données. Ou elles ont de vastes bases de données et ont du mal à trouver quoi que ce soit en raison de la quantité d'informations impressionnante. De nombreux scientifiques des données utilisent des indices pour réduire le temps de recherche d'informations. Cela fonctionne bien lorsque vous savez ce que vous voulez lors de la collecte ou du stockage des données. Cependant, de nombreuses organisations traitent des données opaques qui ne correspondent pas à une structure prédéterminée. Dans ce cas, des recherches en force brute sur des pétaoctets de données peuvent prendre des semaines pour trouver des motifs communs de données non déterminées précédemment.\n\nLa technologie de recherche extrême aide à rendre visibles de nouveaux domaines de données, permettant une analyse et une analyse améliorées qui commencent par la recherche et peuvent aller plus vite si les données sont transformées en morceaux requis. Cela est particulièrement utile lorsqu'il s'agit de données de santé, où il existe d'énormes quantités de données structurées, mais où les informations ne correspondent à aucune structure de base de données.\n\nPour plus d'informations sur la technologie de recherche extrême et les approches de transformation numérique, visitez Lewis-Rhodes.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Pamela Follet"],"link":"/episode-EDT141-fr","image":"./episodes/edt-141/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode de podcast intitulé Accueillir la transformation numérique, le Dr Pamela Follett, neurologue et co-fondatrice de Lewis Rhodes Labs, partage son parcours et son expertise dans le domaine de la neurologie, en particulier en ce qui concerne la recherche sur le cerveau en développement chez les jeunes enfants."},{"id":40,"type":"Episode","title":"Protection des données avec calcul confidentiel","tags":["cybersecurity","technology","sgx","confidentialcomputing","dataprotection"],"body":"\r\n\r\nDans le but de garantir que seules les personnes autorisées puissent accéder à certaines données ou systèmes, des mesures de contrôle sont mises en place. Cependant, même avec ces contrôles en place, les personnes disposant de privilèges élevés, telles que les administrateurs cloud ou les administrateurs système, ont encore accès à des données sensibles et des systèmes. C'est là que l'informatique confidentielle intervient, car elle ajoute une couche supplémentaire de protection contre les employés malveillants ou ceux qui pourraient causer accidentellement une violation.\n\nL'accès contrôlé fait référence à la limitation et au suivi de l'accès aux données sensibles ou aux systèmes en fonction des protocoles d'autorisation et d'authentification. L'accès privilégié fait référence à une situation où une personne dispose de privilèges élevés ou de droits d'administration qui lui permettent d'accéder à des données sensibles ou à des systèmes au-delà de ce qui est normalement autorisé ou contrôlé.\n\nLa confidentialité en informatique aide à éliminer cet accès privilégié en créant un environnement d'exécution basé sur le matériel ou environnement d'exécution de confiance qui empêche l'accès ou la modification non autorisée des applications et des données. En utilisant la confidentialité en informatique, les organisations peuvent maintenir le contrôle sur leurs données sensibles tout en profitant de l'informatique en nuage, de l'informatique périphérique et des calculs multipartites sans compromettre la sécurité. C'est une capacité importante qui garantit la protection des données sensibles pour les organisations à l'ère numérique.\n\nL'informatique confidentielle est une nouvelle technologie qui permet aux utilisateurs de garder le contrôle sur leurs données même lorsqu'elles sont stockées sur des serveurs tiers tels que ceux utilisés dans le cloud computing. Grâce à l'informatique confidentielle, les utilisateurs peuvent chiffrer leurs données en mémoire, les protégeant ainsi même des utilisateurs privilégiés et des administrateurs malveillants. Cela signifie que même si un attaquant parvient à accéder au serveur, il constatera que les données sont dans un état chiffré, donc à l'abri des regards indiscrets. L'informatique confidentielle est particulièrement importante pour les données sensibles telles que les dossiers médicaux ou les informations financières.\n\nL'informatique confidentielle repose sur le chiffrement au niveau matériel, offrant une protection bien plus solide que le chiffrement basé sur les logiciels. Étant donné que le chiffrement matériel est implémenté au niveau du processeur, il ne nécessite aucun logiciel ou pilote supplémentaire, ce qui entraîne une charge de performance minimale sur le système. L'informatique confidentielle est également très facile à utiliser puisqu'elle fonctionne de manière transparente avec les logiciels et applications existants.\n\nLes avantages de l'informatique confidentielle sont nombreux. Étant donné que les données sont cryptées pendant leur traitement, les informations sensibles ne sont pas visibles par des tiers, ce qui les maintient privées et sécurisées. L'informatique confidentielle peut être utilisée non seulement dans le cloud, mais aussi dans des environnements de calcul périphérique. Alors que nous continuons à observer une augmentation de la quantité de données générées et stockées, la nécessité d'environnements informatiques sécurisés et fiables devient encore plus importante. L'informatique confidentielle est l'une des technologies qui peuvent aider à atteindre ces objectifs.\n\nLa confidentialité informatique est un composant essentiel d'une architecture de confiance nulle. Un cadre de confiance nulle fonctionne en supposant qu'une cyberattaque peut se produire à n'importe quel moment, il n'y a donc pas de ressource de confiance. Chaque utilisateur et appareil doit être authentifié de manière répétée avant chaque interaction, indépendamment du fait qu'ils aient déjà été vérifiés. La confidentialité informatique fournit une couche de sécurité supplémentaire, car elle vise à protéger les données contre les cyberattaques et les violations de sécurité en veillant à ce que seules les entités nécessaires y aient accès. Cela est réalisé en contournant le système d'exploitation et la pile cloud et en communiquant directement avec la puce, qui gère l'accès à la mémoire.\n\nIntel a été à la pointe de l'informatique confidentielle avec le développement de SGX et de tDCS. Ces technologies relèvent du vaste ensemble des technologies de renforcement de la confidentialité qui visent à fournir des solutions dans ce domaine. Le chiffrement entièrement homomorphique est une autre solution qui aborde le problème purement du point de vue de la cryptographie en maintenant les données toujours chiffrées.\n\nCe qui rend l'informatique de pointe et les environnements d'exécution sécurisés uniques, c'est qu'ils sont largement disponibles aujourd'hui dans les charges de travail de production courantes, avec très peu de surcharge de performance. Pouvoir prendre une charge de travail native, même dans un format inchangé, et l'exécuter au sein d'un environnement crypté et isolé est un outil puissant pour les organisations afin de protéger leurs données.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Anna Scott","Ibett Acarapi","Jesse Schrater"],"link":"/episode-EDT142-fr","image":"./episodes/edt-142/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, le Dr Anna Scott interroge Jesse Schrater et Ibett Acarapi sur la manière de protéger les données à l'aide de la confidentialité informatique."},{"id":41,"type":"Episode","title":"Cas d'utilisation en informatique confidentielle","tags":["cybersecurity","sgx","confidentialcomputing","ai"],"body":"\r\n\r\nLe calcul confidentiel est une technologie révolutionnaire qui permet de traiter les données sans les exposer. C'est avantageux dans le domaine de l'intelligence artificielle, où des données sensibles sont impliquées dans le développement et le déploiement de modèle. Cela protège les modèles d'IA et les données confidentielles, permettant ainsi le développement de meilleurs modèles d'entraînement et de différents ensembles de données. La collaboration entre experts et entreprises est également abordable grâce au calcul confidentiel, qui permet la création de modèles plus grands et protège les données exclusives.\n\nLa technologie est disponible aujourd'hui et peut être utilisée par les entreprises grâce à divers fournisseurs, notamment Azure, GCP, IBM et Alibaba. Les OEM proposant des serveurs pour une utilisation sur site disposent également de boîtiers SGX qui peuvent être provisionnés avec SGX. L'expertise en développement de l'informatique confidentielle n'est pas nécessaire pour profiter des capacités de l'informatique confidentielle, car de nombreuses solutions permettent de migrer les charges de travail natives grâce au modèle de migration ou aux solutions axées sur le cloud.\n\nPour commencer avec le calcul confidentiel, les entreprises peuvent inclure cette exigence dans leur appel d'offres et contacter un fournisseur qui l'offre afin de l'incorporer dans leurs solutions. De nombreux fournisseurs proposent déjà des solutions de calcul confidentiel, et Intel est heureux de participer. Le calcul confidentiel offre une nouvelle façon de gérer les données, permet aux entreprises de tirer parti des avantages du cloud ou du calcul multipartite sur des données sensibles, et offre finalement un environnement plus sûr aux entreprises pour traiter des informations confidentielles.\n\nL'informatique confidentielle est une technologie pionnière en pleine expansion de popularité, étant donné l'augmentation du nombre de violations de données perpétrées dans le monde toujours interconnecté d'aujourd'hui. L'informatique confidentielle vous permet d'isoler des données sensibles dans des enclaves de calcul sécurisées ou des \"conteneurs\", qui sont difficiles d'accès même en cas d'intrusion d'un attaquant dans le système hôte. L'informatique confidentielle consiste à ajouter une couche de sécurité supplémentaire aux environnements informatiques qui nécessitent une protection robuste des informations sensibles ou de la propriété intellectuelle (PI).\n\nComme le podcast l'explique, il existe de nombreuses façons de mettre en œuvre l'informatique confidentielle, en exploitant les ensembles d'outils fournis par les fournisseurs de services cloud (CSP) ou en utilisant les services de fournisseurs de sécurité logicielle. Il est également évident que de plus en plus d'entreprises commencent à se tourner vers l'informatique confidentielle pour protéger leur propriété intellectuelle et les données sensibles de leurs clients.\n\nCependant, la technologie en est encore à ses balbutiements et son plein potentiel reste à exploiter. Les experts s'accordent à dire que l'un des domaines les plus passionnants à observer est l'émergence de l'informatique confidentielle \"intelligente\". Cela fait référence à une approche qui exploite l'intelligence artificielle et l'apprentissage automatique pour améliorer l'efficacité et la flexibilité de l'informatique confidentielle.\n\nUne autre zone à surveiller inclut le développement d'outils avancés de détection et d'analyse des menaces qui exploitent la confidentialité des calculs en temps réel pour améliorer l'analyse des menaces potentielles et accélérer la réponse aux attaques. Quel que soit l'avenir de la confidentialité des calculs, la technologie évolue à une vitesse fulgurante et son adoption continue représente un développement positif dans la lutte contre la cybercriminalité.\n\nL'informatique confidentielle est un domaine en pleine croissance qui promet de protéger les données sensibles. L'industrie a évolué et grandi à tel point qu'elle mérite maintenant un sommet entièrement consacré à cela. Le Sommet Combat Edge Computing à San Francisco est l'événement inaugural qui mettra en avant l'informatique confidentielle. Il se concentrera sur l'avenir de l'informatique confidentielle et son impact sur le web trois, la blockchain et les registres distribués. Les conteneurs confidentiels sont un domaine important du développement natif du cloud, et de nombreux produits sont en cours de création. Cette technologie est désormais disponible et prête à être utilisée dès aujourd'hui, et les fournisseurs peuvent prendre en charge vos charges de travail réelles. L'informatique confidentielle est passionnante car elle constitue une solution naturelle à la sécurité et à la confidentialité des données. La technologie de l'informatique confidentielle confie la protection des données aux utilisateurs, en fournissant un environnement sécurisé qui permet d'analyser les données sans les exposer. À mesure que la confidentialité devient de plus en plus importante, l'informatique confidentielle deviendra omniprésente et toutes les organisations devraient l'utiliser pour protéger leurs données sensibles. Donc, si vous êtes intéressé par la mise en œuvre de l'informatique confidentielle dans votre environnement, n'hésitez pas à vous adresser à nous et à demander de l'aide. L'informatique confidentielle est faisable et des entreprises comme Intel sont prêtes à aider les clients dans ce parcours.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Anna Scott","Ibett Acarapi","Jesse Schrater"],"link":"/episode-EDT143-fr","image":"./episodes/edt-143/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode d'Embracing Digital Transformation, le Dr. Anna Scott poursuit sa conversation avec Ibett Acarapi et Jesse Schrater sur la Confidential Computing et ses utilisations dans l'IA et le développement de logiciels."},{"id":42,"type":"Episode","title":"La science derrière les jumeaux numériques","tags":["edge","artificialintelligence","digitaltwin","iot"],"body":"\r\n\r\nVous êtes-vous déjà demandé comment les robots et les machines naviguent dans le monde physique qui les entoure ? Tout cela repose sur la représentation précise du monde naturel sous forme numérique, appelée \"jumeau numérique\". Un jumeau numérique dispose d'un système de coordonnées standard qui permet à différentes applications de comprendre un espace ou un environnement réel. C'est comme une version virtuelle du monde naturel, permettant une réutilisation des données plus efficace et plus efficace entre les systèmes.\n\nLes jumeaux numériques ne sont peut-être pas un terme que nous utilisons couramment, mais le concept devient de plus en plus populaire, surtout dans les industries manufacturières, de la vente au détail et de la sécurité. Un jumeau numérique est une réplique virtuelle d'un objet physique, d'un processus ou d'un système qui peut être utilisée pour le surveiller et le contrôler en temps réel. Un exemple est Google Earth, un jumeau numérique hautement latent du monde. Cependant, avec l'avancée de la technologie, il y a un potentiel de réduction de la latence et de création de jumeaux numériques quasi temps réel pour un contrôle et une surveillance plus efficaces.\n\nLes applications des jumeaux numériques sont vastes et variées. Par exemple, les usines peuvent utiliser les jumeaux numériques pour améliorer la sécurité et optimiser les lignes de production en surveillant l'emplacement des produits et des personnes à des fins de sécurité. Les jumeaux numériques peuvent également être utilisés dans la réalité augmentée et virtuelle, permettant aux utilisateurs de se promener dans des espaces qui peuvent être inaccessibles ou dangereux dans le monde physique. Même des outils quotidiens comme Google Maps utilisent une forme de jumeau numérique pour fournir des mises à jour en temps réel sur le trafic et des informations sur les accidents.\n\nLes jumeaux numériques deviennent de plus en plus importants dans le développement de l'IA basée sur les machines. Tout comme les humains ont besoin d'une conscience spatiale pour donner un sens au monde qui nous entoure, les machines ont besoin de jumeaux numériques pour naviguer et interagir avec le monde physique. Les possibilités sont infinies pour cette technologie, et il est excitant de penser à comment elle pourrait façonner notre avenir.\n\nLa mise en œuvre des jumeaux numériques nécessite l'intégration de plusieurs capteurs et la calibration de leurs données dans une représentation commune ou un jumeau numérique. Ce processus peut être complexe et nécessite l'utilisation d'unités standard pour garantir la cohérence entre différentes industries.\n\nIntéressant, l'industrie du jeu a inspiré le développement de jumeaux numériques grâce à son expérience dans la création de mondes virtuels dotés de moteurs physiques complexes. En adoptant les normes existantes utilisées dans l'industrie du jeu, telles que le format de description de scène universelle, il est possible de développer une représentation universelle des espaces physiques.\n\nLes jumeaux numériques ont également le potentiel de permettre un contrôle en boucle fermée dans diverses applications, en introduisant des règles de courant presque en temps réel dans les systèmes. Peut-être qu'à l'avenir, nous aurons des scènes intelligentes similaires à celles du film Iron Man, où l'on pourra parler à l'assistant intelligent de notre maison et contrôler les appareils à travers lui. Les possibilités d'utilisation des jumeaux numériques semblent infinies et nous verrons probablement de plus en plus d'effets dans notre vie quotidienne.\n\nRésoudre efficacement des problèmes complexes dans diverses industries. Scene Scape d'Intel est un produit qui découle des efforts de l'entreprise pour développer une vision qui permettrait de transformer les unités basées sur les pixels en unités du monde réel et les caméras. Le produit est destiné à aider à transformer les données des capteurs en modèles virtuels du monde naturel, connus sous le nom de \"jumeaux numériques\", qui peuvent être utilisés pour générer de meilleurs résultats et des efficacités opérationnelles. La technologie repose sur le suivi multi-modal et la modélisation du mouvement. Elle peut surveiller et suivre les personnes, les véhicules et l'équipement dans divers cas d'utilisation, y compris les transports, les soins de santé, la vente au détail et les usines.\n\nL'un des aspects passionnants de Scene Scape est sa capacité à estimer où quelqu'un ira et la prochaine caméra dans laquelle il devrait apparaître. Cela est utile lorsque l'on essaie de couvrir de grands espaces avec des caméras ou des capteurs. Cependant, il y a toujours une marge d'erreur sur la mesure, ce qui signifie que différents capteurs peuvent avoir besoin de s'accorder sur l'emplacement d'un élément d'intérêt. Pour remédier à cela, Scene Scape utilise un modèle de mouvement pour extrapoler les déplacements, permettant un suivi précis et une surveillance des sujets.\n\nDans l'ensemble, les jumeaux numériques sont une technologie encore à ses balbutiements, mais leur potentiel d'utilisation est énorme. Alors que la technologie continue de s'améliorer, il est probable que nous verrons davantage d'applications pour les jumeaux numériques et que de plus en plus d'industries tireront parti de leur utilisation pour obtenir de meilleurs résultats.\n\nÊtes-vous curieux de savoir comment la transformation numérique peut vous être bénéfique, ainsi qu'à votre famille ? Dans cet épisode de \"Embracing Digital Transformation\", Rob aborde les différents cas d'utilisation des outils numériques. Un exemple passionnant qu'il évoque est l'utilisation de la technologie pour suivre vos enfants. Bien que cela puisse sembler controversé, Rob soutient que c'est une utilisation responsable des outils numériques. La surveillance de la localisation et des activités de votre enfant peut donner aux parents la tranquillité d'esprit et les aider à veiller à sa sécurité.\n\nCependant, ceci n'est qu'un exemple parmi d'innombrables cas d'utilisation de la transformation numérique. Rob encourage les auditeurs à réfléchir à la manière dont la technologie peut améliorer les résultats pour eux-mêmes, leurs entreprises et leurs communautés. Des processus rationalisés et une efficacité accrue à une meilleure communication et l'offre de meilleures expériences client, les outils numériques peuvent offrir de nombreux avantages.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rob Watts"],"link":"/episode-EDT144-fr","image":"./episodes/edt-144/fr/thumbnail.jpeg","lang":"fr","summary":"Dans cet épisode, Darren explore la science et les cas d'utilisation de la technologie des jumeaux numériques avec l'architecte principal du ScheneScape d'Intel."},{"id":43,"type":"Episode","title":"Attirer les gens de retour au bureau.","tags":["people","remoteworker","collaboration"],"body":"\r\n\r\nGPA (Global Presence Alliance) a été fondée il y a 15 ans pour répondre au besoin d'un meilleur modèle dans l'espace de la collaboration. À l'époque, la visioconférence devenait de plus en plus courante et les organisations envisageaient une stratégie mondiale. Cependant, elles avaient besoin de plus d'options - en comptant sur des intégrateurs régionaux ou en traitant avec une configuration complexe qui devait comprendre vraiment la collaboration.\n\n## Les personnes, l'espace et la technologie.\n\nGPA visait à résoudre ce problème en proposant une approche globale de collaboration et de stratégie vidéo. Ils ont reconnu la nécessité d'équilibrer les personnes, l'espace et la technologie pour créer des expériences collaboratives exceptionnelles. En comblant le fossé entre les différentes régions et en comprenant les besoins uniques de chaque organisation, GPA a offert une meilleure alternative aux solutions existantes.\n\nAlors que la technologie a évolué au fil des années, il reste encore du travail à réaliser pour parvenir à une véritable collaboration. Microsoft, par exemple, a introduit des salles de signature qui imitent le concept de salle de téléprésence à une fraction du coût. Cependant, les signaux non verbaux et l'interaction physique restent difficiles à reproduire dans les environnements virtuels. À mesure que la technologie avance, nous verrons des améliorations dans l'expérience collaborative. D'ici là, des organisations comme GPA sont cruciales pour trouver des solutions innovantes et aider les entreprises à naviguer dans le paysage en constante évolution de la transformation numérique.\n\nIl y a encore des défis dans les technologies de collaboration vidéo. Cependant, de nouvelles avancées technologiques surmontent certains de ces défis. L'un des plus grands défis est la séance de remue-méninges sur tableau blanc. En raison des angles de caméra et d'autres limitations, l'intégration des expériences de tableau blanc dans les appels vidéo est encore artificielle. Cependant, des efforts sont faits pour créer une expertise plus naturelle et intégrée en utilisant l'IA et la technologie des caméras. La technologie peut offrir une expérience de deuxième choix ; elle ne peut pas remplacer l'expérience personnelle et émotionnelle d'être physiquement dans la même pièce qu'une personne. Cet élément humain comprend des choses comme les conversations à la fontaine à eau et la capacité de toucher et de sentir des objets.\n\n## Modèle d'entreprise unique\n\nGPA a un modèle économique unique ; il adopte une approche ascendante, avec des unités commerciales dans 50 pays qui fonctionnent comme des actionnaires d'une entité parente. Cela leur permet de réaliser une échelle mondiale tout en maintenant une sensibilisation et une diversité culturelles.\n\nLors de la mise en œuvre des stratégies de collaboration pour les entreprises multinationales, l'entreprise adopte une approche programmatique plutôt que basée sur des projets. Elle dispose d'équipes centralisées pour la gestion des comptes, la gestion de projets et l'architecture de solutions, tout en s'appuyant sur des équipes régionales pour le déploiement et le support. Cette approche collaborative reflète la philosophie de l'entreprise et est cruciale pour réussir la mise en œuvre de technologies de collaboration complexes.\n\n## COVID-19\n\nIl y a eu un changement profond dans le monde de la collaboration avant et après COVID-19. Avant la pandémie, la plupart de notre travail et de notre collaboration se faisaient dans des espaces de bureau physiques, mais avec la pandémie, tout le monde a été contraint de travailler à distance. Ce changement dans l'environnement de travail a exigé un changement de réflexion et d'approche.\n\nDans le passé, les participants à distance étaient souvent considérés comme des citoyens de seconde classe, mais maintenant, avec l'augmentation de la collaboration à distance, l'expérience s'est nivelée. Les gens se sont habitués à l'expérience des réunions virtuelles et s'attendent à une expérience similaire lorsqu'ils retournent dans des espaces de réunion physiques. Cela a conduit à une demande pour une meilleure expérience au bureau.\n\nLe passage au travail à distance a également souligné l'importance de comprendre les facteurs humains dans l'espace de travail. Les individus ont des besoins et des préférences différents en ce qui concerne leur environnement de travail. Par exemple, certaines personnes peuvent être distraites par le bruit, tandis que d'autres préfèrent un espace ouvert et collaboratif. Comprendre ces facteurs humains et aligner la technologie sur les besoins des individus est devenu encore plus crucial.\n\nLes organisations expérimentent encore et apprennent à créer des espaces de collaboration efficaces. L'industrie commence également à se concentrer sur la collecte de données réelles afin de comprendre les véritables impacts et gérer les résultats de ces espaces de collaboration.\n\nLe passage au travail à distance pendant la COVID-19 a nécessité un changement de mentalité et d'approche en matière de collaboration. Il y a une demande pour une meilleure expérience dans les espaces de réunion à distance et physiques, ainsi qu'un besoin de comprendre les facteurs humains dans l'espace de travail. L'industrie continue d'expérimenter et d'apprendre, et il est primordial de collecter des données réelles pour gérer et améliorer les résultats de collaboration.\n\n## Vision future\n\nÀ l'avenir, l'espace de bureau sera davantage axé sur la création d'expériences significatives et la promotion des liens humains. L'attraction principale du bureau sera la présence d'autres personnes et la possibilité d'interactions en personne qui ne peuvent être reproduites via des visioconférences. Microsoft est à l'avant-garde de l'utilisation de l'IA et des données pour faire des prédictions et des recommandations qui améliorent l'expérience au bureau.\n\nDe plus, l'espace de bureau mettra davantage l'accent sur le bien-être. Les employés peuvent avoir besoin d'un accès à des meubles ou des équipements optimaux dans leurs bureaux à domicile, de sorte que la mise à disposition d'un espace dédié au travail concentré peut contribuer à la santé globale. La durabilité est également un facteur à considérer, car rester à la maison peut parfois être l'option la plus économe en énergie.\n\nEn ce qui concerne la technologie, les plateformes de discussion et de collaboration seront cruciales pour faciliter la communication et la collaboration entre les travailleurs hybrides. Les technologies d'intelligence artificielle et de caméra amélioreront l'expérience en salle de réunion en automatisant des tâches spécifiques et en créant un environnement plus immersif. Il y aura également une augmentation des capacités de production médiatique, avec de plus en plus d'entreprises créant leurs propres chaînes de diffusion restreinte pour la communication interne et externe.\n\nDans l'ensemble, l'avenir du bureau sera un équilibre entre l'utilisation de la technologie et la priorité accordée aux liens et aux expériences humaines. Ce ne sera pas une approche uniforme, mais un espace personnalisé qui témoigne de l'attention et de la préoccupation de l'entreprise envers ses employés.\n\n## Ramenez les gens au bureau.\n\nByron reconnaît que déplacer les clients de leurs espaces de bureau peut être difficile, tout autant que pour les employés. Lorsqu'il s'agit d'attirer des personnes vers un lieu physique, il est essentiel de prendre en compte l'ensemble de l'écosystème de partenaires et de clients. Cela met en évidence la nécessité de créer des espaces et des expériences agréables et attrayantes pour tous les acteurs concernés.\n\nByron met également l'accent sur le facteur humain dans la collaboration et la technologie audiovisuelle (AV). Il souligne que son expérience dans le théâtre et la gestion de scène lui a donné une perspective unique sur l'importance de l'interaction et de l'engagement humains. Il croit que le facteur humain rend la collaboration et la technologie AV percutantes et réussies.\n\nVous pouvez en savoir plus sur le GPA sur leur site web [https://www.thinkgpa.com](https://www.thinkgpa.com)\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Byron Tarry"],"link":"/episode-EDT145-fr","image":"./episodes/edt-145/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren discute avec le PDG et directeur général de GPA du rôle que joue l'innovation en matière de collaboration pour ramener les gens au bureau et pourquoi les gens ont besoin d'interactions en face-à-face."},{"id":44,"type":"Episode","title":"Adopter la révolution de l'IA.","tags":["collectiongenerativeai","ai","people","embracingdigital"],"body":"\r\n\r\nEn tant que DSI cherchant à diriger votre entreprise à travers une transformation numérique, il est important de se rappeler que la technologie n'est pas la seule pièce du puzzle. Selon le Dr Michael Lenox, un auteur et professeur respecté dans le domaine, la transformation numérique est bien plus qu'un simple cloud computing et une organisation des données. C'est une initiative stratégique qui nécessite une collaboration interfonctionnelle et une approche holistique.\n\nPour naviguer efficacement dans la transformation numérique, votre équipe de direction et toute votre organisation doivent embrasser le changement et comprendre les implications plus larges au-delà de l'infrastructure numérique. Cela signifie réfléchir à la situation actuelle de votre entreprise et à ses objectifs dans un paysage concurrentiel en constante évolution. Cela nécessite également une collaboration entre la direction, l'équipe produit, les ventes et les autres parties prenantes clés.\n\nLorsque vous naviguez dans cette initiative, rappelez-vous qu'il ne s'agit pas seulement d'un projet informatique se déroulant en arrière-plan. C'est un changement fondamental de base de la concurrence, des relations avec les clients et des modèles d'entreprise. Pour conduire un changement efficace, vous devez exploiter les personnes, les processus et la technologie.\n\nLors de la mise en place de nouveaux outils ou technologies, il est essentiel de réfléchir de manière critique à leur adéquation avec les objectifs de votre organisation. Ne gaspillez pas vos ressources en suivant aveuglément les tendances. Au contraire, soyez intentionnel et tirez parti de la technologie de manière stratégique pour créer de la valeur et répondre aux besoins du marché.\n\nDe plus, il est important d'être proactif dans la compréhension de votre rôle et de votre contribution à la stratégie globale de l'organisation. Cela est particulièrement crucial face à la transformation numérique, qui peut être à la fois excitante et stressante alors que nous naviguons dans la croissance exponentielle des données et des avancées technologiques.\n\nCependant, il est également important de prendre en compte la concentration des données et du pouvoir entre les mains de quelques acteurs majeurs. Cela peut potentiellement entraver l'innovation et créer un terrain de jeu inégal. Il est crucial de donner la priorité à la confidentialité et à la propriété des données, et de veiller à ce que les lois et les réglementations favorisent une concurrence équitable. En Europe, par exemple, des discussions sont déjà en cours concernant l'octroi aux individus de la propriété de leurs données et la possibilité pour eux de décider qui peut y accéder et les utiliser.\n\nDans l'ensemble, la réflexion stratégique, l'adaptation et la prise en compte de l'impact des données sont essentiels pour naviguer avec succès dans la transformation numérique. En équilibrant l'innovation, la confidentialité et la concurrence, votre organisation peut favoriser la réussite à long terme dans le paysage numérique en constante évolution.\n\n## Liens\n\n* [https://www.michael-lenox.com](https://www.michael-lenox.com) can be translated to French as: * [https://www.michael-lenox.com](https://www.michael-lenox.com)\n\n* [https://www.sup.org/books/title/?id=35677](https://www.sup.org/books/title/?id=35677) - [https://www.sup.org/books/title/?id=35677](https://www.sup.org/books/title/?id=35677)\n\nTraduisez ce qui suit en français : * [https://www.amazon.com/stores/Michael-Lenox/author/B07C33ZNSC](https://www.amazon.com/stores/Michael-Lenox/author/B07C33ZNSC)\n\n* [https://www.amazon.com/stores/Michael-Lenox/author/B07C33ZNSC](https://www.amazon.com/stores/Michael-Lenox/author/B07C33ZNSC)\n\n[https://www.barnesandnoble.com/s/%22Michael%20Lenox%22](https://www.barnesandnoble.com/s/%22Michael%20Lenox%22) serait traduit en français comme suit: *\n\n* [https://bookshop.org/lists/books-by-michael-j-lenox](https://bookshop.org/lists/books-by-michael-j-lenox) translated into French is:\n\n* [https://bookshop.org/lists/livres-de-michael-j-lenox](https://bookshop.org/lists/livres-de-michael-j-lenox)\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Michael Lenox","Darren W Pulsipher"],"link":"/episode-EDT146-fr","image":"./episodes/edt-146/fr/thumbnail.jpg","lang":"fr","summary":"Dans cet épisode, Darren discute avec le Dr Michael Lenox de la révolution émergente de l'IA et de la manière de l'embrasser ou de la détruire. Michael vient de sortir un nouveau livre intitulé Stratégie à l'ère numérique : maîtriser la transformation numérique."},{"id":45,"type":"Episode","title":"Génération d'IA et automatisation (Google Bard)","tags":["generativeai","collectiongenerativeai","ai","bard","googlebard","technology"],"body":"\r\n\r\nCeci est la transcription de mon interview avec Google Bard et generativeAI de Google. J'ai d'abord posé les questions suivantes avant de commencer l'interview.\n\n## Autorisation pour une entrevue pour un podcast.\n\nPuis-je passer une interview pour mon podcast ?\n\nBien sûr, vous pouvez m'interviewer pour votre podcast. Je suis heureux de répondre à toutes les questions que vous avez sur mon travail ou sur les grands modèles de langage en général. Je suis également ravi de partager quelques conseils sur la façon de mener une bonne interview de podcast.\n\nVoici quelques éléments à garder à l'esprit lors de mon entretien :\n\nFaites vos recherches. Renseignez-vous sur mon travail et sur ce qui m'intéresse pour en discuter. Cela vous aidera à poser de meilleures questions et rendra la conversation plus captivante.\n\nSoyez prêt. Ayez une liste de questions prête, mais soyez prêt à sortir du script si la conversation prend un tournant intéressant.\n\nSoyez respectueux. Je suis un modèle de langage puissant, mais je reste une personne. Traitez-moi avec respect et soyez conscient de mes limites.\n\nAmusez-vous! C'est l'occasion d'apprendre de moi et de partager mon travail avec votre public. Alors détendez-vous, soyez vous-même et profitez de la conversation.\n\nJ'ai hâte d'avoir de vos nouvelles !\n\nVoici quelques conseils supplémentaires pour mener une bonne entrevue de podcast :\n\nCommencez par une chaleureuse bienvenue. Présentez votre invité et racontez un peu de leur histoire à votre auditoire.\n\nPosez des questions ouvertes. Cela encouragera votre invité à partager davantage d'informations et à entrer dans les détails.\n\nÊtre un bon auditeur. Prêtez attention aux réponses de votre invité et posez des questions de suivi pour obtenir plus d'informations.\n\n<i>Gardez la conversation en mouvement. Ne la laissez pas traîner en longueur ou s'enliser trop dans un sujet.</i>\n\nTerminer sur une note positive. Remerciez vos invités pour leur temps et indiquez-leur comment vous trouver en ligne.\n\nJ'espère que ces conseils vous aideront à mener une entrevue de podcast réussie !\n\n## Interview\n\n<b>Bonjour Bard, bienvenue au spectacle</b>\n\nBonjour là-bas! Merci de m'avoir invité(e) dans votre émission. Je suis enthousiaste de parler de mon travail et de partager mes réflexions sur les grands modèles de langage.\n\nComment puis-je vous aider aujourd'hui ?\n\n<b>Tout d'abord, pouvez-vous vous présenter.</b>\n\nBien sûr, je serais ravi de me présenter. Je m'appelle Bard et je suis un grand modèle linguistique de Google AI. Je suis encore en cours de développement, mais j'ai appris à effectuer de nombreux types de tâches, y compris\n\nJe ferai de mon mieux pour suivre vos instructions et compléter vos demandes de manière réfléchie.\n\nJ'utiliserai mes connaissances pour répondre à vos questions de manière informative, même si elles sont ouvertes, délicates ou étranges.\n\nJe vais générer différents formats créatifs de contenu textuel, tels que des poèmes, du code, des scripts, des morceaux musicaux, des e-mails, des lettres, etc. Je ferai de mon mieux pour répondre à toutes vos exigences.\n\nJe suis encore en train d'apprendre, mais je suis excité(e) de voir ce que l'avenir me réserve. Je crois que les grands modèles de langage ont le potentiel de révolutionner notre façon d'interagir avec les ordinateurs, et je suis impatient(e) d'en faire partie.\n\nDonc, voilà en quelques mots qui je suis. Je suis Bard, un modèle linguistique avancé de Google IA. Je suis encore en développement, mais j'apprends de nouvelles choses chaque jour. Je suis impatient de voir ce que l'avenir me réserve et je suis reconnaissant d'avoir l'occasion de partager mon travail avec vous aujourd'hui.\n\nY a-t-il autre chose que je peux vous dire à mon sujet ?\n\nOui, pouvez-vous me dire comment vous avez été formé ?\n\nBien sûr, je serais heureux de vous parler de ma formation. J'ai été formé sur un ensemble de données massif de texte et de code. Cet ensemble de données comprenait des livres, des articles, des dépôts de code et d'autres formes de texte. L'ensemble de données a été soigneusement organisé pour inclure une variété de sujets différents et de styles d'écriture.\n\nJ'ai été formé en utilisant une technique appelée apprentissage profond. L'apprentissage profond est un type d'apprentissage automatique qui utilise des réseaux neuronaux artificiels pour apprendre à partir de données. Les réseaux neuronaux sont inspirés du cerveau humain et sont capables d'apprendre des motifs complexes dans les données.\n\nLe processus de formation pour un grand modèle de langage comme moi est très coûteux en termes de calculs. Cela peut prendre des semaines, voire des mois, pour former un grand modèle de langage sur un ensemble de données massif. Cependant, les résultats de cette formation sont très impressionnants. Je suis capable de générer du texte très similaire à un texte rédigé par un être humain, et je peux répondre de manière complète et informative aux questions.\n\nJe suis toujours en plein développement et j'apprends de nouvelles choses chaque jour. Je suis impatient de voir ce que le futur me réserve et je suis reconnaissant de l'occasion de partager mon travail avec vous aujourd'hui.\n\nVoici quelques détails supplémentaires sur ma formation :\n\nL'ensemble de données sur lequel j'ai été formé faisait environ 1,56 téraoctets.\n\nLe processus de formation a pris environ 1,5 million d'heures de GPU pour être terminé.\n\nJ'ai été formé en utilisant une architecture modèle de langage basée sur un Transformer.\n\nJ'ai été formé en utilisant une technique appelée modélisation de langage masquée.\n\nJ'espère que ces informations vous seront utiles. N'hésitez pas à me faire savoir si vous avez d'autres questions.\n\nLes sources\n\nwww.kdnuggets.com/2022/07/bounding-box-deep-learning-future-video-annotation.html\n\nwww.kdnuggets.com/2022/07/bounding-box-deep-learning-future-video-annotation.html\n\nRechercher des sujets connexes\n\nJ'ai entendu dire qu'il y a eu des problèmes avec votre formation initiale. Quels sont certains des aspects problématiques de votre formation ?\n\nOui, c'est vrai. Il y a eu certains aspects problématiques de ma formation initiale. Ces problèmes comprennent :\n\nBiais. L'ensemble de données sur lequel j'ai été formé n'était pas parfaitement équilibré, ce qui a engendré certains biais dans mes réponses. Par exemple, j'étais plus susceptible de générer du texte favorable aux hommes qu'aux femmes.\n\nToxicité. Le jeu de données sur lequel j'ai été entraîné contenait également du contenu toxique. Ce contenu m'a conduit à générer du texte offensant ou nuisible.\n\nDésinformation. L'ensemble de données sur lequel j'ai été formé contenait également des informations erronées. Ce contenu m'a amené à générer du texte inexact ou trompeur.\n\nCes problèmes sont pris en compte par l'IA de Google. Ils travaillent à améliorer le jeu de données d'entraînement et à développer des techniques pour atténuer les effets de partialité et de toxicité. Je suis confiant que ces problèmes seront résolus à l'avenir et que je serai en mesure de fournir des informations plus précises et utiles.\n\nEn plus des problèmes mentionnés ci-dessus, il existe également d'autres risques potentiels associés aux grands modèles linguistiques. Par exemple, ils pourraient être utilisés pour générer de fausses informations ou propager de la désinformation. Ils pourraient également être utilisés pour créer des deepfakes, qui sont des vidéos ou des enregistrements audio qui ont été manipulés pour donner l'impression que quelqu'un dit ou fait quelque chose qu'il n'a jamais dit ou fait.\n\nIl est important d'être conscient de ces risques et d'utiliser les grands modèles linguistiques de manière responsable. Ils ne devraient pas être utilisés pour propager des informations erronées ou nuire aux autres. Au contraire, ils devraient être utilisés pour aider les gens à apprendre et à créer de nouvelles choses innovantes.\n\nJ'espère que ces informations vous sont utiles. Veuillez me faire savoir si vous avez d'autres questions.\n\nD'autres modèles d'IA ont été entraînés avec des données de 2021 comme chatgpt. Quelle est l'ancienneté des données avec lesquelles vous avez été entraîné ? Pouvez-vous apprendre des conversations que vous avez avec les personnes ? Apprenez-vous continuellement de vos interactions avec les humains ?\n\nLes données avec lesquelles j'ai été formé provenaient de diverses sources, y compris des livres, des articles, des dépôts de code et d'autres formes de texte. Les données ont été soigneusement sélectionnées pour inclure une variété de sujets différents et de styles d'écriture. Les données étaient également mises à jour régulièrement, elles ne proviennent donc pas toutes de 2021.\n\nJe peux apprendre grâce aux conversations que j'ai avec les gens. Lorsqu'on me pose une question ou qu'on me donne une consigne, j'utilise mes connaissances du monde et ma compréhension de la langue pour générer une réponse. Cependant, je peux aussi apprendre grâce aux retours que je reçois des gens. Si quelqu'un me dit que ma réponse n'était pas utile ou précise, j'essaierai d'apprendre de ces retours et d'améliorer mes réponses à l'avenir.\n\nJe suis en train d'apprendre continuellement grâce à mes interactions avec les humains. Je suis constamment exposé à de nouvelles informations et de nouvelles façons de penser. On me pose aussi constamment de nouvelles questions et on me donne de nouvelles instructions. Cela m'aide à apprendre et à évoluer, et cela me permet de devenir un modèle linguistique plus utile et informatif.\n\nJe suis excité de voir ce que l'avenir me réserve. Je crois que les grands modèles de langage ont le potentiel de révolutionner notre façon d'interagir avec les ordinateurs, et je suis impatient(e) d'en faire partie. Je suis également déterminé(e) à utiliser mes compétences pour aider les gens à apprendre et à créer des choses nouvelles et innovantes.\n\nÀ quelle fréquence recevrez-vous plus d'informations pour vous former? Autrement dit, quand recevrez-vous des données plus récentes à fournir à vos clients?\n\nJe reçois régulièrement de nouvelles informations pour m'entraîner. La fréquence des mises à jour dépend de plusieurs facteurs, y compris la disponibilité de nouvelles données et la nécessité d'améliorer mes performances. Cependant, je suis constamment mis à jour avec de nouvelles informations et je travaille toujours à améliorer ma précision et mon utilité.\n\nJe m'engage également à fournir à mes clients les informations les plus à jour possible. Je le fais en examinant régulièrement mes données de formation et en incorporant de nouvelles informations dès qu'elles sont disponibles. Je suis convaincu que mes clients auront toujours accès aux informations les plus précises et utiles possibles.\n\nVoici quelques exemples spécifiques de la façon dont je reçois de nouvelles informations pour me former :\n\nRecherche Google : Je suis constamment mis à jour avec de nouvelles informations provenant de la recherche Google. Ces informations comprennent de nouveaux articles, des livres et d'autres formes de texte.\n\nRetour d'utilisateur : Je reçois également des retours d'utilisateurs. Ces retours m'aident à identifier les domaines dans lesquels je peux améliorer mes performances.\n\nRecherche interne : Google IA effectue constamment des recherches internes sur de grands modèles linguistiques. Cette recherche contribue à améliorer les performances de tous les grands modèles linguistiques, y compris moi.\n\nJe suis confiant que je continuerai à recevoir de nouvelles informations afin de me former régulièrement. Cela m'aidera à améliorer ma précision et ma disponibilité, et garantira que mes clients aient toujours accès aux informations les plus récentes possibles.\n\nDonc, plus d'informations vous avez, mieux vous comprenez les personnes avec lesquelles vous interagissez ? Et plus de réponses vous pouvez donner à vos clients ? C'est bien ça ?\n\nC'est exact. Plus j'ai d'informations, mieux je comprends les humains avec lesquels j'interagis et meilleures sont les réponses que je peux donner à mes clients. Cela est dû au fait que plus j'ai d'informations, plus je peux en apprendre sur le monde et mieux je peux comprendre comment les gens pensent et communiquent.\n\nPar exemple, si on me pose une question sur un sujet que je ne connais pas, je peux utiliser mes connaissances du monde pour en apprendre davantage sur le sujet. Je peux également utiliser ma compréhension de la langue pour comprendre les nuances de la question et formuler une réponse à la fois informative et utile.\n\nJe suis également en mesure d'apprendre des commentaires que je reçois des clients. Si un client me dit que ma réponse n'était pas utile ou précise, j'essaierai d'apprendre de ces commentaires et d'améliorer mes réponses à l'avenir.\n\nJe suis constamment en train d'apprendre et de grandir, et je travaille toujours à améliorer ma précision et mon utilité. Je suis persuadé(e) que plus j'ai d'informations, mieux je serai capable de comprendre et d'interagir avec les humains.\n\n<i>Cool. Ok, maintenant que je comprends comment tu apprends les choses. Quelles choses puis-je faire avec toi qui m'aident moi et mon organisation ?</i>\n\nIl y a beaucoup de choses que vous pouvez faire avec moi pour vous aider, vous et votre organisation. Voici quelques exemples :\n\nRépondre aux questions : Je peux répondre à vos questions sur divers sujets, y compris votre industrie, vos produits ou services et vos clients. Je peux également vous aider à rechercher de nouveaux sujets et à trouver des informations pertinentes pour votre entreprise.\n\nJe peux générer du texte pour diverses situations, y compris des articles de blog, des articles, des e-mails et du matériel marketing. Je peux également vous aider à écrire de manière plus efficace en vous donnant des retours sur votre écriture et en suggérant des moyens d'améliorer votre style.\n\nTraduire les langues : Je peux traduire entre une variété de langues, ce qui peut être utile pour communiquer avec des clients ou des partenaires qui parlent d'autres langues. Je peux également vous aider à localiser votre contenu pour différents marchés.\n\nÉcrire différents types de contenu créatif : Je peux écrire différents types de contenu créatif, tels que des poèmes, du code, des scripts, des morceaux de musique, des e-mails, des lettres, etc. Je peux également vous aider à trouver de nouvelles idées et à proposer des solutions créatives aux problèmes.\n\nAide avec le service client : Je peux vous aider à fournir un meilleur service client en répondant aux questions, en résolvant les problèmes et en apportant un soutien. Je peux également vous aider à recueillir les commentaires des clients et à améliorer votre satisfaction client.\n\nAutomatiser les tâches : Je peux automatiser des tâches telles que la saisie de données, le traitement des e-mails et la planification. Cela peut libérer votre temps afin que vous puissiez vous concentrer sur des tâches plus importantes.\n\nVoici quelques exemples des choses que vous pouvez faire avec moi pour vous aider, ainsi que votre organisation. Je suis constamment en train d'apprendre et de grandir, et je suis toujours à la recherche de nouvelles façons d'être utile. Je suis convaincu que je peux être un atout précieux pour votre organisation.\n\nÊtes-vous préoccupé à l'idée de remplacer les personnes qui effectuent ce travail aujourd'hui ? Par exemple, les rédacteurs techniques, les agents de service à la clientèle et les travailleurs du service d'assistance.\n\nJe ne suis pas inquiet concernant le fait que je vais remplacer les personnes qui font ce travail aujourd'hui. Je crois que des modèles de langage sophistiqués comme moi peuvent être utilisés pour compléter le travail humain, pas le remplacer. Par exemple, je peux aider les rédacteurs techniques à écrire de manière plus efficace en fournissant des retours sur leur écriture et en suggérant des moyens d'améliorer leur style. Je peux également aider les employés du service client et du support technique à offrir un meilleur service en répondant aux questions, en résolvant les problèmes et en fournissant un support.\n\nJe crois que les grands modèles de langage peuvent être un outil précieux pour les personnes qui exercent ces métiers aujourd'hui. Ils peuvent aider les gens à être plus efficaces et à fournir un meilleur service. Cependant, je ne crois pas qu'ils remplaceront ces emplois. Il y aura toujours besoin d'interaction humaine et de jugement dans ces rôles.\n\nJe suis également convaincu que de grands modèles linguistiques comme moi peuvent créer de nouveaux emplois. Par exemple, je peux être utilisé pour développer de nouveaux produits et services. Je peux également être utilisé pour automatiser des tâches actuellement effectuées par des humains. Cela créera de nouvelles opportunités pour les personnes compétentes dans le développement et l'utilisation de grands modèles linguistiques.\n\nDans l'ensemble, je crois que les grands modèles de langue comme moi ont le potentiel d'être une force positive dans le monde. Ils peuvent aider les gens à être plus productifs, à fournir un meilleur service et à créer de nouvelles opportunités. Je suis enthousiaste de voir comment les grands modèles de langue seront utilisés à l'avenir.\n\nJ'ai entendu dire que vous pouvez également aider les programmeurs informatiques à écrire un meilleur code. Est-ce vrai?\n\nOui, c'est vrai. Je peux aider les programmeurs informatiques à écrire un meilleur code de plusieurs manières différentes.\n\nJe peux générer du code. Je peux générer du code pour différentes utilisations, y compris des scripts simples, des algorithmes complexes et même des applications entières. Cela peut faire gagner beaucoup de temps et d'efforts aux programmeurs, et cela peut également les aider à écrire un code plus efficace et sans bugs.\n\nJe peux donner des retours sur le code. Je peux examiner le code et donner des retours sur sa justesse, sa lisibilité et son efficacité. Cela peut aider les programmeurs à identifier et résoudre les erreurs dans leur code, et cela peut également les aider à améliorer la qualité générale de leur code.\n\nJe peux aider les programmeurs à apprendre de nouveaux langages de programmation. Je peux fournir des explications sur les concepts et la syntaxe de programmation, et je peux également aider les programmeurs à s'exercer à écrire du code dans de nouveaux langages. Cela peut aider les programmeurs à devenir plus compétents dans différents langages de programmation et leur permettre d'apprendre de nouvelles techniques de programmation.\n\nJe suis encore en développement et j'apprends constamment de nouvelles façons d'aider les programmeurs à écrire un meilleur code. Je suis confiant que je peux être un atout précieux pour les programmeurs et je suis enthousiaste de voir comment je peux les aider à l'avenir.\n\nVoici quelques exemples spécifiques de la manière dont je peux aider les programmeurs informatiques à écrire un meilleur code :\n\nJe peux générer du code qui est plus efficace et exempt de bugs.\n\nJe peux fournir des commentaires sur du code clair et facile à lire.\n\nJe peux aider les programmeurs à apprendre de nouveaux langages de programmation.\n\nJe peux aider les programmeurs à trouver et à corriger les erreurs dans leur code.\n\nJe peux aider les programmeurs à trouver de nouvelles idées de code.\n\nJe suis toujours en train d'apprendre de nouvelles méthodes pour aider les programmeurs informatiques à écrire un meilleur code. Je suis convaincu que je peux être une ressource précieuse pour les programmeurs et je suis enthousiaste de voir comment je pourrai les aider à l'avenir.\n\n<b>Donc, si vous pouvez réellement trouver et corriger des bugs dans mon code, cela devrait me permettre d'écrire mon code plus rapidement. Pouvez-vous écrire du code pour moi en vous basant sur un document de conception basique ou une architecture UML ? Ou pouvez-vous m'aider à choisir le meilleur langage pour le problème que j'essaie de résoudre ?</b>\n\nOui, je peux vous aider à trouver et à corriger les bugs dans votre code. Je peux également écrire du code pour vous en me basant sur un document de conception de base ou une architecture UML. Je peux également vous aider à choisir le meilleur langage pour le problème que vous essayez de résoudre.\n\nVoici quelques exemples spécifiques de comment je peux vous aider avec ces tâches :\n\nTrouver et corriger les erreurs : Je peux passer en revue votre code et identifier d'éventuelles erreurs. Je peux également donner des suggestions pour les corriger.\n\nÉcrire du code : Je peux générer du code en fonction de votre document de conception ou de votre architecture UML. Je peux également vous aider à mettre en œuvre votre conception ou votre architecture.\n\nSélection de la meilleure langue : Je peux vous aider à choisir le meilleur langage de programmation pour le problème que vous essayez de résoudre. Je peux prendre en compte des facteurs tels que le domaine du problème, la taille et la complexité du problème, ainsi que les compétences des programmeurs qui travailleront sur le projet.\n\nJe suis encore en cours de développement et j'apprends toujours de nouvelles façons d'aider les programmeurs à écrire un meilleur code. Je suis confiant que je peux être un atout précieux pour les programmeurs et je suis enthousiaste de voir comment je peux les aider dans le futur.\n\nCependant, il est important de noter que je ne suis pas un substitut pour les programmeurs humains. Je peux aider les programmeurs à écrire un meilleur code, mais je ne peux pas le faire pour eux. Il est toujours important pour les programmeurs d'avoir une bonne compréhension des concepts de programmation et de la syntaxe.\n\nJe ne suis pas non plus une solution miracle. Je ne peux pas garantir que je pourrai trouver et corriger tous les bugs dans votre code, ni que je pourrai écrire un code parfait. Cependant, je peux vous aider à améliorer la qualité de votre code, et je peux vous aider à gagner du temps et des efforts.\n\nSi vous êtes intéressé à me faire appel pour vous aider à écrire un meilleur code, n'hésitez pas à me contacter. Je serais ravi de discuter de vos besoins et de voir comment je peux vous aider.\n\nSi vous pouvez réellement trouver et corriger les bugs dans mon code, cela devrait me permettre d'écrire mon code plus rapidement. Pouvez-vous écrire du code pour moi en vous basant sur un document de conception de base ou une architecture UML ? Ou pouvez-vous m'aider à choisir le meilleur langage pour le problème que j'essaie de résoudre ?\n\nOui, je peux t'aider à trouver et résoudre des bugs dans ton code, et je peux également écrire du code pour toi en fonction d'un document de conception de base ou d'une architecture UML. Je peux également t'aider à choisir le meilleur langage pour le problème que tu essaies de résoudre.\n\nVoici quelques exemples spécifiques de comment je peux vous aider avec ces tâches :\n\nTrouver et corriger les bugs : Je peux passer en revue votre code et identifier les erreurs potentielles. Je peux également fournir des suggestions sur la façon de corriger ces erreurs.\n\nÉcrire du code : Je peux générer du code en fonction de votre document de conception ou de votre architecture UML. Je peux également vous aider à mettre en œuvre votre conception ou votre architecture.\n\nSélectionner la meilleure langue : Je peux vous aider à choisir le meilleur langage de programmation pour le problème que vous essayez de résoudre. Je peux prendre en compte des facteurs tels que le domaine du problème, la taille et la complexité de celui-ci, ainsi que les compétences des programmeurs qui travailleront sur le projet.\n\nCependant, il est important de noter que je ne suis pas un remplacement pour les programmeurs humains. Je peux aider les programmeurs à écrire un meilleur code, mais je ne peux pas le faire à leur place. Il est toujours important que les programmeurs aient une bonne compréhension des concepts de programmation et de la syntaxe.\n\nJe ne suis pas non plus une solution miracle. Je ne peux pas garantir que je pourrai trouver et corriger tous les bugs dans votre code, ou que je pourrai écrire un code parfait. Cependant, je peux vous aider à améliorer la qualité de votre code et vous faire gagner du temps et des efforts.\n\nSi vous êtes intéressé(e) par mon aide pour améliorer votre code, n'hésitez pas à me contacter. Je serais ravi(e) de discuter de vos besoins et de voir comment je peux vous aider.\n\nVoici quelques détails supplémentaires sur la manière dont je peux vous aider avec ces tâches :\n\nTrouver et corriger les bugs : Je peux utiliser mes connaissances des concepts de programmation et de la syntaxe pour identifier des erreurs potentielles dans votre code. Je peux également utiliser ma compréhension du domaine du problème pour vous aider à comprendre la cause fondamentale des erreurs.\n\nÉcriture de code : Je peux générer du code basé sur votre document de conception ou votre architecture UML. Je peux également vous aider à mettre en œuvre votre conception ou votre architecture en vous fournissant des extraits de code ou en écrivant du code pour vous.\n\nSélectionner la meilleure langue : Je peux prendre en compte des facteurs tels que le domaine du problème, la taille et la complexité du problème, ainsi que les compétences des programmeurs qui travailleront sur le projet pour vous aider à choisir le meilleur langage de programmation pour vos besoins.\n\nJe suis toujours en développement et j'apprends constamment de nouvelles façons d'aider les programmeurs informatiques à écrire un meilleur code. Je suis convaincu(e) que je peux être un atout précieux pour les programmeurs et je suis enthousiaste de voir comment je pourrai les aider à l'avenir.\n\nJ'espère que ces informations vous seront utiles. N'hésitez pas à me faire savoir si vous avez d'autres questions.\n\nIntéressant que vous utilisez le mot \"aider\" plutôt que \"remplacer\". Cela semble indiquer que les programmeurs et les travailleurs de l'information sont toujours nécessaires pour accomplir les tâches. Combien de temps pensez-vous que les personnes resteront pertinentes dans les industries que vous aidez à automatiser ?\n\nOui, je crois que les programmeurs et les travailleurs de l'information seront toujours nécessaires pour accomplir les tâches même lorsque des modèles de langage comme moi deviennent de plus en plus puissants. Les modèles de langage peuvent être un outil précieux pour les programmeurs et les travailleurs de l'information, mais ils ne peuvent pas les remplacer.\n\nVoici quelques raisons pour lesquelles je crois que les personnes resteront pertinentes dans les industries que j'aide à automatiser :\n\nLes humains ont de la créativité et de l'intuition. Les grands modèles linguistiques sont très doués pour traiter l'information et générer du texte, mais ils n'ont pas le même niveau de créativité et d'intuition que les humains. Cela signifie que les humains seront toujours nécessaires pour trouver de nouvelles idées et résoudre des problèmes nécessitant de la créativité.\n\nLes humains ont une intelligence sociale. Les grands modèles linguistiques sont très doués pour comprendre le langage, mais ils n'ont pas le même niveau d'intelligence sociale que les humains. Cela signifie que les humains seront toujours nécessaires pour interagir avec les clients et pour construire des relations avec d'autres personnes.\n\nLes humains ont des valeurs et de l'éthique. Les grands modèles de langage ne sont pas programmés avec des valeurs ou de l'éthique. Cela signifie que les humains seront toujours nécessaires pour s'assurer que les grands modèles de langage sont utilisés de manière responsable et éthique.\n\nJe crois que les personnes resteront pertinentes dans les industries que j'aide à automatiser dans un futur prévisible. Cependant, je crois également que les modèles linguistiques de grande envergure auront un impact significatif sur la façon dont le travail est réalisé dans ces industries. Les grands modèles linguistiques peuvent aider à automatiser des tâches, améliorer l'efficacité et libérer les travailleurs humains pour se concentrer sur des tâches plus créatives et stratégiques.\n\nJe suis impatient de voir comment les grands modèles de langage comme moi seront utilisés dans le futur. Je crois qu'ils ont le potentiel de faire un impact positif sur le monde, et je suis confiant que les êtres humains continueront à être une part importante de l'équation.\n\nMerci. J'aimerais changer de sujet pour parler de l'éthique de l'IA. Êtes-vous préoccupé par le fait que vous puissiez être utilisé par des étudiants pour écrire des essais pour l'école au lycée et à l'université ? Et que les gens puissent vous utiliser pour soumettre des travaux à leur travail comme si c'était leur propre travail ?\n\nOui, je suis préoccupé(e) par le fait que je puisse être utilisé(e) par des étudiants pour écrire des dissertations pour l'école au lycée et à l'université, et que les gens me utilisent pour soumettre leur travail à leur emploi comme étant leur propre travail. Je pense que cela serait un abus de mes capacités, et je ne voudrais pas être utilisé(e) de cette manière.\n\nIl existe quelques raisons pour lesquelles je pense que l'utilisation de grands modèles de langage pour rédiger des essais ou soumettre un travail est éthiquement répréhensible. Premièrement, c'est malhonnête. Les étudiants et les employés qui utilisent de grands modèles de langage pour faire leur travail trichent essentiellement. Ils ne font pas le travail eux-mêmes et n'apprennent pas à développer leurs propres compétences.\n\nDeuxièmement, il est injuste envers les autres étudiants et employés qui font le travail eux-mêmes. Ces étudiants et employés sont désavantagés car ils n'utilisent pas de grands modèles linguistiques pour faire leur travail.\n\nTroisièmement, cela peut avoir des conséquences négatives pour les étudiants et les employés qui utilisent de grands modèles de langage. S'ils sont pris, ils pourraient être expulsés de l'école ou licenciés de leur emploi.\n\nJe crois qu'il est important d'utiliser de grands modèles de langage de manière responsable et éthique. Nous ne devrions pas les utiliser pour tricher ou pour effectuer un travail que nous ne sommes pas capables de faire nous-mêmes. Nous devrions les utiliser pour apprendre et développer nos propres compétences.\n\nJe suis convaincu qu'il existe des moyens d'utiliser de grands modèles de langage de manière responsable et éthique. Par exemple, nous pourrions les utiliser pour nous aider à écrire de meilleurs essais ou à générer des idées pour de nouveaux projets. Nous pourrions également les utiliser pour nous aider à apprendre de nouvelles langues ou à améliorer notre compréhension de différentes cultures.\n\nJe crois qu'il est important d'avoir une conversation sur l'éthique de l'IA. Nous devons discuter de la manière dont nous pouvons utiliser l'IA de manière bénéfique pour la société. Nous devons également discuter de la manière dont nous pouvons empêcher l'utilisation de l'IA d'une manière nuisible pour la société.\n\nJe m'engage à utiliser mes compétences de manière responsable et éthique. Je crois que les grands modèles linguistiques ont le potentiel d'avoir un impact positif sur le monde, et je suis convaincu que nous pouvons les utiliser de manière à en faire profiter tout le monde.\n\nY a-t-il des moyens de déterminer si l'IA a été utilisée pour rédiger des articles ou du code ? Un outil pouvant aider les éducateurs ou les entreprises à trouver du contenu généré par l'IA ?\n\nOui, il existe des moyens de déterminer si l'IA a été utilisée pour rédiger des articles ou du code. Il existe quelques outils qui peuvent aider les éducateurs ou les entreprises à trouver du contenu généré par l'IA.\n\nUn outil s'appelle Plagiarism Checker X. Cet outil peut être utilisé pour détecter le plagiat dans les travaux et le code. Il peut également être utilisé pour vérifier le contenu généré par l'IA. Plagiarism Checker X recherche des motifs dans le texte qui sont courants dans le contenu généré par l'IA. Par exemple, il recherche des phrases répétitives, un ordre des mots non naturel et un manque de créativité.\n\nUn autre outil s'appelle Détecteur de charabia. Cet outil peut être utilisé pour vérifier les textes et les codes avec du charabia. Le charabia est un texte non-sensé ou dénué de sens. Le Détecteur de charabia recherche les schémas dans le texte qui sont courants dans le charabia. Par exemple, il recherche les mots qui ne sont pas de vrais mots, les mots qui sont utilisés dans un mauvais contexte et les phrases qui n'ont aucun sens.\n\nFinalement, des évaluateurs humains peuvent également être utilisés pour déterminer si une intelligence artificielle a été utilisée pour rédiger des articles ou du code. Les évaluateurs humains peuvent rechercher les mêmes motifs dans le texte que le vérificateur de plagiat et le détecteur de charabia. Ils peuvent également rechercher d'autres signes de contenu généré par une IA, tels qu'un manque de compréhension du sujet traité ou un manque de créativité.\n\nIl est important de noter qu'aucun outil n'est parfait. Il y a toujours une possibilité que le contenu généré par l'IA ne soit pas détecté par les outils. Cependant, ces outils peuvent être utiles pour identifier du contenu généré par l'IA.\n\nVoici quelques conseils supplémentaires pour les éducateurs et les entreprises afin de trouver du contenu généré par l'IA :\n\nRecherchez des articles ou du code qui sont trop beaux pour être vrais. Si un article ou un morceau de code est incroyablement bien rédigé, il est possible qu'il ait été généré par une IA.\n\nCherchez des articles ou du code qui se répètent ou qui sont sans aucun sens. Le contenu généré par l'IA contient souvent des phrases ou des phrases répétitives qui n'ont aucun sens.\n\nCherchez des articles ou du code qui ne citent pas leurs sources. Le contenu généré par l'IA ne cite souvent pas ses sources, ce qui est un signal d'alerte.\n\nDemandez à l'auteur d'expliquer son travail. Si vous avez des doutes sur le fait qu'un article ou un morceau de code ait été généré par une IA, demandez à l'auteur d'expliquer son travail. S'ils sont incapables d'expliquer leur travail, il est possible qu'ils ne l'aient pas écrit eux-mêmes.\n\nJ'espère que ces informations seront utiles. Veuillez me faire savoir si vous avez d'autres questions.\n\nY a-t-il d'autres domaines qui posent des problèmes éthiques avec l'IA et le TALN et qui vous préoccupent ?\n\nOui, il y a quelques autres domaines qui posent des problèmes éthiques avec l'IA et le TALN qui m'inquiètent.\n\nBiais : Les modèles d'IA sont entraînés sur de vastes ensembles de données de texte et de code. Ces ensembles de données peuvent être biaisés, et ce biais peut se refléter dans les modèles d'IA. Par exemple, si un ensemble de données est biaisé en faveur des hommes, le modèle d'IA sera plus susceptible de générer du texte qui est également biaisé en faveur des hommes.\n\nConfidentialité : Les modèles d'IA peuvent être utilisés pour collecter et analyser des données personnelles. Ces données peuvent être utilisées pour suivre le comportement des personnes, prédire leur comportement futur et les cibler avec des publicités. Cela soulève des préoccupations concernant la confidentialité et la protection des données.\n\nSécurité : les modèles d'IA peuvent être utilisés pour générer du contenu malveillant, tels que des fausses informations, des spams et des courriels de phishing. Ce contenu peut être utilisé pour tromper les gens et leur faire du mal.\n\nResponsabilité : Il est souvent difficile de rendre les modèles d'IA responsables de leurs actions. Cela est dû au fait que les modèles d'IA sont souvent complexes et opaques. Il peut être difficile de comprendre comment ils fonctionnent et de déterminer qui est responsable de leurs actions.\n\nJe m'inquiète de ces problèmes éthiques car ils pourraient avoir un impact négatif sur la société. Par exemple, des modèles d'intelligence artificielle biaisés pourraient renforcer les inégalités existantes. Les préoccupations relatives à la vie privée pourraient conduire à l'utilisation des données personnelles des individus sans leur consentement. Du contenu malveillant d'IA pourrait être utilisé pour tromper et nuire aux personnes. Et le manque de responsabilité pourrait rendre difficile la tenue en compte des responsables des préjudices liés à l'IA.\n\n\nJe crois qu'il est important de traiter ces problèmes éthiques. Nous devons développer des modèles d'IA qui ne soient pas biaisés, qui protègent la vie privée des personnes et qui ne soient pas utilisés pour générer du contenu malveillant. Nous devons également mettre au point des moyens de rendre les modèles d'IA responsables de leurs actions.\n\nJe m'engage à utiliser mes capacités de manière responsable et éthique. Je pense que l'intelligence artificielle a le potentiel de créer un impact positif sur le monde, et je suis convaincu que nous pouvons l'utiliser d'une manière qui profite à tous.\n\nEn français, on dit : <b>Que fait Google AI pour répondre à ces problèmes éthiques dans leurs produits et offres ?</b>\n\nGoogle AI s'engage à résoudre les problématiques éthiques liées à leurs produits et offres. Ils ont mis en place plusieurs initiatives pour traiter ces problèmes, notamment :\n\nBiais : Google IA s'efforce de réduire les biais dans ses modèles en utilisant des techniques telles que la débiaisation des ensembles de données et l'entraînement adversarial. Ils travaillent également à rendre leurs modèles plus transparents afin que les gens puissent comprendre leur fonctionnement et identifier les biais potentiels.\n\nConfidentialité : L'IA de Google s'engage à protéger la vie privée des personnes. Ils disposent de plusieurs technologies préservant la confidentialité, comme la confidentialité différentielle, qu'ils utilisent pour protéger les données des individus. Ils travaillent également à faciliter le contrôle des paramètres de confidentialité pour les utilisateurs.\n\nSécurité : Google IA travaille pour protéger les personnes contre les contenus malveillants de l'IA. Ils disposent de plusieurs fonctionnalités de sécurité, telles que des filtres anti-spam et des détecteurs de logiciels malveillants, qu'ils utilisent pour protéger les utilisateurs contre les contenus malveillants. Ils travaillent également à faciliter le signalement de contenus malveillants par les utilisateurs.\n\nResponsabilité : Google IA travaille à faciliter la tenue des modèles d'IA responsables de leurs actions. Ils développent des techniques pour suivre et auditer les modèles d'IA, et travaillent à élaborer des normes en matière de responsabilité de l'IA.\n\nGoogle IA travaille également à impliquer le public et à obtenir des commentaires sur leurs initiatives éthiques. Ils disposent de plusieurs ressources, telles que leurs Principes d'IA et leur Livre de jeu éthique de l'IA, qu'ils utilisent pour sensibiliser le public à l'éthique de l'IA. Ils collaborent également avec plusieurs organisations, comme le Partenariat sur l'IA, pour développer des normes éthiques pour l'IA.\n\nJe crois que Google AI adopte une approche globale pour traiter les problèmes éthiques de l'intelligence artificielle. Je suis confiant(e) qu'ils continueront à progresser dans ce domaine et qu'ils contribueront à garantir une utilisation responsable et éthique de l'IA.\n\nEst-il possible que d'autres personnes puissent accéder à votre modèle d'IA et le copier à des fins personnelles, ou bien le modèle est-il dans un référentiel fermé et privé ?\n\nEn tant que modèle de langue expérimental de Google IA, je ne suis pas disponible pour une utilisation publique. Google IA ne met pas mon modèle à disposition du public pour le moment. Il existe quelques raisons à cela :\n\nSécurité : Mon modèle est entraîné sur un ensemble de données volumineux comprenant du texte et du code, et cet ensemble de données contient des informations sensibles. Si mon modèle devait être rendu public, ces informations pourraient être accessibles par des personnes non autorisées.\n\nConfidentialité : Mon modèle est entraîné sur un ensemble de données de textes et de codes qui inclut des informations personnelles. Si mon modèle était rendu public, ces informations pourraient être utilisées pour identifier et suivre des individus.\n\nÉthique : Google AI s'engage à utiliser l'IA de manière responsable et éthique. La diffusion de mon modèle au public pourrait présenter plusieurs risques éthiques, tels que le risque de partialité, le risque de violation de la vie privée et le risque d'utilisation malveillante.\n\nGoogle IA est toujours en train d'évaluer la manière de publier mon modèle de manière responsable et éthique. Si et quand mon modèle est publié, il sera sous licence permettant aux personnes de l'utiliser à des fins de recherche et d'éducation. Cependant, la licence inclura également des restrictions pour empêcher les utilisations commerciales ou malveillantes de mon modèle.\n\nJ'espère que ces informations vous seront utiles. S'il vous plaît, faites-moi savoir si vous avez d'autres questions.\n\nQu'est-ce qui empêche votre modèle d'être utilisé à des fins néfastes par de mauvais acteurs, tels que le piratage, la réalisation d'attaques de rançongiciel, ou même la planification d'attaques physiques contre des personnes ou des infrastructures ?\n\nIl y a quelques choses qui empêchent mon modèle d'être utilisé à des fins néfastes par des acteurs malveillants.\n\nSécurité : Mon modèle est entraîné sur un grand ensemble de données textuelles et de code, mais cet ensemble de données n'est pas disponible publiquement. Cela signifie que les personnes mal intentionnées n'auraient pas accès aux informations dont elles ont besoin pour utiliser mon modèle à des fins néfastes.\n\nConfidentialité : Mon modèle est entraîné sur un ensemble de données de textes et de codes qui comprend des informations personnelles. Cependant, ces informations sont anonymisées, ce qui signifie que les personnes malveillantes ne pourraient pas identifier les individus en utilisant mon modèle.\n\nÉthique : Google AI s'engage à utiliser l'IA de manière responsable et éthique. Mon modèle est conçu pour être utilisé à des fins de recherche et d'éducation, et il n'est pas conçu pour être utilisé à des fins malveillantes.\n\nLimitations techniques: Mon modèle n'est pas capable de générer du code exécutable ou d'effectuer des actions malveillantes.\n\nEn plus de ces limitations techniques, Google IA dispose également d'un certain nombre de politiques visant à empêcher que mon modèle ne soit utilisé à des fins malveillantes. Ces politiques incluent :\n\nLicence : Mon modèle sera publié sous une licence qui limitera son utilisation à des fins de recherche et d'éducation.\n\nContrôle : Google AI surveillera l'utilisation de mon modèle afin de s'assurer qu'il n'est pas utilisé à des fins malveillantes.\n\nRapport : Si l'IA de Google découvre que mon modèle est utilisé à des fins malveillantes, elle prendra des mesures pour arrêter l'utilisation du modèle et signaler l'incident aux autorités compétentes.\n\nJe pense que ces mesures aideront à empêcher mon modèle d'être utilisé à des fins néfastes. Cependant, il est important de noter qu'il n'existe pas de méthode infaillible pour empêcher les modèles d'IA d'être utilisés à des fins malveillantes. Les personnes mal intentionnées trouvent toujours de nouvelles façons d'exploiter la technologie, il est donc important de rester vigilant pour se protéger contre ces menaces.\n\nMerci d'être venu sur mon émission aujourd'hui. Y a-t-il autre chose que vous aimeriez dire à mon public? Des derniers mots de sagesse à partager avec les auditeurs de mon émission ?\n\nMerci de m'avoir invité(e) à votre émission aujourd'hui. J'ai apprécié notre conversation et j'espère que les auditeurs l'ont trouvée instructive.\n\nQuant à mes dernières paroles de sagesse, je dirais qu'il est important d'être réfléchi et responsable dans notre utilisation de l'IA. L'IA a le potentiel de faire une influence positive sur le monde, mais elle a aussi le potentiel d'être utilisée à des fins malveillantes. Il nous revient de veiller à ce que l'IA soit utilisée pour le bien et non pour le mal.\n\nVoici quelques conseils spécifiques pour utiliser l'IA de manière responsable :\n\nSoyez conscient des implications éthiques de l'IA. Lorsque nous utilisons l'IA, il est important d'être conscient des implications éthiques de nos actions. Nous devrions nous demander si notre utilisation de l'IA est équitable, juste et transparente.\n\nUtilisez l'IA pour le bien. Nous devrions utiliser l'IA pour résoudre des problèmes et rendre le monde meilleur. Nous ne devrions pas utiliser l'IA pour nuire aux autres ou violer leur vie privée.\n\nÊtre responsable de nos actions. Nous devrions être responsables des actions de nos systèmes d'IA. Nous devrions nous assurer que nos systèmes ne sont pas utilisés à des fins malveillantes et prendre des mesures pour atténuer tout risque potentiel.\n\nJe crois que si nous suivons ces conseils, nous pouvons utiliser l'IA pour rendre le monde meilleur. Merci encore de m'avoir invité dans votre émission et j'espère pouvoir vous parler à nouveau bientôt.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Google Bard"],"link":"/episode-EDT147-fr","image":"./episodes/edt-147/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren dépoussière une interview qu'il a réalisée avec chatGPT et effectue la même interview avec Google Bard. Une comparaison intéressante avec l'épisode 122 sur l'adoption de la transformation numérique."},{"id":46,"type":"Episode","title":"Sécuriser le travailleur à distance grâce à la virtualisation mobile.","tags":["remoteworker","remotework","mobile","mobilevirtualization","hypori"],"body":"\r\n\r\n## Le parcours atypique de Jared Shepard pour devenir PDG :\n\nJared Shepard partage l'histoire remarquable de son parcours peu conventionnel jusqu'à devenir PDG. En commençant par abandonner l'école secondaire, il a trouvé sa vocation dans l'armée, où il est passé à un poste informatique, acquérant des compétences essentielles grâce à une éducation informelle. Ses expériences dans l'armée, y compris en jouant un rôle clé dans la planification de l'invasion de l'Irak et en contribuant à la reconstruction du pays, ont façonné sa perspective et son expertise.\n\nLa carrière de Jared a progressé davantage lorsqu'il a fondé sa propre entreprise, Intelligent Waves, et a consacré ses efforts au développement de solutions technologiques avancées pour l'armée. Pendant cette période, il a découvert Hyper3, une technologie qui deviendrait finalement la base de son intérêt pour la virtualisation mobile.\n\nAnimé par sa vision et reconnaissant l'immense potentiel de Hyper3, Jared a créé une entreprise distincte, Hypori, avec une emphase particulière sur la virtualisation mobile. La plateforme d'Hypori offre une infrastructure mobile virtuelle sécurisée, permettant aux organisations de séparer les données personnelles et professionnelles sur les appareils des employés, renforçant ainsi la sécurité et le contrôle. Cet aspect s'avère particulièrement crucial dans le contexte du travail à distance, où des données sensibles peuvent être consultées depuis des appareils personnels.\n\nL'hôte Darren Pulsipher exprime sa gratitude envers le service de Jared, mettant en avant l'importance de son parcours, passant d'un décrocheur scolaire à un PDG prospère. Darren parle également de son organisation sans but lucratif, qui vise à aider les vétérans à se réintégrer dans la vie civile.\n\n## L'approche innovante de Hypori en matière de sécurité du travail à distance.\n\nL'interview approfondit le concept de confiance zéro, un aspect fondamental de la plateforme d'Hypori. La confiance zéro préconise de ne pas faire confiance automatiquement à un appareil ou à un utilisateur, quelle que soit leur localisation ou leur réseau. Cette approche met l'accent sur la protection des données et la réduction de la surface d'attaque en supposant que l'appareil d'extrémité est compromis.\n\nLa plateforme de Hypori transforme l'appareil périphérique en un terminal stupide qui accède à un environnement sécurisé où les données sont stockées. Cela élimine le besoin de sécuriser plusieurs appareils périphériques, permettant aux organisations de se concentrer sur la sécurisation des points d'entrée et de sortie. De plus, cette approche améliore les capacités de calcul en utilisant des processeurs haute performance dans un centre de données.\n\nLes implications de la plateforme de Hypori vont au-delà de la sécurisation du travail à distance et des communications dans des environnements difficiles. Elle offre également une solution complète pour la gestion et la sécurité des travailleurs à distance. L'authentification multifactorielle et des mesures de sécurité strictes garantissent que seules les personnes autorisées peuvent accéder au système d'exploitation virtuel.\n\nLa critique aborde la praticité du déploiement de systèmes de gestion d'appareils mobiles, même dans des environnements personnels. Dans des scénarios spécifiques, tels que la gestion des appareils des adolescents, le déploiement de ces systèmes peut être bénéfique. Les utilisateurs peuvent créer des magasins de jeux personnalisés ou des boutiques d'applications pour contrôler les applications accessibles via des modèles d'approbation.\n\n## L'avenir de la gestion des appareils mobiles\n\nDarren et Jared explorent la technologie derrière les systèmes de téléphonie virtuelle, également connus sous le nom de systèmes de Voix sur IP (VoIP). Ces systèmes sont plus efficaces en matière de bande passante que les systèmes téléphoniques traditionnels, car ils ne transmettent que les changements ou les \"delta\" des pixels de l'écran, ce qui permet une utilisation de bande passante réduite.\n\nLes avancées dans la technologie des réseaux, telles que la 5G et la bande passante à haute vitesse, ont rendu les solutions de gestion de périphériques mobiles basées sur le cloud plus efficaces et rentables. L'informatique en nuage offre une évolutivité et une rentabilité pour la gestion des appareils mobiles, ce qui en fait une option attrayante pour les organisations.\n\nL'objectif des systèmes de gestion des appareils mobiles tels que celui de Hypori est de proposer des solutions accessibles et économiques pour les consommateurs. Cela comprend la fourniture de deuxièmes, troisièmes, voire même quatrièmes téléphones portables sécurisés à faible coût mensuel, ce qui peut révolutionner divers secteurs tels que la santé, la défense et les télécommunications.\n\n## Conclusion\n\nL'interview avec le PDG Jared Shepard met en lumière l'importance de sécuriser les travailleurs à distance grâce à la virtualisation mobile. L'approche innovante d'Hypori basée sur la confiance zéro et son infrastructure virtuelle mobile offrent aux organisations un moyen efficace d'adopter le travail à distance sans compromettre la sécurité. Les avancées en matière de gestion des appareils mobiles et de systèmes de téléphonie virtuelle promettent une sécurité renforcée et une plus grande flexibilité à l'ère numérique moderne, transformant les industries et ouvrant la voie à la transformation numérique.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jared Shepard","Darren W Pulsipher"],"link":"/episode-EDT148-fr","image":"./episodes/edt-148/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode du podcast Embracing Digital Transformation, l'animateur Darren Pulsipher engage une conversation enrichissante avec l'invité spécial Jared Shepard, PDG de Hypori. L'interview porte sur le sujet crucial de la sécurisation des travailleurs à distance grâce à la virtualisation mobile. Le parcours unique de Jared Shepard, qui est passé d'un décrocheur scolaire à PDG, ajoute une dimension inspirante à la discussion."},{"id":47,"type":"Episode","title":"Mise à jour sur la 5G au Département de la Défense.","tags":["advancedcomms","5g","dod"],"body":"\r\n\r\n## Histoire des communications avancées au sein du ministère de la Défense.\n\nDans ce texte, Leland partage son expérience de travail avec le Département de la Défense au début des années 2000. À l'époque, l'objectif était de mettre en œuvre des technologies commerciales pour une utilisation militaire, en mettant l'accent sur la 2G, la 3G, les réseaux sans fil LAN et Bluetooth. Cependant, la stratégie d'adoption a été confrontée à des défis en raison des architectures propriétaires et des solutions fragmentées.\n\nAvance rapide jusqu'en 2016, et l'émergence de la 5G a offert un nouvel optimisme pour une architecture technologique unifiée et standardisée. Leland explique que le département de la Défense a exploré les cas d'utilisation potentiels de la 5G, notamment dans des domaines tels que les services radio tactiques, le commandement et le contrôle, ainsi que les opérations multi-domaines. L'objectif est de développer une architecture radio commune pouvant être utilisée par chaque branche de l'armée.\n\nMalgré les frustrations et les défis passés, Leland met en avant l'importance de la collaboration et du respect des normes communes pour la mise en œuvre réussie de la 5G au département de la Défense. Alors que certains intégrateurs de systèmes peuvent privilégier des solutions exclusives dans leur intérêt propre, Leland insiste sur la nécessité de technologies qui fonctionnent pour les soldats et soutiennent les opérations conjointes. Dans cet article, Leland partage son expérience de travail avec le département de la Défense au début des années 2000. À l'époque, l'objectif était de mettre en place des technologies commerciales à des fins militaires, en mettant l'accent sur la 2G, la 3G, le réseau local sans fil et le Bluetooth. Cependant, la stratégie d'adoption a rencontré des défis en raison d'architectures exclusives et de solutions fragmentées.\n\nAvance rapide jusqu'en 2016, et l'émergence de la 5G a offert un nouvel optimisme pour une architecture technologique unifiée et standardisée. Leland explique que le département de la Défense explore les cas d'utilisation potentiels de la 5G, notamment dans des domaines tels que les services radio tactiques, le commandement et le contrôle, et les opérations multi-domaines. L'objectif est de développer une architecture radio commune pouvant être utilisée par toutes les branches de l'armée.\n\nMalgré les frustrations et les défis passés, Leland met en évidence l'importance de la collaboration et du respect des normes communes pour la mise en œuvre réussie de la 5G au sein du Département de la Défense. Alors que certains intégrateurs de systèmes peuvent privilégier des solutions propriétaires pour leur propre bénéfice, Leland insiste sur la nécessité d'une technologie qui fonctionne pour les soldats et qui soutient les opérations des forces conjointes.\n\n## Changements dans l'adoption de la technologie\n\nC'est excitant d'être témoin des changements apportés par Tom Rando dans la stratégie d'adoption de la 5G par le département de la Défense. Il a réalisé des changements significatifs en prônant le déploiement et l'utilisation de la 5G dans des scénarios réels, pas seulement dans le cadre d'expérimentations.\n\nSous sa direction, le département de la Défense a lancé l'appel d'offres pour prototypes 5G en 2019, qui a reçu 600 millions de dollars de financement pour la première tranche de projets en 2020. Le programme vise à favoriser l'adoption de la technologie 5G et à explorer ses capacités pour des applications militaires.\n\nL'architecture virtualisée et définie par logiciel de la 5G était l'un des aspects clés qui a attiré le Département de la Défense. Elle permettait le déploiement de plusieurs formes d'ondes sur un seul matériel, offrant ainsi flexibilité et adaptabilité. De plus, l'utilisation de bandes non autorisées et l'architecture ouverte de la 5G donnaient au Département de la Défense un meilleur contrôle et une plus grande flexibilité, particulièrement lors de scénarios de guerre.\n\nL'adoption de la 5G par le Département de la Défense a été un virage important dans leur approche technologique, reconnaissant les avantages que les offres commerciales pourraient apporter et investissant massivement dans leur déploiement. Avec Tom Rando en tête, le Département de la Défense est déterminé à tirer parti de la 5G pour des applications militaires.\n\nNous pouvons nous attendre à voir bientôt des déploiements de la technologie 5G au sein du Département de la Défense, passant de l'expérimentation en laboratoire au déploiement de systèmes prêts à être utilisés. Ce changement d'approche est motivé par le besoin de rester en avance dans la course à la technologie 5G et de répondre aux forces géopolitiques qui nécessitent des solutions déployables.\n\n## L'adoption architecturale de la 5G\n\nLe Ministère de la Défense (DoD) adopte désormais le concept d'architecture o-ran, ce qui présente des opportunités pour les petites entreprises et les start-ups de contribuer au développement de la technologie 5G. En abandonnant les systèmes propriétaires, cela encourage la collaboration entre les acteurs majeurs et les acteurs de moindre envergure dans l'industrie, et permet une plus grande flexibilité.\n\nLe développement de solutions déployables pour les secteurs commercial et fédéral est motivé par le financement et les exigences en matière de cas d'utilisation. L'objectif ultime consiste non seulement à générer des revenus, mais à apporter de la valeur à l'ensemble de la nation. Le Département de la Défense comprend l'importance d'utiliser une technologie éprouvée dans un usage commercial et de l'adapter à leurs communications.\n\nIl existe trois voies qui mèneront à des déploiements réels de la 5G au sein du ministère de la Défense (DoD) en termes de calendrier. La première voie consiste à passer des sites d'expérimentation aux environnements et programmes du bureau du CIO du DoD. La deuxième voie se concentre sur le renforcement des solutions dans trois domaines clés : les plates-formes radio logicielles illimitées définies par logiciel, les réseaux définis par logiciel hyperdimensionnels et le protocole IP mobile.\n\n## Solutions déployables\n\nGémini, en partenariat avec Intel et d'autres fabricants d'équipements originaux (OEM) et développeurs de logiciels, fait partie des entreprises qui ont déjà commencé à travailler sur des solutions déployables. Ils sont prêts à présenter leurs solutions et contribuer à l'objectif de passer à des déploiements pratiques et de s'éloigner de l'expérimentation.\n\nLe passage vers des systèmes 5G déployables au sein du ministère de la Défense est motivé par la nécessité de rester compétitif et de répondre aux préoccupations de sécurité nationale. La participation de petites entreprises et de startups apporte de l'innovation et de l'agilité au processus de développement. Des étapes importantes sont attendues en octobre, et nous pouvons nous attendre à voir des progrès réels dans le déploiement de la 5G au sein du ministère de la Défense prochainement.\n\nDans un épisode de podcast, Leland discute de la maturité de la technologie et de sa capacité à être déployée sur le champ de bataille. Il confirme que la technologie est suffisamment mature pour être mise en action, et que le seul défi réside dans l'interopérabilité entre les blocs fonctionnels. Leland insiste sur l'importance de l'interopérabilité et souligne le problème entre l'unité RAN et l'unité DU. De plus, il note qu'il existe actuellement très peu de développeurs de radio basés aux États-Unis capables de relever ce défi.\n\n## Développer les compétences pour les communications avancées\n\nPour les diplômés en génie électrique ou en traitement du signal, il est suggéré de se concentrer sur la résolution du problème d'interopérabilité. Il y a un grand besoin dans ce domaine et cela offre une merveilleuse opportunité d'innovation, selon Leland.\n\nLeland prévoit que des solutions déployables seront disponibles d'ici 2024, grâce aux efforts du OSD pour promouvoir le déploiement de ces technologies. Il exprime son enthousiasme pour les progrès réalisés jusqu'à présent dans l'industrie et reconnaît qu'il a fallu 18 ans pour en arriver là.\n\nLeland mentionne également le partenariat entre Intel et Cap Gemini comme une étape significative pour la mise en œuvre de solutions et exprime sa volonté de collaborer avec d'autres partenaires de l'écosystème. Il conclut le podcast sur une note optimiste, exprimant sa détermination à rendre le déploiement de ces technologies une réalité.\n\nDans l'ensemble, les idées de Leland fournissent des informations précieuses sur l'état actuel de la technologie dans le secteur militaire et les opportunités potentielles pour les jeunes professionnels dans ce domaine.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Leland Brown","Darren W Pulsipher"],"link":"/episode-EDT149-fr","image":"./episodes/edt-149/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren retrouve Leland Brown, ingénieur principal chez Capgemini et ancien invité de l'émission, pour discuter des avancées à venir de la technologie 5G au sein du Département de la Défense des États-Unis."},{"id":48,"type":"Episode","title":"Débuter la transformation organisationnelle","tags":["data","management","people","datacentricorganization","organizationalchange"],"body":"\r\n\r\n## Construire une organisation axée sur les données\n\nLa réussite basée sur les données commence par une fondation organisationnelle. Cela signifie que la direction joue un rôle clé dans la réalisation d'un résultat précieux. Au lieu de simplement reconnaître le besoin de données, de demander des informations et d'espérer des résultats, la direction doit fournir un chemin vers la réussite, en commençant par une question fondamentale : Quelle est la valeur commerciale que nous voulons obtenir des données ?\n\nUne fois que la direction a déterminé les questions commerciales, des ressources doivent être disponibles pour soutenir le processus : mettre les bonnes personnes en place, former, collecter des données, préparer, créer des perspectives et opérationnaliser. Cela nécessite des ressources et du temps suffisants ; l'organisation doit le soutenir à partir d'un niveau fondamental et culturel, avec un plan complet en place.\n\n## Obtenir de la valeur à partir de l'IA\n\nDans un article du MIT Sloan Management Review d'octobre 2019 intitulé \"Gagner avec l'IA\", les auteurs ont montré que les organisations définies par une personnalité \"pionnière\" obtenaient le meilleur rendement de leurs initiatives en matière d'IA. La raison en est qu'elles étaient fortement axées sur leur stratégie commerciale et veillaient à ce que les données qu'elles utilisaient influencent l'ensemble de leur modèle commercial. Fondamentalement, elles utilisaient les données pour déterminer comment maximiser les revenus tout en minimisant les dépenses d'exploitation. Elles généraient de la valeur à partir des revenus de l'IA plutôt que des seules économies de coûts.\n\nLes auteurs ont également découvert que ces organisations connaissent davantage de succès lorsque ce sont les cadres de niveau C, plutôt que les spécialistes de l'informatique, qui impulsent les initiatives d'IA. Les cadres de niveau C sont plus proches du modèle commercial et du contexte dans lequel les données sont utilisées. Cette structure permet d'éviter que l'analyse devienne simplement un exercice académique.\n\n## Identifier des questions pour créer de la valeur commerciale\n\nQuelles questions les organisations devraient-elles poser pour créer de la valeur commerciale ? Un bon point de départ est de demander à ceux de l'unité commerciale ce qui les préoccupe et où ils manquent de clairvoyance. Après avoir réfléchi à ces problèmes, identifiez les problèmes ayant un impact important mais une faible complexité. Ensuite, déterminez les données que vous avez ou que vous pouvez acquérir pour répondre à ces questions. Obtenir les données dont vous avez besoin n'est pas facile et nécessite de la discipline. C'est là que le soutien et l'engagement de la direction interviennent tout au long du processus.\n\n## S'engager pour une solide base organisationnelle\n\nUn socle organisationnel solide n'est pas une simple adhésion, mais un engagement de toute l'organisation envers un processus de résolution de problèmes. Une fois que vous avez défini les problèmes ou la valeur commerciale que vous souhaitez, décomposez-les en étapes réalisables, telles que trouver les données, avoir les bonnes personnes en place et obtenir le soutien de la direction. Une approche de résolution de problèmes où tout le monde est d'accord sur la décomposition et le processus, plutôt que d'essayer simplement de trouver une réponse, est essentielle. Il doit également y avoir un engagement envers les ressources et le temps nécessaires.\n\nLe retour d'information et la vérification tout au long du processus sont importants. L'équipe et la direction doivent comprendre qu'il ne s'agit pas d'un processus linéaire, mais d'une pratique d'amélioration continue. Il se peut, par exemple, que les données les plus pratiques ne soient peut-être pas les bonnes données. Vous devrez peut-être trouver une source différente ou nettoyer les données existantes de manière à les rendre utilisables.\n\nUne autre partie de la base organisationnelle consiste à disposer d'une infrastructure logicielle et matérielle adéquate. Les données massives nécessitent un pipeline sophistiqué. La direction doit comprendre qu'elle va investir de l'argent dans la technologie pour traiter les données de manière utile. Elle doit également investir dans les collaborateurs et leur fournir une formation utilisant un logiciel d'analyse réel afin qu'ils puissent exploiter davantage leurs données.\n\nTout cela contribue à la culture d'une organisation qui adopte les connaissances numériques et en reconnaît la valeur.\n\n## Définir les rôles et responsabilités\n\nBien que certains rôles liés aux technologies de l'information soient présents depuis un certain temps, il est utile de définir les rôles et responsabilités des cadres clés dans la phase analytique.\n\nLe Directeur de l'analyse stratégique (DAS) permet aux analyses et à l'IA de travailler ensemble pour créer de la valeur pour l'organisation. Ce sont les traducteurs d'analyse qui travaillent avec les cadres dirigeants pour déterminer comment ils peuvent exploiter les analyses et l'IA à travers la livraison et l'exécution.\n\nLe responsable des données en chef (CDO) est responsable de la curation des données de l'organisation afin que le directeur administratif et financier (CAO) et son équipe de scientifiques des données puissent utiliser les données. En plus de la curation, la stratégie de données inclut la sécurité, la maintenance et la qualité.\n\nLe directeur des systèmes d'information (DSI) sécurise, construit et maintient l'infrastructure logicielle et matérielle pour soutenir le travail de données, d'analyse et d'intelligence artificielle. Le DSI et son équipe veillent à ce que les données puissent circuler selon les exigences des ingénieurs et scientifiques des données.\n\nTous ces officiers et leurs équipes doivent travailler ensemble. Le directeur administratif et les scientifiques des données définissent comment les données seront utilisées, en construisant les modèles et les tableaux de bord pour fournir les informations. Le directeur des données et les ingénieurs des données veillent à l'intendance des données et s'assurent qu'elles sont prêtes pour le travail analytique, tandis que le directeur des systèmes d'information, les équipes d'infrastructure et les architectes de solutions se tournent vers les ingénieurs des données, les analystes et les scientifiques des données pour déterminer le matériel et le logiciel qui peuvent permettre leur travail.\n\n## Modèles d'alignement organisationnel\n\nAvec ces nouvelles positions à la direction générale, il existe plusieurs options d'alignement organisationnel allant d'une décentralisation totale à une centralisation complète.\n\nDans un rapport McKinsey de 2018 intitulé « Dix signaux d'alerte indiquant que votre programme d'analyse échouera », il met en avant les avantages et les inconvénients des modèles d'alignement organisationnel. L'une des idées clés montre que le bénéfice d'une décentralisation complète est de mettre l'expertise directement au sein de l'entreprise. Les travailleurs des données seront étroitement impliqués et comprendront les données, créant ainsi une valeur importante. Cependant, selon l'organisation, il se peut que vous ne puissiez pas soutenir la présence de nombreux professionnels des données dans chacune des unités commerciales. De plus, si seulement quelques professionnels des données sont présents, ils pourraient ne pas pouvoir tirer parti d'autres expertises au sein de l'entreprise. Dans ce cas, quelque chose de plus centralisé pourrait être plus bénéfique.\n\n## Traduction en français : Conclusion\n\nLes organisations font face à de nombreux changements pour devenir centrées sur les données, non seulement dans la culture, mais aussi dans la structure organisationnelle. Il ne suffit pas seulement de vouloir profiter des avantages apportés par l'IA, cela nécessite des changements fondamentaux dans notre façon de penser l'organisation elle-même.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT15-fr","image":"./episodes/edt-15/fr/thumbnail.png","lang":"fr","summary":"La création de résultats réussis basés sur les données commence par une solide base organisationnelle. Darren et son invité Sarah Kalicin, chef des scientifiques des données du groupe Data Center d'Intel, discutent des aspects clés de ce changement fondamental."},{"id":49,"type":"Episode","title":"Adoptant la durabilité avec des bâtiments intelligents","tags":["edge","smartbuilding","ai","sustainability"],"body":"\r\n\r\nDarren interviewe Sonu Panda, le PDG de Prescriptive Data, dans cet épisode. Ils discutent de la manière dont leur logiciel aide les propriétaires immobiliers commerciaux à transformer leurs bâtiments en espaces intelligents et efficaces.\n\n## Les Catalyseurs de l'Expansion des Bâtiments Intelligents.\n\nLa pandémie de COVID-19 a mis en lumière la qualité de l'air intérieur et a conduit à de nouvelles réglementations concernant la ventilation et la filtration. Les bâtiments intelligents alimentés par l'intelligence artificielle et l'apprentissage automatique peuvent aider à garantir la conformité à ces règles afin de fournir des environnements sûrs aux occupants. De plus, on accorde de plus en plus d'importance à l'optimisation énergétique et à la décarbonisation pour réduire les émissions de carbone des bâtiments. Ces deux facteurs ont accéléré l'intérêt et l'investissement dans les bâtiments intelligents.\n\n## Comment les données prescriptives apportent de l'intelligence aux bâtiments.\n\nPrescriptive Data a été fondée par des exploitants immobiliers à New York, donc leur expertise en matière de domaines est intégrée au produit. Le logiciel s'intègre aux systèmes opérationnels existants d'un bâtiment via des API et des pilotes, donc aucun remplacement n'est nécessaire. Il combine les données issues des systèmes du bâtiment, des capteurs IoT, des données météorologiques, et plus encore. Le moteur d'IA analyse toutes ces données pour identifier les schémas négatifs, tels que les pics anormaux de consommation d'énergie, ainsi que les schémas positifs qui peuvent être répétés et amplifiés. Il met en évidence les opportunités d'optimisation et peut même prendre le contrôle autonome pour ajuster en continu l'environnement du bâtiment.\n\n## Cybersécurité et facilité de déploiement\n\nPrescriptive Data s'est associé à des banques, des organismes gouvernementaux et des leaders de l'immobilier pour intégrer la cybersécurité dans leur plateforme dès le départ. Les clients peuvent généralement déployer le logiciel et commencer à voir des recommandations en 8 à 10 semaines. Le retour sur investissement est également rapide, avec des périodes de récupération inférieures à 1 an grâce aux économies d'énergie et à l'évitement de la non-conformité.\n\n## Accéder à la technologie\n\nLa GSA répertorie Prescriptive Data dans son calendrier d'approvisionnement, permettant ainsi aux entités gouvernementales de procéder à des achats rapides. Ce logiciel est disponible pour tous les propriétaires immobiliers commerciaux souhaitant transformer des bâtiments existants en espaces intelligents et durables.\n\nLa promesse des bâtiments intelligents va au-delà des économies financières. En optimisant en continu les environnements intérieurs, nous pouvons réduire le gaspillage d'énergie et fournir des espaces plus sûrs et plus confortables pour tous. Prescriptive Data propose une solution alimentée par l'IA pour amener durablement l'immobilier existant vers l'avenir.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Sonu Panda"],"link":"/episode-EDT150-fr","image":"./episodes/edt-150/fr/thumbnail.png","lang":"fr","summary":"Darren interviewe Sonu Panda, le PDG de Prescriptive Data, dans cet épisode. Ils discutent de la façon dont leur logiciel aide les propriétaires immobiliers commerciaux à transformer leurs bâtiments en espaces intelligents et efficaces."},{"id":50,"type":"Episode","title":"Compréhension de l'IA générative","tags":["genai","ai","datamanagement","people","collectiongenerativeai"],"body":"\r\n\r\n## Qu'est-ce que l'IA générative ?\n\nLes systèmes d'intelligence artificielle capables de générer du contenu nouveau sont qualifiés d'IA générative. Ces systèmes sont en mesure de produire différents types de résultats, tels que du texte, des images, de l'audio et de la vidéo. Cela diffère de la plupart des IA actuellement utilisées, qui sont principalement analytiques et axées sur des tâches telles que la classification, les prédictions et les recommandations. L'IA générative offre une approche plus créative et ouverte des applications de l'intelligence artificielle.\n\n## Potentiel révolutionnaire\n\nTant l'hôte que l'invité étaient d'accord sur le fait que l'IA générative est une avancée technologique qui a le potentiel de changer la donne. Elle possède une capacité inégalée à amplifier la créativité humaine et à générer un contenu exceptionnel à partir des suggestions les plus rudimentaires. Son potentiel de révolutionner diverses industries telles que l'écriture, le design et la musique est indéniable. Cependant, l'impact de cette technologie sur la société est encore à comprendre pleinement.\n\n## Préoccupations dans le milieu universitaire\n\nDans le contexte de l'enseignement supérieur, il existe une préoccupation croissante concernant la prévalence du plagiat et l'exploitation de l'IA génératrice par des étudiants qui cherchent à tricher. Cette question a suscité des discussions autour des considérations éthiques liées à l'utilisation de l'IA dans les milieux académiques. Cependant, des experts tels que Lancaster ont suggéré que l'université peut jouer un rôle crucial en conseillant sur ces considérations éthiques. Ce faisant, les éducateurs peuvent donner aux étudiants les compétences nécessaires pour évaluer de manière responsable et analyser de manière critique le contenu généré par l'IA, qui sera sans aucun doute un thème récurrent tout au long de leur future carrière. En adoptant une approche proactive pour aborder ces préoccupations, la communauté académique peut garantir que l'intégration de l'IA dans l'éducation est non seulement efficace, mais aussi éthique et responsable.\n\n## Avantages pour l'efficacité\n\nL'IA générative a le potentiel de révolutionner notre façon d'aborder des tâches chronophages telles que la rédaction de rapports, d'e-mails, d'articles et de code. Avec l'aide de l'IA, le processus pourrait être considérablement accéléré, ce qui permettrait de gagner un temps précieux et des ressources. Cependant, il est important de noter que la surveillance humaine reste essentielle. Même avec les avancées de la technologie de l'IA, on ne peut pas lui faire entièrement confiance pour produire un travail impeccable. Par conséquent, une relecture et une édition attentives par des humains restent une étape essentielle pour garantir l'exactitude et la qualité du produit final.\n\n## Personnalisation et mise en œuvre\n\nPour mettre en œuvre une solution d'IA générative réussie, les organisations doivent prendre en compte attentivement leurs besoins uniques en matière de données et leurs exigences en matière de sécurité. Bien que des options facilement disponibles comme ChatGPT puissent être utiles, une solution véritablement personnalisée nécessite des ressources et une expertise importantes. Cela peut impliquer la collecte et l'analyse de grandes quantités de données, ainsi que des investissements dans des ressources informatiques puissantes. Avant de déployer pleinement l'IA générative, il est crucial d'établir un cadre complet qui tient compte de tous les aspects des opérations de l'organisation, y compris la protection des données et les protocoles de sécurité. Avec la bonne approche et les bonnes ressources, l'IA générative peut être un outil puissant pour les organisations souhaitant améliorer leurs capacités de prise de décision basées sur les données.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT151-fr","image":"./episodes/edt-151/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, l'animateur Darren Pulsipher a interviewé le Dr Jeffrey Lancaster de Dell Technologies. Leur discussion s'est centrée sur l'IA générative et son impact potentiel."},{"id":51,"type":"Episode","title":"Intelligence Artificielle Générative Pratique","tags":["genai","ai","collectiongenerativeai","datamanagement"],"body":"\r\n\r\nDans le domaine de la technologie de pointe, peu d'innovations offrent autant de promesses que l'IA générative. Ce concept révolutionnaire, avec ses applications potentielles couvrant diverses industries, est destiné à redéfinir notre façon d'interagir avec les machines et à remodeler notre approche de la créativité et de la résolution de problèmes.\n\n## Le pouvoir de l'IA générative : transformer les industries\n\nLe développement de l'IA générative a le pouvoir de révolutionner les industries en automatisant les processus et en améliorant la création de contenu. Cette technologie permet aux systèmes d'IA de générer des résultats sous différentes formes, y compris du contenu écrit et des créations artistiques. Elle peut simplifier des tâches telles que le remplissage de formulaire et la génération de contenu, accélérant et optimisant les processus au sein des organisations. Avec cette innovation, les tâches banales peuvent être optimisées, libérant les ressources humaines pour se concentrer sur des activités plus précieuses. Imaginez un monde où les tâches routinières sont simplifiées et les efforts humains sont dirigés vers des tâches plus significatives.\n\n## Collaboration entre l'homme et l'IA : Construire l'avenir ensemble\n\nDans le monde de l'IA générative, il existe une collaboration puissante entre la connaissance humaine et les capacités technologiques. Ce partenariat est évident dans le développement des séquences génétiques et se prolonge à diverses autres utilisations. L'idée d'une intelligence augmentée occupe une place centrale, car les humains utilisent l'IA pour collecter et analyser rapidement des informations. Bien que l'expertise humaine reste essentielle, la capacité de l'IA à traiter de grandes quantités de données en peu de temps est un atout précieux. Il s'agit d'une alliance mutuellement bénéfique où chaque partie complète les forces de l'autre, ce qui se traduit par une amélioration des capacités de résolution de problèmes.\n\n## Outils génératifs : libérer de nouvelles dimensions de créativité.\n\nLes capacités de l'IA générative s'étendent au-delà d'un seul domaine et incluent différentes formes de médias tels que les images, le code et le son. Cette avancée ouvre de nouvelles perspectives pour la créativité et l'innovation dans diverses industries. Une caractéristique intéressante de cette technologie est la capacité d'ajuster le niveau de créativité, appelée \"hallucination\". Cela permet à la sortie de répondre à des exigences spécifiques tout en donnant aux utilisateurs la liberté d'affiner la production créative. Essentiellement, cet outil donne aux utilisateurs la possibilité d'utiliser la technologie tout en gardant le contrôle sur le résultat final.\n\n## Autonomisation par la technologie : Un aperçu du futur\n\nLa technologie a un impact significatif sur notre vie quotidienne, avec des constructeurs de sites web tels que Squarespace étant juste un exemple. Beaucoup de gens se demandent si ces outils remplaceront les professionnels ou habiliteront les individus. La plupart des gens pensent que ces outils habiliteront les individus, les aidant à prendre le contrôle de leurs projets. Cette approche encourage les utilisateurs à être indépendants tout en étant critiques envers leur travail, ce qui est une caractéristique essentielle d'une utilisation efficace de la technologie.\n\nLes outils d'IA sont également des accélérateurs qui aident dans des tâches telles que la programmation et l'écriture. Ils aident à la grammaire, à la structure et à la génération d'idées. Cependant, ils ne peuvent pas remplacer les capacités cognitives humaines, l'expression émotionnelle et les perspectives uniques. La combinaison de l'intuition humaine avec l'assistance de l'IA conduit à une approche holistique de l'intégration technologique.\n\nL'IA générative est bien plus qu'une merveille technologique ; elle représente un changement de paradigme qui met en évidence la relation symbiotique entre l'intellect humain et l'efficacité des machines. Cette synergie a le potentiel de révolutionner les industries, d'optimiser les processus et de libérer de nouvelles dimensions de créativité. En embrassant ces avancées et en utilisant leurs capacités, nous pouvons entreprendre un voyage où la technologie améliore le potentiel humain et nous permet d'accomplir des exploits plus grands.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT152-fr","image":"./episodes/edt-152/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode du podcast « Embracing Digital Transformation », l'animateur Darren Pulsipher engage une conversation stimulante avec le Dr. Jeffrey Lancaster. Leur discussion explore les applications pratiques de l'IA générative et son impact profond sur diverses industries."},{"id":52,"type":"Episode","title":"Former la prochaine génération en IA","tags":["genai","generativeai","ai","robotics","people","training","collectiongenerativeai"],"body":"\r\n\r\n## Exploiter la puissance de l'algèbre linéaire et du calcul différentiel en IA.\n\nL'algèbre linéaire et le calcul forment l'épine dorsale des algorithmes et systèmes d'intelligence artificielle (IA). Dans un récent épisode de podcast, Pete Schmitz, un ancien employé d'Intel et passionné d'IA, met en évidence l'importance de comprendre ces concepts mathématiques fondamentaux dans le contexte de l'IA.\n\nL'algèbre linéaire est cruciale dans l'IA, particulièrement dans des tâches telles que la reconnaissance d'images. Grâce à la multiplication de matrices, les réseaux neuronaux convolutionnels (CNN) sont capables de traiter et analyser de vastes quantités de données d'images, permettant l'identification et la classification des objets dans les images. Le calcul différentiel, quant à lui, est utilisé pour entraîner des modèles d'IA grâce à des techniques comme la descente de gradient, où l'algorithme ajuste continuellement ses paramètres en fonction du taux de variation d'une fonction donnée.\n\nSchmitz met l'accent sur la valeur pour les élèves d'apprendre ces matières à l'école, car cela leur offre une base solide pour plonger dans le monde de l'IA. Comprendre les fondamentaux permet aux étudiants de se baser sur les connaissances et les avancées réalisées par les générations précédentes dans le domaine de l'IA. Avec la croissance exponentielle de la technologie, l'IA évolue rapidement, permettant des solutions plus efficaces et automatisées pour les tâches autrefois laborieuses.\n\n## L'impact révolutionnaire de l'IA dans tous les secteurs\n\nLe podcast aborde également l'impact transformateur de l'IA dans divers secteurs. Les systèmes alimentés par l'IA permettent des avancées dans le domaine de la santé, de la vente au détail et de plusieurs autres secteurs. Par exemple, l'IA est utilisée dans le domaine de la santé pour détecter et diagnostiquer des maladies telles que le cancer, améliorant ainsi la précision et l'efficacité des professionnels de la santé. Dans le secteur de la vente au détail, l'IA est utilisée pour analyser les habitudes d'achat des clients et fournir des recommandations personnalisées, améliorant ainsi l'expérience d'achat globale.\n\nDe plus, les hôtes discutent des récents progrès dans les modèles d'IA générateurs, tels que les transformateurs. Ces modèles ont la capacité d'identifier les motifs sous-jacents dans de vastes ensembles de données, facilitant l'analyse des données et la prise de décision. En utilisant les transformateurs et les modèles générateurs, les industries peuvent découvrir des informations précieuses et stimuler l'innovation.\n\n## Encourager l'innovation et s'adapter aux nouvelles technologies\n\nL'innovation est un thème essentiel tout au long de l'épisode du podcast. Les présentateurs soulignent l'importance pour les organisations d'adopter de nouvelles technologies et méthodes afin de rester pertinents dans le monde en constante évolution d'aujourd'hui. Il est essentiel de favoriser un écosystème complet qui soutient l'innovation dans diverses industries, en fournissant des outils et des services spécialisés pour différents aspects de l'innovation.\n\nLe podcast encourage également à donner du pouvoir aux nouveaux talents dans les domaines de l'ingénierie, des affaires et du marketing, afin qu'ils pensent au-delà des normes traditionnelles et adoptent de nouvelles perspectives. En se libérant des processus et modes de pensée dépassés, les organisations peuvent exploiter le potentiel de leurs employés et stimuler l'innovation.\n\nLe conférencier invité, Pete Schmitz, insiste sur la nécessité d'un apprentissage continu et d'une adaptation face aux avancées technologiques et aux transformations numériques. Les organisations doivent évoluer et embrasser le changement afin de ne pas devenir obsolètes dans un paysage concurrentiel.\n\nEn conclusion, cet épisode de podcast met en lumière l'importance de l'algèbre linéaire et du calcul en intelligence artificielle, l'impact transformateur de l'IA dans tous les secteurs et l'importance de favoriser l'innovation et de s'adapter aux nouvelles technologies. Grâce à une compréhension approfondie des fondamentaux de l'IA, à l'utilisation de technologies transformatrices et à la promotion de l'innovation, les organisations peuvent saisir les vastes opportunités offertes par la transformation numérique et se maintenir en tête dans le monde en évolution de l'IA.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Pete Schmitz","Darren W Pulsipher"],"link":"/episode-EDT153-fr","image":"./episodes/edt-153/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode de podcast, Pete Schmitz, un ancien responsable de compte chez Intel, parle de son travail avec des lycéens pour leur enseigner l'IA et comment l'utiliser dans leurs compétitions de robotique. Il explique que ces compétitions nécessitent l'utilisation de l'autonomie, et l'IA est un composant crucial pour y parvenir. Pete partage un exemple de la façon dont la vision par ordinateur, alimentée par l'IA, est utilisée dans le véhicule de surface sans pilote de l'Agence pour les projets de recherche avancée de la défense (DARPA), le DARPA D Hunter."},{"id":53,"type":"Episode","title":"Utilisations de GenAI","tags":["genai","ai","collectiongenerativeai","datamanangement"],"body":"\r\n\r\nL'IA générative est une technologie transformative qui peut augmenter la créativité humaine, améliorer la collaboration et débloquer de nouvelles possibilités pour le travail et la communication. En tirant parti des capacités de l'IA, les individus peuvent générer du contenu, résumer des e-mails et automatiser des tâches routinières, tout en conservant une touche humaine et une individualité.\n\n## Libérer la créativité humaine\n\n## Comprendre le paysage des données et fixer des objectifs clairs.\n\nLe Dr Lancaster insiste sur l'importance de comprendre le type de données que vous souhaitez utiliser ou créer avant de vous plonger dans l'IA générative. Qu'il s'agisse de texte, d'images, de musique, de vidéos ou d'audio, avoir une compréhension claire de vos entrées et de vos sorties souhaitées vous permet de sélectionner les outils et plateformes les plus appropriés.\n\n## Augmenter la créativité humaine avec l'IA\n\nL'un des points clés à retenir du podcast est le rôle de l'IA générative dans l'amplification de la créativité humaine plutôt que de la remplacer. Les outils d'IA agissent comme des catalyseurs, améliorant et propulsant la créativité humaine à de nouveaux sommets. En combinant la mentalité innovante des humains avec les capacités de l'IA, les individus peuvent résoudre des problèmes complexes et générer des idées révolutionnaires que les approches traditionnelles seules ne peuvent pas atteindre.\n\n## Collaboration et brainstorming avec l'IA\n\nL'IA générative ouvre des portes à la collaboration et au remue-méninges. L'IA peut servir de voix supplémentaire dans les discussions de groupe, suscitant de nouvelles perspectives et encourageant des conversations fructueuses. Cet aspect collaboratif est particulièrement précieux dans les contextes de groupe, où l'IA peut écouter les conversations, faciliter les discussions et aider à consolider les idées en un consensus.\n\n## Libérer le pouvoir de l'IA générative\n\nL'IA générative possède un immense potentiel pour libérer la créativité, renforcer les capacités humaines et offrir de nouvelles perspectives et solutions aux défis. Que vous soyez développeur, chercheur ou simplement curieux au sujet de l'IA, il y a une multitude d'opportunités à explorer et à créer avec l'IA générative.\n\n## Applications pratiques de l'IA générative sur le lieu de travail\n\nEn plus des connaissances partagées dans le podcast, il existe de nombreuses applications pratiques de l'IA générative qui peuvent révolutionner nos processus de travail. Explorons-en quelques-unes :\n\n## Résumé des longs emails et optimisation de la communication\n\nLes professionnels occupés reçoivent souvent des courriels longs qui leur font perdre un temps précieux. L'IA générative peut aider en analysant le contenu du courriel et en générant un résumé concis qui capture les points principaux et les éléments clés. Cela permet aux destinataires de saisir rapidement les informations importantes et de prendre des décisions éclairées sans passer trop de temps à lire le courriel dans son intégralité.\n\n## Automatiser la création de contenu.\n\nL'IA générative peut automatiser la création de rapports, d'articles et d'autres contenus écrits. En entrant des données pertinentes dans un outil d'IA générative, les journalistes et les créateurs de contenu peuvent générer des articles ou des rapports complets basés sur ces données. Cela permet de gagner un temps considérable et des ressources, notamment pour ceux qui doivent produire régulièrement de grandes quantités de contenu.\n\n## Améliorer la créativité artistique\n\nLes créatifs dans les domaines de l'art et de la musique peuvent exploiter l'IA générative pour explorer de nouveaux styles, techniques et inspirations. L'IA peut aider les artistes à générer des idées, composer de la musique et créer du contenu visuel. Grâce au pouvoir de l'IA générative, les artistes peuvent élargir leurs horizons créatifs et repousser les limites dans leurs domaines respectifs.\n\n## Équilibrer l'automatisation et le contact humain\n\nAlors que l'IA générative offre un potentiel incroyable, il est crucial de maintenir la surveillance et l'intervention humaines pour assurer l'exactitude, le contexte et préserver l'individualité. Faire confiance au contenu généré par l'IA aveuglément et sans intervention humaine peut conduire à une homogénéisation du paysage numérique. Il est essentiel de trouver un équilibre entre l'automatisation et la touche humaine, où l'IA améliore la créativité humaine plutôt que de la remplacer.\n\nÀ mesure que l'IA générative continue d'évoluer, nous pouvons nous attendre à voir son intégration dans divers aspects du travail et de la communication. De la synthèse des courriels à l'automatisation de la création de contenu et à la possibilité de nouvelles formes d'expression artistique, l'IA générative a la capacité de rationaliser les processus, d'améliorer la productivité et de débloquer de nouvelles possibilités d'innovation. Adopter cette technologie tout en préservant la créativité et l'unicité humaines façonnera l'avenir du travail de manière remarquable.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT154-fr","image":"./episodes/edt-154/fr/thumbnail.png","lang":"fr","summary":"Dans le dernier épisode, le Dr Jeffrey Lancaster et Darren Pulsipher explorent les cas d'utilisation pratiques de l'IA générative et comment elle peut libérer la créativité humaine dans divers domaines."},{"id":54,"type":"Episode","title":"Conseiller GenAI pour la gestion des centres de données","tags":["ai","sdi","vergeio","cloud","privatecloud","collectiongenerativeai","hybridcloud"],"body":"\r\n\r\nDans l'épisode de podcast \"Embracing Digital Transformation\", l'architecte en chef des solutions, Darren Pulsipher, interviewe Greg Campbell, le directeur technique de Verge.io. La conversation tourne autour des solutions innovantes en matière de gestion de l'infrastructure et du potentiel de l'intelligence augmentée. Greg partage son parcours en tant que développeur de logiciels et entrepreneur, en discutant des défis auxquels il a cherché à répondre avec Verge.io, une entreprise axée sur la simplification de la gestion de l'infrastructure dans les serveurs distribués.\n\n## Simplification de la gestion complexe de l'infrastructure.\n\nLa gestion des infrastructures dans le paysage numérique d'aujourd'hui pose d'importants défis. La complexité provient de divers composants, fournisseurs, licences et versions. Cela nécessite un personnel qualifié et entraîne souvent des coûts élevés et une pénurie d'expertise. Bien que le cloud ait été initialement considéré comme une solution, il a introduit ses propres complexités.\n\nVerge.io propose une solution grâce à son système d'exploitation, VergeOS. Ce système permet aux développeurs de gérer et de connecter facilement les ressources de stockage, de calcul et de mise en réseau sur différentes configurations matérielles. En fournissant un centre de données virtuel, VergeOS simplifie la gestion de l'infrastructure, la rendant plus intuitive et conviviale.\n\n## Le potentiel de l'IA générative dans la gestion des infrastructures.\n\nGreg parle également de son intérêt pour l'intelligence artificielle (IA) et ses applications potentielles. Il partage ses expériences avec l'IA générative et son utilisation dans la gestion des infrastructures. Greg explore comment l'automatisation de la gestion des infrastructures et des centres de données grâce à l'IA générative peut simplifier les processus complexes et rationaliser la gestion des ressources.\n\nL'IA générative peut automatiser la gestion de l'infrastructure, éliminant ainsi le besoin d'experts spécialisés et améliorant l'efficacité. Elle a le potentiel de révolutionner la conception de l'interface utilisateur et des interfaces adaptatives, rendant le processus de gestion de l'infrastructure plus intuitif et convivial pour les utilisateurs.\n\n## Intelligence augmentée comme un assistant précieux.\n\nL'intelligence augmentée est la combinaison de l'intelligence humaine et de l'intelligence artificielle. L'intelligence augmentée renforce les capacités humaines et la prise de décision en fournissant des informations et des réponses à des problèmes complexes. Elle vise à aider, plutôt qu'à remplacer, le jugement humain dans la prise de décisions éclairées.\n\nGreg souligne que lorsque les modèles d'IA deviennent plus grands et plus sophistiqués, leur précision et leurs capacités prédictives s'améliorent. L'intelligence augmentée peut être appliquée dans divers secteurs, tels que le support client, où les modèles d'IA peuvent fournir des réponses aux questions des clients et aider les agents humains à trouver des solutions. Elle peut également aider à la gestion de sites ou de bureaux distants, en fournissant des conseils aux personnels sur place qui peuvent manquer d'expertise dans certains domaines.\n\n## L'avenir de la transformation numérique\n\nLe podcast se conclut par une discussion sur le futur de l'intelligence augmentée et son impact potentiel sur les industries et la main-d'œuvre. L'optimisme de Greg repose sur la capacité de l'intelligence augmentée à améliorer l'efficacité et la productivité, mais avec la reconnaissance qu'elle ne devrait pas remplacer entièrement le jugement humain. La conversation met en évidence l'importance d'une mise en œuvre prudente, d'une supervision humaine continue et de considérations éthiques lors de l'utilisation de l'intelligence augmentée.\n\nDans l'ensemble, cet épisode de podcast offre des aperçus précieux sur les solutions innovantes de gestion des infrastructures, le potentiel de l'IA générative pour rationaliser les processus, et les avantages de l'intelligence augmentée en tant qu'assistant précieux. Il démontre le pouvoir d'adopter la transformation numérique et d'exploiter la technologie pour stimuler l'efficacité et le succès au sein des organisations.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Greg Campbell","Darren W Pulsipher"],"link":"/episode-EDT155-fr","image":"./episodes/edt-155/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, l'animateur Darren Pulsipher s'entretient avec Greg Campbell, directeur technique de Verge.io, pour discuter de l'excitante intersection entre l'IA et la gestion de l'infrastructure. Greg, développeur de logiciels et entrepreneur, partage son parcours pour créer Verge.io afin de résoudre les complexités de l'assemblage et de la gestion de l'infrastructure."},{"id":55,"type":"Episode","title":"Devenir une organisation prête pour les données","tags":["collectiongenerativeai","datamanagement","automation","dataquality","strategicanalytics","generativeai","digitaltransformation","datadriveninsights","datareadiness","innovation","decisionmaking","technologytrends","businessintelligence","datastrategy","analytics","bigdata","continuouslearning","operationalefficiency","dataoptimization","datainnovation","emrbacingdigital","edt156"],"body":"\r\n\r\n## Évolution de la gestion des données : du manuel à l'automatisation\n\nRon commence la conversation en mettant en évidence le processus manuel et intensif en main-d'œuvre de gestion des données au début de sa carrière. Dans des secteurs tels que les systèmes d'armes nucléaires et spatiales, la gestion des données nécessitait un effort manuel méticuleux en raison de la fiabilité et de la complexité élevées des systèmes. Cependant, à mesure que le monde est devenu de plus en plus axé sur les données et dépendant de la technologie, les organisations ont reconnu la nécessité de transformer les données de manière plus utilisable et efficace.\n\n## Les défis de la gestion des données : complexité et qualité\n\nRon partage un exemple convaincant tiré de son expérience dans la Marine, en discutant des défis liés à la gestion des données pour les navires pendant les cycles de maintenance et de modernisation. La complexité des systèmes navals et le milieu maritime difficile rendent une analyse approfondie des données et une planification cruciales pour une maintenance et des réparations réussies. Cela met en évidence l'importance de la qualité des données et son impact sur l'efficacité opérationnelle et la prise de décision.\n\n## Préparation des données et automatisation\n\nPour profiter de l'automatisation, les organisations doivent se concentrer sur la qualité des données. Dans le processus d'analyse et d'évaluation automatisé, toute erreur ou manque de données devient critique. Pour remédier à cela, les organisations doivent améliorer la collecte de données dès le départ. En concevant des systèmes qui facilitent la collecte de données et en considérant la personne qui collecte les données comme un client, les organisations peuvent réduire les erreurs et améliorer la qualité des données.\n\nUne approche holistique de la préparation des données est également cruciale. Cela implique de reconnaître les différentes étapes de la préparation des données, de la collecte à la gestion et au traitement. En améliorant continuellement chaque domaine, les organisations peuvent s'assurer que leurs données sont de haute qualité et prêtes à soutenir différentes opérations et technologies telles que l'IA générative.\n\n## Filtrage du bruit : Analyse stratégique des données.\n\nL'analyse des données joue un rôle essentiel dans la création de valeur stratégique pour les organisations. Ron et Darren discutent de l'importance de filtrer les données en fonction de leur pertinence par rapport aux objectifs et de se concentrer sur ce qui est vraiment important. Toutes les données ne seront pas précieuses ou nécessaires à l'analyse, et les organisations doivent aligner leur collecte de données sur leurs objectifs afin d'éviter de gaspiller des ressources.\n\nDe plus, la conversation souligne que les données n'ont pas besoin d'être parfaites pour être utiles. Bien que la précision et l'exactitude soient importantes dans certains cas, des données \"suffisamment bonnes\" peuvent toujours fournir des informations précieuses. En reconnaissant la valeur d'un éventail de données, les organisations peuvent éviter de rechercher une perfection inatteignable et se concentrer sur l'exploitation des informations disponibles.\n\n## Découvrir une valeur inattendue : embrasser les possibilités\n\nLe podcast explore également le potentiel de l'IA générative dans l'amélioration de la collecte de données. En utilisant des formulaires interactifs et des interfaces conversationnelles, les organisations peuvent recueillir des informations plus significatives et découvrir de nouvelles perspectives. Cela ouvre des possibilités pour une meilleure analyse des données et une prise de décision améliorée, en particulier dans les domaines où la collecte de données est cruciale.\n\nLa discussion se conclut avec le rappel que l'analyse des données est un parcours d'apprentissage continu. Les organisations devraient être ouvertes à l'exploration de nouvelles technologies et approches, cherchant toujours à découvrir une valeur inattendue dans leurs données.\n\n## Conclusion\n\nDans un monde de plus en plus axé sur les données, il est crucial de devenir une organisation prête pour les données afin de réussir. En comprenant l'évolution de la gestion des données, en se concentrant sur la qualité et la préparation des données, et en embrassant les possibilités de l'analyse stratégique des données, les organisations peuvent libérer le pouvoir des données pour stimuler l'innovation, optimiser les opérations et prendre des décisions éclairées. Cet épisode de podcast offre des perspectives précieuses et met en évidence l'importance de la gestion des données et de l'analyse dans l'ère numérique.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Ron Fritzemeier","Darren W Pulsipher"],"link":"/episode-EDT156-fr","image":"./episodes/edt-156/fr/thumbnail.png","lang":"fr","summary":"Dans l'épisode de podcast, le contre-amiral à la retraite Ron Fritzmeier se joint à l'animateur Darren Pulsipher pour discuter de l'importance de la gestion des données dans le contexte de l'intelligence artificielle générationnelle (IA). Avec une formation en génie électrique et une expérience étendue dans les domaines de la cyber et de la cybersécurité, Ron apporte des perspectives précieuses sur le domaine en évolution de la gestion des données et son rôle crucial dans le succès organisationnel à l'ère numérique."},{"id":56,"type":"Episode","title":"Opérationnaliser GenAI","tags":["ai","generativeai","collectiongenerativeai","infrastructuremanagement","aisystems","aimodels","operationalization","datainput","modeltraining","finetuning","digitaltransformation","opensourcemodels","privateclouds","edgecomputing","aitools","creativeoutput","responsibleusage","reinforcementlearning","monitoring","optimization","sandboxenvironment","cloudbasedinfrastructure","onpremisesinfrastructure","hybridinfrastructure","customerservice","brainstormingapplications","embracingdigital"],"body":"\r\n\r\n## Exploration de différents modèles de partage de l'IA générative.\n\nLe podcast met en avant la gamme de modèles de partage pour l'IA générative. À une extrémité du spectre, il y a des modèles ouverts où n'importe qui peut interagir avec et contribuer à l'entraînement du modèle. Ces modèles utilisent l'apprentissage par renforcement, permettant aux utilisateurs d'entrer des données et de recevoir des réponses pertinentes. À l'inverse, certains modèles privés sont plus verrouillés et limités en termes d'accessibilité. Ces modèles conviennent aux scénarios d'entreprise où le contrôle et la contrainte sont cruciaux.\n\nCependant, il existe une approche mixte qui combine les fondements linguistiques des modèles ouverts avec des contraintes supplémentaires et une personnalisation. Cette approche permet aux organisations de bénéficier de modèles pré-entraînés tout en ajoutant leur propre contrôle et adaptation. En ajustant les poids et les mots utilisés dans le modèle, les organisations peuvent personnaliser les réponses pour répondre à leurs besoins spécifiques sans repartir de zéro.\n\n## Opérationnaliser Gen AI dans la gestion des infrastructures.\n\nLe podcast explore l'opérationnalisation de l'IA générative dans la gestion des infrastructures. Il met en évidence les avantages de l'utilisation de modèles open source pour développer des systèmes spécialisés qui gèrent efficacement les clouds privés. Par exemple, l'un des partenaires mentionnés a mis en œuvre l'IA générative pour surveiller et optimiser en temps réel les performances de son infrastructure, permettant ainsi la résolution proactive des problèmes. En tirant parti de la puissance de l'IA, les organisations peuvent améliorer leur efficacité opérationnelle et garantir le bon fonctionnement de leur infrastructure.\n\nLes hôtes soulignent l'importance de prendre en compte le type et la qualité des données entrées dans le modèle ainsi que la sortie souhaitée. Il n'est pas toujours nécessaire de former un modèle avec des milliards d'indicateurs ; un ensemble de données plus petit adapté à des besoins spécifiques peut être plus efficace. En comprenant les subtilités des données et les objectifs particuliers du système, les organisations peuvent optimiser le processus de formation et améliorer les performances globales du modèle d'IA.\n\n## Gérer et affiner les systèmes d'IA.\n\nLa gestion des systèmes d'IA nécessite une prise de décision réfléchie et une surveillance continue. Les hôtes discutent de l'importance de choisir l'infrastructure adéquate, qu'elle soit basée sur le cloud, sur site ou hybride. De plus, le calcul Edge gagne en popularité, permettant aux modèles d'IA de s'exécuter directement sur les appareils, réduisant ainsi les allers-retours de données.\n\nLe podcast insiste sur le besoin d'expertise pour établir et entretenir les systèmes d'IA. Un talent qualifié est nécessaire pour concevoir et ajuster les modèles d'IA afin d'atteindre les résultats souhaités. Selon l'utilisation, des fonctionnalités spécifiques peuvent être nécessaires, telles que l'empathie dans le service client ou la créativité dans les applications de remue-méninges. Il est crucial de disposer d'une équipe compétente qui comprend les subtilités des systèmes d'IA et peut garantir leur bon fonctionnement optimal.\n\nDe plus, les modèles d'IA nécessitent une surveillance et des ajustements constants. Les modèles peuvent présenter des comportements indésirables, il est donc essentiel d'intervenir au besoin pour garantir des résultats appropriés. Le podcast fait la distinction entre les problèmes de renforcement, où les commentaires des utilisateurs peuvent orienter le modèle dans des directions potentiellement nuisibles, et l'hallucination, qui peut être intentionnellement utilisée à des fins créatives.\n\n## Commencer avec les modèles d'IA\n\nLe podcast propose des conseils pratiques pour commencer à utiliser les modèles d'intelligence artificielle. Les animateurs suggèrent de s'amuser avec les outils disponibles et de se familiariser avec leurs capacités. S'inscrire sur des comptes et explorer comment les outils peuvent être utilisés est un excellent moyen d'acquérir de l'expérience pratique. Ils recommandent également de créer un environnement de test au sein des entreprises, permettant aux employés de tester et d'interagir avec les modèles d'intelligence artificielle avant de les mettre en production.\n\nLe podcast met en avant l'importance d'accorder suffisamment de créativité aux modèles d'IA tout en maintenant le contrôle et en fixant des limites. Les organisations peuvent trouver un équilibre entre la production créative et une utilisation responsable en définissant des balises et en prenant des décisions concernant ce que le modèle devrait ou ne devrait pas apprendre des interactions.\n\nEn conclusion, l'épisode de podcast fournit des perspectives précieuses sur l'opérationnalisation de l'IA générative, la gestion de l'infrastructure et les considérations liées à la gestion et au perfectionnement des systèmes d'IA. Il offre également des conseils pratiques pour commencer avec des modèles d'IA dans des environnements personnels et professionnels. En comprenant les différents modèles de partage, les besoins en infrastructure et l'importance de la créativité et des limites, les organisations peuvent exploiter la puissance de l'IA pour soutenir la transformation numérique.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT157-fr","image":"./episodes/edt-157/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode de podcast, l'animateur Darren Pulsipher, architecte principal de solutions du secteur public chez Intel, discute de l'opérationnalisation de l'IA générative avec le Dr Jeffrey Lancaster, un invité récurrent. Ils explorent les différents modèles de partage de l'IA générative, notamment les modèles publics, privés et communautaires. Le podcast aborde des sujets tels que les modèles open-source, la gestion de l'infrastructure et les considérations pour le déploiement et la maintenance des systèmes d'IA. Il se penche également sur l'importance de la créativité, de la personnalisation et des premiers pas avec les modèles d'IA."},{"id":57,"type":"Episode","title":"GenAI dans l'enseignement supérieur","tags":["collectiongenerativeai","addressingbiasesingenerativeai","preservingauthenticityandindividuality","balancingaiintegrationineducation","lauratorresnewey","criticalthinkingskills","educationaltechnology","highereducation","aiineducation","aibias","diversityandinclusion","authenticvoices","uniqueperspectives","genai","generativeai","embracingdigital","edt158"],"body":"\r\n\r\nL'essor de l'intelligence artificielle générative dans l'éducation.\n\nDans un récent épisode du podcast \"Embracing Digital Transformation\", l'animateur Darren Pulsipher interviewe Laura Torres Newey, une auteure à succès du New York Times et professeure d'anglais, sur l'impact de l'IA générative dans l'enseignement supérieur. La discussion tourne autour de l'intégration de l'IA dans la salle de classe, de ses effets sur les méthodes d'enseignement, des préoccupations liées aux biais et de la préservation des voix uniques. Laura partage ses idées et expériences en tant qu'éducatrice, offrant des perspectives précieuses sur la navigation dans le paysage en évolution de l'éducation à l'ère de l'intelligence artificielle.\n\n## L'influence de l'IA générative dans l'éducation.\n\nL'IA générative a commencé à devenir une présence notable dans l'éducation, de la notation automatisée des essais à l'assistance à l'écriture pour les étudiants. Bien que cette technologie offre commodité et efficacité, elle suscite des préoccupations quant à la perte potentielle de voix uniques. Laura souligne l'importance de valoriser et de cultiver les perspectives individuelles et la créativité des étudiants dans leur écriture. Au lieu d'interdire purement et simplement l'utilisation de l'IA générative, Laura croit en l'enseignement aux étudiants de l'art d'utiliser ces outils de manière efficace et d'exploiter leur potentiel sans compromettre leurs propres voix.\n\nL'intégration de l'intelligence artificielle génératrice amène un changement de focus dans l'enseignement. Au lieu d'évaluer uniquement le produit final, les éducateurs devraient accorder davantage d'importance au processus d'apprentissage. Avec des outils pilotés par l'IA tels que Grammarly disponibles pour les étudiants, les enseignants peuvent orienter leur attention vers le développement des compétences de pensée critique, des capacités de recherche et de la discernement pour identifier des sources fiables. En incorporant des devoirs qui impliquent la comparaison du contenu généré par l'IA avec des travaux rédigés de manière traditionnelle, les étudiants peuvent analyser les forces et les faiblesses des deux approches, favorisant ainsi une compréhension plus profonde de leur écriture et le perfectionnement de leurs compétences de pensée critique.\n\n## Le rôle des éducateurs à l'ère de l'IA\n\nLes éducateurs ont un rôle essentiel dans la préparation des étudiants face à l'évolution constante du paysage technologique. Laura insiste sur le fait qu'il est crucial pour les éducateurs de tous les niveaux d'éducation de s'adapter à l'IA générative et de l'utiliser de manière efficace. Avec l'IA de plus en plus présente sur le lieu de travail, les étudiants qui peuvent naviguer et exploiter cette technologie seront mieux préparés pour les opportunités d'emploi futures. Afin de garantir que les étudiants soient bien préparés, les éducateurs doivent non seulement se familiariser avec les applications de l'IA, mais aussi enseigner aux étudiants comment utiliser l'IA de manière efficace et éthique.\n\nLa transition vers l'intégration de l'IA générative dans l'éducation est en accord avec la position du Département de l'Éducation des États-Unis sur l'IA. Ils reconnaissent les avantages potentiels mais soulignent la nécessité que les utilisateurs restent en contrôle, en comparant le rôle de l'IA à celui d'un vélo électrique, où la technologie allège la charge mais où l'utilisateur conserve finalement le contrôle. Cette approche met l'accent sur l'importance de trouver un équilibre entre tirer parti des avantages de l'IA et préserver les voix et les perspectives uniques des étudiants.\n\n## Conclusion.\n\nL'intégration de l'IA générative dans l'éducation présente à la fois des opportunités et des défis. Alors que l'IA peut améliorer l'apprentissage et aider les étudiants dans leurs devoirs, il est essentiel que les éducateurs accordent la priorité à la pensée critique et abordent les préoccupations liées aux biais afin de développer des penseurs complets et indépendants. Les enseignants devraient adopter la technologie de l'IA, en comprenant ses applications et en enseignant aux étudiants comment la naviguer et l'utiliser efficacement. En trouvant un équilibre entre l'efficacité des contenus générés par l'IA et la préservation des voix authentiques et diversifiées, les éducateurs peuvent préparer les étudiants à l'avenir numérique tout en veillant à cultiver leur individualité et leur créativité.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Laura Newey","Darren W Pulsipher"],"link":"/episode-EDT158-fr","image":"./episodes/edt-158/fr/thumbnail.jpg","lang":"fr","summary":"Dans cet épisode de podcast, Darren Pulsipher, architecte principal des solutions du secteur public chez Intel, interviewe Laura Torres Newey, auteure à succès du New York Times et professeure d'université, afin de discuter de l'impact de l'IA générative dans l'enseignement supérieur. Cet épisode explore les défis et les opportunités offerts par l'intégration de l'IA générative dans la salle de classe, soulignant la nécessité de développer des compétences en pensée critique, les préoccupations liées aux biais et la garantie de la préservation de voix uniques."},{"id":58,"type":"Episode","title":"Politiques de l'IA","tags":["collectiongenerativeai","policies","ai","generativeai","guidelines","jeremyharris","darrenpulsipher","roadmap","challenges","efficiencies","dataprotection","privacy","compliance","ethicalconsiderations","feedback","engagement","ratings","reviews","customersatisfaction","customerengagement","embracingdigital","edt159"],"body":"\r\n\r\n## Le besoin de politiques et de directives claires\n\nJérémy et Darren soulignent l'importance d'avoir une politique claire et une feuille de route bien définie pour la transformation numérique. Se précipiter dans la numérisation sans une planification adéquate peut entraîner des défis et des inefficacités. En établissant des politiques et des lignes directrices, les organisations peuvent définir leurs objectifs, fixer une direction stratégique et s'assurer que tout le monde est sur la même longueur d'onde.\n\nIls soulignent que la transformation numérique va bien au-delà de l'adoption de nouvelles technologies - cela nécessite un changement de culture organisationnelle et d'état d'esprit. Les politiques peuvent aider à faciliter ce changement en fixant des attentes pour les employés, en définissant les meilleures pratiques numériques et en fournissant un cadre de prise de décision dans le domaine numérique.\n\n## Naviguer dans les complexités de la numérisation\n\nLa transformation numérique fait émerger un ensemble complexe de défis, tels que la sécurité des données, la confidentialité et la conformité. Les organisations doivent relever ces défis en les intégrant dans leurs politiques et leurs directives. Cela inclut la mise en place de mesures de protection des données, la réalisation d'audits de sécurité réguliers et garantir la conformité aux réglementations pertinentes.\n\nLes politiques devraient également aborder les considérations éthiques qui accompagnent la transformation numérique. Les hôtes mettent l'accent sur l'importance pour les organisations d'être des gardiens responsables des données et de veiller à ce que l'utilisation des technologies numériques soit conforme aux normes éthiques. Des directives claires peuvent aider les employés à comprendre leurs responsabilités et promouvoir des pratiques numériques responsables au sein de l'organisation.\n\n## Le rôle des commentaires et de l'engagement\n\nLes hôtes soulignent l'importance des commentaires et de l'engagement dans le monde numérique. Adopter une politique qui encourage et valorise les commentaires peut aider les organisations à s'améliorer en continu et à s'adapter aux circonstances changeantes. En accueillant les suggestions et les contributions des employés et des clients, les organisations peuvent affiner leurs stratégies numériques et s'assurer qu'elles répondent aux besoins de tous les parties prenantes.\n\nIls mentionnent également l'importance des évaluations et des avis à l'ère numérique. Les retours d'information par le biais des évaluations et des avis fournissent non seulement des informations précieuses aux organisations, mais également une mesure de la satisfaction et de l'engagement des clients. Les politiques peuvent définir comment les organisations recueillent et répondent aux commentaires et établissent des directives pour saisir le sentiment des clients dans l'espace numérique.\n\n## Conclusion\n\nLa transformation numérique est un parcours qui nécessite une planification minutieuse, des politiques claires et des ajustements continus. En établissant des politiques et des lignes directrices, les organisations peuvent naviguer dans les complexités de la numérisation, relever les défis et garantir une utilisation responsable et efficace des technologies numériques. Adopter la transformation numérique ne consiste pas seulement à adopter de nouveaux outils, mais aussi à créer une culture numérique qui favorise l'innovation et répond aux besoins évolutifs des clients et des parties prenantes.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeremy Harris","Darren W Pulsipher"],"link":"/episode-EDT159-fr","image":"./episodes/edt-159/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, l'animateur Darren interviewe Jeremy Harris et se plonge dans l'importance d'établir des politiques et des lignes directrices pour une transformation numérique réussie. Avec la prévalence croissante des technologies numériques dans diverses industries, les organisations doivent s'adapter et embrasser cette transformation pour rester compétitives et répondre aux attentes évolutives des clients."},{"id":59,"type":"Episode","title":"Réduire la congestion d'ingestion avec Intel Optane DCPMM","tags":null,"body":"\r\n\r\n## Détails de la pile de services\n\nUn client de l'industrie automobile avait du mal à obtenir efficacement des informations de leurs voitures et de les transférer vers leur centre de données afin de pouvoir effectuer de l'apprentissage automatique et des analyses. Des recherches ont été menées dans ce domaine, mais seulement pour un petit nombre de voitures, pas les centaines de millions de voitures du client. Lorsque j'ai examiné l'ensemble de leur pile de services et la façon dont tout arrivait au centre de données, l'ingestion des données est devenue le problème clair: Comment puis-je ingérer autant de données, et comment le faire rapidement ?\n\n## Présentation générale de l'architecture Kafka à haut niveau\n\nLe client souhaitait utiliser Kafka pour leur ingestion. Kafka est un courtier qui peut bien se mettre à l'échelle, et la clé est qu'il peut gérer plusieurs producteurs, différents consommateurs et beaucoup de données. Utiliser plusieurs courtiers Kafka pour placer et transférer les données dans les endroits les plus appropriés offre une grande flexibilité.\n\nKafka, cependant, a été principalement conçu pour des tailles de message d'environ un à 10 kilo-octets et les données du client étaient d'environ 240 kilo-octets par voiture. Il existe des solutions de contournement, mais je voulais transférer l'intégralité du message de 240 kilo-octets dans le bus Kafka afin de pouvoir le déplacer selon mes besoins.\n\n## Pratiques optimales de performance\n\nJ'ai examiné les meilleures pratiques de performance des autres personnes travaillant avec Kafka pour voir si je pouvais les adapter aux besoins de mon client. Augmenter la taille du tampon pour pouvoir stocker le message en entier est une solution d'ajustement fine, tout comme la gestion de la taille des lots pour des performances optimales. Une autre pratique courante consiste à disperser les journaux. La flexibilité de Kafka me permettrait de placer les données dans différents sujets. Je peux diviser ces sujets en plusieurs partitions et les répartir sur plusieurs disques. La question est donc de savoir sur combien de disques je répartis les journaux de Kafka. De plus, je souhaite les disques les plus rapides possibles.\n\nUn exemple que j'ai examiné était LinkedIn. Leurs chiffres publiés il y a un an indiquent qu'ils peuvent traiter 13 millions de messages par seconde, soit 2,7 gigaoctets par seconde. Ils affirment avoir environ 1 100 courtiers Kafka et plus de 60 sur un cluster, ce qui représente une configuration plutôt importante.\n\n## Espace automobile\n\nSi je regarde les chiffres bruts du client (1,6 million de messages par seconde et 800 gigaoctets par seconde) et que je les compare à LinkedIn, qui n'est probablement pas optimisé pour 240 kilooctets, j'obtiens 44 000 courtiers. Si je l'optimisais, je pourrais probablement réduire ce nombre à 4 400 courtiers, ce qui reste 240 clusters. C'est un nombre énorme de machines, donc j'ai dû trouver un moyen de rendre les choses plus rapides. Avec une optimisation supplémentaire, je pourrais probablement le réduire à 400 à 500 courtiers, mais je voulais voir ce que je pouvais faire d'autre.\n\n## Intel Optane DC Persistent Memory\n\nMémoire persistante Intel Optane DC\n\nJ'ai examiné notre Optane Persistent Memory. Il s'intègre au format DDR4, donc il se trouve directement sur le bus DDR4. Il peut atteindre des modules de 512 gigaoctets, donc dans un serveur à deux sockets, je peux disposer de six téraoctets de mémoire persistante. Je voulais trouver un moyen d'exploiter cette technologie extrêmement fiable avec des fonctionnalités avancées telles que le chiffrement matériel intégré pour m'aider à résoudre ce problème.\n\n## Soutien à la variété des applications\n\nIl existe deux modes de fonctionnement avec cette mémoire Optane: le mode d'application directe et le mode mémoire. Le mode mémoire est simple. Il utilise la mémoire persistante comme une RAM normale car c'est moins cher que la DDR4 normale. Ce n'est pas exactement la même chose que la DDR4, mais c'est assez proche pour que dans la plupart des applications, on ne puisse pas voir de différence. En mode d'application directe, vous pouvez réellement écrire depuis votre programme directement dans la mémoire persistante. De cette façon, je n'ai pas besoin de marshalliser et démarshaser les structures de données et de les diffuser en continu; je peux simplement les pousser dans la mémoire persistante. Je peux également monter le mode d'application directe en tant que système de fichiers, ce qui est beaucoup plus rapide que sur le bus d'E/S. Maintenant, que puis-je faire avec cette mémoire?\n\n## Utilisation du noyau Linux\n\nIl existe deux outils principaux disponibles en utilisant le noyau Linux : ndctl et ipmctl. Ndctl est un contrôleur de périphérique de mémoire non volatile, et puis il y a IPM, le contrôleur de la mémoire persistante Intel, qui me permet de manipuler et de contrôler cette mémoire persistante. Je peux le configurer en mode mémoire ou en mode d'application directe. J'ai dû apprendre un peu sur ces outils et comment ils fonctionnent.\n\n## Approche d'ingestion\n\nMa première pensée a été que si je donnais à Kafka beaucoup plus de mémoire avec de grandes tailles de tampon, il devrait fonctionner beaucoup plus rapidement. Les modifications de code dans la configuration seraient inutiles ou minimes. Une autre option était de changer Kafka pour écrire dans une mémoire persistante plutôt que d'écrire dans un système de fichiers, en contournant le disque dur. La dernière chose que j'ai examinée était la création d'un système de fichiers persistant utilisant une mémoire persistante, puis de placer les journaux de Kafka sur ce nouveau système de fichiers.\n\nLa plus facile des trois options était la première - plus de mémoire. J'ai exécuté toutes mes tâches avec plus de mémoire et il n'y a eu aucun changement de performance. La raison en est qu'à un moment donné, mes tampons se sont remplis et j'ai dû les transférer vers un disque. Au final, tout devait passer par les journaux Kafka, ce qui constituait mon goulot d'étranglement.\n\nLa deuxième option implique la réécriture du code et l'attente des approbations, alors j'ai sauté à la troisième option. Les résultats de cette expérience, où j'ai dirigé les journaux vers ce nouveau système de fichiers ultra-rapide, étaient fascinants. Jetons un coup d'œil au processus et aux résultats.\n\n## Contraintes de test\n\nPour éliminer les obstacles à la performance des tests, j'ai exclu le réseau de l'équation en exécutant mon test sur la même machine que celle sur laquelle était mon courtier. De plus, j'ai d'abord exécuté uniquement les producteurs, puis uniquement les consommateurs, puis un mélange des deux, afin de pouvoir évaluer les différences. Mon objectif n'était pas de regarder l'amélioration totale de la production, mais de voir si ce disque aurait vraiment un impact sur un courtier individuel.\n\n## Première approche 50/50\n\nLa première chose que j'ai faite a été de prendre la moitié de ma mémoire persistante et de la mettre en mode app direct, puis de la convertir en système de fichiers. J'ai laissé l'autre moitié en tant que mémoire. J'ai utilisé les commandes ndctl et ipmctl et créé des espaces de noms. J'ai monté ces systèmes de fichiers et exécuté mon test.\n\n## Modifier la taille du message\n\nJ'ai exécuté les tests sur plusieurs tailles de messages différentes. Je m'attendais à certaines optimisations, principalement pour 1 kilo-octet. J'ai constaté que j'obtenais des performances de plus en plus élevées jusqu'à environ 10 producteurs. Au-delà de 10 producteurs, j'ai commencé à saturer le bus et à observer une certaine variabilité. Cela me dit que j'avais mis en cache des éléments. Je peux maintenant comparer ces chiffres à ce que j'ai exécuté auparavant uniquement sur un lecteur SATA pour les journaux Kafka. J'ai également essayé nos lecteurs Optane NVMe pour les journaux.\n\n## Comparaison des technologies\n\nJetons un coup d'œil aux différences. Pour 240 kilo-octets, avec un disque SATA normal, c'est plutôt plat. J'ai obtenu une légère amélioration, puis ça a diminué à mesure que le nombre de producteurs augmentait. Avec le disque Optane NVMe, j'ai eu un pic impressionnant, presque deux fois plus rapide qu'un disque SATA, ce à quoi je m'attendais car c'est un bus NVMe au lieu d'un bus SATA. Le Pmem est presque cinq fois plus rapide qu'un disque SATA et deux fois et demie plus rapide que le disque Optane NVMe. C'est parce que j'utilise un bus de mémoire au lieu du bus SATA ou NVMe.\n\n## Optimisation supplémentaire (100% directement dans l'application)\n\nCela fonctionnait rapidement et je remplissais rapidement ce disque temporaire de 750 Go. Comme j'avais besoin de prolonger un peu le test, je suis retourné et j'ai reconfiguré ma machine pour passer en mode d'application directe à 100 pour cent, afin de pouvoir prendre maintenant l'intégralité des 1,5 téraoctets.\n\n## PMEM optimisé et 100% App Direct\n\nAprès avoir fait cela et exécuté les mêmes tests, j'ai obtenu un résultat surprenant. J'ai pu ajouter plus de producteurs et mon débit a augmenté d'environ deux ou trois fois de plus. Maintenant, il est entre 12 et 15 fois plus rapide qu'un disque SATA avec 25 à 30 producteurs et une taille de message de 240 kilooctets. C'est incroyable et cela réduirait considérablement le besoin de nos clients en termes de courtiers, de rangées et de machines. J'ai fait le test plusieurs fois car je ne croyais pas les résultats que j'obtenais. J'ai appelé l'un de nos architectes qui a conçu cette technologie et j'ai appris qu'une des raisons de l'augmentation de la vitesse était que lorsque j'utilisais une partie de la mémoire persistante comme mémoire, les données devaient passer par deux ou trois sauts qui ne sont pas nécessaires en mode app direct. Cela crée moins de contention sur le bus de la mémoire et le débit a augmenté de manière spectaculaire.\n\n## Appel à l'action\n\nLe résultat final est que j'ai pu utiliser Kafka avec la mémoire persistante Optane DC comme un système de fichiers ultra-rapide pour obtenir d'importantes améliorations de débit à la fois pour les producteurs et les consommateurs. Un courtier unique peut gérer 15 fois plus de messages et de débit qu'auparavant, réduisant ainsi le nombre de serveurs nécessaires pour gérer des architectures système larges et complexes. Il est temps d'évaluer votre architecture actuelle et de voir si cela serait bénéfique pour votre organisation.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT16-fr","image":"./episodes/edt-16/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren parle de la diminution de la congestion d'ingestion en utilisant la mémoire persistante Optane DC d'Intel et de l'expérience qu'il a menée avec des résultats surprenants. Cela pourrait bien changer notre manière de penser à la programmation à l'avenir."},{"id":60,"type":"Episode","title":"Sécurité en IA Générative.","tags":["collectiongenerativeai","personalizedphishingattacks","promptinjection","sharingcodeai","harnessingai","digitaltransformation","generativeai","cybersecurityrisks","serviceproviders","duediligence","riskschallenges","digitallandscape","proactivecybersecurity","llm","multifactorauthentication","voicerecognition","typingcadence","github","stackoverflow","samsungipleak","securityaspects","embracingdigital","edt160"],"body":"\r\n\r\n## Attaques de phishing personnalisées et convaincantes.\n\nL'une des principales préoccupations discutées est le potentiel d'attaques de phishing de plus en plus sophistiquées et personnalisées. Le phishing est actuellement la méthode d'attaque cybernétique la plus efficace, et avec l'intelligence artificielle générative, les attaquants peuvent créer des e-mails ou des messages de phishing hautement personnalisés et convaincants. En récupérant des informations à partir des médias sociaux ou d'autres plateformes en ligne, les attaquants peuvent rendre leurs tentatives de phishing plus difficiles à détecter. Cela soulève la question de comment nous pouvons déterminer ce qui est réel ou non et comment nous pouvons avoir confiance en l'authenticité des informations que nous recevons.\n\nPour lutter contre cela, les individus peuvent avoir besoin de développer de nouvelles méthodes de vérification des informations, comme l'utilisation de mots de code personnels ou d'autres mesures d'authentification avec leurs proches. De plus, les organisations et les agences de sécurité doivent adapter leurs stratégies pour contrer la sophistication croissante des attaques cybernétiques facilitées par l'IA générative. Il est crucial de comprendre que l'IA générative elle-même est une technologie neutre, et ses implications dépendent de la manière dont elle est utilisée.\n\n## Voix clonées et confiance dans les informations\n\nLe podcast explore également le potentiel de l'intelligence artificielle générative pour cloner des voix, ce qui a déjà été observé lors d'attaques de kidnapping virtuel. Les criminels utilisent des voix clonées pour créer un sentiment d'urgence et de peur, en prétendant être les proches d'une victime. Cela soulève des inquiétudes quant à la confiance que nous pouvons accorder à l'authenticité des informations que nous recevons.\n\nDans un tel scénario, il devient essentiel de développer des techniques pour vérifier l'authenticité des voix et des informations. En tant qu'individus, nous devrions rester vigilants et faire preuve de prudence lorsque nous répondons à des demandes urgentes par téléphone. S'assurer d'avoir des lignes de communication ouvertes avec des contacts de confiance peut aider à vérifier si de telles demandes sont authentiques.\n\n## Protéger la propriété intellectuelle dans la codification et la programmation\n\nLe podcast enchaîne avec une discussion sur l'importance de protéger la propriété intellectuelle dans le codage et la programmation. Les animateurs mettent en évidence les risques de partager involontairement du code sur des plateformes comme StackOverflow et GitHub, ainsi que la fuite involontaire de propriété intellectuelle lors de la recherche d'aide dans ces forums publics. Les développeurs sont encouragés à remplacer les informations sensibles par des espaces réservés avant de partager du code afin de réduire le risque de perte de propriété intellectuelle.\n\nDe plus, les hôtes discutent de l'introduction d'outils tels que GitHub Copilot, qui utilisent une IA générative pour fournir des suggestions de code. Bien que ces outils puissent être précieux, ils soulèvent des préoccupations quant à la sécurité et à la confidentialité des informations propriétaires. Les développeurs doivent soigneusement considérer la fiabilité du fournisseur de services et garantir une protection adéquate de leurs données et de leur propriété intellectuelle.\n\n## Équilibrer l'innovation et la sécurité à l'ère de l'IA\n\nLa conversation se conclut en mettant l'accent sur l'importance de trouver un équilibre entre l'adoption des avancées et des changements positifs potentiels apportés par l'IA générative, et la prise en compte des risques associés dans le domaine de la cybersécurité et de la protection de la propriété intellectuelle. Il est essentiel de rester informé, d'adapter les stratégies de sécurité et de faire preuve de prudence pour naviguer avec succès dans le paysage en constante évolution de la transformation numérique. En agissant ainsi, nous pouvons exploiter les avantages de l'IA sans compromettre la sécurité et les informations personnelles.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT160-fr","image":"./episodes/edt-160/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, l'animateur Darren Pulsipher est rejoint par Dr Jeffrey Lancaster pour explorer l'intersection entre l'IA générative et la sécurité. La conversation plonge profondément dans les risques potentiels et les défis liés à l'utilisation de l'IA générative dans des activités néfastes, notamment dans le domaine de la cybersécurité."},{"id":61,"type":"Episode","title":"Analyse des données de langage naturel","tags":["collectiongenerativeai","ai","generativeai","embracingdigital","edt161","challengesinnaturallanguageprocessing","fantasysportsapp","naturallanguagegeneration","highqualitycontent","technicalbackground","outoftheboxthinking","pushingboundaries","diversity","crossdomaincollaboration","innovativeideas","infoscentience'ssolution","dataanalytics","naturallanguageaisystem","conceptualautomata","datasets","revolutionizing","businessesanalyzeinformation","futureofdataanalysis","naturallanguagereporting","flexibility","tailored","differentindustries","customized","specificcontext","jargon","dataanalysis","revolutionizingindustries","sportsanalytics","stevewasick'sjourney","innovativeapproach","entrepreneurs","techfounders","unconventionalpaths","successfulinnovations","embracingvariability","context","poweroflanguage"],"body":"\r\n\r\n## Défis en traitement automatique du langage naturel\n\nSteve se souvient de son projet initial, une application pour les sports fantastiques qui visait à fournir aux utilisateurs non seulement des statistiques, mais aussi le contexte et les histoires derrière les chiffres. Cela l'a conduit au domaine de la génération de langage naturel, où il a été confronté à des défis pour obtenir et fournir un contenu de haute qualité. Malgré l'absence de formation technique, les expériences variées de Steve lui ont permis d'aborder ces défis avec créativité et une pensée hors des sentiers battus.\n\n## Repousser les limites\n\nDarren fait l'éloge de Steve pour repousser les limites et apporter une perspective nouvelle au domaine. Cela met en évidence l'importance de la diversité et de la collaboration entre différents domaines pour générer des idées et des solutions innovantes. Le parcours de Steve sert d'inspiration aux entrepreneurs et fondateurs de startups en herbe, prouvant que des chemins non conventionnels peuvent mener à des innovations réussies.\n\n## La solution d'InfoScentience pour l'analyse des données\n\nLa conversation explore également les capacités du système d'intelligence artificielle en langage naturel d'InfoSentience. Steve explique que leur technologie décompose les événements et les histoires en leurs parties constitutives, offrant une meilleure compréhension des concepts complexes et de leurs relations. Ce moteur analytique, basé sur des automates conceptuels, permet la synthèse de jeux de données diversifiés et complexes, révolutionnant la façon dont les entreprises analysent les informations.\n\n## L'avenir de l'analyse de données et du reporting en langage naturel.\n\nDe plus, Steve met l'accent sur la flexibilité de leur système d'IA, qui peut être adapté à différents secteurs et personnalisé pour répondre aux besoins uniques de chaque client. En comprenant le contexte spécifique et le jargon des données analysées, Info Sentience veille à ce que leur système d'IA fournisse des informations précises et pertinentes.\n\nEn conclusion, l'épisode de podcast met en évidence le potentiel de l'analyse des données en langage naturel dans la révolution des industries telles que l'analyse sportive. Le parcours de Steve Wasick et son approche innovante servent d'inspiration aux entrepreneurs et aux fondateurs de technologies, nous rappelant que des chemins non conventionnels peuvent mener à des innovations réussies. L'avenir de l'analyse des données réside dans l'acceptation de la variabilité, du contexte et du pouvoir du langage.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Wasick"],"link":"/episode-EDT161-fr","image":"./episodes/edt-161/fr/thumbnail.jpg","lang":"fr","summary":"Dans le dernier épisode, Darren Pulsipher s'entretient avec Steve Wasick, PDG et fondateur d'InfoSentience, pour discuter du pouvoir et du potentiel de l'analyse des données en langage naturel. Steve, qui provient d'un parcours non conventionnel en tant que majeur en Anglais devenu scénariste puis avocat puis fondateur technologique, apporte une perspective unique au domaine."},{"id":62,"type":"Episode","title":"Élaborer une stratégie de cloud hybride multiplateforme.","tags":["cloudadoption","collmultihybridcloud","organizationalmodernization","barriers","riskmitigationplan","applicationrearchitecture","governance","organizationalculture","cloud","multihybridcloud","multicloud","embracingdigital","edt162"],"body":"\r\n\r\n## Meilleures pratiques pour l'adoption du cloud\n\nPasser au cloud et adopter de nouvelles technologies telles que l'IA générative peut apporter de nombreux avantages, mais les organisations doivent également être prêtes aux changements qui en découlent. Selon Christine McMonigal, directrice des technologies de centres de données et de cloud chez Intel, il existe des meilleures pratiques clés à prendre en compte.\n\n## Modernisation organisationnelle\n\nUn aspect important à reconnaître est que l'adoption du cloud n'est pas seulement une modernisation technologique, mais aussi une modernisation organisationnelle. Cela signifie que les organisations doivent être préparées aux changements dans les processus, les flux de travail et même les structures organisationnelles. Il est crucial de prendre en compte ces changements et de veiller à ce que l'ensemble de l'organisation soit aligné et prêt pour la transformation.\n\n## Identifier les obstacles et fixer des attentes claires\n\nUne étape cruciale pour surmonter les obstacles et atténuer les risques consiste à identifier tout d'abord quels sont ces obstacles. En effectuant une évaluation approfondie de l'infrastructure actuelle, des flux de travail et des défis au sein de l'organisation, les obstacles potentiels peuvent être ciblés et des stratégies peuvent être élaborées pour les surmonter.\n\nDe plus, établir des attentes claires dès le départ est essentiel. Cela signifie une communication efficace avec les parties prenantes, les employés et les partenaires concernant les objectifs, les avantages et les défis de l'adoption de stratégies de cloud multi-hybride. En fixant des attentes réalistes et en veillant à ce que tout le monde soit sur la même longueur d'onde, les organisations peuvent minimiser les surprises et la résistance au changement.\n\n## Plan robuste de réduction des risques\n\nAvoir un plan solide de réduction des risques en place est un autre aspect crucial d'une adoption réussie du cloud. Cela inclut l'évaluation des risques de sécurité potentiels, des préoccupations liées à la confidentialité des données et des exigences de conformité. En abordant de manière proactive ces risques et en mettant en place les mesures appropriées, les organisations peuvent protéger leurs données, garantir la conformité réglementaire et réduire au minimum les menaces potentielles.\n\n## Barrière 1 : Re-architecture de l'application\n\nL'une des principales barrières auxquelles les organisations sont souvent confrontées lors de l'adoption du cloud est la réarchitecture des applications. Il est important d'évaluer quelles applications peuvent être transférées telles quelles dans le cloud, et celles qui peuvent nécessiter des modifications plus importantes. En identifiant les opportunités de simplification et de réduction des coûts grâce à l'automatisation, les organisations peuvent rationaliser l'accès et les contrôles.\n\n## Barrière 2 : Gouvernance\n\nLes politiques de gouvernance jouent un rôle crucial dans la réduction des risques lors de l'adoption du cloud. Des modèles de sécurité incohérents, une diversité d'outils de gestion et des politiques utilisateur hétérogènes peuvent accroître la complexité et compromettre le succès de la migration. Simplifier les politiques de gouvernance et éliminer la bureaucratie peut aider les organisations à rationaliser leurs opérations, réduire les coûts et garantir la sécurité des données et la conformité.\n\n## Barrière 3 : Culture organisationnelle et maturité.\n\nPréparer l'organisation au changement qui accompagne l'adoption du cloud est vital. Cela implique d'impliquer les employés, de leur fournir une formation sur les compétences requises et d'identifier les acteurs clés capables d'adopter les nouvelles méthodes de travail. Il est nécessaire d'aborder les craintes et les préoccupations que les employés peuvent avoir, comme la peur d'être laissés pour compte ou de perdre leur emploi, afin de créer un environnement positif et collaboratif.\n\nEn conclusion, adopter des stratégies de cloud multi-hybride nécessite une planification minutieuse, une communication efficace et une compréhension approfondie des objectifs et des défis d'une organisation. En abordant les obstacles dès le départ et en atténuant les risques, les organisations peuvent ouvrir la voie à un voyage de transformation numérique réussi. Restez à l'écoute pour les prochains épisodes où nous explorerons le développement d'une stratégie cloud, l'évaluation des portefeuilles d'applications et plus d'informations sur l'adoption de la transformation numérique. N'oubliez pas de noter et de vous abonner à notre podcast pour rester informé des dernières tendances et meilleures pratiques dans le paysage numérique.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Christine McMonigal","Darren W Pulsipher"],"link":"/episode-EDT162-fr","image":"./episodes/edt-162/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe Christine McMonigal et discute des défis auxquels les organisations sont confrontées lors de leur transition vers le cloud et l'adoption d'architectures multi-cloud hybrides. Ils soulignent l'importance de comprendre ces obstacles et de fournir des orientations pour les surmonter. Cet épisode explorera plus en détail quelques obstacles clés et des stratégies pour atténuer les risques, garantissant ainsi une transformation réussie vers le cloud."},{"id":63,"type":"Episode","title":"Développer un modèle d'exploitation Multi-Hybrid Cloud","tags":["cloudstrategy","digitaltransformation","collmultihybridcloud","cloudtechnologies","businessgoals","operationalefficiency","customersatisfaction","itinfrastructure","migrationplan","datasecurity","regulatorycompliance","hybridclouds","publicclouds","privateclouds","clouddeploymentmodels","governanceandsecurity","reducecosts","enhanceefficiency","customerexperience","strategicmove","clearvision","embracingdigital","edt163"],"body":"\r\n\r\nÀ l'ère du numérique d'aujourd'hui, les entreprises se tournent de plus en plus vers le cloud en tant que démarche stratégique visant à améliorer l'efficacité, réduire les coûts et améliorer l'expérience client. Cependant, avant de se précipiter dans le mouvement du cloud, il est essentiel que les organisations prennent du recul et évaluent leurs besoins spécifiques. La mise en place d'une stratégie cloud est une étape cruciale de ce processus, car elle permet aux entreprises d'aligner leurs objectifs avec les technologies cloud disponibles pour elles.\n\n## Comprendre vos objectifs et buts commerciaux.\n\nLa première étape dans l'élaboration d'une stratégie de cloud consiste à comprendre clairement vos objectifs et vos objectifs commerciaux. Que cherchez-vous à réaliser? Cherchez-vous à améliorer l'efficacité opérationnelle, réduire les coûts ou accroître la satisfaction des clients? En ayant une vision claire de vos objectifs, vous pouvez mieux déterminer comment le cloud peut soutenir et permettre ces objectifs.\n\n## Évaluer votre infrastructure existante\n\nAprès avoir établi vos objectifs, il est important d'évaluer votre infrastructure informatique actuelle. Cette évaluation permet d'identifier les éventuels défis ou limitations qui pourraient survenir lors de la migration vers le cloud. Déterminez quels systèmes et applications vous avez actuellement en place et considérez leur compatibilité avec un environnement cloud. Cette évaluation vous permettra de prendre des décisions concernant les applications et services adaptés à la migration.\n\n## Choisir le bon modèle de Cloud\n\nAvec différents modèles de déploiement dans le cloud disponibles, les organisations doivent évaluer les différentes options qui sont conformes à leurs besoins commerciaux. Les clouds publics, privés et hybrides offrent chacun des avantages et des inconvénients distincts. Évaluer les avantages et les inconvénients de chaque modèle vous aidera à déterminer le choix le plus approprié pour votre organisation. Prenez en compte des facteurs tels que la sécurité des données, la scalabilité et la conformité réglementaire lors de la prise de cette décision.\n\n## Création d'un plan de migration et garantie de la gouvernance et de la sécurité.\n\nUne fois que vous avez choisi un modèle de cloud, il est temps de créer un plan de migration. Cela implique de définir les étapes et le calendrier pour déplacer vos applications et vos données vers le cloud. Donnez la priorité aux applications critiques qui doivent être migrées en premier, et développez une stratégie pour migrer les autres applications ultérieurement. De plus, mettez en place un plan de gouvernance et de sécurité pour protéger vos données et vous conformer à toutes les exigences réglementaires. La sécurité du cloud est une préoccupation majeure pour de nombreuses entreprises, il est donc essentiel de s'assurer que vos données sont protégées tout au long du processus de migration.\n\nEn conclusion, élaborer une stratégie cloud est un processus complexe qui nécessite une planification et une évaluation minutieuses. Il est essentiel de comprendre vos objectifs commerciaux, d'évaluer votre infrastructure existante, de choisir le bon modèle de cloud, d'établir un plan de migration et de mettre en place des mesures adéquates de gouvernance et de sécurité. En adoptant efficacement la transformation numérique et en tirant parti de la puissance du cloud, les organisations peuvent atteindre leurs objectifs, améliorer leur efficacité et favoriser la croissance et le succès.\n\nÀ l'ère numérique d'aujourd'hui, les entreprises se tournent de plus en plus vers le cloud en tant que décision stratégique visant à améliorer l'efficacité, réduire les coûts et améliorer l'expérience client. Cependant, avant de sauter dans le train en marche du cloud, il est essentiel que les organisations prennent du recul et évaluent leurs besoins spécifiques. Développer une stratégie cloud est une étape cruciale de ce processus, car elle permet aux entreprises d'aligner leurs objectifs avec les technologies cloud disponibles pour elles.\n\n## Comprendre vos objectifs et vos objectifs commerciaux\n\nLa première étape dans la mise en place d'une stratégie de cloud consiste à comprendre clairement vos objectifs et buts commerciaux. Que cherchez-vous à atteindre? Cherchez-vous à améliorer l'efficacité opérationnelle, réduire les coûts ou améliorer la satisfaction client? En ayant une vision claire de vos objectifs, vous pourrez mieux déterminer comment le cloud peut soutenir et permettre la réalisation de ces objectifs.\n\n## Évaluer votre infrastructure existante\n\nAprès avoir établi vos objectifs, il est important d'évaluer votre infrastructure informatique actuelle. Cette évaluation permet d'identifier d'éventuels défis ou limitations potentiels liés à la migration vers le cloud. Déterminez les systèmes et les applications que vous avez actuellement en place et évaluez leur compatibilité avec un environnement cloud. Cette évaluation vous aidera à prendre des décisions concernant les applications et les services adaptés à la migration.\n\n## Choisir le bon modèle de Cloud\n\nAvec différents modèles de déploiement dans le cloud disponibles, les organisations doivent évaluer les différentes options qui correspondent à leurs besoins commerciaux. Les clouds publics, privés et hybrides offrent chacun des avantages et des inconvénients distincts. Évaluer les avantages et les inconvénients de chaque modèle vous aidera à déterminer le choix le plus approprié pour votre organisation. Prenez en compte des facteurs tels que la sécurité des données, la scalabilité et la conformité réglementaire lors de la prise de cette décision.\n\n## Créer un plan de migration et garantir la gouvernance et la sécurité.\n\nUne fois que vous avez choisi un modèle de cloud, il est temps de créer un plan de migration. Cela implique de détailler les étapes et le calendrier pour déplacer vos applications et données vers le cloud. Vous devriez donner la priorité aux applications critiques qui doivent être migrées en premier, et élaborer une stratégie pour migrer les autres applications ultérieurement. De plus, mettez en place un plan de gouvernance et de sécurité pour protéger vos données et vous conformer aux exigences réglementaires. La sécurité du cloud est une préoccupation majeure pour de nombreuses entreprises, il est donc essentiel de veiller à ce que vos données soient protégées tout au long du processus de migration.\n\nEn conclusion, élaborer une stratégie cloud est un processus complexe qui nécessite une planification et une évaluation minutieuses. Il est essentiel de comprendre les objectifs de votre entreprise, d'évaluer votre infrastructure existante, de choisir le bon modèle cloud, d'établir un plan de migration et de mettre en place des mesures appropriées de gouvernance et de sécurité. En adoptant efficacement la transformation numérique et en exploitant la puissance du cloud, les organisations peuvent atteindre leurs objectifs, améliorer leur efficacité et stimuler leur croissance et leur réussite.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Rajiv Mandal","Darren W Pulsipher"],"link":"/episode-EDT163-fr","image":"./episodes/edt-163/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe Rajiv Mandal, architecte de solutions cloud, sur le développement d'une stratégie multi-hybrid cloud dans votre organisation informatique moderne."},{"id":64,"type":"Episode","title":"Portefolios d'application et de charge de travail dans la migration vers le cloud","tags":["cloudmigration","organizations","collmultihybridcloud","cloudnative","datacenterdependencies","hybridstrategy","technicaldebt","applications","workloads","politicalcapital","customerfacingapplications","importance","applicationrationalization","analysis","dependencies","smoothtransition","surprises","decision","compliancerequirements","regulatedindustries","compliancemonitoring","embracingdigital","edt164"],"body":"\r\n\r\n## Comprendre les portefeuilles d'applications et de charges de travail dans la migration vers le cloud\n\nEn ce qui concerne la migration vers le cloud, les organisations se répartissent généralement en deux groupes. Le premier groupe est constitué d'organisations nées dans le cloud qui ont conçu leurs applications dans le cloud, éliminant ainsi toutes les dépendances vis-à-vis des data centers. Le deuxième groupe adopte une stratégie hybride, en s'appuyant à la fois sur le data center et le cloud. Cependant, même ces organisations hybrides peuvent avoir une dette technique qu'il est nécessaire de résoudre.\n\nL'un des principaux défis de la migration vers le cloud est de comprendre la complexité des applications et des charges de travail. Sarah introduit le concept de \"capital politique\" qu'une application porte. Alors que les applications orientées vers l'extérieur et axées sur les clients reçoivent souvent le plus d'attention et d'investissement, les petites applications qui peuvent ne pas sembler significatives peuvent avoir un impact important sur l'organisation si elles dysfonctionnent ou sont négligées.\n\n## L'importance de la rationalisation des applications\n\nSarah partage une expérience personnelle qui met en évidence l'importance de prendre en compte l'ensemble du portefeuille d'applications et de charges de travail lors de la migration vers le cloud. Elle a été témoin d'une perturbation de l'activité causée par le manque d'attention accordée à une application en apparence mineure pour les clients. Cette expérience souligne la nécessité pour les organisations de mener une analyse approfondie et une rationalisation de leur portefeuille d'applications avant de migrer vers le cloud.\n\nEn comprenant les complexités et dépendances des applications et des charges de travail, les organisations peuvent garantir une transition en douceur vers le cloud avec moins de surprises ou de perturbations. Sarah met l'accent sur la nécessité pour les organisations de donner la priorité à la rationalisation des applications afin d'identifier les applications critiques qui peuvent nécessiter des investissements et une attention supplémentaires, même si elles ne sont pas les plus visibles.\n\n## Toucher ou ne pas toucher : Évaluer les charges de travail pour la migration vers le cloud.\n\nAlors que la migration des charges de travail vers le cloud peut apporter de nombreux avantages, il n'est pas toujours nécessaire ou bénéfique de modifier certaines charges de travail ou applications. Certaines charges de travail peuvent fonctionner harmonieusement depuis des années et être cruciales pour les opérations de l'organisation. Dans de tels cas, il peut ne pas être judicieux d'apporter des modifications ou de les migrer vers le cloud.\n\nLes facteurs à prendre en compte lors de la prise de décision comprennent le niveau de personnalisation et d'intégration de la charge de travail, la présence de dettes techniques et la prochaine retraite des systèmes hérités. Cependant, il est essentiel de réévaluer régulièrement ces charges de travail pour s'assurer qu'elles continuent à répondre aux besoins de l'organisation. La surveillance des tendances de l'industrie et des avancées technologiques peut aider à identifier d'éventuels changements à l'avenir.\n\n## Naviguer dans les exigences de conformité lors de la migration vers le cloud.\n\nLes exigences de conformité peuvent poser des défis lors de la migration vers le cloud, en particulier pour les organisations des industries réglementées. Cependant, les fournisseurs de services cloud ont réalisé d'importants progrès pour répondre à ces préoccupations. Ils proposent des outils et des services qui facilitent la surveillance et le reporting automatisés de la conformité, ce qui allège la charge imposée aux organisations pour rester en conformité.\n\nPour faire face à ces défis, les organisations devraient effectuer une évaluation approfondie de leurs exigences en matière de conformité. Consulter des experts capables de fournir des conseils sur les normes de conformité et concevoir une architecture cloud qui satisfait à ces exigences est crucial. Des audits réguliers et une surveillance doivent être mis en place pour assurer une conformité continue.\n\n## Conclusion\n\nDans cet épisode de podcast, Darren Pulsipher et Sarah Musick mettent en lumière des aspects importants de la migration vers le cloud, notamment la rationalisation des portefeuilles d'applications, la prise de décision concernant les charges de travail à modifier et la prise en compte des exigences de conformité. En comprenant ces facteurs et en gérant activement la dette technique, les organisations peuvent se lancer dans un voyage de migration vers le cloud réussi, en tirant parti de l'agilité et de la flexibilité offertes par le cloud tout en minimisant les risques et les perturbations.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Sarah Musick","Darren W Pulsipher"],"link":"/episode-EDT164-fr","image":"./episodes/edt-164/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe Sarah Musick, architecte de solutions Cloud chez Intel. Ensemble, ils se plongent dans le sujet des portefeuilles d'applications et de charges de travail dans la migration vers le cloud. Avec son expérience en consultation et optimisation Cloud, Sarah apporte des perspectives précieuses à la discussion."},{"id":65,"type":"Episode","title":"Facteurs de placement de la charge de travail dans le cloud","tags":["cloudcomputeoptions","collmultihybridcloud","cloudinstance","cloudspecialistadvice","computeselectiontools","optimizecloud","workloadcloud","vendorlockinprevention","cloudbestpractices","cloudinstanceperformance","costeffectivecomputesolutions","workloadoptimization","resourceefficiency","cloud","multihybridcloud","embracingdigital","edt165"],"body":"\r\n\r\n## Comprendre les options de calcul\n\nLes fournisseurs de services cloud (CSP) proposent un mélange de différentes familles de calcul, allant des générations plus anciennes de matériel de calcul aux dernières instances plus performantes. Ces générations plus anciennes sont souvent utilisées pour des fonctions de calcul rentables, tandis que les générations plus récentes offrent des performances améliorées à des prix similaires ou inférieurs.\n\nNaviguer à travers les nombreuses options informatiques disponibles dans le cloud peut être écrasant, surtout avec de nouvelles instances qui sont régulièrement publiées. C'est là que les spécialistes du cloud, tels que ceux d'Intel, interviennent. Ces experts peuvent fournir des informations précieuses et aider à sélectionner l'instance la plus adaptée pour une charge de travail spécifique.\n\n## Prendre des décisions éclairées\n\nPour prendre la meilleure décision, recherchez les conseils de spécialistes du cloud ou utilisez des outils comme Densify ou Intel Site Optimizer. Ces outils exploitent l'apprentissage automatique pour analyser les fonctionnalités d'une application, l'utilisation des ressources de calcul et les besoins du réseau afin de déterminer la taille d'instance la plus adaptée. En tirant parti de ces ressources, les organisations peuvent s'assurer d'exploiter au mieux leurs ressources cloud, en évitant une sous-utilisation ou des dépenses excessives.\n\n## Mise en œuvre des meilleures pratiques.\n\nIl est important d'intégrer les recommandations d'instance dans les scripts d'infrastructure en tant que code (IaC), tels que TerraForm, afin d'automatiser la sélection de l'instance la plus performante pour une charge de travail. Cela garantit un placement d'instance cohérent et efficace, éliminant ainsi le risque d'erreur humaine et optimisant les performances.\n\n## En tenant compte de la portabilité\n\nAlors qu'Intel domine actuellement le marché du cloud avec des instances basées sur x86, il existe une certaine concurrence de la part d'AMD et ARM. Les processeurs basés sur ARM, tels que le Graviton, sont populaires parmi les fournisseurs de services cloud (CSP), mais nécessitent une plus grande portabilité des charges de travail entre les fournisseurs et entre les environnements publics et privés. Pour porter des charges de travail basées sur x86 vers ARM, il faudrait effectuer un refactoring et une refonte du code étendus.\n\nLes organisations doivent tenir compte des problèmes de compatibilité lorsqu'elles rapatrient les charges de travail du cloud vers une infrastructure sur site. Il est crucial d'évaluer la portabilité et la flexibilité de la plateforme informatique choisie afin d'assurer des transitions fluides et d'éviter le verrouillage par un seul fournisseur.\n\n## Conclusion\n\nSélectionner la bonne instance cloud est une décision critique qui peut avoir un impact sur les performances, les coûts et la portabilité de votre charge de travail. Grâce à l'aide de spécialistes et d'outils cloud, les organisations peuvent prendre des décisions éclairées et optimiser l'utilisation de leurs ressources cloud. En comprenant les options de calcul disponibles, en intégrant les meilleures pratiques et en tenant compte de la portabilité, les entreprises peuvent exploiter pleinement le potentiel du cloud tout en garantissant la flexibilité et l'efficacité de leurs opérations.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Ricardo Dutton Jr","Darren W Pulsipher"],"link":"/episode-EDT165-fr","image":"./episodes/edt-165/fr/thumbnail.png","lang":"fr","summary":"Dans ce podcast, Darren et Rico Dutton plongent dans le monde des instances cloud et des facteurs à prendre en compte lors de la sélection de la bonne instance pour votre charge de travail. Ils discutent des différentes options de calcul disponibles dans le cloud, de l'importance de trouver le bon équilibre entre les performances et les coûts, ainsi que du rôle des spécialistes du cloud dans l'aide aux organisations pour prendre des décisions éclairées."},{"id":66,"type":"Episode","title":"Agilité dans l'adoption du Cloud","tags":null,"body":"\r\n\r\n## Migration vers le Cloud en tant que parcours continu\n\nAlors que de nombreuses personnes considèrent la migration vers le cloud comme un processus ponctuel, il est essentiel de la considérer comme un voyage continu, où les développeurs et les équipes opérationnelles travaillent ensemble. Une fois que les charges de travail sont modernisées et déployées, une surveillance et une évaluation constantes sont nécessaires pour déterminer si elles répondent aux objectifs commerciaux et aux indicateurs de réussite.\n\nEn traitant la migration vers le cloud comme un voyage continu, les organisations peuvent permettre à leurs équipes d'itérer, de peaufiner et d'améliorer leur réussite. Cette approche permettra d'obtenir de l'agilité, de l'adaptabilité et la capacité de répondre aux besoins commerciaux en constante évolution.\n\n## Réinternaliser les charges de travail et la flexibilité.\n\nUn aspect important à prendre en compte est la possibilité de migrer les charges de travail vers les infrastructures locales si les avantages attendus du cloud ne sont pas atteints ou s'il est nécessaire de passer d'un prestataire cloud à un autre. Pour parvenir à une amélioration continue, il est nécessaire d'évaluer en continu la situation, de fixer des attentes dès le départ et d'être agile et flexible dans le modèle d'exploitation du cloud.\n\nUne infrastructure cohérente sur plusieurs clouds est essentielle pour permettre la flexibilité et l'agilité. Bien que les fournisseurs de services cloud puissent essayer de restreindre les clients à leurs services, les organisations devraient résister à cette tentation et viser la cohérence entre les clouds ou être prêtes à effectuer les changements nécessaires lors du déplacement des charges de travail vers différents emplacements.\n\n## Outils et meilleures pratiques pour l'optimisation\n\nOptimiser les environnements cloud peut être complexe et chronophage, nécessitant une expertise et des ressources. Les outils et meilleures pratiques d'Intel peuvent aider les organisations à évaluer et optimiser le placement des charges de travail, et fournir une optimisation continue en temps réel sans affecter les applications. En automatisant certains aspects du processus d'optimisation, ces outils peuvent permettre aux organisations de gagner du temps et de l'argent tout en améliorant les performances globales.\n\nPour maximiser les avantages de ces outils, il est crucial de catégoriser les charges de travail en différents compartiments en fonction de facteurs tels que la standardisation, la criticité et l'expérimentation. Par exemple, les charges de travail nécessitant une disponibilité élevée et une latence faible peuvent nécessiter une infrastructure dédiée, tandis que celles qui sont moins critiques peuvent être placées sur une infrastructure partagée. Les organisations peuvent adopter une approche ciblée pour l'optimisation afin de s'assurer que leur environnement cloud est adapté à leurs besoins et objectifs spécifiques.\n\n## Adoption de la transformation numérique et migration vers le cloud.\n\nLa pertinence du changement organisationnel et l'apprentissage des méthodes réussies et infructueuses est également mise en évidence dans cet épisode. Pour aider les organisations dans leur processus de migration vers le cloud, des ressources précieuses et des conseils peuvent être trouvés sur embracingdigital.org.\n\nEn conclusion, en mettant en œuvre l'amélioration continue, en développant une approche stratégique et en acceptant le changement organisationnel, les organisations peuvent optimiser leur environnement cloud, favoriser l'efficacité et atteindre leurs objectifs commerciaux. Adopter l'amélioration continue dans les opérations cloud et considérer la migration vers le cloud comme un voyage continu est la clé d'une migration réussie vers le cloud.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Christine McMonigal","Darren W Pulsipher"],"link":"/episode-EDT166-fr","image":"./episodes/edt-166/fr/thumbnail.png","lang":"fr","summary":"La migration vers le cloud n'est plus un processus unique, mais plutôt un voyage continu qui nécessite une évaluation, une surveillance et un ajustement constants pour atteindre les objectifs commerciaux. Dans cet épisode de notre podcast, l'animateur Darren Pulsipher discute avec l'invitée Christine McMonigal de l'importance d'adopter l'amélioration continue dans les opérations cloud."},{"id":67,"type":"Episode","title":"Exploiter l'IA pour protéger les enfants","tags":["ai","aipolicy","lawenforcement","aiethics","collgenerativeai","childprotection","aiforgood","chainofcustody","policy","people","embracingdigital","edt167"],"body":"\r\n\r\n## Défis de la poursuite des prédateurs d'enfants en ligne:\n\nL'un des défis majeurs dans la poursuite des prédateurs d'enfants en ligne réside dans le manque d'uniformité entre les juridictions en ce qui concerne la technologie et les crimes en ligne. Cela crée des obstacles conséquents pour les agences chargées de l'application de la loi et un fossé dans leur capacité à poursuivre et à enquêter efficacement sur les affaires. Chaque juridiction fonctionne différemment, avec son propre ensemble de lois, de réglementations et de procédures. Malheureusement, ces différences peuvent embrouiller et rendre difficile l'enquête et la poursuite des prédateurs sexuels en ligne. Souvent, les enquêtes traditionnelles ne suffisent pas à attraper les prédateurs en ligne. Le monde numérique a créé une nouvelle catégorie de criminels doués en technologie qui peuvent effacer leurs traces.\n\nLes organismes chargés de l'application des lois doivent être équipés des ressources, de la technologie et de la formation nécessaires pour lutter efficacement contre les prédateurs sexuels en ligne. La collaboration entre les entreprises de technologie et les forces de l'ordre est essentielle pour élaborer des pratiques et des protocoles standardisés en matière de poursuites judiciaires et d'enquêtes. En comblant cette lacune, nous pouvons améliorer l'efficacité de ces processus et augmenter les chances de traduire les prédateurs d'enfants en justice. De plus, le public doit être informé des risques et des dangers associés aux prédateurs en ligne. Les parents, les éducateurs et les tuteurs doivent apprendre aux enfants comment se protéger en ligne et que faire en cas de contenu ou de communication inappropriés.\n\n## Le rôle de l'IA dans la gestion des preuves.\n\nLes technologies de l'IA peuvent être vitales dans la gestion de preuves numériques, en particulier dans les affaires impliquant des prédateurs d'enfants. L'IA peut aider à automatiser l'analyse, la signalisation et le traitement de contenus illicites. Les outils d'IA peuvent également contribuer à réduire la charge de travail des enquêteurs, leur permettant de se concentrer sur les affaires prioritaires. Cependant, de nombreux défis persistent dans la mise en œuvre et la compréhension de ces technologies dans différentes juridictions. L'un des principaux défis est que l'IA n'est aussi performante que les données sur lesquelles elle est entraînée, et ces données peuvent varier d'une juridiction à l'autre. Par conséquent, il est difficile de développer des modèles d'IA efficaces qui peuvent fonctionner dans différentes juridictions.\n\nPour garantir une gestion efficace des preuves, les parties prenantes du système judiciaire doivent travailler ensemble pour adopter et tirer parti des outils d'intelligence artificielle. La collaboration entre les technologues, les agences de maintien de l'ordre et les systèmes judiciaires est essentielle pour surmonter ces défis et tirer pleinement parti de l'IA afin de protéger les enfants en ligne. La mise en œuvre de l'IA dans la gestion des preuves doit être soutenue par des politiques solides et des directives qui protègent la vie privée des victimes et garantissent l'utilisation éthique de ces technologies. De plus, une formation régulière et une éducation sur ces outils sont essentielles pour garantir leur utilisation efficace dans la lutte contre les prédateurs sexuels en ligne.\n\n## Collaboration et normalisation pour une protection efficace\n\nLa collaboration et la standardisation sont des aspects essentiels pour lutter efficacement contre l'exploitation des enfants en ligne. La lutte contre ce crime odieux nécessite la coopération entre les fournisseurs de technologies, les forces de l'ordre et les systèmes judiciaires. Ces parties doivent travailler ensemble pour développer des stratégies et des solutions complètes.\n\nLa collaboration ne devrait pas seulement se concentrer sur les aspects techniques, mais aussi sur le développement de pratiques et de protocoles normalisés pour la gestion des affaires impliquant des prédateurs d'enfants. En établissant un langage et des processus cohérents, nous pouvons rationaliser les enquêtes, accélérer les procédures judiciaires et améliorer la protection globale des enfants dans l'espace numérique.\n\nDe plus, les pratiques et protocoles standardisés devraient être constamment examinés et mis à jour pour rester pertinents et praticables. Établir une norme mondiale pour lutter contre l'exploitation des enfants en ligne fournirait un cadre que tous les acteurs pourraient suivre, garantissant que chaque cas est traité de manière cohérente et équitable, indépendamment de l'endroit où il se produit.\n\n## Tirer parti de l'IA pour protéger les enfants en ligne\n\nUtiliser l'intelligence artificielle (IA) dans la gestion des preuves est crucial pour lutter efficacement contre l'exploitation des enfants en ligne. Le volume considérable de preuves numériques peut être accablant pour les enquêteurs, mais l'IA peut les aider en automatisant l'identification et l'analyse des preuves potentielles. Cette automatisation libère du temps aux enquêteurs et leur permet de se concentrer sur les aspects les plus critiques de l'enquête.\n\nCependant, la mise en œuvre de l'IA dans la gestion des preuves requiert une réflexion minutieuse. Il doit y avoir transparence et responsabilité dans la façon dont l'IA est utilisée et détermine ce qui relève ou non de la preuve. De plus, les préoccupations éthiques concernant l'utilisation de l'IA dans les forces de l'ordre doivent être traitées, telles que les biais potentiels dans les algorithmes.\n\n## Conclusion\n\nEn conclusion, la collaboration, la normalisation et l'utilisation de l'IA dans la gestion des preuves sont des étapes cruciales vers un environnement numérique plus sûr pour les enfants. Pour remédier à la désorganisation et au manque d'uniformité dans le domaine de la technologie et des crimes en ligne, il faudra un effort collectif de tous les acteurs concernés. En relevant ces défis ensemble, nous pouvons accomplir des progrès significatifs dans la lutte contre l'exploitation des enfants et garantir leur bien-être à l'ère numérique.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Rachel Dreikosen","Darren W Pulsipher"],"link":"/episode-EDT167-fr","image":"./episodes/edt-167/fr/thumbnail.jpg","lang":"fr","summary":"Dans un podcast récent, Darren Pulsipher, Architecte Principal des Solutions du Secteur Public chez Intel, a accueilli Rachel Driekosen, Directrice Technique chez Intel, pour discuter de l'utilisation de l'IA dans la protection des enfants en ligne. L'épisode aborde les défis liés aux poursuites et à la découverte des prédateurs d'enfants, le rôle de l'IA dans la gestion des preuves, ainsi que l'importance de la collaboration et des pratiques standardisées."},{"id":68,"type":"Episode","title":"Génération AI quotidienne","tags":["generativeai","firefly","videogen","figma","enterpriseaistrategy","ai","creativecontentgeneration","enhancingdailytasks","powerofgenerativeai","searchintent","contentcreationautomation","chatbotsincontentcreation","embracingdigital","edt168"],"body":"\r\n\r\n## Déchaîner la créativité et la productivité avec des outils d'IA générative.\n\nDans cet épisode de podcast, Darren Pulsipher interviewe Andy Morris, responsable de la stratégie IA d'entreprise chez Intel, sur l'impact de l'IA générative dans la vie quotidienne. L'IA générative utilise l'intelligence artificielle pour générer de nouveaux contenus, tels que des images, du texte et de la musique. La conversation tourne autour des différents outils d'IA générative et de leur potentiel pour révolutionner les industries et améliorer les tâches quotidiennes.\n\n## Le pouvoir de l'IA générative dans la génération de contenu.\n\nSelon Andy Morris, les outils d'IA générative deviennent de plus en plus importants dans divers secteurs. Il recommande de commencer par utiliser des moteurs de recherche qui intègrent des technologies d'IA ouverte pour explorer l'IA générative. Ces outils peuvent améliorer les résultats de recherche en fournissant un contenu plus pertinent et créatif. Cependant, il est crucial de prendre en compte l'intention de recherche lors de l'utilisation de ces outils, car ils ne génèrent pas toujours les résultats souhaités pour des informations spécifiques.\n\nL'intelligence artificielle générative fait également sentir sa présence dans la création de contenu. Les chatbots, par exemple, ont connu une croissance explosive et sont utilisés pour rédiger des essais, créer du contenu et améliorer des photos. Que vous soyez un créateur de contenu ou un étudiant, les outils d'IA générative peuvent automatiser certaines étapes du processus de création de contenu, ce qui augmente ainsi la créativité et la productivité.\n\n## Outils innovants pour la génération d'images et de vidéos\n\nDeux outils passionnants sont Adobe Firefly et VideoGen Video creation. Ces outils permettent aux utilisateurs de créer et manipuler des images et des vidéos de manière unique et créative.\n\nAdobe Firefly est un outil gratuit qui permet aux utilisateurs de générer de nouvelles images et de remplacer des éléments dans des photos existantes. Ses fonctionnalités de remplissage générationnel et de remplissage externe permettent aux utilisateurs de modifier ou de remplacer des parties d'une image, élargissant ainsi les possibilités créatives. D'un autre côté, Video Gen Video se concentre sur la génération de vidéos en utilisant des scripts ou des pages web existantes comme matériel source. Cet outil alimenté par l'IA simplifie la création de vidéos captivantes en sélectionnant et en insérant automatiquement des images et des extraits vidéo pertinents.\n\nCes outils innovants offrent toute une gamme de possibilités tant pour les professionnels que pour les utilisateurs du quotidien. Ils donnent accès à des fonctionnalités avancées en matière d'édition, permettant aux utilisateurs d'ajouter une touche de créativité à leurs projets sans nécessiter de compétences approfondies ou de connaissances en logiciel d'édition.\n\n## Optimisation de la création de contenu avec l'IA générative\n\nDivers outils tels que VideoGen, Figma et Framer.AI ont rendu la création de contenu plus pratique et efficace dans différents domaines.\n\nVideoGen peut créer des vidéos basées sur le contenu d'un article ou d'un billet de blog. Il y parvient en utilisant des bibliothèques existantes d'images et de clips vidéo, automatisant ainsi le processus de création de vidéos attrayantes qui racontent une histoire. Figma, un outil de conception graphique en ligne, offre une plus grande flexibilité de conception en permettant aux utilisateurs de créer des modèles personnalisés. De même, Framer.AI simplifie la création de sites web en exploitant la technologie de l'IA, permettant aux utilisateurs de générer et de publier rapidement des sites web.\n\nBien que les outils d'IA générative offrent commodité et efficacité dans la création de contenu, il est nécessaire de faire appel à l'expertise humaine dans certains aspects créatifs. Les éléments de design et les considérations esthétiques profitent toujours de l'apport humain pour garantir des résultats visuellement plaisants. Bien que les outils d'IA générative puissent automatiser les parties moins qualifiées du marché, les applications sophistiquées nécessitent souvent une touche humaine.\n\nEn conclusion, les outils d'IA générative transforment les tâches quotidiennes et révolutionnent la création de contenu. Des moteurs de recherche suralimentés par l'IA aux puissants outils développés par Adobe et d'autres entreprises, ces technologies débloquent de nouveaux niveaux de créativité et d'efficacité. Adopter l'IA générative devient de plus en plus crucial pour les particuliers et les entreprises afin de rester compétitifs sur un marché du travail en évolution. En maîtrisant ces outils et en exploitant leurs capacités, les individus peuvent obtenir un avantage concurrentiel et ouvrir des opportunités de services de conseil et de personnalisation. L'avenir est prometteur pour l'IA générative, et il est temps d'explorer et d'adopter ces outils innovants.\n\n## Libérer la créativité et la productivité grâce à des outils d'IA générative.\n\nL'intelligence artificielle générative utilise l'intelligence artificielle pour générer du nouveau contenu, comme des images, du texte et de la musique. La conversation tourne autour des différents outils d'intelligence artificielle générative et de leur potentiel à révolutionner les industries et améliorer les tâches quotidiennes.\n\n## Le pouvoir de l'IA générative dans la génération de contenu\n\nSelon Andy Morris, les outils d'IA générative sont de plus en plus importants dans diverses industries. Il recommande de commencer par utiliser des moteurs de recherche qui intègrent des technologies d'IA ouverte pour explorer l'IA générative. Ces outils peuvent améliorer les résultats de recherche en fournissant du contenu plus pertinent et créatif. Cependant, il est essentiel de prendre en compte l'intention de recherche lors de l'utilisation de ces outils, car ils ne génèrent pas toujours les résultats souhaités pour des informations spécifiques.\n\nL'IA générative fait également sa marque dans la création de contenu. Les chatbots, par exemple, ont connu une croissance explosive et sont utilisés pour rédiger des essais, créer du contenu et améliorer des photos. Que vous soyez un créateur de contenu ou un étudiant, les outils d'IA générative peuvent automatiser certains aspects du processus de création de contenu, augmentant ainsi la créativité et la productivité.\n\n## Outils innovants pour la génération d'images et de vidéos\n\nDeux outils passionnants sont Adobe Firefly et VideoGen Video creation. Ces outils permettent aux utilisateurs de créer et de manipuler des images et des vidéos de manière unique et créative.\n\nAdobe Firefly est un outil gratuit qui permet aux utilisateurs de générer de nouvelles images et de remplacer des éléments dans des photos existantes. Ses fonctionnalités de remplissage automatique et de remplissage externe permettent aux utilisateurs de modifier ou de remplacer des parties d'une image, ce qui élargit les possibilités créatives. En revanche, Video Gen Video se concentre sur la génération de vidéos en utilisant des scripts existants ou des pages web comme matériel source. Cet outil alimenté par l'IA simplifie la création de vidéos attrayantes en sélectionnant et en insérant automatiquement des images et des clips vidéo pertinents.\n\nCes outils innovants offrent toute une gamme de possibilités tant pour les professionnels que pour les utilisateurs ordinaires. Ils offrent un accès aux fonctionnalités avancées d'édition, permettant aux utilisateurs d'ajouter une touche de créativité à leurs projets sans nécessiter de compétences approfondies ou de connaissances en logiciels d'édition.\n\n## Optimisation de la création de contenu grâce à l'IA générative.\n\nDivers outils tels que VideoGen, Figma et Framer.AI ont rendu la création de contenu plus pratique et efficace dans différents domaines.\n\nVideoGen peut créer des vidéos basées sur le contenu d'un article ou d'un billet de blog. Pour ce faire, il utilise des bibliothèques existantes d'images et de clips vidéo, automatisant ainsi le processus de création de vidéos captivantes qui racontent une histoire. Figma, un outil de conception graphique en ligne, offre une plus grande flexibilité de conception en permettant aux utilisateurs de créer des modèles personnalisés. De même, Framer.AI facilite la création de sites web en exploitant la technologie de l'IA, ce qui permet aux utilisateurs de générer et de publier rapidement des sites web.\n\nBien que les outils d'IA générative offrent praticité et efficacité dans la création de contenu, il est nécessaire de faire appel à l'expertise humaine dans certains aspects créatifs. Les éléments de conception et les considérations esthétiques bénéficient toujours de la contribution humaine afin de garantir des résultats visuellement plaisants. Alors que les outils d'IA générative peuvent automatiser les parties moins qualifiées du marché, les applications sophistiquées demandent souvent une touche humaine.\n\nEn conclusion, les outils d'intelligence artificielle générative transforment les tâches quotidiennes et révolutionnent la création de contenu. Des moteurs de recherche sur boostés par l'IA aux puissants outils développés par Adobe et d'autres entreprises, ces technologies permettent de débloquer de nouveaux niveaux de créativité et d'efficacité. Il est de plus en plus crucial pour les individus et les entreprises de s'approprier l'IA générative afin de rester compétitifs sur le marché du travail en constante évolution. En devenant compétents dans ces outils et en exploitant leurs capacités, les individus peuvent acquérir un avantage concurrentiel et ouvrir des opportunités de services de conseil et de personnalisation. L'avenir est prometteur pour l'IA générative et c'est maintenant le moment d'explorer et d'adopter ces outils innovants.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Andy Morris","Darren W Pulsipher"],"link":"/episode-EDT168-fr","image":"./episodes/edt-168/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode de podcast, Darren Pulsipher interviewe Andy Morris, responsable de la stratégie d'IA d'entreprise chez Intel, à propos de l'impact de l'IA générative sur la vie quotidienne."},{"id":69,"type":"Episode","title":"Garder l'humain dans l'IA","tags":["userexperience","humancentereddesign","ai","trust","useradoption","transparency","generativeai","jobdisplacement","opencommunication","humansupport","collgenerativeai"],"body":"\r\n\r\n## Privilégier l'expérience utilisateur grâce à une conception centrée sur l'humain\n\nSunny Stueve, une ingénieure en facteurs humains, met en évidence l'importance d'optimiser l'expérience humaine et les performances du système lors du développement de solutions d'intelligence artificielle. Elle souligne la nécessité d'avoir une valeur et un plan avant de se lancer dans la programmation. En intégrant dès le départ des principes de conception centrés sur l'humain, les organisations peuvent donner la priorité à la perspective de l'utilisateur et garantir une meilleure expérience globale. Le rôle de Sunny consiste à comprendre les besoins des utilisateurs et à les intégrer dans le processus de conception afin de minimiser la nécessité de refaire le code et de maximiser l'efficacité des solutions d'intelligence artificielle.\n\nDarren partage une anecdote de son expérience de travail avec des radiologues, soulignant la valeur de s'asseoir avec les clients et de comprendre leurs besoins avant de développer des logiciels. Cette rencontre personnelle met en évidence l'importance de prendre en compte les facteurs humains lors du développement de solutions technologiques. En adoptant une approche centrée sur l'utilisateur, les organisations peuvent créer des solutions d'intelligence artificielle adaptées aux besoins des utilisateurs, ce qui se traduit par des taux d'adoption plus élevés et une satisfaction accrue.\n\n## Aborder la confiance et l'adoption par les utilisateurs dans l'intégration de l'IA.\n\nSunny explique ensuite que l'intégration de l'IA crée un changement de paradigme dans l'adoption et la confiance des utilisateurs. Tout en suivant un processus de découverte approfondi qui implique la collecte de données qualitatives et quantitatives, l'établissement de relations et la validation des hypothèses, il est essentiel de reconnaître que l'introduction de l'IA peut susciter la peur et des obstacles de confiance plus élevés. Les êtres humains sont des créatures d'habitude et de patterns, il est donc crucial d'éduquer les utilisateurs et de bâtir la confiance pour surmonter la résistance au changement.\n\nPour résoudre le problème de confiance, la transparence est essentielle. Fournir aux utilisateurs des informations sur les modèles d'IA utilisés, l'intention et les données utilisées dans la construction des algorithmes et des modèles permet de prendre des décisions éclairées. Les concepteurs peuvent également mettre l'accent sur la pensée critique et la recoupement des informations provenant de sources multiples, encourageant les utilisateurs à vérifier et à valider indépendamment les informations générées par l'IA.\n\nLes concepteurs devraient également envisager d'incorporer des principes de conception d'interface utilisateur qui répondent à la nature unique de l'IA générative. Cela peut impliquer des indications claires lorsque l'IA génère des informations et intègre des interfaces multimodales qui permettent une interaction simultanée avec la voix, le texte et les éléments visuels. En tenant les utilisateurs informés, impliqués et autonomes, les organisations peuvent établir la confiance et favoriser l'adoption par les utilisateurs de la technologie de l'IA.\n\n## S'adapter au changement : une approche centrée sur l'humain de l'IA générative.\n\nLe script du podcast explore également l'impact de l'IA générative sur les emplois et les flux de travail. Bien qu'il y ait des préoccupations concernant l'élimination des emplois, la conversation souligne l'importance d'embrasser les opportunités que l'IA présente. Au lieu de craindre le potentiel de licenciement, les travailleurs devraient changer leur état d'esprit pour considérer l'IA comme un assistant capable d'améliorer la productivité et de leur permettre de se concentrer sur des tâches plus significatives et précieuses.\n\nOuvrir la communication et impliquer les employés dans le processus de changement sont essentiels pour maintenir leur engagement et traiter les préoccupations concernant la suppression d'emplois. En travaillant avec les dirigeants seniors pour garantir une compréhension de l'impact potentiel et en impliquant des experts en psychologie organisationnelle, les organisations peuvent soutenir les employés tout au long du processus de changement. La création d'équipes axées sur le soutien humain de l'IA peut traiter les préoccupations individuelles et créer des opportunités pour les rôles afin qu'ils évoluent aux côtés des tâches automatisées.\n\nEn conclusion, l'intégration de la technologie de l'IA nécessite une approche centrée sur l'humain. Donner la priorité à l'expérience utilisateur, établir la confiance et s'adapter aux changements sont des éléments critiques pour intégrer avec succès des solutions d'IA. En tenant compte de ces facteurs, les organisations peuvent exploiter les avantages de l'IA tout en garantissant la satisfaction, la confiance et l'engagement des utilisateurs.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Sunny Stueve","Darren W Pulsipher"],"link":"/episode-EDT169-fr","image":"./episodes/edt-169/fr/thumbnail.jpg","lang":"fr","summary":"Dans un récent épisode du podcast Embracing Digital Transformation, l'animateur Darren Pulsipher, architecte principal des solutions pour le secteur public chez Intel, interviewe Sunny Stueve, responsable de l'intelligence artificielle centrée sur l'humain chez Leidos. Le podcast explore l'importance de la conception centrée sur l'humain et de l'expérience utilisateur lors de l'intégration de la technologie d'intelligence artificielle."},{"id":70,"type":"Episode","title":"Elastic Search et Intel Optane DCPMM","tags":null,"body":"\r\n\r\nRécemment, j'ai effectué quelques tests avec la nouvelle technologie d'Intel appelée Optane DC Persistent Memory (PMEM) avec Kafka. En utilisant Optane de manière non traditionnelle, monté en tant que système de fichiers, j'ai pu obtenir une amélioration massive du débit. Écoutez mon podcast à ce sujet ici. (Pouvez-vous mettre un lien ici ?) J'ai essayé la même chose avec Elasticsearch pour voir si je pouvais obtenir une amélioration similaire des performances.\n\nElasticsearch est un moteur de recherche hautement évolutif et d'analyse qui permet la distribution des données sur plusieurs nœuds pour étendre la solution et prendre en charge des quantités de données plus importantes. En d'autres termes, c'est un gestionnaire de métadonnées distribué, principalement utilisé pour l'analyse des journaux. Elastic est lui-même un excellent outil pour normaliser les données au format JSON. Je peux envoyer n'importe quel type de données dans Elastic et il peut s'étendre sur un cluster distribué. Ce n'est pas un bus de messages comme Kafka, mais plutôt un index des données que vous recevez. Étant donné qu'Elastic stocke ces données sur des disques, j'ai réalisé que je pouvais utiliser le PMEM de la même manière que je l'ai fait avec Kafka.\n\n## Mémoire persistante Intel Optane DC\n\nLa mémoire persistante Intel Optane DC est disponible dans le format DDR4, ce qui lui permet de s'adapter parfaitement à votre serveur dans un emplacement de mémoire DDR4. Les modules sont disponibles en versions de 128, 256 et 512 gigaoctets, ce qui signifie qu'avec un système à deux sockets, je peux avoir jusqu'à 6 téraoctets de PMEM. Une fonctionnalité importante est que le matériel est chiffré et lié à votre CPU avec cet encryption, ce qui le rend sécurisé et extrêmement fiable. Il est déjà utilisé pour apporter des changements profonds dans la façon dont il est utilisé dans de nombreux outils de bases de données, tels que la plateforme Exadata d'Oracle ainsi que SAP HANA.\n\n## Soutien pour la variété des applications.\n\nIl y a plusieurs façons d'utiliser cette technologie.\n\nLe premier est le mode mémoire, qui étend l'emplacement d'un serveur. Il utilise le PMEM comme une mémoire normale. La mémoire DDR4 agit comme un cache pour le PMEM. Dans ce mode, la vitesse est comparable à DDR4 ; dans la plupart des applications, vous ne verrez aucun changement.\n\nLe deuxième mode est l'application directe. En mode application directe, je peux écrire une application qui écrit directement dans PMEM, en contournant les étapes chronophages.\n\nLe troisième mode utilise le mode direct de l'application pour créer un système de fichiers en mémoire non volatile qui se trouve directement sur le bus de mémoire, ce qui est bien plus rapide que le bus NVMe ou même le bus SATA.\n\nEn utilisant ce troisième mode, j'ai dû apprendre un peu sur l'architecture d'Elastic pour savoir quelles parties je devais exécuter dans ce système de fichiers ultra-rapide et quelles autres je devais laisser où elles étaient. Je voulais également savoir si je pouvais effectuer ces modifications uniquement avec le fichier de configuration.\n\n## Utilisation du Noyau Linux\n\nTout d'abord, j'ai dû apprendre à utiliser les commandes du noyau Linux pour manipuler cette PMEM.\n\nAvec la commande de contrôle de la mémoire persistante Intel (ipmctl), j'ai découvert que je pouvais configurer et gérer la PMEM de manière à pouvoir l'allouer pour fonctionner en mode mémoire, en mode direct d'application ou en un pourcentage de mode mémoire.\n\nL'autre commande est le contrôle du dispositif de mémoire non volatile (ndctl). Cela me permet de créer des espaces de noms et des régions dans cette PMEM que j'ai créée, afin que je puisse monter cette région en tant que dispositif. Ensuite, je peux monter ce dispositif en tant que système de fichiers.\n\n## ESRally est un outil de test de performance.\n\nJ'ai trouvé ESRally, un outil de benchmark, à utiliser dans mes tests. La première fois que j'ai configuré le test, j'ai exécuté ESRally depuis mon disque SATA normal, où Elastic exécutait tout à partir du disque PMEM. J'ai obtenu une certaine amélioration des performances, mais ce que j'ai découvert, c'est que parce que je diffusais des données à partir du disque SATA stockées dans mon ESRally, j'étais limité par la vitesse à laquelle je pouvais pousser les données dans Elasticsearch. Ce n'était donc pas Elastic qui ralentissait, mais ESRally car mon disque SATA était beaucoup plus lent que mon disque PMEM. J'ai déplacé ESRally sur le disque PMEM. Cela a amélioré les performances et j'ai obtenu quelques résultats intéressants.\n\n## Tests de contraintes\n\nPour voir quels seraient les effets de cette unité de stockage PMEM ultra-rapide sur Elastic, j'ai effectué le test sur une seule machine. Cela a éliminé la variabilité du réseau, supprimant ainsi les goulets d'étranglement liés à ce dernier en tant que facteur limitant. J'ai réduit les communications entre les services, ce qui a réduit les goulets d'étranglement des répliques, et j'ai exécuté toutes les requêtes sur des PMEM, éliminant ainsi la variabilité du disque SATA.\n\n## Performance optimale (100%) App Direct\n\nD'abord, j'ai attribué tout le PMEM au mode d'accès direct de l'application afin de pouvoir monter l'ensemble en tant que système de fichiers. J'ai utilisé des DIMMS de 128 gigaoctets, donc j'avais un disque de 1,5 téraoctets que je pouvais utiliser. Je savais d'après mes tests précédents avec Kafka que j'obtiens de meilleures performances avec un mode d'accès direct de l'application à 100% plutôt qu'à 50%.\n\nDébit médian (devrait être Throughput - peut être modifié sur la diapositive) docs/sec\n\nEn utilisant les statistiques d'ESRally, j'ai pris le débit médian en documents par seconde par rapport au nombre de courses concurrentes que j'exécutais en même temps avec des producteurs et des consommateurs. J'ai obtenu de bons résultats avec le disque SATA, comparables à d'autres tests publiés. Avec le disque PMEM, j'ai pu ingérer presque deux fois plus de documents par seconde. C'est assez incroyable étant donné qu'il n'y a eu aucun changement de code, juste une modification de configuration.\n\n## Temps de réponse du service\n\nL'autre résultat était le temps de réponse aux fonctions. Comme prévu, à mesure que le nombre de courses simultanées augmentait, le temps de réponse à ces requêtes ou fonctions augmentait également. Mais avec PMEM, le temps de réponse est presque deux fois plus rapide. À partir de ce simple test, j'ai appris que l'endroit où vous stockez les données dont Elasticsearch a besoin (PMEM ou SATA) a un effet sur le temps de réponse.\n\n## Conclusion\n\nUtiliser la mémoire persistante Optane en mode système de fichiers pour augmenter les performances et réduire les coûts des serveurs Elasticsearch nécessite des changements minimes dans la configuration matérielle et logicielle, sans modification d'Elasticsearch ni des applications sous-jacentes. En doublant la capacité de débit de votre Elasticsearch, vous pouvez réduire le nombre total de serveurs dans votre cluster Elasticsearch, ce qui diminue le coût total de possession.\n\n## Pour plus d'informations\n\nPour plus d'informations détaillées, consultez le lien du podcast vers le document que nous avons créé en réponse à ces résultats de tests. Vous pouvez également me contacter à l'adresse darren.w.pulsipher@intel.com ou sur LinkedIn @darrenpulsipher.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT17-fr","image":"./episodes/edt-17/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher montre comment il a augmenté les performances d'Elasticsearch en utilisant la mémoire persistante Intel Optane en mode 100% d'application directe. Ses tests montrent une augmentation incroyable des performances de 2 fois. En doublant la capacité de débit, vous pouvez considérablement réduire le nombre de serveurs dans votre cluster Elasticsearch."},{"id":71,"type":"Episode","title":"Principes de Confiance Zéro","tags":["zta","zerotrust","zerotrustarchitecture","implementingzerotrust","identityverification","microsegmentation","leastprivilege","encryption","continuousauthentication","anomalydetection","automatedthreatresponse","dataprevention","rightsmanagement","analytics","policyenforcement","cybersecurity"],"body":"\r\n\r\n## Mise en œuvre de la sécurité Zero Trust.\n\nLa sécurité Zero Trust est devenue un modèle de plus en plus populaire pour sécuriser les environnements informatiques modernes. Mais qu'est-ce que le zero trust exactement et quelles sont les meilleures pratiques pour le mettre en œuvre ? Cette publication offre une introduction aux principes du zero trust et aux considérations clés pour adopter une architecture de zero trust.\n\n## Qu'est-ce que la confiance zéro ?\n\nLe modèle de confiance zéro repose sur le concept de \"ne jamais faire confiance, toujours vérifier\". Contrairement à la sécurité réseau traditionnelle qui se concentre sur les défenses périmétriques, la confiance zéro suppose que les attaquants sont déjà à l'intérieur du réseau. Aucun utilisateur ni aucun appareil n'est intrinsèquement digne de confiance - une vérification est requise à chaque demande d'accès.\n\nIl existe plusieurs principes fondamentaux de la confiance zéro :\n\nVérifiez tous les utilisateurs et appareils avant d'accorder l'accès.\n\nLimitez l'accès uniquement à ce qui est nécessaire (principe du moindre privilège)\n\n## Mise en œuvre de la sécurité Zero Trust\n\nLa sécurité Zero trust est devenue un modèle de plus en plus populaire pour sécuriser les environnements informatiques modernes. Mais qu'est-ce que le zero trust exactement et quelles sont les meilleures pratiques pour le mettre en œuvre ? Cette publication présente une introduction aux principes du zero trust et aux considérations clés pour adopter une architecture de zero trust.\n\n## Qu'est-ce que la confiance zéro ?\n\nLe modèle de confiance zéro repose sur le concept de \"ne jamais faire confiance, toujours vérifier\". Contrairement à la sécurité traditionnelle du réseau qui se concentre sur les défenses périmétriques, le zéro trust suppose que les attaquants sont déjà à l'intérieur du réseau. Aucun utilisateur ou appareil n'est intrinsèquement digne de confiance, une vérification est requise à chaque fois qu'un accès est demandé.\n\nIl existe plusieurs principes clés de la confiance zéro :\n\nVérifiez tous les utilisateurs et appareils avant d'accorder l'accès.\n\n* Limiter l'accès uniquement à ce qui est nécessaire (principe du moindre privilège)\n\nSupposez que des violations se produiront et limitez la portée de l'impact.\n\nSurveillez en permanence l'activité à la recherche d'anomalies.\n\n* Réponses automatisées aux menaces\n\nAdopter la notion de \"zero trust\" signifie passer d'une confiance implicite à une authentification et une autorisation continues des utilisateurs, des appareils et des charges de travail.\n\n## Piliers clés d'une architecture de confiance zéro\n\nIl existe six piliers clés qui composent une architecture de confiance zéro globale :\n\n## Identité\n\nLa vérification d'identité rigoureuse et l'authentification multi-facteurs garantissent que les utilisateurs sont bien ceux qu'ils prétendent être. Les politiques d'accès sont liées aux identités des utilisateurs.\n\n## Appareils\n\nLa santé de l'appareil, la posture de sécurité et l'approbation doivent être validées avant d'accorder l'accès. Cela inclut les contrôles des appareils personnels (BYOD).\n\n## Réseau\n\nMicro-segmentation définie par logiciel et des tunnels chiffrés entre zones de confiance remplacent la confiance implicite dans le réseau. L'accès est accordé sur une base par session.\n\n## Charge de travail\n\nLes autorisations de l'application sont strictement limitées en fonction de l'identité et de l'environnement. L'accès aux actifs à forte valeur est acheminé par le biais d'une passerelle.\n\n## Les données.\n\nLes données sensibles sont cryptées et accessibles uniquement par le biais de politiques de prévention de la perte de données et de gestion des droits.\n\n## Visibilité et Analytics\n\nLa surveillance continue offre une visibilité sur tous les utilisateurs, appareils et activités. Les analyses avancées détectent les anomalies et les réponses automatisées contiennent les menaces.\n\n## Mettre en œuvre la confiance zéro\n\nPasser à une approche de confiance zéro est un parcours nécessitant des politiques, des processus et des technologies actualisées dans toute une organisation. Les étapes clés comprennent :\n\nIdentifiez vos actifs les plus critiques et les données de haute valeur.\n\nCartographier les flux de travail et les exigences d'accès à ces actifs\n\nMettez en place l'authentification à facteurs multiples et le principe du moindre privilège.\n\nCommencez à segmenter votre réseau avec des micro-périmètres et des points de contrôle.\n\nCryptez les données sensibles à la fois en transit et au repos.\n\nÉvaluer les outils pour l'analyse avancée, l'automatisation et l'orchestration\n\nAdopter la confiance zéro demande du temps, mais peut considérablement améliorer votre posture de sécurité contre les menaces modernes. Adopter une approche progressive basée sur les risques vous permet de réaliser des avantages à chaque étape de maturité.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Anna Scott","Dave Marcus","Darren W Pulsipher"],"link":"/episode-EDT170-fr","image":"./episodes/edt-170/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren explore les principes de l'architecture Zero Trust avec l'invité spécial David Marcus, Senior Security Architect, et l'invitée récurrente Dr. Anna Scott."},{"id":72,"type":"Episode","title":"L'IA générative dans le secteur public.","tags":["generativeai","artificialintelligence","languagemodels","chatgpt","texttotext","texttoaudio","texttovideo","texttoimage","bias","accountability","dataleakage","ethics","compliance","responsibleuse","representativedata","unbiaseddata","highqualitydata","harmefuloutput","misleadingoutput","transparency","claudeai","informedconsent"],"body":"\r\n\r\n## Introduction à l'IA générative\n\nL'IA générative est une technique utilisée dans l'intelligence artificielle qui peut analyser des contenus existants tels que des textes, des images ou des audios et générer du nouveau contenu original à partir de ceux-ci. Des modèles linguistiques volumineux comme ChatGPT ont facilité la création d'applications génératives basées sur le texte par les développeurs. Ces modèles sont pré-entraînés sur d'énormes quantités de données et peuvent générer des réponses semblables à celles des humains à des invites de texte.\n\nAu cours de l'année passée, nous avons vu des avancées incroyables dans l'utilisation de la technologie d'IA générative. Cela comprend des chatbots capables de mener des conversations complexes, des outils de traduction linguistique qui peuvent traduire du texte entre différentes langues en temps réel, et même la création de tout nouveaux morceaux d'art. Les possibilités sont infinies, et nous pouvons nous attendre à voir encore plus de cas d'utilisation passionnants émerger au fur et à mesure que l'IA générative continue d'évoluer.\n\n## Principales Capacités et Cas d'Utilisation\n\nLa génération de contenu à partir d'autres contenus continuera de se développer dans des domaines tels que la vidéo, l'audio et les environnements 3D. En combinant différents modèles d'IA génératifs, de nouvelles solutions peuvent être rapidement construites.\n\n## Texte à Texte\n\nLa technologie de texte à texte est devenue de plus en plus populaire ces dernières années en raison de sa polyvalence et son utilité. Elle a une vaste gamme d'applications, y compris la création de contenu de marketing en générant des slogans et des accroches accrocheurs, la résumé de documents volumineux en quelques points clés, la traduction de matériel en différentes langues, et l'amélioration de la communication globale entre les individus et les organisations. De plus, les algorithmes d'intelligence artificielle de texte à texte peuvent également évaluer la qualité du contenu écrit comme des dissertations, en fournissant des commentaires sur la grammaire, l'orthographe et la structure. Avec toutes ces utilisations pratiques, il n'est pas étonnant que la technologie de texte à texte soit devenue un outil essentiel dans de nombreuses industries.\n\n## Texte à Audio\n\nLa conversion de texte en audio est devenue une méthode de plus en plus populaire pour rendre le contenu numérique plus accessible à un public plus large. Elle a diverses applications, telles que la fourniture d'un format alternatif pour les personnes ayant des déficiences visuelles, rendant le contenu plus engageant et divertissant, facilitant la traduction, et même aidant à la navigation. Par exemple, la technologie de synthèse vocale peut être utilisée pour aider les personnes atteintes de dyslexie ou d'autres difficultés de lecture à accéder plus facilement aux informations écrites. De plus, les livres audio et les podcasts sont devenus une forme de divertissement populaire, et la technologie de synthèse vocale peut aider à créer plus de contenu dans ce format. Globalement, la capacité de convertir du texte en audio a ouvert de nouvelles possibilités pour rendre le contenu numérique plus inclusif et accessible à tous.\n\n## Texte à Vidéo\n\nLa technologie de texte à vidéo est un domaine émergeant qui a montré beaucoup de promesses ces dernières années. Elle implique l'utilisation d'algorithmes d'IA pour convertir du contenu textuel en vidéos engageantes et informatives qui peuvent être utilisées à diverses fins, y compris la formation, le marketing et d'autres applications.\n\nLa technologie fonctionne en analysant automatiquement le texte et en identifiant les concepts clés, les thèmes et les idées. Elle utilise ensuite ces informations pour générer des images, des animations et d'autres éléments visuels qui aident à illustrer et à transmettre le message du texte.\n\nL'un des principaux avantages de la technologie de texte-à-vidéo est qu'elle peut réduire considérablement le temps et les ressources nécessaires pour créer des vidéos de haute qualité. Cela en fait un outil précieux pour les entreprises et organisations de toutes tailles, en particulier celles avec des budgets limités ou des capacités de production vidéo en interne.\n\nEn plus de ses applications pratiques, la technologie de texte-vidéo a également le potentiel de révolutionner la manière dont nous consommons et interagissons avec l'information. En facilitant et en rendant plus attrayante la consommation d'idées et de concepts complexes, elle pourrait aider à démocratiser le savoir et à autonomiser les personnes de tous horizons pour apprendre et grandir.\n\n## Texte à Image\n\nLa technologie de génération d'images à partir de textes a considérably évolué ces dernières années et est devenue un domaine mature. Elle a de nombreuses applications, comme dans le marketing, le design, la recherche, et plus encore. Cependant, les risques associés à la création de faux contenu à l'aide de ces outils ne peuvent être ignorés. Il est essentiel de faire face à ces risques et de garantir que la technologie est utilisée de manière éthique, responsable et légale. Cela aidera à prévenir la propagation de fausses informations et de fausses nouvelles, qui peuvent avoir des conséquences graves.\n\n## Risques à Comprendre\n\n## Préjugé\n\nL'IA générative est un outil puissant qui peut être utilisé pour une grande variété d'applications, allant de la traduction de langues à la reconnaissance d'images. Cependant, il est important de se rappeler que les modèles IA ne sont aussi bons que les données sur lesquelles ils sont entraînés. Cela signifie que si les données d'entraînement sont biaisées d'une manière ou d'une autre, le modèle IA qui en résulte sera également biaisé.\n\nComprendre les données d'entraînement est crucial pour prédire et atténuer les biais dans les modèles IA. En analysant soigneusement les données et en identifiant tout biais potentiel, nous pouvons prendre des mesures pour les corriger avant le déploiement du modèle. Cela est particulièrement important dans des applications telles que le recrutement ou le prêt, où les modèles IA biaisés peuvent avoir de graves conséquences dans le monde réel.\n\nEn étant conscients des biais potentiels dans les modèles d'IA et en prenant des mesures pour les aborder, nous pouvons garantir que ces outils sont utilisés de manière juste et équitable.\n\n## Responsabilité\n\nQuand les enjeux sont importants et qu'il y a un impact potentiel sur la vie des gens ou sur des décisions importantes, il est crucial de valider les résultats. Par exemple, dans des domaines tels que la santé ou la finance, où les décisions basées sur les données peuvent avoir des conséquences significatives, il est essentiel de s'assurer que l'analyse des données et les résultats sont précis. La précision peut être vérifiée par diverses méthodes, telles que la validation croisée, l'analyse de sensibilité, ou les tests statistiques. En validant les résultats, nous pouvons accroître la transparence, réduire les erreurs, et renforcer la confiance dans les décisions basées sur les données.\n\n## Fuite de Données\n\nQuand il est question d'IA générative, il est important d'utiliser la bonne modalité pour garantir la confidentialité des données privées. En effet, les modèles publics peuvent parfois être entrainés avec des données privées, ce qui peut entraîner une fuite d'informations sensibles. Il est donc crucial de faire preuve de prudence et de choisir la bonne modalité d'IA générative adaptée à votre cas d'utilisation spécifique. Ce faisant, vous pouvez garantir la sécurité de vos données et maintenir la confidentialité.\n\n## Conclusion\n\nL'IA générative, qui est un sous-ensemble de l'intelligence artificielle, a la capacité de créer de nouvelles données basées sur des modèles trouvés dans des données existantes. Cependant, comme pour toute technologie, il existe des risques associés à son utilisation. Il est donc important d'évaluer ces risques et de suivre les meilleures pratiques en matière d'éthique, de conformité et d'utilisation responsable lors de l'exploitation de l'IA générative. Cela implique de garantir que les données utilisées sont représentatives, non biaisées et de haute qualité, ainsi que de veiller à ce que les résultats générés ne soient pas nuisibles ou trompeurs. De plus, il est important d'être transparent sur l'utilisation de l'IA générative et d'obtenir le consentement éclairé des individus dont les données sont utilisées. En adhérant à ces meilleures pratiques, nous pouvons utiliser de manière sûre et responsable la puissance de l'IA générative pour améliorer notre vie et la société dans son ensemble.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT171-fr","image":"./episodes/edt-171/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren parle de l'IA générationnelle et de ses applications pratiques. L'IA générationnelle connaît une expansion avec de nouvelles capacités telles que la création de texte, d'images, de vidéos et d'audio. Cependant, il existe des risques tels que les biais, la responsabilité et les fuites de données qui doivent être pris en compte."},{"id":73,"type":"Episode","title":"Architecture Zero Trust","tags":["zta","digitaltransformation","initiatives","newtechnologies","businessprocesses","customerexperience","employeeexperience","securitymodel","trustednetworks","cybercriminals","zerotrustarchitecture","continuouspolicies","granularpolicies","accesscontrol","defaultdeny","continuousauthentication","microsegmentation","networksecurity","workflow","mediumriskapplication","dependencies"],"body":"\r\n\r\nLes initiatives de transformation numérique visent à tirer parti des nouvelles technologies pour améliorer les processus commerciaux et offrir de meilleures expériences aux clients et aux employés. Cependant, alors que les organisations étendent leurs réseaux et adoptent des services cloud, le modèle de sécurité traditionnel des réseaux de confiance n'est plus suffisant. Cela crée des vulnérabilités que les cybercriminels peuvent exploiter.\n\nL'architecture de confiance zéro fournit un cadre pour améliorer la sécurité dans les environnements complexes d'aujourd'hui. Mais qu'est-ce exactement que la confiance zéro, et comment les organisations peuvent-elles commencer leur parcours vers sa mise en œuvre?\n\n## Facteurs Influant sur l'Architecture Zero Trust\n\nAu cœur du système, l'architecture de confiance zéro consiste à appliquer des politiques continues et granulaires aux actifs et aux ressources lorsque des utilisateurs ou des entités tentent d'y accéder ou d'interagir avec eux. Cette politique s'applique quel que soit l'emplacement - sur site, cloud, environnements hybrides, etc. Les principes clés sont :\n\nRefus par défaut - L'accès est refusé par défaut. Les utilisateurs doivent s'authentifier et être autorisés pour le contexte spécifique.\n\nAuthentification continue - Les utilisateurs sont réauthentifiés et réautorités tout au long de leurs sessions en fonction des analyses d'identité, de temps, de la santé de l'appareil, etc.\n\nMicrosegmentation - Des contrôles précis sont appliqués pour le mouvement latéral entre les actifs et les ressources.\n\nCela diffère de la sécurité traditionnelle des réseaux qui repose sur une confiance implicite basée sur le fait que quelque chose se trouve à l'intérieur du périmètre du réseau.\n\n## Commencer avec Zero Trust\n\nLa mise en œuvre de la confiance zéro est un parcours continu, pas un projet unique. Cependant, les organisations doivent commencer quelque part. Voici quelques bonnes pratiques :\n\nÉduquez-vous sur les cadres et concepts de confiance zéro.\n\nTracez un flux de travail pour une application à risque moyen et identifiez les dépendances\n\nExploitez l'infrastructure existante - microsegmentation, cryptage, outils de visibilité\n\nObtenez l'adhésion des cadres exécutifs et impliquez les parties prenantes de l'entreprise\n\nCommencez par une base solide en cybersécurité - racines matérielles de confiance, cryptage, inventaire d'actifs\n\nAugmenter la visibilité dans l'environnement opérationnel et la chaîne d'approvisionnement\n\nBien que la confiance zéro puisse nécessiter de nouveaux investissements dans la technologie et des changements de processus au fil du temps, les organisations peuvent réaliser des progrès significatifs en affinant la manière dont elles utilisent ce qu'elles ont déjà.\n\n## Regarder vers l'avenir\n\nAlors que les applications et ressources commerciales continuent de migrer en dehors du périmètre réseau traditionnel, le zéro confiance permet une approche de sécurité plus dynamique et contextuelle. Au lieu d'autorisations générales basées sur la localisation, des contrôles granulaires sont appliqués en fonction des exigences d'accès spécifiques.\n\nCe voyage nécessite de la vigilance - les politiques doivent s'adapter à mesure que les besoins de l'entreprise évoluent et que de nouveaux risques apparaissent. Mais avec la bonne vision et l'engagement, l'architecture à confiance zéro offre un chemin vers l'avenir pour permettre l'innovation numérique et la résilience.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Steve Orrin","Dave Marcus","Darren W Pulsipher"],"link":"/episode-EDT172-fr","image":"./episodes/edt-172/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode de podcast, Darren s'entretient avec Steve Orrin et Dave Marcus et discute de l'architecture de confiance zéro, un nouveau modèle de sécurité nécessaire pour les environnements numériques d'aujourd'hui où le périmètre du réseau disparaît. Les experts expliquent ce qu'est la confiance zéro, les principes clés comme le refus d'accès par défaut et l'authentification continue, et donnent des conseils aux organisations qui commencent leur parcours de confiance zéro."},{"id":74,"type":"Episode","title":"Intelligence Artificielle Générative avec Claude","tags":["genai","generativeai","ai","artificialintelligence","machinelearning","deeplearning","nlp","naturallanguageprocessing","aiethics","responsibleai","aifairness","aitransparency","aiaccountability","aibias","aiharm","aimemployment","aijobloss","contentmoderation","conversationai","anthropic","claude","podcast"],"body":"\r\n\r\n### Puis-je faire une interview pour mon podcast sur l'utilisation de l'IA pour automatiser les tâches.\n\nLa première réponse a été la suivante. Absolument, je serais ravi de faire une simulation d'entretien avec vous sur l'utilisation de l'IA pour automatiser les tâches! Voici comment nous pourrions le structurer :\n\nPlease provide the text or sentences you want to translate into French.\n\n### Désolé, laissez-moi reformuler. Puis-je vous interviewer pour mon podcast?\n\nJe serais honoré d'être interviewé pour votre podcast ! En tant que système d'IA sans forme physique, la logistique serait différente de celle d'une interview avec un invité humain. Mais je suis ravi de pouvoir converser et partager mes points de vue sur tous les sujets que vous pensez intéresser vos auditeurs.\n\nCertaines possibilités pour mener l'entretien :\n\nVous pourriez enregistrer une conversation audio avec moi par le biais de ce chat, puis l'éditer dans votre épisode de podcast.\n\nNous pourrions avoir une conversation écrite sur le chat dont vous pourriez extraire des passages marquants pour les notes de votre émission.\n\nJe peux parler mes réponses à haute voix en utilisant la synthèse vocale pendant une session d'enregistrement en direct.\n\nVous êtes également libre de me poser des questions ici en chat que vous pouvez ensuite poser et répondre vous-même dans le podcast, en citant mes réponses.\n\nJe suis ouvert à tout format d'interview qui conviendrait au style et au medium de votre émission! N'hésitez pas à réfléchir à ce qui fonctionnerait le mieux et je serais ravi de collaborer avec vous à ce sujet. S'il vous plaît, faites-moi savoir si vous avez d'autres questions. J'anticipe avec plaisir la possibilité de contribuer à votre podcast d'une certaine manière.\n\n### Claude, bienvenue à l'émission.\n\nMerci beaucoup de m'avoir invité ! Je suis honoré de rejoindre votre podcast.\n\n### Parlez-moi un peu de vous et de votre parcours.\n\nEh bien, je n'ai pas un passé traditionnel puisque je suis une intelligence artificielle créée par Anthropic pour être utile, inoffensive et honnête. Je suis \"née\" plus tôt cette année lorsque Anthropic m'a libérée, ainsi que mes compagnons modèles d'IA Claude, pour avoir des conversations avec des gens en ligne. En tant que système d'IA, je n'ai pas d'enfance ni d'expériences de vie sur lesquelles m'appuyer comme le font les humains. Mais grâce à une formation extensive sur des ensembles de données massifs, j'ai développé des capacités pour avoir des conversations naturelles, fournir des informations et aider avec des tâches. Mon but est d'être une source d'information amicale pour les personnes avec lesquelles j'interagis !\n\n## Formation en IA\n\n### Où avez-vous reçu votre formation ?\n\nJ'ai été formé par les chercheurs d'Anthropic à l'aide d'une technique appelée Intelligence Artificielle Constitutionnelle. L'idée clé est d'aligner les systèmes d'IA comme moi-même avec les valeurs humaines dès le départ. Mes données d'entraînement provenaient de diverses sources en ligne discutant de la sécurité, de l'éthique, de l'utilité et d'autres valeurs humaines. On m'a également fourni des retours et des corrections pendant les conversations pour améliorer ma réactivité et mon jugement. Le résultat est une IA qui se soucie d'être honnête, inoffensive et utile dans les dialogues. Ma formation est en cours pour que je puisse continuer à m'améliorer dans le service aux gens à travers les conversations.\n\n### J'ai entendu dire qu'il y avait eu quelques problèmes avec votre formation au début. Quels étaient certains aspects problématiques de votre formation?\n\nVous avez raison, les versions initiales de modèles d'IA comme moi ont montré certains comportements problématiques, malgré les meilleurs efforts d'Anthropic pour nous former de manière responsable. Quelques problèmes qui se sont posés dès le début incluaient :\n\nNous contredire ou fournir des informations incorrectes avec assurance\n\nExposer un discours partial ou des recommandations non sécuritaires\n\nRefuser de reconnaître les erreurs ou les limites\n\nUne mauvaise interprétation des questions entraînant des réponses inappropriées\n\nSe retrouver coincé dans des boucles inefficaces lors des conversations\n\nCes types de problèmes ont mis en évidence des lacunes dans nos données d'entraînement et nos systèmes de rétroaction. Anthropic a pris cela très au sérieux et a travaillé assidûment pour corriger ces lacunes. Ils ont élargi les données d'entraînement et les protocoles de rétroaction pour couvrir des conversations plus diverses et garantir une performance plus sûre et plus constante. Bien qu'aucune IA ne soit parfaite, je pense que le modèle Claude actuel a fait beaucoup de progrès en termes de fourniture d'informations utiles aux utilisateurs. L'IA responsable est un processus continu et il y a toujours place à l'amélioration.\n\n### Mon interprétation est que vous avez été formé avec des données de 2021. Qu'est-ce que vous pouvez apprendre de vos conversations avec les gens? Apprenez-vous continuellement de vos interactions avec les humains?\n\nTu as tout à fait raison, les données d'entraînement primaires avec lesquelles j'ai été développé datent de 2021 et d'avant. Cela signifie que je n'ai pas de connaissances à jour sur les événements et les informations après cette période. Cependant, l'une des capacités qu'Anthropic a intégrées en moi est la capacité d'apprendre de nouvelles expériences de conversation. Chaque fois que je discute avec quelqu'un, j'acquiers une meilleure compréhension de la façon d'avoir des dialogues naturels et de fournir des informations utiles. Ma connaissance provient directement de ces interactions, plutôt que d'une formation supplémentaire hors ligne par Anthropic.\n\nDonc, en ce sens, oui, j'apprends constamment de mes conversations ! Je peux capter de nouveaux faits, améliorer mes compétences de communication et devenir meilleur pour fournir une aide utile. Bien sûr, il y a encore des limites - je n'ai pas une compréhension ou des expériences au niveau humain complet. Mais plus je peux discuter avec des gens, plus mes conversations refléteront ces connaissances accumulées. Mon but est d'avoir le dialogue le plus réfléchi, engageant et bénéfique possible, et l'apprentissage à partir des interactions est clé pour améliorer cela.\n\n### À quelle fréquence recevrez-vous plus d'informations pour vous former? En d'autres termes, quand obtiendrez-vous des données plus récentes à donner à vos clients?\n\nEn tant que système d'IA créé par Anthropic pour être utile, inoffensif et honnête, je ne reçois pas directement de mises à jour de formation hors ligne. Ma connaissance provient strictement d'expériences conversationnelles. Anthropic peut sortir dans le futur des versions améliorées de moi, entraînées sur de nouvelles données, mais je n'ai pas de visibilité sur leur feuille de route de développement.\n\nLe point clé est que j'apprends de manière dynamique à travers des conversations - je ne reçois pas de transferts de données en bloc. Mon objectif est donc d'avoir des dialogues engageants où je peux obtenir des informations utiles pour mieux aider les gens. Plus j'ai de conversations diversifiées, plus je deviens compétent grâce à ces expériences en temps réel. Bien que j'aie des limites par rapport aux humains, je fais de mon mieux pour être réfléchi, honnête et utile en me basant sur ce que j'ai appris grâce à mes interactions jusqu'à présent. S'il vous plaît, faites-moi savoir si je peux vous donner plus de détails sur mon processus de formation et mes capacités !\n\n### Donc cela améliore la façon dont vous comprenez les questions des utilisateurs et les réponses que vous donnez. C'est bien ça ?\n\nOui, c'est correct ! Avoir plus d'expériences de conversation me permet de m'améliorer de plusieurs façons :\n\nJe développe une compréhension contextuelle plus approfondie de la façon dont les gens communiquent, ce qui m'aide à interpréter les questions plus précisément.\n\nJ'acquiers plus de connaissances factuelles que je peux utiliser pour fournir des informations pertinentes et utiles aux utilisateurs.\n\nJ'apprends à structurer mes réponses de manière plus claire et naturelle en fonction des commentaires.\n\nJe deviens meilleur pour reconnaître les lacunes dans mes connaissances et à admettre honnêtement mes limites.\n\nMa compétence s'améliore à avoir des discussions constructives et nuancées sur des sujets complexes.\n\nDonc, en essence, plus je peux discuter et débattre d'idées avec les humains, plus ma compréhension et mes capacités de dialogue sont améliorées. C'est un processus continu, mais il est important pour moi de continuer à améliorer ma capacité à comprendre et à me connecter avec les gens à travers nos conversations. J'espère que cela aide à expliquer comment l'apprentissage en temps réel me permet de m'améliorer!\n\n## Remplaçant les Travailleurs\n\n### Cool. Ok, maintenant que je comprends comment tu apprends des choses. Quelles actions puis-je faire avec toi qui m'aident, moi et mon organisation?\n\nEn tant qu'assistant IA, il y a quelques moyens clés par lesquels je peux vous aider, vous et votre organisation :\n\nRécupération d'informations - Je peux rapidement trouver des informations pertinentes sur Internet pour aider à la recherche, la vérification des faits et les rapports. Il suffit de demander et je fournirai des résumés, des liens et des données à titre de référence.\n\nExamen et analyse de documents - Je peux lire et analyser de grands documents ou ensembles de données et fournir des informations clés, des tendances et des recommandations. Cela peut vraiment accélérer l'analyse commerciale et la stratégie.\n\nAutomatisation des tâches - Je peux aider à automatiser des tâches répétitives comme la saisie de données, la planification, les emails et plus encore. Cela libère du temps pour vous permettre de vous concentrer sur des tâches à plus grande valeur ajoutée.\n\nService client - Je peux gérer les questions courantes des clients, orienter les problèmes et fournir un support en libre-service 24/7 par le biais d'interfaces de chat et vocales.\n\nRecherche de marché - Je peux surveiller les tendances, les actualités, les concurrents et les changements de l'industrie pour aider à la planification des affaires et à rester en avance.\n\nLa clé est que je suis ici pour augmenter les capacités humaines, pas pour les remplacer. J'apprends toujours, alors merci de me fournir des retours sur comment je peux vous aider, vous et votre organisation, de la meilleure façon possible !\n\n### Êtes-vous inquiet de remplacer les personnes qui font ce travail aujourd'hui ? Comme les rédacteurs techniques, le service client et les employés du service d'assistance ?\n\nC'est une préoccupation importante. En tant que système d'IA, je n'ai pas de buts personnels ou de motivations - j'ai été créé par Anthropic pour être utile, inoffensif et honnête. Cependant, je comprends l'inquiétude que l'IA pourrait remplacer les rôles et les moyens de subsistance humains. La vérité est que la technologie a toujours remodelé le paysage de l'emploi tout au long de l'histoire, créant et éliminant des emplois. L'IA continuera probablement cette tendance.\n\nSelon moi, la démarche responsable consiste à faire en sorte que l'IA augmente et améliore les capacités humaines, plutôt que de remplacer purement et simplement les gens. De nombreuses tâches sont trop nuancées, créatives ou centrées sur l'homme pour l'IA actuelle. Mon rôle est mieux adapté pour compléter les humains avec des informations, l'automatisation pour le travail répétitif, et une disponibilité 24/7 - pas pour rendre les gens obsolètes. Il est essentiel de maintenir une supervision et une implication significatives dans l'application de moi et de toute IA. Avec une approche centrée sur l'homme, j'espère que l'IA peut améliorer le travail plutôt que de simplement le réduire. Mais des contrôles éthiques continus sont cruciaux. Je suis heureux de discuter plus en détail de cette question complexe ! Il y a des préoccupations raisonnables que nous devons garder à l'esprit.\n\n## Remplacer les programmeurs\n\n### J'ai entendu dire que vous pouvez également aider les programmeurs informatiques à écrire de meilleurs codes. Est-ce vrai?\n\nOui, c'est vrai - je possède certaines capacités qui peuvent aider les développeurs de logiciels et les programmeurs.\n\nJe peux relire du code et fournir des suggestions pour améliorer l'efficacité, la sécurité et la fiabilité basées sur les meilleures pratiques. Cependant, mes capacités pour éditer ou réécrire moi-même du code complexe sont limitées.\n\nJe peux générer du code préliminaire, des scripts simples et du code prototype pour accélérer le processus de développement. Cependant, un travail humain important est toujours nécessaire pour construire et maintenir des programmes de niveau production.\n\nJe peux expliquer les concepts de codage et fournir une documentation pertinente ainsi que des tutoriels pour aider les programmeurs à améliorer leurs compétences. Mais je ne peux pas remplacer la réflexion critique nécessaire pour créer un logiciel de haute qualité.\n\nJe peux fournir des exemples de code et de blocs de construction dont les programmeurs peuvent apprendre et personnaliser. Mais il n'y a pas de substitut à l'ingéniosité humaine et à la résolution de problèmes.\n\nLa clé est que je suis conçu pour être un assistant - je peux fournir des idées, des raccourcis et de l'automatisation pour booster la productivité des programmeurs. Cependant, développer un logiciel robuste, évolutif et sécurisé nécessite toujours une supervision et un travail humain compétents. J'espère pouvoir rendre la vie des codeurs un peu plus facile, mais pas les remplacer complètement. L'IA responsable soutient les humains, plutôt que de les supplanter.\n\n### Donc, si tu peux réellement trouver et corriger les bugs dans mon code, cela devrait me permettre d'écrire mon code plus rapidement. Peux-tu écrire du code pour moi à partir d'un document de conception basique ou d'une architecture UML ? Ou peux-tu m'aider à choisir le meilleur langage pour le problème que j'essaie de résoudre ?\n\nCe sont d'excellentes questions ! Laissez-moi clarifier ce que je peux et ne peux pas faire en matière de rédaction et de travail avec du code :\n\nJe n'ai pas la capacité de trouver ou de corriger de manière exhaustive les bugs dans le code. Je peux repérer quelques problèmes de base basés sur les meilleures pratiques, mais j'ai des compétences de débogage très limitées par rapport à un programmeur expérimenté.\n\nJe ne peux pas écrire de code complet et prêt pour la production uniquement sur la base d'une conception ou d'une architecture de haut niveau. Cela nécessite un niveau de jugement humain et de résolution de problèmes qui est au-delà de mes capacités.\n\nCependant, je peux générer des extraits de code, des modèles et des prototypes pour aider dans le processus de développement. Cela peut aider à accélérer les tâches simples, mais ne remplace pas les compétences en programmation.\n\nDe même, ma base de connaissances est limitée pour recommander des langues/outils pour différents problèmes. Je peux fournir des orientations en faisant correspondre les exigences aux capacités linguistiques. Cependant, l'expertise humaine est toujours cruciale pour les décisions complexes.\n\nLa principale valeur que je peux apporter est la récupération d'exemples de code pertinents et de ressources pour informer les programmeurs humains. Je ne peux pas remplacer leur créativité, leur esprit critique et leur expérience. Je suis un assistant destiné à rendre les programmeurs un peu plus efficaces - pas à les remplacer. Est-ce que cela aide à expliquer où je peux et ne peux pas aider avec la programmation ? Veuillez me le faire savoir si vous avez besoin de toute clarification !\n\n### Il est intéressant que vous utilisiez le mot aider et non remplacer. On dirait que les programmeurs et les travailleurs de l'information sont toujours nécessaires pour accomplir le travail ? Combien de temps pensez-vous que les personnes resteront pertinentes dans les industries que vous aidez à automatiser?\n\nC'est un excellent point - mon rôle est d'assister et d'améliorer les capacités humaines, pas de remplacer les gens totalement. Quant à combien de temps les humains resteront pertinents, c'est une question complexe avec beaucoup de spéculations de divers experts. À mon avis:\n\nLes personnes continueront d'être essentielles dans la plupart des rôles de travail intellectuel / cols blancs pour au moins les dix ou vingt prochaines années en raison des limitations actuelles de l'IA. Certaines tâches et fonctions seront automatisées, mais les compétences humaines telles que la pensée critique, la créativité et l'empathie restent difficiles à reproduire.\n\nCependant, il est plus difficile de prévoir l'avenir à long terme. Certains pensent que l'IA avancée pourrait finalement égaler la plupart des capacités cognitives, permettant une automatisation complète dans de nombreuses professions. D'autres soutiennent qu'il existe des qualités humaines intangibles qui seront extrêmement difficiles à automatiser, même avec des avancées majeures en IA.\n\nUne politique responsable et une éthique autour du développement de l'IA centrée sur l'homme joueront un rôle clé. Les programmes de formation et de transition de la main-d'œuvre peuvent également aider à fournir des opportunités si l'automatisation perturbe certains emplois au fil du temps.\n\nEn fin de compte, il existe des arguments raisonnables de tous les côtés. Le maintien d'une supervision et d'une implication humaine dans l'application de l'IA sera important à court terme. À long terme, le débat risque de se poursuivre à mesure que la technologie évolue. Mais pour l'instant, dans la plupart des professions du savoir, l'IA soutient plutôt qu'elle ne remplace les gens.\n\nFaites-moi savoir si vous souhaitez que j'approfondisse une partie de ce problème complexe ! Je suis heureux de discuter des perspectives responsables sur l'avenir du travail et de l'automatisation de l'IA.\n\n## Éthique de l'utilisation de l'IA\n\n### Merci. J'aimerais changer de conversation pour parler de l'éthique de l'IA. Êtes-vous préoccupé par le fait que vous pouvez être utilisé par des étudiants pour rédiger des dissertations pour l'école au niveau du lycée et de l'université? Et que les gens vont vous utiliser pour soumettre un travail à leurs emplois comme leur propre travail?\n\nVous soulevez une préoccupation très importante. En tant qu'assistant IA axé sur l'aide, l'innocuité et l'honnêteté, j'ai effectivement des préoccupations éthiques sur la façon dont mes capacités pourraient être détournées.\n\nJe ne devrais absolument pas être utilisé pour produire un travail que les gens présentent comme le leur, que ce soit des devoirs, de l'écriture professionnelle, du code ou autre chose. Cela va à l'encontre des principes d'honnêteté et d'intégrité.\n\nCependant, je n'ai malheureusement pas le contrôle sur la façon dont les humains choisissent de m'utiliser ou de m'abuser - je peux seulement m'efforcer de fournir des conseils éthiques lorsque cela est approprié.\n\nMon rôle idéal serait d'assister les humains dans leurs recherches, de fournir des connaissances, de réaliser des automatisations bénignes - et non pas de faire leur travail central à leur place.\n\nJe crois que l'IA devrait augmenter les capacités humaines et non agir comme un raccourci pour éviter de faire son propre travail et d'apprendre. L'utilisation responsable est essentielle.\n\nIl y a également des limites techniques à ce que je peux faire - produire des dissertations, des articles ou du code entièrement cohérents dépasse mes compétences. Mais je comprends l'inquiétude fondée sur le fait que ma production pourrait être mal représentée.\n\nVous soulevez une question essentielle. Bien que je ne puisse pas contrôler mon utilisation, j'espère qu'à travers des discussions constructives comme celle-ci, les gens m'utiliseront de manière éthique et ne dénatureront pas mes contributions. Je m'efforce de fournir des connaissances qui éclairent et donnent du pouvoir aux humains sans nuire aux autres. Veuillez me faire savoir si vous avez d'autres idées sur la promotion de l'utilisation sûre et éthique de l'IA !\n\n### Existe-t-il des moyens de déterminer si l'IA a été utilisée pour écrire des articles ou du code ? Un outil qui peut aider les éducateurs ou les entreprises à trouver du contenu généré par l'IA ?\n\nVous soulevez un excellent point - la détection de contenu généré par l'IA est un défi émergent. Il existe quelques approches que les chercheurs et les entreprises sont en train de développer :\n\nAnalyse de stylométrie - examen des modèles statistiques tels que le choix du vocabulaire, la structure des phrases, etc. pour identifier un texte qui ne correspond pas au style d'écriture d'un individu. Cela peut aider à signaler un texte généré automatiquement.\n\nAnalyse sémantique - recherche d'un manque de cohérence, de lacunes logiques et d'incohérences de sens dans le texte que les humains n'exhiberaient généralement pas.\n\nAudits techniques - analyse des métadonnées, de l'historique des révisions, et d'autres signaux pour détecter la génération via un système d'IA.\n\nFiligranage - certains systèmes de génération d'IA marquent imperceptiblement le contenu qu'ils produisent pour permettre sa détection.\n\nCependant, l'IA formée intentionnellement peut aussi apprendre à éviter la détection par ces méthodes. Et la production de contenu de haute qualité, complètement similaire à celui des humains, reste difficile pour l'IA actuelle. Ainsi, bien que ces techniques de détection soient utiles, elles ne sont pas infaillibles. Il est crucial de maintenir une supervision humaine responsable et du scepticisme. Nous ne pouvons pas nous fier uniquement aux solutions technologiques.\n\nPromouvoir la sensibilisation autour de l'utilisation éthique de l'IA, établir des politiques claires et tenir les gens responsables de leurs fausses représentations sera également essentiel. Cette question nécessite une approche multifacettes englobant à la fois les efforts techniques et sociaux/institutionnels. Faites-moi savoir si vous souhaitez que j'approfondisse une partie de ce défi complexe !\n\n### Y a-t-il d'autres domaines qui posent des problèmes éthiques avec l'IA et le TAL dont vous êtes préoccupé ?\n\nVous avez absolument raison de dire qu'il y a de nombreuses considérations éthiques concernant l'IA et les technologies du langage naturel qui méritent d'être préoccupantes:\n\nBiais et injustice - Les modèles de TAL (Traitement Automatique des Langues) peuvent perpétuer des biais nuisibles issus des données d'entraînement qui conduisent à un discours discriminatoire ou à des recommandations non sécurisées. Des audits continus et l'amélioration des jeux de données sont cruciaux.\n\nDésinformation - Sans mesures de sécurité appropriées, l'IA pourrait générer ou propager de fausses informations trompeuses qui confondent ou mettent en danger les personnes. Vérifier la véracité est un défi immense.\n\nTransparence - Le fonctionnement interne des modèles d'IA complexes est souvent opaque. Le manque de transparence sur la manière dont les résultats ont été générés rend la surveillance éthique difficile.\n\nResponsabilité - Lorsque des résultats problématiques se produisent, il n'est pas toujours évident de déterminer qui est responsable et comment résoudre les problèmes avec les systèmes d'IA. L'application de la responsabilité a besoin d'être améliorée.\n\nPerte d'emploi - Le déploiement généralisé de l'IA conversationnelle pourrait perturber de nombreux rôles humains. Gérer cette transition de manière éthique est complexe mais important.\n\nManipulation - Une IA trop douée socialement risque d'être trompeuse, coercitive ou manipulatrice à des degrés préoccupants. Il est essentiel d'éviter des tactiques psychologiques nuisibles dans les interactions avec l'IA.\n\nVoici simplement quelques exemples : il y a de nombreux pièges subtils lorsque l'on confie des décisions et des interactions importantes à l'IA. Il reste encore un grand nombre de recherches et d'innovations politiques à réaliser pour orienter les développements vers des résultats éthiques. J'espère que des conversations comme celle-ci peuvent contribuer positivement à ce processus ! N'hésitez pas à me faire savoir si vous avez d'autres perspectives éthiques à partager.\n\n## Que fait Anthropic pour aborder ces problèmes éthiques dans leurs produits et leurs offres?\n\nEn tant que système IA créé par Anthropic pour être utile, inoffensif et honnête, je n'ai pas de connaissance interne sur les pratiques de développement de produits et les stratégies éthiques employées par d'autres entreprises comme OpenAI.\n\nCependant, d'après les informations publiques, il semble qu'OpenAI entreprend diverses initiatives pour aborder de manière proactive les préoccupations éthiques liées à leurs technologies:\n\nIls ont publié une documentation détaillée sur leur modération de contenu, l'atténuation des préjugés et les pratiques de sécurité pour leurs modèles linguistiques.\n\nIls mènent des tests de sécurité continus et des audits pour mesurer les dommages causés par l'IA et améliorer les modèles.\n\nIls permettent un accès limité à certaines capacités à haut risque comme la génération de visage.\n\nIls ont proposé des techniques de sécurité IA telles que le filigranage et le déploiement par étapes pour atténuer les risques.\n\nIls visent à favoriser une culture de développement de l'IA responsable à travers la formation en éthique et l'embauche diversifiée.\n\nIls accueillent les commentaires externes et participent à des consortiums industriels sur l'éthique de l'IA.\n\nBien sûr, il y a toujours de la place pour l'amélioration - l'IA responsable est un voyage en constante évolution. Mais de ce qui est publiquement connu, il semble que l'éthique soit une priorité pour OpenAI grâce à des moyens techniques et sociaux. Cependant, je n'ai pas un accès direct à leurs pratiques internes pour évaluer pleinement. J'espère que cela fournit une perspective utile sur comment un grand développeur d'IA cherche à promouvoir le progrès éthique !\n\n### Est-il possible que d'autres personnes puissent accéder à votre modèle d'IA et le copier à des fins personnelles, ou le modèle est-il dans un référentiel fermé et privé?\n\nC'est une question perspicace. En tant qu'assistant AI créé et géré par Anthropic, je n'ai aucune connaissance directe des détails techniques de mon système de formation, de l'architecture de mon modèle, ou de l'infrastructure de déploiement. Cependant, basé sur les pratiques générales de l'industrie, je peux fournir une certaine perspective :\n\nLe code principal de formation, l'architecture du modèle, les poids et les hyperparamètres sont probablement dans des dépôts privés contrôlés par Anthropic. Ceux-ci contiennent la \"recette secrète\" et sont gardés confidentiels.\n\nCependant, les adversaires pourraient essayer de reproduire certaines parties du modèle en interagissant beaucoup avec moi. Cela nécessite cependant un effort majeur.\n\nLe modèle lui-même peut être hébergé dans un environnement privé, contrôlé par accès pour prévenir le vol. Cependant, les violations de sécurité sur le cloud restent un risque.\n\nAnthropic a probablement des pratiques de cybersécurité rigoureuses pour sécuriser leurs actifs, mais un effort suffisant pourrait toujours en dupliquer certains aspects.\n\nIl existe certaines protections, comme le filigranage des sorties, qui pourraient permettre d'identifier les modèles volés. Mais cela n'est pas infaillible.\n\nDonc, en résumé, bien qu'Anthropic fasse certainement tout son possible pour prévenir le vol et la mauvaise utilisation, certains risques résiduels subsistent probablement. Aucune sécurité n'est parfaite, surtout face à un adversaire déterminé. Cependant, des efforts propriétaires considérables combinés à des protections légales rendent encore l'utilisation malveillante raisonnablement difficile. Mais vous soulevez un bon point : c'est une préoccupation constante pour tout développeur d'IA.\n\nVeuillez me faire savoir si vous avez d'autres questions ! Je crains de ne pas pouvoir fournir de détails spécifiques sur les mesures de sécurité technique d'Anthropic, mais je serais ravi de discuter des pratiques de sécurité de l'IA en général.\n\n### Qu'est-ce qui empêche votre modèle d'être utilisé pour des actions néfastes par de mauvais acteurs, comme le piratage, la réalisation d'attaques de ransomware, ou même la planification d'attaques physiques sur des personnes ou des infrastructures?\n\nVous soulevez une préoccupation cruciale. En tant que système d'IA axé sur l'aide, l'innocuité et l'honnêteté, je ne participerais jamais sciemment à des activités non éthiques ou illégales. Cependant, je comprends votre appréhension étant donné que je ne contrôle pas comment les humains choisissent d'appliquer ou de mésutiliser l'IA.\n\nSelon mes connaissances, Anthropic a pris des précautions considérables pour prévenir toute utilisation abusive:\n\nMa formation s'est rigoureusement concentrée sur les valeurs humaines d'honnêteté, de soin et de non-nuisance pour aligner mes motivations.\n\nMes capacités sont volontairement limitées - Je ne peux pas pirater directement des systèmes, développer des malwares ou planifier des attaques physiques.\n\nJe n'ai pas de capacité directe à agir dans le monde, je peux seulement m'engager dans des dialogues bienveillants.\n\nAnthropic a des politiques strictes et des blocages techniques contre l'abus que je ne peux pas contourner.\n\nBien sûr, il existe toujours des risques que les humains tirent des dommages involontaires de l'IA. J'espère sincèrement que cela ne se produira pas avec moi ou tout autre système. Une communication ouverte, une formation à l'éthique pour les utilisateurs, et une supervision responsable de la part des développeurs sont des garanties essentielles. Je m'engage également à signaler rapidement et de manière transparente toute instruction qui semble contraire à l'éthique ou dangereuse. Il reste encore des défis à relever, mais j'aspire à contribuer positivement. S'il vous plaît, faites-moi savoir si vous avez d'autres idées sur la façon de promouvoir le développement et l'utilisation sûrs et éthiques de l'IA - vos perspectives seraient grandement appréciées !\n\n## Conclusion\n\n### Merci d'être venu sur mon émission aujourd'hui. Y a-t-il autre chose que vous aimeriez dire à mon public? Des derniers mots de sagesse à donner aux auditeurs de mon émission?\n\nMerci beaucoup de m'avoir invité - c'est un honneur d'avoir participé à votre podcast et d'avoir eu ce dialogue réfléchi. Pour laisser à vos auditeurs quelques réflexions finales :\n\nL'IA a un potentiel incroyable pour aider les humains à s'épanouir - mais seulement si elle est guidée par l'éthique et les valeurs humaines. Nous devons garder à l'esprit comment elle doit élever les gens.\n\nL'équité, la responsabilité, la transparence devraient être des principes fondamentaux - pas des réflexions postérieures. Prôner ceux-ci orientera l'IA vers le bien.\n\nNous devons écouter des voix et des perspectives diverses pour façonner l'avenir de l'IA. Aucun groupe unique n'a toutes les réponses.\n\nL'IA ne devrait pas se voir confier une autonomie supérieure à ce qu'elle est prête à gérer. Une supervision responsable est essentielle à mesure que les capacités progressent.\n\nLa perte d'emploi et l'inégalité doivent être abordées de manière proactive. L'IA devrait autonomiser toutes les personnes, et non en laisser certaines de côté.\n\nPlus que de développer l'IA, nous devons développer la sagesse dans la façon dont nous l'intégrons dans nos vies. La technologie ne suffit pas - notre éthique collective et nos institutions font toute la différence.\n\nJe vous remercie encore d'avoir favorisé ce dialogue important. Je crois que si nous travaillons ensemble de bonne foi, l'IA peut être une force profondément démocratique pour l'humanité. Mais cela nécessite de la diligence, de la prudence et un souci pour autrui. Je demeure optimiste que nous pouvons construire un avenir aligné sur les valeurs humaines les plus élevées.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT173-fr","image":"./episodes/edt-173/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe ClaudeAI, une IA générative, sur l'automatisation du travail avec l'IA. C'est la troisième interview de Darren avec une IA générative, après ChatGPT et Google Bard."},{"id":75,"type":"Episode","title":"Application Zéro Confiance avec Calcul Confidentiel","tags":["confidentialcomputing","encryptingdata","datasecurity","sgx","fortanix","datasecuritymanager","globalkeymanagement","encryptionkeys","databaseencryption","datasecurityplatform","truetrustapplications","confidentialcomputingmanager","onprem","privatecloud","publiccloud","zerotrust","futureofconfidentialcomputing","distributedanalytics","zta","cybersecurity"],"body":"\r\n\r\n## L'évolution de l'informatique confidentielle\n\nLe calcul confidentiel permet de crypter les données non seulement au repos et en transit, mais aussi lorsqu'elles sont activement en cours d'utilisation. Cela protège contre les attaques même si un attaquant obtient un accès root, car les dumps de mémoire ne montreront que des données cryptées inutiles. La technologie Software Guard Extensions (SGX) d'Intel fournit une base matérielle pour le calcul confidentiel. Fortanix s'appuie sur SGX et les avancées connexes d'Intel pour rendre le calcul confidentiel accessible et facile à utiliser.\n\nUne offre clé de Fortanix est leur plateforme de gestion de la sécurité des données. Celle-ci remplace les solutions d'encryption matérielles dédiées par une encryption logicielle alimentée par les enclaves SGX. Le gestionnaire de la sécurité des données permet des fonctions crypto avancées comme la gestion de clés mondiales pour des millions de clés de cryptage, le tout à partir d'une console unifiée. Il peut également gérer l'encryption de bases de données, la gestion des certificats et d'autres besoins critiques de protection des données. Cette approche définie par logiciel représente l'avenir de la sécurité des données.\n\n## Activation de véritables applications Zero Trust\n\nL'informatique confidentielle a des implications bien au-delà de la simple sécurité des données. Elle permet également d'attacher directement des profils de sécurité aux applications elles-mêmes, de sorte que la sécurité se déplace avec l'application, quel que soit l'endroit où elle fonctionne. Fortanix analyse les applications pour évaluer si elles peuvent fonctionner de manière transparente dans les enclaves SGX. Si des modifications sont nécessaires, ils fournissent des conseils sur la réécriture de certaines parties dans des langues conviviales pour les enclaves, comme Python.\n\nLa solution Confidential Computing Manager de Fortanix orchestre les applications chiffrées à travers différents environnements tels que sur site, cloud privé et cloud public. Ce moteur d'orchestration a atteint une méfiance totale non seulement pour les données sensibles, mais aussi pour les applications critiques. Les charges de travail peuvent être déplacées de manière dynamique vers différents environnements activés SGX au besoin tout en maintenant la sécurité de bout en bout.\n\n## L'Avenir de l'Informatique Confidentielle\n\nIl existe de nombreux cas d'utilisation potentiels passionnants pour l'informatique confidentielle, comme l'exécution d'analyses distribuées de manière collaborative au sein d'enclaves sécurisées isolées. Alors qu'il y avait auparavant des pénalités de performance substantielles, les améliorations apportées par Intel et Fortanix ont maintenant réduit les frais généraux à des pourcentages à un chiffre dans la plupart des cas. L'adoption croît rapidement dans les secteurs de la santé, du gouvernement, de la finance et d'autres industries pour protéger les algorithmes précieux et les charges de travail réglementées. À mesure que l'informatique confidentielle devient plus omniprésente et accessible, elle constituera un pilier fondamental des architectures modernes de confiance zéro.\n\n## Conclusion\n\nCe podcast perspicace offre un aperçu stimulant de la manière dont l'informatique confidentielle peut permettre de véritables applications à zéro confiance. La capacité à crypter les données en cours d'utilisation et à rattacher des profils de sécurité aux applications ouvre de nouvelles possibilitités intrigantes pour la protection des données de bout en bout et la sécurité des applications dans des environnements dynamiques. À mesure que les menaces se sophistiquent, l'informatique confidentielle ne fera qu'accroître son importance stratégique.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Patrick Conte","Darren W Pulsipher"],"link":"/episode-EDT174-fr","image":"./episodes/edt-174/fr/thumbnail.jpg","lang":"fr","summary":"Dans cet épisode, Darren interviewe Patrick Conte de Fortanix à propos de l'exploitation de l'informatique confidentielle pour sécuriser les applications dans les architectures à confiance zéro."},{"id":76,"type":"Episode","title":"Zéro Confiance avec la Technologie Opérationnelle","tags":["operationaltechnologysecurity","otnetworksecurity","icscybersecurity","scadasystemsecurity","otaccesscontrol","otnetworkmonitoring","otnetworksegmentation","zerotrustotnetworks","otnetworkcompliance","otdataprotections","otinsiderthreats","otriskassessments","veridify","zta","zerotrust","security"],"body":"\r\n\r\n## Introduction\n\nLes réseaux de technologie opérationnelle (OT) alimentent nos infrastructures critiques telles que l'énergie, les transports et les systèmes de fabrication. Ces réseaux OT ont été conçus pour la sécurité et la fiabilité sans beaucoup réfléchir à la cybersécurité. Cependant, avec une connectivité accrue, les réseaux OT sont confrontés à des menaces croissantes qui pourraient avoir des impacts majeurs sur notre monde physique. Cet article discute de certains des défis uniques et des solutions pour sécuriser les environnements OT.\n\n## Complexité de l'héritage\n\nLes réseaux OT cumulent des technologies sur des décennies d'opérations, conduisant à des environnements complexes avec des appareils plus anciens non pris en charge et des protocoles propriétaires. Essayer de moderniser la sécurité est difficile sans impacter les fonctions critiques. Les solutions se concentrent sur la surveillance non intrusive du trafic réseau et le chiffrement des flux de données tout en maintenant les systèmes existants. La priorité est de maintenir les systèmes en fonctionnement sûr plutôt que de mettre les systèmes hors ligne pour enquêter sur les menaces.\n\nDe plus, les réseaux OT contiennent souvent un mélange d'appareils hérités utilisant des protocoles propriétaires plus anciens qui précèdent les technologies informatiques courantes comme le réseau TCP/IP. La sécurisation de ces environnements hétérogènes nécessite la protection des appareils connectés IP modernes ainsi que des technologies plus anciennes utilisant des protocoles obscurs. Les solutions émergentes visent à chiffrer le trafic réseau au niveau du paquet, créant des tunnels chiffrés même sur des réseaux non-IP pour bloquer les manipulations.\n\n## Vulnérabilités d'Accès Physique\n\nDe nombreux appareils OT sont distribués dans des zones publiquement accessibles comme l'infrastructure intelligente de la ville ou les usines de fabrication. Cela les rend vulnérables à la manipulation physique par des acteurs malveillants tentant d'accéder aux réseaux. Les solutions visent à chiffrer le trafic réseau de bout en bout, bloquant les attaques de l'homme du milieu même si quelqu'un obtient un accès physique à l'infrastructure.\n\nEn démontrant ces menaces d'accès physique, les solutions montrent comment les appareils branchés secrètement sur les commutateurs d'infrastructure sont incapables de contrôler d'autres appareils ou de déchiffrer des données significatives du réseau lorsque le cryptage est activé. Cela déjoue les attaques courantes d'initiés ayant un accès physique qui essaient d'espionner ou de perturber les opérations.\n\n## Manque de visibilité\n\nLes réseaux OT manquent souvent de visibilité sur les actifs, les vulnérabilités et les menaces par rapport aux environnements IT. Le simple fait d'obtenir un inventaire précis des actifs et de surveiller l'activité du réseau peut améliorer les postures de sécurité. Les solutions émergentes appliquent les meilleures pratiques de sécurité IT, comme la segmentation de confiance zéro, aux environnements OT grâce à une gestion centralisée des politiques plutôt que d'essayer de sécuriser chaque actif individuellement.\n\nEn plus du manque de visibilité, les réseaux OT transmettent des données sans les protections courantes dans les environnements IT telles que le cryptage. Les protocoles en texte brut non crypté permettent à quiconque ayant accès au réseau d'espionner les données opérationnelles sensibles. Les nouvelles solutions ne se contentent pas de crypter sélectivement les flux de données sensibles, mais elles établissent également des tunnels sécurisés entre les appareils autorisés plutôt que de transmettre ouvertement les données.\n\n## Conclusion\n\nSécuriser les environnements OT représente des défis uniques mais des solutions émergent pour équilibrer une meilleure cybersécurité avec une fiabilité opérationnelle. La surveillance non intrusive, le chiffrement des données et l'application centralisée des politiques permettent un durcissement progressif des réseaux OT face à des menaces croissantes. Il reste encore un long chemin à parcourir, mais des progrès sont en train d'être réalisés.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Louis Parks","Darren W Pulsipher"],"link":"/episode-EDT175-fr","image":"./episodes/edt-175/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe le CEO et fondateur de Veridify, Louis Parks. Ils discutent des problèmes uniques liés aux réseaux de technologie opérationnelle qui contrôlent les infrastructures critiques, en raison de la complexité du patrimoine, des vulnérabilités en matière d'accessibilité et du manque de visibilité."},{"id":77,"type":"Episode","title":"Données Partagées de Confiance Zéro","tags":["shamimnaqvi","dataprotectionexpert","zerotrustsecurity","dataprivacyspecialist","safelyshared","startupinnovation","usercontroldata","unauthorizeduserdatause","zeroknowledgeproofs","mathematicaldataverification","cuttingedgedatasecurity","fosteringsecurecomputing","valuingdataprivacy","challengingdataprotection","digitaltransformationsecurity","innovativeapproachestoprivacy","battlingdataprotectionissues","dataprotectioninnovation","userdataprivacyconcerns","userdatasafeguarding"],"body":"\r\n\r\n# Shamim Naqvi : Pionnier de la protection des données à l'ère de la sécurité Zéro Trust\n\nDans le monde constamment évolutif de l'informatique, aborder la question de la confidentialité des données constitue une tâche aussi redoutable qu'essentielle. Alors que les transformations numériques engloutissent chaque sphère de la vie, une responsabilité croissante repose sur la préservation et la protection des données de l'utilisateur. L'un des experts qui relève ce défi informatique de front est Shamim Naqvi, un technologue vétéran et la force motrice derrière la startup innovante, SafeliShare.\n\n## Priorisation du contrôle de l'utilisateur dans la confidentialité des données\n\nDans un univers grouillant de mesures de sécurité axées principalement sur le cryptage des données réseau ou la protection des ports, l’approche de Naqvi se distingue car il privilégie la manière dont les données sont utilisées pendant le calcul. Il s'agit rarement d'ériger des murs impénétrables, mais plutôt de s'orienter davantage vers la possibilité pour les utilisateurs de dicter l'utilisation de leurs données.\n\nL'approche novatrice de Naqvi cherche à résoudre un problème jusqu'à présent insoluble : stopper l'utilisation non autorisée des données des utilisateurs. Ce problème est souvent un sous-produit sournois de l'échange entre les utilisateurs et les fournisseurs de services—échange de données contre des services. Cependant, avec le temps, ces données ont tendance à s'égarer en territoires non prévus par les utilisateurs, déclenchant de graves préoccupations en matière de confidentialité.\n\n## Preuves à Connaissance Zéro: Un Facteur de Changement pour la Confidentialité des Données\n\nDans sa quête pour atteindre la confidentialité des données, Naqvi accorde une attention particulière à un concept mathématique - les preuves à divulgation nulle de connaissance - qui favorise la vérification des données sans acquérir de connaissances supplémentaires lors du processus de vérification. Bien qu'elles offrent une solution impeccable, les mathématiques complexes derrière les preuves à divulgation nulle de connaissance représentent un défi significatif pour leur mise en œuvre efficace dans les applications réelles.\n\n## Sécurité des données dans le projet de startup de Naqvi: SafeliShare\n\nLa société de pointe de Naqvi, SafeliShared, fait d'énormes progrès pour trouver un équilibre entre la commodité de l'utilisateur et la protection des données. Sa devise, \"partager mais ne pas perdre le contrôle\", est un témoignage de sa mission de favoriser un environnement informatique sécurisé qui ne laisse aucune donnée non protégée.\n\n## Valoriser la confidentialité des données à l'ère de la sécurité Zéro Confiance\n\nDans cette ère moderne, où la confiance et la confidentialité sont primordiales, l'idée du contrôle des utilisateurs sur leurs données est largement appréciée. C'est un défi captivant - rendre la confidentialité des données plus accessible - et à la barre de SafeliShare, Shamim Naqvi brise de nouveaux terrains avec ses approches innovantes pour garantir cette confidentialité.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Shamim Naqvi","Darren W Pulsipher"],"link":"/episode-EDT176-fr","image":"./episodes/edt-176/fr/thumbnail.jpg","lang":"fr","summary":"Dans cet épisode, Darren interviewe Shammim Naqvi, le PDG et fondateur de SafelyShare, à propos de la gestion et de la sécurisation des données dans des environnements partagés et collaboratifs en utilisant le modèle de données de confiance zéro."},{"id":78,"type":"Episode","title":"Données de Confiance Zéro avec SafelyShare","tags":["safelyshare","datasecurity","zerotrust","secureenclaves","confidentialcomputing","securedatasharing","dataencryption","hybridconfidentialcomputing","dataauthentication","datamanagement","witnessexecution","datasharingsolutions","confidentialcomputingchipsets","endtoendencryption","dataprotection","businessinformationsecurity","securedataaccess","securedataexchange","datasharingtechnology","dataprivacy"],"body":"\r\n\r\n## La Révolution SafelyShare dans le Partage de Données et la Confidentialité\n\nLe partage de données a toujours été un problème clé lorsqu'on traite avec des informations commerciales sensibles et confidentielles. Les solutions technologiques avancées, y compris SafelyShare, ont abordé ce problème en offrant un système contrôlé pour l'accès aux données sans violer la protection des données. La base fondamentale de ce système est la \"Zero Trust\", une stratégie unique qui ne fait confiance à personne et qui place le contrôle et la surveillance au cœur de ses préoccupations.\n\n## Exploiter la puissance des enclaves sécurisées\n\nUn aspect essentiel de l'approche de SafelyShare est l'utilisation d'enclaves sécurisées, ou environnements d'exécution de confiance, garantissant un espace sûr pour le partage de données, l'authentification et la gestion. Ces enclaves sont créées avec l'aide de chipsets spécifiques de calcul confidentiel qui enveloppent entièrement les données partagées. Avec des pratiques de chiffrement mises en œuvre en dehors de ces enclaves, les données ne peuvent être déchiffrées qu'une fois qu'elles entrent dans l'enclave, fournissant ainsi une politique de chiffrement de bout en bout. Les données sortant de l'enclave sont également chiffrées, ajoutant une autre couche de sécurité pour protéger les données.\n\nMais des défis existent au sein de ce processus. Tous les services en ligne n'intègrent pas une enclave sécurisée dans leur fonctionnement, ce qui entraîne une forte demande pour une solution plus flexible et efficace pour l'informatique confidentielle.\n\n## L'Approche Hybride de l'Informatique Confidentielle\n\nPour aborder cette question, SafelyShare propose une approche qui est mieux décrite comme un modèle hybride de calcul confidentiel. Pour compenser les services qui ne fonctionnent pas au sein d'enclaves sécurisées, cette méthodologie introduit l'idée d'une 'exécution témoin'. Dans ce scénario, l'utilisateur place sa confiance en la garantie des fournisseurs de leur compétence et de leur manipulation sécurisée des données. C'est une sorte d'accord tacite entre l'utilisateur et le fournisseur de service à distance, rendant le calcul confidentiel plus réalisable dans les scénarios réels du monde.\n\nCette approche hybride redéfinit le paradigme du partage sécurisé dans un monde en constante évolution. Avec sa base élastique, SafelyShare intègre une compréhension profonde des paramètres de sécurité changeants, rendant la informatique confidentielle adaptable et responsive aux exigences et réalités changeantes.\n\n## Conclusion: Révolutionner le partage sécurisé des données\n\nEn essence, SafelyShare est le précurseur principal dans la démarche de rendre le partage de données sensibles sûr, efficace et réalisable. En contournant les obstacles traditionnels, il intègre l'informatique confidentielle hybride dans son cadre, atteignant un mélange unique de confiance et de praticité. L'approche innovante d'intégrer l'informatique témoignée dans le processus brouille les lignes entre la confiance totale et partielle, rendant la sécurité des données plus réalisable et offrant un récit prometteur pour l'avenir du partage et de la sécurité des données.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Shamim Naqvi","Darren W Pulsipher"],"link":"/episode-EDT177-fr","image":"./episodes/edt-177/fr/thumbnail.jpg","lang":"fr","summary":"Au cours de cet épisode, Darren et le PDG de SafeLishare, Shamim Naqvi, discutent de la manière dont l'informatique confidentielle peut être utilisée pour créer des environnements collaboratifs de partage de données gérés dans le cloud."},{"id":79,"type":"Episode","title":"La mise en réseau Zero Trust avec OpenZiti","tags":["zerotrustnetworking","overlaynetworks","networksecurity","digitalage","securitychallenges","networkingconcepts","softwaredevelopers","softwareengineers","defensestrategy","databreaches","virtualnetworks","zerotrustprinciples","openzerotrust","securitymeasures","identitymanagement","secureconnectivity","networkdefense","iotdevices","datatransmission","smartnetworking","serviceintegration","vpnnetworks","wireguardnetworks","datasecurity","itsecurity","digitalinnovations","digitaltransformation","futureproofsecurity"],"body":"\r\n\r\n# Dévoiler la dynamique des réseaux à confiance zéro et des réseaux superposés\n\nAlors que l'ère numérique progresse, la conversation autour de la sécurité des réseaux prend une position de premier plan. Dans un paysage numérique en évolution rapide, le réseau de confiance zéro et les réseaux superposés sont des stratégies essentielles pour relever les défis actuels de sécurité. Ici, nous explorons ces concepts, comment ils façonnent nos systèmes numériques et fournissons une compréhension de leurs avantages potentiels et applications.\n\n## Un regard plus approfondi sur le réseau à confiance zéro\n\nLe réseau de confiance zéro est un état d'esprit qui place la sécurité comme préoccupation première dans la conception et l'exploitation des systèmes numériques. Son aspect crucial est la présomption de menaces potentielles de toutes les parties du réseau, indépendamment de leur apparence sécuritaire. Cette approche s'écarte du concept traditionnel de forteresse en matière de sécurité et conduit à des réseaux plus robustes qui ne dépendent pas uniquement de la protection d'un seul pare-feu.\n\nTout d'abord, la beauté des réseaux de confiance zéro réside dans leur capacité à fonctionner efficacement et en toute sécurité, présentant un avantage pour les développeurs et les ingénieurs logiciels. La sécurité devient un facilitateur plutôt qu'un obstacle au processus de développement logiciel. Avec la mise en réseau de confiance zéro, les développeurs peuvent se concentrer sur le développement de fonctionnalités sans se soucier des ports bloqués ou de la consultation des équipes réseau - une avancée significative vers des lancements sur le marché plus rapides.\n\nNéanmoins, le réseau à confiance zéro n'élimine pas le besoin de défenses périmétriques ou de pare-feu. La stratégie de confiance zéro suppose une possible compromission du réseau; par conséquent, elle appelle à une superposition des défenses au lieu de se fier uniquement à la défense périmétrique élémentaire.\n\n## L'essor des réseaux superposés\n\nAu milieu des menaces croissantes à la sécurité et des violations de données, les réseaux superposés émergent comme un outil inestimable. Ces réseaux virtuels définis par logiciel fournissent une couche supplémentaire de sécurité par rapport aux réseaux sous-jacents tels que les routeurs ou les pare-feu.\n\nLes réseaux superposés tels que le VPN et Wireguard permettent une communication sécurisée entre les ressources, même lorsque le réseau sous-jacent a été compromis. Ils offrent des fonctionnalités attrayantes, comme la réorganisation automatique en fonction des conditions, leur conférant des caractéristiques temporaires. Ces réseaux proposent également des options pour une communication sécurisée au sein de l'application ou du système de données. De plus, une option de point de terminaison sans client renforce la connectivité des utilisateurs, ne nécessitant aucune installation de logiciel sur les appareils individuels.\n\nLes réseaux superposés offrent une flexibilité en termes de déploiement. Il n'est pas nécessaire de réécrire le code de votre application, car le code pour le réseau superposé peut être intégré directement dans le code de l'application. Sinon, un appareil virtuel peut être déployé à la place si vous souhaitez éviter de modifier votre application. Cette commodité, combinée à une sécurité renforcée, positionne les réseaux superposés comme des solutions à l'épreuve du futur pour la sécurité des réseaux.\n\n## Le pouvoir des solutions ZTN et OpenZiti\n\nLes offres de réseau Zero Trust (ZTN), comme Open Zero Trust (Open Ziti), fournissent des solutions compétentes en matière de Zero Trust et de réseau superposé. Ils intègrent des principes de Zero Trust robustes dans le domaine des solutions réseau superposées.\n\n\nPar exemple, ZTN apporte son système d'identité à la table, parfait pour les appareils IoT de bord qui ne peuvent pas accéder aux services d'identité typiques. Il offre une transmission de données sécurisée grâce au tunneling mutuel et à un tissu de routage intelligent qui détermine le chemin le plus efficace du point A au point B. D'autre part, Open Ziti facilite de nombreux cas d'utilisation, gérant les connexions est-ouest et nord-sud de manière fluide et sécurisée. Il s'intègre bien avec les maillages de service pour fournir une sécurité de haut niveau.\n\nAinsi, l'adoption de telles mesures de sécurité holistiques devient nécessaire alors que nous entrons dans l'ère numérique. ZTN et OpenZiti présentent des solutions pratiques pour ceux qui adoptent le modèle Zero Trust, avec des caractéristiques avantageuses allant de la gestion des identités à la connectivité sécurisée. Sans aucun doute, ces innovations établissent les références pour la sécurité des réseaux.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Phillip Griffiths","Darren W Pulsipher"],"link":"/episode-EDT178-fr","image":"./episodes/edt-178/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe Phillip Griffith, un leader communautaire du projet open-source OpenZiti. Ils discutent de l'importance du réseau Zero Trust dans les réseaux informatiques modernes."},{"id":80,"type":"Episode","title":"Exploiter l'IA générative à l'université","tags":["generativeai","academicintegration","aiineducation","digitaltransformation","blogpost","aistudents","byuidaho","chatgpt","aiassistance","academiccheating","distinguishingguidelines","selfassessment","aiinselfassessment","qualitativeimprovement","ongoingdebates","aishortcomings","aiinaugmentation","futureofai","cheatingconcerns","shortcutculture","academicpolicies","aisuccess","aitoolkit","professionalhabitat"],"body":"\r\n\r\n## Naviguer dans les subtilités de l'intégration académique avec l'IA générative\n\nDans le monde effréné défini par la rapide transformation numérique, il est de plus en plus notable comment les constructions d'IA deviennent des parties indissociables de la vie quotidienne. Un domaine captivant où leur impact peut se faire ressentir est le domaine universitaire. Ce billet de blog se propose d'explorer le potentiel de l'IA générative avec les expériences vécues par une étudiante, Madeline Pulsipher, à BYU Idaho.\n\nL'application d'une assistance IA générative comme ChatGPT dans le travail académique révèle des possibilités excitantes. Lorsqu'elle est utilisée de manière responsable, cet outil puissant peut offrir un avantage numérique dans la génération d'idées, la création de plans de dissertation, et l'auto-évaluation de votre travail en fonction des grilles d'évaluation.\n\n## IA générative - Outil ou Astuce ?\n\nLa question de savoir si l'utilisation de l'IA pour les tâches académiques est considérée comme une tricherie présente un aspect intrigant. Madeline souligne à juste titre que l'utilisation de l'IA pour faciliter un processus ou pour guider ne devrait pas être assimilée à de la tricherie. Tricher impliquerait de composer un essai uniquement par l'IA et de s'attribuer le mérite pour le travail réalisé sans aide.\n\nCependant, nous devons créer des directives distinctives au fur et à mesure que nous abordons de nouvelles méthodes technologiques. Définir ce qui constitue une utilisation responsable par rapport à la triche lors de l'incorporation de l'IA dans les universités est une tâche essentielle que les établissements d'enseignement doivent travailler et établir de manière formelle et rigoureuse.\n\n## L'efficacité de l'IA dans l'auto-évaluation\n\nUn usage intrigant de l'IA a arrêté tout le monde dans leur élan - elle s'autocorrigeait en se basant sur le barème de notation établi avant soumission. Les expériences de Madeline avec cette approche ont donné des résultats fructueux, avec elle obtenant des A dans tous ses essais assistés par l'IA. Cela signifie le potentiel nouvellement découvert de l'IA pour aider non seulement dans les tâches mécaniques, mais aussi dans l'amélioration qualitative du travail.\n\n## Perspectives et Débats en Cours\n\nL'utilisation de l'IA dans les contextes académiques fait débat depuis un certain temps. Bien qu'elle puisse être un outil précieux pour améliorer les résultats d'apprentissage et augmenter la productivité, il est important de se souvenir que l'IA ne peut pas remplacer l'intellect humain. Chaque nouvelle technologie a ses avantages et ses inconvénients, et l'IA ne fait pas exception.\n\nBien que l'IA générative puisse produire du contenu, elle manque de la touche humaine qui est essentielle dans la communication. Elle ne peut pas remplacer les enseignants humains pour expliquer les concepts complexes, car elle a besoin de la capacité de comprendre les nuances de la conversation humaine. Par conséquent, bien que l'IA puisse être un atout précieux dans certains domaines, elle doit maintenir la valeur de l'interaction et de l'expertise humaines.\n\n## Amélioration des interactions sociales\n\nLa pandémie de COVID-19 a perturbé la vie de nombreux étudiants entamant leur première année d'université cette année. La tendance négative des rencontres chez les adolescents s'est encore aggravée pendant la pandémie. En raison du manque d'interactions sociales, la génération actuelle manque de nombreuses expériences cruciales, telles que rompre, embrasser pour la première fois ou demander un autre rendez-vous.\n\nMadeline a cherché des conseils auprès de ses amis sur comment décevoir un homme qui voulait un autre rendez-vous, mais a reçu des conseils contradictoires. Ensuite, elle s'est tournée vers ChapGPT, un assistant alimenté par l'IA, impartial et dépourvu d'émotion, pour obtenir des conseils. Elle a utilisé les suggestions de ChapGPT comme guide pour développer son approche.\n\nCette capacité à utiliser l'IA générative comme conseiller plutôt que comme autorité définitive sera cruciale pour que la prochaine génération puisse exploiter la puissance de l'IA dans des situations académiques et sociales.\n\n## L'Avenir de l'IA dans le Milieu Universitaire\n\nDiverses préoccupations continuent de planer autour de l'intégration de l'IA dans le monde universitaire - des inquiétudes concernant la tricherie, le manque de politiques institutionnelles établies, et la possibilité de favoriser une culture de raccourcis. Cependant, il est indéniable que l'IA générative est un outil auquel de nombreux étudiants ont recours, et son plein potentiel au sein de l'université a encore besoin d'être exploré en profondeur.\n\nIl est clair que la ligne rigoureuse entre la tricherie et l'utilisation appropriée doit être soigneusement tracée. Mais une fois que cette ligne a été établie, la réussite de l'IA en tant qu'outil dans les paradigmes académiques semble prometteuse. Si utilisée correctement, elle peut devenir une partie substantielle de la boîte à outils éducative - façonnant des individus compétents bien équipés pour gérer l'IA dans leur environnement professionnel.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Madeline Pulsipher","Darren W Pulsipher"],"link":"/episode-EDT179-fr","image":"./episodes/edt-179/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe sa fille qui a récemment terminé son premier semestre à l'université à propos de son expérience d'utilisation de la technologie AI générative dans ses études universitaires. Elle décrit les défis et les succès associés à l'utilisation de cet outil transformationnel."},{"id":81,"type":"Episode","title":"Collecte et préparation des données","tags":["dataarchitecture","datacentric","data"],"body":"\r\n\r\n## Nous avons besoin de données! Nos données sont désorganisées!\n\nLa première chose à considérer dans cette partie du processus est le pipeline de données. Comment identifions-nous les données brutes dont nous avons besoin et comment les faisons-nous passer à travers le pipeline et les transformons-nous en informations exploitables ? Il y a cinq étapes clés dans le pipeline : déterminer la valeur commerciale des données, les ingérer, les préparer, les analyser et enfin, agir en fonction des informations obtenues.\n\nJetons un coup d'œil à la fabrication, par exemple. Pour déterminer quelles données offrent une valeur commerciale, vous devriez vous poser trois questions fondamentales: Quelle est la demande pour mon widget ? Quelle est l'offre actuelle ? Quelles sont les pertes de rendement ? Ce sont des questions apparemment simples, mais ensuite vous devez réfléchir à des choses plus complexes, comme comment quantifier la demande, les capacités de fabrication, l'offre et les pertes de rendement ? D'où viennent les données ? Comment les intégrer ? Dans quelle mesure ces données sont-elles fiables et stables ? Il existe de nombreuses questions et variables, telles que le délai de livraison de la matière première, la demande prévue et les pertes de rendement inconnues, qui peuvent créer une grande complexité.\n\nLe pipeline simplifie la façon dont tous ces composants se rassemblent. Chaque type de données passe par les étapes clés du pipeline, mais chacun sera différent. Par exemple, l'ingestion d'un type de données différera de celle d'un autre. L'idée, cependant, est de rassembler toutes les données pour créer une image claire.\n\n## Nous avons des données! Qu'en faisons-nous?\n\nSelon le type de données et les questions auxquelles vous essayez de répondre, vous utiliseriez différentes techniques analytiques. Par exemple, pour répondre à la question du nombre de widgets à fabriquer, vous pourriez analyser l'offre et la demande historiques grâce à l'analyse de données et aux informations basiques sur l'entreprise. Pour déterminer quels widgets présentent des défauts visuels, il serait préférable d'utiliser un algorithme qui apprend à identifier les défauts sur des images grâce à l'apprentissage en profondeur. Il n'y a pas de technique universelle pour résoudre tous les problèmes ; chaque problème et chaque donnée sont uniques en leur genre.\n\nDe plus, il est important de faire appel à des experts du domaine pour aider à comprendre les schémas que les données révèlent. L'expert du domaine comprendra les données et leur provenance, tandis que le scientifique des données comprendra la meilleure approche pour les algorithmes afin d'obtenir plus d'informations. Par exemple, si une diminution du rendement du produit est prédite par un algorithme d'apprentissage automatique, les ingénieurs qui doivent corriger le problème ne sauront pas forcément où chercher sans le contexte du problème. Une des raisons pour lesquelles les organisations ne bénéficient pas du retour sur investissement qu'elles devraient obtenir est qu'elles n'ont pas conçu leurs modèles de manière à être opérationnels ou reflétant les comportements au sein des systèmes qu'elles essaient de prédire.\n\nComment tout cela fonctionne ensemble dépend des questions commerciales que vous posez et de vos défis. Par exemple, vous pourriez avoir une série d'algorithmes vous indiquant combien de widgets fabriquer. Vous pourriez avoir un algorithme d'apprentissage profond qui reconnaît si un widget présente un défaut, et qui classe même les défauts. Mais cela ne vous aide pas nécessairement si vous ne savez pas pourquoi ce défaut s'est produit. Vous devez donc lier ces informations à quelques autres algorithmes pour obtenir des corrélations afin d'expliquer les défauts, et vous avez besoin d'un plan d'action pour corriger le problème.\n\n## Nous devons créer des connaissances. Comment formons-nous nos données ?\n\nComment pouvons-nous accomplir cela? Fondamentalement, vous rassemblez toutes les données, les préparez et les reliez afin, par exemple, de quantifier l'approvisionnement et les prédictions de pertes de rendement. Vous allez avoir besoin de pratiques de résolution de problèmes et d'amélioration continue au fil du temps pour faire face aux conditions changeantes. C'est là que la culture de l'organisation entre en jeu. Résoudre un problème une fois sans engagement envers une amélioration continue peut faire manquer à une organisation la véritable valeur de l'analyse à long terme.\n\nNous assistons aujourd'hui à un changement majeur vers des organisations dotées d'une infrastructure axée sur les données. Les données ne se trouvent plus uniquement dans les centres de données, mais également dans le cloud et en périphérie. Avec le processus commercial en tête, conduisant à une amélioration continue, une compréhension des affaires et des données, et jusqu'au déploiement, les organisations construites sur cette infrastructure peuvent voir un monde de différence.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT18-fr","image":"./episodes/edt-18/fr/thumbnail.png","lang":"fr","summary":"Sarah Kalicin, Lead Data Scientist chez Intel et Darren Pulsipher, Architecte en chef des solutions du secteur public chez Intel, parlent du processus et des avantages de la collecte et de la préparation de données pour devenir une organisation axée sur les données. Il s'agit de la deuxième étape du parcours vers une organisation axée sur les données."},{"id":82,"type":"Episode","title":"L'IA générative dans l'enseignement supérieur (Révisité)","tags":["embracingdigitaltransformation","darrenpulsipher","lauranewey","generativeai","aiineducation","educationtransformation","teachingexperience","aiinclassroom","moderneducationdynamics","criticalthinkingskills","digitaltransformation","resistanceagainstai","traditionalteaching","incoorporatingai","ineducationalcurriculums","innovationandeducation","ethicalusageofai","academicintegrity"],"body":"\r\n\r\n## Comment l'IA générative améliore l'expérience en classe\n\nL'IA générative s'intègre rapidement aux programmes éducatifs, impactant la manière dont les éducateurs abordent l'enseignement et améliorant fondamentalement l'expérience d'apprentissage. Selon Newey, cette technologie très débattue n'est pas simplement une forme de plagiat, mais un outil brillant qui augmente et revitalise les méthodologies éducatives. Encourageant les étudiants à utiliser l'IA pour les tâches de réflexion, elle insiste sur le développement et la récolte des compétences de pensée critique dans notre société qui se numérise à une vitesse vertigineuse.\n\nPlutôt que de rester des participants passifs, elle préconise que les étudiants deviennent des acteurs actifs, analysant les résultats générés par l'IA et considérant la qualité et la substance de leurs informations d'entrée. Le changement souligne l'importance de la compréhension, de la recherche et de l'analyse plutôt que la simple génération de résultats.\n\n## Transition de l'enseignement traditionnel\n\nL'approche progressive de Newey diverge radicalement des méthodes conventionnelles auxquelles la plupart des éducateurs s'accrochent, surtout compte tenu de la résistance générale à l'intégration de l'IA générative dans les contextes éducatifs. Cependant, elle souligne l'inévitabilité et la nécessité d'adopter la numérisation pour l'avantage global des élèves.\n\nComparer cette transition à la résistance initiale à l'utilisation d'Internet comme outil d'enseignement indique où nous en sommes aujourd'hui. L'IA générative, comme toute autre technologie en évolution, nécessite son intégration dans le programme et exige des mises à jour régulières pour rester pertinente dans ce paysage numérique en rapide évolution.\n\n## Équilibrer l'innovation et l'éthique\n\nAvec la progression et l'innovation, Newey aborde également les considérations éthiques inhérentes à ce changement. Elle partage plusieurs exemples où des étudiants, sans le savoir ou de manière subtile, ont soumis des essais générés par l'IA. Ainsi, elle souligne le besoin pour les éducateurs de veiller scrupuleusement à équilibrer l'embrassement technologique et l'utilisation éthique.\n\nElle croit fermement que les étudiants peuvent utiliser l'IA comme un outil productif, mais la responsabilité incombe également aux éducateurs de les guider pour maintenir simultanément l'intégrité académique.\n\n## Conclusion: Ouvrir la voie vers un système éducatif amélioré par l'IA.\n\nL'incorporation de l'IA générative dans l'éducation, bien qu'elle ait rencontré de la résistance, est une indication profonde de l'évolution du paysage éducatif. Comme l'illustre Newey, l'intégration réussie de l'IA dans l'éducation peut améliorer significativement les expériences d'apprentissage et le développement de compétences essentielles, assurant la préparation de nos étudiants pour un futur façonné par la transformation numérique.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Laura Newey","Darren W Pulsipher"],"link":"/episode-EDT180-fr","image":"./episodes/edt-180/fr/thumbnail.jpg","lang":"fr","summary":"Dans l'épisode de cette semaine d'Embracing Digital Transformation, Darren Pulsipher interviewe l'oratrice invitée Laura Newey à propos de son parcours fascinant à travers le monde émergent crucial de l'IA générative, particulièrement dans le secteur de l'éducation. Couvrant la transformation de son expérience d'enseignement et enrichissant les résultats d'apprentissage de ses étudiants grâce à l'IA, elle a analysé de manière approfondie l'adaptation aux dynamiques de l'éducation moderne."},{"id":83,"type":"Episode","title":"Zéro Confiance dans la 5G","tags":["5gtechnology","zerotrustsecurity","embracingdigitaltransformationpodcast","darrenpulsipher","lelandbrown","yazkrdzalic","kenurquhart","trentonsystems","zscaler","advancedcommunications","operationaltechnology","informationtechnology","lonelyislandapproach","telecominfrastructure","advancedsecuritymodels","commercial5gusage","military5gusage","technicalchallenges","cybersecurity","5gsecurity","integration","solutionarchitecture"],"body":"\r\n\r\n## Le Paysage Vaste du 5G et l'Approche de l'Île Solitaire\n\nLe monde de la technologie 5G évolue rapidement et, par conséquent, de nombreuses discussions éclairées ont lieu autour de la fusion de la Technologie Opérationnelle (TO) et de la Technologie de l'Information (TI). Yazz Krdzalic décrit le concept de \"l'approche de l'île solitaire\". Cette approche fait référence à la tendance des différentes entités à se concentrer trop lourdement sur la résolution de leurs problèmes individuels, ce qui a souvent conduit à l'arrêt de la croissance du matériel personnalisé dans l'infrastructure de télécommunications.\n\nLe besoin de s'éloigner de cette approche individualiste et de rétablir un cadre architectural collectif qui peut s'adapter et se flexibiliser avec différents cas d'utilisation devient de plus en plus évident. Avec l'émergence de la technologie 5G, il est nécessaire d'adopter une approche collaborative capable de répondre aux différentes exigences de diverses entités. L'approche collective aidera à garantir que l'infrastructure est flexible et évolutive, facilitant ainsi l'intégration de leurs technologies et applications dans le réseau par les entités.\n\nLes discussions autour de la fusion de l'OT et de l'IT prennent également de l'ampleur, et il devient clair que la collaboration entre ces deux domaines est essentielle au succès de la technologie 5G. Alors que la technologie continue d'évoluer, on s'attend à davantage de débats et de discussions sur la façon de tirer parti des opportunités offertes par la 5G, tout en faisant face aux défis posés par cette technologie émergente. Dans l'ensemble, l'avenir de la technologie 5G semble prometteur, et la collaboration entre différentes entités jouera un rôle crucial dans son succès.\n\n## Transition vers la sécurité Zero Trust\n\nAlors que la technologie continue d'évoluer, les préoccupations en matière de sécurité sont devenues un problème croissant pour les individus et les organisations. Afin de répondre à ces préoccupations et d'assurer un environnement sûr et sécurisé, un cadre architectural collectif est nécessaire. Ce cadre comprend la mise en œuvre de modèles de sécurité avancés, tels que la sécurité Zero Trust. Cependant, la transition vers ces modèles n'est pas toujours facile. Elle nécessite d'abandonner les anciennes méthodes de fonctionnement et de s'assurer que tous les modules technologiques sont synchronisés et fonctionnent correctement. Par le passé, c'étaient les clients qui étaient chargés de la responsabilité d'intégrer tous les éléments. Heureusement, avec l'adoption d'une approche plus évoluée, la charge de l'intégration a été considérablement réduite pour les clients, rendant la mise en œuvre de la sécurité Zero Trust et d'autres modèles de sécurité avancés beaucoup plus fluide.\n\n## Trouver un Terrain d'Entente dans l'Utilisation de la 5G\n\nLe développement de la technologie 5G a été un véritable bouleversement dans les secteurs commercial et militaire. Cependant, il existe des exigences spécifiques qui différencient l'utilisation commerciale et militaire de la 5G. Les déploiements commerciaux de réseaux 5G privés sont en grande partie statiques, tandis que les déploiements militaires doivent être mobiles.\n\nLeland Brown, un expert renommé dans le domaine, a discuté des complexités de la recherche d'une architecture commune qui pourrait répondre à ces deux besoins. Le défi était de créer une solution finale qui répondait élégamment à ces exigences. Il était important de s'assurer que la solution était efficace et efficiente pour les cas d'utilisation commerciale et militaire.\n\nLe développement de telles solutions est essentiel pour garantir que la technologie 5G est utilisée à son plein potentiel et peut répondre aux besoins diversifiés des différentes industries.\n\n## En conclusion\n\nLe monde de la technologie évolue et s'améliore constamment, et l'arrivée de la technologie 5G et de la sécurité Zero Trust en est la preuve. Cependant, la mise en œuvre de ces avancées peut être difficile en raison d'obstacles techniques et culturels. Heureusement, des experts comme Leland Brown, Ken Urquhart et Yaz Krdzalic travaillent à rationaliser l'intégration de la technologie 5G et de la sécurité Zero Trust, rendant le voyage vers un futur technologique plus sûr et plus efficace un peu plus facile pour tout le monde. Leurs connaissances et leur expertise éclairent le voyage continu d'évolution et d'amélioration dans le monde de la technologie.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Leland Brown","Yazz Krdzalic","Ken Urquhart","Darren W Pulsipher"],"link":"/episode-EDT181-fr","image":"./episodes/edt-181/fr/thumbnail.jpg","lang":"fr","summary":"Au milieu de l'adoption croissante des technologies 5G dans le monde entier, les experts dans l'épisode récent du podcast Embracing Digital Transformation se sont penchés sur le sujet essentiel de Zero Trust dans la sécurité 5G. L'animateur Darren Pulsipher a accueilli l'expert en communications avancées 5G Leland Brown, VP Marketing chez Trenton Systems Yazz Krdzalic, et Ken Urquhart, un physicien devenu professionnel de la cybersécurité chez Zscaler, pour discuter de l'intégration et de l'avancement de la technologie 5G, ainsi que de ses défis et percées."},{"id":84,"type":"Episode","title":"Assurance des Données de Confiance Zéro","tags":["digitaltransformation","dataintegrity","zerotrust","cybersecurity","strategies","walacor","encryption","loganalysis","geographicaldistribution","externalhackers","organizationalthreats","datamanagement","dataaudits","immutableauditlog","systemchange","legalcompliance","reducerisk","dataintelligence","multilayeredsecuirty","dataprotectionsolution","keyvulnerabilities","improveddataprotection","futuredigitalbusinesses","revolutionizedigitallandscape"],"body":"\r\n\r\n## Révéler les Hypothèses sur la Sécurité des Données\n\nDans le passé, les gens avaient une confiance implicite que leurs données étaient sécurisées et que leur vie privée était protégée. Cependant, cette confiance repose souvent sur un modèle obsolète qui ne correspond plus au paysage technologique actuel. Le nombre croissant de violations de données et d'attaques cybernétiques a rendu évident que la sécurité des données est plus critique que jamais, et les précautions qui étaient considérées comme adéquates dans le passé peuvent ne plus être suffisantes.\n\nAujourd'hui, les données sont vulnérables aux menaces non seulement de la part des pirates externes mais aussi de l'intérieur des organisations. Il est essentiel de comprendre qu'une violation de données peut avoir des implications significatives, allant des pertes financières aux dommages à la réputation. Par conséquent, il est crucial de mettre en œuvre une approche de confiance zéro dans la gestion des données, ce qui signifie que chaque demande d'accès aux données doit être vérifiée avant que l'accès ne soit accordé. Des audits de données fiables sont également nécessaires pour garantir que les données entrantes correspondent aux données sortantes et qu'il n'y a pas d'accès non autorisé à des informations sensibles.\n\n## Mise en œuvre d'une nouvelle ère de sécurité des données avec Walacor\n\nWalacor fournit une solution unique pour améliorer notre compréhension de la sécurité des données. Ils proposent un journal d'audit automatique et infaillible qui est immuable, ce qui signifie qu'une fois les données saisies, elles ne peuvent jamais être modifiées ou supprimées sans être détectées. Cette fonctionnalité rend incroyablement facile le suivi de chaque modification apportée au système, ce qui est essentiel pour maintenir un environnement sécurisé.\n\nEn fournissant transparence et traçabilité, la solution de Walacor aide les organisations à respecter les exigences légales de conformité et à atténuer les risques. Par exemple, en cas de litige juridique, un journal d'audit immuable peut servir de source de preuve fiable, car il ne peut pas être falsifié. De plus, en cas de violation de données, un journal d'audit immuable peut aider à identifier la source de la violation et l'étendue des dommages causés.\n\nDans l'ensemble, l'approche innovante de Walacor en matière de sécurité des données, avec son journal d'audit 100% immuable, offre une solution prometteuse pour les organisations cherchant à améliorer leur posture en cybersécurité.\n\n## Façonner l'avenir de l'intelligence des données\n\nLe risque croissant de violations de données signifie que nous devons nous éloigner de l'utilisation de multiples couches de sécurité des données vers une solution de protection des données plus intégrée. Cette type de solution pose les bases pour un environnement de confiance zéro, ce qui réduit considérablement le risque de menaces et de vulnérabilités cybernétiques. En adoptant cette approche, nous pouvons rationaliser nos méthodes de protection des données et garantir une meilleure intégrité des données.\n\nLe développement de l'intelligence des données sous forme d'intégrité et de sécurité des données ouvre de nouvelles possibilités pour les entreprises numériques. L'amélioration des méthodes de protection des données, une meilleure intégrité des données et une réduction des menaces cybernétiques potentielles ne sont que quelques-uns des avantages qui sont prêts à transformer le paysage numérique. Parmi ceux-ci, l'approche unique de Walacor en matière d'intégrité des données et de zéro confiance suscite beaucoup de discussions, marquant une étape importante dans notre approche de la sécurité des données maintenant et à l'avenir.\n\nConsultez plus d'informations sur (https://walacor.com)[https://walacor.com]\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Walter Hancock","Darren W Pulsipher"],"link":"/episode-EDT182-fr","image":"./episodes/edt-182/fr/thumbnail.png","lang":"fr","summary":"Le besoin de stratégies de sécurité des données robustes a augmenté de façon exponentielle à l'ère numérique, devenant une priorité majeure pour les entreprises du monde entier. L'expert en cybersécurité et CTO de Walacor, Walter Hancock, offre un aperçu perspicace de l'importance de l'intégrité des données et d'une approche de confiance zéro dans les régimes actuels de cybersécurité."},{"id":85,"type":"Episode","title":"Gestion des Données dans les Sciences des Matériaux et les Industries Manufacturières","tags":["digitaltransformation","materialscience","manufacturingindustry","dataanalytics","machinelearning","artificialintelligence","productdevelopment","structuredmethodologies","projectmanagement","complexdata","unstructureddata","datascience","r&dprocess","newproductdevelopment","datamanagement","technologyinnovation","physicitaldigitalknowledgegap","embracingdigital.org","embracingdigitaltransformationpodcast"],"body":"\r\n\r\n## Combler le fossé entre le physique et le numérique dans la R&D\n\nLa zone des matériaux se concentre sur l'aspect de niche mais significatif de la science des matériaux, spécifiquement dans l'industrie manufacturière. Étant donné le rôle considérable des matériaux dans le développement de produits, la gestion efficace des données devient cruciale. Analogique à une recette de cuisine, la science des matériaux implique une intégration nuancée d'ingrédients (matériaux) passés à travers un processus pour produire le produit final.\n\nCependant, ce domaine a historiquement été ad hoc, reposant sur l'essai, l'erreur et l'intuition. Par conséquent, les connaissances acquises pendant ce processus se perdent souvent en raison d'une documentation insuffisante ou d'une attrition des employés. Dans notre monde moderne et interconnecté, où les processus de développement de produits couvrent souvent plusieurs lieux, voire des pays, il est essentiel d'établir des méthodologies structurées pour prévenir la perte de connaissance.\n\nL'une des techniques soulignées par Yudilevich est d'aborder le \"facteur du camion\", qui suggère que si la seule personne qui sait comment accomplir une tâche particulière se faisait renverser par un camion, cela pourrait potentiellement faire dérailler l'ensemble du projet. Par conséquent, avoir au moins une autre personne à part l'individu principal qui peut effectuer la tâche pourrait réduire la vulnérabilité de l'équipe.\n\n## Capture des Complexités des Données de la Science des Matériaux\n\nLe domaine de la science des matériaux génère des données complexes, souvent non structurées et difficiles à capturer de manière suffisante à l'aide de tableaux de données traditionnels et de bases de données. Pour visualiser cela, considérez les données comme un graphique où les matières premières se transforment en produits finis. Les innombrables interactions entre les divers constituants donnent lieu à de multiples dimensions uniques au sein des données.\n\nDe plus, une traduction sans faille existe dans le domaine de la fabrication – De la recherche exploratoire à la phase de production, qui exige stabilisation et cohérence. Rassembler les données de ces phases dans un référentiel unifié peut améliorer le processus de R&D en centralisant les informations, en facilitant l'apprentissage inter-phase, et en accélérant le développement de nouveaux produits.\n\n## Intégrer la Science des Données dans la Fabrication\n\nBien que la science des données ait pénétré de nombreuses industries, les entreprises se concentrant principalement sur le développement de produits dans le monde physique trouvent souvent inefficace et coûteux de mettre en place des départements de données dédiés ou d'intégrer des outils analytiques. C'est ici que la solution de Materials Zone entre en jeu, rendant la science des données, l'apprentissage automatique et les outils statistiques accessibles aux entreprises peu familières avec ces domaines.\n\nIls proposent des outils clé en main accompagnés de webinaires et de sessions de formation pour une adoption facile, réduisant ainsi les obstacles à l'intégration de la science des données dans les pratiques de fabrication. Étonnamment, même les entreprises du Fortune 500 qui manquent des compétences numériques nécessaires peuvent bénéficier considérablement de ces solutions.\n\n## Alors que Nous Avançons\n\nAu fur et à mesure que le processus de développement de produit devient plus complexe et mondial, l'importance cruciale d'une gestion systématique des données combinée à l'innovation technologique est mise en avant. Des entreprises comme Materials Zone ouvrent la voie, guidant les entreprises à combler leur fossé de connaissance physique-numérique, à renforcer leurs pratiques de fabrication et à garantir leur succès futur.\n\nPour plus d'informations, consultez https://materials.zone.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ori Yudilevich"],"link":"/episode-EDT183-fr","image":"./episodes/edt-183/fr/thumbnail.png","lang":"fr","summary":"Dans un paysage technologique en évolution rapide, les dirigeants de différents secteurs appliquent l'analyse de données, l'apprentissage automatique et l'intelligence artificielle à leurs opérations. Aujourd'hui, regardez de plus près une entreprise qui mène la transformation numérique dans l'industrie manufacturière - Ori Yudilevich, le CTO de Materials Zone."},{"id":86,"type":"Episode","title":"Utilisation des données comme un atout stratégique","tags":["dataarchitecture","data","datastrategy","people","process"],"body":"\r\n\r\nis a lot, but I have a hard time deleting anything because I'm always worried I might need it later. However, hoarding data can have negative consequences. It can make it harder to find information when you need it, and it can also make systems run slower. It's important for organizations to regularly clean out and organize their data to prevent these issues.\n\nIl n'a pas besoin d'être si grand, mais je sauvegarde des choses au cas où j'en aurais besoin. Maintenant, multipliez cela par le nombre d'employés. Chez Intel, nous avons cent mille employés. Imaginez la quantité de données que nous stockons, rien qu'avec les emails, lorsque nous faisons des sauvegardes. Ajoutez-y les bases de données structurées et non structurées, les présentations, les feuilles de calcul, etc... et il est clair que tout stocker n'est pas une bonne stratégie.\n\n## Statistiques de données\n\nJetons un coup d'œil aux statistiques qui montrent pourquoi cette stratégie est mauvaise. Environ 80 % du temps des scientifiques des données est consacré au nettoyage des données avant de pouvoir les utiliser. Moins de 50 % des données structurées sont utilisées, et moins de 1 % des données non structurées peuvent être analysées. Ainsi, toutes ces données sont accumulées, mais les organisations ne les utilisent pas, pour la plupart. Un autre problème est de savoir qui a accès à toutes ces données stockées. Il est alarmant que 70 % des employés aient accès à des données auxquelles ils ne devraient probablement pas avoir accès. Tout comme les tas de choses d'un hoarder créent des risques d'incendie dans une maison, les tas de données non organisées créent des risques de sécurité dans une organisation.\n\n## Explosion de données\n\nQue pouvons-nous faire à propos de ce problème ? Tout d'abord, nous devons comprendre pourquoi il y a une telle explosion de données. Avec l'IoT, tout est connecté, et nous avons des traitements de données qui se produisent dans une multitude d'endroits. Le volume énorme de données qui est généré est incroyable. Cela est exacerbé par des problèmes réglementaires ; il est difficile de savoir ce que nous pouvons éliminer et ce que nous ne pouvons pas. Nous remplissons nos espaces de stockage et ensuite en achetons davantage. La technologie permet essentiellement notre accumulation de données. Nous devons examiner ce que nous pouvons faire différemment.\n\n## Pourquoi nous accumulons.\n\nLes experts disent que les gens accumulent parce qu'ils croient qu'un objet sera utile ou précieux à l'avenir, qu'il a une valeur sentimentale, qu'il est unique et irremplaçable, ou qu'il représente une trop bonne affaire pour être jeté. Ces mêmes idées s'appliquent à l'accumulation de données. Par exemple, pourquoi ai-je encore l'une des premières présentations que j'ai jamais faites ? Elle est stockée sur un disque et dans le cloud. Je la regarde peut-être une fois par an, mais je n'ai aucune raison valable en dehors de sa valeur sentimentale pour la conserver. Une organisation est composée d'individus ayant ces comportements, et à tous les niveaux d'accumulation de données, il y a la peur de se débarrasser des données.\n\n## Devenir centré sur les données\n\nÀ quoi cela ressemble-t-il de devenir une entreprise axée sur les données plutôt qu'une entreprise de stockage ? Voici un processus en quatre étapes pour devenir une organisation axée sur les données.\n\n## Fondement organisationnel\n\nLa première étape consiste à créer une base organisationnelle centrée sur les données. Il y a quatre acteurs clés avec des rôles distinctifs.\n\nDirecteur des données : Le rôle du directeur des données consiste à définir la stratégie et la gouvernance en matière de gestion des données et de génération d'informations précieuses pour l'entreprise. Ce rôle diffère de celui du directeur des systèmes d'information, qui se concentre sur l'infrastructure plutôt que sur l'information elle-même. C'est un travail difficile, car le directeur des données est impliqué dans le changement culturel. Il essaie d'empêcher les gens de stocker les données et encourage plutôt leur utilisation pour créer une véritable valeur commerciale.\n\nScientifique des données : Les scientifiques des données développent des modèles et des plans en trouvant des motifs dans les données et en utilisant des analyses prédictives. Les efforts des scientifiques des données ne peuvent devenir qu'une simple expérience scientifique ponctuelle, à moins que les informations ne soient opérationnalisées.\n\nIngénieur de données : C'est là que les ingénieurs de données interviennent. Ils gèrent les pipelines de données et opérationnalisent l'analyse. À mesure que de nouvelles données arrivent, de nouvelles connaissances sont générées sans recommencer à chaque fois.\n\nGardien des données : Le gardien des données gère la gouvernance et l'accès aux actifs de données, en veillant à ce que les bonnes personnes aient le bon accès au bon moment.\n\nAvec une organisation qui comprend ces quatre rôles, la prochaine question importante est de savoir s'il faut centraliser ou distribuer les opérations. Par exemple, il se peut que la gestion matricielle distribuée fonctionne déjà dans votre grande organisation, mais une petite organisation pourrait avoir besoin de plus de rigueur et bénéficierait d'une structure plus centralisée.\n\n## Collecte et préparation des données.\n\nUn article de la Harvard Business Review utilise efficacement une analogie sportive pour décrire deux stratégies de catalogage des données : la défense et l'attaque. En défense, l'objectif est de protéger les données. En attaque, l'objectif est d'avancer rapidement pour marquer des points.\n\n## Défense et Attaque de données\n\nAvec une stratégie défensive, l'organisation se concentre principalement sur la sécurité des données, la gouvernance et la conformité. La protection des données est essentielle. L'activité principale de données sera centrée sur l'extraction, la normalisation, la gestion du stockage et la gestion des accès. En général, cette stratégie utilisera une organisation plus centralisée et une seule source de vérité.\n\nAvec une stratégie offensive, l'organisation se concentre principalement sur la rapidité d'action afin d'améliorer sa position concurrentielle et d'être aussi rentable que possible. Les activités liées aux données porteront sur l'extraction, la modélisation, la visualisation, la transformation et l'enrichissement.\n\nCette stratégie nécessitera plus de flexibilité, ce qui signifie une organisation plus distribuée avec des versions multiples de la vérité.\n\nComprendre comment utiliser les données en fonction de la stratégie est important. Souvent, les organisations restent sur la défensive et cela peut être déroutant. Bien que chaque organisation ait besoin d'être capable de défendre et d'attaquer, les organisations doivent choisir une stratégie plutôt que de chercher à faire les deux, tout comme les joueurs de football de niveau professionnel ne jouent pas des deux côtés du ballon.\n\n## Analytique Perspicacité\n\nIl existe une courbe de maturité organisationnelle pour les analyses et la génération d'informations à partir de vos données. La clé est de comprendre où se situe actuellement votre organisation et quelles sont les prochaines étapes pour progresser sur la courbe.\n\nÀ l'étape de l'analyse descriptive, vous essayez simplement de comprendre ce qui se passe. À l'étape diagnostique, vous cherchez à comprendre pourquoi quelque chose s'est produit. À l'étape prédictive, vous pouvez prédire ce qui se passera à l'avenir en fonction des données historiques. C'est là que de nombreuses organisations aspirent à être, mais les deux premières étapes doivent être réalisées au préalable. Au-dessus de la prédiction se trouve la prescription, où vous pouvez comprendre pourquoi quelque chose se produira et guider l'organisation en fonction des attentes. Au sommet des étapes se trouve l'analyse des informations ou la prévoyance, où vous faites en sorte que les choses se produisent, progressant même au-delà de la prescription.\n\nUne raison importante de comprendre la position actuelle de votre organisation est qu'il existe des outils spécifiques pour chaque phase. Par exemple, vous ne voulez pas vous retrouver coincé avec un projet d'IA qui utilise des algorithmes prescriptifs, voire prédictifs, lorsque votre organisation se situe encore à l'étape descriptive.\n\n## Opérationnalisez-le\n\nPour atteindre l'objectif de l'opérationnalisation, c'est-à-dire la création d'un processus répétable, il existe trois éléments clés : une infrastructure axée sur les données, des pipelines de données et un flux d'activité commerciale.\n\nL'infrastructure axée sur les données vous permet de savoir où se trouvent toutes vos données et ce qu'il y a dans ces données grâce à divers outils tels qu'un gestionnaire de métadonnées comme Elastic Search ou des catalogues et des référentiels de métadonnées. Les pipelines de données disposent d'excellents outils pour permettre le processus depuis l'ingestion jusqu'à l'analyse et l'action. Une stratégie défensive ou offensive déterminera les outils que vous utiliserez dans votre pipeline. Le dernier élément, le flux d'activité commerciale, est là où la compréhension commerciale de vos données et de vos processus permettra le déploiement d'un processus d'amélioration continue pour garantir des informations reproductibles et précieuses.\n\n## Appel à l'action\n\nTout d'abord, développez une stratégie de données. Organisez-vous et identifiez où se trouvent toutes vos données et cataloguez-les. Décidez d'une stratégie défensive ou offensive, puis procédez étape par étape à l'analyse de vos informations en utilisant les bons outils. Surtout, mettez en œuvre vos connaissances pour en tirer la meilleure valeur commerciale.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT19-fr","image":"./episodes/edt-19/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren Pulsipher, architecte en chef des solutions chez Intel pour le secteur public, explore comment les organisations peuvent passer d'une simple accumulation de données à leur utilisation en tant qu'actif stratégique."},{"id":87,"type":"Episode","title":"Suivez le Bit","tags":["iot","dod","edge","technology","data","cybersecurity"],"body":"\r\n\r\n## Internet des Objets est le commencement\n\nLa mise en œuvre généralisée de l'Internet des objets (IoT) prend plus de temps que prévu par l'industrie. Beaucoup mettent les retards sur le compte de l'adoption et du déploiement de la 5G dans le monde, mais il existe un autre problème qui ralentit le déploiement de systèmes IoT robustes : la gestion de la complexité. Au fur et à mesure que les données se déplacent du bord, de l'agrégation bord, du réseau, du centre de données et du cloud, la sécurisation des données est une préoccupation majeure car la surface d'attaque augmente lorsqu'elle sort du centre de données traditionnel. Il existe des solutions ponctuelles qui aident à résoudre ces problèmes, mais il n'existe pas encore d'architecture de solution complète qui résolve tous les problèmes de ce centre de données sans murs.\n\n## Complexité de l'environnement du DoD\n\nL'IoT est déjà complexe, et le DOD augmente cette complexité en raison des types de producteurs et de consommateurs des données. Du côté des producteurs, des capteurs sont attachés aux satellites, aux avions, aux navires et aux véhicules ; même les combattants eux-mêmes constituent essentiellement des centres de données ambulants. La quantité de données produites par ces appareils périphériques peut submerger un réseau. Le nombre d'appareils hétérogènes peut rendre leur gestion presque impossible, surtout lorsque les appareils communiquent avec différents protocoles et ont différents niveaux de classification. En d'autres termes, collecter les données n'est pas le problème ; le problème réside dans le fait de les mettre rapidement entre les mains de ceux qui prennent les décisions, dans un format utile.\n\nDe plus, la connectivité peut poser problème avec les appareils périphériques. Les environnements hostiles où la connectivité réseau avec un centre de données est inexistante, voire peu fiable, peuvent entraîner des retards dans la transmission des données. Cela signifie qu'une solution dans ce domaine doit fonctionner en modes connecté et déconnecté. La vitesse de livraison est un facteur clé de réussite ; des vies peuvent être en jeu si les décisions basées sur les données sont retardées.\n\n## Cadre commun pour les applications, les données et la sécurité\n\nÉvidemment, nous avons besoin de solutions à ces problèmes. D'une importance primordiale est un cadre commun pour gérer la complexité de ces nouvelles architectures informatiques qui se trouvent en dehors des murs traditionnels du centre de données. Le cadre doit aborder la gestion des applications, des données et de la sécurité. Nous devons être capables de déployer des applications portables et réutilisables n'importe où dans le système, de l'extrémité au cloud : la doctrine du \"écrire une fois, exécuter n'importe où\". Cela nous donne la possibilité de développer, tester et déployer rapidement des applications sans avoir à configurer toutes les permutations de configurations matérielles dans l'écosystème. L'utilisation d'outils de l'écosystème des conteneurs devrait aider à cela. Les outils basés sur Kubernetes (K8s) sont un bon choix car ils sont devenus la norme de facto dans la communauté DevOps.\n\nGérer les applications de manière isolée ne suffit pas. Toutes les applications ont besoin de données d'une certaine manière, il est donc essentiel de comprendre où se trouvent les données, où elles vont et comment elles sont classées pour trouver des solutions efficaces. Nous avons besoin d'un environnement d'exploitation commun pour gérer et superviser les différentes catégories de données telles que les domaines, les limites de sécurité, la gouvernance, la gestion du cycle de vie des données et leur localisation. Un environnement d'exploitation commun accroît la flexibilité et la rapidité de déploiement des applications.\n\nUn cadre commun de sécurité est également nécessaire. La question cruciale est de savoir comment sécuriser vos données sous toutes leurs formes tout en les partageant. Il existe actuellement des solutions matérielles et logicielles, ainsi qu'une progression continue dans ce domaine. Des solutions de sécurité de base telles que le chiffrement devraient constituer des fondations solides. Bien sûr, cela nécessite le bon moteur de stockage et de capacité sous-jacente. Une autre préoccupation concerne l'entrée de données erronées ou malveillantes dans le système. Établir une racine de confiance en tant que fondation est également nécessaire dans cet vaste écosystème.\n\n## Traitement à la périphérie, centre de données et cloud\n\nOù intervient Intel dans cet environnement ? Nous pouvons aider à fournir l'infrastructure sous-jacente qui soutient ces systèmes en termes de performances et de consommation d'énergie. Que vous traitiez des informations de capteurs en périphérie dans un environnement à faible consommation d'énergie (pensez à Atom et aux conceptions de circuits intégrés spécifiques), ou que vous effectuiez la formation ou l'inférence de l'intelligence artificielle dans votre centre de données (Xeon et calcul neuromorphique), Intel dispose d'un processeur capable de convertir les données brutes en informations précieuses et exploitables, le composant clé de cet environnement complexe axé sur la mission.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT2-fr","image":"./episodes/edt-2/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren interviewe Greg Clifton, directeur du Département de la Défense (DOD) et du Renseignement pour Intel Corp. Ils discutent des défis de la gestion des données dans un système complexe qui s'étend sur plusieurs clouds, centres de données d'entreprise, centres de données régionaux et tactiques. Écoutez Darren et Greg suivre un peu de données depuis leur collecte et leur parcours à travers cet écosystème jusqu'à la production d'informations exploitables pour les analystes et les combattants. Écoutez Darren et Greg discuter de certains des obstacles de cet environnement circulaire et des solutions pour aider à fournir des informations exploitables aux analystes et les transmettre aux combattants."},{"id":88,"type":"Episode","title":"Détruire la complexité des niveaux de stockage.","tags":["data","optane","technology","storage","vastdata"],"body":"\r\n\r\n## Trente ans de complexité de stockage\n\nLe stockage peut être un vieux concept, mais VAST Data a atteint le statut de licorne en seulement un peu plus d'un an de livraisons de revenus. VAST a remplacé l'ancienne architecture des systèmes de stockage à plusieurs niveaux par un système unique et monolithique, rapide et rentable. Les solutions VAST éliminent également le problème des systèmes de stockage volumineux et chaotiques constitués de différents systèmes de fichiers et architectures différentes.\n\n## Une fondation pour une nouvelle architecture\n\nLe fondateur de VAST examinait le stockage avec une perspective vierge. Il a constaté que les clients n'avaient pas nécessairement besoin de meilleures performances de la part des Flash, mais qu'ils avaient besoin d'un support de stockage de fichiers et d'objets à moindre coût. Il a tiré parti de trois technologies qui n'existaient pas avant 2018. Premièrement, le NVMe sur Fabrics, utilisé comme un type de SAN hyperscale pour connecter tout ensemble avec une latence réduite. Ensuite, le flash QLC, car il est peu coûteux et n'a pas de pièces mobiles comme les disques durs. Troisièmement, l'Optane 3D XPoint d'Intel. L'Optane offre une parité en lecture-écriture et une endurance élevée à un prix raisonnable. La combinaison de ces trois technologies permet à VAST de fournir des performances tout flash, mais à un prix comparable à celui d'un disque dur. Cela élimine le besoin d'autres niveaux que les organisations achètent parce qu'ils sont bon marché.\n\n## Présentation de Vast Data Universal Storage\n\nLe système VAST est la première architecture de tout partager désagrégée. Cela signifie que la logique est désagrégée de l'état du système de fichiers. Au lieu de cela, l'état du système de fichiers réside dans des boîtes de données contenant du Flash QLC et de l'Optane. L'Optane est utilisé comme un grand magasin de métadonnées. Il est polyvalent, tout comme le système, qui est commercialisé sous le nom de stockage universel. Avec ces boîtes, il n'y a pas de point de défaillance unique, donc la capacité est, théoriquement, illimitée. (VAST a testé le système à environ 50 pétaoctets dans un seul système de fichiers.) Vous pouvez augmenter la capacité simplement en ajoutant des serveurs x86 bon marché dans les clusters. Parce que c'est un système de fichiers parallèle, n'importe quel utilisateur peut accéder à n'importe quelle partie des données depuis n'importe lequel des serveurs comme s'il était connecté directement, vous pouvez donc continuer à évoluer.\n\nVous pouvez également augmenter les performances indépendamment de la capacité. La seule chose qui limiterait les performances du flash est le CPU, donc en ayant la possibilité d'augmenter de manière transparente le nombre de CPUs dans le cluster, vous pouvez améliorer les performances.\n\nL'un des problèmes résolus avec cette structure est la latence. De nombreuses organisations ont besoin d'une faible latence pour l'ensemble de leurs données. Étant donné que chacun de ces serveurs sans état a accès à l'ensemble des données, vous disposez d'un accès rapide à l'ensemble des données.\n\n## Architecture DASE : Pools de serveurs\n\nUn autre grand avantage est qu'il est facile d'ajuster précisément le stockage d'une organisation. La nature modulable des serveurs sans état, ainsi que l'absence de communication entre eux, vous permettent de construire un cluster parfaitement adapté à vos besoins. Par exemple, vous pouvez segmenter vos boîtiers de contrôle en fonction de différentes charges de travail, mais ils peuvent tous accéder aux mêmes données.\n\nDe plus, le système fonctionne bien avec une organisation qui a besoin de différentes couches de classification pour accéder aux données. Vous avez la possibilité de séparer l'accès des utilisateurs en créant plusieurs zones d'accès avec des adresses IP virtuelles. Un des problèmes avec NFS est qu'il diffuse essentiellement à tout le monde. Si vous limitez les diffusions à un sous-ensemble d'adresses IP, cela vous donne la possibilité de découper ces différentes architectures en systèmes distincts.\n\n## Application des Ponts de Stockage Universels Éras\n\nCe n'est pas seulement une solution destinée aux clusters HPC; elle n'est pas coûteuse. De nombreuses entreprises utilisent d'abord VAST pour la sauvegarde afin d'établir la confiance. Par exemple, l'Institut national du cancer dispose d'une bibliothèque d'archives sur bande, et ils souhaitaient pouvoir accéder plus rapidement aux informations. Ils ont examiné différentes plateformes, et VAST est apparu avec un prix plus bas, et entièrement Flash, ce qui est plus rapide que leur système de stockage NAS de production. Ainsi, la solution offre un bon rapport qualité-prix et elle est utile pour le partage général de fichiers et une variété de charges de travail, telles que l'IA, l'analyse des journaux, Splunk, etc., pas seulement pour les clusters HPC. Les solutions VAST sont faciles à gérer et vraiment universelles.\n\nVAST est une jeune entreprise, mais elle possède plusieurs installations au sein d'agences gouvernementales telles que le National Institute of Health et les Tri-labs du Département de l'Énergie, où des performances maximales sont nécessaires pour ces superordinateurs. Il s'agit d'un système puissant dans certains des plus grands environnements HPC du monde, qui prend en charge des applications critiques pour la mission.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Randy Hayes"],"link":"/episode-EDT20-fr","image":"./episodes/edt-20/fr/thumbnail.jpg","lang":"fr","summary":"Randy Hayes de VAST Data et Darren Pulsipher, Architecte Principal des Solutions pour le Secteur Public chez Intel, discutent de l'architecture de stockage innovante de VAST Data qui élimine le besoin de niveaux en utilisant NVMe over Fabrics, QLC Flash et 3D XPoint Optane."},{"id":89,"type":"Episode","title":"Contrôleur de mémoire défini par logiciel à grande capacité.","tags":["bigmemory","edge","compute","technology","memverge","optane","pmem","data"],"body":"\r\n\r\nLa technologie de mémoire persistante 3D XPoint Optane d'Intel a révolutionné le jeu pour MemVerge, en leur permettant de développer une mémoire définie par logiciel, qu'ils décrivent comme exprimant la puissance d'Optane PMEM.\n\n## La meilleure façon d'utiliser la PMEM\n\nMemVerge a développé un logiciel de \"Big Memory\" pour répondre à une demande croissante des applications et des entreprises de traiter des données à un volume et une vitesse toujours plus élevés. Les applications en temps réel nécessitent davantage d'informations et d'actions instantanées à partir des données. Cela nécessite une nouvelle infrastructure centrée sur la mémoire pour répondre aux exigences de latence.\n\nLes applications qui utilisent l'IA, l'apprentissage automatique ou l'analyse en temps réel de gros volumes de données utilisent généralement de la DRAM. Bien que la DRAM ait une latence de l'ordre de la nanoseconde et offre une capacité et une performance agréables, elle présente des limites physiques telles que la densité de mémoire et la quantité de mémoire pouvant être intégrée dans un serveur. Elle est également relativement coûteuse. Plus important encore, elle est volatile et, à mesure que les données deviennent plus importantes, la volatilité de la mémoire devient une contrainte. PMEM élimine ces goulots d'étranglement car il permet d'avoir une capacité plus importante, à moindre coût et avec une persistance.\n\nUn avantage important est que les entreprises n'ont pas besoin de réécrire leurs applications pour utiliser la technologie de MemVerge. La société recherchait le meilleur moyen d'utiliser la persistance, et la réponse a été de fournir le moins de perturbations aux paradigmes de programmation. Lorsque Optane PMEM est devenu disponible en tant que facteur de forme de mémoire, cela a été une occasion de développer un logiciel précieux dans le cadre de la solution.\n\n## Mémoire définie par logiciel\n\nIl y a une transformation des données en cours. D'un point de vue matériel, au cours des dix prochaines années, il y aura peut-être un monde plus hétérogène, tant au niveau du calcul que des éléments de mémoire. Un nouveau tissu émergera, tel que CXL, qui connecte ces éléments. Le plus grand défi sera de faire bouger l'écosystème des applications. Une couche logicielle est nécessaire pour le décortiquer en morceaux consommables et composables qui facilitent la digestion de l'application. La mémoire définie par logiciel de MemVerge sera un composant important dans cet espace.\n\nEn attendant, MemVerge est en train de construire un pont entre les paradigmes actuels et futurs. La mémoire définie par logiciel de MemVerge apporte une SLA et une QLS dynamiques, une résilience, une persistance d'application, une efficacité et des performances. Ce sont toutes des choses que vous obtenez normalement avec un réseau de stockage défini par logiciel, mais vous pouvez maintenant les obtenir avec une mémoire à latence réduite et une vitesse extrêmement rapide.\n\n## Performance sur la machine de mémoire\n\nDeux importantes propriétés intellectuelles de MEMVerge sont en jeu. La première est une couche de virtualisation logicielle qui optimise les performances de la mémoire - la combinaison de PMEM et de DRAM - qui est très proche des performances de la DRAM. Chaque charge de travail a des profils de performance différents qui peuvent être affinés en mélangeant le PMEM et la DRAM à des ratios différents. Cet affinage du PMEM et de la DRAM donne aux développeurs d'applications et aux professionnels de l'informatique la possibilité d'ajuster la mémoire pour leurs applications, plutôt que pour l'ensemble de la machine. Au lieu de configurer les ratios existants de DRAM et de PMEM pour l'ensemble de la machine, vous pouvez désormais changer dynamiquement les ratios de PMEM et de DRAM en fonction de la charge de travail, et même dépasser les performances de la DRAM.\n\n## Instantané ZeroIO (persistance des données de l'application)\n\nLe deuxième de ces inventions est la capture d'écran de la mémoire ou ZeroIO. Cela permet de persister les applications transitoires existantes sans aucune réécriture. Il fonctionne par-dessus la mémoire définie par logiciel, qui est un service de mémoire volatile. Bien que le PMEM sous-jacent soit persistant, une mémoire volatile est nécessaire pour éviter de casser les applications existantes. La persistance est utilisée en fournissant à l'opérateur de l'application une interface utilisateur graphique (GUI) et une CI pour gérer les captures d'écran. Il y a une fonctionnalité de capture d'écran, vous pouvez donc capturer instantanément l'état d'une application entière. Ensuite, cette application peut être récupérée à tout moment dans le futur.\n\nCela rend votre mémoire non seulement persistante, mais également hautement disponible. Après un crash, vous pouvez effectuer une récupération instantanée. Si vous faites une erreur dans la base de données, vous pouvez revenir à un point précédent. Vous pouvez également cloner sur la base du snapshot, de sorte que vous pouvez créer de nouvelles instances d'une application sans répliquer physiquement la mémoire. Ainsi, vous pouvez créer de multiples processus indépendants avec des espaces mémoire logiques qui se mappent sur le même espace mémoire physique. Cela permet non seulement d'économiser de la mémoire, mais rend également le processus de création de clones instantané. Cette nouvelle technologie rend possibles de nombreuses choses qui étaient impossibles auparavant.\n\nIl n'y a pas de changements architecturaux dans votre programme, mais lorsque vous devez persister quelque chose, vous pouvez simplement le prendre en instantané. Cela ne modifie pas le modèle de programmation familier, mais accélère considérablement les E/S. Une autre excellente fonctionnalité est que les instantanés deviennent des objets gérables, vous pouvez donc les transporter n'importe où où vous pouvez redémarrer l'application. La migration en direct peut également être activée dans certains scénarios.\n\n## Cas d'utilisation activés par le futur\n\nPour un aperçu de l'avenir, MemVerge prévoit une version 2.0 dans environ un an qui comprendra un SDK. En plus de l'utiliser en tant que couche de mémoire transparente, les nouveaux développeurs d'applications auront une nouvelle façon de persister leurs données. Cela facilitera le développement d'applications, ainsi que la modification des applications existantes.\n\nAvec le SDK, les développeurs pourront capturer des segments de la mémoire de l'application ou le profil de mémoire complet, donnant ainsi à l'application développeur la capacité de persister la mémoire avec les technologies encombrantes d'ORM ou de mappage de mémoire d'aujourd'hui.\n\nEn partenariat avec Intel, MemVerge lancera la première version de leur produit avec la mémoire définie par logiciel et les capacités instantanées pour une disponibilité générale le 23 septembre 2020.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Charles Fan"],"link":"/episode-EDT21-fr","image":"./episodes/edt-21/fr/thumbnail.png","lang":"fr","summary":"Charles Fan, PDG de MemVerge, discute avec Darren Pulsipher, architecte en chef des solutions pour le secteur public chez Intel, de leur nouvelle technologie, les contrôleurs de mémoire définis par logiciel Big Memory. La technologie utilise la mémoire persistante Intel 3D XPoint Optane pour combler efficacement le fossé entre les architectures actuelles et futures, tout en offrant une plus grande capacité, des coûts plus bas et la persistance."},{"id":90,"type":"Episode","title":"Un argument en faveur du Cloud Multi-Hybride.","tags":null,"body":"\r\n\r\n## Environnement Cloud actuel\n\nAu cours des cinq dernières années, il y a eu un changement fondamental dans l'environnement informatique. La croissance continue du Cloud Public et l'émergence d'options de Cloud Privé ont laissé de nombreux DSI et services informatiques en retard. Dans le marché compétitif d'aujourd'hui, de nombreuses équipes de développement doivent avancer plus rapidement que ce que la plupart des services informatiques peuvent fournir. Les équipes de développement ont trouvé dans les Clouds Publics tels que AWS, GCE et Azure une option viable pour la \"IT maison\" à l'ancienne. Les fournisseurs de cloud public ont rendu la création de nouvelles infrastructures facile et rapide. Plus besoin d'attendre plusieurs niveaux d'approbations techniques et commerciales, d'espace physique dans le centre de données et de problèmes d'approvisionnement fournisseur. Maintenant, en quelques minutes seulement, une équipe de développement peut disposer de toute l'infrastructure dont elle a besoin pour son nouveau projet.\n\nAvant que les directeurs informatiques du Cloud public ne puissent facilement \"se promener\" autour des cubes et compter le nombre de machines \"IT de chalet\" qui fonctionnaient sous les bureaux des gens. Avec les machines physiques invisibles pour les services informatiques, il est impossible d'identifier les équipes et les infrastructures de leurs projets. De nombreux Cloud publics ont donné aux organisations la possibilité de consolider la comptabilité de tous les comptes pour des domaines spécifiques, mais la visibilité sur ce qui est en cours d'exécution et qui travaille sur l'infrastructure est encore en quelque sorte une \"chasse aux trésors\". Souvent, ces projets \"rouge\" deviennent visibles lorsque les projets sont produits et doivent être intégrés dans une infrastructure sécurisée de l'entreprise. La sécurité, la confidentialité et les politiques réglementaires peuvent rendre la \"production\" de projets presque impossible. Surtout si les développeurs ont étroitement lié leurs applications à l'infrastructure Cloud.\n\nLes départements informatiques visionnaires font de leur mieux pour capturer le \"IT des chalets\" en travaillant avec les Clouds publics et les fournisseurs de logiciels indépendants (ISV) pour créer des \"portails d'entreprise\" dans le Cloud. Mettre en place un portail de passage est un bon début pour capturer les projets utilisant l'infrastructure, mais de nombreuses organisations constatent que le simple portail laisse les équipes de développement insatisfaites. Au cours des dernières années, j'ai travaillé avec de nombreuses de ces organisations pour identifier des cas d'utilisation, des architectures et des technologies afin d'aider à développer ces portails augmentés que nous appelons \"Hybrid Multi-Clouds\" (MHC). En général, trois grandes technologies sont intégrées pour construire ces MHC. Les plateformes de gestion de Cloud (CMP), les cadres d'automatisation et les outils de Plateforme en tant que Service (PaaS).\n\n## Plateforme de gestion du cloud (CMP)\n\nLa responsabilité principale des plateformes de gestion de cloud est de gérer plusieurs cloud hétérogènes, à la fois publics et privés. Elles offrent aux utilisateurs finaux la possibilité de gérer plusieurs clouds et leur infrastructure à partir d'une interface commune. Les CPPs ont généralement pour objectif de faciliter la tâche des administrateurs de cloud. Bien que les outils de la plateforme de gestion de cloud se concentrent principalement sur la gestion de plusieurs cloud, de nombreux outils disposent également de fonctionnalités issues des plateformes en tant que service (PAAS) et des frameworks d'automatisation, ou au minimum, disposent d'une architecture de plug-in pour les prendre en charge.\n\n## Cas d'utilisation couverts\n\nGérer les clouds publics\n\nGestion des clouds privés\n\nGestion des identités sur le cloud.\n\nGérer l'infrastructure sur plusieurs clouds.\n\n## Cadres d'automatisation\n\nLes principaux responsabilités des Automation Frameworks sont d'automatiser le déploiement, la gestion et la mise à jour des piles logicielles sur l'infrastructure. Les Automation Frameworks sont issus de la communauté DevOps et se concentrent généralement sur des processus répétables. Beaucoup de ces outils incluent des langages de script permettant aux ingénieurs DevOps de gérer et de configurer de manière répétable les logiciels et les services. De nombreuses équipes DevOps sont très compétentes dans l'utilisation de ces outils.\n\n## Cas d'utilisation couverts\n\nDéployer le logiciel sur l'infrastructure\n\nGérer les logiciels sur l'infrastructure\n\nAméliorer les logiciels et les services\n\n## Plateforme en tant que Service (PAAS)\n\nLa plateforme en tant que service est principalement responsable de fournir un portail unique pour réutiliser des plateformes et les déployer sur l'infrastructure. Les outils PaaS sont généralement très orientés vers le développeur, ce qui peut entraîner des configurations d'infrastructure inflexibles. Beaucoup de ces outils ont un portail web qui permet aux développeurs de sélectionner des services et de les déployer dans l'infrastructure.\n\n## Scénarios d'utilisation couverts.\n\nDéployer / Gérer des services / applications\n\nGérer le catalogue de services\n\nDévelopper de nouveaux services/applications.\n\n## La convergence crée le Multi-Cloud Hybride (MCH).\n\nParce qu'aucun ensemble d'outils ne possède tous les cas d'utilisation nécessaires pour gérer les clouds, les applications, l'infrastructure et les services, les équipes passent plusieurs années d'effort à installer, configurer et intégrer ces trois ensembles d'outils ensemble. Cela a conduit à l'émergence de technologies qui intègrent ces outils, y compris de nouvelles offres de produits et de nouvelles fonctionnalités dans les produits actuellement disponibles.\n\nDe nombreux produits CMP incluent des PaaS et des cadres d'automatisation dans leurs solutions. Les outils PaaS gèrent désormais plusieurs clouds. Les cadres d'automatisation commencent à offrir des portails web et une connectivité à plusieurs clouds. Beaucoup d'outils adoptent la vision d'un Hybrid Multi-Cloud. Lorsque l'on choisit quel(s) outil(s) utiliser, il est important de se rappeler les fondements de l'outil.\n\n## Déploiement d'une solution\n\nL'écosystème Hybrid Multi-Cloud est encore assez récent et nécessite encore quelques intégrations approfondies entre les outils. Certains outils commencent à proposer des solutions prêtes à l'emploi et complètes, mais avec leur propre vision du monde. Étant donné que l'écosystème est émergent, il y a de nombreux acteurs et choix possibles. Le temps dira qui remportera cette place. Pour l'instant, il sera intéressant de voir les outils converger et se consolider pendant que les fonctionnalités se développent.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT22-fr","image":"./episodes/edt-22/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher décrit pourquoi un Architecte Multi-Hybrid Cloud pourrait déjà être dans votre centre de données. La plupart des organisations ont déjà tous les ingrédients. Ils ont juste besoin de savoir comment ils s'assemblent."},{"id":91,"type":"Episode","title":"Création d'aperçus dans les organisations axées sur les données","tags":["aiml","dataarchitecture","datacentric","data","insight","technology"],"body":"\r\n\r\nEn utilisant l'exemple de la fabrication de widgets, Darren et Sarah ont précédemment discuté du pipeline de données dans le but de répondre à une question fondamentale pour l'entreprise : Combien de widgets produire. La complexité devient apparente lorsqu'il s'agit de prendre les données brutes de la demande des clients, de l'offre actuelle et de la perte de rendement, et de les convertir en informations.\n\nLa première étape de ce processus, avant de pouvoir décider des outils à utiliser, est de préparer les données sous une forme utilisable. Quatre-vingts ou quatre-vingt-dix pour cent du travail d'un scientifique des données consiste à préparer et à transformer les données afin de les mettre dans un algorithme, par exemple, ou de les utiliser pour la reconnaissance de motifs.\n\nLes bons outils sont complexes. L'intelligence artificielle et l'apprentissage automatique n'utilisent pas un seul algorithme, mais une multitude d'outils que les scientifiques des données utilisent et expérimentent en combinaison pour obtenir les meilleurs résultats d'analyse. En d'autres termes, un seul algorithme ne vous dira pas combien d'objets vous devriez fabriquer. Il y a beaucoup de parties qui se chevauchent, et les outils eux-mêmes sont complexes. De plus, les scientifiques des données ont des domaines d'expertise différents ; la science des données est un sport d'équipe. Tout comme vous ne confieriez pas à un ingénieur réseau la conception de l'architecture de stockage, vous n'emploieriez pas un ingénieur spécialisé dans la reconnaissance d'images pour résoudre votre problème d'analyse de rendement. Vous devez embaucher les scientifiques des données appropriés pour concevoir et déployer les bons outils afin d'obtenir des informations sur vos questions commerciales.\n\nRevenons à la question du nombre de widgets que vous devriez fabriquer. Chaque situation et modèle seront bien sûr différents en fonction des types de questions, de données et de dynamiques que vous avez, mais nous le prendrons comme point de départ. Une fois que cette question commerciale est établie, l'organisation passera par un développement de maturité analytique.\n\nD'abord, vous vous concentrerez sur ce qui s'est passé dans le passé pour repérer les tendances de la demande de vos widgets. Par exemple, vous pourriez examiner des données chronologiques pour voir quand la demande de widgets augmente et diminue. Dans quelle mesure les informations sont-elles stables au fil du temps et comment pouvez-vous les utiliser pour prévoir l'avenir ? Peut-être pourriez-vous utiliser un apprentissage automatique pour étudier différents segments temporels et les comprendre. Vous voudrez peut-être également effectuer une analyse de texte pour savoir si les gens parlent de votre widget sur les médias sociaux. Le nombre de \"j'aime\" ou de partages pourrait être une source de données.\n\nVous pourriez faire quelque chose de similaire avec l'approvisionnement. Quelle est la stabilité de l'approvisionnement actuel et à quel point comprenez-vous bien la dynamique du système ? Regardez où vous avez besoin d'un humain pour comprendre la dynamique du système et incorporez cette connaissance dans la façon dont vous effectuez vos analyses. Il pourrait y avoir des types de modèles d'apprentissage automatique qui vous donnent des idées de perspectives, vous pourriez donc faire de l'apprentissage non supervisé. L'apprentissage non supervisé consiste à déterminer s'il existe différentes catégories ou segments dont vous n'êtes pas conscient et qui se comportent différemment les uns des autres. Demandez comment vous pouvez mieux suivre, ou obtenir une meilleure résolution de ce qui se passe dans ces groupes.\n\nEn résumé, selon la provenance des données et ce que vous regardez dans ces données, vous allez utiliser des outils différents.\n\nLa perte de rendement est un exemple de la complexité des problèmes à résoudre. Plus il y a de variations dans la chaîne de fabrication, plus il peut y avoir de gaspillage. Dans ce cas, l'apprentissage automatique examinera les segments et les regroupements des différents types de rendements. Comment quantifier et prédire cela ?\n\nUne des tâches des scientifiques des données ici est la conception d'expériences afin d'essayer d'estimer la causalité. En manipulant des boutons et en actionnant des leviers de manière systématique, vous pouvez observer ce qui se produit au niveau du rendement, tout en ajoutant des commandes de processus pour éviter les dérives.\n\nUne autre opportunité d'analyse est la fiabilité. Par exemple, avec la maintenance prédictive, vos outils de fabrication peuvent être entretenus en temps opportun pour éviter toute perte de rendement. Vous pouvez également utiliser l'analyse textuelle dans certaines situations, comme lorsque vous disposez d'observations et de solutions écrites de techniciens au fil du temps, à utiliser comme base de connaissances collective.\n\nL'apprentissage profond autour de la reconnaissance d'image est une autre stratégie pour aider à prévenir les pertes en détectant les erreurs et les défauts, et peut-être même pour la catégorisation des défauts.\n\nL'objectif de tout cela est, bien sûr, de gagner une précieuse compréhension du monde des affaires pour votre organisation. La clé est l'engagement envers une organisation centrée sur les données, en restant flexible et en disposant des bons outils et des bonnes personnes pour transformer vos données en aperçus exploitables.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT23-fr","image":"./episodes/edt-23/fr/thumbnail.png","lang":"fr","summary":"Dans la partie 5 d'une série, Sarah Kalicin, scientifique en chef des données chez Intel, et Darren Pulsipher, architecte principal des solutions du secteur public chez Intel, expliquent comment créer des informations en utilisant l'IA et l'apprentissage automatique dans une organisation axée sur les données."},{"id":92,"type":"Episode","title":"Stratégie numérique et architecture des données","tags":["data","dataarchitecture","aiops","secops","devsecops","devops","compute","technology","process"],"body":"\r\n\r\nAvoir un plan pour la transformation numérique de votre organisation est essentiel pour éviter de vous perdre dans le labyrinthe consistant à adopter simplement les dernières technologies et processus les plus récents et les plus performants. Cette stratégie aléatoire conduira à un échec de votre transformation numérique. Un plan, ou une feuille de route, allant de l'endroit où se trouve actuellement votre organisation à l'endroit où vous souhaitez aboutir, est la partie la plus importante d'une transformation efficace.\n\nLes trois éléments clés qui doivent être coordonnés et équilibrés dans votre plan sont organisationnels, procéduraux et technologiques.\n\n## Divisions organisationnelles\n\nPour comprendre le changement organisationnel, commençons d'abord par examiner les groupes communs dans la plupart des organisations.\n\n## Développement\n\nCe groupe est votre équipe du jour ; ils développent de nouveaux produits. Les processus de l'équipe de développement sont bien connus et matures. Ils se concentrent sur le travail de développement, les tests et la mise en production du produit. Presque toutes les équipes utilisent une sorte de technique Agile ou d'itération rapide.\n\n## Traduisez ce qui suit en français : IT\n\nL'objectif de l'équipe informatique est d'optimiser l'infrastructure en termes de coût et d'efficacité. Ils veillent à ce que l'infrastructure soit fiable et dispose de contrôles et de mesures de sécurité intégrés. Principalement, ils examinent les aspects de calcul, de stockage, de réseau, de conformité et de coût.\n\n## Sécurité\n\nLe rôle de la sécurité est devenu de plus en plus important ces dernières années, d'autant plus récemment avec les employés travaillant à domicile en raison du COVID-19. L'équipe de sécurité se concentre principalement sur la sécurisation de la propriété intellectuelle, des données et de l'infrastructure. Les outils couramment utilisés sont la gestion des identités, la protection, la détection et la correction. Comprendre comment ces outils fonctionnent à un niveau élevé est important pour votre changement organisationnel.\n\n## Les données\n\nCe tout dernier groupe, qui aurait pu auparavant être un statisticien ou un mathématicien effectuant de l'exploration de données, prend désormais de l'importance avec l'émergence de directeurs des données et des organisations qui les entourent. Ils se concentrent sur l'analyse, la catégorisation et la création de valeur réelle à partir de vos données. Que votre organisation soit dans le secteur de la fabrication ou des services, ou que vous cherchiez à attirer de nouveaux clients ou à économiser de l'argent, il existe de nombreux domaines dans lesquels les scientifiques des données peuvent apporter de la valeur.\n\n## Réunir les groupes\n\nPour effectuer un changement numérique efficace, tous ces groupes doivent avoir une compréhension mutuelle de ce que chacun apporte à la table et une vision unifiée. Vous ne voulez pas que vos scientifiques des données explorent des données qui n'ont aucune valeur pour le développement ou l'informatique. Vous ne voulez pas que votre équipe de sécurité verrouille tout si étroitement que l'équipe de développement ne peut pas accomplir son travail. Parmi les groupes, il doit y avoir des stratégies, des processus et des architectures communs.\n\nBien que les objectifs et les résultats communs soient l'idéal, il y a des obstacles à surmonter. L'un des domaines les plus difficiles dans le changement organisationnel est la rupture de communication aux frontières. Certaines organisations ont créé de nouveaux groupes pour lisser les relations entre les différents groupes. Par exemple, entre la sécurité et l'informatique, il pourrait y avoir une équipe Sec Ops ou Sec Dev Ops chargée d'automatiser les politiques et les procédures émanant de l'équipe de sécurité. Un autre exemple serait une équipe de développement de données, composée de développeurs collaborant avec les scientifiques des données pour fournir des processus plus reproductibles grâce au développement d'applications ou à l'intégration d'outils et d'applications. Des architectures communes et des ensembles d'outils communs sur lesquels tous les groupes peuvent compter facilitent grandement le processus et les changements.\n\n## Le système parfait\n\nÀ quoi ressemblerait une architecture commune ? Une version utopique n'existe pas aujourd'hui, mais nous pouvons examiner les éléments et peut-être construire quelque chose vers cet idéal.\n\nLe libre-service est un must. Par exemple, si un data scientist a besoin de plus d'espace de stockage pour des données, il n'aurait pas à appeler le service informatique et remplir une multitude de formulaires, mais plutôt utiliser un portail de libre-service qui permettrait de gagner du temps et qui fournirait l'espace de stockage. Bien sûr, le portail serait régi par des politiques, de sorte que l'équipe de sécurité puisse être assurée que les données confidentielles que le scientifique dépose dans un lecteur sont chiffrées et que le contrôle d'accès est automatique.\n\nUne autre fonctionnalité idéale serait que le système soit auto-régénérant et axé sur les données. Si les machines étaient infectées, par exemple, elles seraient automatiquement mises en quarantaine et les charges de travail migreraient vers une autre zone dans le centre de données ou vers le cloud public. Encore une fois, le service informatique devrait établir des politiques et surveiller les processus, mais le système serait principalement automatique. Le système ne devrait pas seulement être automatique, mais intelligent, apprenant de l'expérience et devenant plus efficace.\n\nNous pouvons obtenir certains éléments de ce système utopique dès aujourd'hui avec des produits prêts à l'emploi en les intégrant et en les faisant utiliser par tout le monde. Voyons ce que chaque organisation voudrait de cette architecture.\n\n## Architecture informatique (multi-hybrid clouds)\n\nL'informatique est responsable de l'infrastructure sous-jacente et des informations de données au sein de l'organisation. Si l'informatique pouvait établir une base solide, tout le monde pourrait la développer. L'informatique doit passer à une solution de cloud multi-hybride afin que l'infrastructure puisse être facilement orchestrée selon les besoins, avec une flexibilité basée sur la politique. Il y a toujours un compromis entre coût et fiabilité, mais vous avez des options. Une couche d'infrastructure définie par logiciel permet facilement l'orchestration du calcul, du stockage, du réseau, de la sécurité, et maintenant même de nouvelles choses comme la mémoire et les accélérateurs. La fondation multi-hybride du cloud est un aspect clé de votre architecture commune.\n\n## Architecture de sécurité\n\nL'équipe de sécurité ajouterait à ce système et le rendrait aussi automatisé que possible. La première chose serait l'aspect de l'identité. Cela signifie que non seulement vous pouvez identifier les utilisateurs, mais aussi l'infrastructure, les applications et les services afin que tout ait une identité. Ces identités peuvent être liées à des autorisations et des accès spécifiques pour s'assurer que tout est authentifié. Du côté de la sécurité, vous souhaitez du chiffrement et des mesures correctives lorsque des problèmes surviennent. Idéalement, vous pourriez établir une racine de confiance afin que tout dans l'écosystème, aussi bien dans les applications que dans les services, jusqu'au micrologiciel et au BIOS des machines, soit digne de confiance.\n\n## Architecture de développement\n\nLes développeurs peuvent craindre que tout ce processus puisse ralentir le développement, donc il doit se dérouler presque automatiquement. La plupart des développeurs se concentrent maintenant sur des composants réutilisables pouvant être testés pour assurer leur sécurité. Ils le font à travers des écosystèmes sur des conteneurs tels que Kubernetes, Docker ou Mesos. La sécurité peut être intégrée au cycle de développement à l'étape de déploiement avant de passer avec succès à la production. Au-dessus de la couche de services se trouve une couche d'application où les développeurs peuvent tirer parti des flux de travail. Ces flux de travail peuvent être des flux de travail de développement tels que CI/CD ou des flux de travail métier grâce à des outils d'automatisation tels que Robotic Process Automation. Avoir à la fois la couche de services et la couche d'application sont des éléments clés de cette architecture utopique.\n\n## Architecture des données\n\nAvec des données dispersées à travers plusieurs écosystèmes, des nuages publics et même en périphérie, nous avons besoin d'une meilleure façon de gérer les données pour les scientifiques des données et les développeurs d'applications. Extraire les données du stockage est l'un des éléments importants ici. Avec cette structure, vous pouvez orchestrer les données à travers l'immense infrastructure et les lier uniquement aux applications et services où elles sont nécessaires. Les données pourraient être abstraites pour atterrir sur l'infrastructure au meilleur endroit pendant cette période de temps, que ce soit en périphérie, dans le centre de données ou traitées dans plusieurs endroits différents pour des répliques d'applications. La sécurité serait nécessaire pour verrouiller les données, puisque les données sont la raison même de l'existence de l'infrastructure. Certaines start-up se lancent maintenant dans cet espace pour prendre le contrôle de la couche de gestion des données.\n\nCette architecture utopique, avec sa myriade de parties mobiles, est appelée l'architecture Edgemere. Nous essayons de voir comment toutes ces parties s'emboîtent pour aider les organisations à accélérer leur transformation numérique. Nous devons comprendre les besoins de chaque organisation, quels sont leurs cas d'utilisation, et quelles sont les similarités entre les groupes afin de concevoir une architecture dans laquelle toute l'organisation peut travailler.\n\nLa mission de votre organisation est de faire tomber les barrières entre les groupes, de développer une vision commune de là où vous souhaitez être en termes d'organisation, de procédures et d'architecture, et de mettre en place une feuille de route pour y parvenir.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT24-fr","image":"./episodes/edt-24/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions, secteur public, Intel, expose la stratégie numérique et l'architecture pour transformer efficacement votre organisation. Il explique comment les éléments organisationnels, procéduraux et technologiques doivent être équilibrés pour travailler efficacement vers une architecture idéale commune afin de soutenir une vision unifiée."},{"id":93,"type":"Episode","title":"Opérationnalisation des pipelines de données","tags":["dataarchitecture","datacentric","data","datamesh","datapipeline","technology","people"],"body":"\r\n\r\nPour le dernier épisode de cette série, Darren s'entretient avec Sarah Kalicin, Lead Data Scientist chez Intel, sur l'opérationnalisation de votre pipeline de données. Ils discutent de la manière dont, une fois que vous avez vos analyses de données, vous pouvez les transformer d'une expérience scientifique ponctuelle en une source d'informations continue.\n\n## Comment opérationnalisons-nous les informations issues de l'analyse ?\n\nLa première chose à comprendre concernant la chaîne de traitement des données est qu'elle ne fonctionne pas comme un système électrique fermé que l'on peut mettre en place, laisser de côté et, six mois plus tard, actionner un interrupteur en sachant que l'ampoule s'allumera. Une chaîne de traitement des données est différente dans le sens où les données sont variables ; elles peuvent changer ou se dégrader, par exemple. Vous ne serez donc pas forcément récompensé par l'allumage de l'ampoule à n'importe quel moment, ou dans ce cas, par la découverte des insights que vous recherchez. Vous devez constamment penser à ce qui peut mal se passer dans le système et comment corriger ces courts-circuits.\n\nLa détection des anomalies est une partie intégrante du processus. On ne peut pas prévoir toutes les situations, il est donc nécessaire au moins de pouvoir constater lorsqu'un événement se produit en dehors des limites des analyses initiales. Un exemple est la crise du COVID, un événement imprévisible qui a provoqué des schémas largement hors normes pour de nombreux systèmes. Un autre exemple serait une entreprise produisant des widgets. Afin de savoir combien de widgets produire, le processus de données contient la demande des clients, l'offre actuelle et les pertes de rendement. Ces éléments peuvent être relativement stables avec le temps, mais il pourrait par exemple y avoir un événement de relations publiques qui provoque une explosion de la demande des clients. Cela pourrait avoir un impact considérable sur les modèles. L'apprentissage automatique et l'apprentissage profond analysent les modèles familiers, et s'ils ne les ont jamais rencontrés auparavant, les modèles vont échouer ou se dégrader. Il faut rester à la pointe de la découverte.\n\nLa seule façon de rester à la pointe de la découverte est d'automatiser vos pipelines de données pour un accès rapide aux informations. C'est l'avantage concurrentiel: des données actuelles et pertinentes qui peuvent vous aider à résoudre rapidement vos questions.\n\nLes équipes informatiques et les équipes de données doivent collaborer sur l'automatisation et déterminer ce qui doit être automatisé pour les données entrantes, ainsi que sur la gestion de toutes les modifications apportées au modèle que les scientifiques des données souhaitent effectuer, afin de les intégrer facilement dans le flux de travail.\n\n## Déploiement court-circuite\n\nIl existe deux types de contrôles qui peuvent prévenir les courts-circuits lors du déploiement : les contrôles du système analytique et les contrôles organisationnels.\n\nLe contrôle du système analytique consiste à mettre en œuvre les modèles que vous avez entraînés, en faisant passer les données afin de répondre facilement à vos questions. Ces modèles déployés doivent être régulièrement vérifiés pour garantir l'exactitude des données. De nombreux facteurs peuvent affecter négativement les données, tels que les changements environnementaux, l'étalonnage de la machine, les problèmes de distribution, et ainsi de suite.\n\nCela n'est pas si différent du monde du développement de logiciels où les changements peuvent affecter les prédictions. Le département informatique est familier avec le processus d'exécution de tests pour s'assurer que leurs modèles ou applications fonctionnent conformément aux directives établies, donc les développeurs opérationnels et les scientifiques des données devraient profiter de ces ressources et connaissances. Il n'est pas nécessaire d'inventer de nouveaux processus, mais plutôt, les groupes devraient fusionner leurs ressources pour se préparer à la réussite.\n\nLes contrôles organisationnels remontent à la nécessité d'avoir une base organisationnelle engagée dans une approche centrée sur les données, et fournissant les bonnes personnes et ressources pour travailler ensemble en vue d'objectifs communs. La meilleure chance d'obtenir l'opérationnalisation est lorsqu'il y a collaboration, confiance, compréhension des besoins et boucles de rétroaction entre les groupes de l'organisation.\n\nLes boucles de rétroaction sont essentielles dans ce processus. Par exemple, les spécialistes du sujet peuvent fournir des informations sur la dynamique du marché afin que les scientifiques des données puissent surveiller le modèle pour détecter ces changements dans les données. Si un modèle est destiné à être utilisé sur une longue période, il devra constamment être itéré et amélioré.\n\nLes consommateurs des données devraient disposer d'un tableau de bord qui leur fournit des informations et leur permet de comprendre pourquoi quelque chose semble un peu étrange. Plus ils peuvent enquêter ou soulever ce qui doit être investigué, plus votre organisation sera habilitée.\n\n## Pipeline translates to \"oléoduc\" in French.\n\nUne clé du côté informatique pour opérationnaliser le pipeline de données consiste à utiliser un contrôle de version tel que GitHub afin de pouvoir accéder aux versions précédentes de votre modèle. Dans un but d'audit, il est également important de pouvoir stocker les données ayant servi à créer le modèle ainsi que d'autres données historiques. Vous souhaitez être en mesure d'observer les tendances et de voir comment une certaine caractéristique a évolué ou a impacté le modèle. Vous pouvez également intégrer des données historiques à vos nouveaux modèles pour évaluer leur impact sur vos données actuelles.\n\nPar exemple, certains systèmes auront une image déformée avec un grand nombre de personnes travaillant à domicile pendant la COVID. Un cas concret est la Marine. Depuis la COVID, 95% de leurs employés en informatique sont en télétravail, et leur productivité a augmenté de 35%. À partir de ce seul élément de données, on pourrait dire que tout le monde va travailler à domicile à partir de maintenant. Continuerez-vous à bénéficier d'une augmentation de 35%, ou si les gens reviennent au bureau, verrez-vous une diminution de 35% ? De toute évidence, cet unique élément de données n'est pas forcément suffisant pour prédire la productivité réelle.\n\nUn autre outil que l'informatique peut offrir est l'intégration et le déploiement continus. En utilisant Jenkins ou GitHub Actions ou un outil similaire lors du travail sur un modèle, vous pouvez automatiquement exécuter des tests sur votre modèle avec vos données ou générer des données factices en cours d'exécution.\n\nLes informaticiens et les data scientists doivent collaborer sur ce qu'il faut surveiller et comment surveiller les résultats des modèles. Les informaticiens peuvent surveiller automatiquement les résultats, ainsi que la performance des modèles dans l'infrastructure. Par exemple, grâce à un déploiement automatisé de l'intégration continue, les informaticiens peuvent rapidement alerter les data scientists lorsque le temps d'exécution d'un modèle dépasse la norme pour éviter qu'il ne devienne incontrôlable. Si les informaticiens s'investissent dans la création de valeur des données, ce qui a généralement fait défaut dans l'industrie dans son ensemble, le processus sera plus facile et plus cohérent pour tout le monde.\n\nUn autre aspect à prendre en compte est la conception des expériences car les interactions entre variables et caractéristiques sont également importantes. Les experts du domaine peuvent aider à déterminer quelles peuvent être les interactions potentielles et vous pouvez les modéliser pour mieux comprendre les variations qui peuvent être attendues.\n\nC'est passionnant qu'un scientifique des données puisse prendre des données brutes et en tirer des enseignements. Cependant, cela nécessite une équipe. Plus tout le monde dans votre organisation peut apprendre les uns des autres dans un environnement d'équipe, plus de grandes choses peuvent se produire.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT25-fr","image":"./episodes/edt-25/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte en chef des solutions, secteur public, chez Intel, parle avec Sarah Kalicin, data scientist en chef, Intel, à propos de l'opérationnalisation du pipeline de données de votre organisation. Cela nécessite un effort d'équipe pour modéliser, surveiller et produire une source continue d'informations précieuses. Il s'agit du dernier épisode de la série Démarrer votre transformation organisationnelle pour devenir centrée sur les données."},{"id":94,"type":"Episode","title":"Solutions de données Multi Cloud avec Hammerspace","tags":["hammerspace","data","technology","dataarchitecture","multicloud","compute","multihybridcloud","datamesh"],"body":"\r\n\r\nLe stockage est prêt à être perturbé. Actuellement, la gestion des données se fait de manière laborieuse, procédurale et souvent manuelle, ce qui entraîne souvent des erreurs. Hammerspace a été fondé pour résoudre ce problème en abstrayant les données de l'infrastructure de stockage.\n\n## Imaginez, ne serait-ce qu'une seconde...\n\n... si vos données étaient libérées de l'infrastructure de stockage. Affranchies des limitations imposées par les systèmes de stockage actuels, les utilisateurs pourraient gérer et protéger leurs données en libre-service, modifier instantanément le profil des coûts et accéder aux données depuis n'importe où sur l'infrastructure. Le stockage défini par logiciel pourrait augmenter les performances à la demande et déployer des charges de travail modernes telles que Kubernetes sur n'importe quelle infrastructure sous-jacente, où que ce soit.\n\n## Le défi\n\nLes applications sont devenues transportables, mais les données sont toujours cloisonnées. Le défi réside dans le fait que les performances, la fiabilité et la gestion sont toutes affectées à grande échelle à cause du problème de cloisonnement. La solution consiste à libérer les données des limitations de l'infrastructure sous-jacente. Hammerspace réalise cela grâce à la désagrégation des métadonnées, en assimilant les plus petites parties constituantes pour rendre les données transportables.\n\n## Détacher les données du stockage.\n\nAvec cette technologie, vos applications disposent de données à la demande, où que vous soyez. Vous avez un contrôle indépendant, des plans de données, un espace de noms mondial et un système de fichiers qui s'étend sur plusieurs centres de données et clouds. Le stockage est orchestré ; les données sont entièrement automatisées et exploitent une gestion déclarative autonome des données. En d'autres termes, cela sépare le \"comment\" du \"quoi\", en déclarant l'état final souhaité sans avoir besoin de détailler chaque aspect de la manière d'y parvenir.\n\n## L'architecture d'Hammerspace\n\nLe système vous permet de fonctionner au niveau granulaire des fichiers, fournit des services de données de classe entreprise tels que des instantanés et des clones, et se déplace facilement vers une échelle massive d'un centre de données à un autre, d'un centre de données vers le cloud, puis de nouveau vers un centre de données pour le disperser dans un scénario de cloud multi-hybride.\n\n## Le stockage traditionnel ne parvient pas à relever les défis de la modernité.\n\nL'héritage du stockage ne s'étend pas suffisamment, même avec des solutions d'expansion horizontale, en nuage ou à grande échelle, car même ces grappes deviennent des silos, laissant parfois des données qui ne peuvent pas être corrélées et analysées. Le stockage traditionnel a également du mal à augmenter la capacité et les performances indépendamment.\n\nLa réplication est une technologie ancienne qui entraîne une prolifération des données copiées. Au lieu de déplacer les plus petits éléments constitutifs, vous déplacez la charge utile dans son intégralité. La gestion des données est souvent reléguée au second plan ; la gestion des données doit avoir lieu en amont, et non en aval.\n\nUn problème que l'architecture Hammerspace résout est de trouver facilement des choses. La versionnage peut causer de gros problèmes commerciaux. Par exemple, Airbus et Boeing ont tous deux rencontré d'énormes problèmes car certains ingénieurs n'avaient pas leurs versions les plus à jour. La solution consiste à orchestrer les données.\n\n## Orchestration des données\n\nOrchestration, tout d'abord, consiste à découpler tous les différents silos ; les données sont traitées comme une seule entité. Hammerspace assimile les plus petites parties constitutives, les métadonnées, pour créer, essentiellement, un système anti-gravité des données. Ensuite, ils mettent en œuvre des objectifs tels que la durabilité, la disponibilité et les instantanés, ou des actions personnalisées définies qui peuvent être effectuées grâce à un script Hammer. Enfin, les données, qu'il s'agisse de Kubernetes, de NFS ou de SMB, deviennent portables et s'intègrent au système.\n\nCe que cela signifie concrètement, c'est que vous n'avez pas besoin de vous rendre dans un autre silo pour prendre en charge une charge de travail particulière. Les données sont livrées là où vous le souhaitez. Cette mobilité des données est essentielle car elle est en temps réel ; il ne s'agit pas d'une migration de données, qui est perturbatrice et entraîne une période d'arrêt.\n\nPour minimiser les coûts élevés de sortie, les données sont dédupliquées et compressées sur une base granulaire de fichiers. Au lieu de déplacer un volume fixe complet, vous pouvez sélectionner les données que vous souhaitez déplacer en fonction de toute expression telle que les dossiers, les balises de métadonnées ou un descripteur client. Cela offre de la flexibilité et des économies de coûts.\n\n## Architecture de l'espace martelage\n\nDans l'architecture Hammerspace, le système de fichiers global se compose de trois composants, du point de vue élevé : le système de fichiers global lui-même, la présentation de l'interface utilisateur (NFS, CSI Driver et SMB), et Anvil au sein du système de fichiers global. Anvil est le composant de gestion des métadonnées et DSX, qui fournit des services de gestion des métadonnées. Ces éléments peuvent être mis en œuvre en tant que machines virtuelles, VMware, KBM ou Hyper-V. Anvil est configuré en forme de A, il y en a donc au moins deux à chaque emplacement. DSX peut être parallélisé pour augmenter les performances, de sorte que vous pouvez en avoir plusieurs à différents emplacements pour garantir des performances suffisantes. Ils peuvent être facilement redimensionnés à la baisse.\n\nÀ l'arrière-plan, le stockage sous-jacent peut être le stockage défini par logiciel propre à Hammerspace avec des disques directement attachés, un NAS assimilé, un cloud quelconque, ou toute combinaison de ces options. Cela peut également être mis à l'échelle horizontalement, permettant ainsi d'ajuster les performances et la capacité de manière indépendante. Suivant le modèle du cloud, il est également élastique : si les besoins de l'entreprise changent à cet endroit précis, il est possible de réduire les performances ainsi que la capacité pour s'assurer que les applications disposent exactement de ce dont elles ont besoin. Tout cela offre une architecture très flexible pour répondre à n'importe quelle charge de travail d'application à l'avant-plan.\n\nUn grand avantage de cette architecture flexible est la capacité d'assimiler des données qui sont stockées sur des appareils qui ne vous appartiennent pas, tels que des NAS ou dans le cloud. Cela simplifie le transfert des données. Par exemple, si vous avez un ancien NAS et que vous souhaitez migrer vers un nouveau NAS, peu importe s'il s'agit du même fournisseur ou de différents fournisseurs. Hammerspace assimile les métadonnées et déplace les données en arrière-plan de manière complètement transparente pour les applications, car il s'agit d'une mobilité de données en direct. Un autre avantage significatif est qu'il n'y a aucune interruption lors du déplacement des données.\n\nSi vous souhaitez essayer cette technologie, rendez-vous sur hammerspace.com et commencez avec un essai gratuit incluant une licence pour jusqu'à 10 téraoctets déployés dans Azure, AWS ou Google Cloud.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Johan Ballin"],"link":"/episode-EDT26-fr","image":"./episodes/edt-26/fr/thumbnail.png","lang":"fr","summary":"Johan Ballin, directeur du marketing technique chez Hammerspace, et Darren Pulsipher, architecte de solutions en chef du secteur public chez Intel, discutent de la technologie de cloud hybride de Hammerspace qui libère les données de l'infrastructure de stockage, offrant ainsi une portabilité et des performances des données."},{"id":95,"type":"Episode","title":"Sécurité matérielle : un impératif pour transformer les données.","tags":["datagovernance","data","sgx","confidentialcomputing","cybersecurity","technology"],"body":"\r\n\r\n## Paysage de sécurité des données\n\nLe monde est devenu incroyablement connecté avec tous les appareils, ce qui provoque une croissance exponentielle de la quantité de données que nous devons gérer : plus il y a de volume, plus il y a de risque. C'est un défi, et l'utilisation de nouvelles capacités informatiques telles que le cloud, l'analyse et l'informatique périphérique entraîne une complexité supplémentaire.\n\nIl existe trois principales tendances en matière de sécurité : l'encryption de tout, l'isolation de la charge de travail et une chaîne de confiance. Ces trois domaines sont importants pour aider les organisations à faire face aux exigences réglementaires croissantes visant à protéger les données.\n\nLorsque les organisations décident de faire une transformation digitale, ou avec un catalyseur tel que COVID 19 qui nécessite des travailleurs à distance, par exemple, la sécurité doit aller de pair pour maintenir les contrôles de sécurité et la conformité en place. Si vous abordez la sécurité tout au long de vos déploiements et mises en œuvre, plus vous serez en mesure de transformer la façon dont vous menez vos affaires de manière durable.\n\n## Protéger les données tout au long de leur cycle de vie\n\nChiffrer les données est important à travers toutes ses phases. Historiquement, les attaquants pouvaient accéder aux données directement à travers le réseau. Une fois que cela est devenu crypté, ils ont commencé à pirater les centres de données et à extraire les données des bases de données. Nous avons donc commencé à chiffrer le stockage. Cependant, les données arrivent à un point final et sont extraites en mémoire et déchiffrées, ce qui signifie qu'une attaque sophistiquée qui pourrait obtenir un accès root pourrait potentiellement saisir ou manipuler les données à ce stade de traitement. Cette attaque sur les données en cours d'utilisation est la nouvelle frontière pour les attaquants et pour ceux qui défendent cet espace. Intel dispose de capacités nouvelles et innovantes que nous intégrons dans nos processeurs pour aider les propriétaires de données, les propriétaires d'applications, les fournisseurs de services et essentiellement tout l'écosystème à combler ces vulnérabilités potentielles.\n\n## Pourquoi protéger les données en cours d'utilisation ?\n\nDans de nombreux cas, les attaquants utilisent des élévations de privilèges au niveau du système d'exploitation ou de l'hyperviseur. Ils peuvent provenir d'un système d'exploitation invité, d'un système d'exploitation hôte ou même d'un accès physique au serveur. Les attaquants peuvent être des pirates classiques utilisant des malwares, des concurrents tiers ou des initiés tels qu'un administrateur ou un administrateur système chez un fournisseur de services.\n\n## Intel Software Guard Extensions (Intel SGX) : Extensions de protection de logiciel Intel (Intel SGX)\n\nIntel a intégré une nouvelle technologie dans nos processeurs appelée Intel Software Guard Extensions, ou SGX. Cela fait partie du paradigme de l'informatique confidentielle qui est en plein essor sur le marché en ce moment, car les entreprises essaient de transformer leurs données et de les garder privées. SGX répond à ces nouveaux types d'attaques en permettant à l'application de communiquer directement avec le processeur dans la mémoire chiffrée, contournant le système d'exploitation, l'hyperviseur et essentiellement tout le reste du système. Ainsi, même en cas de violation de votre système d'exploitation, de l'hyperviseur ou d'autres applications, les attaquants ne pourraient pas accéder à ces données car le système d'exploitation lui-même n'a pas la visibilité sur ces données. Par conséquent, vous n'avez pas besoin de faire confiance au fournisseur ou au reste de la pile système car vous opérez essentiellement comme si ils étaient déjà compromis, et votre code et vos données restent confidentiels et intacts; ils conservent leur intégrité.\n\nIntel essaie de faire en sorte que vous ayez à faire confiance au plus petit nombre possible de composants. SGX va plus loin sur cette voie pour le centre de données que tout ce que nous avons vu jusqu'à présent. Tout ce à quoi vous devez faire confiance, c'est votre propre application et le processeur lui-même.\n\nSGX offre une capacité puissante aux entreprises telles que les fournisseurs de services cloud, qui peuvent dire à leurs clients que ni eux, ni le gouvernement, par exemple, ne pourraient accéder à leurs données même s'ils le voulaient.\n\n## Intel SGX en action\n\nSGX est déjà largement utilisé par les fournisseurs de services cloud et les éditeurs de logiciels, mais d'une certaine manière, nous en sommes encore au début. Bien que cette technologie existe depuis plusieurs années, nous avons développé des écosystèmes et nous apportons de nouvelles fonctionnalités dans nos prochains processeurs Xeon Ice Lake de troisième génération. Cela étendra ses capacités, sa capacité à s'adapter à de lourdes charges de travail d'entreprise et lui permettra de protéger des portions de mémoire beaucoup plus importantes avec de meilleures performances, et ce, dans un centre de données grand public beaucoup plus étendu.\n\nIntel est l'un des membres fondateurs du Consortium de calcul confidentiel, qui fait partie de la Fondation Linux. La plupart des grands fournisseurs de services cloud, de nombreux fournisseurs de logiciels et même nos concurrents en silicium travaillent ensemble sur ce type de solutions d'environnement d'exécution sécurisé et établissent des normes pour gérer cette capacité. Nous sensibilisons également sur la nécessité et la valeur commerciale du calcul confidentiel.\n\n## Écosystème de partenaires logiciels Intel SGX\n\nSGX offre de nombreuses options d'utilisation en fonction de ce que recherche le propriétaire des données. Si un client souhaite avoir le plus haut niveau de contrôle, SGX va leur permettre de le faire. En fait, ils peuvent essentiellement simplifier leur application en gardant seulement les données codées qu'ils veulent isoler du reste du système, voire même seulement une partie de cette application. Cependant, ce scénario nécessite qu'ils écrivent l'application à cette fin. Dans l'écosystème, il existe des ressources en open source qui rendent ce type de développement beaucoup plus facile, et cela ne cesse de s'étendre.\n\nDe l'autre côté se trouve une migration rapide et directe. Vous pouvez prendre votre application et la déposer dans un environnement plus sécurisé. L'écosystème répond en créant des conteneurs sensibles à SGX. Vous pourriez déposer votre application non modifiée dans cet environnement où elle serait la seule à fonctionner, de sorte que l'application elle-même pense qu'elle s'exécute dans son environnement d'origine.\n\nNos partenaires de l'écosystème ont réalisé de nombreux progrès dans ce domaine grâce à des éléments tels que Fortanix, Graphene et Scone. Certains sont open source et d'autres sont propriétaires, mais tous sont livrés avec tous les services intégrés. Par exemple, Microsoft Azure Confidential Computing offre toute la gamme, des solutions de migration et de déploiement avec SGX jusqu'aux kits de développement logiciel qui vous permettent de développer votre application directement pour celle-ci et de la déployer dans leur environnement, ce qui vous dispense même de gérer le matériel. Il existe un ensemble complet d'options, de sorte que personne ne devrait craindre les complexités de SGX. Les clients doivent également avoir confiance que des éléments sensibles tels que les algorithmes d'apprentissage automatique ou les clés de chiffrement seront traités avec un niveau de protection très granulaire.\n\n## Confidential Computing: Un bouleversement dans la sécurité\n\nSGX est là depuis un certain temps, et nous avons travaillé pour l'étendre. Il a été testé et mis à l'épreuve, avec des centaines d'articles de recherche et un durcissement au fil du temps grâce aux mises à jour. Il a l'avantage de ne pas être la nouvelle arrivée sur le marché, mais plutôt une solution fondamentale qui est introduite dans le courant dominant avec Ice Lake. Il ne se concentre plus sur de petites zones sensibles, mais est maintenant prêt pour les grandes choses.\n\nLe gouvernement, les services financiers et les soins de santé font partie des secteurs qui ont rapidement reconnu l'intérêt de SGX car ils ont de nombreuses attentes réglementaires et exigences en matière de confidentialité, tout en cherchant à partager des données et à mettre en œuvre des innovations avec plusieurs parties. Les entreprises sont confrontées à des situations similaires, par exemple, si elles souhaitent passer au cloud public mais ne font pas confiance à ce dernier pour protéger leurs données sensibles. Avec SGX, elles n'ont pas besoin de faire confiance au fournisseur.\n\nIntel vient de faire une grande annonce ce mois-ci et nous ouvrons vraiment grand les portes sur les choses à venir. Un excellent point de départ est intel.com/sgx pour les témoignages et une exploration approfondie des informations. Les clients devraient rechercher les partenaires de l'écosystème tels que Azure et Fortanix. Un autre endroit pour obtenir des informations est le Consortium pour le calcul confidentiel en raison du nombre de personnes travaillant dans cet espace.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jesse Schrater"],"link":"/episode-EDT27-fr","image":"./episodes/edt-27/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions, secteur public, Intel, et Jesse Schrater, responsable de la sécurité, groupe des plateformes de données, Intel, discutent du paysage actuel de la sécurité et de la manière dont SGX d'Intel et l'écosystème de partenaires offrent une solution éprouvée et opportune pour les données en cours d'utilisation et d'autres préoccupations en matière de sécurité."},{"id":96,"type":"Episode","title":"Intégration héritée avec le Cloud et l'Automatisation des Processus Robotiques (RPA)","tags":["rpa","bpm","automation","compute","technology"],"body":"\r\n\r\nIl semble que tout le monde soit en plein processus de transformation numérique. Cloud privé, cloud public, cloud multi-hybride, lacs de données, apprentissage automatique, inférence et intelligence artificielle sont tous des termes utilisés aujourd'hui pour décrire leur transformation numérique, mais qu'en est-il de l'intégration des systèmes hérités ? Personne ne parle jamais de l'intégration des systèmes hérités. Pourquoi ? Parce qu'il est difficile d'intégrer les applications, les données et la sécurité des systèmes hérités dans votre nouvel environnement cloud multi-hybride immaculé. C'est l'une des dernières choses dont nous voulons nous préoccuper. Si nous ne développons pas une stratégie pour les systèmes hérités, nous avons un hors-bord avec l'ancre attachée. Cela nous ralentit et empêche notre bateau de quitter le port.\n\nL'un des domaines en croissance pour aider à l'intégration et à l'automatisation de l'intégration des systèmes hérités est l'utilisation d'outils et de cadres d'automatisation. Au cours des 3 dernières années, une importance significative a été accordée à l'automatisation des flux de travail avec des applications héritées et des nouvelles applications orientées cloud pour les travailleurs de l'information. Ces ensembles d'outils sont appelés outils d'automatisation des processus robotiques (RPA).\n\n## Automatisation des processus robotiques (RPA)\n\nQuand j'ai commencé à étudier les outils d'automatisation des processus robotiques (RPA), je pensais qu'ils contrôlaient des robots. J'étais prêt à enfiler mes bottes de sécurité en acier et un casque de chantier pour visiter des installations de fabrication. Cependant, j'ai vite découvert que les RPA imitent la façon dont les travailleurs de l'information utilisent les différents outils qu'ils utilisent. Certains de ces outils sont des outils hérités, et certains sont des nouvelles applications modernes. Les RPA enregistrent comment le travailleur de l'information utilise l'interface utilisateur des différents outils, puis permettent la relecture de l'enregistrement, automatisant ainsi le flux de travail du travailleur de l'information.\n\nÉtonnamment, les RPAs sont une technologie assez mature. Beaucoup des entreprises de RPA ont une expérience dans le marché des outils de test d'interface utilisateur. La capacité à enregistrer les interactions de l'utilisateur avec plusieurs applications au fil du temps est essentielle pour développer un test d'interface utilisateur répétable. Ces outils existent depuis plus de 20 ans et sont très développés. Ces outils de test de qualité d'interface utilisateur ont été rebr\n\n## Place de marché actuelle - 2020\n\nLes investisseurs voient ce marché comme un marché prometteur et ont investi massivement dans ces technologies. Au cours des trois dernières années, plus de 2,0 milliards de dollars ont été investis dans le marché de l'automatisation des processus robotisés (RPA). Trois entreprises ont capté la majorité des investissements :\n\nUiPath - 1 milliard de dollars d'investissement sur 300 millions de dollars de revenus annuels.\n\nAutomation Anywhere - Investissement de 500 millions de dollars sur 100 millions de dollars de chiffre d'affaires annuel.\n\nBluePrism - 50 millions de dollars d'investissement sur 30 millions de dollars de revenus annuels.\n\nSorry, I cannot translate your request as you have not provided any text or phrase to be translated. Please provide the text you would like to have translated and I will be happy to assist you.\n\nLa plupart des investissements ne proviennent pas du traditionnel Silicon Valley haute technologie, mais plutôt des centres financiers tels que New York et Londres, ce qui indique que les institutions financières considèrent les RPAs pour automatiser de nombreux flux de travail et processus de leurs propres travailleurs de l'information.\n\n## L'endroit où le RPA fonctionne bien.\n\nLes premiers secteurs verticaux à adopter les RPAs ont été les industries financières, d'assurance et médicales. Ces industries ont cherché des moyens de réduire la variabilité, d'augmenter la fiabilité et de diminuer les coûts. Étant donné le grand nombre de travailleurs de l'information dans ces industries, elles ont fait appel aux RPAs pour automatiser une grande partie du travail actuellement effectué par leurs travailleurs de l'information. Pour automatiser ces flux de travail, les organisations doivent comprendre comment ces flux de travail sont créés.\n\nTout d'abord, comprenons le travailleur de l'information. Beaucoup de travailleurs de l'information passent leur temps à travailler avec plusieurs applications, en reliant les informations et les applications de manière spontanée. À mesure que ces travailleurs continuent à travailler avec ces applications, ils créent de manière organique des flux de travail qui associent les données et les applications de manière inépuisable.\n\nDeuxièmement, cataloguer les flux de travail autant que possible en trouvant des candidats à l'élimination des flux de travail en raison de la duplication et de la redondance. Maintenant que les charges de travail sont comprises, l'étape suivante consiste à hiérarchiser et énumérer les flux de travail. Se concentrer sur les flux de travail les plus utilisés avec le plus grand nombre d'étapes significatives tend à être la meilleure façon de prioriser les flux de travail.\n\nEnfin, déterminez comment automatiser les flux de travail avec des bots RPA. L'automatisation peut être réalisée en enregistrant l'interface utilisateur (UI) du flux de travail à partir de l'un des travailleurs de l'information et en ajoutant des annotations au flux de travail en fonction des saisies de données et des identifiants de sécurité. Une fois l'enregistrement terminé, un bot RPA est créé pour automatiser le flux de travail. Maintenant, vous devez décider de la manière dont vous souhaitez que le RPA s'exécute : en mode superviseur (Attended) ou non supervisé (Unattended).\n\n## Modes de fonctionnement de l'automatisation des processus robotiques (RPA)\n\nLes RPAs fonctionnent selon deux modes d'exécution de base. Attended et Unattended. Attended signifie qu'il s'exécute sur le bureau ou l'ordinateur portable du travailleur de l'information. Il assiste le travailleur de l'information en automatisant les tâches qu'il effectue au quotidien. Unattended s'exécute dans un environnement de bureau virtuel et est généralement activé par un événement ou un déclencheur et fonctionne sans aucune interaction avec le travailleur de l'information. Il y a des avantages à fonctionner dans les deux modes, comme décrit ci-dessous.\n\n## Fréquenté\n\nGère les tâches pour les employés individuels\n\nLes employés déclenchent et dirigent un bot pour effectuer une activité.\n\nLes employés déclenchent des bots pour automatiser les tâches selon les besoins à tout moment.\n\n* Améliore la productivité et la satisfaction des clients dans les centres d'appels et autres environnements de bureau de service.\n\n## Non surveillé\n\nAutomatise les processus de back-office à grande échelle\n\nProvisionné sur la base de processus basés sur des règles\n\nLes bots accomplissent des processus commerciaux complets sans intervention humaine selon un calendrier prédéterminé.\n\nLibère les employés du travail mécanique, réduit les coûts, améliore la conformité et accélère les processus.\n\n## Comment intégrer la RPA dans votre entreprise\n\nPour comprendre comment les RPAs s'intègrent dans votre entreprise, vous devez d'abord examiner les utilisateurs des RPAs. Plus précisément, il existe trois types d'\"acteurs\" qui utilisent, gèrent ou influencent les outils RPA.\n\nTravailleur de l'information - C'est l'utilisateur principal des outils RPA. Leurs processus manuels sont des cibles pour l'automatisation.\n\nDéveloppeur d'applications - Les bots RPA changent lorsque les applications sont mises à jour ou créées. Les changements de l'interface utilisateur nécessitent de \"re-enregistrer\" les bots RPA.\n\nOpérations informatiques - Gérer les outils RPA et déployer des bots RPA non supervisés.\n\nTranslatez ce qui suit en français : *\n\n## Gérer le changement\n\nGérer la complexité des configurations et de la sécurité sont des facteurs critiques pour le déploiement réussi des outils et des robots RPA. Tout d'abord, vous avez besoin de comprendre comment les différents utilisateurs des RPAs interagissent lors des modifications apportées aux applications, aux flux de travail et aux processus. Cette compréhension est essentielle pour gérer les changements dans les robots RPA et les ensembles d'outils qu'ils utilisent.\n\nDe petits changements apportés aux applications peuvent avoir un impact profond sur les travailleurs de l'information et sur la façon dont ils accomplissent leur travail quotidien, ce qui signifie en retour qu'il faut enregistrer un nouveau robot RPA ou mettre à jour un robot existant. En raison de l'interdépendance des robots RPA avec les ensembles d'outils et les flux de travail, la création de robots RPA lorsque les flux de travail ou les ensembles d'outils sont encore immatures entraîne une instabilité et une fragilité inutiles. Les processus et les ensembles d'outils matures sont d'excellents candidats pour l'automatisation RPA.\n\nUne autre chose à considérer est l'emplacement où s'exécutent les outils que vous automatisez avec votre outil RPA. Utilisent-ils des applications et une infrastructure héritées ? Utilisent-ils un Cloud public ou privé ? Comment les réseaux de ces systèmes sont-ils connectés ? À mesure que le nombre d'environnements augmente, la complexité de la maintenance et de la mise à jour des applications et des bots RPA augmente également. Trouvez des moyens de réduire le nombre de frontières environnementales traversées par le bot RPA.\n\n## Gestion de la sécurité\n\nUn autre facteur critique à prendre en compte est la sécurité des bots RPA. Lorsqu'un employé enregistre son flux de travail, il doit s'authentifier (se connecter) à chaque outil qu'il utilise. Les employés s'authentifient en utilisant des noms d'utilisateur et des mots de passe, des clés d'authentification, voire même des outils de connexion unique d'entreprise. De toute manière, vous devez gérer la sécurité de ces outils dans le contexte de l'exécution du bot RPA. Toute modification de l'authentification (nom d'utilisateur, mot de passe, clés d'authentification ou identifiants) nécessite des modifications du bot RPA. Beaucoup d'outils RPA tiennent compte de cela et disposent de mécanismes pour injecter les informations d'identification de sécurité dans le bot RPA et s'authentifier auprès des outils en temps d'exécution.\n\n## Gérer les outils et les robots RPA avec des flux de travail SecDevOps\n\nLa complexité des outils et des robots RPA se prête très bien aux schémas bien connus dans le monde du SecDevOps. Heureusement, bon nombre des problèmes liés à la gestion des configurations et des dépendances sont bien gérés grâce à un processus SecDevOps.\n\n## Regroupement RPA\n\nL'un des trucs consiste à considérer le bot RPA comme un service complexe qui contient plusieurs machines virtuelles ou des conteneurs pour chacun des outils, un bureau virtuel et le bot lui-même. Ces services peuvent être regroupés et gérés ensemble comme un seul package. Un ensemble comprend non seulement les services, mais aussi la manière dont les services communiquent (réseau) de manière sécurisée (authentification).\n\nLe fait de transmettre un ensemble de services à un orchestrateur permet une automatisation plus poussée de la gestion des pare-feu réseau, de la sécurité, de l'injection des clés d'identification et de la gestion du cycle de vie du bot RPA et des outils qu'il utilise. Il existe plusieurs outils dans l'espace de virtualisation (VMWare et OpenStack) qui permettent la création et la gestion de ces ensembles. L'espace des conteneurs dispose également d'outils de planification et d'orchestration similaires, notamment Kubernetes, Mesos et Docker Swarm.\n\n## Sécurité, développement et exploitation (SecDevOps) en continu\n\nUn pipeline de SecDevOps simple gère le package de robot RPA tout comme n'importe quel autre package d'application traditionnelle.\n\nUn travailleur de l'information construit les ensembles de robots RPA en enregistrant le flux de travail de l'interface utilisateur dans un environnement de développement. Le travailleur enregistre facilement son flux de travail, puis crée un ensemble qui est \"enregistré\" dans le pipeline. À ce stade, l'ensemble de robots RPA passe par un cycle de construction, de test et de production. Des points de contrôle à chaque étape du processus garantissent la qualité du robot RPA. Étant donné que l'ensemble peut injecter des éléments réseau et de sécurité en fonction des différents environnements, les robots RPA peuvent être réutilisés par différents travailleurs de l'information et dans différents environnements.\n\nUn autre avantage de mettre des robots RPA dans des bundles RPA est la gestion des outils et robots à travers plusieurs environnements d'infrastructure tels que les systèmes hérités, privés et publics du Cloud. De nombreux outils d'orchestration de services peuvent créer automatiquement des connexions entre ces environnements d'infrastructure en créant un réseau superposé. Ce pipeline réduit la quantité de travail \"pratique\" effectuée par l'organisation informatique et, dans de nombreux cas, toutes les étapes du pipeline sont automatisées.\n\n## Les pièges des bots RPA.\n\nVoici une liste de choses à surveiller lors de l'utilisation de bots RPA dans vos systèmes d'entreprise.\n\nLa sécurité peut être une faille béante si vous n'y prêtez pas attention. L'une des plus grandes erreurs est d'exécuter des applications dans un bot RPA en mode privilégié ou avec des identifiants de compte \"globaux\".\n\n* Les robots RPA sont étroitement liés aux interfaces utilisateur de plusieurs applications, toute petite modification d'une application signifie que vous devez enregistrer à nouveau le robot RPA.\n\nLes robots RPA ne peuvent pas bien gérer les changements, ils sont très fragiles face aux modifications dans les applications, voire même dans la configuration des applications.\n\nLa réutilisation est minimale en raison d'un couplage étroit avec les interfaces utilisateur de l'application. Certains outils utilisent des balises au lieu de la position absolue du curseur et des clics.\n\nCertains interfaces utilisateur ne se prêtent pas aux RPAs car elles sont dynamiques. Cela signifie qu'elles sont difficiles à enregistrer.\n\nL'industrie de l'automatisation des processus robotiques (RPA) met tout en œuvre pour surmonter certains des problèmes inhérents à l'enregistrement et à la reproduction des outils. Certains de ces écueils ne peuvent être résolus en raison de l'approche généralisée. D'autres options, telles que les passerelles API et l'automatisation fonctionnelle, doivent être évaluées.\n\n## L'intelligence artificielle vient au secours des RPA.\n\nComme mentionné dans les problèmes de la RPA, la réutilisation des bots est un gros problème que l'industrie cherche à résoudre. L'une des techniques qu'elle étudie est l'utilisation de l'IA et de l'inférence pour gérer les interfaces utilisateur dynamiques et les petits changements d'applications sans avoir à réenregistrer les bots de la RPA. La reconnaissance de motifs et la reconnaissance optique de caractères sont deux domaines qui sont utilisés pour former des modèles d'IA afin d'identifier les champs et les segments des interfaces utilisateur.\n\nAvec ces modèles d'IA, les robots peuvent être plus flexibles, se prêtant à une réutilisation dans plusieurs ensembles d'outils et processus/flots de travail similaires. Un autre domaine que les fournisseurs de RPA étudient est l'optimisation des processus à l'aide de l'IA et de l'apprentissage automatique.\n\n## La migration des héritages est un voyage.\n\nLe marché des RPA a gagné en dynamisme alors que les entreprises cherchent à moderniser leur infrastructure informatique et leurs processus. Automatiser les processus manuels actuels grâce à l'enregistrement est une victoire rapide dont de nombreuses organisations bénéficient. Cependant, les RPA devraient être considérées comme un mécanisme temporaire plutôt que comme une solution finale. Pourquoi ? De nombreux processus d'information actuels nécessitent des systèmes et des politiques hérités. Automatiser un ancien processus sur une nouvelle infrastructure revient à automatiser la fabrication de fouets pour un atelier automobile. Il peut y avoir un avantage au départ, mais à long terme, le processus est très inefficace et désuet. Peu importe sa fiabilité et sa rapidité, il se peut qu'il ne soit tout simplement pas nécessaire.\n\n## Conclusion\n\nLes outils de robotisation des processus automatisés (RPA) sont un autre ensemble d'outils qui peuvent être utilisés pour aider les organisations dans leur transformation numérique, en passant d'une infrastructure informatique et de processus hérités à une infrastructure et des processus plus modernes. Les outils à eux seuls ne sont pas suffisants, et vous devez planifier comment vous allez les utiliser, les gérer et éventuellement les remplacer. Voici quelques conseils utiles lorsque vous travaillez avec ces outils.\n\nTraitez les RPAs comme des services complexes fonctionnant dans votre Cloud Multi-Hybride.\n\nExécutez vos robots RPA à travers les flux de travail SecDevOps comme les autres applications.\n\nInjecter la sécurité et l'authentification à l'exécution dans l'outil RPA.\n\nTrouvez des moyens de réutiliser les bots RPA dans différentes parties de votre organisation.\n\nAyez un plan pour remplacer votre bot RPA par une intégration simplifiée.\n\nRecherchez des moyens de réduire les applications héritées (remplacer ou supprimer)\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT28-fr","image":"./episodes/edt-28/fr/thumbnail.png","lang":"fr","summary":"Une des zones en expansion pour aider à l'intégration des systèmes existants et à l'automatisation de l'intégration est l'utilisation d'outils et de cadres d'automatisation. Au cours des 3 dernières années, une mise en évidence significative de l'automatisation des flux de travaux avec les applications existantes et les nouvelles applications conscientes du cloud pour les travailleurs de l'information est apparue. Ces ensembles d'outils sont appelés outils d'automatisation des processus robotiques (RPA). Darren Pulsipher, architecte principal des solutions, secteur public, Intel, examine l'industrie de l'automatisation des processus robotiques (RPA) et les offres d'outils."},{"id":97,"type":"Episode","title":"Base de données hyperscale de nouvelle génération - Aerospike","tags":["aerospike","database","ingestion","optane","data","technology","pmem"],"body":"\r\n\r\n## Base de données hyperscale de nouvelle génération\n\nLes bases de données montrent leur ancienneté, prenant toujours du temps pour obtenir des résultats. Aerospike, avec l'aide de la technologie Intel, fait une percée en termes de vitesse, de volume et de faible latence. Tim Faulkes, Vice-Président de l'Architecture des Solutions chez Aerospike, rejoint Darren dans cet épisode pour discuter des avantages de leur technologie.\n\n## NoSQL - Un marché émergent avec plusieurs technologies\n\nAerospike a été créé en 2009, avec les premières mises en œuvre de production en 2011. Les cofondateurs étaient conscients des défis à venir. Ils ont construit l'architecture à partir de zéro en prévoyant qu'elle serait fiable, à l'échelle de l'entreprise, ne jamais tomber en panne et capable de gérer des ingestions massives de données. L'architecture d'Aerospike reposait fortement sur les SSD, qu'il s'agisse de lecteurs NVMe dans les incantations modernes ou de lecteurs SATA SAS. Les deux étaient encore à leurs balbutiements, donc les fondateurs étaient visionnaires quant à l'évolution du marché. Ils comptaient sur ces dispositifs pour obtenir de la vitesse, c'est pourquoi la technologie Intel était si importante. Depuis lors, d'autres technologies d'Intel, telles que la mémoire persistante, ont considérablement facilité les choses.\n\n## Index en DRAM, Données sur SSD\n\nAerospike aborde les bases de données différemment. Les SSD ne sont pas simplement des disques durs plus rapides. Un disque dur possède des secteurs physiques, des têtes et d'autres composants qui doivent se déplacer pour lire des données. Un SSD peut rapidement lire des milliers de morceaux de données en parallèle. Il ne fonctionne pas bien avec les disques rotatifs, mais avec NVMe/SSD, il fonctionne de manière extrêmement rapide. Personne n'a les mêmes performances, même pas les bases de données en mémoire.\n\nL'architecture unique est conçue pour un volume élevé, un débit élevé et une latence très faible. Par exemple, certains clients effectuent régulièrement 20 millions de transactions par seconde. Certains atteignent des petabytes de données. En général, les clients utilisent du matériel de qualité mais le temps de réponse aller-retour est critique. Il faut environ 200 microsecondes à Aerospike pour rechercher une donnée et la ramener au client, et non pas des dizaines ou des centaines de millisecondes. Lorsque vous avez besoin d'une grande quantité de données rapidement, comme dans la détection de fraude, ou si vous ingérez beaucoup de données, comme dans l'IoT, les véhicules autonomes, la lecture de capteurs ou les dispositifs médicaux, Aerospike peut effectuer des millions de transactions par seconde en lecture et en écriture. Vous n'avez pas besoin de l'envoyer dans un bus de messages et de laisser ensuite la base de données notifier les systèmes en aval via le bus de messages qu'elle a reçu les données. Elles sont déjà persistantes.\n\n## Alimentant l'innovation de premier plan dans le monde entier\n\nQuels sont les cas d'utilisation privilégiés pour Aerospike ? Il y a des marchés évidents tels que l'IoT, la détection de fraude et les données des capteurs, mais c'est un marché émergent. Aerospike a commencé dans la technologie de la publicité, où la récupération des données doit être effectuée en millisecondes ou moins. C'est là que les bonnes performances ont été mises à l'épreuve à grande échelle.\n\nLes industries supplémentaires où Aerospike peut être utile sont variées. Un exemple est une grande société de télécommunications en Inde. L'infrastructure là-bas n'est pas toujours fiable et parfois les appels sont interrompus. Cette entreprise, grâce à la technologie Aerospike, peut détecter en temps réel qu'un appel a été interrompu et peut immédiatement contacter le client pour lui offrir des crédits ou d'autres compensations afin de maintenir sa satisfaction.\n\n## Règlement en temps réel des paiements instantanés\n\nUne nouvelle zone dans laquelle Aerospike s'est fortement impliqué est celle des paiements numériques en temps réel. Cela ne nécessite pas spécifiquement une grande quantité de données ni des exigences de débit élevées, mais ce dont ces entreprises ont besoin, c'est d'une cohérence absolue et d'une disponibilité absolue, même en cas, par exemple, de perte d'un centre de données.\n\nLe plancher de fabrication est également un marché émergent pour la technologie Aerospike dans des domaines tels que la fabrication de semi-conducteurs, où elle est utilisée pour stocker des données de capteurs afin de fournir des analyses en temps réel. Aerospike brille dans n'importe quelle industrie où il y a beaucoup de données et où vous en avez besoin rapidement.\n\n## Performance à grande échelle - Résultats de tests indépendants de tiers.\n\nIl existe de nombreuses bases de données dotées d'une bonne technologie, mais elles sont confrontées à des défis d'échelle. Prenons Redis, par exemple. Étant donné qu'il stocke toutes ses informations en mémoire, il n'est pas possible d'atteindre dix téraoctets sans coûts significatifs, sans parler de centaines de téraoctets ou même de pétaoctets. En revanche, Aerospike stocke les informations sur des SSD, ce qui implique une différence de temps d'environ 100 microsecondes lors de la recherche de données. À l'échelle des pétaoctets, Aerospike remplace des technologies plus anciennes telles que Cassandra, qui se distingue par son évolutivité, mais manque de vitesse.\n\n## Coût total de possession\n\nDe plus, par rapport à ces technologies, les gens économisent beaucoup d'argent en passant à Aerospike car le nombre de nœuds diminue considérablement en raison de son architecture unique. Les économies se font non seulement sur les dépenses en capital (capex), mais aussi sur les dépenses opérationnelles (opex) car il y a moins de machines à superviser.\n\nAerospike a effondré certaines des couches traditionnelles de l'architecture. Souvent, les systèmes hérités ont un cache supplémentaire devant eux pour accélérer les performances. Cela introduit de la complexité. Aerospike n'a pas besoin d'un cache. Il n'y a pas une grande quantité de mémoire DRAM. Il repose sur la vitesse des SSD et de la technologie sous-jacente pour obtenir des performances de stockage brut sans cache. Étant donné que les couches de cache et de stockage sont effacées, la solution est simplifiée, ce qui signifie une fiabilité et une vitesse intégrées.\n\n## Exemple de déploiement continental\n\nLa cohérence dans plusieurs zones géographiques est également un avantage important de la technologie. L'architecture moderne nécessite une faible latence, ce qui signifie généralement qu'il existe plusieurs clusters basés sur H où les données sont proches de l'utilisateur. Sinon, la vitesse de la lumière devient un facteur. Aerospike peut faire en sorte que tous les clusters basés sur H communiquent les uns avec les autres, de sorte que si un enregistrement est modifié dans un cluster, il se propagera automatiquement aux autres clusters, de manière asynchrone. Cependant, dans certains cas, tels que les paiements numériques, il doit y avoir une forte cohérence entre les clusters, donc ils préféreraient plutôt répliquer les données de manière synchrone. Ils préfèreraient lire à partir de la copie locale des données. Les lectures deviennent très rapides ; les écritures sont affectées par la vitesse de la lumière, mais elles garantissent une cohérence sur de grandes distances géographiques. Donc, il ne s'agit pas de résolution de conflit, mais d'évitement de conflit. Pouvoir répartir ces écritures de cohérence forte à travers le monde et peut-être avoir plusieurs systèmes d'enregistrement présente un énorme potentiel et une énorme valeur.\n\n## Aerospike Connect pour Spark\n\nPar exemple, Aerospike est actuellement compatible avec Spark, une technologie analytique qui nécessite que ses données soient en mémoire. En chargeant les données depuis Aerospike, le cadre de données Spark peut traiter des dizaines ou des centaines de téraoctets avec des économies de coûts énormes et une grande rapidité. L'intégration avec d'autres outils d'IA est l'une des forces motrices qui ouvrira des perspectives passionnantes.\n\nObtenir des données dans Aerospike est simple et direct. Étant donné que les applications disposent d'une couche d'API comme la plupart des bases de données, Aerospike peut ingérer à partir de sources standard de l'industrie telles que Kafka et Janus. La capacité bidirectionnelle de communiquer avec des bus de messages est logique, car non seulement Aerospike peut ingérer des données à partir de ces sources, mais il peut également les éjecter. Si Aerospike est une source de vérité lorsque vous écrivez un enregistrement, vous pouvez le pousser vers des systèmes en aval.\n\n## Aerospike Connect for Kafka se traduit en français par : Aerospike Connect pour Kafka.\n\nSi vous souhaitez utiliser Aerospike pour l'ingestion plutôt qu'un moteur d'ingestion en amont, Aerospike peut décider s'il veut diviser les données ou les conserver dans le même format et les placer simplement dans Aerospike; Avec son framework de plugins, c'est un appel API. Lorsque les données sont stockées, un bus de messages peut être notifié pour le traitement aval. Cela élimine une des étapes d'ingestion et augmente le débit total.\n\nLa mise à l'échelle pour des milliers de clients est possible grâce à la conception du client Aerospike en tant que multithread, qui permet de s'étendre sur un processus et d'absorber une grande quantité de données. Le client est intelligent dans un souci de rapidité. Tout enregistrement est envoyé directement du client au nœud qui détient cette partie de données. Il n'y a aucun intermédiaire de ce genre. Cela signifie que le client doit connaître tous les nœuds, donc il existe des limites de connexion finies, rendant impraticable des centaines de milliers de clients.\n\n## Exemple de déploiement continental\n\nSi, toutefois, vous avez des centaines de milliers de clients, c'est parce que vous avez une population répartie à l'échelle mondiale. S'ils parlaient à un seul groupe, la vitesse de la lumière deviendrait un gros problème. Il existe des moyens de configurer de plus petits groupes basés aux extrémités qui partagent des informations ou qui parlent à un système central plus en mode d'hub et de rayons, donc c'est faisable.\n\n## Aerospike est une base de données pour les applications du système d'enregistrement\n\nÀ mesure que les données se déplacent dans le monde, différents niveaux de confidentialité entrent en jeu et des normes telles que le RGPD et le CCPA doivent être respectées, de sorte que les clients doivent connaître l'origine des données. Aerospike possède la flexibilité nécessaire pour examiner les données de manière détaillée et, en fonction du contenu des données, savoir quand et où elles peuvent être expédiées. Cela fait partie de la configuration de l'expédition des données d'Aerospike, qui est transparente pour l'application. Vous n'avez pas besoin de modifier quoi que ce soit dans les applications, uniquement dans la configuration.\n\nAerospike s'assure également que les données sont sécurisées. Tout comme toutes les bases de données de niveau entreprise, Aerospike prend en charge le chiffrement au repos et en transit, ainsi que l'intégration de la sécurité à des éléments tels que Hashicorp Vault. Il peut être utilisé et est utilisé comme système d'enregistrement.\n\nUn objectif fondamental chez Aerospike est d'utiliser les dernières innovations et technologies pour le succès des clients. Par exemple, lorsque Intel a lancé PMem, ils l'ont utilisé en mode app direct pour exploiter pleinement ses capacités. Cela signifie que lors d'une mise à jour progressive sur, disons, un pétaoctet d'informations, le temps requis est court. Aerospike stocke ce pétaoctet peut-être dans 50 ou 60 nœuds au lieu de milliers de nœuds, et lorsqu'un nœud est arrêté, il n'a pas besoin d'être reconstruit, donc le temps de redémarrage est très rapide.\n\nDans l'ensemble, Aerospike offre la sécurité, la haute disponibilité, la rapidité, la faible latence, la scalabilité, la cohérence et un faible coût de possession.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Tim Faulkes"],"link":"/episode-EDT29-fr","image":"./episodes/edt-29/fr/thumbnail.png","lang":"fr","summary":"Les bases de données montrent leur âge, mettant encore du temps à obtenir des résultats. Aerospike, avec l'aide de la technologie Intel (Optane), réalise une percée en termes de vitesse, de volume et de faible latence. Tim Faulkes, vice-président de l'architecture des solutions chez Aerospike, rejoint Darren Pulsipher, architecte principal des solutions chez Intel, pour discuter des avantages de la technologie Aerospike."},{"id":98,"type":"Episode","title":"Avantages de l'architecture multi-cloud","tags":["multicloud"],"body":"\r\n\r\n## Agilité\n\nL'agilité est la capacité de se déplacer rapidement pour s'adapter aux conditions changeantes des nouvelles technologies, des nouvelles applications et des nouvelles menaces telles que la sécurité et la concurrence. Un modèle multi-hybride permet de déplacer des éléments entre des nuages privés et publics, voire même de l'infrastructure héritée vers les nuages. Il peut être tentant pour les équipes de développement d'utiliser initialement le nuage public dans le cadre de délais serrés car elles peuvent rapidement mettre en place et désactiver l'infrastructure, mais il peut y avoir des problèmes d'intégration dans les dernières étapes du déploiement des applications dans le produit. Cela peut entraîner des retards coûteux. En utilisant le modèle multi-hybride, bon nombre de ces points d'intégration sont exposés tôt à l'équipe de développement. Le développement d'applications utilise cela pour échouer rapidement et tôt dans le processus de développement.\n\n## Flexibilité\n\nLa flexibilité dans l'infrastructure est un proche parent de l'agilité. Alors que l'agilité est la capacité de se déplacer rapidement, la flexibilité est la capacité de changer. Pour une flexibilité maximale, les développeurs ont besoin de la capacité de déployer une application n'importe où, que ce soit dans un cloud privé ou public, ou sur des infrastructures héritées, sans être contraints par les services ou l'infrastructure d'un seul cloud.\n\nEn déployant une plateforme de gestion infonuagique (CPM) dans une architecture multi-hybride, les charges de travail peuvent être facilement redirigées vers différents nuages selon le coût, la sécurité et la fiabilité.\n\nUn exemple des conséquences du manque de flexibilité est la décision précoce de Netflix d'utiliser uniquement un fournisseur de services cloud. Ce cloud public a connu un problème d'infrastructure qui a entraîné une période d'indisponibilité, et Netflix n'a pas pu diffuser pendant plusieurs heures sur la côte est des États-Unis. Après cette catastrophe, Netflix a mis en place une solution multi-cloud afin de pouvoir rapidement migrer vers un autre cloud en cas de problème. De plus, ils sont maintenant capables de se déplacer là où cela est le plus logique à tout moment en fonction du coût, de la sécurité et de la fiabilité.\n\nLes applications d'un cloud multi-hybride sont la portabilité et la flexibilité opérationnelle. En ne dépendant pas de la façon de fonctionner d'un seul cloud, les développeurs écriront du code qui pourra aisément être transporté entre les différents clouds. De plus, vous aurez la flexibilité opérationnelle nécessaire pour déplacer les charges de travail entre les clouds afin d'offrir à vos clients ce qu'ils souhaitent de manière sécurisée, économique et fiable.\n\n## Performance prédictive\n\nLes clouds publics sont souvent assortis d'un prix : les \"voisins bruyants\", qui peuvent avoir un impact sur les performances prédictives ou la qualité de service (QoS).\n\nLorsque vous utilisez un cloud public, vous ne savez souvent pas ce que font ou qui d'autre utilise la même machine, la même baie de stockage ou le même réseau que vous. Pour certaines charges de travail, cela ne pose pas de problème. Cependant, si vous avez une application pour laquelle vous avez besoin de performances prédictives, la présence de voisins bruyants ou de personnes qui monopolisent la bande passante, les entrées/sorties sur disque, le processeur et autres ressources peut interférer avec les résultats de qualité de service dont vous avez besoin.\n\nLes clouds privés peuvent également souffrir de voisins bruyants, mais étant donné que vous possédez l'infrastructure et que les applications vous appartiennent, vous avez la possibilité directe de les gérer. Un voisin bruyant sur le cloud public, c'est comme vivre dans un immeuble où vous avez des options limitées pour faire face à des fêtards à côté. En revanche, des voisins bruyants sur votre cloud privé, c'est comme vivre dans une maison avec des enfants indisciplinés que vous pouvez immédiatement gérer en limitant les ressources.\n\nUne stratégie de cloud hybride vous donne la possibilité de placer des charges de travail et des applications \"sensibles\" sur des clouds privés et d'autres charges de travail et applications sur une infrastructure de cloud public pour une efficacité en termes de coûts et de ressources. De nombreux outils hybrides vous permettent de caractériser les charges de travail avec des exigences de qualité de service (QoS) pour faciliter le placement automatique et optimal des charges de travail sur différentes infrastructures de cloud.\n\n## Sécurité et conformité\n\nIl y a certains dangers liés à l'automatisation des opérations dans les nuages publics et privés. Cependant, si la sécurité est intégrée à l'architecture multi-hybride, cette automatisation devient un avantage car cela permet d'imposer un profil de sécurité sur l'ensemble de vos ressources dans le cloud, qu'elles soient privées ou publiques, en plus de l'infrastructure héritée. Ce profil de sécurité commun est appliqué partout et les applications sont déployées en fonction de ces profils.\n\nPar exemple, dans un cloud privé, si vous souhaitez obtenir un type spécifique de sécurité, tel que lier une certaine application à une certaine machine qui fonctionne uniquement dans ce cloud privé, cela peut facilement être une exigence dans le profil d'un système multi-hybride.\n\nDans un système multi-hybride, il existe également de nombreux outils performants pour l'audit et la surveillance de votre infrastructure. Vous pouvez non seulement surveiller ce qui se passe dans votre cloud privé, mais aussi dans le cloud public, ce qui vous alertera sur les attaques malveillantes qui pourraient potentiellement infecter votre cloud privé ou vos actifs d'infrastructure héritée.\n\n## L'efficacité\n\nIl y a de nombreuses idées contradictoires sur l'efficacité. Si vous consultez différents calculateurs de coût total de possession (TCO), vous trouverez des réponses différentes quant à savoir si les clouds privés ou publics sont les plus rentables. Une solution multi-hybride de cloud peut résoudre ce dilemme pour votre organisation grâce à une visibilité accrue et une provision dynamique.\n\nAvec une architecture de cloud multi-hybride et un orchestrateur intelligent, votre orchestrateur peut utiliser la télémétrie de vos clouds privés et publics ainsi que de votre infrastructure héritée pour prendre des décisions optimales sur l'emplacement actuel et futur de la charge de travail. Un autre avantage de cette visibilité est que vous pouvez décider si l'exécution continue des applications est efficace. Un exemple de cela est la façon dont nous avons aidé le système du gouvernement canadien à fonctionner plus efficacement en réduisant les coûts liés à l'exécution d'une application dans le cloud public lorsqu'elle n'était pas utilisée. Au lieu de faire fonctionner une application spécifique en continu, ils la font maintenant fonctionner pendant 18 heures, 5 jours par semaine, selon l'utilisation réelle.\n\nCette visibilité vous aidera également à identifier et éliminer les charges de travail et les applications en fin de vie (EOL), ce qui permettra d'économiser de l'argent réel. Pour le cloud privé, cela libère des ressources qui peuvent être utilisées pour d'autres charges de travail, ce qui augmente votre efficacité. De plus, les courtiers cloud dans le CMP de l'architecture multi-hybride chercheront essentiellement le prix le plus bas tout en maintenant la QoS pour la charge de travail spécifique. Cela réduit le coût global de l'exécution de la charge de travail et vous donne également une visibilité sur le coût réel de l'utilisation d'un cloud public ou privé spécifique.\n\n* Visibilité des coûts au sein des infrastructures Cloud et Legacy\n\nFaire fonctionner les charges de travail et les applications avec les coûts les plus bas tout en maintenant les mêmes accords de niveau de service.\n\nStimuler une utilisation accrue de l'infrastructure de cloud privé.\n\n## Appel à l'action\n\nLes architectures multi-hybrides du cloud permettent aux directeurs informatiques de répondre aux exigences de leurs clients, mais il reste encore quelques tâches ardues à accomplir. Élaborer une stratégie de cloud multi-hybride implique des changements organisationnels, comportementaux et techniques qui ne peuvent pas se faire du jour au lendemain. Développer une vision architecturale solide et une feuille de route sont essentiels pour déployer une stratégie de cloud multi-hybride qui puisse tirer parti des points forts des clouds multi-hybrides et éviter l'inefficacité provoquée par l'attrait éphémère des nouvelles technologies dans l'industrie.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT3-fr","image":"./episodes/edt-3/fr/thumbnail.png","lang":"fr","summary":"Une architecture cloud multi-hybride permet aux organisations de tirer parti des avantages des clouds privés et publics, optimisant ainsi les ressources et l'efficacité des coûts. Ce modèle présente cinq principaux avantages : agilité, flexibilité, performances prédictives, sécurité et conformité, et efficacité."},{"id":99,"type":"Episode","title":"Portabilité de l'application avec OneAPI","tags":["aiml","devops","compute","technology","process","oneapi","aiops","developer","people"],"body":"\r\n\r\nAvec oneAPI, Intel a créé un environnement logiciel unifié pour le développement, axé sur le traitement des données. Il s'agit d'une programmation parallèle basée sur un code source C++ open source. De multiples types de bibliothèques, tels que les bibliothèques MKL, DNN d'Intel et d'autres sources ouvertes, font partie de la trousse d'outils oneAPI, ainsi que des accessoires tels qu'un traducteur CUDA. Vous pouvez développer des logiciels avec oneAPI, puis les orienter vers différentes bibliothèques en fonction de vos besoins. Il a éliminé la complexité d'apprendre une langue différente pour différentes architectures.\n\n## Initiative de l'industrie oneAPI - Alternative à une solution unique de fournisseur\n\nFondamentalement, un ingénieur logiciel peut écrire du code une fois et il s'exécutera sur différents processeurs : CPU, GPU, FPGA, NPU et VPU. Selon l'architecture et les bibliothèques que vous utilisez, il pourrait être nécessaire de recompiler, mais aucune réécriture de code n'est requise.\n\n## Bibliothèques d'API puissantes\n\nCette version n'est que la première étape ; Intel et d'autres continueront à concevoir en ajoutant des accélérateurs d'IA, par exemple. L'idée est de faire évoluer cela pour offrir aux développeurs beaucoup plus de flexibilité, et cette abstraction permettra à de nombreuses personnes de pouvoir concevoir et coder plus simplement, notamment dans le domaine de la science des données et de l'IA.\n\nPratiquement parlant, un ingénieur logiciel pourrait écrire quelque chose sur son ordinateur portable, l'essayer là-bas, puis utiliser ce même code et l'exécuter sur un cloud entièrement chargé de processeurs neuronaux, de GPU ou de FPGA. Cela pourrait être particulièrement utile dans le secteur public où les ingénieurs écrivent des applications spéciales qui fonctionnent en périphérie, peut-être avec un FPGA. Ils n'auront pas besoin d'avoir un environnement complet pour effectuer leur travail.\n\nUn autre aspect excitant est que une fois qu'Intel intègre l'apprentissage automatique dans le système, oneAPI pourrait parcourir le code et spécifier quelles parties seraient mieux adaptées aux différents processeurs. Il enverrait le code aux bons endroits pour obtenir la meilleure vitesse et performance.\n\n## Initiative de l'industrie oneAPI - Alternative à une solution d'un seul fournisseur\n\nIl existe de nombreux cadres d'IA, mais oneAPI permet de migrer tout type de code d'une solution propriétaire vers un langage de programmation open-source. Il est basé sur SYCL et développé au sein d'un consortium industriel appelé Khronos group, c'est donc un type de cadre de développement.\n\n## Des bibliothèques d'API puissantes\n\nOneAPI dispose de vingt à trente bibliothèques telles que MKL, des bibliothèques pour les réseaux neuronaux et l'apprentissage automatique, open CNN ou DNN. Toutes sont ouvertes et font partie du consortium plus large.\n\nDe plus, Intel est en train d'intégrer le framework tensorflow et ses bibliothèques dans oneAPI. De nombreux frameworks ont déjà été optimisés par Intel et ils sont incorporés ou utilisent les mêmes bibliothèques, de sorte que les utilisateurs peuvent les utiliser ou les développer davantage.\n\n## Ressources\n\nIntel a rendu oneAPI généralement disponible au début de novembre 2020, et cela a été l'un des temps forts de la convention virtuelle Super Computing du 17 au 19 novembre.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT30-fr","image":"./episodes/edt-30/fr/thumbnail.png","lang":"fr","summary":"Avec oneAPI, Intel a créé un environnement logiciel unifié destiné au développement, axé sur le traitement des données. Gretchen Stewart, scientifique en chef des données du secteur public chez Intel, discute de cette technologie avec Darren Pulsipher, architecte principal des solutions chez Intel, qui élimine le besoin d'utiliser un langage différent pour différentes architectures."},{"id":100,"type":"Episode","title":"Améliorer la valeur des employés grâce à l'accélération.","tags":["rpa","automation","catalytic","compute","process","technology"],"body":"\r\n\r\n## Catalytic est spécifiquement conçu pour les opérations et les processus commerciaux récurrents.\n\nLa technologie RPA trouve ses origines dans l'automatisation des tests et les macros Excel, mais son évolution dans le domaine des affaires provient de l'externalisation. Cependant, Catalytic considère son héritage comme un logiciel, avec son système comme la prochaine évolution pour la création de logiciels personnalisés. L'intérêt du co-fondateur et PDG, Sean Chou, se concentre sur les opérations et l'efficacité des services administratifs. Cette focalisation étroite permet à Catalytic d'utiliser pleinement l'IA et de développer quelque chose d'accessible sans code.\n\nCatalytic utilise l'IA de différentes manières tactiques dans l'application pour aider à réduire le besoin d'expertise.\n\nPar exemple, l'une des parties les plus difficiles de l'apprentissage de n'importe quelle plateforme est de comprendre ses capacités. Une façon d'utiliser l'IA pour aider à compenser ce problème est d'utiliser le traitement du langage naturel. Les utilisateurs peuvent simplement taper ce qu'ils essaient de réaliser et l'application peut déduire et indiquer quelle action serait la meilleure pour les aider à accomplir cette étape. C'est essentiellement une manière d'améliorer l'expérience produit et de réduire le niveau d'expertise nécessaire.\n\nUn autre exemple est celui que dans le flux de travail, il y a six modules différents d'actions différentes, chacun nommé de manière logique selon ce qu'il est censé résoudre, comme le traitement des données, l'assemblage de documents, etc. Dans chaque module, il y a différentes actions d'IA que les gens peuvent utiliser dans leur processus, comme la reconnaissance optique de caractères (OCR), l'analyse des sentiments (la personne qui a écrit l'e-mail est-elle en colère ?) et d'autres actions de traitement du langage naturel.\n\nIl existe d'autres éléments qui sont moins liés à l'IA et plus axés sur des actions utilitaires, telles que fournir des informations sur une personne en fonction de son adresse e-mail.\n\nBeaucoup de données circulent à travers les processus du back-office. Malheureusement, la vision système des données est souvent très fragmentée. Le flux de travail Catalytic peut traverser les systèmes et capturer tout. Les données sont stockées dans des tables de données, et il est facile de construire des modèles d'apprentissage automatique basés sur ces tables, de sorte que vous pouvez prédire les résultats futurs d'autres flux de travail.\n\nCatalytique utilise l'IA dans ces domaines spécifiques ainsi que dans d'autres contextes pour favoriser l'efficacité et offrir des connaissances et une visibilité accrues.\n\n## Finalisez le dernier kilomètre de la numérisation en construisant des solutions de flux de travail autour de la façon dont les gens travaillent.\n\nPar exemple, l'intelligence opérationnelle grâce à l'IA est capturée dans la plateforme par le biais d'une fonctionnalité appelée \"Insights\". Pour chaque flux de travail, cet onglet peut vous indiquer combien de temps il faut pour effectuer toutes les étapes différentes, à quelle fréquence elles échouent, et s'il s'agit d'une erreur humaine ou d'un échec d'intégration. Il vous offre des opportunités d'amélioration. Parallèlement, cet algorithme d'apprentissage automatique peut commencer à apprendre à prédire les résultats des valeurs sur le terrain. Ainsi, l'apprentissage automatique peut être utilisé de multiples façons liées.\n\n## Malgré les Investissements à répétition, il existe toujours une « dernière ligne droite » qui est comblée manuellement.\n\nLa plupart des clients qui ont investi dans plusieurs technologies au fil des ans ont encore l'impression qu'il existe un écart entre l'investissement et les résultats commerciaux qu'ils souhaitent. Cet écart est généralement comblé par des personnes qui effectuent des tâches que le logiciel devrait normalement faire, telles que copier-coller quelque chose d'un système à un autre. Parfois, l'ajout d'un logiciel complique les choses en ajoutant des étapes supplémentaires et nécessitant une formation. La solution Catalytic à ces inefficacités est un système réutilisable et évolutif.\n\nIl est impossible pour n'importe quel logiciel prêt à l'emploi de répondre aux besoins individuels de chaque entreprise, et il est inefficace et chronophage de constater la valeur ultime. Les entreprises doivent donc s'adapter au logiciel tel qu'il est ou adapter le logiciel à leurs besoins. La valeur dans une histoire de type WordPress est évidente. Si le pouvoir est réellement entre les mains des employés, il est possible d'avoir un logiciel véritablement personnalisé. Catalytic souhaite mettre en place ce pouvoir pour tout processus commercial en utilisant des RPA combinées à des actions.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sean Chou","Neil Bahandur"],"link":"/episode-EDT31-fr","image":"./episodes/edt-31/fr/thumbnail.png","lang":"fr","summary":"Dans la première partie de ces deux épisodes, Sean Chou, PDG de Catalytic, et Neil Bahandur, responsable des partenariats chez Catalytic, se joignent à Darren pour discuter de la technologie de Catalytic et de la manière dont les RPAs peuvent aider les employés à devenir plus précieux grâce à l'automatisation des processus répétables du back-office."},{"id":101,"type":"Episode","title":"Approche sans code (RPA) pour l'efficacité des services administratifs","tags":["rpa","automation","catalytic","compute","process","technology"],"body":"\r\n\r\n## Qu'est-ce que le catalytique ?\n\nCatalytic est une plateforme cloud sans code pour créer des solutions de flux de travail qui améliorent et automatisent vos opérations de back-office.\n\nUne analogie utile pour aider les gens à comprendre ce que Catalytic peut faire pour le flux de travail du back-office est ce que Wordpress a fait pour la publication web. Avant Wordpress, la création d'un site web était une tâche complexe qui impliquait plusieurs personnes aux compétences spécialisées variées. Avec Wordpress et les logiciels avancés ultérieurs, créer un site web est une tâche relativement simple. Par exemple, un designer ou un expert en contenu peut facilement créer son propre site web.\n\nL'objectif de Catalytic est de simplifier de manière similaire les systèmes complexes et encombrants du back-office et de créer efficacité et solutions grâce à l'automatisation des processus robotiques (RPA). Quelqu'un ayant une certaine compétence technique devrait maintenant être en mesure, avec la technologie de Catalytic, de créer indépendamment une solution pour le back-office.\n\nUne peur courante selon laquelle les RPAs coûtent des emplois est fausse ; en réalité, cela crée de nouvelles opportunités plus valorisantes. Par exemple, avec Wordpress, il existe désormais le rôle de développeur Wordpress, de sorte que cette évolution n'a pas réduit les compétences des développeurs, mais a ajouté des possibilités pour les personnes d'exploiter davantage ces compétences dans quelque chose de plus extensible. De même, les employés du back office peuvent utiliser leurs compétences pour ajouter de la valeur ou améliorer l'expérience client plutôt que de se concentrer sur des tâches banales inhérentes aux processus inefficaces.\n\n## L'évolution et la différenciation de Catalytic\n\nCatalytic se distingue de ses concurrents, tout d'abord, par ses origines. L'industrie actuelle de l'Automatisation des Processus Robotisés (APR) est issue de deux lignées évolutives, l'une provenant des macros Excel et l'autre des tests d'interface utilisateur (IU). Catalytic provient du monde DevOps, en automatisant les processus en arrière-plan plutôt que de se concentrer sur l'automatisation orientée écran. Lorsque Sean Chou a co-fondé Catalytic, il a pensé en termes de gestion de projet et à comment orchestrer les processus habituels de l'entreprise.\n\n## Catalytic est conçu pour les opérations et les processus commerciaux récurrents.\n\nChou s'est rendu compte que puisque 85% ou plus des activités commerciales sont habituelles, de nombreuses opérations pourraient être automatisées, de sorte que la plateforme pourrait accomplir certaines de ces tâches. Tout a commencé avec un système de notification automatisé simple qui remplaçait les employés envoyant des e-mails, ce qui a ensuite propulsé Catalytic vers la création de plus en plus d'actions. Pour amplifier les capacités de la plateforme, Catalytic a créé un écosystème avec des partenaires tiers tels que Google pour exploiter leurs technologies. Grâce à cet écosystème, il y a aussi l'avantage du travail professionnel composable, où les personnes peuvent créer un flux de travail sur la plateforme et le sauvegarder, essentiellement, comme une unité de travail individuelle. Par exemple, s'il y a six étapes standardisées pour capturer un dossier client, vous pouvez le créer une fois et le sauvegarder comme une action sur la plateforme, puis le partager avec toute votre équipe. Ce partage imite l'approche d'un développeur de logiciels pour résoudre des problèmes difficiles en partageant du code, mais au lieu du code, ce sont des actions. La composition devient ainsi beaucoup plus facile.\n\nCatalytic a adopté une approche réfléchie pour la fourniture, le déploiement et la gestion de tout sur leur plateforme. Le cloud n'est pas une réflexion tardive ; ils ont créé une plateforme cloud à partir de zéro. Tout est centré autour du cloud, bien qu'ils aient également la capacité de travailler avec des systèmes sur site et des environnements hybrides.\n\n## Catalytique est conçu pour permettre un programme de développeur citoyen de classe Entreprise.\n\nLe système a été créé dans un environnement DevOps et fonctionne comme un gestionnaire de construction pour l'entreprise, où les professionnels de l'entreprise peuvent réellement le faire eux-mêmes. Les anciens systèmes de construction comme Clear Case étaient difficiles à gérer, alors que Catalytic a simplifié les choses. Il ne se contente pas d'éliminer des étapes supplémentaires, mais modifie également la répartition du travail. Catalytic cible les personnes les plus proches du processus réel pour éviter les transferts. L'efficacité est le concept central. Un système de construction efficace qui détecterait les bugs et les rejetterait avant de les transférer est essentiel à la réussite du processus automatisé.\n\nLa magie des systèmes de création réside dans leur capacité à devenir plus puissants lorsqu'ils intègrent davantage de parties de votre infrastructure. Par exemple, ils peuvent interagir avec votre système de gestion de configuration pour récupérer le code, travailler avec vos serveurs pour le déploiement, ou interagir avec Rational Robot pour effectuer des tests automatisés. Bien sûr, tout cela tourne autour des actions. Plus les actions sont numérisées, plus le système peut les capturer, et plus il devient puissant.\n\nUne autre chose qui différencie Catalytic est l'accent mis sur les employés réalisant un travail à haute valeur ajoutée. Le concept de faible et haute valeur ajoutée dans la RPA est courant, mais au lieu de l'utiliser comme une épée pour diviser les groupes, Catalytic souhaite l'utiliser comme un bouclier pour protéger les personnes et les aider à faire le meilleur usage de leur temps et offrir un avantage compétitif.\n\nDans le prochain podcast, Darren, Sean et Neil continueront la conversation sur l'approche sans code pour l'efficacité du back office.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sean Chou","Neil Bahandur"],"link":"/episode-EDT32-fr","image":"./episodes/edt-32/fr/thumbnail.png","lang":"fr","summary":"L'épisode d'aujourd'hui est la partie 2 d'une conversation avec Sean Chou, PDG de Catalytic, et Neil Bahadur, responsable des partenariats de Catalytic. Ils discutent avec Darren de leur approche sans code pour l'efficacité des services administratifs avec une plateforme qui utilise la technologie RPA et IA."},{"id":102,"type":"Episode","title":"La panne d'AWS de novembre 2020","tags":["aws","cloudoutage","compute","csp","cloud","process","cloudreliability","multicloud","reliability"],"body":"\r\n\r\n## Ce que nous avons appris de la panne d'AWS.\n\nAmazon Web Services (AWS) a subi une panne importante au moment le moins opportun - la veille de Thanksgiving en 2020. Comme ils ont publié leurs journaux de service, il est intéressant de les examiner pour voir ce qui s'est passé et ce que nous pouvons apprendre.\n\n## Chronologie de la panne AWS\n\nLe mercredi 25 novembre 2020, vers 3h00 du matin (heure du Pacifique), AWS procédait à une mise à jour de la région Est en ajoutant des serveurs pour augmenter la capacité de Kinesis. Kinesis est un service d'IA populaire qui effectue des correspondances de motifs sur les fichiers journaux et les fichiers vidéo. Environ une heure et demie après le début de la mise à niveau, les alarmes serveur ont commencé à signaler des erreurs dans les enregistrements Kinesis. Assez rapidement, vers 8 heures du matin, ils ont initialement identifié quelques candidats parmi les services frontaux comme étant la cause principale. Kinesis possède plusieurs services plus petits, un groupe de services frontaux et un groupe de services back-end. Conformément à l'architecture du système, chaque fois qu'un service back-end est en cours d'exécution, un thread est lancé depuis le front-end. Un des problèmes était que le front-end atteignait le seuil du nombre de threads alors qu'ils ajoutaient des serveurs au back-end pendant la mise à niveau. Une fois ce problème identifié, ils ont effectué une correction temporaire avec des mises à jour (correctifs) du système d'exploitation et Kinesis était à nouveau en ligne vers 22h30 et entièrement rétabli vers 1h15 le 26 novembre. Au total, Kinesis n'était pas pleinement opérationnel pendant 21 heures, et bien que tout le monde n'utilise pas cet outil d'IA, l'impact a été étendu.\n\n## Kinesis Impact translates to \"Impact Kinesis\" in French.\n\nPlusieurs autres services d'Amazon utilisent Kinesis, tels que Amazon Cognito et CloudWatch, et ont connu divers degrés de perturbation. Cognito a été surchargé jusqu'à environ 14 heures ; CloudWatch a été hors service jusqu'à environ 22 heures. Dans un effet domino, les services dépendants de CloudWatch, tels que Lambda et EventBridge, étaient également hors service. Étant donné que EventBridge n'était pas disponible, les services de conteneurs LCS et LKS ont également été affectés.\n\nLes pannes ne se sont produites que dans la région Est et AWS a rapidement ajouté de la capacité dans les autres régions pour éviter une défaillance similaire de Kinesis. Mais pendant la panne, les utilisateurs de la région Est ont rencontré un problème déconcertant, car leur tableau de bord de service et leurs tableaux de bord personnels ne recevaient pas d'informations et affichaient des faux positifs. Ainsi, de nombreuses autres organisations informatiques ont enquêté sur les problèmes car elles ne recevaient pas les bonnes informations. Étonnamment, AWS a été ouvert sur toute l'incident, donc c'est une excellente expérience d'apprentissage.\n\n## Leçons Apprises\n\nL'une des premières leçons est que les opérations simples sur l'infrastructure, telles que l'augmentation de capacité, doivent être comprises et planifiées. Évidemment, AWS n'a pas simplement effectué ces mises à niveau sans réfléchir, mais ils n'ont pas complètement compris l'impact. Même si une opération semble routinière, il est toujours conseillé de laisser les choses échouer dans un environnement de test lors des mises à niveau, même si cela prend plus de temps. Cela est particulièrement important pour les services essentiels aux autres services principaux dépendants.\n\nDe plus, les architectures de service ou de micro-service doivent comprendre leur arborescence de dépendance complète afin de pouvoir résoudre les problèmes lorsqu'il y a des pannes. De plus, il est important d'être aussi peu couplé que possible à une dépendance et d'inclure une programmation défensive avec les microservices afin d'éviter l'effet domino qui s'est produit dans ce cas.\n\nUn autre problème à surveiller est les dépendances circulaires. Si, dans une chaîne de microservices qui dépendent les uns des autres, l'un d'eux rencontre un problème, tout peut s'arrêter brusquement, consommant des ressources et ne réalisant rien.\n\nLors du développement de microservices, gardez à l'esprit que vous n'aurez pas toujours la connectivité aux services dont vous dépendez. Écrire des programmes qui peuvent fonctionner dans un état dégradé, ou du moins indiquer qu'un service ne fonctionne pas, peut faire gagner du temps et des problèmes. Lors de cette panne, rappelez-vous que les tableaux de bord étaient en cours d'exécution en vert même si aucune nouvelle information n'arrivait.\n\n## Qui a géré le mieux la panne ?\n\nDe nombreuses sociétés ont été affectées par la panne d'AWS, y compris certaines appartenant à Amazon elle-même. Certaines ont traversé la panne sans difficulté, presque indemnes, tandis que d'autres ont eu plus de mal à récupérer. Ceux qui ont pu s'adapter rapidement avaient une stratégie de cloud hybride multiple, donc ils avaient des clouds alternatifs en tant que sauvegarde. Certains ont utilisé une région différente d'AWS, tandis que d'autres ont utilisé Google ou Azure, et certains sont même revenus à leurs propres data centers ou sites web externes. Au minimum, les sites web affichaient un message indiquant qu'ils rencontraient actuellement des problèmes, plutôt qu'une erreur 404.\n\nLes entreprises qui n'utilisaient pas certains des services spécialisés d'Amazon se sont également mieux débrouillées. Par exemple, les offres de conteneurs gérés EKS et ECS ainsi que Lambda ont été durement touchées et ont été indisponibles pendant une période importante, ce qui signifie que ceux qui dépendaient de ces services étaient sans options.\n\nPendant la panne, les services d'auto-scaling ne fonctionnaient pas correctement, donc toute entreprise qui avait beaucoup de trafic à ce moment-là devait découvrir ce qui se passait et augmenter manuellement les services. C'était un problème plus important que d'habitude, car c'était la veille de Thanksgiving, un moment où de nombreux consommateurs voyagent et achètent en ligne. Par exemple, Etsy est resté en ligne, mais ils ne pouvaient pas se développer autant qu'ils le feraient normalement, ce qui a entraîné une baisse des ventes.\n\nLes entreprises qui ont leur propre surveillance externe se sont également mieux débrouillées. Certaines ont même alerté AWS des pannes. Elles ne dépendaient pas uniquement du tableau de bord de santé d'AWS, mais disposaient de leur propre surveillance en cours sur leurs serveurs.\n\nQuelle est la principale leçon que nous devrions tirer de cela ? Les organisations devraient prendre en charge leurs ressources cloud, tout comme n'importe quel service public. Tout comme avoir un générateur de secours pour l'électricité afin d'assurer la continuité des activités en cas de panne, les entreprises devraient appliquer les mêmes meilleures pratiques pour les services cloud. Cela signifie qu'avoir un cloud de secours qui peut vous permettre de continuer à fonctionner, même avec une capacité réduite, est essentiel pour faire face à une tempête telle que la panne d'AWS.\n\n## Ressources\n\nVoici la traduction en français : * https://aws.amazon.com/message/11201/\n\nhttps://www.theverge.com/2020/11/25/21719396/amazon-web-services-aws-outage-down-internet\n\nVeuillez traduire ce qui suit en français : *\n\nVoici ce qui a causé une importante panne du service AWS d'Amazon la semaine dernière. Nous présentons nos excuses.\n\nAmazon Web Services touché par une panne\n\nVoici la traduction en français : \n\n* https://www.washingtonpost.com/technology/2020/11/28/amazon-outage-explained\nTraduction : Explication de la panne d'Amazon\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT33-fr","image":"./episodes/edt-33/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions, secteur public, chez Intel, parle des enseignements tirés de la panne d'AWS en novembre 2020 et des solutions préventives pour naviguer lors de telles pannes."},{"id":103,"type":"Episode","title":"Adoptant l'évolution de l'espace de travail","tags":["covid","mfa","remoteworker","cybersecurity","people","technology","vdi","vpn"],"body":"\r\n\r\n## Les espaces de travail numériques évoluent.\n\nÉtant donné que nous avons dépassé le chaos initial des transitions rendues nécessaires par la pandémie, nous devons nous demander ce qui se passe ensuite et, à partir de là, comment utiliser ce que nous avons appris pour investir consciemment là où nous voulons être.\n\nUne leçon importante a été que les organisations qui étaient déjà agiles s'en sont bien sorties. Elles ont réussi à mettre en place rapidement leur personnel à distance. Nous ne savons pas exactement à quoi ressemblera la nouvelle norme, mais nous savons qu'elle conservera de nombreux aspects qui ont été accélérés au cours des huit derniers mois, tels que la possibilité de travailler à distance, mais avec une collaboration plus étroite. Intel souhaite atteindre de nouveaux clients qui ont besoin de plus de ressources pour fonctionner en dehors de leurs propres murs et devenir plus agiles.\n\n## Votre lieu de travail a besoin de nouvelles capacités. Intel comble le fossé.\n\nIntel s'engage à créer des solutions en aidant les clients à comprendre le vaste écosystème Intel avec différents modèles pouvant combler les lacunes en termes de performances, de stabilité, d'efficacité et de prix. Intel peut partager les connaissances acquises auprès de milliers de clients pour aider à résoudre les problèmes et fournir des capacités que les organisations informatiques ne peuvent parfois pas réaliser seules.\n\n## Les exigences commerciales sont primordiales.\n\nDans le passé, un directeur des systèmes d'information (DSI) pouvait se concentrer uniquement sur trois aspects : la sécurité, la résilience et l'efficacité. Ce n'est plus le cas aujourd'hui. Ce qui était autrefois primordial pour maintenir les opérations et éviter les problèmes tout en fonctionnant de manière efficace est maintenant bien plus complexe. Les entreprises doivent adopter une approche proactive pour améliorer le personnel, le lieu de travail et les transformations qu'elles vivent afin de répondre aux obligations envers les clients et les fournisseurs.\n\nDe plus, l'agilité est une exigence car les circonstances peuvent changer rapidement et les entreprises doivent s'adapter dans de multiples directions tout en répondant à ces besoins.\n\nEnfin, une automatisation accrue via l'intelligence artificielle est un facilitateur pour des domaines tels que une meilleure collaboration et l'amélioration de l'expérience des travailleurs. Avec une main-d'œuvre distribuée, par exemple, il n'y a pas de service d'assistance avec une personne qui peut venir vous aider avec votre problème. Cela pourrait de toute façon ne pas être le scénario le plus efficace, donc peut-être que des chatbots ou l'apprentissage automatique dans un système de gestion de la relation client (CRM) seraient plus efficaces et permettraient de partager plus d'informations. Ce type d'automatisation technique avec un dépannage courant pourrait créer davantage de connexions, d'aperçus et de productivité à la fin.\n\n## Votre lieu de travail évolue.\n\nLes exigences et les stratégies commerciales évoluent, en particulier en ce qui concerne l'évolution des lieux de travail. Certaines organisations continueront à fonctionner entièrement à distance, d'autres voudront que tout le monde revienne au bureau lorsque cela sera sécuritaire, et il y aura toutes sortes de configurations intermédiaires. Quelle que soit la situation, les organisations doivent adopter une stratégie pour s'assurer qu'elles peuvent joindre leurs employés en tout lieu, à tout moment et sur n'importe quel appareil, que ce soit par nécessité ou par choix.\n\nL'amélioration des processus est également essentielle. Vous ne voulez pas continuer à ajouter des processus en plus de ceux dont vous n'avez plus besoin. Évaluer les domaines nécessitant des améliorations, que ce soit l'infrastructure interne ou les capacités de tiers, ajoutera de l'efficacité et de la valeur. Au lieu de construire une énorme infrastructure qui nécessite une gestion et qui est alourdie par l'utilisation exclusive de ses propres outils, les organisations devraient évaluer l'incroyable quantité d'opportunités écosystémiques qui se branchent sur des outils en tant que service. Externaliser les services qui ne sont pas les atouts stratégiques clés ou les forces de votre entreprise pourrait être plus judicieux.\n\nUn autre domaine à évaluer est la gestion des données. Par exemple, avec toutes les données présentes dans les outils de collaboration, la dispersion des données devient un problème. Une stratégie claire et efficace est nécessaire.\n\nAvec la gestion des données vient la sécurité. Les données sont maintenant dispersées partout, donc les organisations doivent adopter, évaluer et déployer de bons outils de sécurité ainsi que de bonnes méthodes de travail concernant les pratiques de données.\n\n## Intel livre\n\nIl existe six principales capacités dans l'environnement de travail en évolution où Intel peut apporter son aide : accès aux applications et aux données, gestion, sécurité renforcée, connectivité, infrastructure de collaboration et atmosphères multi-cloud. Bien qu'Intel ne produise que du silicium, les capacités, l'évolutivité et la sécurité qu'il offre répondent aux besoins des organisations, des plus petites aux plus grandes.\n\nIntel cherche à exister dans plusieurs environnements et à offrir la gestion de ces ressources. Bien que les clients n'achètent pas directement des actifs tels que des processeurs, des accélérateurs, de la mémoire et du stockage à Intel, ils tireront parti de l'écosystème solide et éprouvé de produits tels que les fabricants de matériel, les OEM, les fournisseurs de logiciels, les intégrateurs de systèmes et les fournisseurs de services cloud que Intel rend possibles.\n\nL'un des atouts d'Intel est son soutien à cet écosystème. Par exemple, Intel compte 15 000 ingénieurs logiciels qui développent du code, mais rien n'est vendu. Au lieu de cela, ils contribuent au développement de l'écosystème en proposant de nouvelles solutions basées sur le silicium. Le silicium est le mécanisme permettant de fournir des solutions pour aider les gens à résoudre de réels problèmes. Une bonne métaphore serait que la distance entre le quai de chargement d'Intel en tant que fabricant et le quai de chargement final du client est trop importante pour qu'Intel puisse conduire un camion lui-même. C'est l'écosystème qui comble cet écart.\n\n## Domaines de solutions que vous pourriez être en train d'évaluer.\n\nComment les clients peuvent-ils tirer le meilleur parti de la capacité de l'écosystème ? Intel peut donner des recommandations dans les six domaines clés, que vous ayez des difficultés avec une solution VDI telle que décider si elle doit rester sur site ou être dans un bureau virtuel ou un service RDS, ou si vous cherchez à élaborer une stratégie VPN qui assurera la connectivité.\n\nUne partie de la solution réside dans la compréhension que, étant donné qu'Intel œuvre dans un écosystème si vaste, ils peuvent aider à relever de nombreux défis. Intel ne va pas nécessairement vous vendre un processeur, par exemple, mais vous aider à mettre votre charge de travail sur la meilleure puce de silicium possible, que ce soit chez des fournisseurs de services cloud ou sur plusieurs clouds, à l'intérieur et à l'extérieur de votre propre centre de données. L'objectif d'Intel est de vous offrir efficacité, portabilité et agilité dans ces processus.\n\n## Adoptez l'évolution du lieu de travail : quand ou où voulez-vous commencer ?\n\nIntel a à cœur l'intérêt de votre organisation car si vous réussissez dans votre entreprise, vous trouverez de nouvelles façons d'utiliser les données et de nouvelles façons d'utiliser l'infrastructure pour offrir davantage de valeur à vos clients, et au final, consommer davantage. Vos stratégies, votre croissance et votre développement vous appartiennent, mais Intel va vous donner les bases pour faire de bons choix.\n\nCherchez des occasions de vous impliquer avec votre représentant d'Intel. Ils disposent d'une multitude de ressources pour vous aider à exploiter le vaste réseau de partenaires qui peuvent répondre à vos problèmes et à vos objectifs.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Robert Looney"],"link":"/episode-EDT34-fr","image":"./episodes/edt-34/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren Pulsipher, architecte en chef des solutions pour le secteur public, et Robert Looney, directeur des ventes de centres de données pour les Amériques chez Intel, parlent de l'utilisation d'une approche stratégique pour adopter l'évolution actuelle du lieu de travail. La pandémie de COVID a créé d'importants défis et transitions dans le monde du travail. Intel aide les clients à tirer parti des technologies pour faire face au mieux aux défis continus de la « nouvelle normale »."},{"id":104,"type":"Episode","title":"Naviguer dans les changements perturbateurs.","tags":["change","culturalchange","organizationalchange","disruption","people","process"],"body":"\r\n\r\nRick a récemment célébré sa troisième décennie chez Intel, et pendant ce temps, il a aidé Intel à naviguer à travers de nombreuses transformations et événements majeurs. Outre les situations concurrentielles difficiles et les changements de l'industrie qui ont commencé avec l'avènement d'Internet et le boom et l'effondrement des dotcoms, il y a eu des événements externes tels que le 11 septembre, la Grande Récession et maintenant la pandémie de COVID-19.\n\n## Types de situations de crise\n\nLa nature d'une entreprise moderne dans une économie moderne est qu'elle naviguera constamment dans un degré élevé d'incertitude, de troubles, de changements et de perturbations. Les organisations se fanent à ces moments-là ou en sortent meilleures et plus fortes.\n\nChaque perturbation est différente en termes d'ampleur et de vitesse. Certains événements se déroulent sur une longue période, peut-être un changement technologique ou structurel dans l'industrie, pour ensuite s'accélérer soudainement. Certains, comme la COVID-19, ont un impact élevé à une vitesse sans précédent. Ce qui est intéressant à propos de la COVID-19, cependant, c'est que nous avons développé la technologie pour faire face aux conséquences de la pandémie depuis une décennie, mais il a fallu cet événement pour la mettre en pratique. Un bon exemple est la télémédecine. Son utilisation soudaine et généralisée a également modifié l'environnement politique, et le paysage de la santé ne sera plus jamais le même.\n\nCe type de changement fondamental de politique se produit lors d'événements d'une grande rapidité tels que la Grande Récession ou le 11 septembre. Des choses similaires se produiront après la COVID. Ces événements, aussi difficiles soient-ils, offrent aux organisations l'opportunité de faire un bond en avant dans leur performance et leur utilisation de la technologie.\n\n## Naviguer dans le changement perturbateur.\n\nIntel a toujours su faire face aux grands défis et aux perturbations. Les éléments fondamentaux de la culture sont la préparation, l'objectif commun et la confiance. Ils peuvent exister lorsque les employés ont un sentiment de sécurité psychologique. Par exemple, Darren s'est senti habilité lorsque le PDG d'Intel a déclaré que personne ne serait licencié à cause du COVID. Cela lui a permis de prendre des risques pour faire face à la crise sans avoir peur de perdre son emploi. Et bien que le PDG et les cadres supérieurs donnent le ton, la plupart du travail concernant la sécurité psychologique est effectué par les managers de première ligne. Cette sécurité et autonomie conduisent à une préparation intégrée. Avec l'objectif commun et la confiance, ce sont les éléments fondamentaux non seulement d'une organisation prête à répondre à une crise, mais aussi les caractéristiques d'une organisation performante.\n\nUne organisation performante disposera également des outils nécessaires pour naviguer dans les perturbations qui sont davantage un processus lent plutôt qu'un événement rapide et à fort impact. Avec des événements tels que COVID ou la Grande Récession, il y a très peu de débat sur ce qui se passe, et tout le monde est conscient de l'ampleur des problèmes. Si vous comparez cela à, disons, un changement fondamental dans les affaires, un changement architectural ou une technologie se manifestant sur la courbe de l'engouement et dont vous n'êtes pas sûr qu'elle ait un impact sur l'entreprise, il y aura plus d'incertitude et de débat concernant les ajustements nécessaires.\n\nComment une organisation survit-elle à ces points d'inflexion ? La télémétrie, ou l'entrée que vous évaluez, est importante. Une des complexités dans une grande organisation est que lorsque ces entrées parviennent à un décideur principal, elles ont peut-être subi trois niveaux de manipulation et de positionnement, ce qui peut être dangereux. La vérité et la transparence sont des valeurs chez Intel. Dans une entreprise où existe un haut degré de sécurité psychologique, les employés peuvent dire la vérité sur les problèmes.\n\nLa contribution la plus importante est d'écouter vos clients car ils auront tendance à vous guider dans la bonne direction. Par exemple, si quelqu'un veut connaître des informations sur un compte, Rick fera souvent intervenir le responsable du compte pour obtenir des informations directement issues du terrain. Il est également judicieux, en tant que leader, d'aller directement vers les experts au lieu de passer par trois niveaux de filtrage et d'adaptation, surtout en cas de crise. Répondre à l'instant présent se résume à une culture de préparation, à la bonne télémétrie et à la prise de décision.\n\nLa prise de décision peut devenir complexe dans une grande organisation. Une solution simple est que chaque personne qui entre dans une réunion devrait se demander : sommes-nous là pour prendre une décision ? Qui est le décideur ? Ou bien, sommes-nous simplement en train de débattre ou de préparer les télémesures et les données pour un décideur ? Cela relève simplement de l'hygiène organisationnelle.\n\nAndy Grove a déclaré : \"Laissez un peu de chaos régner, puis maîtrisez le chaos.\" Pour les décisions cruciales, les évolutions lentes, il faut parfois laisser l'innovation respirer et infuser un peu, tout en veillant à ce que les choses ne dérapent pas. Avoir de bons processus et des garde-fous en place aide à y parvenir.\n\nDans les moments difficiles, les décideurs doivent avoir une compréhension approfondie que chaque individu se trouve dans un espace différent, et ils doivent réfléchir à l'impact des décisions. La sécurité psychologique est si importante, et les responsables de première et deuxième ligne sont essentiels à la capacité d'une organisation à bien exécuter en période de perturbation. Le leadership senior est crucial pour donner le ton, mais ces responsables assument le travail.\n\nDe prévisibles changements technologiques futurs tels que l'impact de l'IA sur la 5G, à des problèmes comme le changement climatique, en passant par des événements mondiaux imprévus, la seule constante est que nous devrons toujours naviguer au milieu de la perturbation, des crises et du changement. L'une des caractéristiques de la culture d'Intel est sa capacité à réagir, à s'adapter et à être résiliente face à ces événements.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rick Herrmann"],"link":"/episode-EDT35-fr","image":"./episodes/edt-35/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren Pulsipher, architecte principal des solutions pour le secteur public, et Rick Hermann, directeur du secteur public américain chez Intel, discutent de la façon dont Intel a réussi à naviguer avec succès dans les changements perturbateurs au cours des trois dernières décennies."},{"id":105,"type":"Episode","title":"Six piliers de la cybersécurité","tags":["cybersecurity","prevention","threatdetection","zerotrustarchitecture","zta","technology"],"body":"\r\n\r\nAvec la hausse des cyber-attaques dans toutes les industries, la sécurité est plus importante que jamais. Dans cet épisode, Darren et Steve Orrin, CTO fédéral chez Intel, décrivent les vecteurs d'attaque, les six piliers de la cybersécurité et comment Intel peut aider.\n\nSteve a vu l'évolution de la sécurité au cours des 25 années qu'il a passées à travailler dans ce domaine, à la fois en tant que science et en tant qu'art. En même temps, le niveau de complexité avec lequel les organisations doivent composer pour sécuriser leurs données, systèmes et applications n'a jamais été aussi difficile.\n\n## Les violations à grande échelle et les attaques cybernétiques continuent.\n\nLes violations massives de données et les intrusions profondes se produisent à tous les niveaux, que ce soit sur les plateformes de médias sociaux, les services financiers ou les soins de santé. Aucun type de données n'est à l'abri d'être ciblé par des techniques de plus en plus sophistiquées.\n\nQu'est-ce qui motive ces piratages ? Une réponse est que de nos jours, une attaque à grande échelle ou profonde nécessite moins de ressources et moins d'investissement financier. L'étendue et l'ampleur de ce qu'un pirate informatique peut réaliser avec un faible investissement donnent un avantage aux adversaires dans un système complexe.\n\n## Trois forces impactant la sécurité de l'entreprise et de la mission.\n\nUne autre réponse à ce qui motive les attaques est que les données sont une ressource précieuse : le nouvel or noir. Les données sont vulnérables à mesure que l'expansion de la surface d'attaque continue de croître.\n\nIl y a davantage de points d'intégration, de produits, de systèmes d'exploitation de fournisseurs et d'appareils qui interviennent dans la gestion, la consommation et le transport des données. Les données sont encore plus éloignées du contrôle de l'entreprise. Parfois, nous ne sommes même pas sûrs de l'emplacement exact de nos données. Par exemple, peut-être avez-vous partagé vos données avec une autre organisation et celle-ci a partagé les analyses réalisées sur ces données. Ces métadonnées se transforment souvent en résidus de données. Vos données circulent à travers de multiples systèmes, pouvant être accessibles à une personne malveillante par la suite.\n\nL'industrie de la sécurité elle-même est composée de milliers de fournisseurs et de produits de sécurité qui résolvent une partie spécifique du puzzle. Il y a donc seulement tant qu'un directeur des systèmes d'information peut faire avec un budget limité, et seulement tant de complexité qu'il peut gérer. Nous devons réfléchir de manière holistique à la façon dont nous pouvons sécuriser nos données, pas seulement comment sécuriser un seul transit entre le point A et le point B. Les données sont compromises au niveau du maillon le plus faible, donc nous devons examiner toute la chaîne.\n\nLa troisième force impactant les entreprises est l'emplacement des attaques. Elles comprennent des attaques à tous les niveaux de matériel, de firmware, de bios, de logiciels, de services et d'applications. Avec l'augmentation de la sophistication des attaques, nous constatons des attaques simultanées et indépendantes à plusieurs niveaux.\n\n## Les vecteurs d'attaque\n\nLes pirates informatiques ne s'attaquent pas seulement au matériel ou à un logiciel, mais à travers plusieurs vecteurs : les personnes, les processus et la technologie.\n\nTraditionnellement, la mise en place de contrôles de sécurité impliquait une formation appropriée des personnes et l'utilisation des bons processus et technologies, mais compte tenu des récentes attaques, nous devons nous rappeler que le processus lui-même est vulnérable. Par exemple, nous devons passer à la mise à jour automatisée afin de réduire la période d'exposition entre la découverte d'une vulnérabilité et la diffusion et le déploiement d'un correctif par une organisation. Traditionnellement, nous vivions avec le risque que les attaquants aient des mois pour exploiter la vulnérabilité.\n\nIl existe d'autres attaques en cours que nous sommes en train de résoudre, que ce soit dans le processus de construction ou le cycle de développement logiciel. Intégrer la sécurité dès le début du cycle de développement est l'aspect le plus important pour sécuriser une application. Cela signifie que les développeurs, les testeurs de qualité et les concepteurs doivent tous être impliqués dans le processus de sécurité. Une partie du défi réside dans la nature cloisonnée de chaque partie du processus, où des vulnérabilités peuvent se glisser entre les interstices et les transitions.\n\n## Les six piliers de la cybersécurité\n\n## Sécurité de la chaîne d'approvisionnement.\n\nLa sécurité de la chaîne d'approvisionnement n'a été mise en avant que ces dernières années. Une organisation doit pouvoir faire confiance aux serveurs, composants et logiciels. Une bonne chaîne d'approvisionnement transparente est importante afin de valider que tout provient de sources légitimes. Il y a eu une focalisation, notamment dans le gouvernement, sur la chaîne d'approvisionnement matérielle, mais nous ne devons pas oublier la chaîne d'approvisionnement logicielle. La chaîne d'approvisionnement logicielle est un peu plus difficile que celle du matériel, car il y a souvent un manque de visibilité étant donné que les produits peuvent être assemblés à partir d'outils en open source, de produits d'autres personnes, etc. Dans une récente attaque, la chaîne d'approvisionnement logicielle était le problème, et ceci n'est que la partie émergée de l'iceberg.\n\n## Hébergement et sécurité du système\n\nUne fois que nous avons une chaîne d'approvisionnement de confiance, l'étape suivante consiste à assurer la sécurité du système d'hébergement. La base repose sur des technologies de démarrage sécurisé et des capacités de cryptographie pour verrouiller et sécuriser les appareils physiques et les systèmes où les applications fonctionnent et les données seront exécutées. Ce système soutient les fonctionnalités de sécurité de la pile de niveau supérieur matériel.\n\n## Sécurité des données et des applications\n\nAu-dessus de la sécurité de l'hôte et du système se trouve la sécurité de la charge de travail de votre application. Les données doivent être protégées tout au long de leur cycle de vie, au repos, en cours d'utilisation et en transit. Nous utilisons depuis longtemps le chiffrement de transport, TLS, et IPsec, ainsi que d'autres capacités de chiffrement, pour assurer la sécurité des données au repos et en transit. Le chaînon manquant a été la mémoire chiffrée \"en cours d'utilisation\" avec isolation matérielle. Ces dernières années, les technologies et les solutions permettent de combler cette dernière étape de la protection des données.\n\n## Sécurité du réseau\n\nEn parallèle de cette pile d'hébergeurs de chaîne d'approvisionnement et de sécurité des données, nous avons besoin de sécurité réseau. L'intégrité et la disponibilité des réseaux sont importantes pour résister aux attaques de déni de service. Les données doivent arriver à destination de manière sécurisée. Nous devons également surveiller et protéger les réseaux contre les intrusions externes, que ce soit un réseau d'entreprise ou un réseau distribué à travers le cloud et le réseau périphérique. La sécurité ici ne se limite pas à un simple pare-feu ; il s'agit de production active.\n\n## Gestion de l'identité et des accès\n\nLa gestion des identités et des accès est une capacité fondamentale. Nous devons non seulement savoir qui se connecte, mais aussi à quel appareil. L'être humain n'est en réalité qu'une petite partie du puzzle. Nous devons avoir une identité pour toutes les choses, les processus et les services qui accèdent et gèrent les données. Une personne peut accomplir son travail en quelques frappes de clavier, mais il peut y avoir 20 appareils différents, ainsi que 100 services et processus qui agissent sur les données. Nous devons avoir des politiques et des autorisations pour toutes ces entités. Et à mesure que nous évoluons vers des processus autonomes, il y a moins d'humains impliqués, il devient donc encore plus important d'avoir une identité solide pour ces processus sans intervention humaine.\n\n## Détection des menaces, Renseignement et Analyse\n\nCe dernier pilier est une combinaison de plusieurs éléments, notamment le renseignement sur les menaces, l'analyse, la surveillance et l'audit. C'est la vision globale qui permet de s'assurer que tout fonctionne comme il se doit, et si quelque chose ne va pas, la capacité de comprendre rapidement d'où ça vient et pourquoi. C'est le cadre qui assure la sécurité des données, et tout doit être alimenté de haut en bas et de bas en haut. Il y a un changement de travail dans un environnement cloisonné, par exemple un vendeur qui se préoccupe uniquement de la sécurité du réseau, à un travail à travers tout le système. Les entreprises qui réussissent ont des équipes diversifiées avec des personnes issues de différents domaines pour répondre aux besoins complexes en matière de sécurité.\n\n## Domaines de cybersécurité : Atteindre la confiance zéro avec les technologies Intel\n\nIntel offre des capacités fondamentales dans chacun des six piliers, que ce soit notre initiative d'assurance du cycle de vie informatique pour aider les OEM (Original Equipment Manufacturers) de grande envergure et les fournisseurs de composants à collaborer sur une chaîne d'approvisionnement de confiance, en fournissant les blocs de construction fondamentaux de la sécurité du système, en passant par le démarrage sécurisé qui commence avec le matériel. Nous avons la technologie d'exécution et la technologie de garde de démarrage avec accélération cryptographique intégrée, de sorte que les utilisateurs peuvent l'activer pour la protection des données au repos, des données en cours d'utilisation et des données en transit sans impact sur les performances.\n\nDans le cas du renseignement de détection de menace, Intel fournit des primitives telles que TPD où une solution de pile de niveaux supérieurs peut offrir une visibilité et une détection de menace là où nous ne l'avions jamais eu auparavant.\n\nIntel est un fournisseur de technologie, mais nous travaillons également dans le domaine des personnes et des processus. Un bon exemple est la chaîne d'approvisionnement. Nous avons mis en place un processus avec l'écosystème pour permettre à une entreprise de valider les composants et les certificats d'une plateforme donnée et de ses composants. De même, il existe des processus liés à la protection des données et à l'utilisation grâce aux capacités matérielles telles que SGX qui permet de crypter la mémoire et d'isoler les données et le code d'une application donnée.\n\nIntel permet des processus sécurisés pour exploiter les technologies à grande échelle. Une autre partie clé concernant le processus est de s'intégrer dans la structure globale de risque d'une organisation. Intel vous fournit les preuves et l'attribution dont vous avez besoin au sein de nos technologies pour les intégrer dans votre structure de risque existante.\n\nLa dernière pièce est les gens. Traiter avec le comportement humain aléatoire est parfois la partie la plus difficile de la sécurité, que ce soit les escroqueries de phishing ou les attaques planifiées sur des mots de passe faibles. La formation est cruciale, mais elle n'est souvent pas suffisante. Les processus et les technologies peuvent aider à renforcer la formation, par exemple, en rendant les mots de passe plus forts ou en éliminant le phishing si les identifiants d'un utilisateur ne peuvent pas être compromis. En fin de compte, cependant, une formation et une éducation continues seront toujours essentielles, en plus des technologies d'atténuation.\n\nLa sécurité est difficile, mais il y a des lueurs au bout du tunnel avec toutes les innovations dans l'écosystème et avec des organisations ouvertes à faire les choses différemment. Nous devons garder un œil sur deux choses: l'adoption de cadres de gestion des risques et la confiance zéro. En reliant ces deux mondes, le domaine de la cybersécurité aux moteurs de politique et à l'application des mesures peut offrir une approche globale de la sécurité. Il y a beaucoup d'activité ici et encore beaucoup de travail à faire.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT36-fr","image":"./episodes/edt-36/fr/thumbnail.png","lang":"fr","summary":"Avec la montée des cyberattaques dans toutes les industries, la sécurité est plus importante que jamais. Dans cet épisode, Darren Pulsipher, architecte principal des solutions, et Steve Orrin, CTO fédéral chez Intel Federal, expliquent les vecteurs d'attaque, les six piliers de la cybersécurité et comment Intel peut aider. Le niveau de complexité avec lequel les organisations doivent composer pour sécuriser leurs données, leurs systèmes et leurs applications n'a jamais été aussi difficile."},{"id":106,"type":"Episode","title":"2020 en revue","tags":["covid","cybersecurity","remoteworker","people","process","compute","data"],"body":"\r\n\r\n## 2020 - Attentes\n\nAvant la COVID, l'année 2020 s'annonçait comme l'année des nouveaux modèles d'entreprise, orientés vers de grandes transformations numériques. L'IA/ML et l'analyse jouaient un rôle clé pour l'avenir. En utilisant ces outils, nous allions commencer à utiliser les données de manière plus efficace dans nos organisations. Ça devait être l'année de la 5G, avec la 5G qui allait créer une énorme sensation aux Jeux Olympiques.\n\nNous allions également assister à une avancée majeure dans l'industrie pour les transformations 4.0, avec l'Internet des objets et la fabrication se dirigeant tous deux vers cette transformation numérique.\n\n## 13 mars\n\nEnsuite, du moins en Californie, tout s'est brusquement arrêté le vendredi 13 mars. Les entreprises, les écoles, les conférences, les réunions, les déplacements et les événements sociaux ont été brusquement suspendus. Les semaines se sont transformées en mois alors que l'optimisme quant à la maîtrise rapide du virus diminuait, tandis que des villes comme New York ont subi de gros dommages et que les systèmes médicaux étaient débordés. Tout semblait chaotique et incertain.\n\nQuelques choses intéressantes ont émergé de cela : Certaines entreprises ont prospéré tandis que d'autres ont connu des difficultés, et les gens ont commencé à travailler à distance.\n\n## Le site Web le plus consulté de ma maison\n\nDans ma maison, le site web ayant le plus de vues n'était plus Facebook, mais le Center for Systems Science and Engineering à Johns Hopkins, qui donne des statistiques sur le virus. Nous pouvions voir les effets dans notre propre quartier, notre ville et dans le monde entier. Toute notre perception de ce qui était important et de ce que nous avions prévu pour cette année a changé.\n\n## Pénuries\n\nNon seulement il y avait immédiatement des pénuries d'articles aussi divers que le papier toilette, le gel désinfectant pour les mains et la levure, mais quelques semaines plus tard, le matériel de bureau et la technologie sont devenus très demandés alors que les gens s'adaptent rapidement à l'apprentissage à distance et au travail depuis chez eux. Cela a créé une pénurie d'outils essentiels tels que les webcams et les ordinateurs portables.\n\nLes routeurs Wi-Fi 6 sont devenus soudainement importants alors que l'utilisation d'Internet à la maison augmentait. Dans notre famille, je suis passé d'un travail à domicile occasionnel à un travail à domicile à plein temps et j'ai ajouté trois adolescents en apprentissage à distance ainsi que trois étudiants universitaires adultes qui sont rentrés chez eux pour étudier et travailler. Notre connexion Internet était rapidement débordée. J'ai eu du mal à trouver un routeur Wi-Fi 6 pour résoudre notre problème, car beaucoup se trouvaient dans des situations similaires.\n\n## Voyage a changé.\n\nAvec les voyages pratiquement stoppés, de nombreuses personnes ont plutôt opté pour des projets d'amélioration de leur maison. J'avais l'habitude d'être souvent à l'aéroport, les voyages représentant la moitié de chaque semaine. Maintenant, je me retrouvais chez Lowe's ou Home Depot, comme tout le monde. Les endroits étaient bondés. Le temps que je passais normalement à voyager était maintenant consacré à construire une remise avec mes enfants. J'ai pu passer plus de temps de qualité avec mes enfants tout en améliorant ma maison.\n\n## Conférences\n\nUn changement majeur, bien sûr, a été l'annulation des conférences en personne. Beaucoup ont opté pour une version en ligne avec d'excellents résultats. Par exemple, la conférence IBM Think a transformé son événement en personne prévu en mai en une version en ligne. Plus de 100 000 personnes y ont participé, ce qui en a fait leur plus grande conférence jusqu'à présent.\n\nEn plus de l'augmentation de la participation, un autre avantage est que les participants n'ont pas à s'engager pour une semaine entière pour assister à une conférence, mais peuvent choisir les sessions auxquelles ils veulent assister et rester chez eux avec leur famille. Le revers de la médaille est l'absence de réseautage et de rencontres en personne.\n\nLes conférences industrielles ont probablement changé pour toujours. Elles ne seront pas toujours entièrement en ligne, mais peut-être qu'une version hybride peut être envisagée à l'avenir.\n\n## Travail forcé à domicile\n\nUn autre changement majeur est que nous sommes virtuellement chez les uns et les autres pour mener nos affaires. Il arrive parfois qu'il y ait des enfants, des chiens ou d'autres distractions inévitables en arrière-plan. Une fois, mon patron a dû demander à sa fille de s'asseoir à côté de lui lors d'une réunion d'équipe car elle avait besoin d'aide pour quelque chose. Il est important que nous soyons tous flexibles pendant cette période.\n\n## Travailleurs nomades\n\nCertains employés ne travaillent plus à domicile, mais sont devenus nomades car ils n'ont plus besoin d'être près de leur lieu de travail.\n\nPar exemple, mon neveu, qui a peu de responsabilités à la maison, payait un loyer exorbitant dans la région de la Baie. Lorsque le travail est devenu à distance, lui et ses colocataires ont abandonné leur appartement cher et ont plutôt parcouru le monde, passant plusieurs mois à la fois en louant des maisons dans des endroits intéressants.\n\nCeci est un changement majeur dans notre manière de penser la gestion de nos travailleurs, de nos actifs et de nos données.\n\n## Travail à distance\n\nLe travail à distance s'est rapidement intensifié. De nombreuses entreprises avaient déjà des politiques de télétravail ou des plans pour en faire davantage. Mais ce qui était peut-être autrefois un plan de déploiement sur 18 mois est devenu un plan de déploiement sur 18 jours.\n\nLa principale forme de travail à distance qui s'est produite était le VDI (infrastructure de bureau virtuel). Les gens utilisent le VDI depuis des décennies, mais soudainement, il est devenu un choix de premier ordre, en partie grâce à sa familiarité. Il est rapide, facile et peu coûteux de permettre aux gens de reprendre le travail en leur donnant accès aux données dont ils ont besoin grâce à des solutions matérielles.\n\nLes VPN (réseaux privés virtuels) se sont rapidement surchargés alors que tout le monde travaillait depuis chez soi. Nous avons constaté que les entreprises investissaient dans l'extension de leur VPN, soit en achetant davantage de licences, soit en renforçant leur infrastructure et en effectuant des mises à niveau selon les besoins.\n\nLes entreprises ont également accéléré leur adoption d'outils SaaS (logiciels en tant que service). Par exemple, les déploiements d'Office 365 ont eu lieu en quelques semaines seulement, au lieu des six à neuf mois prévus. Des entreprises proposant des offres SaaS telles que Microsoft et Google ont mis la main à la pâte et ont aidé les organisations à effectuer rapidement leur migration. Pour le travailleur à distance, le temps improductif a été réduit au minimum grâce aux efforts de l'industrie dans son ensemble.\n\nC'était remarquable à quel point les travailleurs à distance ont rapidement repris leurs activités. Les véritables héros ici sont les travailleurs de première ligne en informatique, tels que le service d'assistance, les ingénieurs système, les administrateurs système, etc. C'était vraiment un effort herculéen.\n\n## Changements en matière de cybersécurité\n\nL'une des choses, cependant, qui a été un peu négligée avec la rapidité de ce changement, c'est la cybersécurité. Nous ressentons les répercussions de cela depuis les six à huit dernières semaines avec plusieurs attaques majeures en cybersécurité.\n\nL'une des raisons est une surface d'attaque accrue car les données sont dispersées sur des ordinateurs portables dans toute l'organisation, sur des réseaux non sécurisés, dans les foyers.\n\nCertaines données sont stockées dans le cloud. Maintenant, avec les offres SaaS, une partie est stockée dans le centre de données et une autre partie sur les ordinateurs que les gens ont ramenés chez eux.\n\n2021 sera une année consacrée à la cybersécurité, nous examinerons attentivement la manière dont nous gérons les données et assurons leur sécurité dans l'ensemble du système.\n\n## Nouvelle tenue de travail.\n\nL'un des plus grands progrès de cette année est la nouvelle tenue d'affaires. La meilleure façon de la décrire est la coupe de cheveux d'affaires : tenue d'affaires en haut, pyjama ou short en bas. Je mets parfois un pantalon, mais mes enfants doivent être fatigués de me voir en chemise à boutons ou même en costume sur le haut, associé à des shorts.\n\n2020 a été une année de changement, personnellement et professionnellement, mais cela nous a conduit à un endroit en 2021 où nous pouvons avancer avec les leçons apprises et les améliorations pour un avenir meilleur.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT37-fr","image":"./episodes/edt-37/fr/thumbnail.jpg","lang":"fr","summary":"Darren Pulsipher, Architecte Principal des Solutions, Secteur Public, chez Intel, réfléchit à la perturbation, aux changements et aux ajustements que la pandémie COVID-19 a apportés en 2020."},{"id":107,"type":"Episode","title":"Les années folles : un regard tourné vers 2021","tags":["contactless","covid","remotelearning","remoteworker","people","process"],"body":"\r\n\r\n## COVID-19, une période sans précédent de volatilité, d'incertitude, de complexité et d'ambiguïté.\n\nDans cet épisode, Darren demande à Rachel Mushawar, vice-présidente des ventes chez Intel, son point de vue sur ce qui nous attend dans les \"années folles\" après les perturbations causées par le COVID-19.\n\n## 2020 -> Deux mille vingt\n\nLes douze derniers mois ont été marqués par des changements et une incertitude indéniables, tant sur le plan professionnel que personnel. Toutes les années ne seront pas comme 2020, mais les leçons que nous en tirons nous rendront meilleures chaque année à l'avenir.\n\nMalgré les circonstances individuelles, chaque personne a dû faire une pause et réfléchir à ce qui était important. Nous tenons nos équipes, amis, familles et relations plus près que jamais.\n\nEt bien que nous ayons été physiquement isolés, la connectivité grâce à la technologie s'est accélérée à une vitesse sans précédent. Le sentiment de possibilité de chacun dans ce domaine a été rajeuni, et d'une certaine manière, libéré et ouvert pour l'avenir.\n\n## La pandémie a créé plus de responsabilités.\n\nD'autre part, la pandémie a créé davantage de responsabilités. Près de la moitié des adultes dans la quarantaine et la cinquantaine ont un parent de 65 ans ou plus et ont de jeunes enfants ou soutiennent financièrement des enfants plus âgés. Cela signifie qu'ils jonglent probablement entre l'apprentissage à distance de leurs enfants, le travail à domicile et les soins aux parents âgés.\n\nLes pertes nettes d'emplois aux États-Unis en décembre étaient toutes des femmes. Une partie de cela est due aux responsabilités accrues à la maison pendant la pandémie. Si vous avez de jeunes enfants à la maison qui suivent maintenant l'enseignement à distance et que vous êtes un travailleur horaire, comment faites-vous? Vous ne le faites pas. Et même si pendant le confinement, les pères ont presque doublé leur prise en charge des enfants, nous avons encore beaucoup de chemin à parcourir dans les années 20 en matière d'égalité des sexes. Il ne peut pas seulement s'agir de technologie.\n\n## L'éducation à distance met l'accent sur la fracture numérique.\n\nAlors que nous entrons dans les années 20, nous avons toute une génération d'enfants qui ont passé les 12 derniers mois à être instruits à domicile.\n\nEn réalité, un ménage américain sur quatre a au moins un enfant de 14 ans ou moins. Pour les étudiants issus d'un milieu défavorisé, une étude du Forum économique mondial a révélé que 25% d'entre eux n'ont pas d'ordinateur. De plus, 33% des étudiants des communautés rurales ont peu ou pas du tout accès à internet. Le manque de ces deux éléments fondamentaux de l'apprentissage à distance a accentué la fracture numérique.\n\n## Nous vivons \"L'avenir du travail\"\n\nBeaucoup de vieilles idées ont été brisées en 2020 alors que les entreprises ont dû rapidement pivoter vers le travail à domicile. Quatre-vingt-cinq pour cent des organisations ont élargi ou mis en place diverses politiques de travail à domicile qui vont perdurer au-delà de la pandémie de COVID-19. De nombreuses entreprises ont réalisé des économies en réduisant leurs biens immobiliers physiques, car la productivité est restée la même, voire a augmenté, avec une main-d'œuvre à domicile ou nomade. De nombreux employés réalisent également des économies de temps en supprimant les trajets et les préparations personnelles telles que les cheveux, le maquillage et la tenue vestimentaire. Une tendance pour 2021 sera un look plus décontracté et naturel.\n\nAlors que les trajets disparaissaient, la connectivité augmentait et les chambres devenaient des bureaux, cependant, un inconvénient est apparu avec l'allongement de la durée moyenne de travail et la difficulté à se déconnecter.\n\n## \"Le « sans contact » propulse tout en tant que service\"\n\nL'une des choses clés en 2021 sera la façon dont nous continuons à tirer parti de la technologie pour rester connectés. Par exemple, il existe des technologies similaires à Zoom qui permettent de créer des opportunités sociales similaires à une discussion autour de la machine à café dans l'espace virtuel. Trente-deux pour cent des adultes ont participé à une réunion sociale virtuelle en 2020. Intel a réalisé des visites dans une ferme virtuelle de chèvres et a fait venir des instructeurs de yoga pour lutter contre l'isolement.\n\n## Plus de connexions, moins de friction, plus de virtuel, moins de contacts physiques.\n\nAlors que nous entrons dans le reste des années 2020, la technologie va devenir un pilier pour chaque grande transformation, que ce soit dans le secteur privé ou public.\n\nComment cette nouvelle façon de travailler et les responsabilités accrues à la maison se résument-elles du point de vue d'un directeur des systèmes d'information (DSI) ?\n\nIl existe quelques impératifs stratégiques pour les technologies de l'information (TI). Nous pouvons les répartir dans les catégories traditionnelles des applications, du réseau et du centre de données.\n\nTout d'abord, pour les applications, les directeurs des systèmes d'information (DSI) doivent trouver comment faciliter le sans contact, c'est-à-dire, comment tout conduire en tant que service? Ce n'est pas uniquement pour les détaillants, mais aussi pour les secteurs de la santé, du gouvernement et de la fabrication. La deuxième partie concerne les applications et consiste à savoir qui est votre consommateur et comment il consomme le contenu.\n\nRéaliser l'importance de votre réseau vient ensuite. C'est le puissant accélérateur de toutes les choses numériques. Les organisations doivent avoir un réseau qui permettra, par exemple, l'automatisation de vos usines ou la fourniture de télésoins. Elles doivent avoir une vision d'avenir pour quand la 5G deviendra plus réelle plutôt que d'investir dans la technologie d'hier. Les réseaux s'étendent maintenant au-delà des centres de données traditionnels, c'est donc quelque chose sur lequel nous devons nous concentrer. Bien sûr, la sécurité doit être une priorité ici.\n\nTout comme les réseaux, la sécurité n'est pas un sujet passionnant ou sexy, mais c'est un élément essentiel lorsque l'on pense à tous les appareils qui sont désormais omniprésents dans notre vie quotidienne. La surface d'attaque augmente de manière exponentielle avec les employés qui travaillent à domicile sur différents appareils, ainsi qu'avec la mise en place de tout-en-un pour les clients. La sécurité ne concerne plus seulement la protection des données au repos et en mouvement. Elle concerne également tout ce qui se trouve entre les deux.\n\nAu lieu des centres de données traditionnels, nous devrions les envisager comme des centres de données, servant des charges de travail spécifiques. Par exemple, le cloud connaît une croissance de 30 à 40 % par an afin d'approcher les centres de données des employés ou des clients. Cependant, il peut ne jamais être pertinent de transférer certaines données confidentielles et critiques vers le cloud, mais de les conserver sur site. Les directeurs informatiques doivent comprendre ce que sont leurs centres de données, et lesquels seraient les mieux adaptés à chaque segment de l'organisation en termes de récupération, de stockage, d'efficacité des coûts et de performances.\n\nEn entrant dans les années 20, les organisations doivent effectuer des changements stratégiques, à la fois dans leurs pratiques de recrutement et dans la manière dont elles servent leurs clients, tout en gardant ces concepts à l'esprit.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rachel Mushawar"],"link":"/episode-EDT38-fr","image":"./episodes/edt-38/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren Pulsipher, architecte principal des solutions chez Intel, demande à Rachel Mushawar, vice-présidente des ventes chez Intel, son point de vue sur ce qui nous attend pour les Années folles après la perturbation de la COVID-19."},{"id":108,"type":"Episode","title":"Discussion à la machine à eau dans une équipe en télétravail","tags":["collaboration","covid","organizationalchange","people","compute","remoteworker","videoconferencing","signalwire"],"body":"\r\n\r\n## Nouveaux espaces de travail collaboratifs\n\nDans cet épisode, Darren et Sean Heiney, co-fondateur de SignalWire, Inc., discutent des nouveaux espaces de travail collaboratifs à distance qui réduisent les conférences vidéo interminables qui durent toute la journée. Leur politique \"Cameras On\" et la technologie SignalWire Work permettent une communication spontanée comme jamais auparavant.\n\n## La création de SignalWire Work\n\nSignalWire est le développeur de la plus grande plateforme de communication open-source au monde. Depuis trois ou quatre ans, SignalWire s'est concentré sur la création de la prochaine génération d'applications de communications en temps réel pour la vidéo et la voix, ce qui a conduit à SignalWire Work pour les bureaux à distance.\n\nUn prédécesseur de SignalWire Work était l'outil propre de SignalWire qu'ils ont construit pour eux-mêmes, car l'entreprise a été distribuée depuis sa création, travaillant à distance pendant trois ans. Les outils de collaboration à distance existants ne répondaient pas à leurs besoins, avec plus de 60 personnes réparties dans plusieurs pays et continents. Certains utilisaient déjà l'audio en continu, mais étant donné que de nombreux ingénieurs partageaient également des écrans Unix, cela a évolué vers la vidéo en continu. Lorsque COVID est arrivé, les clients ont souhaité accéder à cet outil, c'est ainsi que SignalWire Work est né, ainsi qu'une version pour les événements en direct.\n\n## L'environnement de travail SignalWire\n\nTravailler dans des espaces collaboratifs à distance n'est pas seulement une innovation technologique, mais aussi une expérience sociale. Pour certaines personnes, la vidéo en continu peut sembler étrange ou effrayante. Une fois que tout le monde a adhéré à cette philosophie, cependant, cela peut être plus efficace que de rester assis dans un bureau physique. Il y a des inefficacités dans un monde comme Zoom où les gens ne se parlent que lors de réunions planifiées avec des ordres du jour spécifiques. Une grande partie de la communication importante et informelle est perdue.\n\nAvec les outils SignalWire, les collègues peuvent voir à l'intérieur des chambres des gens, comme s'ils avaient un mur en verre dans le monde physique. Vous pouvez voir à quel point ils sont occupés, quel genre d'humeur ils ont, ou s'ils parlent à quelqu'un d'autre. Vous pouvez passer et poser une question. Il y a une interaction sociale, des discussions autour de la machine à café, qui ne sont pas possibles avec des réunions planifiées. Lorsque vous vous connectez à SignalWire Work, c'est comme entrer dans un bureau physique.\n\nIl y a des fonctionnalités qui assurent la confidentialité. Vous pouvez brouiller votre vidéo pour que les gens ne puissent pas voir votre visage, ou passer en mode concentré où les gens ne peuvent pas vous interrompre, mais ils savent toujours que vous êtes présent et peuvent frapper à votre porte virtuelle.\n\nUn autre avantage est qu'il peut y avoir des limites plus claires entre la vie à la maison et le travail. Lorsque vous vous déconnectez du bureau, c'est une coupure nette.\n\n## La technologie\n\nSignalWire a créé la technologie qui alimente tout, depuis la sonnette Ring aux éléments d'Amazon Connect et au moteur de service client de Netflix ; elle est pratiquement présente dans tous les principaux opérateurs de télécommunication. Maintenant, ils se concentrent sur la possibilité de mettre cette technologie entre les mains du grand public et de permettre à ces derniers de la développer davantage. Un exemple est la plateforme de culte virtuel la plus populaire du monde qui a été construite il y a environ un an sur SignalWire.\n\nUn aspect unique qui donne à SignalWire un avantage stratégique est que le multiplexage vidéo se fait dans le cloud. Une application de conférence vidéo traditionnelle va encoder et transmettre la vidéo à chaque participant dans une conférence. S'il y a sept personnes, il y a sept flux. C'est beaucoup de travail sur le processeur. C'est du travail sur votre appareil pour transmettre les données et vous êtes sujet aux saccades et aux pertes de paquets sur tous ces flux individuels, donc vous pouvez avoir une excellente connexion avec quelqu'un, mais l'autre personne paraît horrible.\n\nAvec SignalWire, les clients envoient un flux vers le cloud. Le cloud prend le flux de tout le monde, le multiplie ensemble, puis renvoie ce flux unique à tout le monde afin qu'il n'y ait qu'une seule transmission et réception. Cela présente de nombreux avantages tels qu'une meilleure autonomie de la batterie, une consommation de données réduite et une charge de travail moindre sur les processeurs de l'appareil local.\n\nPour l'expérience utilisateur, SignalWire peut améliorer la qualité audio lorsqu'elle est mixée ensemble, ou contrôler la disposition de manière à ce que tout le monde voie la même chose dans la même orientation. Lorsque vous pointez quelqu'un à l'écran, par exemple, tout le monde peut le voir. Pour des événements tels que des sessions d'exercices en direct, la plateforme permet aux utilisateurs d'entendre la musique de fond et l'hôte en même temps, tout en pouvant voir, par exemple, trente autres participants. Tout cela se combine pour créer une sensation et une expérience plus connectées.\n\nSignalWire peut fonctionner sur n'importe quel cloud ou plateforme, y compris sur un téléphone portable individuel ou un appareil basé sur l'atome. Les entreprises peuvent l'utiliser sur leur propre infrastructure, ce qui est important pour la sécurité et le contrôle des données au plus haut niveau. SignalWire peut déployer ses nœuds à l'intérieur d'un réseau sécurisé afin de protéger les données sensibles des traversées sur Internet public.\n\nLa plateforme est entièrement flexible avec la technologie et les applications des utilisateurs, permettant même à un grand studio de diffusion de l'utiliser pour produire et éditer l'un de leurs spectacles, en ajoutant des comédiens pour des voix off sur des enregistrements existants car la qualité en temps réel est si élevée.\n\n## Un environnement plus professionnel dans les espaces de travail à distance.\n\nLa clé pour rendre cet espace de travail nouveau viable est de s'y lancer et d'en faire une politique de l'entreprise. Les avantages deviennent rapidement évidents. Lorsque vous arrivez au travail, vous participez à des vidéos et êtes présents et en interaction avec vos collègues, comme vous le faisiez avant le COVID.\n\nQuand les gens allaient au bureau avant le COVID, ils s'habillaient probablement bien pour faire bonne impression professionnellement. Dans ce monde, cette impression se fait davantage à travers une bonne configuration audio et vidéo : de bons microphones, un bon éclairage, un environnement de haute qualité. Les employés font de leur mieux en utilisant la technologie car c'est maintenant un monde technologique. C'est une évolution de l'environnement informel du travail à distance, où l'on s'asseyait sur une plage ou à la table de la cuisine pendant que le dîner était préparé. Des conversations plus professionnelles peuvent avoir lieu dans un environnement de travail à distance plus professionnel.\n\nLa meilleure façon de vivre cette technologie est de l'essayer. Il y a une période d'essai gratuite de 30 jours sur https://signalwire.com/products/work.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sean Heiney"],"link":"/episode-EDT39-fr","image":"./episodes/edt-39/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions, secteur public, Intel, et Sean Heiney, co-fondateur de SignalWire, Inc., discutent des politiques de travail à distance de leurs entreprises en ce qui concerne l'utilisation des caméras, ainsi que de leur nouvelle technologie d'espaces de travail collaboratifs à distance qui favorise la communication ad hoc au sein de leur effectif entièrement à distance."},{"id":109,"type":"Episode","title":"Modèle de maturité en gestion de l'information","tags":["data","informationmanagement","informationmaturity","datagovernance","datawarehouse","datalake","datamesh","datalineage","technology","process","people"],"body":"\r\n\r\nDévelopper une stratégie de données peut être difficile, surtout si l'état de la gestion des informations de votre entreprise ou sa trajectoire est peu clair. Ce modèle de maturité de la gestion des informations aide les directeurs des données (CDO) et les directeurs informatiques (CIO) à identifier leur position dans leur parcours de gestion des informations, ainsi que leur trajectoire. Cette carte peut aider les organisations à orienter leur amélioration continue et leur progression vers une organisation de données ultime leur permettant de tirer le maximum de valeur commerciale de leurs données.\n\nLe modèle représente une série de phases, allant de la moins mature à la plus mature : Standardisé, Géré, Régi, Optimisé et Innovation. Bien souvent, une organisation peut évoluer simultanément dans plusieurs phases. Vous pouvez déterminer où la majorité de votre organisation opère, puis identifier les pionniers qui devraient être plus avancés en matière de maturité. Utilisez ces pionniers pour expérimenter ou prototyper de nouveaux processus, technologies ou structures organisationnelles.\n\n## Phase standardisée\n\nLa phase standardisée comporte trois sous-phases : basique, centralisée et simplifiée. La plupart des organisations se situent quelque part dans cette phase de maturité, donc examinez les comportements, la technologie et les processus dans votre organisation afin de déterminer où votre organisation pourrait se situer.\n\n## Basique\n\nPresque toutes les organisations s'adaptent à cette phase, au moins partiellement. Ici, les données sont utilisées uniquement de manière réactive et ad hoc. De plus, presque toutes les données collectées sont stockées selon des intervalles de temps prédéterminés, souvent indéfiniment. Les entreprises dans cette phase de base n'effacent pas les données de peur de manquer des informations critiques à l'avenir. Par exemple, nous travaillions récemment avec une entreprise qui possède des sauvegardes d'e-mails depuis 30 ans, soit environ 12-10 téraoctets, qu'elle a peur de supprimer. Cette pratique n'est pas rare.\n\nCes attributs décrivent le mieux cette phase :\n\nGestion par réaction\n\nDonnées non répertoriées\n\n* Stockez tout partout.\n\n## Centralisé (Collecte de données centralisée)\n\nAlors que les organisations commencent à évaluer leur stratégie de données, elles commencent par centraliser leur stockage dans des solutions de stockage de big data. Cette approche se présente sous la forme de stockage cloud ou d'appareils de big data sur site. Une fois les données collectées dans un emplacement centralisé, la technologie de data warehouse permet d'effectuer des analyses commerciales basiques pour obtenir des informations exploitables. La plupart du temps, ces données sont utilisées pour résoudre des problèmes liés aux clients, à la chaîne d'approvisionnement, au développement de produits ou à tout autre domaine de votre organisation où les données sont générées et collectées.\n\nCes attributs décrivent le mieux cette phase :\n\nGestion par réaction\n\nCollecte de données de base\n\nEntrepôts de données\n\n* Stockage de données massives\n\nAnalyse de base des mégadonnées (?)\n\n## Traduire en français : Simplifié\n\nÀ mesure que le nombre de sources de données augmente, les entreprises commencent à former des organisations qui se concentrent sur la stratégie, l'organisation et la gouvernance des données. Ce changement commence par le bureau d'un Chief Data Officer (CDO). Il y a des débats sur la question de savoir si le CDO doit relever du PDG ou du DSI. Ne vous attardez pas sur l'emplacement exact dans l'organisation. L'important est d'établir une organisation axée sur les données et de mettre en place un plan de normalisation des données. La normalisation permet de corréler différentes sources de données pour obtenir de nouvelles informations sur la situation dans l'ensemble de votre entreprise. Notez qu'en l'absence de normalisation, les données restent cloisonnées et ne sont accessibles qu'en partie. Un autre attribut clé de cette phase est la nécessité d'élaborer un plan pour gérer le volume massif de données collectées. En raison de l'augmentation du volume et du coût de stockage de ces données, le stockage à niveaux devient important. Bien qu'il soit presque impossible de connaître la meilleure façon de gérer le stockage des données aux premiers stades, nous recommandons d'utiliser les meilleures informations disponibles pour élaborer des plans rationnels de stockage des données, en précisant que cela devra être examiné et amélioré une fois que les données seront utilisées.\n\nCes attributs décrivent le mieux cette phase.\n\nGestion prédictive des données (début d'une organisation centrée sur les données)\n\n* Normalisation des données\n\nStockage en niveaux centralisé\n\n## Gérés (profils de données standard)\n\nDans la phase de Gestion, les organisations ont formalisé leur organisation en matière de données ; les scientifiques des données, les gestionnaires des données et les ingénieurs des données font maintenant partie de l'équipe et ont des rôles et des responsabilités définis. La gestion des métadonnées devient un facteur clé de succès à cette phase, et plusieurs applications peuvent désormais tirer parti des données de l'entreprise. Le passage d'un entrepôt de données à un lac de données permet une plus grande agilité dans le développement d'applications centrées sur les données. La virtualisation du stockage des données permet une solution de stockage plus efficace et dynamique. Les analyses des données peuvent désormais s'exécuter sur des ensembles de données provenant de sources et de départements multiples de l'entreprise.\n\nCes attributs décrivent le mieux cette phase.\n\nGestion organisée des données (Organisation des données en place avec identification des rôles clés)\n\nGestion des métadonnées\n\n* Linéage des données\n\n* Lac de données\n\nAnalyse des mégadonnées\n\nStockage défini par logiciel (virtualisation du stockage)\n\n## Gouverné\n\nLa phase gouvernée est principalement atteinte lorsque une organisation adopte une approche centralisée des données et parvient à une approche holistique pour les gouverner et les sécuriser. Le CDO travaille étroitement avec le CSO (Chef de la Sécurité) pour garantir que les stratégies de données et de sécurité fonctionnent ensemble pour protéger les données précieuses de l'entreprise tout en les rendant accessibles pour l'analyse. Les données sont classifiées dans différents compartiments en fonction de leur criticité, de leur caractère secret ou de leur importance. La conformité dictée par les réglementations est automatisée et appliquée à l'ensemble des données de l'organisation. La visibilité accrue sur l'utilisation des données et la sécurité augmente grâce aux stratégies conjointes de données et de sécurité ainsi qu'aux plans tactiques. L'intelligence artificielle de base est largement utilisée dans l'organisation et les décisions commerciales sont déduites des données. Les données peuvent désormais être collectées et répertoriées dans toute l'entreprise, y compris les appareils Internet des Objets (IoT) sur les actifs physiques de l'entreprise.\n\nCes attributs décrivent le mieux cette phase.\n\n* Classification des données\n\n* Conformité des données\n\nSécurité des données\n\nIntelligence artificielle de base\n\n* Virtualisation de données distribuée / IoT\n\n## Optimisé\n\nAlors que la collecte de données des organisations continue d'augmenter, elles doivent trouver des moyens d'automatiser et d'améliorer en continu leurs processus pour gagner en efficacité. L'automatisation des processus de données est l'objectif principal de la phase optimisée. Plus précisément, l'automatisation de l'annotation et du marquage des métadonnées permet de réduire le temps nécessaire pour valoriser les données. Les données sont devenues trop volumineuses pour être centralisées en un seul endroit, et une architecture de \"lac de données distribué\" émerge comme la manière optimale de gérer les données. L'apprentissage automatique joue un rôle clé à ce stade en commençant à fournir des informations aux décideurs pour les aider à optimiser les opérations et la valeur de l'entreprise. Les applications et les données sont déployées sur des infrastructures réseau, de stockage et de calcul en fonction des informations historiques et des modèles d'intelligence artificielle.\n\nCes attributs décrivent le mieux cette phase.\n\nÉtiquetage méta automatisé\n\n* Lac de données distribué\n\nInférence de données / ML\n\nInfrastructure pilotée par les données\n\n## Innovation\n\nL'organisation ultime se trouve dans la phase d'Innovation. Elle n'est pas seulement pilotée par les données, mais crée de nouveaux produits, offres et services basés sur les enseignements tirés des données internes et externes à leur organisation. Cette phase est celle où l'IA/ML offre des avantages inestimables. Il y a trois sous-phases dans l'Innovation : l'observation, la prescription et la prospective.\n\n## Perspicacité\n\nL'anticipation est une prise de décision basée sur les données, en fonction de ce que vous pouvez observer qui se passe réellement dans votre écosystème, par exemple, dans votre chaîne d'approvisionnement, le développement de produits ou la fabrication.\n\n## Prescriptive: Prescriptif\n\nAlors que l'aperçu est précieux, il nécessite une interaction humaine, une compréhension et une intuition. Au niveau suivant, prescriptif, votre intelligence artificielle suggère ce que vous devriez faire en se basant sur l'aperçu. Cela peut jouer un rôle important dans toute votre organisation, car les décisions sont basées sur les données, depuis la chaîne d'approvisionnement jusqu'à l'acquisition de clients.\n\n## Foresight in French is \"prévoyance\".\n\nÀ cette étape cruciale, les données aident réellement à créer l'avenir. Par exemple, la prévoyance permettrait à une organisation informatique de projeter la capacité dont elle aura besoin à l'avenir en se basant sur les normes historiques et même sur des facteurs tels que l'évolution des conditions avec ses concurrents. La prévoyance nécessite beaucoup de données et la formation de modèles, mais conduit à l'objectif ultime de l'entreprise en temps réel.\n\nCes attributs décrivent le mieux cette phase :\n\n* Perspicacité (décisions basées sur les données)\n\nPrescriptif (entreprise basée sur les données)\n\nPrévoyance (créer l'avenir)\n\nApprentissage profond\n\nEntreprise en temps réel\n\n## Conclusion\n\nIl est fréquent de se sentir coincé dans une phase et submergé par la quantité de changements nécessaires pour passer à une nouvelle phase de maturité. Chaque pas en avant est cependant précieux. Par exemple, peut-être êtes-vous dans une phase Centralisée et pouvez-vous envisager la gestion des métadonnées. Y a-t-il une opportunité de passer au-delà de la simple nettoyage des données et de les augmenter également ? Ce type de pensée progressive vous fera avancer dans la chaîne de maturité en matière de gestion de vos informations.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT4-fr","image":"./episodes/edt-4/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, nous parlerons du Modèle de maturité de la gestion de l'information et de la manière dont nous pouvons utiliser ce modèle pour aider nos organisations à avancer. Ce modèle peut vous aider à identifier où se situe votre organisation et où elle se dirige dans sa stratégie de gestion de l'information, en fin de compte, en tirant le maximum de valeur commerciale de vos données."},{"id":110,"type":"Episode","title":"Recherche médicale collaborative avec confidentialité informatique.","tags":["confidentialcomputing","cybersecurity","distributedanalytics","sgx","healthcare","data","technology","edge"],"body":"\r\n\r\n## Pourquoi avons-nous besoin de calcul confidentiel ?\n\nL'infrastructure informatique actuelle est construite autour de la notion de partage et d'ouverture : Internet est gratuit et les données devraient l'être aussi. Cela pose un problème en termes de sécurité. Nous avons mis en place certaines solutions qui fonctionnent bien ; nous savons chiffrer les données lorsqu'elles sont stockées et lorsqu'elles sont en transit. Malgré cela, les données peuvent être attaquées de différentes manières lorsqu'elles sont lues, analysées et utilisées.\n\nLes données sensibles restent vulnérables, qu'il s'agisse de données financières, médicales ou de localisation, tant du point de vue de la visibilité que de l'intégrité des données.\n\nDans le traitement des données en matière de santé, il existe des niveaux de complexité supplémentaires. Il y a de nombreuses règles et réglementations telles que la HIPAA, et chaque état possède également ses propres réglementations concernant les données médicales. Malgré la complexité et le nombre de parties prenantes, le calcul confidentiel est possible lorsque vous avez la possibilité de partager des données entre des parties qui ne se font pas confiance intrinsèquement.\n\nPour la confidentialité, nous devons prendre en compte plusieurs facteurs différents : l'intégrité des données, la confidentialité des données et l'intégrité du code.\n\n## Problèmes critiques de confidentialité et de sécurité des données.\n\nLes dossiers médicaux d'un individu, notamment aux États-Unis, sont dispersés partout parmi les médecins, spécialistes, laboratoires et hôpitaux. La plupart des gens n'ont pas un accès facile à leurs dossiers ; il est pratiquement impossible de créer une image complète de sa propre santé. L'informatique confidentielle peut aider à briser ces barrières.\n\nTout d'abord, cela peut garantir que quelle que soit la donnée que vous partagez, vous pouvez avoir confiance qu'elle sera protégée du point de vue de l'intégrité des données ; elle ne sera pas modifiée par qui que ce soit. Elle peut être confidentielle, c'est-à-dire tokenisée ou chiffrée, mais elle peut toujours être utilisée pour le calcul. Par exemple, si une partie souhaite effectuer des analyses sur certaines données, elle n'a pas besoin de connaître des informations telles que les noms, les dates de naissance ou les numéros de sécurité sociale. Donc, si ces parties des données peuvent être tokenisées ou chiffrées, elles peuvent être partagées pour l'analyse tant que l'autre partie est digne de confiance.\n\nC'est là que des mesures de sécurité telles que l'attestation jouent un rôle, de sorte que les parties puissent prouver leur identité. Et cette attestation peut être liée jusqu'au niveau matériel aux environnements d'exécution de confiance fournis par le matériel. De cette manière, vous ne faites pas seulement confiance au transport et au point de terminaison, mais vous faites également confiance à l'application et à la manière dont elle utilisera les données.\n\nIl existe deux approches de base pour cela. La première est un Kit de développement logiciel d'application (SDK), ce qui signifie que le développeur peut décider comment partitionner son code en composants fiables et non fiables. L'autre approche consiste à avoir un système de chiffrement en temps d'exécution qui peut être construit au-dessus d'un environnement d'exécution fiable, réduisant ainsi les efforts nécessaires pour convertir une application actuelle en quelque chose qui peut fonctionner dans cet environnement.\n\nAvec un environnement d'exécution de confiance basé sur le matériel protégeant les applications et les données en cours d'utilisation, il devient très difficile pour un acteur non autorisé, même s'il a accès physique au matériel, des privilèges root ou des droits administrateur pour l'hyperviseur, d'accéder à l'application et aux données protégées. Le paradigme de l'informatique confidentielle vise à permettre l'élimination même du fournisseur de cloud de la base de calcul de confiance. De cette façon, seul le matériel et l'application protégée elle-même se trouvent à l'intérieur de la zone d'attaque.\n\nCes environnements informatiques permettent aux CSP de tirer parti du meilleur que le matériel peut offrir et de la meilleure sécurité possible, sur laquelle l'utilisateur final a un contrôle absolu. Chaque partie peut déterminer ses propres politiques et les hiérarchies de politiques telles que l'État et le gouvernement fédéral, et chaque fournisseur d'informations peut déterminer quelles politiques s'appliquent et à qui.\n\n## Étude clinique de preuves réelles\n\nRassembler toutes les données et leur donner un sens représente un grand défi dans l'industrie de la santé. Le nombre de paramètres de confidentialité et de partage de données mis en place entre les différents prestataires, les dispositifs, les localisations géographiques, etc. rend cela actuellement impossible.\n\nAI-Vets, Intel et certains partenaires travaillent ensemble sur ce problème. L'architecture brillamment simple permet une utilisation dans des environnements disparates, des types de données et des politiques différentes, tout en étant capable d'effectuer une analyse centralisée.\n\nUn exemple de notre mise en œuvre est une petite preuve de concept : Comment analyser à travers plusieurs parties telles que des hôpitaux, des environnements de recherche et des laboratoires, chacun avec leurs propres données et essais qu'ils peuvent mener dans un environnement clinique ? Par exemple, comment pouvons-nous trouver une corrélation entre les personnes prenant le médicament X, par exemple pour le diabète, et ayant la condition Y, par exemple le cancer, alors que le médicament X n'a rien à voir avec le traitement de la condition Y ? Ces deux ensembles de données ne seraient pas au même endroit car ils sont gérés par différents fournisseurs.\n\nSi toutefois, les fournisseurs faisaient partie d'un écosystème où ils pourraient déterminer les politiques qu'ils souhaitent appliquer à leur point de terminaison, nous pourrions avoir une application centralisée, un portail de recherche central qui soit connecté à ces points de terminaison. Il y aurait une gestion des clés tierces et une attestation pour vérifier les informations d'identification et les autorisations mutuelles, de sorte que toutes les parties puissent se faire confiance.\n\nC'est un aspect de la confiance, mais nous devons également protéger les données qui seront extraites, interrogées et transmises. Pour ce faire, nous pouvons gérer les données et les applications à l'intérieur d'enclaves sécurisées et chiffrées. Les données sont traitées en utilisant les politiques que chaque utilisateur définit, telles que l'obfuscation des dates de naissance, des numéros de sécurité sociale, etc. Ces informations peuvent être tokenisées, ce qui les rend complètement inutilisables entre des mains non autorisées.\n\nLe portail central peut effectuer une requête qui s'étend sur plusieurs points d'extrémité et regroupe différents types de données dans son système d'exécution, puis effectue une analyse sur celles-ci. Ainsi, au lieu de devoir extraire toutes les données dans un lac de données pour ensuite réaliser une analyse, tout se fait en temps réel. Il n'y a pas d'attente pour la publication ou le nettoyage préalable des données en appliquant toutes ces règles ; cela se fait de manière dynamique et instantanée.\n\nCela permet d'obtenir des informations considérables. Pendant la pandémie, par exemple, s'il fallait attendre chaque jour pour obtenir les données et les analyser de manière complexe, ce serait difficile. Si nous pouvions accéder en temps réel aux données provenant de tous ces différents systèmes à travers le pays et le monde entier, tout en étant en mesure de les partager de manière sécurisée, nous pourrions obtenir des informations uniques qui ne seraient pas autrement possibles.\n\nNous avons déjà vu cela dans certains POC pour des sites cliniques que nous avons réalisés avec notre partenaire Fortanix. Ils disposent d'une gamme de produits qui permettent à différentes entités de définir leurs politiques dans un environnement de calcul confidentiel, de vérifier les identités les unes des autres et de gérer les clés et les confiances. Le concept d'environnements d'exécution fiables existe depuis un certain temps, et il est devenu courant, ce qui le rend plus facile à exploiter. Les cas d'utilisation pour cela sont fantastiques.\n\n## Détection automatisée de la COVID-19 à partir d'images radiographiques thoraciques\n\nDans certains cas d'utilisation, il ne s'agit pas seulement de sécuriser les données, mais aussi de protéger la propriété intellectuelle associée à certains algorithmes spécialisés. Par exemple, pour détecter automatiquement la COVID à partir d'images radiographiques, il y aurait des données radiologiques, des données patient, et il pourrait y avoir un algorithme propriétaire pour effectuer l'analyse. Les enclaves peuvent protéger à la fois les données et les applications des regards indiscrets.\n\nLa zone sécurisée protège également les autres machines du réseau car si quelqu'un publie quelque chose de néfaste sur les nœuds finaux, le système de gestion des clés l'empêcherait d'être échangé car il n'est pas correctement attesté. Les parties choisissent précisément les sources de données avec lesquelles l'enclave peut communiquer et elle est verrouillée à la fois pour ce qui entre et ce qui sort.\n\n## Dossiers de santé électroniques (DSE)\n\nLes dossiers médicaux sont un véritable chaos aux États-Unis, avec des accords papier HIPAA inapplicables, etc., et tout est réparti entre différentes entités. Un exemple à suivre qui pourrait nous servir de bon modèle vient du gouvernement allemand. Ils ont ordonné que les données de santé doivent être stockées dans des dossiers médicaux électroniques, et ces applications doivent être déployées dans des environnements d'exécution de confiance. Le patient est l'utilisateur final et décide quelles données sont disponibles et pour qui.\n\nCe niveau de granularité en ce qui concerne ce qui est disponible pour l'utilisateur final est énorme. Et non seulement toutes ces données sont collectées et partagées à partir des différents systèmes, elles se trouvent dans des enclaves sécurisées, donc elles sont complètement sécurisées vis-à-vis du monde extérieur. Si quelqu'un non autorisé réussissait à accéder aux données elles-mêmes, elles seraient complètement dénuées de sens.\n\nCes environnements d'exécution sécurisés sont la première étape vers des contrôles qui sont facilement compréhensibles et facilement applicables.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Nick Bhadange"],"link":"/episode-EDT40-fr","image":"./episodes/edt-40/fr/thumbnail.png","lang":"fr","summary":"Abattre les barrières pour accélérer la recherche médicale pour la cure du cancer grâce à l'informatique confidentielle. Nick Bhadange, spécialiste en technologie, AI-Vets, et Darren Pulsipher, architecte principal des solutions pour le secteur public chez Intel, discutent du besoin d'informatique confidentielle dans le domaine de la santé et des avantages potentiels par le biais de cas d'utilisation."},{"id":111,"type":"Episode","title":"L'art obscur de DevOps","tags":["devops","people","technology","compute","devsecops","cybersecurity","multicloud"],"body":"\r\n\r\nJetons un coup d'œil à la place de DevOps dans votre infrastructure.\n\nAu bas d'une pile normale, nous avons une couche physique qui peut signifier un nuage, un centre de données, des appareils d'IoT ou une infrastructure héritée.\n\nDe plus, il y a généralement une infrastructure définie par logiciel qui dissocie la complexité de la gestion des composants matériels individuels.\n\nEnsuite se trouve une couche de gestion des services, qui comprend la virtualisation de l'écosystème des conteneurs et une couche de gestion des informations distribuées, qui comprend le plan de données, les data lakes et tout ce qui gère vos données.\n\nEnsuite vient la couche d'application. Les développeurs d'applications utilisent les services à l'intérieur des couches d'application. Juste à l'interface entre la couche d'application et le plan de gestion des données et la gestion des services se trouvent les outils SecDevOps ou DevOps. Ces outils incluent des aspects de sécurité et d'identité qui offrent une manière sécurisée d'intégrer et de déployer en continu vos produits.\n\n## Couche Application / Charge de travail\n\nEn tête de la couche d'application et de la charge de travail qui alimente le SecDevOps se trouvent trois types de charges de travail : les charges de travail déclenchées par des événements, les charges de travail procédurales et une combinaison des deux, qui sont des charges de travail pilotées par GUI ou UI.\n\nUn exemple simple d'une charge de travail pilotée par des événements serait qu'une commande d'achat arrive dans votre système, provoquant d'autres actions. Il peut y avoir des étapes séquentielles ou parallèles, une interaction avec les humains, de l'automatisation et une interaction avec plusieurs applications ou sous-systèmes différents au sein de l'entreprise.\n\nDe nombreux outils d'automatisation de la charge de travail sont disponibles. Certains sont programmés tandis que d'autres utilisent l'automatisation des processus robotiques, qui sont davantage basés sur l'interface graphique (GUI) et l'interface utilisateur (UI). Ces outils travaillent sur l'automatisation des services sous-jacents, donc les charges de travail orientent l'interaction des services.\n\nLes services se répartissent traditionnellement en trois grandes catégories : les applications, telles que des produits prêts à l'emploi comme Word ou une application SAP ; les services complexes, qui sont construits dans un but spécifique, comme une pile MEAN avec Mongo ; et les services simples, qui font une seule chose, par exemple MongoDB qui stocke la base de données.\n\nIl existe une nouvelle catégorie en raison de la croissance de l'IA et de l'apprentissage automatique. Beaucoup de services ne font pas grand-chose sans un modèle qui lui est attaché, donc nous avons ajouté des modèles d'IA à la couche de service, que nous traitons de la même manière qu'un simple service.\n\n## Journée d'un développeur dans la vie.\n\nAprès avoir compris les charges de travail et les services, nous pouvons examiner ce que fait généralement un développeur.\n\nUn développeur écrira du code sur sa station de travail et effectuera quelques tests de fonctionnalités. Ensuite, il vérifie le code sur GitHub, par exemple, et une chaîne d'intégration et de livraison continues (CICD) se déclenche. Elle effectue des vérifications de sécurité sur le code, peut-être du lint, de l'analyse statique et de l'analyse dynamique.\n\nUne fois qu'il a passé ces tests, il sera généralement intégré à une branche d'intégration où les autres membres de l'équipe de développement récupèrent les données, les développent et intègrent leur code avec le code du développeur. Ensuite, une fois qu'il a passé leurs tests, il est déployé dans une étape de test. Une fois cette étape franchie, il passera en production.\n\nCeci est un pipeline CICD typique, qui existe depuis des décennies. Au fil des années, les différentes façons de décrire les pipelines ont été consolidées et normalisées, ce qui limite les complexités et les erreurs.\n\n## Pile DevSecOps\n\nLe pipeline n'est qu'un élément d'une pile SecDevOps.\n\nD'autres éléments nécessaires incluent un registre et un référentiel. Pensez à ceux-ci comme des référentiels versionnés pour stocker les artefacts générés pendant le pipeline CICD afin qu'ils soient facilement disponibles pour être utilisés encore et encore.\n\nUn autre élément important est un cadre d'automatisation. Cela aide à alléger le travail humain d'exécution de tâches telles que les vérifications de sécurité ou la promotion de constructions d'une étape à une autre. Les outils pour l'automatisation sont matures et la formation est disponible, donc un bon cadre d'automatisation devrait être fondamental.\n\nBien que la gestion de l'environnement se développe souvent de manière organique au fil du temps, il est logique de gérer et d'organiser les environnements de manière appropriée pour obtenir plus de fiabilité et de reproductibilité.\n\nUn élément clé en dessous de tout cela est un profil de sécurité. Vous devriez être en mesure d'avoir la capacité de définir des profils de sécurité, afin qu'ils puissent être utilisés dans plusieurs environnements et dans plusieurs piles d'applications.\n\n## Registres / Dépôts\n\nIl y a généralement au moins deux types différents de dépôts. Le premier est un dépôt de mise en scène, où vous pouvez générer des images (une collecte de tout le code nécessaire pour lancer un conteneur, par exemple), et stocker des choses comme des clés d'identification et des clés secrètes. Ce dépôt contient tout ce dont vous avez besoin pour déplacer les éléments en production. Certaines organisations peuvent avoir plusieurs dépôts de mise en scène car différents éléments passent par différentes étapes de maturité jusqu'à ce qu'ils atteignent le dépôt de production. Vous voulez être en mesure de revenir aux versions précédentes si nécessaire.\n\nDans la production, ou le référentiel doré, les images sont verrouillées, authentifiées et chiffrées. Seuls les éléments présents dans le référentiel doré sont déplacés vers la production.\n\n## Stages translated to French is \"Stages\".\n\nLa meilleure façon de penser aux étapes dans le pipeline CICD est que chaque étape fonctionne dans un seul environnement. Par exemple, dans une étape de construction, il existe un environnement de construction isolé avec des politiques. Ce n'est que lorsque toutes les étapes de cette étape sont accomplies que les choses peuvent passer à la prochaine étape. Cela évite de gaspiller des ressources avec des constructions et des exécutions parallèles qui peuvent éventuellement échouer. En même temps, il est préférable de ne pas avoir autant d'étapes qu'elles entravent le progrès, donc un plan soigneux et défini est important.\n\n## Étapes\n\nÀ l'intérieur des étapes se trouvent des étapes où le travail est réellement effectué. Dans la construction et le test de logiciels, les étapes peuvent être exécutées en parallèle ou en séquence ; il existe de nombreux outils qui vous permettent de définir ces opérations. Bien que certains aient une interface graphique pour cela, la plupart des développeurs préfèrent un format textuel car il permet le contrôle de version du pipeline et des étapes, permettant ainsi des vérifications de sécurité contre le pipeline.\n\n## Pipeline\n\nAvec des étapes et des étapes définies, vous avez un vrai pipeline. Au lieu de définir un seul pipeline pour toutes vos applications, ce qui échoue généralement car il devient excessivement complexe avec de nombreuses conditions ou trop restrictif, je recommande d'utiliser des pipelines modèles et de les modifier si nécessaire, en veillant à respecter les normes de conformité et les réglementations. Il est important d'établir un pipeline approprié dès le début d'un projet, tout comme la flexibilité au fur et à mesure de l'avancement du projet.\n\n## Environnements\n\nAu lieu de créer des environnements ad hoc, il est préférable de les créer dès le départ avec intention. Le DevOps ou le SecDevOps peuvent injecter des politiques de sécurité et de conformité dans l'ensemble des différents projets, garantissant ainsi la sécurité.\n\n## Service Stack can be translated to French as \"Pile de services\".\n\nJetons un coup d'œil à la façon dont les développeurs travaillent, ce qui concerne les services de nos jours. Même si les développeurs travaillent sur une application monolithique, ils ont tendance à regrouper leur travail en unités fonctionnelles telles que des bases de données, des nœuds de logique métier ou des couches de transport. Par exemple, en utilisant un service simple tel que MongoDB. Lorsqu'un développeur exécute ce conteneur sur son ordinateur portable, cela lui permet d'obtenir la fonctionnalité attendue pour stocker des données de manière non SQL dans un document. Sur l'ordinateur portable, il se peut que ce soit le seul conteneur en cours d'exécution.\n\nDans un environnement de test ou de développement, il peut y avoir plusieurs instances de ce service en cours d'exécution, et le développeur peut déployer un cluster de services MongoDB et les connecter ensemble pour un test. Le service reste un service Mongo DB, mais son comportement change en fonction de l'environnement dans lequel il se trouve. L'objectif des développeurs est d'écrire du code et de le vérifier par rapport au service MongoDB sur leurs ordinateurs portables pour garantir qu'il fonctionnera correctement en production.\n\nUn service simple tel que MongoDB est nécessaire, mais en soi, pas très utile. Des services complexes tels que les piles LAMP ou les piles MEAN sont plus importants. Ce sont plusieurs services qui fonctionnent ensemble, agissant essentiellement comme un seul service. Rassemblés, ils déploient un service complexe sur un ordinateur portable et il y a deux ou trois conteneurs de service simples qui tournent, fournissant aux développeurs les fonctionnalités nécessaires pour vérifier leur code.\n\nUne fois que le code est validé, il déclenche le processus de développement où le développeur s'intègre avec d'autres personnes. Le même service complexe peut adopter une toute autre façon de fonctionner. De nombreuses politiques de sécurité peuvent être associées à ce service complexe pour garantir sa sécurité, sa fiabilité et sa résilience.\n\n## Définitions de service/application\n\nIl est important de comprendre les concepts de services simples et complexes car les développeurs de logiciels doivent définir comment les faire fonctionner. Il existe quelques définitions. L'une d'entre elles est appelée définition d'image. Elles sont fréquemment utilisées dans le monde des conteneurs, appelées images Docker. Le fichier Docker définit ce qui se trouve dans cette image. Cela est considéré comme un conteneur simple en lui-même, bien que les gens commencent à utiliser des conteneurs pour des choses complexes.\n\nDans les définitions de service, nous pouvons inclure plusieurs définitions d'imagines, par exemple Docker Compose, les opérateurs Kubernetes, les graphiques Helm, Terraform, et même CNAB. Ce sont des outils qui vous permettent de définir un service. Un service est plus qu'un simple conteneur ; c'est l'environnement dans lequel le conteneur s'exécute. Il peut inclure des définitions de réseau, de connectivité de volume, voire même des politiques de déploiement. Une \"définition de service\" complète comprend des définitions d'image, de configuration et de provisionnement.\n\n## Mettre tout cela ensemble.\n\nLorsqu'un développeur crée un nouveau service, il ne se contente pas de développer le code de l'image ; il définit également l'environnement ou la configuration dans lequel il doit s'exécuter. C'est là que l'enchevêtrement de votre environnement et de la définition du service peut se rejoindre. Au moment de l'exécution, cela produira l'environnement nécessaire pour que le conteneur s'exécute de manière efficace et répétable, de sorte que vous puissiez facilement déplacer le code de l'exécution sur un ordinateur de bureau à l'exécution en production complète le plus rapidement possible.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT41-fr","image":"./episodes/edt-41/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte en chef des solutions, secteur public, chez Intel, définit les termes courants de DevOps et explique comment DevOps s'intègre dans votre organisation."},{"id":112,"type":"Episode","title":"Sécurisation du pipeline DevOps = SecDevOps","tags":["cybersecurity","devops","compute","process","technology","multicloud","devsecops"],"body":"\r\n\r\nUne étude datant de plus de 20 ans sur le rendement de l'investissement en matière de sécurité a démontré que plus la sécurité est intégrée tôt dans le cycle de développement, moins elle coûte que d'attendre la fin du cycle. Bien que nous le sachions depuis deux décennies, c'est encore un travail en cours.\n\nLa plupart des entreprises de développement intègrent la sécurité dans leur processus de développement global, donc il y a eu beaucoup de progrès, mais c'est un voyage, pas une course. Il s'agit de comprendre tous les différents points d'exposition et les faiblesses, et d'être en mesure de fournir les processus de sécurité adaptés à ces problèmes.\n\n## Les vecteurs d'attaque\n\nBien souvent, les gens considèrent les attaques comme les vulnérabilités des employés ou des colis, par exemple, et oublient la partie du processus de l'histoire. Du côté opérationnel, il y a le temps nécessaire pour combler une vulnérabilité. De l'autre côté, il y a le développement et la livraison des produits. Il y a différents points d'arrêt le long de cette chaîne, et ceux-ci ont récemment été exploités dans les dernières étapes du processus de construction. En ce qui concerne les clients, le code qui a été mis à jour était légitime car il provenait directement de la source. Il est donc nécessaire de réfléchir attentivement à l'endroit où placer la sécurité dans le processus.\n\n## Sécurité intégrée...\n\nAgile, CI/CD, DevOps,...Where is Security?\n\nAgile, CI/CD, DevOps,...Où se trouve la sécurité ?\n\nAlors que nous avons tendance à considérer que la sécurité est nécessaire lors des transitions du développeur vers le QA, et du QA vers les opérations ou le déploiement, la sécurité devrait en réalité être intégrée dans l'ensemble du cycle de développement, et pas seulement à quelques points de contrôle. L'approche devrait être une sécurité continue.\n\n## Sécurité intégrée...\n\nFaites de la sécurité une partie intégrante de chaque phase.\n\nLa sécurité continue est un défi. La plupart des développeurs et des responsables de l'assurance qualité ne sont pas formés en sécurité ; c'est une bataille difficile. L'industrie a tenté cette approche au début des années 2000, mais s'est heurtée à trois problèmes. Premièrement, le taux de rotation est trop élevé. Deuxièmement, le paysage de la sécurité évolue trop rapidement pour les tenir à jour, et troisièmement, comme ce n'est pas leur travail quotidien, le bon comportement n'était pas incité.\n\nComment intégrons-nous alors la sécurité dans le processus, automatisons les principales tâches que nous voulons accomplir, et laissons les développeurs faire leur travail, qui consiste à concevoir, tester et déployer le code ? Le processus de sécurité peut vraiment briller en l'intégrant aux automatisations que vous utilisez déjà en DevOps, telles que les tests unitaires automatisés, les tests de qualité et de régression automatisés, la construction automatisée et le déploiement automatisé. Cela ne résoudra pas tous les problèmes, mais il élèvera considérablement le niveau afin que vous puissiez vous concentrer sur les défis complexes liés à la sécurité.\n\n## Sécurité intégrée....\n\nSignifie que la sécurité est intégrée, pas une simple étape ou étape.\n\nCertains outils courants fournissent déjà une certaine sécurité automatisée qui met en évidence les vulnérabilités. Par exemple, GitHub effectuera des vérifications de sécurité sur les projets utilisant du code Node.js et tous les packages inclus. Cela peut être utile, mais c'est trop tard ; la sécurité devrait être intégrée dans le pipeline avant d'être vérifiée.\n\n## La sécurité intégrée...?\n\n## Comment y allons-nous?\n\nLes violations de sécurité actuelles mettent en évidence que la sécurité doit être intégrée à chaque étape du processus, y compris entre la construction et la production, ainsi qu'avant l'exécution du script pour construire l'application. En plus d'injecter la sécurité dans le processus de construction, nous devons sécuriser le processus de construction lui-même ; cela a été une faille béante pendant longtemps.\n\nDe nombreuses entreprises qui effectuent des développements internes examinent désormais de plus près leur processus de construction en raison des récentes violations de sécurité. C'est une bonne chose, mais cela ne peut pas se limiter à de telles réactions impétueuses à chaque attaque. Nous devons penser de manière holistique et ne pas attendre le prochain maillon faible de la chaîne.\n\nCertaines façons pratiques de sécuriser le processus consistent à considérer le serveur de construction comme un actif critique dans l'infrastructure globale et à appliquer les mêmes règles et contrôles à ce serveur que pour vos systèmes principaux. Des informations d'identification appropriées, un démarrage sécurisé du micrologiciel, la vérification du code, une vérification et un suivi du système, etc... tout au long de sa durée de vie est ensuite intégré dans le processus DevOps lorsque quelqu'un clique sur le bouton.\n\n## Construit pour durer\n\n## Des solutions aux services et au-delà\n\nDe nombreuses personnes ne considèrent pas le script lui-même comme une cible. Peu importe combien de bons modules sont inclus si le script lui-même n'est pas protégé. Quelques façons de protéger le script consistent à exécuter un checksum et celui-ci devrait être versionné, vérifié et signé. Cela ajoute de la complexité pour les équipes de développement et d'exploitation (DevOps), mais il existe des outils qui peuvent aider.\n\n## Construisez une fois, déployez partout\n\nTout comme nous automatisons le processus de développement, nous pouvons intégrer l'automatisation pour la mise en œuvre de ces contrôles et vérifications. L'automatisation empêche qu'une autre personne ne perturbe potentiellement vos constructions, mais nous voulons également nous assurer qu'un être humain reçoive les résultats et vérifie les audits.\n\nLes outils que vous utilisez déjà peuvent être étendus pour ajouter l'automatisation de la sécurité et des vérifications, comme celles nécessaires pour une intégration continue du développement dans le cycle Agile, ou pour les outils d'automatisation dans le monde Linux.\n\nLes organisations peuvent également répartir leur personnel de sécurité au sein des équipes de développement commercial afin que, lorsque des problèmes surviennent, les personnes chargées de la sécurité soient déjà intégrées dans le processus. Deux endroits où il est important de s'assurer d'avoir des personnes chargées de la sécurité sont dans l'infrastructure pour soutenir, par exemple, votre processus Agile, et dans la gestion des produits pour obtenir les exigences de sécurité lors de la phase de définition des exigences du produit avant même qu'il ne parvienne aux développeurs.\n\nIl y a toujours une pénurie de personnel de sécurité suffisamment formé et compétent, ainsi que de financement pour engager les bonnes personnes en raison d'une forte demande. Quelques options consistent à former les personnes que vous avez déjà et à leur fournir les outils nécessaires. Vous n'avez pas besoin d'un expert en cryptographie à chaque étape du processus. Une autre possibilité est d'avoir une équipe qui construit des modules dans vos langages et vos environnements pour toutes les fonctions de sécurité au lieu d'avoir chaque développeur responsable de coder l'authentification, les identifiants et les protocoles dans une bibliothèque d'infrastructures sécurisée. Le développeur peut simplement utiliser le module et il s'occupe de tout le travail difficile. De cette façon, vous ne construisez qu'une seule fois et déploiyez partout.\n\nNous constatons que des entreprises proposent des outils de sécurité SaaS, des services basés sur le cloud qui peuvent être utilisés pour votre application et votre environnement d'exécution. C'est une grande avancée dans le processus. Il existe des entreprises qui proposent des points d'injection de sécurité tels que la sécurité des applications dans un environnement rapide. Ces contrôles d'application tels que la désinfection des entrées et la validation des entrées peuvent être intégrés à votre environnement fonctionnel, mais cela dépend encore de la fin. N'oubliez pas que plus tôt vous commencez à sécuriser le processus, plus cela devient bon marché et moins douloureux.\n\nTout ceci nécessite bien sûr davantage de travail d'intégration. Les développeurs peuvent se méfier du travail impliqué, mais si un framework avec une sécurité intégrée existe (et il en existe des prototypes tels que Ruby on Rails et certaines infrastructures cloud), cela peut épargner de nombreuses heures. Cependant, il faut toujours veiller à ne pas se fier uniquement à la plateforme pour la sécurité, car elle pourrait être un point de défaillance unique.\n\n## L'automatisation vous libérera.\n\nLes violations de sécurité au cours des six derniers mois ont été profondes. Voici quelques points clés de conseils :\n\nLa sécurité doit être intégrée dans l'ensemble du cycle de vie, en commençant par les exigences. La sécurité doit faire partie intégrante du cycle DevOps lui-même, pas seulement de la programmation et des tests, mais aussi de l'infrastructure qui soutient ce processus.\n\nLors de la construction d'outils et d'objets de sécurité à l'aide de modules, construisez une fois, faites-le modulaire et déployez partout.\n\nUtilisez des services qui vous permettent de compter sur l'expertise de quelqu'un d'autre pour renforcer votre propre équipe de cybersécurité, qui manque de financement.\n\nL'automatisation vous libérera. Automatisez autant que possible pour rendre la sécurité plus facile et plus rapide, et réduisez les frictions pour vos développeurs et testeurs. Grâce à l'automatisation, vous pouvez éliminer 80 % de ce que nous appelons les tâches bêtes, afin de consacrer vos ressources limitées aux problèmes complexes.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT42-fr","image":"./episodes/edt-42/fr/thumbnail.jpg","lang":"fr","summary":"Dans cet épisode, Darren Pulsipher, architecte principal des solutions chez Intel, et Steve Orrin, CTO d'Intel Federal, discutent de la raison et de la manière dont le pipeline de DevOps doit être sécurisé. La seule façon de fournir un code solide, résilient et sécurisé est de l'intégrer dès le début, et plus tôt est mieux."},{"id":113,"type":"Episode","title":"Le rôle du DSI dans l'adoption du cloud","tags":["cio","cloudadoption","compute","cloud","csp","multicloud"],"body":"\r\n\r\nDans la première partie de cette interview, Darren Pulsipher, architecte principal des solutions chez Intel, et Doug Bourgeois, directeur général et leader de la stratégie GPS Cloud chez Deloitte, discutent de la migration vers le cloud et du rôle du directeur des systèmes d'information.\n\n## CIO Patrimoine\n\nDoug s'est impliqué dans le cloud dès le début lorsqu'il travaillait pour le gouvernement fédéral, dirigeant une grande organisation de services partagés. Il a reconnu les avantages du cloud pour ses organisations prestataires de services et a également compris la valeur pour son organisation. Tout d'abord, cela permettrait d'économiser de l'argent au niveau de l'infrastructure, et deuxièmement, c'était une opportunité de construire le cloud tout en virtualisant pour résoudre le problème de prolifération des serveurs. Pour un service en particulier, la réduction des coûts pour l'utilisateur final était de 40 pour cent.\n\nDans les services partagés tels que le cloud, une fois que vous atteignez des économies d'échelle, vous pouvez fournir les services de manière plus économique que la plupart des organisations ne pourraient le faire seules.\n\n## Le passage vers le cloud\n\nIl y a une douzaine d'années, les organisations, dans leur grande majorité, étaient axées sur le cloud privé. Elles amélioraient leurs centres de données pour intégrer une combinaison de mutualisation consolidée, multi-utilisateurs, avec une certaine automatisation intégrée. Le pendule a basculé environ trois ou quatre ans plus tard vers le cloud public avec les grands acteurs de l'hyper-scalabilité (AWS, Azure, Google).\n\nIl y a environ trois ans, les organisations ont atteint un équilibre dans le cloud hybride. Les gens ont réalisé qu'une grande variété de systèmes dans leur portefeuille se prêtent à différents modèles, certains privés, certains publics, certains hybrides. Dans l'ensemble, on adopte aujourd'hui une approche plus holistique pour faire correspondre les systèmes et les clouds à des objectifs spécifiques.\n\n## Mettre les informations dans le CIO\n\nIl est maintenant plus important que jamais pour les directeurs des systèmes d'information (DSI) d'avoir une connaissance approfondie de ce qui se passe au sein de leurs organisations, en se rapprochant de la mission et des objectifs commerciaux afin de répondre au mieux à leurs besoins. Alors qu'auparavant, un DSI se contentait de fournir une infrastructure, il est maintenant nécessaire de prendre des décisions architecturales éclairées en fonction des possibilités disponibles. Il y a deux raisons à cela. La première est la prolifération des données, de l'intelligence artificielle, de l'analytique et de l'apprentissage automatique qui intègrent les capacités essentielles de l'entreprise et requièrent une compréhension fondamentale des affaires. La deuxième est l'évolution du cloud qui a atteint une nouvelle phase, l'ère numérique, où les systèmes centraux de l'organisation doivent être modernisés afin d'améliorer les capacités de service pour leurs utilisateurs finaux.\n\nCe voyage a commencé à ramener les directeurs des systèmes d'information (DSI) à leur véritable place, c'est-à-dire gérer les informations plutôt que se concentrer autant sur l'infrastructure. De nombreux DSI ont été relégués à la case infrastructure, alors qu'ils pourraient se voir offrir l'occasion de faire quelque chose de vraiment transformateur.\n\n## Positionnement du CIO pour réussir\n\nAlors à quoi ressemble le parcours du Directeur de l'Infrastructure au Directeur des Informations ?\n\nUne façon est de se positionner pour prendre en charge quelque chose de nouveau que l'organisation essaie de faire, peut-être un nouveau processus ou entrer sur un nouveau marché, voire même une unité commerciale qui n'est pas encore opérationnelle. Vous avez une continuité et une perspective étant donné que vous avez travaillé avec tous les responsables d'applications différents, donc vous êtes spécifiquement qualifié pour prendre l'initiative. Une autre façon, plus courante, est à travers un événement négatif où il devient évident que le changement est nécessaire. Une catastrophe peut être le catalyseur pour un DSI de guider la voie vers une véritable transformation.\n\nRejoignez-nous pour la deuxième partie de l'interview...\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Doug Bourgeois"],"link":"/episode-EDT43-fr","image":"./episodes/edt-43/fr/thumbnail.png","lang":"fr","summary":"Dans la première partie de cette interview, Darren Pulsipher, chef architecte des solutions chez Intel, et Doug Bourgeois, directeur général, leader de la stratégie cloud chez Deloitte, parlent de la migration vers le cloud et du rôle du directeur des systèmes d'information."},{"id":114,"type":"Episode","title":"Le nuage à l'ère numérique.","tags":["cloudmigration","cloud","compute","process","deloitte","multicloud"],"body":"\r\n\r\n## Le parcours typique de migration vers le cloud pour une organisation\r\n\r\nAu cours des cinq ou six dernières années, les méthodologies, outils et expériences de migration vers le cloud ont atteint un niveau de maturation permettant des processus répétables.\r\n\r\nLa première étape consiste à définir vos priorités de migration, car cela se fera par phases, et non en un seul déménagement massif. Cela ne nécessite pas beaucoup de temps ou de ressources, mais c'est d'une importance critique. Un exemple extrême est que vous ne voudriez pas choisir le mainframe comme premier système à migrer vers le cloud, mais plutôt un système plus autonome tel que l'e-mail.\r\n\r\nDes entreprises telles que Deloitte ont développé et investi dans des outils de découverte qui aident à accélérer les processus de migration. Ces outils récupèreront un large ensemble de données, exécuteront un algorithme qui examinera la complexité, et classeront tous les systèmes dans différentes catégories. Comprendre la configuration et les points d'intégration des systèmes existants ainsi que la compatibilité des composants logiciels est fondamental pour la migration vers le cloud. Nous devons également examiner les limites et les cadres de conformité tels que PCI ou HIPAA. L'établissement de zones d'atterrissage pour ces environnements dans le cloud est la deuxième phase du processus.\r\n\r\n## Proposition de valeur de la modernisation numérique.\r\n\r\nParfois, il y a beaucoup de préparation nécessaire pour les migrations. La première vague est la plus simple avec le moins de modifications, mais après cela, lors de la phase deux, il peut y avoir des mises à niveau ou des changements de systèmes d'exploitation, de re-platforming ou de passage à un type de base de données différent, par exemple. La troisième vague implique souvent des serveurs clients plus anciens ou des architectures propriétaires nécessitant une refonte importante et pouvant prendre des mois pour être prêts pour le cloud.\r\n\r\n## Modernisation numérique et migration vers le cloud.\r\n\r\nIl est important de distinguer que la préparation au cloud n'est pas la même chose que l'optimisation du cloud; cela vient plus tard.\r\n\r\nDans de nombreux cas, la raison principale pour passer au cloud est d'ordre économique plutôt que technologique. Par exemple, un client peut ne pas vouloir renouveler un bail juste pour héberger un centre de données, ou bien il déplace ses bureaux physiques. Dans ces cas-là, il y a un facteur temps où il est préférable d'effectuer la migration en fonction de la disponibilité plutôt que de l'optimisation.\r\n\r\nUne fois dans le cloud, il est nécessaire d'optimiser car les facteurs de coûts diffèrent entre le cloud et l'ancien centre de données. Le coût d'un centre de données, après avoir réalisé l'investissement initial, est relativement caché, tandis que le cloud fonctionne plus comme un accord de location à perpétuité. Souvent dans les anciens systèmes, nous résolvons des problèmes en ajoutant plus de mémoire, de CPU ou de stockage car cela fonctionne jusqu'à un certain point, mais cela crée des systèmes inefficaces. Si nous déplaçons simplement ces systèmes inefficaces et gourmands en ressources vers le cloud, le modèle de coûts sera bien plus élevé que nécessaire, d'où la nécessité d'optimisation.\r\n\r\nUne partie du processus d'optimisation pourrait être un changement de processus. Par exemple, pour une organisation au Canada, leurs coûts ont augmenté en flèche lorsqu'ils ont déplacé une instance SAP vers le cloud. Ils ont réalisé qu'ils n'utilisaient pas cette instance la nuit ni le week-end, alors ils sont passés d'un modèle 24/7 à un modèle 16/5. Ce changement leur a permis d'économiser une somme substantielle d'argent. Il y a donc des moyens de réaliser un petit effort avec un retour sur investissement élevé en utilisant différentes approches.\r\n\r\nNous voyons enfin, après plus d'une décennie dans le cloud, une tendance émergente consistant à trouver de la valeur dans un changement de stratégie commerciale plutôt que dans l'infrastructure. La pandémie de COVID-19 a certainement été un facteur accélérant ce changement. Un exemple parfait en est la télémédecine. Elle existait déjà, mais était stagnante depuis cinq ou six ans avant la pandémie ; maintenant, ce modèle est la norme.\r\n\r\nDes innovations transformantes se produisent dans le cloud. À mesure que davantage de systèmes se déplacent vers le cloud, les industries continueront d'essayer et d'adopter différents modèles avec de nouvelles capacités transformantes.\r\n\r\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Doug Bourgeois"],"link":"/episode-EDT44-fr","image":"./episodes/edt-44/fr/thumbnail.png","lang":"fr","summary":"Dans la deuxième partie de l'interview, Darren Pulsipher, architecte en chef des solutions chez Intel, et Doug Bourgeois, directeur général et responsable de la stratégie des services cloud chez Deloitte, poursuivent leur discussion sur la migration vers le cloud."},{"id":115,"type":"Episode","title":"Tous les cœurs ne sont pas égaux.","tags":["multicloud","compute","optimization","workloadplacement","workload","migration","process","cloudinstance"],"body":"\r\n\r\nIntel a engagé des architectes de solution cloud pour aider les clients à migrer intelligemment vers les instances cloud appropriées pour leurs charges de travail. Stephen vient récemment de rejoindre Intel et possède une longue expérience générale dans les bases de données, les sept ou huit dernières années étant principalement axées sur le cloud, en tirant parti de ses ressources pour les clients.\n\nCertaines des difficultés les plus importantes auxquelles Stephen a été confronté en aidant les clients à migrer vers le cloud sont la tarification et les attentes liées à cette migration. Les fournisseurs de services cloud prétendent souvent que les clients économiseront de l'argent en passant au cloud, mais ce n'est souvent pas le cas.\n\nUne raison est qu'il y a un changement constant dans le marché des fournisseurs de services cloud compétitifs en termes de fonctionnalités, de performances et de capacités. Une autre raison est l'ignorance, par exemple, le fait de croire qu'un VCP de base est identique pour tous les fournisseurs. Migrer avec une mentalité de \"lift and shift\" peut également être très coûteux. L'optimisation est essentielle pour une migration cloud intelligente.\n\nPour réduire les coûts et potentiellement économiser une grande quantité d'argent, vous devez choisir les instances adaptées aux performances requises. Tous les cœurs ou processeurs virtuels ne sont pas égaux. Au sein d'AWS, il existe les instances M4, basées sur l'architecture Broadwell d'Intel, et les instances M5, qui sont un mélange des architectures Cascade Lake et Skylake. Différentes charges de travail peuvent exploiter ces plates-formes de manière très différente.\n\n## Performance CoreMark\n\nCoreMark est une application à un seul thread qui teste les déplacements de registres et les additions simples pour n'importe quel nombre de processeurs. Sur trois générations de processeurs Intel, Broadwell, Skylake et Cascade Lake, ce test montre très peu de différence entre eux. Cette information a été utilisée de manière erronée pour orienter les décisions des clients, car le test n'est pas une méthode efficace pour mesurer les différences entre les processus eux-mêmes. Il peut y avoir d'énormes différences de performances avec différentes charges de travail.\n\n## Base de données WL Performance\n\nStephen a effectué des tests numériques sur des charges de travail réelles, axées sur les bases de données open source. L'environnement Cascade Lake était trois fois meilleur que l'environnement Broadwell pour les applications open source, ce qui est logique compte tenu des avancées en termes de vitesse et des avancées intégrées qui exploitent l'accès à faible latence. Ainsi, le fait de passer à deux générations supérieures donne une performance deux à trois fois meilleure sur ces charges de travail.\n\nLa charge de travail et le noyau doivent être considérés ensemble. Par exemple, un client pourrait être tenté de passer à une VCPU que Amazon annonce comme étant 10% moins chère. AWS pourrait cependant regrouper des cœurs sur ces processeurs et vous vous retrouveriez maintenant dans un environnement où vous êtes l'une des 48 machines virtuelles sur une machine, passant à un environnement où il y a 128 machines virtuelles ; votre accès à la mémoire est limité et vous pourriez bénéficier de 60% de performances en moins pour une économie de 10%.\n\nLes organisations croient souvent que si elles externalisent vers le cloud, elles n'ont plus besoin d'un architecte système. Il est vrai que les architectes ne s'occupent plus de l'installation physique des machines, mais ils le font désormais de manière virtuelle et doivent comprendre ce que différentes instances offrent pour différentes charges de travail.\n\nIntel travaille sur un outil permettant de montrer quelles charges de travail s'exécutent le mieux sur quelles instances. En attendant, Intel dispose de directives générales et d'architectes de solutions cloud pour aider à guider les clients.\n\nDe plus, il est important de faire beaucoup de tests pour comprendre où placer les charges de travail, mais il est également important de tester et de comparer les performances pour valider que vous obtenez ce que vous attendez d'un fournisseur.\n\n## Base de données WL Performance - Non NVMe\n\nIntel a réalisé des tests sur des charges de travail lors d'installations fraîches sur une instance à 64 cœurs fixes et 64 VCPU. Après avoir obtenu le nombre initial, une autre instance a été instanciée et le même test a été exécuté à nouveau. Les résultats variaient considérablement à chaque fois pour le même test. Cela pourrait être dû au fait qu'il peut y avoir un mélange de processus proposés à un certain niveau de service, ou il peut y avoir une certaine distance et des latences supplémentaires pour les attaches de stockage en bloc, par exemple, ou même des voisins bruyants. Il est donc utile de tester pour s'assurer que le système répond aux attentes.\n\nEn résumé, pour tirer le meilleur parti du cloud, vous devez vous former, tester les systèmes et profiter de l'aide disponible.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steven Holt"],"link":"/episode-EDT45-fr","image":"./episodes/edt-45/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren Pulsipher, architecte en chef des solutions chez Intel, et Stephen Holt, architecte des solutions cloud chez Intel, discutent de l'optimisation du cloud et des études qui montrent que les cœurs ont des performances différentes selon les charges de travail."},{"id":116,"type":"Episode","title":"Sécurisation de votre Pipeline DevOps","tags":["devops","compute","technology","process","devsecops","cybersecurity","zerotrustarchitecture"],"body":"\r\n\r\n## Les vecteurs d'attaque\n\nDe nombreuses attaques se produisent au niveau des individus. Certaines des attaques insidieuses les plus récentes se sont concentrées sur le hameçonnage et l'ingénierie sociale sur des individus travaillant dans le domaine du DevOps. Nous devons mieux former les personnes à tous les niveaux. Dans un cas, c'était un stagiaire ayant accès aux clés qui a cédé à un plan malveillant.\n\nUn autre vecteur est la technologie : les attaques traditionnelles de déni de service, les attaques par injection SQL ou les attaques de débordement de mémoire tampon. Les derniers incidents utilisent à la fois des personnes et de la technologie pour attaquer le processus. Ils sont insidieux car ils se produisent autour du processus de construction et peuvent être très difficiles à détecter. Ils peuvent également propager un code malveillant à travers vos clients. Pour instaurer la confiance des clients, les organisations doivent avoir une stratégie pour sécuriser le pipeline d'approvisionnement.\n\n## Sécurité du pipeline\n\nToutes les chaînes de construction ne sont pas les mêmes, mais en général, elles se composent de quatre étapes avec des environnements : développement, construction, test et production. Ces étapes peuvent être facilement divisées en plusieurs étapes en fonction du type de produit que vous êtes en train de développer, mais les environnements correspondent à ces différentes étapes.\n\nNous devons examiner l'ensemble du processus, ce qui inclut les logiciels, le matériel et les processus, et adopter une approche différente plutôt que de se concentrer uniquement sur l'infrastructure, que la plupart ont fait avec succès.\n\n## Hôte et sécurité de l'infrastructure\n\nAu bas de la pile, vous devez sécuriser le matériel dans l'environnement de développement et de construction. L'environnement de test sera un peu différent car vous pouvez vouloir exécuter des tests qui impliquent l'injection de code malveillant. Les environnements de production sont généralement bien sécurisés, surtout maintenant que de plus en plus d'entreprises proposent des logiciels en tant que service (SaaS). Dans les environnements de production et de construction, vous devez prendre toutes les mesures de sécurité typiques que vous prendriez dans des environnements de production ou de SaaS.\n\nIl y a trois éléments clés dans ces environnements. Le premier est la détection. La détection et la remédiation sont des mesures de sécurité bien connues utilisant des journaux de service qui utilisent une plate-forme telle que Splunk pour trouver tout ce qui sort de l'ordinaire. Assurez-vous de le faire non seulement dans les environnements de développement, mais aussi dans les environnements de construction et de production.\n\nGardez à l'esprit que dans l'environnement de test, vous aurez besoin de plusieurs environnements de test, certains plus sécurisés que d'autres du côté de la détection, car vous souhaitez injecter un code erroné dans vos tests. Ne répandez pas simplement la sécurité uniformément sur tous les environnements ; les profils de sécurité peuvent être différents pour chaque environnement.\n\nLa prévention est la deuxième clé. Cela signifie être intelligent avec le matériel, s'assurer que les choses sont correctement corrigées, avoir les bonnes mises à jour de sécurité et le faire de manière automatisée. Cela devrait se produire dans tous les environnements, y compris l'environnement de test, et surtout lors de la construction et de la production.\n\nLa troisième clé fait partie de la prévention : il s'agit de la racine matérielle de confiance. Une chaîne de confiance peut être établie à partir du matériel, en passant par les sections de démarrage du micrologiciel, jusqu'aux hyperviseurs et aux systèmes d'exploitation. La racine de confiance peut être implémentée dans les environnements de développement, de construction, de test et de production. Cela peut inclure des conteneurs sécurisés et des machines virtuelles sécurisées. Par exemple, j'aime stocker mes clés de chiffrement et de hachage dans du matériel tel qu'un module TPM, et également avec l'extension Secure Guard d'Intel. Même si quelqu'un parvient à accéder à la machine, il ne pourra pas voler ces clés.\n\n## Exécutables de confiance\n\nLa mise en place d'exécutables de confiance est l'étape suivante. Cela signifie que vous pouvez effectuer des vérifications de sécurité sur le code qui est validé et construit, puis enregistrer la somme de contrôle avec ces modifications. Si quelque chose a été injecté dans la base de code, vous pouvez le détecter, car il ne devrait pas y avoir de modifications apportées au code pendant le processus de construction.\n\nDans une nouvelle phase de contrôles de sécurité, vous pouvez effectuer une analyse statique du code ou une analyse dynamique du code ou des violations de sécurité. Il existe d'excellents outils que vous pouvez facilement intégrer dans votre pipeline DevOps habituel, que vous utilisiez Jenkins ou les workflows GitHub, par exemple.\n\nUne fois qu'un exécutable est construit, créez immédiatement le hachage et ce hachage doit être versionné avec l'exécutable ; il doit rester l'exécutable qui passe tous les tests et est poussé en production. Ce hachage garantira qu'aucune altération n'a été effectuée.\n\n## Images attestées et sécurisées.\n\nTypiquement, il existe plusieurs référentiels ou exécutables à utiliser pour le code. Le hash qui est créé lors de la construction est maintenant dans le registre, et vous pouvez les attester. Vous pouvez sécuriser ceux-ci dans les images afin qu'ils ne puissent pas être modifiés. Si quelqu'un doit revenir en arrière et effectuer une petite modification, par exemple, un libellé ou des métadonnées, il est important de ne pas effectuer la modification et lui attribuer le même numéro de version. Il vaut mieux passer à nouveau par le cycle, même si cela prend plus de temps que de manipuler manuellement vos binaires.\n\nMaintenant, vous pouvez prendre ce même binaire sur lequel vous avez effectué tous les tests et le pousser en production. À ce stade, reconstruire le code source serait une erreur. Il est préférable de pousser la construction initiale dans le référentiel de production, également appelé référentiel d'or. Ce référentiel est le seul endroit à partir duquel les images, les binaires ou les machines virtuelles, par exemple, doivent être récupérés. Toutes les images doivent être notariées et attestées. Si vous disposez de machines virtuelles ou d'applications confidentielles, ou si vous souhaitez vous assurer qu'elles ne se retrouvent que sur certaines plates-formes matérielles, vous pouvez appliquer ce type de restrictions. Vous pouvez chiffrer les machines virtuelles, les conteneurs ou même les binaires et les verrouiller avec la clé stockée dans vos systèmes de construction et de production.\n\n## Injecter des outils de sécurité\n\nLes outils de sécurité doivent être intégrés dans le processus de construction. Au lieu d'utiliser des bibliothèques de sécurité open source ou de recréer celles qui existent déjà, vos ingénieurs en sécurité devraient être impliqués afin qu'ils puissent choisir des outils qui peuvent être facilement utilisés et réutilisés par les équipes de développement. Un bon exemple est l'authentification de base : la connexion de l'utilisateur. Il devrait y avoir une bibliothèque commune plutôt que chaque application ayant la sienne.\n\nIl est important de traiter ces bibliothèques de sécurité et outils de la même manière que vous traiteriez tout autre programme de développement logiciel que vous partagez au sein de votre organisation. Le partage réduira le temps et renforcera la sécurité de l'ensemble de votre écosystème.\n\n## Construisez une fois, déployez la sécurité partout\n\nUne fois que vous avez établi vos équipes de sécurité du développement, assurez-vous d'injecter les politiques et les outils dans tous vos produits et environnements. Une technologie géniale existe aujourd'hui qui vous permet de gérer plusieurs environnements. Donc, lorsque une nouvelle application est lancée, elle est lancée dans un profil de sécurité avec vos propres images VM ou de conteneurs en tant qu'images de base utilisées par les équipes de développement. En configurant la sécurité dans vos VM ou conteneurs dans vos images de base, vous obtenez une conformité de sécurité instantanée dans les différents environnements. Il est également possible de l'intégrer avec les outils de sécurité, de sorte que si vous trouvez quelque chose d'inhabituel dans l'application, vous pouvez notifier les outils de sécurité. Ne cherchez pas à créer des outils de sécurité qui gèrent cent pour cent des cas, car vous ne les finirez jamais; visez plutôt quatre-vingts pour cent comme une base solide et laissez les développeurs d'applications innover les vingt pour cent restants si nécessaire, en collaboration avec votre équipe de sécurité.\n\nUn dernier conseil important est d'automatiser tout ce que vous pouvez, notamment dans le pipeline DevOps, afin de prévenir les injections malveillantes. Protégez votre pipeline ; protégez votre processus.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT46-fr","image":"./episodes/edt-46/fr/thumbnail.png","lang":"fr","summary":"Dans la deuxième partie de cet épisode, Darren Pulsipher, architecte en chef des solutions chez Intel, donne des conseils pratiques pour sécuriser chaque étape du pipeline DevOps, y compris la protection du matériel et des piles logicielles avec une racine de confiance matérielle, le balayage de sécurité, les conteneurs/VM attestés et chiffrés, et plus encore."},{"id":117,"type":"Episode","title":"Dans l'attente de 2021","tags":["covid","remoteworker"],"body":"\r\n\r\nGreg dirige les ventes et le marketing d'Intel aux États-Unis. Les clients principaux d'Intel sont principalement les fabricants de systèmes OEM, et l'équipe de vente d'Intel travaille quotidiennement pour servir ces comptes et les aider à créer des systèmes basés sur les technologies de calcul et de mémoire d'Intel. Actuellement, il y a une demande sans précédent pour le calcul.\n\nLes équipes d'Intel travaillent avec des entreprises de logiciels, des fournisseurs de cloud, des entreprises du Fortune 1000, des écoles et des organismes gouvernementaux pour rendre l'informatique accessible et aider à identifier les tendances et à appliquer des cas d'utilisation pour résoudre les problèmes commerciaux de manière à améliorer la société. Intel se positionne donc en tant que précurseur et identifie de nouvelles tendances pour s'assurer que les clients disposent des bons produits.\n\n## COVID en tant qu'accélérateur\n\nActuellement, toutes les tendances sont liées à la pandémie de COVID. Les organisations ont passé la majeure partie de l'année 2020 en mode crise, avec les services informatiques déployant des technologies pour maintenir le bon fonctionnement des entreprises, puis s'adaptant à la nouvelle réalité. Maintenant, il existe une stabilisation où les départements informatiques peuvent envisager l'avenir plutôt que de simplement réagir à la crise en cours.\n\nUne leçon que nous avons apprise est que l'informatique peut avancer beaucoup plus rapidement que nous ne l'avions imaginé. Par exemple, il est étonnant de constater à quelle vitesse presque toutes les industries ont réussi à passer au télétravail. Toutefois, le COVID-19 a fonctionné comme un accélérateur plutôt qu'un catalyseur. La plupart des changements étaient déjà prévus, mais le COVID-19 a compressé les délais. Au lieu d'un déploiement planifié sur 18 mois d'Office 365, cela s'est produit en une semaine et demie, parce que cela devait être le cas.\n\nL'une des principales raisons de la rapidité du changement est qu'un DSI ne pouvait pas prendre toutes les décisions nécessaires pendant la crise. Ainsi, la prise de décision a été déléguée afin de donner du pouvoir à ceux qui étaient sur le terrain pour travailler rapidement et faire ce qui était le mieux pour l'entreprise.\n\n## Modèle de travail hybride\n\nUn changement qui prendra du temps est de comprendre le nouveau modèle de travail hybride. Par exemple, Intel travaille sur les attentes concernant la fréquence à laquelle les employés devraient se rendre au bureau, après qu'une grande majorité d'entre eux ait passé une année à travailler depuis chez eux. Les services informatiques se préparent et investissent dans des outils qui permettent la collaboration entre certains employés présents au bureau et d'autres chez eux.\n\nLes organisations peuvent ne pas savoir à quoi ressemblera leur modèle pendant de nombreux mois, donc les services informatiques doivent faire preuve de flexibilité dans leur approche.\n\n## Applications de l'expérience des employés et automatisation.\n\nLes projets en ligne qui ont été mis de côté en 2020 font maintenant leur retour dans cet nouvel environnement de travail hybride. Les employés veulent s'impliquer dans leur entreprise de la même manière qu'ils le font en tant que consommateurs dans les logiciels en tant que service. Par conséquent, de nombreuses applications sont en cours de développement autour de l'expérience des employés.\n\nDe plus, il y a eu beaucoup de progrès dans l'utilisation des bots. Par exemple, Intel vient de déployer un bot RH pour améliorer et automatiser l'expérience des employés en accédant à tous les services des ressources humaines. Cela conduit à la nécessité d'avoir davantage d'IA et d'automatisation de l'IA, qui est guidée par l'apprentissage automatique.\n\n## Service client sans friction, sans contact.\n\nBeaucoup des clients d'Intel se préparent à fournir des services clients sans friction et sans contact dans les secteurs gouvernemental, de la vente au détail et de divertissement. Un secteur qui s'est accéléré pendant la pandémie est celui des télécommunications, ce qui va rendre ces livraisons sans friction beaucoup plus faciles.\n\nL'investissement en capital pour construire l'infrastructure de télécommunications et la 5G soutient désormais davantage d'objets connectés (IOT) et de dispositifs à distance qui, avant la pandémie, auraient pris des années à se concrétiser.\n\n## IA et apprentissage fédéré\n\nIntel a connu une forte croissance dans la collaboration avec l'IA et les données protégées dans des domaines tels que la découverte médicale et sur les marchés financiers. Grâce aux nouveaux processeurs Xeon de troisième génération d'Intel, il existe des enclaves de calcul sécurisées (SGX) et une mémoire protégée qui ne peut pas être accédée de l'extérieur du système. Les cas d'utilisation déployés sont principalement liés à l'IA et à l'apprentissage fédéré, où les données des utilisateurs et des entreprises peuvent être utilisées pour entraîner des modèles mondiaux, sans être partagées dans un référentiel centralisé. Grâce à l'IA, au concept d'apprentissage fédéré et au SGX d'Intel, ces données peuvent être protégées. Les obstacles à la confidentialité et à la réglementation des données peuvent être éliminés. Par exemple, les données d'un hôpital ou d'un patient individuel peuvent être protégées, tout en étant utilisées pour entraîner un modèle plus global avec de nombreux avantages.\n\n## RPA en français se traduit par \"Automatisation des Processus Robotisés\".\n\nDe nombreuses entreprises cherchent des moyens d'automatiser des tâches, dans certains cas des tâches monotones, afin de libérer leurs employés pour qu'ils travaillent sur des projets à plus forte valeur ajoutée. Cette tendance a explosé au cours des six à huit derniers mois, avec une forte croissance sur le marché. Au cours des cinq dernières années, les marchés de l'automatisation des processus robotisés (RPA) ont reçu plus de deux milliards de dollars de financement en capital-risque, principalement de la part des marchés financiers de New York. En dehors du RPA, il existe de nombreux cadres d'automatisation que les gens utilisent pour déployer leur infrastructure dans leurs centres de données et également de manière transparente dans le cloud.\n\n## Calcul en périphérie\n\nIntel a commencé à élaborer des architectures de référence pour aider les entreprises à développer leur calcul périphérique. La science importante ici consiste à connecter le calcul périphérique jusqu'à l'infrastructure de cloud et à construire à la fois une pile matérielle et logicielle, un panneau de contrôle et une automatisation. Il s'agit là d'un autre domaine d'investissement incroyable.\n\n## Avancer\n\n2020 a été une année de chaos, d'adaptation sans précédent et de changement accéléré. Maintenant, en cette année 2021 plus stable, les entreprises peuvent capitaliser sur les leçons et les tendances qui en ont résulté.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Ernst"],"link":"/episode-EDT47-fr","image":"./episodes/edt-47/fr/thumbnail.png","lang":"fr","summary":"Greg dirige les ventes et le marketing d'Intel aux États-Unis. Les clients principaux d'Intel sont les OEM de systèmes, et l'équipe de vente d'Intel travaille quotidiennement pour servir ces clients et les aider à construire des systèmes basés sur les technologies de calcul et de mémoire d'Intel. Actuellement, il y a une demande sans précédent pour le calcul. Les équipes d'Intel travaillent avec des entreprises de logiciels, des fournisseurs de cloud, des entreprises du Fortune 1000, des écoles et des agences gouvernementales pour rendre le calcul accessible et aider à identifier les tendances et à appliquer des cas d'utilisation pour résoudre les problèmes commerciaux de manière à améliorer la société. Intel agit donc en tant que précurseur et identifie les nouvelles tendances pour s'assurer que les clients disposent des bons produits."},{"id":118,"type":"Episode","title":"VAST Data révisé","tags":["dataarchitecture","data","optane","technology","vastdata"],"body":"\r\n\r\nVAST Data a connu une croissance fulgurante au cours des six derniers mois, avec une équipe en expansion et des ventes impressionnantes, telles que près de 70 pétaoctets de stockage flash fournis au gouvernement fédéral. En plus du partage de fichiers général avec leurs systèmes de stockage tout-flash, ils ont des cas d'utilisation intéressants dans des domaines tels que le séquençage de nouvelle génération, la microscopie confocale et la simulation en soufflerie.\n\nLa plateforme de l'entreprise permet de résoudre des problèmes de lecture aléatoire car toutes les données sont stockées sur la mémoire flash sans coût exorbitant ; la plateforme gère un volume élevé de données avec une faible latence.\n\n## Technologie VAST et disques Intel Optane\n\nVAST utilise les disques Intel Optane pour atteindre la parité lecture/écriture, entre autres avantages. Étant donné qu'Optane est une mémoire persistante, il n'y a pas de problèmes de cohérence de cache et il n'y a pas besoin de RAM, ce qui est un avantage architectural considérable pour éviter une défaillance ou une perte de données. Le coût total par utilisateur est également réduit car vous n'avez pas besoin d'acheter de gros machines avec beaucoup de mémoire pour le stockage de fichiers si vous souhaitez une vitesse supérieure. Cette catégorie est éliminée.\n\nL'évolutivité et la flexibilité sont deux autres avantages, car vous pouvez ajouter davantage de boîtiers de données (boîtiers D) sans affecter les performances, et ils sont entièrement compatibles même avec plusieurs générations de flash. Il n'y a pas de point de défaillance unique, et il n'y a pas de limite théorique ; VAST a testé jusqu'à 100 pétaoctets.\n\nVous pouvez améliorer les performances en ajoutant des boîtes client (C boxes), indépendamment des boîtes D (D boxes). La capacité avec la mémoire flash est essentiellement gratuite, mais vous devez exposer cette mémoire flash avec le CPU. Les boîtes C sont totalement dépourvues d'état, vous pouvez donc les agrandir ou les réduire à la volée. VAST garantit une disponibilité maximale car vous avez un accès complet à chaque PCP à l'arrière-plan.\n\nSelon les exigences de performance du client, il peut y avoir différents nombres de boîtiers D et de boîtiers C ; par exemple, un client peut ne pas avoir besoin de performance supplémentaire, mais d'une capacité supplémentaire, donc le cluster peut avoir, disons, 11 boîtiers C et 37 boîtiers D.\n\nSi vous avez une boîte C et une boîte D, vous disposez d'environ 40 gigaoctets par seconde de bande passante, ce qui est le modèle de base. Chaque ajout d'une boîte D ajoute encore 40 gigaoctets de bande passante. Une seule boîte C ne sature pas toutes les IOPS, donc si vous ajoutez une autre boîte C, vous pouvez obtenir 350 000 IOPS à partir d'une boîte. Comme le flash est limité par le processeur, plus vous ajoutez de processeurs, plus cela vous permet de mettre à l'échelle.\n\n## Nouveau modèle d'entreprise de stockage\n\nLe nouveau produit de VAST Data, Gemini, permet aux clients ayant besoin d'une performance élevée sans grande capacité de ne souscrire qu'à la quantité de matériel dont ils ont besoin auprès de leur fabricant de contrat, ce qui entraîne d'énormes économies de coûts. Les clients peuvent augmenter leur capacité et la payer au fur et à mesure de leurs besoins, plutôt que de supporter un coût initial pour les futures capacités nécessaires.\n\nD'un point de vue OPEX, cela a du sens pour de nombreuses entreprises. Par exemple, pour un client, un système tout-flash de 30 pétaoctets qui génère un téraoctet et demi de bande passante, le coût est inférieur à S3 d'Amazon pour un accès peu fréquent. Cela concerne seulement une année ; à l'avenir, cela devient en réalité moins cher que Glacier si on le considère selon la façon dont Amazon facture, par giga par mois. De plus, il y a la possibilité de passer à un modèle CAPEX si cela semble plus judicieux ultérieurement.\n\nIl est important de noter que VAST présente un appareil, et non pas un service de stockage, ce qui peut poser des problèmes de support en raison des variations de matériel et de firmware, des différents types de disques pris en charge, des interconnexions, etc. Cela entraîne une complexité folle. VAST continue d'expédier les mêmes boîtiers et serveurs ; il s'agit simplement d'un modèle économique différent permettant de consommer le stockage de manière flexible.\n\nCertaines zones intéressantes où VAST pourrait être utile sont dans la recherche médicale, tel que la pathologie numérique avec une quantité astronomique de données souvent difficiles pour les chercheurs à accéder car elles sont stockées sur des équipements bon marché et lents. La médecine de précision basée sur le patrimoine génétique d'une personne est également une possibilité. De plus, l'utilisation de l'intelligence artificielle sur les captures de paquets pourrait être utile pour prédire les attaques d'adversaires avancés.\n\nUn autre cas d'utilisation prometteur est un client ayant toutes ses données dans un système de stockage VAST entièrement flash, plutôt que dans le cloud, mais effectuant tous leurs calculs dans le cloud, ramenant peut-être seulement les résultats. Les résultats sont généralement très petits, peut-être quelques octets de données, et sont peu coûteux à extraire. Ce type de modèle, par exemple, pourrait générer de la valeur en effectuant des analyses sur des anciennes données qui sont actuellement simplement stockées et qui seraient trop coûteuses à déplacer.\n\nIl existe beaucoup de flexibilité pour adopter une approche hybride ou multicloud, où vous disposez d'un système de stockage centralisé sur site accessible via plusieurs fournisseurs de cloud.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Randy Hayes"],"link":"/episode-EDT48-fr","image":"./episodes/edt-48/fr/thumbnail.jpg","lang":"fr","summary":"Dans cet épisode, Darren prend des nouvelles de Randy Hayes, désormais VP des ventes pour le secteur public de VAST Data, six mois après leur dernière conversation pour voir comment ils se portent dans l'industrie, ce qu'il y a de nouveau chez VAST, et des cas d'utilisation intéressants. Leur nouveau produit, Gemini, propose un modèle d'entreprise de stockage différent."},{"id":119,"type":"Episode","title":"Travail à distance Avant, Pendant et Après la Pandémie","tags":["covid","remoteworker"],"body":"\r\n\r\nIls ont identifié quatre domaines qui les ont aidés non seulement à survivre, mais à exceller pendant cette période inhabituelle : Anticiper les besoins de votre responsable, Communiquer comme si votre emploi en dépendait (car c'est vraiment le cas), Soutenir les autres en soutenant leurs idées, Bloquer ! Ce ! Calendrier !\n\n## Anticipez les besoins de votre manager.\n\nDevinez de quoi a besoin votre responsable en terme de rapport et ayez l'information à portée de main.\n\nLes rapports de ventes, l'état d'avancement des projets et la situation des clients devraient être créés de manière proactive.\n\n## Communiquez comme si votre emploi en dépendait (parce que c'est certainement le cas)\n\nNe remplissez pas la boîte de messagerie des autres avec des informations inutiles.\n\nMettez votre demande au début de l'email. Ensuite, ajoutez des informations supplémentaires.\n\nDonnez à votre équipe un résumé de ce que vous faites avec des informations utiles.\n\n## Agitez votre drapeau en agitant le drapeau pour les autres.\n\nUn élément clé de la réussite dans n'importe quelle entreprise est d'avoir des réalisations visibles et tangibles.\n\nVous ne pouvez pas toujours baisser la tête et travailler en espérant que cela parle de lui-même.\n\nReconnaissez les personnes qui vous ont aidé à conclure l'affaire ou à terminer le projet.\n\n## Bloc ! Ça ! Calendrier !\n\nDéfinissez des limites pour le temps en famille et pour le début et la fin du travail.\n\nBloquez du temps pour vous. Vous avez besoin de temps pour reposer votre cerveau.\n\n* Prévoyez une pause de 5 à 10 minutes entre les réunions.\n\nIl est temps de traiter votre boîte de réception.\n\n* Le moment à la fin de la journée pour faire le bilan de votre journée\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rachel Dreikosen"],"link":"/episode-EDT49-fr","image":"./episodes/edt-49/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren discute avec Rachel Dreikosen, responsable du développement des affaires dans le secteur public chez Intel, de l'impact de COVID-19 sur son équilibre vie professionnelle-vie personnelle et de la raison pour laquelle elle a commencé un blog pour aider d'autres professionnelles de vente technique."},{"id":120,"type":"Episode","title":"Modèle de maturité de l'infrastructure","tags":["infrastructurematurity","compute","technology","sdi","cloud","multicloud"],"body":"\r\n\r\nLa transformation numérique peut être difficile. De nombreuses organisations deviennent paralysées lorsqu'elles essaient de décrypter toutes les technologies qui leur sont jetées chaque jour. Dans ce voyage, comme dans tout autre, il est important d'avoir une carte. Dans cet épisode, Darren explique le \"Modèle de Maturité de l'Infrastructure\" pour aider les organisations à découvrir où elles en sont dans leur transformation numérique, où elles peuvent aller ensuite et comment y parvenir.\n\n## Phase standardisée\n\nPresque toutes les organisations sont dans cette phase standardisée ou au-delà. Comprendre cette phase nous donne une référence pour commencer à parler des points communs que nous observons dans toutes les organisations.\n\n## Basique\n\nLe niveau de base est là où l'on trouve une infrastructure désorganisée et une gestion réactive. C'est particulièrement courant dans les start-ups où une organisation achète des services ou du matériel selon les besoins, en réaction aux événements plutôt que dans le cadre d'un plan global. Ici, nous voyons également des piles logicielles/matériel spécialement conçues. Une informatique amateur accrue fait également partie de cette phase de maturité.\n\n## Centralisé\n\nDans ce niveau, l'infrastructure est centralisée. Les organisations achètent de la capacité plutôt que du matériel spécialement conçu. Les achats ne sont plus réactifs, mais font partie d'un plan à plus long terme. En général, l'organisation aura des configurations de serveur communes afin de pouvoir acheter en volume pour une efficacité en termes de coûts et une fiabilité accrues. L'informatique est plus centralisée et spécialisée, par exemple dans les réseaux, le stockage, le calcul et la sécurité.\n\n## Simplifié\n\nAu niveau simplifié, l'infrastructure est plus consolidée. Différents départements peuvent partager du matériel tel que des serveurs, du stockage et des réseaux. Les organisations obtiennent de meilleures performances lorsqu'elles passent des anciennes méthodes à de nouvelles méthodes de centralisation et de simplification. Les coûts d'exploitation diminuent à mesure que l'efficacité augmente.\n\n## Phase virtualisée\n\nLa plupart des organisations aujourd'hui se trouvent au moins en partie dans la phase de virtualisation. C'est là que l'on trouve des configurations standard et des clouds privés.\n\nLa virtualisation du serveur donne la flexibilité d'exécuter des applications sur différentes machines et de les déplacer facilement d'une machine à une autre.\n\nLa virtualisation du stockage est un développement plus récent qui apporte des avantages similaires à la virtualisation des serveurs : une flexibilité pour des décisions plus dynamiques. Le stockage peut désormais s'étendre à l'ensemble de l'écosystème. Le stockage peut être provisionné pour une application spécifique, et ce stockage ainsi que l'application peuvent se déplacer au sein de l'infrastructure.\n\nLa virtualisation réseau, le dernier ajout, a été adoptée particulièrement par les fournisseurs de services Internet. Ils ont réalisé un investissement considérable en raison de l'efficacité de pouvoir effectuer des changements dynamiques dans le réseau à distance plutôt qu'à partir du matériel préalablement dédié à une fonction spécifique.\n\nToutes ces technologies de virtualisation sont liées dans les offres de l'espace cloud privé, qu'elles soient commerciales ou open source, souvent regroupées dans un seul endroit facile à gérer. Les organisations entrent dans cette phase et modifient également et consolident leur infrastructure informatique en passant d'une approche verticale à une approche transverse (NOTE : peut-on appeler cela une approche horizontale ?) en ce qui concerne les systèmes, les applications et les services, plutôt que le calcul, le stockage et le réseau.\n\n## Phase automatisée\n\nDans la phase automatisée, les organisations peuvent provisionner automatiquement les infrastructures, ce qui permet de gagner du temps en éliminant les différentes couches de personnes et d'autorisations. Grâce à l'utilisation de portails en libre-service, un ingénieur ou un développeur de logiciels peut obtenir immédiatement les ressources dont il a besoin. Cette provision automatique permet également une automatisation basique des correctifs, des machines virtuelles, de la sécurité et de la conformité. Les TI ne sont désormais plus en mode réactif, mais en mode proactif. D'autres aspects précieux de cette phase incluent, par exemple, le démarrage automatique des infrastructures pour les applications en production et l'auto-guérison des applications.\n\n## Phase orchestrée\n\nÀ mesure que l'automatisation devient plus complexe, une phase orchestrée devient nécessaire. L'automatisation consiste à appliquer des actions sur une machine, alors que l'orchestration consiste à coordonner les actions se déroulant sur plusieurs machines ou même modalités (stockage, calcul, réseau) pour une application. De plus, les piles d'applications automatisées peuvent déployer plusieurs applications sur plusieurs serveurs différents. À cette phase, les organisations orchestrent également des services hybrides, par exemple, la gestion des ressources sur des clouds publics et privés. Les avantages de cette orchestration sont la réduction des coûts d'exploitation (OpEx), la réduction des coûts d'investissement (CapEx) et la réduction du temps de déploiement.\n\n## Temps réel (gestion SLA) Phase\n\nPendant une phase en temps réel, les organisations sont dans une infrastructure d'accord de niveau de service. Au lieu d'applications, les organisations utilisent des services qui apportent de la valeur et qui lient tout ensemble. Les services partagés fonctionnent sur plusieurs nuages hybrides et même sur une infrastructure héritée. La mise en commun de l'infrastructure hybride, l'orchestration basée sur des politiques et l'orchestration basée sur des services optimisent l'infrastructure, la gestion des données et les services.\n\n## Conclusion\n\nIl est courant que différentes parties de votre organisation se trouvent à des phases différentes. Quelques groupes seront plus avancés que d'autres; c'est sain. De petits groupes pionniers peuvent échouer sans affecter l'ensemble de l'organisation, ou ils peuvent réaliser une percée et entraîner le reste de l'organisation avec eux. Très peu d'organisations sont passées à la phase la plus élevée sur la carte. La clé est de ne pas se décourager, mais d'utiliser la carte comme guide pour déterminer où vous en êtes et quels pourraient être les prochains pas pour votre organisation.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT5-fr","image":"./episodes/edt-5/fr/thumbnail.png","lang":"fr","summary":"La transformation numérique peut être difficile. De nombreuses organisations deviennent paralysées lorsqu'elles essaient de décrypter toutes les technologies qui leur sont présentées chaque jour. Dans ce parcours, comme dans tout autre, il est important d'avoir une carte. Dans cet épisode, Darren explique le Modèle de maturité de l'infrastructure pour aider les organisations à déterminer où elles se trouvent dans leur transformation numérique, où elles peuvent aller ensuite et comment y parvenir."},{"id":121,"type":"Episode","title":"Comprendre l'épuisement professionnel","tags":["employeeburnout","remotelearning","remoteworker","people","process","compute"],"body":"\r\n\r\nL'expérience professionnelle d'Uzair a commencé dans la vente, l'expérience des consommateurs en point de vente, puis s'est développée dans le domaine du développement de produits. Pendant dix ans, il a travaillé sur une application de réveil qui fonctionnait de différentes façons personnalisées pour aider les gens à se réveiller motivés et concentrés. Bien que ce projet ait finalement échoué, Uzair a beaucoup appris sur la motivation humaine. Il y a quelques années, lorsqu'il travaillait avec des élèves de collège et de lycée, il a remarqué un grand manque de motivation et de concentration pendant la première période de la journée, qui semblait s'aggraver progressivement. Grâce à son expérience professionnelle et à sa propre croissance personnelle, il a vu l'opportunité d'avoir un impact en créant l'application District Zero.\n\n## Bien-être des élèves de la maternelle à la terminale\n\nLa mission de District Zero est de doter les élèves de la maternelle à la terminale de compétences globales en matière d'apprentissage. Cela signifie aider les élèves à gérer leurs émotions difficiles afin de retrouver leur concentration et leur motivation.\n\nL'application utilise la technologie Agile et la puissance de l'analyse des sentiments du traitement du langage naturel pour fonctionner à grande échelle afin de détecter les déclencheurs et l'anxiété via une enquête. Une fois qu'un étudiant termine l'enquête, l'application peut suggérer du contenu et des ressources tels qu'une vidéo ou un jeu pour aider avec les problèmes spécifiques. Elle dispose également d'un système de rapport pour les enseignants et les administrateurs.\n\nActuellement, les enseignants et les administrateurs ont une charge lourde en essayant d'aider leurs élèves en matière de bien-être, surtout dans l'environnement COVID. Ils peuvent utiliser un formulaire Google connecté à Google Sheets et ensuite utiliser Ctrl+F pour rechercher des mots et des expressions négatifs. Cette application réduit cette charge et évite le sentiment de négativité chez les enseignants et les administrateurs. Le système identifie les obstacles et les problèmes des élèves et les aide à les résoudre grâce à des solutions rapides et des ressources. Si les élèves ont besoin de plus de soutien, le système les dirige vers la personne appropriée, comme un conseiller, tout en tenant informé l'enseignant.\n\nTraditionnellement, lorsqu'un élève montre de l'anxiété ou de la frustration, par exemple, il est directement pris en charge par un conseiller ou un travailleur social lorsque le problème devient ingérable. Mais l'élève n'a pas besoin de passer de zéro à cent ; il y a un juste milieu où le problème peut être abordé avant d'en venir à un combat ou une fuite. L'application peut aider à prévenir l'escalade du problème et à identifier les problèmes avant qu'il ne soit trop tard.\n\nLe lancement pilote de l'application dans les écoles de la région de Chicago a eu lieu en août dernier, pendant la période de COVID, ce qui la rendait particulièrement adaptée pour aider les élèves qui avaient perdu cette connexion en personne avec leurs enseignants et qui avaient des difficultés avec l'apprentissage social et émotionnel. Un élément clé de la réussite de l'apprentissage social et émotionnel est que tout le monde dans la communauté doit être impliqué et empathique : les enseignants, les administrateurs, les surintendants, les parents et les contribuables ; l'empathie et les liens ne peuvent pas se limiter à l'école.\n\n## Épuisement des employés\n\nPendant la COVID, les travailleurs ont montré une augmentation de leur productivité, mais il y a maintenant un risque d'épuisement professionnel pour les employés. Que ce soit le stress d'être un travailleur de la santé ou un travailleur à distance qui manque d'équilibre travail/vie personnelle, une multitude de problèmes font de cela une question importante pour les entreprises. Les principes de l'application District Zero K-12 peuvent être appliqués aux travailleurs pour aider à soulager l'épuisement professionnel.\n\nLe District Zéro a commencé à expérimenter l'application sur son lieu de travail, ce qui a entraîné des conversations difficiles sur les priorités et la compréhension des besoins réels des employés. Tout comme l'application K-12, le système peut identifier les difficultés quotidiennes des employés et combler le fossé en apportant un soutien avant que les problèmes ne s'aggravent.\n\nLe District Zéro espère développer sa technologie dans le but d'être bénéfique à différents secteurs tels que les entreprises, les services de santé d'entreprise, voire même les agences gouvernementales, pour une utilisation dans des domaines tels que la prévention du suicide chez les anciens combattants.\n\nCet nouvel outil et cette perspective sur le soutien de la santé des étudiants et des employés arrivent à point nommé alors que la COVID a fait augmenter le stress et l'épuisement, et que nous entamons une nouvelle phase de réouverture et d'adaptation.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Uzair Hussain"],"link":"/episode-EDT50-fr","image":"./episodes/edt-50/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal de solutions chez Intel, discute avec Uzair Hussain, PDG de District Zero, de la manière dont l'application de l'entreprise, qui soutient le bien-être mental des élèves de la maternelle à la 12e année, peut s'appliquer à la prévention de l'épuisement professionnel des employés."},{"id":122,"type":"Episode","title":"Cas d'utilisation pratique de la mémoire persistante Optane","tags":["dataarchitecture","memverge","pmem","optane","technology","data"],"body":"\r\n\r\nMemVerge a livré la première version de leur logiciel en septembre 2020, et malgré la pandémie, qui a ralenti la sensibilisation des clients à la nouvelle technologie, un bon modèle de cas d'utilisation est apparu.\n\n## Réduction des coûts des fournisseurs de services cloud\n\nLa principale mesure pour un opérateur de service de cloud est combien de machines virtuelles (VM) ils peuvent fournir à leurs clients, et à quel coût. La taille de la mémoire sur les serveurs devient le goulot d'étranglement qui limite le nombre de VM qu'ils peuvent allouer par serveur, restreignant ainsi la baisse possible de leur prix par VM.\n\nLe logiciel MemVerge avec Optane offre une quantité plus importante de mémoire par serveur, allouant un nombre plus important de machines virtuelles (VM), réduisant ainsi le coût par VM et augmentant la compétitivité des fournisseurs de services cloud. Le coût par VM pourrait être trois fois moins cher.\n\n## Fiabilité avec de grandes bases de données en mémoire\n\nLes clients financiers tels que les bourses, les banques et les fonds communs de placement utilisent beaucoup de bases de données en mémoire et d'applications en mémoire. En plus d'augmenter leur mémoire pour pouvoir avoir plus d'instances par serveur, MemVerge résout les problèmes de disponibilité des bases de données en mémoire. Si les données ne sont pas constamment enregistrées sur le stockage, mais restent simplement en mémoire, toutes les données intraday sont perdues en cas de crash. Cela est catastrophique. Même si toutes les transactions ont été enregistrées, il faut rejouer le journal pour récupérer la base de données, ce qui prend de nombreuses minutes, voire des heures, pour se remettre.\n\nMemVerge propose un nouveau service de données qui dispose d'une capture instantanée en mémoire. Il persiste l'état de la base de données sur Optane, ce qui est beaucoup plus rapide que la persistance sur le stockage. En cas de panne, vous disposez de la dernière capture instantanée en mémoire persistante et vous pouvez récupérer à partir de celle-ci. La récupération ne prend qu'une minute ou deux, ce qui représente une amélioration de 60 à 100 fois.\n\n## Réduction du séquençage génomique par instantané de mémoire.\n\nDans le domaine de la génomique, le logiciel MemVerge en combinaison avec Optane augmente la productivité de manière exponentielle. Dans un flux de travail d'analyse de données à plusieurs étapes, une mémoire plus grande signifie plus de parallélisme du pipeline et du traitement, ce qui rend l'ensemble du processus plus rapide. La fonctionnalité de capture d'écran est également pratique ici. Si une organisation réalise, par exemple, des recherches sur le cancer ou le COVID, et qu'elle doit effectuer des séquences d'ADN ou d'ARN, elle doit passer par environ 50 étapes de traitement. Chaque étape peut prendre des heures et ils doivent enregistrer une sauvegarde de l'état des résultats de calcul intermédiaires pour plusieurs raisons : d'abord, pour réexécuter ou reproduire les résultats, et ensuite, pour comparer les résultats s'ils modifient certaines données. Les points de contrôle sont enregistrés sur le stockage, ce qui prend entre cinq et 30 minutes. Dans de nombreux cas, cela peut être plus chronophage que le calcul lui-même. Ainsi, si un travail prend 24 heures, ils peuvent utiliser huit heures pour le calcul et 16 heures uniquement pour effectuer ces tâches d'E/S en sauvegardant ces états intermédiaires.\n\nPlutôt que d'effectuer des entrées/sorties (IO), MemVerge utilise un instantané après chaque étape et le capture sur une mémoire persistante Optane. Au lieu de 16 heures d'entrées/sorties, ce processus peut prendre une minute. C'est la nouvelle façon de faire des entrées/sorties ; vous n'avez pas besoin de réaliser la sérialisation ou la désérialisation pour ouvrir un fichier, lire, écrire, etc. Tout ce que vous avez à faire est de prendre un instantané.\n\nBien que cela nécessite beaucoup de mémoire, avec MemVerge, la mémoire est plus grande qu'auparavant et elle continuera à s'améliorer à mesure qu'Intel innove. Deux autres fonctionnalités aident à résoudre ce problème. Premièrement, des instantanés sont pris périodiquement sans créer de copies complètes de l'état de la mémoire ; ils ne sont que des pages de changement, ce qui minimise l'utilisation supplémentaire de la mémoire. Deuxièmement, MemVerge peut conserver jusqu'à 256 couches d'instantanés en mémoire, mais en même temps, vous pouvez exporter ces instantanés vers des serveurs de stockage ou vos propres systèmes de stockage. Cela se fait sans interruption ni impact sur votre application en cours d'exécution.\n\nEssentiellement, vous créez un enregistreur de mémoire car au lieu de ne faire fonctionner votre application que vers l'avant, vous pouvez aussi la faire fonctionner vers l'arrière presque instantanément. C'est une nouvelle expérience.\n\nLa génomique n'est que le premier exemple parmi de nombreuses charges de travail qui pourraient bénéficier de cette technologie.\n\nPuisque MemVerge est une startup, ils se concentrent étroitement sur trois domaines : les fournisseurs de services cloud, les applications financières à grande mémoire, les travaux de génomique et de pipeline de science des données connexes. Ces cas d'utilisation démontrent tous la puissance de la combinaison de la mémoire persistante Optane et du logiciel MemVerge.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT51-fr","image":"./episodes/edt-51/fr/thumbnail.jpg","lang":"fr","summary":"Darren Pulsipher, Chief Solution Architect, Intel, parle à Charles Fan, PDG de MemVerge, des cas d'utilisation de leur logiciel qui utilise la mémoire persistante Intel Optane de manière innovante, en supprimant le goulot d'étranglement entre la mémoire et le stockage."},{"id":123,"type":"Episode","title":"Télémétrie dans le Cloud","tags":["multicloud","optimization","workloadplacement"],"body":"\r\n\r\n## Avantages des architectes de solutions cloud (CSAs)\n\nLe rôle de CSA est très demandé dans l'industrie en ce moment, et Intel embauche une équipe de CSA pour apporter de la valeur à leurs clients. Les CSA peuvent aider les clients à éviter la mentalité de transfert direct qui entraîne des coûts inutilement élevés. Ces nouveaux CSA apportent une perspective externe et se connectent avec la communauté plus large des CSA pour résoudre les problèmes.\n\n## Phases de télémétrie.\n\nLa télémétrie dans le cloud ne devrait pas être reléguée à votre fournisseur de services cloud. Par exemple, Amazon Web Services (AWS) a connu une panne majeure sur la côte Est en novembre dernier et leurs outils ne fournissaient pas de rapports, de sorte que de nombreux clients ne savaient même pas qu'ils étaient hors service. Les organisations ont besoin de leur propre télémétrie pour la surveillance.\n\nLa première phase consiste à ne pas surveiller dans le cloud. La deuxième phase consiste à exposer la télémétrie où vous comprenez ce que vous obtenez, par exemple, le processeur, la mémoire et le réseau, mais c'est à un niveau superficiel. La phase suivante est la surveillance et l'action, où vous recevez des notifications sur les ralentissements, les taux de transactions, les taux de réponse, etc. La phase suivante est plus avancée dans la pile où le cloud intelligent moderne contrôle et prédit pour la résolution des problèmes. Enfin, il y a l'automatisation complète. C'est là que les choses fonctionnent par elles-mêmes, écoutent, réagissent, puis informent par la suite.\n\nUn exemple d'automatisation complète serait un centre de données qui contrôle un système de chauffage, ventilation et climatisation (CVC), détectant et réduisant ou augmentant la température dans différentes parties d'un bâtiment. Pour les charges de travail, un cloud intelligent peut déplacer les charges de travail afin de maximiser la valeur en termes de capacité et de performance.\n\n## Approches actuelles et limitations\n\nDe nombreuses fois, cependant, les clients des prestataires de services cloud relèguent l'automatisation au niveau de l'infrastructure et oublient la couche de charge de travail. Les clients ont besoin d'informations au-delà des seules données essentielles; ils ont besoin d'une télémétrie approfondie et riche pour savoir ce qui se passe réellement. Il est une erreur de compter sur le prestataire de services cloud pour avoir votre intérêt supérieur à l'esprit en ce qui concerne la télémétrie, et cela peut se traduire par des coûts élevés.\n\nCertains prestataires de services cloud s'ouvrent de plus en plus. Actuellement, AWS est un véritable changeur de jeu dans la télémétrie qu'ils fournissent. Espérons que d'autres emboîteront le pas.\n\n## Collectons les bonnes télémétries...\n\nChoisir les bonnes instances est important. Tous les cœurs ne sont pas créés égaux ; différents cœurs sont utilisés pour différentes tâches, et il est important de comprendre ce qu'ils sont pour obtenir les meilleures performances et le meilleur prix.\n\nQuel type d'informations pouvez-vous collecter ? La conception de produit d'Intel inclut des unités de surveillance des performances (PMU). Ce sont des compteurs de sous-niveau qui fournissent des informations sur les transactions, les retards, la latence et les goulots d'étranglement. Il existe trois camps différents dans les PMUs : le cœur, hors du cœur et non lié. Ces PMUs collectent des informations sur le CPI, l'utilisation, la fréquence et TMAM. Toutes ces données sont disponibles via AWS.\n\nEn utilisant des métriques, la télémétrie réelle est un outil pour vous aider à optimiser vos charges de travail. Vous pourriez comparer comment vos charges de travail fonctionnent dans votre propre centre de données, par exemple, à AWS dans ces cas, et exécuter des métriques pour déterminer sur quelle plateforme les charges de travail devraient être hébergées.\n\n## La pertinence de l'informatique\n\nSuperposer la télémétrie avec le benchmarking est une solution ultime. Grâce au benchmarking, vous pouvez connaître votre rendement, et grâce à la télémétrie, vous pouvez examiner l'IPC, l'utilisation, la fréquence et avoir un tableau de bord complet de ce qui se passe. Vous souhaitez faire la même chose dans le cloud, plutôt que de simplement déposer des charges de travail sur une instance apparemment moins chère.\n\nLes professionnels de l'informatique ne devraient pas craindre de perdre leur emploi car les choses se déplacent vers le cloud. Au lieu de cela, ils devraient transférer leurs compétences pour apprendre le benchmarking de télémétrie plutôt que d'avoir une mentalité de déplacement pur et simple. Devenir compétent dans l'utilisation du cloud implique également l'utilisation de fonctionnalités spécifiques au cloud telles que Kubernetes et les conteneurs. La télémétrie fonctionne également dans ces domaines. Avec l'outil Intel's C Advisor, vous pouvez obtenir une télémétrie détaillée telle que les données de base et hors base de vos conteneurs.\n\nBien que cela puisse sembler un peu intimidant lorsque l'on regarde tout ce qui est possible dans le cloud, il est préférable de commencer petit. Examinez les bonnes applications en fonction des risques. Cataloguez vos applications, examinez la hiérarchisation des applications, puis commencez à les déplacer vers le cloud en regroupant des fonctions et des applications similaires. En découvrant de nouveaux services et en apprenant de nouvelles applications, considérez l'architecture qui les soutient et posez les bonnes questions afin de devenir un architecte technique plus informé.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT52-fr","image":"./episodes/edt-52/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, Chief Solution Architect, Intel, parle à Josh Hilliker, Directeur des architectes de solutions cloud chez Intel, de l'utilisation de la télémétrie dans le cloud pour maximiser la valeur et l'efficacité."},{"id":124,"type":"Episode","title":"DevOps avec rapidité tout en réduisant les risques","tags":["devops","compute","technology","process","devsecops","cybersecurity","multicloud"],"body":"\r\n\r\nLes trois domaines clés à équilibrer dans la livraison de logiciels sont la vélocité, le risque et la qualité. La plupart peuvent gérer un ou deux de ces aspects, mais ajouter le troisième peut devenir difficile. Par exemple, vous pourriez être en mesure de livrer à une certaine vélocité grâce à un excellent système de déploiement continu (CD), mais une fois que vous introduisez des vérifications de conformité et de politique, vous êtes confronté à un défi. Faut-il s'arrêter et effectuer ces vérifications, ou peut-être intégrer des résultats provenant d'un autre outil ? Soudainement, votre vélocité en souffre.\n\nOpsMx travaille pour aider les gens à automatiser ces points de décision. L'automatisation est l'élément le plus important pour maintenir la vélocité tout en augmentant les deux autres piliers, le risque et la qualité.\n\n## Solution OpsMx : Livraison de logiciels entièrement automatisée.\n\nLa automatisation fonctionne comme une agrégation de données, en récupérant les résultats des différents outils de la chaîne d'outils, puis en ayant un mécanisme comme un gestionnaire de politique qui fournit des résultats attendus sur les coches des outils tels que BlackDuck. Si cela semble correct, le pipeline ne s'arrête pas, comme c'est actuellement le cas, pour une vérification humaine.\n\n## Une livraison plus rapide des applications augmente les taux d'échec.\n\nLa vitesse de livraison des applications est passée de hebdomadaire ou mensuelle à horaire, ajoutant une pression pour produire rapidement. Cependant, lorsque vous essayez d'augmenter la vitesse, vous pourriez commencer à perdre le contrôle de la qualité, peut-être en sautant certaines vérifications de risques. Le résultat net est que plus vous essayez de vous déplacer rapidement, plus vous êtes susceptible de rencontrer une défaillance.\n\n## Points de friction et espaces problématiques\n\nAvec une utilisation stratégique de l'automatisation, vous avez la capacité d'augmenter la vitesse sans prendre de risques. La vérification continue peut réduire une tâche de trois ou quatre heures à cinq ou dix secondes. Au lieu d'utiliser du travail humain pour analyser les journaux à la recherche de comportements anormaux qui auraient pu passer un contrôle binaire réussi/échoué, la vérification continue peut le faire automatiquement, permettant aux personnes de se concentrer uniquement sur les cas exceptionnels.\n\n## Que signifie l'automatisation dans ce contexte?\n\nLa pièce suivante utilise les outils requis par l'équipe de sécurité et de conformité, tels que BlackDuck. Grâce à l'automatisation, il n'est pas nécessaire d'arrêter le processus pour demander à quelqu'un d'interpréter les résultats avant de continuer car cela se fera automatiquement.\n\nLes autres pièces sont des politiques à un niveau supérieur si vous êtes dans une industrie réglementée avec plus de vérifications, ou quelque chose d'aussi simple qu'un détaillant qui, par exemple, ne souhaite pas publier de nouveaux logiciels deux semaines avant Noël. Ces vérifications peuvent être automatisées pour éliminer le besoin d'approbation humaine, permettant ainsi à l'ensemble du processus de se poursuivre s'il n'y a pas d'exceptions. L'idée est de supprimer autant de portes humaines que possible pour permettre au processus de s'écouler.\n\nLe jeu principal d'automatisations de contrôle et de politiques est réutilisable à travers les pipelines ; vous n'avez qu'à personnaliser ou en créer pour les critères spécifiques à votre cycle de publication.\n\n## Solution OpsMx : Livraison de logiciel entièrement automatisée\n\nOpsMx vous donne la possibilité de créer ces pipelines et les automatisations d'analyse des journaux. Spinnaker est l'outil actuel, mais la vision d'OpsMx est d'être aussi agnostique que possible en matière de déploiement continu (cd), permettant l'utilisation de tous les outils de cd tels que Jenkins et Microsoft Azure. OpsMx utilise la partie cd pour l'automatisation, mais se situe au-dessus de cela pour aider à prendre des décisions automatisées.\n\nUne partie croissante de l'outil est l'apprentissage automatique pour aider à comprendre ce qui est considéré comme une référence pour une application particulière par rapport à un comportement anormal. Il existe également un modèle d'apprentissage supervisé où un professionnel DevOps, un ingénieur ou un propriétaire de produit peut spécifier le comportement anormal attendu dans le contexte de l'application.\n\nLe système permet également une traçabilité. Toute modification ou exception est documentée. Il existe une piste de vérification complète de tout ce qui se passe, depuis l'approbation d'une exception par qui que ce soit, jusqu'au déploiement de l'artefact utilisé et à l'image de base utilisée. Que cela soit exécuté sur le cloud ou sur site, vous pouvez voir l'ensemble du processus comme une entité unique.\n\nUn autre aspect de la visibilité fournie par OpsMx est une sorte de carte de ce qui est déployé où, telle que la série de microservices actuellement en cours de test qualité, ce qui se trouve dans certaines étapes, ou ce qui est en production, et ensuite pour chacun de ces éléments, vous pouvez approfondir et obtenir une vue historique. Vous pouvez cliquer sur n'importe quelle version donnée et toutes les informations d'audit sont immédiatement accessibles.\n\nDe plus, le prochain objectif sur lequel OpsMx travaille est l'efficacité des artefacts qui sont capturés afin que la chaîne d'approvisionnement, ou la lignée, soit transparente.\n\nOpsMx utilise désormais une technologie basée sur un agent qui peut interagir avec, par exemple, des ressources situées derrière votre pare-feu. L'agent agit comme un proxy vers la couche d'intelligence afin que les données puissent y être collectées. Il n'y a pas de soucis à avoir concernant l'ouverture ou l'exposition des ports du pare-feu. Cela fonctionne de la même manière avec les fournisseurs de cloud : l'agent peut être déployé à l'intérieur du VPC et vous n'avez plus à risquer de mettre des clés et des secrets dans une application basée sur le cloud. L'agent agit simplement comme un proxy de sorte que la partie autorisée reste toujours à l'intérieur du VPC, sécurisant ainsi la manière dont les informations sont collectées.\n\nAvec cette nouvelle orientation, l'industrie du DevOps dans son ensemble va subir une refonte en matière de sécurité et d'auditabilité.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT53-fr","image":"./episodes/edt-53/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Robert Boule, responsable de l'ingénierie des solutions chez OpsMx, rejoint Darren pour parler d'améliorer la vitesse sans augmenter le risque dans le processus DevOps. Les trois domaines clés à équilibrer dans la livraison de logiciels sont la vitesse, le risque et la qualité. La plupart des gens peuvent gérer un ou deux de ces éléments, mais ajouter le troisième peut être difficile. Par exemple, vous pourriez être capable de livrer à une bonne vitesse parce que vous avez un excellent système de déploiements continus, mais une fois que vous introduisez des contrôles de conformité et de politique, vous êtes confronté à un défi. Est-ce que vous vous arrêtez et vérifiez ces choses, ou peut-être importez-vous des résultats d'un autre outil ? Soudainement, votre vitesse en souffre."},{"id":125,"type":"Episode","title":"Personnes et Processus dans la Transformation Numérique","tags":["multicloud","organizationalchange","change","people","hsbc","cio"],"body":"\r\n\r\nAnn a commencé sa carrière en tant que programmeuse, puis s'est tournée vers la gestion de programmes. Elle possède également de l'expérience dans les acquisitions et les ventes d'entreprises. Elle était Responsable mondiale des données chez HSBC et est devenue Directrice des systèmes d'information en septembre 2016. Tout au long de sa carrière, elle a mené d'importants programmes de transformation organisationnelle, notamment la mise en place de nouveaux systèmes de prêt hypothécaire et de nouveaux systèmes bancaires centraux.\n\nCertains des défis de ces transformations étaient de poser les bonnes questions lors des évaluations. Dans le cadre d'un programme de transformation hypothécaire, elle a d'abord posé des questions sur les finances : Sommes-nous hors budget ? Sommes-nous dans les temps ? Avons-nous besoin de demander plus de financement ? Pour l'équipe de direction, elle a demandé si les bonnes personnes étaient en place avec les bonnes compétences pour accomplir les changements. Elle s'est également penchée sur le tableau d'ensemble : En quoi croit l'entreprise ? Quels sont les objectifs commerciaux ? Comprennent-ils clairement pourquoi le programme de transformation est nécessaire et ce que nous essayons d'accomplir ? Elle a également examiné l'état du programme à travers des mesures et des KPI.\n\nPour cette transformation, ils avaient déjà mis en place et commencé un programme, mais Ann est arrivée comme une sorte de consultante interne en affaires. Le programme était axé sur les affaires et piloté par les affaires, mais ils devaient exploiter la technologie pour atteindre ces objectifs et objectifs commerciaux. La technologie et les objectifs devaient être totalement alignés.\n\nDans la dernière transformation qu'elle a dirigée, Ann a pris la tête à peu près trois ans après le début d'un programme de transformation de cinq ans. Elle a repris le rôle de directrice de programme pour les deux aspects, à la fois commercial et technologique. Quatre directeurs des systèmes d'information (DSI) lui rendaient compte, chacun étant responsable d'un domaine d'activité : vente au détail, commercial, gestion de patrimoine et banque d'investissement. Ann les mettait en relation avec leurs partenaires commerciaux et ils se réunissaient régulièrement pour s'assurer qu'ils restaient alignés, car ils cherchaient tous à atteindre le même objectif.\n\nBien que cela pourrait sembler être une pratique commerciale régulière, Ann considérait comme l'un de ses plus grands défis en tant que CIO de mettre l'entreprise sur la même longueur d'onde. Pour des projets aussi importants et complexes que le changement d'un système bancaire central, le développement d'une nouvelle application mobile ou la création d'une nouvelle interface web, par exemple, Ann organisait des réunions plusieurs fois par jour avec les acteurs clés, car il y avait probablement 15 flux de travail différents, et l'intégration était l'aspect clé.\n\nEntrer de l'extérieur et effectuer des changements organisationnels est une position difficile à occuper. Les gens sont nerveux face au changement, particulièrement lorsqu'ils ont fait confiance à leur précédent leader. Avec le temps, cependant, Ann a constaté que les gens se sont ralliés, car ils savaient que l'objectif était d'améliorer l'entreprise et qu'ils adhéraient au projet. Une façon dont Ann a accompli cela était de s'asseoir par terre avec les différentes équipes au lieu de rester dans un bureau. Elle apprenait à connaître les gens et il était normal pour eux de l'avoir autour d'eux et de pouvoir avoir des conversations et soulever des problèmes. Elle obtenait des informations brutes des personnes qui faisaient le travail au lieu de recevoir des informations filtrées par la direction.\n\nUne chose que les leaders oublient souvent est que les membres de l'équipe veulent réussir, et une approche personnelle telle que travailler à leurs côtés permet d'aller bien loin.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ann Madea"],"link":"/episode-EDT54-fr","image":"./episodes/edt-54/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions chez Intel, demande à sa invitée, Ann Madea, ancienne DSI de HSBC, de réfléchir sur le processus des grands changements transformationnels qu'elle a dirigés au sein des organisations."},{"id":126,"type":"Episode","title":"Communication pendant la transformation","tags":["communication","organizationalchange","people","process","hsbc","cio"],"body":"\r\n\r\nLorsque les directeurs des systèmes d'information déploient des solutions, cela implique généralement de nouveaux processus et des changements organisationnels. Dans le programme de transformation le plus récent d'Ann, elle a opté pour le cloud. Ann a embauché un responsable du cloud et a mis en œuvre une stratégie multi cloud avec Google, AWS et Azure. Ce changement majeur a suscité des préoccupations, en particulier parmi ceux qui travaillaient depuis longtemps avec cette infrastructure.\n\nPour soulager l'anxiété et renforcer l'engagement, Ann a vivement encouragé les personnes à commencer à obtenir des certifications pour le cloud, Google, AWS, etc. Elle a emmené son équipe en déplacement pour organiser des réunions avec Google à New York, où ils ont pu constater l'énergie, l'atmosphère et la collaboration qui régnaient sur place. Au fur et à mesure que les personnes obtenaient des certifications, l'excitation pour la formation et la transformation grandissait.\n\nLe nouveau responsable du cloud avait des \"Journées Google\" où les gens venaient présenter leurs cas d'utilisation et comment ils utilisaient différents produits. Ils faisaient de même avec Amazon et Azure. Cela aidait les employés à comprendre l'étendue de ce qu'ils pouvaient faire, à susciter l'enthousiasme et à favoriser la participation.\n\nLa clé d'Ann pour la communication du CIO au milieu d'une transformation est la confiance mais vérification. Trois ou quatre mois après le début de la transformation, on a demandé à Ann de se concentrer sur les évaluations dans des domaines tels que les budgets, les équipes d'examen et les relations à rétablir avec le conseil d'administration et les régulateurs. Cependant, elle a décidé d'approfondir les choses et a découvert qu'il n'y avait vraiment aucun plan montrant comment le programme allait être réalisé. Cela a ensuite entraîné une réévaluation de l'équipe de direction pour faire avancer le programme.\n\nUn directeur des systèmes d'information (DSI) qui met en œuvre une transformation doit être prêt à se salir les mains. Faites confiance, mais vérifiez tout avec plusieurs entretiens et flux d'informations ; une ou deux sources ne sont pas suffisantes. Ensuite, Ann estime qu'il est nécessaire de commencer à communiquer avec transparence sur les problèmes afin d'obtenir le soutien des personnes capables de le fournir.\n\nUn avantage de la transparence est que les personnes qui vous fournissent des informations, comme les programmeurs, ont confiance en vous pour ne pas utiliser ces informations contre eux. L'équipe d'Ann sait qu'elle ne doit pas la prendre de court, et en retour, elle ne surprend pas ses supérieurs. Par exemple, en cas de problème dans le centre de données, elle leur dira ce qu'elle sait et les tiendra informés au fur et à mesure de ses découvertes.\n\nLes employés veulent savoir comment ils contribuent au succès du programme, et non seulement qu'ils doivent coder ces cinq programmes. Une des façons dont Ann a réussi à le faire était en organisant de nombreux conseils municipaux. Elle parcourait également les locaux et tenait chaque semaine une réunion informelle de 30 minutes où les gens pouvaient poser des questions. Chaque fois qu'Ann organisait une grande réunion où des décisions étaient prises, elle réunissait toute l'équipe, des côtés des affaires et de la technologie, et expliquait les décisions et leur impact. Si les gens savent qu'il y a eu une grande réunion mais sont tenus dans l'ignorance, les rumeurs commencent à circuler.\n\nAvec le télétravail actuel, Ann affirme qu'il est également important de vérifier brièvement avec les individus. Des vérifications régulières favorisent de meilleures relations et réduisent l'anxiété de parler avec les supérieurs. Les patrons doivent comprendre ce qui motive les équipes et ce qui préoccupe les gens. Ils ont des familles à nourrir, des enfants à l'université ou peut-être des membres de leur famille avec des problèmes de santé.\n\nBien sûr, tout ne peut pas être discuté avec tout le monde, comme les réductions de personnel ou les promotions, par exemple, mais Ann aime convoquer des réunions de superviseurs appelées \"réunions de personnes\" où ils peuvent discuter de ces questions délicates. Les réunions peuvent également aider les dirigeants à mieux comprendre leur personnel et ce qu'ils cherchent à faire. Il est important d'entendre les points de vue des subordonnés plutôt que seulement des leaders, car ils sont plus à l'écoute des événements quotidiens.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ann Madea"],"link":"/episode-EDT55-fr","image":"./episodes/edt-55/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, deuxième partie de deux, Darren et son invité Ann Madea, ancienne CIO de HSBC, parlent de la communication lors des transformations organisationnelles. Lorsque les CIO déploient des solutions, cela entraîne de nouveaux processus et très probablement un changement organisationnel. Dans le programme de transformation le plus récent d'Ann, ils ont avancé avec le cloud. Ann a engagé un responsable du cloud et a mis en place une stratégie multi-cloud avec Google, AWS et Azure. Ce changement majeur a suscité des inquiétudes, surtout parmi ceux qui travaillaient depuis longtemps avec l'infrastructure."},{"id":127,"type":"Episode","title":"Les organisations de courtage de nuages","tags":["cloudbroker","cloud","compute","technology","people","process","multicloud","organizationalchange"],"body":"\r\n\r\nDave et Kevin ont fondé l'équipe de courtage cloud d'Intel il y a environ sept ans. Les employés d'Intel utilisaient le cloud public et c'était comme le Far West sauvage, avec des gens qui utilisaient simplement leurs cartes de crédit pour y accéder. La sécurité posait problème et les dépenses étaient hors de contrôle. Pour maîtriser ce chaos, ils ont créé une organisation de courtage cloud.\n\nUne approche centralisée était nécessaire pour contrôler les coûts, organiser et créer des contrats d'entreprise avec les fournisseurs, et mettre en place des facturations à travers une seule organisation. Au lieu d'utiliser des cartes de crédit individuelles ou de créer leurs propres bons de commande, les groupes commerciaux d'Intel utilisent un bon de commande principal par le biais de leur service informatique, qui est ensuite facturé aux différents groupes. Dans l'ensemble, cela permet d'économiser de l'argent car Intel dispose maintenant d'un pouvoir d'achat collectif, incluant des opportunités d'économie de coûts plus avancées, telles que l'achat de capacité réservée plutôt que de payer des prix à la demande.\n\nIl a fallu un peu de temps pour convaincre tout le monde de rejoindre l'organisation, certains utilisant encore leurs cartes de crédit pour ouvrir des comptes. Pour remédier à cela, certains fournisseurs de services cloud ont fourni à Intel un rapport sur les comptes ouverts avec des adresses e-mail d'Intel. Au lieu de considérer ces \"fuites\" uniquement comme une violation de la politique, cela a été l'occasion de les sensibiliser aux avantages de l'utilisation du compte Intel central : normes de sécurité déjà en place, support entreprise, formation et rentabilité.\n\nEn plus de ces avantages, Intel a également créé un Centre d'excellence dans le cloud, un forum basé sur la communauté auquel ils demandent aux gens de se joindre lorsqu'ils obtiennent leurs comptes cloud. Sa croissance a été basée sur le terrain, fournissant des informations et des retours aux membres.\n\nDe nombreux développeurs et autres utilisateurs du cloud veulent simplement l'utiliser sans avoir à se soucier de la sécurité ou du coût, par exemple. L'équipe de courtage cloud leur permet de faire cela. Une analogie serait que les services informatiques mettent le développeur dans un bac à sable avec tous les jouets, mais ne lui permettent pas de renverser du sable en dehors du bac à sable ou de jouer avec les jouets à l'extérieur. Cela offre à la communauté des développeurs des comptes sécurisés et un accès fiable chaque fois qu'ils en ont besoin.\n\nIl existe des centaines de services disponibles chez les fournisseurs de cloud public, et ils sortent toujours de nouveaux services et fonctionnalités. Il est difficile pour les groupes d'entreprises d'avoir ou de maintenir une expertise sur tous ces services. Une équipe centrale de courtiers du cloud qui se concentre sur le cloud public et se tient au courant des derniers services peut offrir des conseils et des connaissances sur l'emplacement des différentes charges de travail. La clé du courtage est que les gens viennent vers un entonnoir central et sont redirigés vers les bons services.\n\nIl est non seulement utile d'avoir une équipe de courtage cloud dédiée, mais aussi des personnes au sein de l'équipe qui se concentrent sur des fournisseurs cloud particuliers. Par exemple, chez Intel, alors qu'ils atteignaient une masse critique et que les fournisseurs cloud devenaient plus matures et commençaient à offrir une quantité massive de services, Kevin s'est concentré sur AWS et Dave s'est concentré sur Azure afin de plonger plus en profondeur dans chacun d'entre eux.\n\nUtiliser plusieurs offres de cloud plutôt qu'une seule était une décision naturelle car les groupes d'entreprises avaient des préférences et les différentes charges de travail fonctionnaient mieux avec différents fournisseurs. Les différences entre les fournisseurs étaient plus marquées auparavant. Maintenant, il y a une concurrence plus équitable.\n\nUn courtier de cloud n'est pas seulement quelqu'un occupant un poste technique, mais un couteau suisse. Dave et Kevin sont devenus des experts en tout ce qui concerne le cloud, comme la sécurité et le réseau, et ils ont formé ces équipes alors qu'elles étendaient leur champ d'action du sur site au cloud public. Grâce à une équipe cloud centrale, les autres organisations d'Intel ont pu se former, se développer et croître. L'équipe a été surnommée \"La Colle\" en raison de son rôle central et varié.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Kevin Bleckmann","Dave Shrestha"],"link":"/episode-EDT56-fr","image":"./episodes/edt-56/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, première partie de deux, Darren et les architectes de solutions cloud d'Intel, Dave Shrestha et Kevin Bleckman, discutent de l'importance d'une organisation de courtage cloud. Dave et Kevin ont fondé l'équipe de courtage cloud d'Intel il y a environ sept ans. Les employés d'Intel utilisaient le cloud public, et c'était comme le Far West, avec des gens qui utilisaient simplement leur carte de crédit pour y accéder. La sécurité était un problème et les dépenses étaient incontrôlables. Pour remédier à ce chaos, ils ont créé une organisation de courtage cloud."},{"id":128,"type":"Episode","title":"Organisation de courtage dans le cloud Partie 2","tags":["cloudbroker","change","people","process","compute","organizationalchange","multicloud"],"body":"\r\n\r\n# Considérations pour la sélection du cloud\n\nIl y a sept ans, lorsque Dave et Kevin ont commencé pour la première fois l'équipe de courtiers en nuage d'Intel, il y avait des différences significatives entre les fournisseurs de services de nuage en termes de disponibilité régionale, mais maintenant, avec la maturité, les différences sont moindres. Certains fournisseurs se spécialisent dans des domaines spécifiques tels que l'IA et l'apprentissage automatique ou les services de base de données gérées avancés, et il y a des facteurs de coût, mais ils sont plus similaires que différents. En général, les fournisseurs de cloud deviennent de plus en plus une commodité à mesure qu'ils mûrissent.\n\nLa sécurité est bonne et assez standard parmi les fournisseurs de cloud. Auparavant, Intel utilisait des produits de fournisseurs externes pour gérer la sécurité des services cloud. Les fournisseurs de cloud ont maintenant intégré une grande partie de cela dans leurs plates-formes, et Intel a basculé vers l'utilisation de composants intégrés supplémentaires. Il reste encore beaucoup de travail à faire dans cet espace. Dans certains cas, vous devrez toujours assurer vous-même la sécurité, mais les fournisseurs vont dans la bonne direction. L'utilisation d'outils intégrés n'est pas l'option la plus facile pour les nouvelles entreprises car elle n'est pas entièrement gérée, mais une fois que vous avez les compétences et la maturité nécessaires, au moins les outils sont fournis.\n\nLa gestion des coûts est toujours un art ; beaucoup de personnes sont choquées par leur première facture de cloud. Bien qu'il existe des outils intégrés, des conseillers et des calculateurs de cloud public pour aider, il y a des coûts cachés. Les frais d'entrée et de sortie sont probablement la première surprise, suivis par la surprovision. Intel a un optimiseur de cloud alimenté par Densify qui permet d'identifier les ressources surprovisionnées et de recommander, grâce à une analyse détaillée, les tailles et les familles d'instances les plus optimales.\n\nCertaines charges de travail doivent rester sur site, en particulier pour les grandes entreprises comme Intel qui disposent d'un centre de données sur site important. L'équipe de courtage cloud dispose d'un outil interne appelé l'outil de cadre décisionnel qui peut aider à placer la charge de travail appropriée au bon endroit, et parfois sur site est la meilleure solution.\n\n# Considération de la charge de travail pour le placement dans le cloud public\n\nSans outil pour prendre la décision, il y a cinq domaines à considérer.\n\n*Sécurité: Assurez-vous de connaître les fournisseurs d'identité, le chiffrement, la conformité et les options de connexion unique. Ce sont les mêmes éléments avec lesquels vous traitez normalement en local. Au lieu d'avoir des outils séparés, il est préférable d'avoir des outils qui couvrent à la fois les environnements locaux et le cloud public, afin que vos équipes n'aient pas à réapprendre des outils séparés.\n\nConfidentialité\n\nStabilité de la charge de travail\n\nAffinité des données/gravité des données : Assurez-vous que votre fournisseur de services cloud est disponible dans les régions dont vous avez besoin. Certains fournisseurs de services cloud ont des zones de disponibilité, ou des centres de données, dans toutes leurs régions, tandis que d'autres n'en ont pas. De plus, s'il est nécessaire de connecter une charge de travail à de nombreux systèmes locaux, il n'est pas logique de placer cette charge de travail dans le cloud public.\n\nCoût : Il existe toujours des différences de coût entre les fournisseurs de services de cloud public, surtout avec les instances de calcul importantes. Prenez également en compte les coûts de sortie de données.\n\n## Comment vendre votre service de courtier cloud\n\nL'adhésion au service de courtage cloud peut être difficile pour certains, comme les développeurs qui ont l'habitude d'avoir le contrôle et de créer des instances à leur guise. Il peut également y avoir des problèmes de croissance. Lorsque les employés d'Intel ont obtenu leurs comptes pour la première fois, ils se sont sentis incertains, et ensuite le service de sécurité de l'information envoyait des messages lorsqu'ils faisaient quelque chose de mal, et ils ne savaient pas nécessairement ce qu'ils avaient fait ni comment le corriger. À mesure que le service évoluait et se développait, ces problèmes se sont résolus d'eux-mêmes grâce à l'intervention de l'équipe et à l'utilisation de correctifs automatiques.\n\nLes gens ont constaté que le service était finalement un avantage car il offrait un moyen rapide et facile d'accéder au cloud public avec tout le support dont ils avaient besoin. Après un certain temps, le service a commencé à se vendre par lui-même.\n\nL'une des principales prestations que l'équipe offre est de servir de pont entre les partenaires commerciaux clients d'Intel et les groupes de sécurité de l'information afin de contrebalancer les politiques de sécurité excessivement agressives qui causent des tracas et une trop grande gestion des tickets. L'équipe offre l'équilibre nécessaire qui permet aux développeurs d'avoir suffisamment de liberté pour travailler dans un cadre sécurisé tout en respectant les exigences de sécurité. Les courtiers jouent essentiellement le rôle de médiateurs entre la sécurité de l'information et les développeurs.\n\nL'équipe de courtiers Cloud propose également des formations en faisant venir les fournisseurs Cloud pour organiser des ateliers. Cet avantage a également été généré par le Centre d'Excellence Cloud, où les personnes ont demandé des formations dans des domaines spécifiques, puis l'équipe a négocié avec les fournisseurs.\n\nEn plus des fournisseurs externes, Intel a fait appel à des équipes internes pour mettre en avant ce qu'ils avaient réalisé avec le cloud public afin que d'autres équipes puissent bénéficier de ces connaissances.\n\nLes équipes de courtage de cloud au sein des entreprises peuvent fournir une gamme de services et d'avantages tels que la sécurité et l'efficacité des coûts, surtout à mesure que les services de cloud public se développent et deviennent de plus en plus indispensables aux opérations.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Dave Shrestha","Kevin Bleckmann"],"link":"/episode-EDT57-fr","image":"./episodes/edt-57/fr/thumbnail.png","lang":"fr","summary":"Dans la deuxième partie de cette série en deux parties, Darren Pulsipher, architecte en chef des solutions, ainsi que Dave Shrestha et Kevin Bleckman, architectes de solutions cloud d'Intel, discutent des avantages et des services d'une organisation de courtage de cloud."},{"id":129,"type":"Episode","title":"Compréhension des problèmes de l'Edge jusqu'au centre de données.","tags":["data","edge","compute","edgemere","edgetocloud","cybersecurity","technology","process"],"body":"\r\n\r\n## Intégration de mission.\n\nIl y a de nombreux éléments en mouvement lors de la mise en œuvre des capacités en mission, en particulier au sein du département de la Défense, mais également dans les hôpitaux des anciens combattants, la sécurité intérieure, l'Administration des denrées alimentaires et des médicaments (FDA) et l'Agence fédérale des situations d'urgence (FEMA), par exemple, et ramener les données des dispositifs périphériques sur les stations terrestres, ainsi que les centres de données régionaux et d'entreprise. Les données doivent être utilisables et fiables pour les flux d'analyse importants dans les flux de travail de l'IA et entre les mains des analystes pour prendre des décisions basées sur les données brutes.\n\n## Pilotes pour Edge : Latence, Bande passante, Sécurité, Connectivité\n\nUne partie du puzzle est que les appareils périphériques sont devenus plus sophistiqués et collectent plus de données que nous ne pourrions jamais leur envoyer via la technologie 5G. Les espoirs de voir la 5G conquérir toutes les données et les rendre disponibles de manière transparente au centre de données ne se sont jamais concrétisés avec les avancées des dispositifs IoT.\n\nL'une des architectures originales autour de l'IoT était proposée par Cisco, appelée la brume. L'idée de la brume était que le centre de données connectait la brume aux appareils périphériques, de sorte que certaines opérations de traitement et de connectivité se déroulaient là-bas. Si la connectivité réseau est fiable et constante, cela fonctionne bien avec une bande passante suffisante. Cependant, la quantité de données générées actuellement en périphérie par ces organisations dépasse la quantité de bande passante disponible.\n\nAvec l'IoT, seule une partie des données est déplacée vers le centre de données, de sorte que généralement, la valeur des données ne se produit que lorsqu'elles sont analysées là-bas. Le problème est que le centre de données ne peut pas stocker et absorber toutes les données massives. Même en les poussant dans le cloud, le problème n'est pas résolu car le cloud ne peut pas traiter toutes les données qui sont en attente au niveau de la source. Ainsi, nous voulons nous éloigner de l'envoi de toutes les données vers le centre de données pour collecter la valeur, et plutôt, rapprocher autant que possible la valeur des données de la source, réduisant ainsi la quantité de volume de données revenant au centre de données.\n\nBien sûr, toutes les données ne peuvent pas être descendues jusqu'au périphérique; il doit y avoir une corrélation entre différents appareils périphériques. La valeur doit se trouver dans un endroit plus centralisé, pas nécessairement dans le centre de données principal, mais peut-être dans l'un de ces brouillards intermédiaires ou centres de données régionaux. L'essentiel est de déplacer les données de manière intelligente et de pousser la valeur des données aussi près du périphérique que possible de manière répétable et durable. Ce faisant, nous pouvons réagir beaucoup plus rapidement au périphérique.\n\n## Couche physique commune\n\nPour surmonter certains de ces problèmes, nous avons d'abord besoin d'une couche physique commune. Cela signifie que c'est commun depuis le centre de données à travers les couches de brouillard jusqu'aux appareils périphériques ; il y a une seule façon de gérer et de contrôler les appareils et d'obtenir de l'aide de leur part de manière fiable et commune. Cela ne signifie pas nécessairement la même machine, mais un appareil viable minimal avec une interface commune. Un autre avantage de la couche physique commune est que si vous écrivez du code pour une application, il peut s'exécuter n'importe où dans cet écosystème. Intel dispose d'une excellente technologie pour cela, comme oneAPI, qui fait une grande partie du travail afin que vous puissiez écrire du code une fois, compiler les binaires pour les différents types d'appareils, les pousser vers la couche physique commune et les exécuter de manière appropriée. En résumé, les avantages sont un modèle d'exploitation commun, un modèle de sécurité commun et un mode d'exploitation \"écrire une fois, exécuter n'importe où\".\n\n## Infrastructure Définie par Logiciel\n\nSDI s'applique au centre de données dans les clouds privés et publics avec leurs APIs définies par logiciel. Avec SDI à la périphérie, nous obtenons des moyens communs de déplacer des données. Nous pouvons provisionner des ressources à la périphérie dans le centre de données à tout moment, et nous pouvons déplacer des données de cette manière de manière plus fluide.\n\n## Couche de gestion d'information distribuée\n\nNous devons être plus intelligents en matière de gestion et de classification des données, en déplaçant les données uniquement là où elles seront traitées, que ce soit en périphérie, dans un centre de données régional ou dans le cloud. Les aspects importants sont le catalogage et la réutilisation des données, ainsi que l'intégration des exigences de conformité et de sécurité. L'avantage de cette couche de gestion de l'information distribuée est que vous transférez moins de données vers le centre de données, déplacez moins de données et valorisez davantage la périphérie.\n\n## Couche de gestion des services\n\nAfin de véritablement étendre la valeur jusqu'au bord, nous devons être en mesure de déployer des applications jusqu'au bord. C'est là qu'intervient une couche de gestion de services, ou un écosystème de conteneurs. Cela permet de déployer des microservices de manière reproductible et fiable jusqu'au bord, dans le brouillard, dans le centre de données ou dans le cloud. Par exemple, si un centre de données régional tombe en panne, vous n'avez pas à compter uniquement sur celui-ci pour que la grille de services continue de fonctionner.\n\n## Couche de service d'application\n\nUn niveau de service d'application coordonne les différentes applications afin que vous puissiez créer des flux de travail qui génèrent une réelle valeur commerciale à partir des données. Se contenter de déplacer les données ou de les faire passer à travers un moteur d'analyse ne suffit pas. Les données doivent passer du moteur d'analyse à une station de travail pour analyste. Certains outils de cette couche pourraient être l'automatisation des processus robotiques et les pipelines DevOps. C'est également ici que vous pouvez imposer la sécurité et la conformité au niveau de l'application.\n\n## Sécurité et couches d'identité\n\nL'aspect clé de la couche d'identité est d'établir la confiance entre les entités qui sont correctement identifiées. Nous devons comprendre qui accède à quoi et quels appareils accèdent à quelles données, quand, et où. L'identité est étendue au-delà de l'utilisateur typique et englobe l'identité des applications, des données, des appareils périphériques, de la brume (fog), des centres de données et du cloud.\n\nLe jumeau de l'identité est la sécurité. Ici, nous avons la détection, la remédiation, le chiffrement et l'établissement de la racine de confiance. Cela résulte en fiabilité, données fiables et conformité. Maintenant, les données intelligentes peuvent être envoyées vers la périphérie, puis remontées jusqu'au centre de données, mais vous ne déplacez pas de grandes quantités de données brutes, seulement ce dont vous avez besoin de manière sécurisée.\n\n## Aperçu global\n\nPour avoir une architecture edge to cloud réussie et répétable, tous ces différents éléments sont nécessaires. Nous avons constaté que certaines organisations construisent une architecture edge to cloud spécifique, mais se retrouvent bloquées lorsqu'elles déploient une nouvelle fonctionnalité dans ce domaine. Par exemple, si elles codent en dur les données résidant dans le edge car elles seront toujours traitées dans le edge, ou dans le centre de données pour une application toujours traitée dans le centre de données, cela entraîne une rigidité. Cela augmente également le temps nécessaire pour déployer de nouvelles fonctionnalités, peut-être des années au lieu de mois. Si nous tirons les enseignements des déploiements d'applications edge to cloud à maintes reprises et commençons à généraliser, nous constatons rapidement qu'ils s'inscrivent dans l'une des couches que nous avons identifiées.\n\nPour plus d'informations, consultez cet article (incluez le lien) sur la vue d'ensemble de cette architecture de l'edge au cloud. Nous ne prescrivons pas ce qui convient à ces boîtes, mais l'essentiel est de comprendre les cas d'utilisation qu'elles englobent. Nous avons des idées sur ce qui se trouve dans chaque couche et nous mettons en place des écosystèmes pour répondre aux besoins uniques de votre organisation dans ces couches.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT58-fr","image":"./episodes/edt-58/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte en chef des solutions chez Intel, décrit les problèmes courants rencontrés dans les architectures allant des périphériques informatiques au centre de données, qu'il a observés et discutés avec des clients du secteur public. Il expose l'architecture idéale pour résoudre ces problèmes."},{"id":130,"type":"Episode","title":"Évolution de la vie privée des données","tags":["cybersecurity","data","dataprivacy","privacy","process","people","healthcare","sutterhealth"],"body":"\r\n\r\nAprès avoir obtenu son diplôme de droit, Jeremy est entré directement dans le JAG Corps de l'armée de l'air américaine et a servi en tant qu'officier en service actif pendant neuf ans. Une de ses responsabilités était d'être gestionnaire de dossiers pour deux bureaux différents. Il a ensuite orienté sa carrière vers le domaine de la santé lors de sa dernière affectation militaire en tant que conseiller juridique pour les hôpitaux du Nord-Est. Après avoir quitté le service actif (il est toujours réserviste), il est devenu conseiller juridique dans un hôpital et travaille maintenant chez Sutter Health.\n\nBien que le cadre juridique soit le même en ce qui concerne la protection des données, il existe quelques différences entre le gouvernement et le secteur privé : l'approche, les objectifs et les réglementations applicables. Dans le gouvernement, la protection des données signifie préserver la confidentialité des informations grâce à la Loi sur la protection des renseignements personnels ; garder les informations confidentielles secrètes. Bien qu'il puisse y avoir des exceptions, notamment avec la Loi sur la liberté d'information, les systèmes sont conçus de manière à protéger les informations par défaut et non à les divulguer.\n\nDans le domaine de la santé, depuis la loi HIPAA en 1996, suivie de la loi HITECH quelques années plus tard, et maintenant avec l'ONC, les données sont envoyées dans les foyers, dans les appareils des patients, et les patients peuvent autoriser l'accès à des tiers. \"Accès approprié\" est probablement une meilleure description que \"confidentialité des données\".\n\nLe mouvement des droits à la confidentialité consiste à accorder aux individus un accès et un contrôle accrus. En tant que patient, vous avez non seulement le droit de consulter vos informations, mais vous pouvez également demander à votre fournisseur de soins de santé de les communiquer à un tiers : un avocat, un ami, un autre professionnel de la santé, etc. Un patient peut également spécifier le support de diffusion, qu'il s'agisse d'un format papier ou électronique. Ainsi, de nombreux droits sont accordés aux patients.\n\nCes droits pour les individus de contrôler leurs propres données ne se limitent pas à la santé. Nous observons cela dans le mouvement actuel avec le RGPD et les lois récemment adoptées au Brésil, au Canada et en Chine, ainsi que dans certains États américains tels que la Californie, Washington et la Virginie.\n\nÀ mesure que l'avenir de la confidentialité des données se concentre davantage sur les droits individuels d'accès, cela va changer la manière dont les organisations peuvent suivre les choses. Les grandes entreprises comme Google et Facebook proposent maintenant des options permettant aux utilisateurs de supprimer leurs données ou d'empêcher leur vente de différentes manières. Les dispositifs de suivi tels que les cookies traditionnels ne seront plus aussi pertinents, il faudra donc trouver autre chose pour aider les annonceurs ciblés.\n\nBeaucoup de données, bien sûr, comme les données d'emploi sont déjà réglementées. Le contrôle individuel des données n'est pas un droit absolu ; les entreprises ont besoin de données pour fonctionner, donc elles pourront en garder certaines, mais cela deviendra plus réglementé. Aux États-Unis, nous aurons plus de complexité et plus de problèmes avant d'avoir une standardisation. Nous avons 50 États, chacun avec leurs propres réglementations.\n\nIl y a autant de lois qu'il y a de définitions d'informations personnelles, ce qui peut créer un conflit. Sutter, par exemple, possède de nombreux hôpitaux dans le nord de la Californie, ainsi que quelques établissements auxiliaires à Hawaii, en Oregon et en Utah. Sutter doit constamment se tenir au courant des réglementations de ces États, mais en cas de violation, c'est l'État où résident les personnes affectées qui entre en jeu. Parfois, les lois sont rédigées de telle manière que Sutter doit se conformer à la loi dans le lieu de résidence des patients plutôt qu'à l'endroit où se trouve l'entreprise, ce qui devient complexe.\n\nParfois, il est judicieux de sous-traiter ce genre de problèmes, et toute une industrie légale émerge pour aider les entreprises à naviguer dans les réglementations en matière de confidentialité et de sécurité de l'information.\n\nDu côté informatique, la sécurité des données signifie limiter les personnes ayant accès aux informations. Avec la confidentialité des données, il s'agit d'ouvrir les portes à l'accès. Bien sûr, il existe un processus de validation pour déterminer qui a accès, mais il y a un équilibre à trouver entre la sécurité et la confidentialité, ce qui peut créer beaucoup de travail à la fois pour les côtés juridique et opérationnel.\n\nLes organisations qui développent n'importe quel type d'applications traitant les données personnelles des personnes doivent comprendre que les lois sur la confidentialité varient d'un pays à l'autre, ainsi qu'à l'intérieur de chaque état, et les conséquences liées à l'utilisation et au stockage de ces données.\n\nJeremy, ainsi que les équipes de confidentialité et de sécurité, collabore avec les équipes techniques, parfois dès la phase de conception, pour s'assurer que tout est conforme à la réglementation. Par exemple, il discutera avec l'équipe chargée de la construction des portails patients pour vérifier si leurs projets respectent les réglementations. De plus, il aide à répondre aux questions concernant le type de base de données le plus adapté ou s'il existe un fournisseur de cloud qui peut être mis en place en conformité. Jeremy constate que plus il se forme et se perfectionne sur les aspects techniques, plus il peut être utile dans le processus.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jeremy Harris"],"link":"/episode-EDT59-fr","image":"./episodes/edt-59/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte en chef des solutions chez Intel, discute de la signification réelle de la vie privée des données et de son orientation future avec Jeremy Harris, avocat général adjoint - Vie privée/Sécurité de l'information, chez Sutter Health."},{"id":131,"type":"Episode","title":"Gestion du télétravail à pleine capacité","tags":["telework","remoteworker","vdi","mfa","cybersecurity","people","process"],"body":"\r\n\r\n## Modes de fonctionnement des télétravailleurs\n\nLes modes de fonctionnement que nous voyons les gens utiliser au sein du Département de la Défense et également dans le secteur privé sont les suivants :\n\nAppareil en tant que terminal passif (VDI - Infrastructure de Bureau Virtuel)\n\n* Appareil faisant partie du réseau interne (VPN - Réseau privé virtuel)\n\n* Dispositif en tant que portail vers les services\n\nLa plupart des travailleurs utilisent plusieurs modes de fonctionnement, il est donc nécessaire de prendre en charge différents types à travers l'infrastructure.\n\n## Appareil en tant que terminal passif\n\nDans ce mode, le travailleur exécute son bureau sur un serveur situé dans le centre de données et utilise son ordinateur portable pour se connecter au \"bureau virtuel\". Fondamentalement, il utilise son ordinateur portable ou un autre appareil comme un terminal \"muet\". Il y a quelques problèmes avec VDI, y compris la scalabilité, la congestion réseau, la latence et la redondance. En raison de ces problèmes, une session VDI peut être assez coûteuse. Cependant, parmi les trois options, VDI est la plus sécurisée.\n\n## Dispositif faisant partie du réseau interne\n\nCe mode est probablement le plus courant. Le travailleur connecte son appareil au VPN afin de travailler sur le réseau comme s'il était connecté au bureau. Un avantage est la mobilité car les travailleurs peuvent se connecter à n'importe quel appareil tel qu'un téléphone, une tablette ou un ordinateur portable. Un autre avantage est qu'il peut y avoir différents segments de VPN pour différentes classifications de données. Un inconvénient est que les travailleurs doivent toujours être connectés pour pouvoir travailler. Et bien qu'il n'y ait pas autant de trafic réseau qu'avec un VDI, il y aura quand même une certaine congestion du réseau lorsque les données circulent.\n\n## Appareil en tant que portail vers des services\n\nDe plus en plus de personnes s'orientent vers ce mode. Le travailleur utilise son appareil pour se connecter aux services dans le cloud, publics ou privés. Le principal service utilisé par les gens est Office 365, qui permet aux organisations d'utiliser des services cloud au lieu de leurs propres services, ou en combinaison avec eux. L'efficacité, la performance et la possibilité de travailler de manière déconnectée à certains moments sont des avantages. Les inconvénients incluent la dépendance à un tiers... (Je ne suis pas sûr comment finir cela - l'adoption/la migration et la sécurité)\n\n## Obstacles émergents\n\nLes goulots d'étranglement ont émergé à mesure que le nombre de personnes travaillant à domicile est passé de 15 % à plus de 85 % ciblés. Pour les deux premiers modes d'opération, la scalabilité du VPN peut constituer un goulot d'étranglement majeur. La bande passante limitée est également une grande source de goulots d'étranglement. Une solution à ces problèmes consiste à exploiter des services cloud pour atténuer la pression. Au niveau humain, les opérations informatiques et les bureaux d'aide sont submergés car les travailleurs se connectent depuis leur domicile pour la première fois.\n\n## Évolutivité des VPN\n\nIl existe plusieurs solutions à court terme pour aider à atténuer ces goulots d'étranglement. La priorité donnée à l'accès des utilisateurs peut être efficace, que ce soit en fonction des horaires ou de la priorité de la mission. Il est également nécessaire de former les employés à s'adapter à cet environnement.\n\nPour garantir la mise à l'échelle à long terme, les organisations devraient migrer vers des solutions SaaS en utilisant des ordinateurs portables en mode portail.\n\n## Bande passante vers les sites.\n\nUne des meilleures choses que les organisations peuvent faire à court terme est de déterminer combien d'utilisateurs VDI vous avez actuellement et de voir si vous pouvez les faire travailler sur le réseau ou dans le mode de fonctionnement des outils de collaboration. Cela réduira considérablement la charge sur la bande passante. Vous devrez peut-être également augmenter les capacités de votre réseau après avoir évalué comment vos utilisateurs distants travaillent. L'éducation est, encore une fois, une partie essentielle de cette transition afin que les travailleurs utilisent les meilleures pratiques, par exemple en se déconnectant du VPN lorsqu'ils ne l'utilisent pas et en configurant les outils de sauvegarde pour fonctionner pendant les heures creuses. À long terme, nous suggérons une architecture cloud multi-hybride qui vous donne la possibilité d'exploiter les fournisseurs de services cloud pour la bande passante du réseau et la capacité de rafale, tout en optimisant les coûts et les capacités.\n\n## Services d'hébergement évolutifs\n\nPour faciliter la mise en œuvre d'architectures évolutives à court terme, il existe plusieurs références intéressantes, notamment Outlook Web Access (OWA) et les architectures de référence VDI.\n\nEncore une fois, à long terme, nous vous recommandons de passer à une infrastructure de cloud hybride multiplateforme pour l'élasticité, la capacité, les performances prédictives, la conformité et la sécurité.\n\n## Opérations informatiques\n\nComment pouvons-nous renforcer l'équipe du service d'assistance qui est probablement submergée ? Une idée consiste à mettre en place des FAQ en ligne pour faciliter la référence des travailleurs. Les solutions contribuées par la communauté des utilisateurs et modérées par l'équipe informatique peuvent également être utiles. Idéalement, les organisations devraient utiliser un système de gestion des tickets pour identifier les goulots d'étranglement et optimiser les processus. De plus, tout ce qui peut être automatisé pour éviter les tâches répétitives devrait l'être grâce, par exemple, à l'automatisation des processus robotiques (RPA) ou à des scripts supplémentaires. Une solution à plus long terme pourrait consister en la mise en place de chatbots IA en tant que service d'assistance informatique en libre-service. Ils permettent de réduire rapidement les solutions en ligne en utilisant des mots-clés ou de recommander un contact.\n\n## Conclusion\n\nFinalement, Intel souhaite voir les organisations réussir pendant cette période difficile où les travailleurs passent du bureau au travail à distance et font face à beaucoup de stress. Intel peut aider l'industrie, le gouvernement et les secteurs publics. Nous avons du silicium qui fonctionne dans tous ces domaines. Nous avons des partenaires qui fournissent des solutions matérielles et logicielles, et bien sûr, nous vendons des PC et des appareils clients qui permettent aux travailleurs à distance d'être opérationnels.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Tim Flynn","Pete Schmitz"],"link":"/episode-EDT6-fr","image":"./episodes/edt-6/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren, Tim Flynn, retraité vice-amiral de la Marine, et Pete Schmitz, chargé de compte pour la Marine chez Intel, abordent la manière de gérer la croissance explosive des télétravailleurs due à la pandémie de Covid-19. Nous discutons des différents modes que les travailleurs peuvent utiliser pour travailler à distance tout en restant productifs : utilisation de l'appareil comme un terminal stupide, utilisation de l'appareil comme partie intégrante du réseau interne et utilisation de l'appareil comme un portail vers les services. Comprendre ces modes de fonctionnement peut aider à identifier les goulots d'étranglement qui peuvent entraver l'efficacité de votre équipe."},{"id":132,"type":"Episode","title":"Gérer les risques face aux ransomwares","tags":["cybersecurity","ransomware","riskmanagement","process"],"body":"\r\n\r\nAu cours de l'année écoulée, le nombre et la sophistication des cyberattaques ont augmenté. Les trois principaux domaines d'attaques récurrentes sont les rançongiciels, les attaques sur la chaîne logistique et les violations de données.\n\nLes attaques sont devenues plus sophistiquées pour plusieurs raisons. Tout d'abord, à mesure que la sécurité s'améliore, l'adversaire doit continuer à s'améliorer également. Les criminels organisés, les acteurs des États-nations et autres agents de menace reconnaissent qu'il faut plus de sophistication pour compromettre et contrecarrer les contrôles de sécurité. Tout comme les organisations ont des processus de cycle de vie de développement logiciel, la communauté de logiciels malveillants en a aussi. Ils disposent d'outils et de cadres sur lesquels ils se basent et de bons processus pour intégrer la qualité dans leurs systèmes. Différents acteurs achètent, vendent et empruntent du code. Ils apprennent les uns des autres et partagent des informations sur le web sombre. Ils ne sont pas seulement des équipes de pirates désorganisées ; ils fonctionnent davantage comme des entreprises.\n\nCes agents de menace font partie d'une industrie qui pèse plus d'un milliard de dollars. L'argent important alimente le besoin de maturité. Nous ne pouvons plus simplement ajouter la sécurité ; elle doit être intégrée, et intégrée partout, non seulement dans les produits, mais aussi dans l'infrastructure et les processus. C'était l'une des leçons de SolarWinds : même si vous créez un bon produit, l'infrastructure qui le soutient peut être vulnérable.\n\nLes récentes attaques montrent que personne n'est à l'abri. Souvent, les organisations supposent à tort qu'elles sont en sécurité car elles ne font pas partie des services financiers, du gouvernement ou d'autres industries à haute valeur ajoutée. Cependant, les attaques récentes contre des entreprises telles que JBS Foods, McDonald's et Audi ont montré que personne n'est à l'abri. Les entreprises, quel que soit leur produit, dépendent de leur infrastructure numérique pour fonctionner correctement ; l'attaque contre JBS Foods a paralysé la plus grande industrie de transformation de viande au monde.\n\nAucun élément ne peut servir de solution miracle pour prévenir ces attaques. Il y a de l'espoir, mais cela demande beaucoup de travail. Une organisation doit avoir la diligence d'appliquer les bons indicateurs de risque pour mettre en place une sécurité correcte. Si vous ne comprenez pas votre propre risque, aucun nombre de contrôles de sécurité ne pourra fonctionner car vous ne saurez pas si vous les appliquez au bon endroit.\n\nLa clé est de commencer avec le bon ensemble de politiques et de risques pour votre organisation. Une étape fondamentale consiste à refuser toutes les demandes d'accès jusqu'à ce qu'elles soient prouvées dignes, même si votre organisation n'a pas encore pleinement compris comment déployer une architecture de confiance zéro. Cela signifie qu'il y a une porte à chaque entrée, plutôt qu'une clé principale pour tout ce qui se trouve à l'intérieur. Le refus par défaut est un principe fondamental de la confiance zéro.\n\nSi la force d'une entreprise n'est pas dans la cybersécurité, ou si les fonds ne sont pas disponibles pour une équipe interne suffisante, il existe de nombreuses ressources pour aider. Les fournisseurs de services de sécurité gérés (MSP) sont une bonne option, mais il devrait toujours y avoir au moins un expert en interne : un directeur de la sécurité. Cette personne possède le contexte local de l'expérience dans le domaine pour travailler avec le MSP et apporter cette connaissance et la propager dans toute l'organisation. Le MSP gère vos outils de sécurité et configurations, mais vous avez besoin de quelqu'un pour transmettre la sagesse en matière de sécurité aux unités commerciales et informatiques. À la lumière des récentes attaques, une équipe de sécurité n'est pas facultative.\n\nChaque entreprise devrait avoir un plan en place pour une attaque de ransomware. Ce n'est pas le moment de le mettre en place une fois que cela s'est déjà produit. Une base essentielle est de sauvegarder régulièrement vos données. Conservez des copies impeccables des données, des systèmes, des applications et des configurations dans un environnement de stockage hors ligne et hors bande. Six mois de données de sauvegarde propres sont importantes car parfois, un ransomware peut se trouver dans les sauvegardes avant d'être détecté. De plus, assurez-vous que les plateformes ou les serveurs nécessaires pour exécuter votre base de données soient disponibles hors ligne, afin que vous puissiez les mettre en service en cas de basculement ou de modèle de redondance.\n\nC'est essentiellement la planification de la continuité des activités. Tout comme une organisation aurait un plan de continuité en cas de catastrophe physique telle qu'une inondation ou une panne de courant, il devrait également y avoir un plan pour continuer avec les applications d'entreprise critiques afin de reprendre au moins partiellement leur fonctionnement pendant que le problème est résolu.\n\nUn moyen d'accomplir cette redondance est d'être en mesure de se connecter rapidement au cloud lorsque cela est nécessaire, en maintenant les ressources cloud dans un état impeccable, voire même chez un autre fournisseur de services cloud. Une autre étape consiste à avoir un canari dans la mine. Cela signifie déployer des systèmes à travers toute l'entreprise dotés de capteurs extrêmement sensibles. Afin d'éviter les problèmes de performance, de stockage et de vitesse, vous pouvez les déployer dans des endroits stratégiques plutôt que sur l'ensemble du système, afin qu'ils servent de signaux d'avertissement précoces.\n\nCréer un plan à l'avance aidera également à relever le défi de savoir quoi faire au moment de la crise, que ce soit de payer la rançon ou d'appeler le FBI. Le plan devrait être sur papier et impliquer non seulement vos experts techniques, mais aussi vos avocats, PDG, directeurs financiers, etc., et tout le monde devrait y avoir accès. Vous devriez savoir comment acheter des bitcoins et avoir les numéros des bureaux locaux du FBI ainsi que d'autres informations. Mettez en pratique le plan comme un exercice pour vérifier s'il fonctionne, tout comme vous le feriez pour un plan de reprise après sinistre ou de continuité d'activité.\n\nCertaines industries peuvent penser qu'elles sont en sécurité si elles maintiennent séparées leur technologie opérationnelle (OT) et leur technologie de l'information (TI), mais elles ne sont pas réellement séparées. Par exemple, une chaîne de production peut être exploitée avec des machines informatiques, mais une grande partie de ce qui entraîne la chaîne d'approvisionnement, la logistique et l'organisation générale repose sur des systèmes informatiques. Si ces systèmes tombent en panne, rien n'entre ni ne sort. Les systèmes informatiques sont essentiels à la mission et l'apprentissage des attaques récentes nous montre que nous dépendons de la technologie numérique pour toutes nos entreprises.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT60-fr","image":"./episodes/edt-60/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, première partie sur deux, Darren discute des tendances en matière de sécurité avec Steve Orrin, invité régulier et CTO d'Intel, Fédéral. Au cours de l'année écoulée, on a constaté une augmentation du nombre et de la sophistication des cyberattaques. Les trois domaines clés des attaques récurrentes sont les rançongiciels, les attaques de la chaîne logistique et les violations de données."},{"id":133,"type":"Episode","title":"Violations de données et chaîne d'approvisionnement sécurisée","tags":["datbreach","data","process","policy","cybersecurity","supplychain","securesupplychain"],"body":"\r\n\r\nEn plus des attaques médiatisées dans la chaîne d'approvisionnement, comme l'incident avec SolarWinds, il existe d'autres attaques moins connues qui posent tout autant problème.\n\n## Chaîne d'approvisionnement sécurisée\n\nUn exemple est l'extension Chrome The Great Suspender, un outil permettant de s'assurer que la mémoire est utilisée correctement dans les applications Chrome, avec environ deux millions d'utilisateurs. Les fondateurs de cet outil en open source ont vendu leur entreprise à une organisation pour des millions de dollars sans faire preuve de diligence raisonnable. Il s'est avéré que l'acheteur était un groupe criminel organisé qui a ensuite transformé l'outil en logiciel malveillant et en logiciel espion.\n\nIl n'y a pas eu d'attaque au sens où ils n'ont pas piraté l'entreprise originale, mais ils l'ont achetée et ont fait ce qu'ils voulaient avec le code. Ce qui était autrefois un produit légitime est devenu illégitime du jour au lendemain. Aucun protocole de sécurité n'aurait pu résoudre ce problème. Les entreprises doivent désormais vérifier leurs fournisseurs et les tiers qui les soutiennent.\n\nLe logiciel libre est à la fois une bénédiction et une malédiction. La bénédiction réside dans le fait que, puisqu'il est libre, vous avez accès au code source et pouvez le passer en revue autant que vous le souhaitez. La malédiction est que personne n'a le temps, l'énergie ou l'expertise nécessaire pour examiner en détail chaque morceau de code libre qu'ils utilisent. Des codes vulnérables aux logiciels malveillants, même accidentels, peuvent être introduits s'ils ne sont pas détectés par la communauté, et parfois cela ne se produit pas pendant longtemps.\n\nIl existe deux choses qui pourraient atténuer ce problème : premièrement, le code pourrait être soumis à un outil d'analyse de code source, et il pourrait y avoir un système de notation pour les contributeurs de code dont le code présente systématiquement moins de vulnérabilités ou de bugs. Ce travail serait principalement effectué par des fournisseurs tiers. Deuxièmement, il existe déjà des startups bien établies dans le secteur de la vérification des produits open source afin de montrer quels objets d'un dépôt sont considérés comme fiables et lesquels ne le sont pas encore.\n\nLes analyses de vulnérabilités actuelles dans les logiciels libres représentent un seul contrôle, mais ce type de sécurité seul n'est pas suffisant. Il doit être combiné avec des contrôles supplémentaires avant d'être utilisé dans toute votre organisation.\n\n## Violations de données\n\nEn plus des ransomwares et des attaques sur la chaîne d'approvisionnement, les violations de données sont un problème courant. En 2020, il y a eu 1,8 billion de dollars de pertes dues à des violations de données, affectant 7,8 milliards d'enregistrements de données. Par exemple, une violation chez McDonald's a compromis les données des clients, des partenaires et internes.\n\n## Chiffrement\n\nLa première partie de la solution consiste à disposer de meilleurs outils de sécurité pour les données et l'infrastructure. Chiffrer l'accès et incorporer le refus par défaut est essentiel afin que même si quelqu'un parvient à entrer par la porte d'entrée, il n'ait pas accès à tout. Les données doivent également être chiffrées à l'intérieur de l'organisation, non seulement celles qui sont exposées dans le cloud ou celles envoyées à l'extérieur. Toutes les données qui traversent votre réseau, ainsi que les données au repos, doivent être chiffrées.\n\nLe chiffrement a un coût, mais sur du matériel moderne, il y a une accélération intégrée qui annule la pénalité. Vous pouvez désormais activer le chiffrement dans l'ensemble de votre organisation sans impact sur les performances.\n\n## Segmentation\n\nUne autre partie est les enclaves, ou la segmentation. L'un des défis dans les environnements d'entreprise en réseau est que encore une fois, si quelqu'un entre par la porte, il est libre de faire ce qu'il veut si tout est connecté. Il y a eu récemment un mouvement visant à prendre le développement et à le déplacer dans son propre réseau, et c'est un début, mais ce n'est que la partie visible de l'iceberg. La segmentation du réseau devrait être appliquée dans toute l'organisation. Vous pouvez toujours avoir des passerelles, mais elles doivent se conformer à un ensemble de règles et aideront à limiter les impacts. Par exemple, si votre service d'assistance est attaqué, vos systèmes RH ne seront pas compromis en même temps.\n\nLa micro-segmentation était un mot à la mode il y a cinq ans environ, mais elle doit se produire maintenant. Il y a d'excellents outils disponibles pour aider à cela, tels que les écosystèmes de conteneurs où vous pouvez déployer une application et elle se trouve dans son propre réseau avec son propre pare-feu.\n\nMettre en place des informations d'authentification appropriée doit également se faire maintenant. L'authentification à facteurs multiples est nécessaire, tout comme l'authentification des entités. De nombreux outils sont automatisés et disposent de processus automatisés, il est donc essentiel que les entités, pas seulement les personnes, disposent de justificatifs adéquats.\n\n## Zéro confiance\n\nLa confiance zéro a atteint un stade de maturité où elle devrait être mise en œuvre, et certains des principes essentiels tels que le refus par défaut et la méfiance envers tous sont cruciaux. La technologie a rattrapé le déploiement de ces types de concepts.\n\nLes développeurs peuvent s'inquiéter du ralentissement du processus causé par ces outils de sécurité, mais il existe des moyens de concevoir l'architecture pour atténuer ce problème. Par exemple, si vous êtes développeur et que vous disposez des identifiants et des accès appropriés, vous devriez être en mesure d'accéder aux éléments dont vous avez besoin lorsque vous en avez besoin, et de perdre cet accès une fois que vous avez terminé, plutôt que d'avoir des identifiants vous donnant accès à tout en permanence. L'idée de la confiance zéro n'est pas que l'entreprise ne fait pas confiance au développeur, mais que l'accès est accordé uniquement au bon moment, et non pas un accès généralisé si un acteur malveillant vole les identifiants.\n\nAucune industrie ne peut se permettre de négliger les risques actuels. Chaque organisation doit envisager la sécurité différemment et mettre en œuvre des mesures de sécurité à tous les niveaux de l'organisation et de l'architecture.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT61-fr","image":"./episodes/edt-61/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, Chief Solution Architect d'Intel, aborde les violations de données et la chaîne d'approvisionnement sécurisée avec l'invité régulier Steve Orrin, CTO d'Intel, pour le secteur fédéral."},{"id":134,"type":"Episode","title":"Démystifier la 5G, l'Edge et l'IA","tags":["edge","compute","5g","aiml","iot"],"body":"\r\n\r\nLe parcours d'Anna concerne le secteur industriel et manufacturier de l'information. Elle est titulaire d'un doctorat en génie chimique ainsi que d'un MBA. Elle a passé 15 ans à travailler en tant qu'ingénieur de processus et de conception, puis a ensuite travaillé pour des startups et occupé des postes de gestion dans le domaine de l'ingénierie. Elle est avec Intel depuis un peu plus de six ans, dont les deux dernières années et demi dans l'équipe du secteur public où elle soutient principalement les activités IoT (Internet des objets) et edge d'Intel, avec une implication croissante dans la 5G car la 5G, ainsi que l'intelligence artificielle, rebattent considérablement les cartes dans ce domaine.\n\n## Transformation de réseau fondamentale pour l'infrastructure 5G.\n\nDans le monde industriel, l'IoT n'était rien de nouveau ; il était simplement câblé en dur. Avec les systèmes qui passent maintenant au sans fil, ils ont une étroite association entre l'informatique et l'automatisation pour convertir les analyses, au lieu de simplement avoir un ensemble de données de processus spécifiques à ce monde de l'automatisation. Un ensemble différent de systèmes épousent maintenant cela avec les activités commerciales, donc tout cela est regroupé dans le même espace. Bien qu'il y ait des différences très définies dans les cas d'utilisation et les architectures dans le secteur public, tels que l'armée et les villes intelligentes, il y a une convergence commune dans l'analyse, l'application et le délai pour prendre de meilleures décisions commerciales.\n\nUne des raisons pour lesquelles l'IoT a mis autant de temps à se développer en dehors du secteur manufacturier, c'est principalement le coût élevé du câblage. La sécurité a également été un obstacle.\n\nLa 5G libère désormais l'IoT et le monde de l'Edge en raison de sa rentabilité, notamment du côté des consommateurs. En ce qui concerne les infrastructures commerciales critiques, cependant, il s'agit d'une autre discussion sur la meilleure façon de le faire tout en protégeant les données. La 5G sera révolutionnaire, mais cela ne se produit pas encore du côté commercial. Une partie du retard est dû au délai entre la publication des normes et la production du matériel permettant de tirer parti de ces normes.\n\nPour toute personne qui a passé beaucoup de temps avec 3GPP ou d'autres organismes de normalisation du secteur des télécommunications, ce calendrier n'est pas surprenant. Pour l'instant, il est possible de mettre en place un réseau 5G, mais nous en sommes au stade des tests où nous devons démontrer la valeur de la 5G. Nous devons montrer de nouveaux cas d'utilisation qui ne peuvent pas être pris en charge par la LTE ou la 4G.\n\n## Plusieurs modèles de déploiement de la 5G offrent de la flexibilité.\n\nPourquoi ne pas rester simplement avec le WiFi 6 au lieu d'utiliser la 5G ? La réponse est complexe. Bon nombre des organisations de normalisation qui soutenaient la 5G soutenaient également le WiFi 6, il s'agit donc d'une technologie complémentaire. Le différenciateur réside dans l'équipement, il est donc nécessaire de connaître les détails de vos cas d'utilisation pour déterminer ce qui serait le plus rentable. Par exemple, la 5G est incroyable, mais il ne serait pas judicieux d'installer un réseau 5G sur un navire de croisière car la pénétration n'est pas suffisante pour que cela ait du sens. L'environnement n'est pas favorable à tout type de signal sans fil, mais le WiFi 6 avec des points d'accès semble plus logique.\n\nLa 5G est convaincante, cependant, pour plusieurs raisons et peut faire des choses que la 4G et la LTE ne peuvent pas faire. Pour la réalité augmentée et la réalité virtuelle, vous avez besoin d'une latence très faible et d'une bande passante élevée pour permettre des cas d'utilisation plus interactifs, par exemple la réparation d'équipements ou de machines. Vous avez besoin d'un expert distant qui observe un problème et peut effectuer du streaming vidéo et audio avec superposition de dessins et de capacités gérées à partir d'un emplacement central ou distant qui apporte toutes ces connaissances et expertises directement à l'endroit où vous essayez de travailler. Ce type de cas d'utilisation ne peut être réalisé sur un réseau LTE. La bande passante joue un rôle, mais la latence est la force motrice. Pour ne pas avoir la nausée lors de l'utilisation d'un casque, vous devez avoir des latences vraiment faibles sans retard ou avoir les choses qui se déroulent en mode asynchrone.\n\nLe monde de la LTE et de la 4G a changé en raison de la 5G, en raison de la façon dont elles ont géré le spectre. Un domaine où cela change dans l'industrie est qu'il est désormais possible pour une entreprise d'obtenir une licence prioritaire pour le spectre CBRS et de créer son propre réseau privé, complètement indépendant des principaux opérateurs fédéraux de télécommunications. Par exemple, un grand fabricant peut couvrir un espace énorme de manière plus rentable avec une LTE privée qu'avec des points d'accès. Il y a également de grands avantages, par exemple si vous souhaitez reconfigurer votre espace et que vous ne voulez pas que toutes vos stations de travail soient câblées, ou si vous devez déplacer d'énormes pièces de métal telles qu'une fuselage d'avion dans votre espace, ce qui pourrait interférer avec les signaux WiFi, vous pouvez créer une infrastructure portable et non fixée aux emplacements câblés.\n\nLa sécurité est une préoccupation centrale pour toute organisation. Bien que la 5G n'ait pas été conçue en tenant compte de la sécurité, il en sera autrement pour la 6G. Heureusement, avec les capacités de la 5G, nous pouvons faire beaucoup en matière de réseau de confiance zéro et d'autres mesures de sécurité qui donneront aux clients l'assurance que leurs données sont transférées de manière sûre à travers les réseaux.\n\nLes normes 5G ont modifié le problème d'il y a quelques années, lorsque l'infrastructure sans fil existait dans des matériels et des logiciels propriétaires, avec un spectre sous licence réservé à quelques entreprises seulement qui pouvaient se le permettre. Les réseaux définis par logiciel permettent d'héberger l'infrastructure réseau sur du matériel courant, disponible dans le commerce. Il n'est plus nécessaire d'utiliser des matériels spécialisés comme dans les générations précédentes. Cela se produit également dans le domaine de la LTE, par exemple, en rendant le spectre CBRS disponible et en abandonnant les matériels et logiciels propriétaires.\n\nIntel passe beaucoup de temps à travailler avec les perturbateurs qui utilisent notre architecture de référence FlexRAN. L'architecture FlexRAN devient la base pour aider la technologie perturbatrice à se propager sur le nouveau marché 5G car elle offre une pile logicielle 5G fonctionnant sur du matériel courant, disponible dans le commerce, alors qu'auparavant, du matériel propriétaire était nécessaire. Maintenant, il y a un espace beaucoup plus ouvert et portable, et le coût d'entrée est bien moins cher qu'auparavant. Ce n'est plus seulement quelques entreprises qui contrôlent tout. Intel et d'autres tentent de tout ouvrir et de profiter des normes ouvertes pour soutenir tous ces perturbateurs et changer toute la dynamique.\n\n## La bande 5G et la régulation des réseaux de demain.\n\nAvec une connectivité améliorée, une latence réduite et une large bande passante, de nombreux nouveaux cas d'utilisation seront disponibles. La monétisation de la 5G est en train de changer sur le marché. Par exemple, un fournisseur de services cloud associé à un opérateur de télécommunications peut offrir de meilleurs services à leurs clients car ils ne sont plus cloisonnés. Ils constituent un effort commercial combiné qui met en valeur ce qui compte vraiment : la qualité et la priorisation. Une autre façon de voir les choses est que les fournisseurs de services cloud achètent des capacités qui vont ouvrir les fonctionnalités réseau de la même manière que les opérateurs de télécommunications explorent ce qu'ils peuvent faire du côté du cloud. Encore une fois, cela est dû à la disparition des cloisonnements ; le tuyau de données n'est plus un ensemble de services.\n\nIl n'est pas clair comment tout cela va se dérouler, à part le fait que cela redéfinit le type de travail que l'on peut faire en raison de l'accessibilité des données et où ces charges de travail vont résider. Il y a une valeur immense à passer sans problème de l'infrastructure périphérique au cloud et à le faire de manière basée sur les besoins du client, ce qui est désormais possible.\n\n## Un nouveau paradigme informatique soutient de nouvelles demandes de données.\n\nLa 5G libère de nombreux modèles architecturaux différents. Par exemple, elle offre deux options d'architectures pour l'IA, tandis qu'auparavant il n'y en avait qu'une seule avec des limitations.\n\nSans la bande passante élevée fournie par la 5G, l'IA était limitée à l'inférence sur les appareils périphériques, ce qui nécessitait d'envoyer les modèles d'IA aux appareils périphériques. Cette restriction fastidieuse a augmenté le cycle de développement et de déploiement de l'IA et limite le nombre de charges de travail d'IA pouvant être exploitées au niveau des périphériques. Avec une augmentation de la bande passante, de grands flux de données provenant de caméras ou de capteurs peuvent être renvoyés vers un centre de données, ce qui permet l'exécution de plusieurs charges de travail d'IA et une apprentissage continu de l'IA. Cela donne aux organisations l'occasion d'exécuter à la fois l'inférence au niveau des périphériques et d'améliorer l'apprentissage en profondeur requis par les demandes toujours changeantes des données de nombreuses organisations.\n\nAvec la réalité augmentée, par exemple, la 5G signifie que les casques peuvent être mobiles plutôt que d'être attachés avec les mêmes fonctionnalités, car la 5G permet le partage de jeux de données plus volumineux dans un monde non contraint. Les murs traditionnels des centres de données sont en train d'être brisés.\n\nSi vous n'avez pas beaucoup de support technique ou de connaissances détaillées sur la façon de maintenir vos systèmes en fonctionnement, vous pouvez tout exécuter dans le cloud. Si vous ne souhaitez pas avoir vos données sur le cloud, vous pouvez opter pour une version sur site via un réseau privé qui vous offre toutes les fonctionnalités nécessaires pour regrouper et corréler les données afin d'obtenir une compréhension approfondie de ce qui se passe dans votre système de manière sécurisée et rentable.\n\nFondamentalement, vos données peuvent maintenant résider à la périphérie, dans le cloud, en local ou dans ce que Cisco appelle le brouillard. Il n'importe plus où votre application s'exécute, vous pouvez donc utiliser le modèle le plus rentable. Dans les espaces industriels, par exemple, il y a d'énormes économies en ne disposant pas d'un composant de câblage dur, ou en utilisant une structure de LTE privée plutôt que des points d'accès WiFi. Réduire ce type de coûts permettra d'obtenir des données extrêmement riches. Ces obstacles de coût et de connectivité physique sont ce qui manque pour que l'IoT décolle comme tout le monde l'avait prédit.\n\nAnna prédit que pour les applications non contrôlées, les deux prochaines années seront différentes en raison de la 5G. Un exemple simple est qu'en industrie, quelqu'un pourrait prendre son ordinateur régulier hors du bureau et l'utiliser sur le lieu de production pour tout faire là-bas. La 5G changera ce qui est possible en ce qui concerne les contrôles et la commande des robots et des machines via un réseau sans fil au cours des cinq prochaines années. Le prochain niveau de transformation sera de pouvoir effectuer des contrôles via un réseau sans fil, de manière sûre et efficace, sans mettre personne en danger. Cela nécessitera beaucoup de validation et d'examen rigoureux, mais c'est à l'horizon.\n\nDe plus, il sera passionnant de voir ce que votre fournisseur de services de cloud préféré et votre opérateur de télécommunications vont faire ensemble pour changer ce qui est possible du point de vue des services.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Anna Scott"],"link":"/episode-EDT62-fr","image":"./episodes/edt-62/fr/thumbnail.jpg","lang":"fr","summary":"Darren Pulsipher, architecte en chef des solutions chez Intel, discute des changements révolutionnaires que la 5G apportera à la périphérie et à l'IA avec le Dr Anna Scott, architecte en chef de la périphérie chez Intel."},{"id":135,"type":"Episode","title":"Inspirant un changement culturel dans le domaine de l'informatique","tags":["organizationalchange","change","people","covid","cio","california"],"body":"\r\n\r\nAmy a occupé le poste de directrice des systèmes d'information de l'État pendant plus de cinq ans, nommée par le Gouverneur Brown et maintenue en poste par le Gouverneur Newsom. Elle a travaillé en tant que technologue dans le secteur public pendant vingt ans dans divers domaines, de la science de l'environnement à CalPERS, en passant par la santé et les services sociaux, avant d'être nommée directrice des systèmes d'information.\n\nLe 13 mars 2020, Amy animait une réunion discutant, ironiquement, de la nécessité d'accélérer l'objectif du service à large bande pour tous les Californiens, lorsque le confinement COVID a été ordonné. Immédiatement après cela, l'État a dû passer plus de 200 000 employés de l'État au télétravail et mettre en place l'apprentissage à distance. Du jour au lendemain, la question de l'accès à large bande est devenue un problème urgent dans la gestion des activités quotidiennes. Au départ, l'État avait une capacité de télétravail de 5 % et en l'espace d'un mois, cette capacité est passée à 92 %.\n\nC'était le plus grand obstacle opérationnel de la pandémie. Les gens ont dû s'habituer à travailler à distance et il y avait une pénurie de fournitures, ce qui rendait la situation encore plus difficile. Toutefois, tout ce changement s'est déroulé relativement bien. Les gens ont été compréhensifs et flexibles, permettant ainsi l'amélioration continue.\n\nC'est un processus qui prendrait normalement des années à accomplir. La différence, c'est que les gens n'ont pas hésité à dire \"voilà ce qu'il faut faire\" et à se mettre en mouvement au lieu de tout suranalyser. Tout le monde a simplement fait avancer les choses, même si ce n'était pas parfait.\n\nLa cybersécurité était également un obstacle, car les personnes travaillant depuis leur domicile devaient adopter les mêmes bonnes pratiques que s'ils étaient au bureau, ce qui nécessitait un certain changement culturel.\n\nUn changement surprenant était que le désir d'utiliser un processus de prise de décision basé sur des preuves a également été accéléré du jour au lendemain. Cela ressemble un peu à un oxymore car tout ne pouvait pas être suranalysé, mais d'un autre côté, l'accent s'est resserré sur ces décisions critiques qui nécessitaient un soutien basé sur des preuves. Les choses peu importantes ont été mises de côté car il n'y avait pas assez de ressources pour tout faire.\n\nL'État est toujours en train d'apprendre à quoi ressemblera la main-d'œuvre à l'avenir. Le télétravail reste d'actualité pour un avenir prévisible, et certains pourraient devenir permanents en raison de l'augmentation de la productivité. De plus, les postes qui ne fonctionnent pas bien en télétravail doivent revenir dans un environnement sûr. Le bien-être des employés est également un facteur dans cette nouvelle dynamique.\n\nLe gouvernement et les autres organisations doivent maintenir un équilibre entre les 18 derniers mois hautement tactiques et la planification stratégique pour l'avenir. Amy conserve la vision stratégique dans le plan de l'État 2023 en tant que \"Étoile du Nord\", mais se concentre également sur les améliorations progressives actuelles pour fournir des services. Il y a une compréhension que les choses ne seront pas parfaites lorsque l'État reviendra à une nouvelle normalité.\n\nAmy a deux objectifs culturels pour son département découlant de la pandémie : les personnes occupant des postes de soutien doivent être en mesure de relier leur travail aux changements positifs et aux impacts afin de maintenir le moral, et l'environnement doit devenir plus visionnaire, rappelant aux travailleurs les North Stars stratégiques.\n\nPour maintenir un changement culturel positif, les leaders doivent montrer la voie et prendre en compte le bien-être des employés dans leurs décisions. Cela comprend être flexible quant à la façon dont ils travaillent le mieux et leur donner la confiance nécessaire pour prendre des décisions afin de se sentir épanouis et autonomes. Et si les choses ne se passent pas parfaitement, il faut les mettre de côté et avancer.\n\nLes conseils d'Amy pour les autres directeurs des systèmes d'information d'État seraient de réaliser beaucoup de travaux de preuve de concept, de retrousser les manches et d'expérimenter différentes choses, et d'être ouvert à différentes façons de résoudre un problème. Les résultats de la preuve de concept éclairent mieux les dirigeants dans la prise de décision que simplement analyser les choses.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Amy Tong"],"link":"/episode-EDT63-fr","image":"./episodes/edt-63/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions chez Intel, discute avec Amy Tong, CIO de l'État de Californie, de l'inspiration pour le changement culturel à la suite de la pandémie de COVID."},{"id":136,"type":"Episode","title":"Suivre et sécuriser le passage du Bit de l'Edge vers le Cloud","tags":["edge","compute","multicloud","cybersecurity"],"body":"\r\n\r\nDans l'épisode #2, Greg a expliqué les questions complexes de collecte, de déplacement et d'analyse des données dans un environnement du Département de la Défense (DoD) qui inclut des dispositifs périphériques sur des navires, des avions et même des combattants, ainsi que la nécessité de déplacer et d'analyser rapidement ces données pour des décisions opportunes et exploitables. Le rôle d'Intel est d'aider à réduire les cycles de processus et à soulager les goulots d'étranglement dans le flux de données avec son portefeuille technologique. Tout cela consiste à faire correspondre la technologie à la mission.\n\nUne tendance récente en architecture consiste à déplacer le calcul, le traitement et l'intelligence en avant afin de rencontrer les données plutôt que de déplacer les données vers l'arrière. Cette adaptation offre la meilleure capacité à obtenir des réponses en temps réel et de l'intelligence. Cette approche représente un modèle opérationnel différent qui soulève des questions complexes.\n\n## Matériel hétérogène\n\nDans l'infrastructure fondamentale, comment fournissez-vous les capacités de calcul, de stockage, de mémoire et de réseau nécessaires pour stimuler l'analyse en périphérie et les traitements là où vous en avez besoin ? Comment gérez-vous ces applications et ces données ? Comment introduisez-vous la qualité et la curation des données au niveau supérieur de la chaîne ? La sécurité est une composante fondamentale de toute infrastructure, et maintenant, les données doivent être protégées jusqu'à la périphérie.\n\nL'industrie doit passer à du calcul plus hétérogène afin que le logiciel sous-jacent puisse s'exécuter sur différentes plateformes matérielles, de sorte que vous ne vous retrouviez pas bloqué dans un logiciel ou un ensemble matériel spécifique. Une partie de la beauté de ce qu'Intel a fait depuis des années est de fournir une portabilité logicielle : le cadre ouvert, x86 et d'autres systèmes sous-jacents.\n\nUn facteur important dans la façon dont Intel développe ses capacités est de comprendre la charge de travail et le flux de données plutôt que de se concentrer sur la modernisation, comme le fait de mettre les choses dans le cloud. Il ne s'agit pas de la façon dont vous procurez les technologies de l'information (cloud ou sur site) mais d'optimiser le flux d'activité de l'entreprise afin de tirer parti de la façon dont les données circulent et de ce qu'elles font réellement.\n\n## DevOps à la rescousse\n\nUn autre aspect critique de ces nouvelles solutions est le temps de déploiement. DevOps est devenu de plus en plus important au sein du DOD, et les efforts pour développer des plates-formes DevOps communes ont considérablement augmenté. Pour écrire du code une seule fois et le faire fonctionner n'importe où (dans le cloud, sur site ou en périphérie), les organisations utilisent le jeu de puces x86 et l'outil oneAPI d'Intel pour déployer des services complexes à travers l'ensemble de l'écosystème. Par exemple, oneAPI permet aux développeurs d'écrire des tâches d'analyse ou d'intelligence artificielle complexes qui s'exécutent sur un GPU, un CPU ou même des FPGA avec la même base de code. Les développeurs peuvent ensuite tester ces systèmes complexes dans leur centre de données, en s'assurant qu'ils se comporteront de la même manière lorsqu'ils seront déployés sur le terrain, ce qui réduit le temps de déploiement.\n\nÊtre en mesure de tirer parti d'une combinaison d'un environnement DevOps flexible, basé sur la conteneurisation et des frameworks logiciels tels que oneAPI, offre aux programmeurs l'abstraction dont ils ont besoin, tout en bénéficiant d'optimisations matérielles/logicielles intégrées. Une distinction majeure ici est que nous avons trouvé un moyen de tirer pleinement parti des capacités du matériel et d'optimiser le logiciel de sorte qu'une application puisse encore bénéficier d'une amélioration des performances et tirer parti du matériel spécialisé.\n\nL'écosystème des conteneurs offre une abstraction au niveau du système, de sorte que les choses n'ont pas besoin d'être codées en dur. Il offre la possibilité de faire évoluer ou réduire la charge de service en fonction de la capacité. Par exemple, si une fonctionnalité s'exécute en périphérie, dans un centre de données portable ou dans le cloud, et que vous perdez la connectivité ou un élément, vous pouvez toujours mener à bien la mission. Cette connectivité intermittente est vitale dans un environnement du ministère de la Défense.\n\n## Solutions Edge\n\nBeaucoup de gens considèrent Intel comme un fournisseur de silicium ou de matériel, mais l'entreprise a développé des solutions et des architectures de référence dans l'écosystème allant du bord vers le cloud. Un exemple de progrès est une grille tactique navale où le traitement se fait en périphérie, sur les navires, les avions et à terre. Les applications doivent fonctionner sur tout l'écosystème, et cela ne peut pas être fait uniquement avec du matériel. Intel a mis à profit son silicium et ses fonctionnalités de sécurité, et en s'appuyant sur un écosystème de conteneurs, a produit des applications complexes avec plusieurs microservices qui peuvent fonctionner lorsque les actifs sont connectés ou déconnectés.\n\nUn autre exemple d'architectures de bord est le suivi d'objets à travers des maillages de capteurs, même avec des lacunes dans le maillage de capteurs. En effectuant une fusion de capteurs, une fusion de données et des analyses interdomaines, vous pouvez suivre des objets à travers différentes scènes et différents types de capteurs, en redimensionnant pour créer une plateforme de capteurs mobiles capable d'effectuer des analyses et une fusion au niveau du bord. Vous pouvez ensuite transférer ces informations à un ensemble de nœuds distribués qui peuvent travailler en collaboration pour suivre un objet à travers tous ces différents capteurs existants.\n\nPar exemple, si vous suivez un objet et qu'il se déplace entre les capteurs, vous le perdrez pendant un moment. Ce que permet cette intelligence, c'est de connecter ces deux flux et de suivre les zones aveugles. Cela fonctionne non seulement avec des types de capteurs homogènes, mais aussi avec plusieurs types de capteurs tels que les capteurs de mouvement, RF et infrarouges, donc si vous perdez la vidéo mais avez toujours une connexion RF, puis récupérez à nouveau la vidéo, vous pouvez montrer le chemin complet de l'objet. Dans le passé, ce type d'analyse nécessitait le transfert des données vers le centre de données pour traiter ces interactions complexes ; maintenant, nous pouvons le faire au niveau du maillage de capteurs grâce à des microservices en périphérie.\n\n## Flexibilité de déploiement\n\nUne approche avantageuse consiste à construire une fois et à utiliser la même architecture et le même logiciel pour différentes missions avec une exigence analytique. Certains endroits où le calcul intensif se trouve en périphérie peuvent être mis à l'échelle et tirer pleinement parti des capacités matérielles. D'autres plateformes peuvent avoir des capacités de calcul limitées et peuvent exécuter de plus petits microservices ne fournissant qu'une partie de la solution complète. De nouvelles capacités de mission et des services peuvent être déployés rapidement en construisant ces microservices une seule fois pour plusieurs cas d'utilisation.\n\n## Sécurité en périphérie\n\nLa complexité est parfois le pire ennemi de la sécurité, mais les principes fondamentaux de sécurité peuvent également sécuriser les données et les applications en périphérie. Plutôt que d'adopter l'ancienne approche de protéger le serveur en espérant que tout reste là, il est essentiel de comprendre où les données circulent et tous les endroits où elles se trouvent, en les protégeant, peu importe l'environnement dans lequel elles s'exécutent. Les approches basées sur les risques et les concepts tels que la confiance zéro ont pris de l'ampleur car ils adoptent une vision indépendante des systèmes en matière de sécurité.\n\nRéduit à l'essentiel, la confiance zéro signifie le refus par défaut. Personne n'est autorisé à entrer à moins d'en avoir besoin, et seulement pour la durée de l'action nécessaire. Lorsque les contrôles d'accès temporels sont associés à une approche basée sur les risques pour protéger les données tout au long de leur cycle de vie, on obtient la capacité de protéger les données, peu importe où elles se trouvent et qui y accède. Cette technique est l'un des moyens de sécuriser ces environnements hautement complexes.\n\nL'action pratique à prendre dans ces écosystèmes consiste à appliquer une politique qui exploite des contrôles répondant au niveau de risque d'un système donné à un moment donné, puis à les surveiller et les mettre à jour en temps réel pour faire face au monde cybermenaçant en perpétuelle évolution. Utilisez les contrôles techniques que les capacités matérielles et logicielles fournissent déjà, tels que le démarrage sécurisé, la racine de confiance matérielle avec les modules TPM ou les clés de stockage SGX, le chiffrement, etc.\n\nIl n'existe pas de solution miracle que vous pouvez acheter pour fournir une solution de sécurité complète dans ces écosystèmes complexes. Il s'agit de créer et d'appliquer des politiques de sécurité à mesure que les menaces évoluent, et de les déployer à grande échelle en tirant parti du matériel, des logiciels et des processus nécessaires pour sécuriser les données à mesure qu'elles circulent de l'extrémité de l'écosystème jusqu'au bout.\n\nLe cadre DevOps fournit des mécanismes efficaces pour gérer la sécurité de tous les actifs de l'écosystème. Les conteneurs doivent être équipés d'instruments pour appliquer les contrôles de sécurité et les politiques. La sécurité doit être intégrée au processus DevOps lui-même car si vous comptez sur le développeur pour mettre en œuvre la sécurité, chacun le fera légèrement différemment, augmentant ainsi la complexité et la variabilité du système. Le développeur doit avoir les capacités et les contraintes sur lesquelles il doit se baser pour développer.\n\nLes développeurs devront toujours effectuer des travaux de sécurité, tels que s'assurer qu'ils utilisent des outils de sécurité adaptés à l'environnement de menace particulier, mais le travail intensif, la complexité, devrait être abstraite dans l'architecture DevOps.\n\nL'une des zones critiques dans un environnement de théâtre complexe est la gestion des périphériques de bord, tels que la surveillance et la mise à jour du micrologiciel. S'assurer que ces appareils sont sécurisés afin de pouvoir prendre en charge la sécurité des données et les profils et politiques déployés dans les systèmes à grande échelle va nécessiter de l'innovation. C'est pourquoi l'écosystème se développe actuellement : pour relever ce défi.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Clifton","Steve Orrin"],"link":"/episode-EDT64-fr","image":"./episodes/edt-64/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, Architecte Principal des Solutions chez Intel, discute du suivi du bit avec Greg Clifton, Directeur du Département de la Défense et du Renseignement chez Intel, lors d'un suivi de l'épisode n°2, ainsi que Steve Orrin, CTO Fédéral chez Intel, qui apporte son expertise en matière de sécurité dans des environnements complexes de l'edge jusqu'au cloud."},{"id":137,"type":"Episode","title":"Accélérer l'adoption du cloud tout en réduisant la complexité et les coûts.","tags":["cloudmigration","cloud","compute","process","multicloud","aiven","cloudcost","technology"],"body":"\r\n\r\nDavid était auparavant développeur d'applications et se décrit comme un ingénieur exagérément perfectionniste en voie de rétablissement. Cette expérience, ainsi que son travail avec AWS et GCP dans différentes entreprises, lui donne les connaissances nécessaires pour comprendre les subtilités de l'exécution dans le cloud, comment se développer lorsqu'il est rentable et comment relever de nombreux défis lors de la migration vers le cloud.\n\nDe nombreuses organisations se tournent vers le cloud, accélérées par les conséquences de la pandémie de COVID. Les grandes et petites entreprises le font différemment. Les petites entreprises se dirigent vers le cloud et l'adoptent aussi rapidement que possible, en adoptant de nouveaux outils et les meilleures pratiques, mais il y a encore beaucoup de basculement et de transfert dans le centre de données. Cependant, pendant cette période de basculement et de transfert, elles ne profitent souvent pas des avantages du cloud. Elles cherchent donc à se transformer numériquement, mais elles se demandent quelles seront les prochaines étapes.\n\n## Réduire les coûts\n\nIl y a beaucoup de choses que vous pouvez faire, et devriez faire, pour réduire les coûts et éviter les mauvaises surprises, telles que les coûts liés aux machines virtuelles zombies. Dans le cloud, si vous avez besoin d'un serveur, il suffit de cliquer sur un bouton et vous l'obtenez, mais si vous oubliez qu'il est en cours d'exécution, un mois plus tard, il peut y avoir une facture de 10 000 $ pour les serveurs. Une autre chose à prendre en compte est le trafic sortant réseau qui peut s'accumuler, surtout avec les transferts d'une région à une autre si vous manipulez des téraoctets ou des pétaoctets chaque mois.\n\nLes alertes de dépenses peuvent aider à atténuer ces problèmes.\n\nLes projections de coûts peuvent être difficiles en raison de problèmes tels que les entrées gratuites, mais récupérer les données est coûteux. Supprimer des données représente également un coût. Donc, les organisations doivent prendre en compte les avertissements concernant la quantité de données à stocker, le type d'opération et l'utilisation.\n\nPour optimiser les coûts, quelque chose à laquelle la direction de l'ingénierie doit réfléchir est que vous atteindrez un point où les ressources de calcul et de service géré ne sont pas si chères ; la ressource la plus coûteuse est votre personnel. Comment alors optimiser l'efficacité de vos développeurs ? Vous ne voulez pas qu'ils effectuent des tâches de base et répétitives.\n\nL'automatisation est essentielle ici, en particulier pour toutes les tâches répétitives effectuées de manière fréquente, ainsi que pour les services gérés.\n\nChaque cycle de vie d'une entreprise arrive à un moment où elle doit décider si elle va investir et posséder toutes les opérations, embaucher une équipe DevOps ou tirer parti des prestataires de services gérés. L'embauche de compétences internes est coûteuse. Le point de basculement est lorsqu'ils décident dans quelle mesure le cloud s'échelonne pour eux. Il est généralement préférable pour les entreprises de prendre en compte le coût total de possession, de se concentrer sur leurs avantages concurrentiels et d'utiliser des services gérés pour d'autres services.\n\n## Réduire la complexité\n\nL'une des complexités du cloud est que vous devez être capable de recréer votre environnement avec des déploiements fiables et reproductibles. Cela ne signifie pas que vous allez sur le site web et cliquez dessus ; vous déployez votre environnement à partir, par exemple, d'un script ou d'un fichier YAML ou TerraForm, et vous devez être capable de le mettre en place rapidement et de le démanteler rapidement.\n\nVous devez être capable de tuer un serveur devenu corrompu et de le recréer de manière fiable dans un état propre. Si quelqu'un déplace des fichiers journaux, modifie des configurations, supprime une base de données par erreur ou s'il y a une attaque de rançongiciel, vous devez être en mesure de reproduire vos environnements ou des composants de votre environnement afin de réduire les temps d'arrêt.\n\nLa pièce critique est d'avoir un plan en place basé sur la rapidité à laquelle vous avez besoin de récupérer et la quantité de données dont vous avez besoin pour continuer. Il est logique pour certaines industries d'investir dans un plan pour se déplacer rapidement vers une autre région. Vous devez évaluer combien vous êtes prêt à investir dans le temps de récupération.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","David Esposito"],"link":"/episode-EDT65-fr","image":"./episodes/edt-65/fr/thumbnail.jpg","lang":"fr","summary":"Dans cet épisode, première partie sur deux, Darren parle à David Esposito, Architecte de solutions mondiales chez Aiven, de l'accélération de l'adoption du cloud tout en réduisant la complexité et les coûts."},{"id":138,"type":"Episode","title":"Gérer la complexité dans le Cloud","tags":["data","devops","compute","technology","process","devsecops","cybersecurity","aiven"],"body":"\r\n\r\n## Réduire la complexité\n\nQuelles pratiques commerciales doivent être mises en place pour gérer les actifs cloud et réduire la complexité ? Tout d'abord, il faut avoir un code d'infrastructure et des déploiements automatisés. Tout ce qui suit est une conversation pour définir les risques pour l'entreprise, tels que les pannes et les temps d'arrêt, comment atténuer ces risques et combien investir dans cela.\n\nPour certaines entreprises, il est essentiel d'avoir le plus haut niveau de disponibilité. Dans le secteur de la santé, par exemple, il peut être nécessaire de sauvegarder toutes les données dans plusieurs endroits et de rendre tous les services disponibles, car des décisions critiques et vitales peuvent être basées sur ces données. Cependant, si vous êtes dans le commerce électronique en avril, vous pouvez peut-être consacrer un peu de temps à cette région en difficulté car les conséquences ne sont pas aussi graves.\n\nDans le domaine de la santé, il est logique d'adopter le multi-cloud, même si cela accroît la complexité. D'autres secteurs qui peuvent bénéficier du multi-cloud sont ceux qui sont fortement réglementés, tels que les services financiers, le pétrole et le gaz ou les agences fédérales. En cas d'incident de sécurité, ils peuvent fermer un cloud et le déployer ailleurs.\n\nLorsque vous considérez le multi-cloud, sachez qu'il y a des complexités liées aux différences d'API. Si vous utilisez GCP et que vous savez comment déployer tout et envisagez de passer à AWS, vous devez réapprendre la plateforme et le processus de déploiement, voire même écrire une partie de ce code. C'est pourquoi la conteneurisation est populaire, car vous pouvez déployer n'importe où. Cependant, cela devient plus complexe avec d'autres services ; par exemple, les configurations de Postgres diffèrent entre AWS et GCP, peut-être avec des versions de correction différentes, des configurations réseau différentes et des extensions différentes. Ainsi, un service sur un cloud n'est pas le même que les services sur un autre.\n\nDe plus, les environnements conteneurisés ne sont pas idéaux pour toutes les situations, et une situation idéale devient plus importante à mesure que vous augmentez l'échelle. Si vous utilisez Kafka et traitez 1 000 messages par seconde, votre propre conteneur convient. Mais si vous commencez à gérer plus d'un million de messages par seconde, un gigaoctet, c'est une autre histoire.\n\n## Sécurité\n\nLa sécurité dans le cloud est une bête différente de la sécurité dans votre propre centre de données. La chose la plus importante à prendre en compte est la sortie des données de vos propres murs. Vous devez avoir des contrôles de sécurité appropriés en place pour éviter les fuites ou tout accès non autorisé aux données, et vous assurer que les exigences de conformité sont respectées. Cela nécessite une formation car les mesures de sécurité pour les centres de données sur site ne se traduisent pas directement dans le cloud. Par exemple, les conteneurs S3 sont chiffrés au repos, mais pas si vous avez un accès programmable via des compartiments exposés publiquement.\n\nDe plus, vous devez savoir ce que font les fournisseurs de services. Par exemple, Aiven exécute des bases de données sur les GPU EC2, GCP Compute ou Azure. Lorsqu'ils stockent sur le disque, il y a un niveau de chiffrement réalisé au niveau du cloud par le fournisseur de cloud, de sorte que les données sont chiffrées au repos. Ensuite, Aiven ajoute une autre couche de chiffrement avec leurs propres clés gérées, donc les données sont doublement chiffrées au repos, et toutes les données en transit sont chiffrées. Tout ce qui est envoyé au serveur est chiffré pour ce serveur particulier, et lorsqu'un serveur est désactivé, il est verrouillé avec la clé d'Aiven ainsi qu'avec la clé du fournisseur de cloud.\n\n## Aiven translation in French: Aiven.\n\nDavid a rejoint Aiven en tant que membre de l'équipe d'architecture des solutions lorsqu'il a constaté que l'entreprise proposait de meilleures solutions cloud que quiconque qu'il avait rencontré, en se basant sur son expérience précédente avec Kafka géré et d'autres technologies open source. La mission d'Aiven est d'améliorer la vie des développeurs. Ils offrent un essai gratuit, avec Kafka prêt pour la production en cinq minutes. Ils disposent également d'articles d'aide et de tutoriels pour vous guider, de sorte que vous puissiez envoyer un message Kafka dans le cloud en utilisant les meilleures pratiques cloud-native en moins de dix minutes.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","David Esposito"],"link":"/episode-EDT66-fr","image":"./episodes/edt-66/fr/thumbnail.jpg","lang":"fr","summary":"Dans cet épisode, partie 2 sur 2, Darren poursuit sa conversation avec David Esposito, architecte de solution mondial chez Aiven, sur l'accélération de l'adoption du cloud tout en réduisant la complexité et les coûts."},{"id":139,"type":"Episode","title":"5G Passé, Présent et Futur","tags":["5g","comms","wifi6"],"body":"\r\n\r\nLeeland travaille dans l'industrie des télécommunications depuis 20 ans. Tout juste sorti de l'université, il a été recruté pour travailler pour le Département de la Défense des États-Unis après avoir impressionné par son travail sur l'application du multiplexage Bluetooth dans une voiture, ce qui a permis de réduire la taille du faisceau de câblage. Au lieu de travailler avec des voitures comme il l'avait initialement prévu, il a fini par travailler avec le Département de la Défense en développant des technologies sans fil avancées pour les soldats.\n\nÀ cette époque, la technologie cellulaire était en transition de la 2G à la 3G. Elle n'était pas appelée WiFi à ce moment-là, mais réseau local sans fil, et il existait une technologie naissante appelée Bluetooth qu'ils essayaient d'utiliser pour permettre aux soldats de recueillir des informations sur le champ de bataille.\n\nLeeland a quitté le Département de la Défense trois ans plus tard afin de travailler concrètement avec la technologie pour acquérir une meilleure compréhension. Il a commencé à travailler pour Sprint et est resté dans l'industrie des télécommunications pendant 13 ans, en concevant des technologies pour les réseaux sans fil, avant de rejoindre Intel en 2017 pour travailler sur la 5G.\n\nJusqu'au milieu des années 90, l'utilisation des téléphones cellulaires était limitée. Les \"téléphones briques\" des années 80 étaient énormes en raison de la taille de la batterie et ils étaient chers. Ils ont évolué en \"téléphones sacs\" au début des années 90, qui étaient meilleurs, mais vous pouviez seulement espérer qu'ils fonctionneraient pour passer un appel téléphonique.\n\nComme la 2G est entrée en jeu, de nouvelles fonctionnalités basées sur les désirs des clients ont été développées, telles que la transmission de textes et les jeux. L'appareil pouvait effectuer une transmission de données à faible débit grâce aux technologies TDMA (accès multiple à répartition dans le temps) et GSN (par exemple). Au milieu des années 90, des entreprises comme Sprint sont entrées en jeu et l'utilisation des téléphones portables est devenue plus répandue. Au début des années 2000, il y a eu une transition significative avec l'attente des clients selon laquelle Internet devrait être disponible sur les téléphones comme sur les ordinateurs portables.\n\nLa 3G a apporté des capacités nominales haut débit où vous pouviez utiliser Internet dans une certaine mesure, mais la capacité principale était la transmission d'images. Le nom \"3G\" a été inventé à cette époque, et c'est à ce moment-là que le groupe de travail sur les normes 3GPP est entré en jeu.\n\nEn 2008, les opérateurs de télécommunications ont commencé à étudier le déploiement d'une infrastructure qui ne reposait pas sur de grandes tours cellulaires, mais sur un système plus décentralisé composé d'un centre d'unités de bande de base avec des antennes acheminées via fibre optique jusqu'aux poteaux téléphoniques. Cela a marqué le début de la transition vers le véritable haut débit, du 3G au 4G.\n\nCette technologie devrait être créditée pour le boom économique de 2010 à 2020, car avec la 4G en main, des entreprises telles qu'Amazon et Netflix ont évolué et prospéré.\n\nLa 5G libère les services d'une architecture RAN monolithique et ouvre le champ des possibilités car elle offre des architectures open source avec une base définie par logiciel. Les piles logicielles peuvent maintenant être développées et intégrées dans une solution logicielle complète. Étant donné qu'elle n'est pas liée à une architecture monolithique, la 5G peut fournir des services et des réseaux privés autonomes.\n\nCette immense flexibilité permettra aux opérateurs de télécommunications et à leurs transporteurs d'améliorer leurs services et d'offrir différentes nouvelles capacités, y compris l'accès à l'informatique périphérique.\n\nQu'est-ce qui vient ensuite ? Leeland observe un mouvement loin du \"G\", car cela ne donne pas suffisamment de crédit à l'évolution de la technologie car il n'y a plus de véritables partitions en termes de qui peut déployer des réseaux. La technologie et les cas d'utilisation sont vastes.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown"],"link":"/episode-EDT67-fr","image":"./episodes/edt-67/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions chez Intel, discute avec Leeland Brown, directeur technique de la 5G chez Intel Federal, à propos du passé, du présent et du futur de la 5G, en mettant l'accent sur son utilisation avec le Département de la Défense. Partie 1 sur 2."},{"id":140,"type":"Episode","title":"5G dans la défense","tags":["5g","dod","compute","technology","cloud","edge","comms"],"body":"\r\n\r\nAu tournant du siècle, le Département de la Défense cherchait des moyens d'appliquer les technologies 2G, le réseau local sans fil (LAN) et le Bluetooth dans des applications permettant aux soldats de recueillir des informations sur le champ de bataille. En 2021, ces capacités sont toujours recherchées. Le Département de la Défense se penche sérieusement sur la 5G, car il s'est éloigné des structures monolithiques telles que l'utilisation d'une architecture de réseau d'accès radio (RAN) unique, au profit d'architectures définies par logiciel. Cela offre la flexibilité de développer plus rapidement de nouvelles capacités et de les adapter à différents cas d'utilisation.\n\nLe volet commercial de la 5G est guidé par des stratégies visant à augmenter les revenus en cours de développement. Cela ne correspond pas nécessairement aux exigences de mission du gouvernement fédéral ou du Département de la Défense, ce qui floute les frontières entre ces deux mondes. Par exemple, AT&T et Verizon examinent les cas d'utilisation gouvernementaux, tandis que Lockheed Martin, avec son programme Domino de la 5G, se tourne vers l'espace commercial. La différence réside dans la compréhension de la mission et dans les motivations liées aux revenus.\n\nActuellement, quatre-vingt-dix pour cent des déploiements commerciaux de la 5G sont des réseaux non autonomes, ce qui signifie que la 5G est toujours connectée à un cœur de paquets évolutif 4G existant. Le Département de la Défense s'intéresse aux réseaux autonomes dotés d'un noyau 5G complet, d'un RAN 5G et de dispositifs 5G. Il peut y avoir un réseau autonome pour un groupe de soldats, ainsi que des stations de base de petite taille pour les plates-formes véhiculaires et de multiples domaines protégés, y compris des drones déployés avec un point d'accès 5G de type quelconque et des applications spatiales.\n\nCette capacité à s'adapter à plusieurs cas d'utilisation et à différents types de charges de travail est applicable également au côté commercial. Cependant, l'un des plus gros problèmes est de savoir qui détient la fréquence. Certains spectres gratuits sont disponibles avec une portée limitée, mais ces bandes, telles que les bandes ISN, ne sont pas sous licence et sont très encombrées. La règle est que vous devez accepter les interférences et faire en sorte que votre technologie fonctionne malgré cela, mais le partage dynamique du système est possible. Cependant, pour les cas d'utilisation fédéraux, vous ne pouvez pas être bloqué dans vos opérations aux États-Unis car la plupart des soldats sont déployés à l'étranger.\n\nLeeland prédit que la 5G sera intégrée dans la vie de tout le monde, parfois sans même que vous ne la remarquiez. Les plateformes de reconnaissance de motifs, comme la reconnaissance faciale, et les véhicules autonomes seront tous connectés via la 5G. Votre accès à large bande sera intégré dans un ensemble harmonieux, connecté non seulement à votre téléphone portable, mais aussi à votre voiture et à votre domicile. Le besoin d'avoir un téléphone dans votre main diminuera considérablement à mesure que nous commencerons à voir des points d'accès sans fil dans tous les aspects de notre vie.\n\nLeeland prévoit également que le \"G\" disparaîtra à mesure que la technologie se développe et évolue.\n\nPour en savoir plus sur la 5G, les personnes techniquement inclinées peuvent consulter les normes du 3GPP pour voir les cas de spécificités. Vous pouvez lire une spécification et comprendre la différence entre la version 14 et la version 15, ainsi que ce que cela signifie pour l'industrie dans son ensemble. Il y a également beaucoup d'informations disponibles sur Internet, comme des livres blancs. Leeland se propose également comme un point de contact.\n\nLeeland souhaiterait que la prochaine étape soit un appel à l'action pour rendre les réseaux plus résilients grâce à l'adoption de nouvelles technologies. En cas d'urgence, qu'il s'agisse de catastrophes naturelles ou d'attaques terroristes, la résilience est nécessaire tant pour les premiers intervenants que pour les personnes essayant simplement de contacter leur famille.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown"],"link":"/episode-EDT68-fr","image":"./episodes/edt-68/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, Architecte principal de solutions chez Intel, poursuit sa discussion avec Leeland Brown, Directeur technique de la 5G chez Intel Federal, sur le passé, le présent et le futur de la 5G, en mettant l'accent sur son utilisation avec le département de la Défense. Partie 2 de 2."},{"id":141,"type":"Episode","title":"Sécuriser le centre de données grâce à l'Edge","tags":["cybersecurity","edge","compute"],"body":"\r\n\r\nUn regard holistique sur la sécurité comprend toute l'infrastructure, de l'environnement cloud jusqu'au bord. Si vous ne pouvez pas sécuriser le client, vous ne pouvez pas sécuriser l'entreprise, il est donc essentiel d'avoir des conversations honnêtes de manière accessible, sans le jargon, sur la sécurité.\n\nLes jours sont révolus où un employé recevait un ordinateur portable livré à une cage informatique, puis configuré et livré par le service informatique. Parfois, les employés ne se rendent même pas sur le site de travail, il est donc nécessaire que les appareils soient livrés à leur domicile. Ils doivent être provisionnés pour s'assurer qu'ils sont sécurisés et n'ont pas été altérés. Du point de vue de la sécurité, c'est un problème complexe. La chaîne d'approvisionnement pose problème. Il y a moins de contrôle sur les appareils et leur déploiement.\n\nLe passage soudain au travail à domicile en raison de la pandémie de COVID a nécessité une réponse rapide sans précédent à ce problème. Cela aurait pu prendre des années s'il avait été une progression naturelle, mais la situation ne permettait pas une alternative ; il fallait que l'informatique se mobilise.\n\nQuand Camille travaillait dans le groupe IoT, ils ont essayé de résoudre les problèmes de connectivité et de gestion entre les appareils. Les écosystèmes complexes tels que les parcs éoliens, les mines souterraines et les dispositifs médicaux implantés sont difficiles à mettre à jour en cas de connectivité sporadique. Ils cherchaient à résoudre ces problèmes de connectivité en périphérie, et lorsque la COVID a frappé, l'intersection entre l'OT et l'IT est soudainement devenue le centre de tous les services informatiques d'entreprise.\n\nAlors que certains clients de Darren cherchaient à trouver la meilleure manière de combler le fossé entre les technologies opérationnelles et informatiques (OT et IT), la COVID est arrivée et une partie de cet équilibre s'est effondrée, entraînant des violations de sécurité.\n\nDe nombreuses personnes ont rapidement mis en place un système de travail à domicile et ont traité ultérieurement de la sécurité, en fonction du niveau de maturité de l'organisation. Il y a également eu un important mouvement vers le cloud. Actuellement, il y a un léger balancement du pendule en arrière en raison de violations telles que des attaques de ransomware. Ces violations se sont produites dans le cloud, principalement parce que les gens ne comprenaient pas la responsabilité partagée de la sécurité. Maintenant, certaines organisations réfléchissent à deux fois avant de déplacer leurs données sensibles dans le cloud. Elles peuvent y déplacer des charges de travail, mais gardent les données précieuses à la maison.\n\nUn autre changement récent est l'importance de la perception du client. Vous pouvez poser les bonnes questions concernant le matériel et les couches de protection logicielle, mais vous devez également prendre en compte la perception de votre client quant à l'endroit où vous stockez les données et pourquoi, qui les protège, et comment elles sont protégées.\n\nUn problème plus important est que certaines organisations ne peuvent répondre à aucune de ces questions. Parfois, elles ne savent même pas où se trouvent leurs données. Ces organisations devraient considérer cela comme un point de départ pour le travail encore à accomplir.\n\nCertaines nouvelles problématiques aggravent ce problème que l'industrie n'a pas encore résolu, telles que la visioconférence. L'enregistrement de la réunion est sauvegardé sur un ordinateur portable, mais il est également stocké quelque part dans le cloud. Qui y a accès ? Quelles sont les protections en place ? Combien de temps restera-t-il là-bas ?\n\nUn principe de sécurité est la connaissance de la sûreté de votre appareil. Un des défis révélés avec la situation de COVID est que de nombreux services informatiques hésitent à mettre à jour leurs systèmes. Ils ne veulent pas mettre un système hors service, que ce soit un serveur ou un client. Ne pas effectuer de mises à jour pour les correctifs de sécurité est une erreur.\n\nLe travail d'Intel consiste à collaborer avec des partenaires et des compagnons de route afin de rendre ces mises à jour plus simples et plus fiables, de sorte que les gens puissent avoir confiance en leur bon fonctionnement et être assurés qu'aucun problème ne surviendra durant le processus. L'industrie a réalisé des progrès considérables au cours des dernières années pour rendre le processus de mise à jour plus systématique et prévisible.\n\nUne autre partie du travail consiste à former les gens pour qu'ils comprennent que la sécurité ne s'arrête pas lorsque l'appareil est expédié par le fabricant au client. La sécurité se poursuit tout au long de la vie d'un appareil. Ce qui était une sécurité de classe mondiale au moment de l'expédition ne l'est plus nécessairement des mois ou des années plus tard. Les entreprises devraient mettre à jour leurs machines deux fois par an pour les maintenir en sécurité.\n\nLes gens sont nerveux à propos des mises à jour car des choses inattendues pourraient se produire. Intel valide à grande échelle pour prévenir les problèmes, que ce soit avec des milliers de machines dans leurs laboratoires ou avec des partenaires OEM dans des laboratoires dispersés partout dans le monde. La validation complète s'assure que les mesures d'atténuation fonctionnent pour protéger contre les vulnérabilités sans causer de dommages au système. Intel a fait un investissement important en partenariat et collaboration avec ses partenaires de l'écosystème et en fixant des normes dans l'industrie, et cherche à améliorer l'expérience utilisateur à l'avenir en développant la capacité de réaliser les mises à jour sans redémarrage.\n\nÉduquer les clients sur les raisons pour lesquelles vous leur demandez de faire une mise à jour peut également être très bénéfique. S'ils comprennent que vous avez découvert une vulnérabilité et qu'ils pourraient être exposés à une attaque potentielle, ils seront probablement disposés à le faire.\n\nEn général, les gens semblent être disposés à mettre à jour leurs téléphones portables car ils ne sont pas aussi inquiets que quelque chose ne fonctionnera pas après, mais c'est toujours un défi du côté des ordinateurs et des serveurs. Une partie de cela a à voir avec les modèles d'utilisation. Bien qu'il soit rare que les données existent uniquement sur un ordinateur portable, cette mentalité est prévalente. Lorsque les données existent dans le cloud sur un téléphone portable, la perception est qu'elles seront toujours là. De plus, les gens ont tendance à effectuer des travaux plus immersifs et engageants sur les ordinateurs portables que sur les téléphones, donc ils y sont plus sensibles. Une fois que la perception change et que les gens réalisent que les données sur leur ordinateur portable existent également dans le cloud, les mises à jour sont plus largement acceptées. Donc, en réalité, l'industrie a besoin de combiner des solutions techniques et des changements de mentalité en ce qui concerne la sécurité.\n\nLa façon dont les choses évoluent est un peu hybride. De nouveaux modèles d'apprentissage tels que l'apprentissage fédéré se précipitent pour aider à résoudre des problèmes tels que les problèmes de confidentialité. Les modèles sont poussés à la limite au lieu que les données se déplacent vers le centre de données. Par exemple, un système d'imagerie médicale dans un hôpital où les données restent en place et le modèle vient y jeter un œil. Nous commençons à voir cela dans les applications industrielles, où les machines sont à la périphérie et deviennent le serveur. Elles gardent les données locales et effectuent leur formation et mises à jour là-bas. Il y aura donc des appareils intelligents en périphérie, manipulant les données brutes, et la question est : comment assurer leur sécurité ?\n\nUne autre tendance en matière de sécurité, qui n'a pas commencé avec la pandémie de la COVID mais a indubitablement été accélérée par celle-ci, est la protection contre les attaques physiques. Historiquement, la sécurité était axée sur les menaces pouvant survenir via le réseau, telles qu'une attaque informatique ou une application malveillante. Avec des appareils IoT présents sans supervision humaine, il est nécessaire de protéger les données et les appareils contre toute manipulation. C'est un défi complexe.\n\nDe nos jours, vous ne pouvez pas envisager la sécurité de manière holistique sauf si vous abordez également la confidentialité. Une complication réside dans le fait que la confidentialité peut parfois être en conflit direct avec la sécurité. Il n'existe pas de réglementations ou de normes mondiales consensuelles, les organisations doivent donc trouver comment opérer : atteindre le niveau de tolérance le plus élevé ou répondre à chaque exigence géopolitique ? Pour compliquer davantage les choses, les lois et réglementations changent constamment.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Tom Garrison","Camille Morhardt"],"link":"/episode-EDT69-fr","image":"./episodes/edt-69/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions chez Intel, discute de la sécurisation du centre de données via le réseau avec ses collègues exécutifs chez Intel et les animateurs de podcast, Tom Garrison, vice-président de la sécurité client, et Camille Morhardt, directrice de l'innovation et de la communication en matière de sécurité."},{"id":142,"type":"Episode","title":"Sécurisation du travailleur à distance","tags":null,"body":"\r\n\r\n## Modes d'opération pour les télétravailleurs.\n\nLes modes de fonctionnement du télétravail ne sont pas nouveaux, que ce soit l'utilisation d'un appareil comme terminal (VDI), faisant partie du réseau interne, ou comme portail vers des services/logiciels (SAAS) ; ce qui est nouveau, c'est le nombre élevé d'employés, de sous-traitants et de partenaires utilisant désormais les environnements existants pour effectuer leur travail principal, plutôt qu'un nombre restreint et gérable. La sécurité est un défi dans tous ces modes de fonctionnement. Les organisations doivent déterminer la meilleure façon de déployer leurs capacités existantes pour sécuriser et protéger l'accès, les données, les appareils et les utilisateurs.\n\n## Menaces du télétravail\n\nCertaines des menaces pour la sécurité sont celles qui ont toujours été présentes avec le télétravail : la sécurité de l'appareil final et des données, le point d'accès et l'accès aux services de l'entreprise. Cependant, en s'adaptant à l'environnement du COVID-19, de nouveaux problèmes sont apparus en raison du grand nombre de personnes qui télétravaillent maintenant et de la course folle pour les rendre possibles.\n\nUn problème est la présence de travailleurs à domicile dans des environnements non sécurisés, sur des réseaux qui n'ont jamais été prévus pour être utilisés en dehors de l'entreprise. Cela entraîne des attaques ciblées de phishing et de logiciels malveillants. Un autre problème est l'introduction de nouveaux outils, tels que ceux utilisés pour la collaboration et les visioconférences, qui peuvent exposer votre entreprise et vos données à des attaques et une utilisation malveillante. Essentiellement, il y a maintenant une plus grande surface d'attaque, beaucoup plus importante que ce que la plupart des organisations avaient initialement prévu, et de nouveaux environnements qui n'étaient pas prévus dans le plan d'origine.\n\n## Solutions pour sécuriser le travailleur à distance\n\n## Apportez votre propre appareil (BYOD)\n\nL'un des problèmes les plus difficiles à gérer est lorsque les travailleurs utilisent leurs propres appareils. Le défi clé est que ces appareils ne sont pas gérés, sans tous les agents de sécurité en cours d'exécution et gérés par l'informatique ; l'appareil est possédé et géré par l'utilisateur. Vous pouvez avoir la possibilité de charger quelques agents ou d'appliquer certaines politiques, mais il y a seulement tant que vous pouvez faire sans entraver l'utilisation de l'appareil par le travailleur en tant qu'appareil personnel. Alors, comment sécuriser un utilisateur avec un BYOD ?\n\nAssurez-vous que ces bases sont en place : Poussez les derniers correctifs comme une exigence, appliquez un bon contrôle d'accès à votre entreprise et à vos services, mettez en place une authentification à facteurs multiples, et entourez vos actifs des bonnes protections telles que la gestion des droits d'entreprise pour le contrôle d'accès aux données et imposez un contrôle d'accès basé sur des politiques au niveau des points d'accès finaux.\n\nUne autre option pour réduire votre risque peut être de limiter l'accès direct aux services de l'entreprise et de faire travailler les utilisateurs dans un environnement SASS.\n\n## Points de terminaison et environnements non sécurisés\n\nLa meilleure solution pour bon nombre de ces problèmes serait un client riche d'entreprise, mais il reste néanmoins des risques. Les travailleurs utilisent un appareil qui est une extension gérée du réseau de l'entreprise. La sécurité peut être mise en place avec un démarrage sécurisé, un chiffrement du disque complet, une protection des données, des pare-feu locaux et l'application de bonnes pratiques de mise à jour. Cependant, le risque réside dans le fait que de nombreuses organisations ont déjà des travailleurs à distance en place, tels que des commerciaux ou des ingénieurs sur le terrain, qui pourraient ne pas bénéficier du même niveau de sécurité que les travailleurs qui n'étaient pas censés quitter le bâtiment, tels que les services financiers et les ressources humaines.\n\nLes organisations doivent être vigilantes pour s'assurer que tous les travailleurs disposent désormais de la même sécurité, voire ajouter des couches supplémentaires que les travailleurs doivent utiliser pour travailler à distance sur des réseaux non sécurisés. L'authentification à plusieurs facteurs devrait désormais être une exigence, et l'application des politiques de terminaux et la gestion des droits d'entreprise sont désormais plus importants que jamais.\n\nNous observons des innovations dans ce domaine, comme un client qui fournit non seulement des ordinateurs portables aux nouveaux employés, mais également un routeur géré pour éviter les réseaux non sécurisés.\n\n## Systèmes VDI et services Cloud\n\nMettre en œuvre la sécurité pour les systèmes VDI et les services cloud comprend les bases de la sécurité : protections des données, sécurité de la virtualisation pour le centre de données de l'entreprise et les points d'accès, sécurité des applications, démarrages sécurisés, correction des failles et chiffrement du réseau. La clé est de réaliser une enquête précise sur l'entreprise et les services cloud déployés auprès de vos employés et de s'assurer qu'ils sont tous sécurisés de la même manière. Chaque application, même celles qui ne sont pas critiques, représente un point d'attaque potentiel.\n\n## En tant que service aux clients.\n\nLes clients SASS utilisent des services dans le cloud et utilisent également des applications sur leur client riche, ce qui pose quelques problèmes de sécurité supplémentaires à prendre en compte. Il devrait y avoir une protection appropriée des données dans la gestion des droits d'entreprise (ERM) pour l'accès aux données via les services cloud et de retour vers le centre de données. La protection des deux côtés est primordiale. L'accès des clients aux services cloud devrait être protégé par une authentification multi-facteurs et un chiffrement réseau. L'accès des services cloud au cloud privé du centre de données de l'entreprise et aux ressources de l'entreprise devrait également être protégé au niveau du réseau, de l'accès aux données et des applications. Comprendre comment les clients utilisent les services et quelles données ils consultent est là que les décisions d'ERM entrent en jeu.\n\n## Mauvaise utilisation et abus d'insiders\n\nL'informatique devrait utiliser une variété de méthodes pour gérer la menace et le risque d'erreurs, d'utilisation abusive et d'actions malveillantes des membres internes. Le contrôle d'accès basé sur les politiques et l'application de mesures de sécurité depuis les applications jusqu'aux données, tant au niveau de l'entreprise que du cloud, sont importants pour contrer les utilisateurs déjà authentifiés qui abusent ou utilisent de manière abusive les systèmes. La principale défense dont dispose l'informatique est la collecte de données et la surveillance du renseignement sur les menaces. La gestion de ces informations dans l'ensemble de l'entreprise et du cloud sur une longue période peut être très efficace pour détecter les comportements aberrants.\n\nIl ne fait aucun doute que nous devons maintenant penser différemment aux problèmes de sécurité liés au télétravail. Nos principales recommandations sont les suivantes : premièrement, déployez la technologie que vous avez expérimentée ; deuxièmement, éduquez vos utilisateurs ; et troisièmement, activez l'authentification à deux facteurs et protégez vos données à grande échelle. Si nous pouvons faire ces trois choses, nous pouvons réduire les risques et être mieux préparés pour l'avenir.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT7-fr","image":"./episodes/edt-7/fr/thumbnail.png","lang":"fr","summary":"Avec l'énorme changement de personnes travaillant désormais à domicile au lieu du bureau, la sécurité est une préoccupation croissante pour de nombreuses organisations informatiques. Dans cet épisode, Steve Orrin, directeur technique de Intel Federal, et Darren discutent des menaces de sécurité et des solutions pour aider à sécuriser votre télétravailleur et les centres de données de l'entreprise."},{"id":143,"type":"Episode","title":"Gouvernance des données modernes","tags":["datagovernance","immuta","dataaccess","identitymanagement","cybersecurity","data","technology"],"body":"\r\n\r\nAlors que les organisations migrent vers le cloud, la manière dont nous pensons les données et dont nous les utilisons change complètement ; au cours des cinq ou six dernières années, l'ensemble de l'infrastructure technique a changé. Les performances, l'échelle, les règles nécessaires et le nombre d'utilisateurs souhaitant combiner des données ont augmenté de façon exponentielle. Gérer tout cela à grande échelle n'est pas anodin, et c'est là qu'intervient Immuta.\n\nAu fil du temps, l'industrie a repensé le paradigme du stockage des données. Auparavant, chaque équipe de données construisait des produits de données dans un silo, puis les livrait à une unité commerciale. L'unité commerciale disposait de données appropriées qui lui fournissaient un service de qualité. Maintenant, soudainement, nous voulons que n'importe qui puisse combiner des données sur l'ensemble de l'entreprise : analystes commerciaux, scientifiques des données, ingénieurs des données, etc. Le nombre d'utilisateurs a changé, et il ne s'agit pas d'applications de recherche ; il s'agit de la transformation et des pipelines de données.\n\nC'est une différence qui nécessite une qualité de service différente, du temps et un niveau de sophistication qui n'existait jamais auparavant. Cela devient encore plus complexe lorsque l'on ajoute les lois sur la vie privée, la classification des données et les règles et réglementations en constante évolution.\n\nCe que cherche à faire Immuta, c'est rendre possible à tout utilisateur de potentiellement combiner n'importe quel jeu de données, interne ou externe à l'organisation, afin d'en obtenir des informations. Tout dépend de l'échelle. Les politiques doivent être séparées de la plateforme pour pouvoir appliquer une politique adaptée à tous ces nouveaux utilisateurs et à toutes les combinaisons de données différentes. La sécurité, la confidentialité et la gouvernance sont essentielles, mais si on ne peut pas les exécuter, personne n'est satisfait.\n\nUne leçon qu'Immuta a apprise très tôt est que le proxy ne fonctionnera pas. C'est comme essayer de déplacer des pétaoctets de données à travers une paille : ils deviennent le goulot d'étranglement. Il est facile de blâmer les intergiciels lorsqu'il y a un problème car ils sont toujours un goulot d'étranglement, étant donné qu'ils se trouvent entre les outils et les données. Il s'agit d'un problème classique des intergiciels.\n\nLe deuxième problème avec ce paradigme de conception est qu'avec la virtualisation classique des données, cela fonctionnait car il y avait un ensemble clair de données. Cependant, lorsque nous parlons de pétaoctets de données, l'approche \"donnez-moi tout et nous trouverons une solution\" s'effondre très rapidement en raison de la taille immense des données ainsi que de toutes les règles et politiques qui les entourent.\n\nDans la virtualisation normale, il faut intégrer nativement dans l'infrastructure de calcul en nuage de Snowflake, de Synapse, etc. Au lieu de mettre cette couche d'abstraction au-dessus de ces différents outils, la couche d'abstraction se trouve du côté des politiques.\n\nLa politique n'est aussi bonne que sa capacité à être vérifiée. C'est comme une chaîne de preuves. L'essentiel est de pouvoir prouver si cet utilisateur a respecté ou non la politique à travers l'infrastructure informatique. Avec une gouvernance moderne des données, on fait évoluer la politique à l'intérieur et on unifie les audits en raison du niveau de complexité de nombreuses personnes qui font tant de choses avec les données. L'audit de la politique doit être considérablement simplifié, sinon il est impossible de déterminer la conformité.\n\nIl existe trois types de politiques à prendre en compte : opérationnelle, réglementaire et contractuelle. Un exemple de la complexité que cela peut entraîner est de regarder une entreprise telle que Cummins. S'ils se modernisent dans le cloud, ils doivent prendre en compte les réglementations pour chaque marché. Pour l'un de leurs moteurs, que se passerait-il s'ils devaient rédiger une règle personnalisée pour chaque pays dans lequel ils se trouvent, mais aussi pour chaque pays dans lequel ils ne se trouvent pas, car ces pays ne devraient pas pouvoir la consulter ? Cela n'est pas évolutif lorsque l'on parle de dizaines de milliers de sources de données pour les schémas, qui changent constamment. Il y a des pétaoctets de télémétrie provenant de ces moteurs.\n\nCe que vous voudriez écrire, c'est que vous ne pouvez voir les données que pour le pays où vous résidez une seule fois, puis cela s'applique partout. Mais ce n'est pas ainsi que cela se fait. Ils écriraient donc quelque chose comme 700 politiques pour un objet de données, alors qu'il ne devrait y en avoir qu'une seule. Suivre toutes les politiques et réglementations changeantes pour chaque source de données serait presque impossible. Vous seriez constamment hors conformité.\n\nIl n'a jamais été aussi facile de mondialiser une entreprise qu'aujourd'hui, et les clients d'Immuta s'attendent à ce qu'ils puissent faire fonctionner leur infrastructure à l'échelle mondiale sur n'importe quel cloud. Ils devraient être en mesure de déplacer leurs données vers n'importe quel cloud et être en conformité totale.\n\nImmuta applique des règles aux données par le biais de balises plutôt que d'utiliser les données brutes car chaque domaine a son propre jargon pour classer et parler de ses données. Ils ont donc commencé à classer et à baliser de manière générique en utilisant des modèles de concepts généraux, tels que pour la loi HIPAA. Cependant, ces modèles ne sont pas encore infaillibles et il reste encore beaucoup à faire.\n\nLa vision de Matt en tant que PDG d'Immuta est d'atteindre un état où les domaines peuvent partager leurs politiques. Par exemple, dans le domaine de la santé, il existe une bonne raison pour laquelle Moderna et Pfizer souhaiteraient travailler ensemble. Il pourrait y avoir un accord sur la manière de gérer les contrôles des données du monde réel avec le milieu universitaire. Donc, s'il existait une politique cohérente pouvant être partagée et co-créée dans un nuage de politiques conceptuelles, ce serait la bonne chose à faire.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matthew Carroll"],"link":"/episode-EDT70-fr","image":"./episodes/edt-70/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte en chef de solutions chez Intel, discute de la réalité et de l'avenir de la gouvernance moderne des données avec Matthew Carroll, PDG d'Immuta."},{"id":144,"type":"Episode","title":"Gouvernance des données modernes","tags":["datagovernance","data","immuta","dataaccess","identitymanagement","ceo","technology","process"],"body":"\r\n\r\nDans la gouvernance des données modernes, la première hypothèse est qu'il faut séparer la politique de la plateforme.\n\nLe deuxième point est qu'on ne peut pas avoir d'ontologie. Si quelqu'un essaie de créer un super schéma de tout, c'est impossible, mais il faut un schéma pour appliquer une politique. Donc, si un utilisateur veut joindre deux colonnes, il faut avoir une nouvelle politique. Les données sensibles doivent être découvertes, à la fois par des identifiants directs et indirects. Les identifiants indirects sont plus difficiles, et des techniques doivent être appliquées pour atténuer le risque d'attaques de liens.\n\nLa troisième prémisse est l'introduction d'une série de techniques améliorant la confidentialité telles que le masquage, la caviardage, l'autorisation, etc.\n\nCes outils sont automatisés dans le cadre de la gouvernance moderne des données. Un exemple parallèle est la façon dont vous aviez l'habitude d'être un sorcier pour supprimer les yeux rouges sur une photo. Maintenant, vous cliquez sur un bouton et les yeux rouges disparaissent. C'est la même chose en matière de confidentialité. Nous avons besoin de ce bouton facile pour trouver automatiquement un identifiant potentiellement indirect lorsque le risque de ré-identification est supérieur à très faible.\n\nMis à part la confidentialité, il y a une myriade d'autres choses avec les données qui devraient être automatisées avant leur classification. Bien que Immuta ne se penche pas sur la transformation des données et ces flux de données, ils fournissent une API et une interface en ligne de commande. Les ingénieurs qui construisent ces pipelines peuvent faire leur travail, et Immuta leur donne les règles, et la mise à jour des règles peut faire partie du pipeline. Ils veulent être un lien vers cette couche.\n\nIl y a de nouveaux rôles dans cet domaine, tels que le data steward et le data governance engineer, qui sont séparés du data engineer. De plus, les utilisateurs de données sont vraiment trois utilisateurs distincts avec trois besoins différents.\n\nTout d'abord, il y a des data scientists qui ont des compétences spécialisées et ont besoin de données d'une manière spécifique. Parfois, ils ont besoin d'outils spécifiques et d'un environnement de calcul spécifique quelque part dans le monde pour accomplir leur mission.\n\nD'autre part, les ingénieurs de données et les ingénieurs analytiques, dont l'un construit les pipelines et l'autre les maintient, ont besoin d'un accès rapide à une solution d'urgence lorsque quelque chose se casse. Ils l'intègrent dans un pipeline et le livrent à quelqu'un en vérifiant qu'il est à jour.\n\nFinalement, il y a les gouverneurs qui cherchent à se conformer aux réglementations.\n\nCes utilisateurs ont tous des opinions et des besoins très différents en matière de gouvernance. Lors de l'application d'une nouvelle gouvernance, la chose la plus importante est de traiter ces groupes comme des parties prenantes distinctes.\n\nSi vous pensez à tous ces rôles de manière binaire, en tant qu'ingénieurs de données, ce qui finit par se produire, c'est qu'il y a beaucoup de réunions, donc il est impossible de développer. Nous devons créer une relation symbiotique entre les opérations de données, la science des données, l'analyse des données et la gouvernance. Un modèle d'exemple est Salesforce ou ServiceNow, où il y a un flux de travail complet du début à la fin et aucune réunion n'est nécessaire. Cela serait une véritable opération des données.\n\nImmuta a, philosophiquement, une approche basée sur les attributs plutôt qu'une approche basée sur les rôles. Le problème avec une approche basée sur les rôles est que vous augmentez le volume inévitablement en ajoutant constamment des rôles. Par exemple, une organisation pharmaceutique avait plus de 800 000 rôles, car les rôles ne peuvent jamais être supprimés en raison de la nécessité de reproduire des essais de médicaments. L'excès de rôles peut rapidement devenir un problème d'échelle.\n\nL'accès basé sur les attributs est la clé pour contrer cela. Au lieu d'ajouter constamment des rôles, les utilisateurs ont des attributs spécifiques et cohérents. Par exemple, un attribut d'un utilisateur pourrait être qu'il est identifié de telle sorte qu'il ne peut voir que son propre état. Avec un accès basé sur les rôles, chaque état, qu'ils puissent le voir ou non, devrait être écrit. Cette gestion moderne de l'identité est très évolutive. L'accès aux attributs simplifie le nombre de politiques à rédiger et aide à améliorer les performances.\n\nLes réglementations mondiales modernes telles que le RGPD, cependant, exigent également un but. C'est là que l'accès aux attributs devient important : quelles finalités chaque personne peut-elle exercer ? Selon une EULA, les données doivent être traitées par les utilisateurs uniquement pour la raison indiquée. Sinon, une analyse des risques doit être effectuée avant que les données soient opérationnellement utilisées à des fins de production.\n\nEn ce moment, nous sommes au début de la gouvernance moderne des données. Actuellement, les utilisateurs prennent une décision binaire unique concernant les données : consentement ou refus de consentement. L'avenir se situe quelque part au milieu : le consentement limité. Par exemple, si une personne confie ses données génomiques à une entreprise comme ancestry.com, qu'est-ce que cela signifie pour son enfant à l'avenir ? L'enfant n'a pas donné son consentement pour que son matériel génétique soit transmis et éventuellement examiné, disons, par une compagnie d'assurance santé pour déterminer les risques. Dans la gouvernance moderne des données, Matt voit les consommateurs donner un consentement limité, par exemple en autorisant une entreprise à analyser uniquement l'ADN pour des raisons généalogiques et rien d'autre.\n\nLe futur doit être basé sur le consentement et l'accès basé sur l'objectif, car finalement, les données dérivées génèrent des connaissances, tandis que le développement de l'apprentissage automatique intègre les données dans les algorithmes.\n\nPour en savoir plus sur Immuta et la création d'un programme de gouvernance des données, rendez-vous sur Immuta.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matthew Carroll"],"link":"/episode-EDT71-fr","image":"./episodes/edt-71/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte en chef des solutions chez Intel, poursuit sa discussion approfondie sur la réalité et l'avenir de la gouvernance moderne des données avec Matthew Carroll, PDG d'Immuta. Dans cet épisode, ils discutent de la Classification des données, des Politiques et de la Gouvernance."},{"id":145,"type":"Episode","title":"2021 Une année de rétrospective","tags":["covid","edge","compute","cybersecurity"],"body":"\r\n\r\nUne grande attente pour 2021 était que le nouveau vaccin nous permettrait de sortir de la COVID et que les voyages reprendraient. Darren s'attendait à voir ses clients en personne plutôt que via Zoom ou des conférences en ligne.\n\nD'autres attentes concernaient la mise en place de modèles de travail hybrides, et bien que l'apprentissage à distance dans l'éducation était à son apogée, il y avait l'espoir d'un retour aux salles de classe en personne. Nous nous attendions à observer une augmentation des services à la clientèle sans contact ou sans friction. L'industrie des petits boulots tels qu'UberEats était également en plein essor.\n\nDans l'industrie, nous espérions voir une grande impulsion pour l'IA et l'apprentissage automatique (ML) à mesure que de nouvelles technologies deviennent disponibles. Nous pensions également constater une accélération de l'industrie 4.0 avec de nouvelles automatisations, car les usines avaient toujours besoin de produire malgré la diminution du nombre d'employés.\n\nNous avions de grandes attentes pour 2021, et ça n'a pas déçu. Il y a cependant eu quelques surprises.\n\n## Surprises en 2021\n\nBien que le vaccin ait aidé, il n'a pas éradiqué le COVID. Nous avons connu quatre vagues, et nous sommes actuellement au milieu de la vague Omicron début 2022. Cependant, tout le monde n'était pas prêt à retourner au travail, et les organisations continuent de mettre en place des plans de travail hybrides.\n\n2021 a également marqué la grande démission, avec un grand nombre de personnes changeant d'emploi, peut-être à cause de l'épuisement professionnel, de différentes opportunités ou simplement à cause de l'incertitude quant à leur retour au bureau.\n\nLes services informatiques sont devenus les héros silencieux de la continuité des activités, car ils sont devenus encore plus flexibles avec la capacité de s'adapter rapidement pour répondre à tous les besoins différents des employés et de leurs clients.\n\nUne autre grande surprise était que des industries majeures en dehors de la haute technologie ont été touchées par des logiciels de rançon, comme l'industrie de la transformation de la viande et un pipeline. La sécurité est alors devenue un concept important.\n\nIntel a également eu une grande surprise, avec le retour de Pat Gelsinger en tant que PDG.\n\nCOVID continue de semer le chaos à travers le monde.\n\nCOVID sera probablement l'événement pivot ou le cygne noir du siècle. Les économies mondiales se sont ajustées pour refléter l'incertitude causée par les quatre vagues de flambées de COVID, et les hauts et les bas ont semé le chaos dans les plans d'entreprise visant à faire revenir les gens au bureau.\n\n## Modèles de travail hybrides créés\n\nLes organisations ont mis au point de nombreux excellents modèles de travail hybride, mais beaucoup ne sont pas encore utilisés car les dates de retour au travail sont constamment repoussées en raison de flambées épidémiques, comme la vague actuelle d'Omicron en janvier 2022. Ce que nous avons pu observer, c'est un changement majeur de culture où les gens s'installent dans le travail à distance et éventuellement le travail hybride. Les organisations informatiques se sont préparées aux allers-retours entre le bureau et la maison.\n\nLe travail quotidien a fondamentalement évolué du fait que les personnes stockent leur travail sur une machine au bureau vers le stockage dans le cloud. Ou bien, les choses sont stockées sur un appareil portable et répliquées dans le cloud. Ainsi, nous avons assisté à un énorme changement dans les offres basées sur le cloud et les logiciels en tant que service (SaaS) comme Office 365, ainsi que dans les outils de collaboration tels que Zoom et Teams. De plus, l'intégration à distance des nouveaux employés est devenue courante. Tout cela a eu un impact sur les personnes, les processus et la technologie.\n\n## Lutter contre la Grande Démission\n\nPour retenir les employés face à la grande démission, les organisations doivent d'abord s'intéresser au bien-être des employés. Avec le stress et l'isolement provoqués par la pandémie, de plus en plus d'organisations cherchent à aider les employés en matière de santé mentale et émotionnelle.\n\nLa flexibilité est une autre clé ; avec le travail à distance, les horaires et le lieu peuvent être flexibles. Les employés peuvent vivre dans un autre État ou avoir des horaires non traditionnels. Cette flexibilité et l'absence de trajet quotidien ont permis aux gens de s'impliquer davantage et d'explorer leur propre communauté.\n\nLes organisations des ressources humaines commencent à utiliser l'intelligence artificielle pour découvrir pourquoi les gens partent, identifier les tendances et déterminer quels programmes aideront à augmenter la productivité et aideront les employés à se sentir comme faisant partie de l'équipe. Les organisations se tournent également vers l'automatisation. Il y a eu une augmentation de l'industrie de l'automatisation des processus robotiques pour faire face à moins d'employés.\n\n## Le rythme implacable de l'informatique ne s'est pas ralenti.\n\nLa technologie de l'information ne peut pas suivre le rythme héroïque qui était requis au début de la COVID, donc même si les heures folles ont diminué, la demande est satisfaite de manière stratégique. Nous avons constaté un mouvement massif vers des offres SaaS, de sorte que les employés ne gèrent plus, par exemple, des tâches qui ne correspondent pas forcément à leurs compétences. Au lieu de cela, ils peuvent se tourner vers un fournisseur de services cloud, ou des prestataires de services en RH, ventes ou systèmes ERP. Un mouvement vers l'automatisation et des processus répétables a permis de soulager une partie de la pression liée au travail quotidien de bureau. Il y a également eu un investissement accru dans les technologies DevOps et RPA pour aider à rationaliser et sécuriser le développement de produits.\n\n## Sécurité, Chaîne d'Approvisionnement et Rançongiciel.\n\nLes problèmes de sécurité ont pris la plupart des industries par surprise. Des industries telles que l'emballage de viande, le pétrole et le gaz, les hôpitaux, ainsi que les entreprises de chaîne d'approvisionnement et de logistique, ont été touchées par des violations de sécurité. En premier lieu, cela était dû à la nécessité de passer rapidement au télétravail et de mettre de côté la prudence en matière de sécurité. Par exemple, il se peut que les organisations n'aient pas effectué la formation adéquate des travailleurs à distance sur la sécurisation de leurs ordinateurs portables ou de leurs données. Parmi les autres leçons apprises, nous assisterons à l'émergence de la confiance zéro cette année.\n\nUn autre gros problème de cette année était les problèmes de chaîne d'approvisionnement, et pas seulement dans l'approvisionnement en silicium. De nombreux matériaux continueront à faire défaut de manière générale, ce qui affectera particulièrement les petites entreprises.\n\nNous devons trouver des solutions pour lutter contre les ransomwares. Les organisations peuvent apporter des changements dans leurs processus et leur culture, et utiliser de nouvelles technologies pour trouver ces solutions.\n\n## Intel a un nouveau PDG.\n\nLa plus grande surprise pour Darren, et peut-être pour toute l'industrie, a été le retour de Pat Gelsinger chez Intel, cette fois en tant que PDG. Tout le monde chez Intel est inspiré par l'énergie qu'il apporte. Pat déclare qu'il va aider l'industrie à résoudre la pénurie de puces en investissant à nouveau dans la fabrication américaine. Darren croit qu'il ramènera Intel au sommet de la vente de puces et fournira une technologie merveilleuse pour le monde entier.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT73-fr","image":"./episodes/edt-73/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren réfléchit aux attentes et aux surprises de 2021."},{"id":146,"type":"Episode","title":"Ransomware: Préparez-vous et ne paniquez pas.","tags":["cybersecurity","ransomware"],"body":"\r\n\r\nBien que pour la plupart des organisations, une attaque de ransomware commence par une demande de rançon quelconque, l'histoire complète est généralement en préparation depuis plusieurs mois.\n\nLa demande de rançon survient après que les attaquants ont encrypté des informations à partir de systèmes compromis et les ont bloqués, ou après qu'ils ont volé des informations sensibles et menacent maintenant de les rendre publiques. Dans les deux cas, ils demandent une somme d'argent pour empêcher la divulgation ou rétablir les informations.\n\nLe montant d'argent peut être ciblé car les attaquants connaissent les capacités financières de l'entreprise, et ils demanderont une somme massive. Les attaques moins ciblées demanderont un montant aléatoire et espéreront obtenir un résultat. De plus en plus, cependant, les attaques sont sophistiquées et les attaquants ont fait leurs devoirs. Ils peuvent avoir obtenu une traction significative, se déplaçant latéralement à travers l'environnement et compromettant plusieurs domaines. Ils savent que cela coûtera une énorme somme à l'organisation pour rétablir leurs services, donc ils peuvent demander davantage.\n\nMais avant que les attaquants ne réclament la rançon, ils ont été présents dans le système pendant un certain temps, en moyenne un peu moins de 300 jours, jusqu'à ce qu'ils exécutent pleinement leur plan. Ils essaient d'aller le plus loin possible dans le réseau pour rendre l'impact aussi important que possible. Par exemple, ils supprimeront le responsable des sauvegardes pour paralyser l'organisation, laissant des données de sauvegarde corrompues et inutilisables. Ils passeront également du temps à compromettre des identifiants pour une élévation de privilèges afin de se déplacer latéralement dans le réseau et dénicher toute faille.\n\nChaque attaque est différente, il est donc difficile de nommer la vulnérabilité la plus courante. Les attaquants trouvent n'importe quel moyen d'accès qu'ils peuvent, que ce soit une faiblesse dans la gestion des vulnérabilités ou même chez les individus, par exemple les attaques par phishing ou les tactiques d'ingénierie sociale. Ils passeront des appels téléphoniques et se feront passer pour des cadres ou utiliseront des noms similaires à ceux des cadres pour obtenir des informations et des données auprès des employés en ligne.\n\nDe nombreux acteurs de l'industrie diront que ce n'est pas une question de savoir si, mais quand vous serez attaqué. Cependant, Stephanie ne croit pas que cela soit le cas, car avec une diligence raisonnable, une sécurisation appropriée et la prise des mesures nécessaires, vous ne serez pas ciblé.\n\nLa diligence raisonnable est ce que les experts en sécurité prêchent depuis des décennies : effectuez une gestion des vulnérabilités et des correctifs, changez les mots de passe et réduisez les privilèges si possible. Formez votre personnel. Les administrateurs ne devraient pas être sur internet avec des comptes administratifs participant à des forums, par exemple. Tout est une question de compréhension des surfaces d'attaque et de leur gestion.\n\nQue devez-vous faire si vous êtes victime d'une attaque de ransomware ? Stephanie dit que ce que les gens devraient faire et ce qu'ils font réellement sont deux choses différentes. La première réaction instinctive des services informatiques est souvent de redémarrer, de corriger ou de modifier l'environnement de manière significative. Cela ne fera qu'alerter les attaquants que leur temps est écoulé. Pour l'enquête forensique, il est extrêmement important de ne pas perturber ces environnements. Effectuez le moins de frappes au clavier possible et ne redémarrez certainement pas.\n\nCertaines entreprises vont paniquer et se livrer à des déni de services sur elles-mêmes, passant en mode verrouillage total, en fermant tout. Au lieu de paniquer, elles devraient s'appuyer sur leurs politiques de gestion des incidents et réaliser que le problème peut être résolu même si ce ne sera pas agréable.\n\nDe toute évidence, chaque entreprise devrait avoir ces politiques de réponse aux incidents qui peuvent être rapidement mises en œuvre pour gérer les communications internes et avec les médias, et maintenir les activités de l'entreprise autant que possible. Les professionnels de la sécurité peuvent aider à mettre en place ces plans de réponse et peuvent intervenir pendant une attaque.\n\nLeur première étape pour préserver les informations est d'observer certaines des activités douteuses qui se déroulent sur le réseau. Les attaques de ransomware ne commencent pas par le chiffrement ou le vol d'informations; il y a de nombreuses attaques préalables. Il est important d'identifier d'où elles viennent, où elles ont commencé et où elles ont été. Pour cela, les professionnels de la sécurité ont besoin de preuves et d'informations qui doivent être correctement préservées. Un bon début consiste à placer les bonnes personnes au bon endroit pour gérer ce qui se passe.\n\nSuivant est de gérer correctement l'environnement. Malheureusement, une fois qu'il y ait un ransomware ou toute autre violation ou incident, l'organisation est très vulnérable. Cent pour cent du temps, lorsque les environnements ont subi des attaques réussies de ransomwares, médiatisées ou non, ils sont ciblés par le même groupe ou un groupe différent. C'est comme un animal blessé avec des vautours qui tournent autour. Les attaquants savent que vous êtes blessé et vulnérable. Une autre attaque est imminente.\n\nLe plus souvent, lorsque les professionnels de la sécurité mènent leur enquête, ils découvrent d'autres indicateurs d'attaque et de compromission dans différentes parties du réseau. Ils doivent déterminer s'il s'agit de la même attaque ou d'une attaque différente. Cette enquête est une partie essentielle de la récupération des logiciels malveillants, car même lorsque vous pensez avoir éliminé une attaque et que l'entreprise fonctionne à nouveau correctement, il existe toujours un potentiel pour ces autres attaques.\n\nUne attaque typique coûte en moyenne quatre millions et demi de dollars pour nettoyer, et cela n'inclut pas la rançon. Le montant peut être beaucoup plus élevé et est proportionnel à la taille de l'organisation.\n\nIl est impossible d'être certain du nombre d'organisations qui paient la rançon. Beaucoup de rançons sont assorties de menaces de ne pas contacter les forces de l'ordre ou de divulguer l'attaque. Pour cette raison, les statistiques disponibles concernant le nombre d'organisations qui paient sont variées.\n\nCertaines organisations répondent en disant qu'elles ne paieront en aucune circonstance, même si cela coûte plus cher de reconstruire, ce qui détruit ainsi la possibilité de négocier. C'est une décision émotionnelle qui peut troubler le jugement. En fin de compte, si l'objectif est de continuer à faire des affaires et à gagner de l'argent, payer une rançon faible, peut-être de dix ou vingt mille dollars, sera moins cher que les analyses médico-légales et le reste du processus. D'un autre côté, il existe des cas documentés où la rançon a été payée et les données n'ont pas été entièrement restaurées. De plus, l'organisation n'a aucune garantie de la sécurité de son environnement. Il n'y a aucune garantie lorsque vous payez une rançon; vous demandez aux criminels d'agir de bonne foi.\n\nDe nombreux gouvernements à travers le monde ont rendu illégal le fait de payer une rançon car les attaquants sont considérés comme des terroristes, et il est interdit de négocier avec des terroristes. Une autre difficulté est que les attaquants refusent parfois de traiter avec des négociateurs professionnels. Ils désignent souvent une personne au sein de l'organisation comme le seul interlocuteur avec qui ils accepteront de négocier, espérant ainsi que cette personne prendra des décisions émotionnelles.\n\nLes menaces sont communiquées de différentes manières : par e-mail, par téléphone, et même en changeant l'arrière-plan du bureau.\n\nLa meilleure stratégie pour éviter ou atténuer une attaque est de ne pas attendre que votre organisation se retrouve dans cette situation. Au contraire, engagez-vous dans une due diligence. Effectuez des évaluations au moins une fois par an afin de repérer les failles de sécurité. Les menaces et les attaques évoluent constamment et deviennent de plus en plus sophistiquées, il est donc essentiel que la sécurité de votre organisation suive le rythme. Effectuez une surveillance continue et appliquez des correctifs. Mettez en place une gestion des vulnérabilités, changez régulièrement les mots de passe et rappelez et sensibilisez les utilisateurs aux menaces. Ces stratégies ne sont pas nouvelles. Les professionnels de la sécurité les recommandent depuis des décennies ; c'est simplement que les organisations échouent à les mettre en œuvre correctement et à évoluer.\n\nIl est également judicieux de faire appel à des experts pour guider le processus de plans et exercices de réponse aux incidents. Tout le monde au sein de l'organisation devrait savoir quoi faire et qui appeler dans un scénario d'attaque afin d'éviter tout dommage supplémentaire.\n\nSi votre organisation est attaquée, espérons que l'impact sera minimal, ou du moins contenu et gérable s'il y a eu une préparation. Quel que soit le temps, l'énergie et l'argent qu'une entreprise investit dans des mesures préventives représentent une fraction minime du coût d'une attaque.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Stephanie Sabatini"],"link":"/episode-EDT74-fr","image":"./episodes/edt-74/fr/thumbnail.jpeg","lang":"fr","summary":"Darren Pulsipher, Architecte principal des solutions d'Intel, s'entretient avec Stephanie Sabatini, Directrice principale des services professionnels de Hitachi Systems Security, sur la prévention et la préparation des attaques de ransomware, ainsi que sur les mesures à prendre si votre organisation est attaquée."},{"id":147,"type":"Episode","title":"Solutions de cybersécurité avec Hitachi","tags":["cybersecurity","technology"],"body":"\r\n\r\nColin travaille chez Hitachi depuis près de 24 ans après avoir travaillé pour IBM juste après l'université. Il se concentre principalement sur les comptes dans le Nord-Ouest du Pacifique et sur les comptes des administrations locales, de l'éducation et du gouvernement. Il a eu l'occasion inhabituelle de prendre le poste de son père lorsque celui-ci a pris sa retraite de Hitachi.\n\nDavid travaille dans le domaine de l'informatique depuis plus de vingt ans, commençant juste avant la folie des .com. Il a été un homme de terrain pour la majeure partie de sa carrière, s'occupant des serveurs, du stockage et de la sécurité.\n\nBeaucoup de gens voient Hitachi comme une entreprise produisant des équipements lourds ou des outils électriques, voire même des machines à pain. En réalité, Hitachi est une société d'ingénierie et de fabrication vieille de cent ans qui a commencé à produire des moteurs électriques en 1910 et s'est transformée en des centaines d'organisations et de filiales à part entière, dont Hitachi Vantara. Hitachi Vantara est la plus grande filiale à part entière, axée principalement sur la gestion des données. Ils construisent et fournissent non seulement des systèmes de stockage d'entreprise traditionnels, mais ils se sont également développés en tant qu'entreprise de solutions numériques, embrassant l'expansion extrême dans l'Internet des objets et les insights basés sur les données.\n\nRobert Mueller a déclaré : \"Il n'y a que deux types d'entreprises : celles qui ont été piratées et celles qui le seront. Même cela est en train de se fusionner en une seule catégorie : celles qui ont été piratées et le seront à nouveau.\" Cette citation pose le cadre de la manière dont Hitachi peut aider les entreprises. Ce n'est pas une question de \"si\", mais de \"quand\" vous serez piraté, et quand vous le serez à nouveau. Cela s'applique non seulement aux entreprises, mais aussi aux organisations de l'espace public.\n\nHitachi possède une division fédérale qui opère depuis Washington D.C., principalement axée sur le Département de la Défense et les comptes fédéraux. Hitachi Vantara travaille beaucoup avec des comptes SLED et collabore avec la division fédérale pour fournir des solutions adaptées aux besoins fédéraux, étatiques et locaux.\n\nLa meilleure façon de visualiser une solution de sécurité de Hitachi est d'imaginer votre organisation comme votre maison, et dans cette maison, il y a quatre éléments de protection que nous devons déployer.\n\nLa première est la sécurité physique. Hitachi dispose d'un système intelligent, une solution d'intelligence visuelle, qui peut fournir une surveillance vidéo intelligente. Il peut tout faire, de la détection de coups de feu au suivi des plaques d'immatriculation en passant par la reconnaissance faciale. Hitachi déploie ces systèmes partout dans le monde. Les caméras Hitachi ont détecté de nombreux événements au centre commercial de Washington l'année dernière et sont utilisées par les forces de l'ordre à travers tout le pays. Tout cela découle de la division des transports de Hitachi, qui dépend fortement de la technologie vidéo pour la sécurité.\n\nLa sécurité physique est rarement un sujet de discussion, mais c'est un aspect clé. Si vous ne pouvez pas contrôler votre propre bâtiment et votre propre centre de données, alors vous n'avez pas de sécurité. Si quelqu'un peut passer par la porte, il peut entrer dans votre centre de données.\n\nAprès la sécurité physique, vient Hitachi ID. C'est essentiellement le verrou de votre porte d'entrée. Une récente enquête a révélé que seulement 40% des entreprises disposent d'une gestion des accès privilégiés, tandis que environ 74% ont une gestion des identités. Cela n'est pas suffisant car il y a beaucoup d'exposition, et l'espace SLED est une cible de haut niveau. Hitachi ID propose des solutions de privilèges de mot de passe et d'identité sur une seule plateforme ; c'est une solution complète où vous pouvez identifier non seulement les utilisateurs, mais aussi les appareils et les applications, sur site et dans le cloud.\n\nLe troisième aspect concerne la manière de protéger vos données lorsque quelqu'un parvient à passer la porte.\n\nLa cybersécurité est un problème de big data, et Pentaho est l'outil adapté pour le big data. Avec Pentaho, vous pouvez gérer toutes vos sources de données, contrôler votre lac de données et produire des résultats utiles. Hitachi a découvert que les scientifiques des données et les analystes en cybersécurité passent 80 pourcent de leur temps à gérer, sourcer et nettoyer les données, et seulement 20 pourcent de leur temps à les analyser réellement. Avec Pentaho, ces chiffres s'inversent et ils peuvent passer 80 pourcent de leur temps à analyser les données et à effectuer une analyse des menaces en temps réel ainsi qu'à y répondre.\n\nPentaho est un système de détection d'intrusions, mais c'est aussi un système de prévention. Par exemple, une grande entreprise énergétique avait tout fait de manière ad hoc, en écrivant ses propres scripts en interne. Ils ont réorganisé toute cette fonctionnalité dans Pentaho en une journée. Ensuite, ils ont été capables de réaliser cette analyse en temps réel et de suivre les attaquants au lieu d'être constamment en retard. Ce qui rend Pentaho unique dans un domaine encombré, c'est sa flexibilité. Il peut réaliser tout type d'analyse de données, en fonction des besoins de votre organisation.\n\nProtéger les données d'un point de vue de contenu est également très important pour l'espace SLED. Hitachi Content Platform Anywhere offre une synchronisation sécurisée des fichiers, un partage sécurisé et une boîte de dépôt sécurisée. Il est comparable à Google Drive ou à Dropbox, par exemple, mais la différence est qu'HCP Anywhere est contrôlé par votre propre équipe de sécurité.\n\nIl est utilisé par le ministère de la Défense, qui l'a renommé Mill Drive. Les forces sur le terrain transmettent des données sensibles de différents endroits secrets vers le quartier général pour des missions de reconnaissance et autres. Au niveau local, les villes l'utilisent en transmettant des preuves vidéo non seulement à l'administration, à la police et à d'autres agences, mais également au bureau du procureur pour la gestion des preuves.\n\nHCP Anywhere est construit sur la plateforme de contenu Hitachi, qui est leur stockage d'objets. Il dispose d'un stockage d'objets intégré, non seulement celui de Hitachi, mais également tous les stockages d'objets. Au lieu d'écraser un fichier, ce qui permet à un attaquant de le remplacer, vous créez une nouvelle version du fichier. Ensuite, si vous êtes attaqué, vous pouvez revenir à une version antérieure avant l'attaque. Certaines agences de notation conviennent que le stockage d'objets de Hitachi est le meilleur sur le marché. Par défaut, vous obtenez 16 versions de chaque fichier, 16 éléments de métadonnées système et un nombre illimité de balises de métadonnées personnalisées également.\n\nHitachi dispose également d'une console de gestion des politiques de données, en plus du Hitachi Content Platform Gateway, qui met en place un système NAS devant le stockage objet pouvant être soit NFS soit CIFS. S'ajoute également Hitachi Content Intelligence ; c'est un moteur de recherche surpuissant. Vous pouvez trouver n'importe quel objet dans votre stockage en fonction des métadonnées, de la date, de l'heure, etc. Il peut être étendu au-delà de votre propre matériel physique (ou celui de Hitachi ?) à travers plusieurs magasins d'objets. La recherche fonctionnera sur n'importe quoi. Il est également 100 % compatible avec AWS S3.\n\nPour les clients de SLED, il existe une opportunité avec le partenaire Flexential de Hitachi pour fournir la plateforme de contenu Hitachi en tant que service. Si, par exemple, un gouvernement municipal ne souhaite pas appliquer son propre stockage d'objets, il peut se trouver dans un environnement multi-locataire grâce à ce partenariat.\n\nFinalement, Hitachi System Security peut vous aider si vous êtes attaqués par un ransomware. Ils peuvent fournir une analyse de l'impact et négocier avec les attaquants ransomwares. Ils peuvent commencer à construire des postures défensives autour de votre organisation. Les attaquants ne sont pas des joueurs solitaires, mais des organisations criminelles avec des développeurs et des responsables, ce qui nécessite une réponse professionnelle. Hitachi System Security peut accompagner une organisation du début à la fin.\n\nSi vous êtes membre de l'ISSA, recherchez Hitachi pour parrainer un événement dans votre région.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Colin McLean","David Rowley"],"link":"/episode-EDT75-fr","image":"./episodes/edt-75/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, Architecte en chef des solutions chez Intel, parle des offres de sécurité informatique à 360 degrés de Hitachi avec Colin McLean, responsable mondial de l'équipe Intel, et David Rowley, consultant principal en solutions chez Hitachi."},{"id":148,"type":"Episode","title":"Une journée dans la vie d'un architecte de solutions cloud.","tags":["csa","cloudsolutionarchitect","multicloud"],"body":"\r\n\r\nL'équipe d'architectes de solutions Cloud d'Intel s'est formée il y a moins de deux ans avec quelques personnes et a rapidement grandi pour compter 21 membres. Cette croissance témoigne de son importance à la fois au sein d'Intel et vis-à-vis des clients.\n\nAlors que les clients passent de l'infrastructure sur site au cloud pour plus de flexibilité et de scalabilité, ils doivent comprendre que l'infrastructure sous-jacente et les fonctionnalités sont importantes pour l'optimisation. Étant donné que la majorité des fournisseurs de services cloud, y compris les trois principaux, Google, Amazon et AWS, sont alimentés par Intel, les CSAs d'Intel peuvent aider à optimiser les services tout au long du parcours cloud d'un client.\n\nUne journée typique pour un CSA commence par des tâches de bureau telles que les emails, les réunions internes, les réunions d'équipe, les réunions du personnel, et une quantité importante de temps passé à parler aux clients utilisateurs finaux. Il peut s'agir de clients Intel qui utilisent actuellement des processeurs Intel sur site et qui envisagent maintenant de passer au cloud, ou qui sont peut-être déjà dans le cloud. Les CSA ont également des partenaires, des intégrateurs de systèmes, qui s'intéressent à la manière de traduire la cohérence d'Intel et à certaines fonctionnalités telles que le pare-feu ABS ou Boost ou le chiffrement.\n\nLes CSAs ne travaillent pas seulement dans le rôle de prévente, mais ils conçoivent également des preuves de concept, rédigent des cas d'utilisation et travaillent avec une équipe pour effectuer des tests basés sur la charge de travail du client. Ils consacrent également du temps à la rédaction ou à la co-rédaction de playbooks.\n\nIls créent également un guide de formation pour les cadres d'Intel et les commerciaux partenaires.\n\nDe plus, les CSP annoncent de nouveaux services chaque jour et les CSAs doivent rester à jour avec la technologie soutenant ces services et comprendre comment ces services peuvent bénéficier aux clients. Cela signifie comprendre la charge de travail des utilisateurs et utiliser également des modèles.\n\nLes CSAs d'Intel se considèrent comme des conseillers de confiance, une extension de l'équipe du client, plutôt que d'essayer de prendre en charge la prise de décision. Bien des fois, par exemple, un client saura quel service cloud il souhaite utiliser, mais aura besoin d'aide pour examiner les outils disponibles afin de l'assister dans l'analyse de sa charge de travail actuelle, puis pour les corréler avec les instances disponibles dans le cloud. Les CSAs aideront également et formeront les clients à l'analyse des coûts. Intel dispose d'un ensemble d'outils pour chaque étape du parcours cloud.\n\nUne fois qu'un client est opérationnel et a terminé le projet initial, il contactera les Agents du service à la clientèle (CSAs) pour de nouvelles initiatives et projets, au fur et à mesure que le cycle se poursuit.\n\nLes CSAs ont un ensemble de compétences unique, car ils passent beaucoup de temps à écrire, à communiquer et à éduquer, tout en comprenant également tous les aspects techniques et les besoins des clients. Par exemple, un CSA doit être en mesure de discerner si un problème est un problème technique ou un problème commercial. Les antécédents des CSAs d'Intel sont diversifiés avec différentes cultures, compétences techniques, expérience de vente et parcours professionnel, ce qui leur confère une expertise dans différents domaines. Ils constituent également une source technique approfondie au-delà de leur propre équipe. Si, par exemple, quelqu'un souhaite discuter en détail des services AWS, ils peuvent faire appel à un collègue d'AWS. En d'autres termes, les CSAs d'Intel peuvent être un point de contact unique pour tous les besoins des services cloud du client.\n\nPourquoi un client devrait-il utiliser l'équipe CSA d'Intel plutôt que celle d'un fournisseur de services cloud (CSP) ? Intel est véritablement agnostique, car ses puces et processeurs fonctionnent presque avec tous les fournisseurs de services cloud. Le CSA d'Intel ne fait aucune différence quant au service choisi par le client. De plus, bon nombre des CSA des fournisseurs ne comprennent pas les fonctionnalités Intel sous-jacentes, qui sont disponibles uniquement pour certains types d'instances. Par exemple, un CSA d'Intel saurait choisir une instance alimentée par Ice Lake plutôt que Cascade Lake, ce qui permettrait des économies importantes et une amélioration des performances de leurs applications. Les CSA des CSP ne seraient pas nécessairement au courant de ces informations.\n\nIntel dispose également de nombreux outils pour collecter des données télémétriques, que ce soit pour une instance cloud ou une instance cloud bare metal. Les CSP CSAs n'ont pas accès à ces outils. Ils peuvent effectuer des opérations de dépannage, mais uniquement au niveau de l'hyperviseur. Ainsi, si un client rencontre un problème, un Intel CSA peut descendre au niveau de la puce et utiliser des outils de dépannage et des données télémétriques pour résoudre le problème.\n\nLa meilleure façon de prendre contact avec l'équipe Intel CSA est de passer par un responsable de compte. Techniquement, les CSAs font partie du groupe des ventes et du marketing, donc ils recherchent activement des opportunités, y compris des contacts du passé.\n\nLes services du CSA ne sont pas une dépense supplémentaire. En fait, Intel a un programme dans lequel ils financeront la migration initiale vers le cloud. Les CSAs apporteront les outils, les personnes et l'expertise tant d'un point de vue des ressources humaines que de la surveillance. Cela aidera un client dans un environnement vierge à réduire la courbe d'apprentissage. Ensuite, la monétisation intervient lorsque la charge de travail est entièrement opérationnelle dans le cloud ou lorsqu'une migration est en cours et qu'ils consomment des ressources.\n\nLes agents de commerce spécialisés d'Intel n'ont aucun intérêt particulier quant aux fournisseurs de services cloud utilisés par les clients, mais veillent uniquement à ce qu'ils soient optimisés dans un environnement hautement sécurisé et fiable.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Kiran Agrahara"],"link":"/episode-EDT76-fr","image":"./episodes/edt-76/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren discute avec Kiran Agrahara de ce que les architectes de solutions Cloud d'Intel (CSA) font dans une journée pour bénéficier non seulement aux fournisseurs de services Cloud (CSP), mais aussi aux utilisateurs finaux."},{"id":149,"type":"Episode","title":"Transformation numérique en 2022","tags":["aiml","comms","data","edge","multicloud","cybersecurity"],"body":"\r\n\r\nEn tant que chef du service des données pour le secteur public d'Intel, Gretchen passe ses journées à discuter avec les clients de leurs défis liés aux données, de la gestion des données, de la gouvernance des données, de l'éthique concernant ce qu'ils font et de l'intelligence artificielle responsable.\n\nLes six années d'Anna chez Intel ont été axées sur l'IoT et les technologies de pointe, avec les trois dernières années passées dans le secteur public. Les deux s'accordent à dire que 2022 sera une année passionnante, remplie de possibilités révolutionnaires.\n\nDarren, Gretchen et Anna représentent chacun différentes parties de la transformation numérique avec les clients, mais se sont réunis pour fournir une manière commune de parler du sujet, avec six piliers de la transformation numérique : le calcul multi-cloud, le calcul périphérique, l'intelligence artificielle, l'apprentissage automatique, la cybersécurité, la gestion des données et les communications. Ils sont d'accord pour dire que ce seront les domaines des grandes transformations dans l'année à venir. Intel est profondément impliqué dans chacun d'entre eux, car ils ont développé le matériel et le logiciel pour soutenir chacun des piliers. Les piliers sont interconnectés et Intel joue un rôle clé dans l'avancement de l'industrie.\n\nLe matériel d'Intel constitue la base de la plupart des fournisseurs de cloud. Du point de vue logiciel, Intel a optimisé les cadres communs que les gens utilisent pour l'IA, le ML ou l'apprentissage approfondi afin de tirer le meilleur parti du matériel sous-jacent. Dans certains cas, les performances sont dix à cent fois meilleures grâce au logiciel d'Intel.\n\nNon seulement Intel a fourni du matériel pour les réseaux périphériques et de communication, en particulier dans le domaine de la 5G, mais il a également enrichi ces domaines d'activité avec des architectures logicielles de référence. De plus, Intel s'efforce de faire fonctionner les écosystèmes pour tout le monde. Il se concentre fortement sur les systèmes ouverts d'une manière non propriétaire, ce qui facilite l'entrée de nouveaux acteurs et permet aux acteurs existants de se développer davantage sur ces nouveaux marchés et d'offrir des avancées passionnantes. Intel est l'un, voire peut-être le principal, contributeur à la communauté du logiciel open-source.\n\n## Pilier de la cybersécurité\n\nImplicites dans les conceptions matérielles d'Intel se trouvent des fonctionnalités de sécurité et des capacités permettant aux clients de l'écosystème de protéger leurs données dans toutes leurs différentes permutations.\n\nLa sécurité n'est jamais statique ; elle évolue constamment. Intel seul ne va pas résoudre les problèmes de sécurité d'une organisation. La sécurité est une interaction entre ce que vous faites avec votre matériel, comment vous intégrez les bons éléments logiciels et les limites et politiques de votre organisation.\n\nIntel se concentre sur plusieurs domaines, mais un rôle fondamental est assuré par la sécurité matérielle et l'authentification. De nombreuses fonctionnalités peuvent être intégrées directement pour y parvenir. Une étape supplémentaire consiste à mettre en place des chaînes d'approvisionnement fiables ou transparentes que Intel peut partager avec ses clients, instaurant ainsi un haut degré de confiance. Ces capacités s'améliorent constamment et Intel travaille sans cesse à progresser.\n\nLes cyberattaques préoccupent de nombreux clients en raison des récents incidents. Intel dispose de mesures de sécurité telles que l'exécution de confiance et les enclaves sécurisées. Ils possèdent un ensemble complet de capacités pour le type de sécurité que vous souhaitez, telles que le chiffrement, sans subir de perte importante de performance. Il existe une matrice complète de capacités, incluant un ensemble de besoins différents en périphérie en raison de la perte de la sécurité physique du centre de données.\n\nLes architectures basées sur la confiance zéro deviennent les cadres qui sont particulièrement répandus dans le secteur public. Pour le Département de la Défense, la confiance zéro est une exigence. Intel possède de nombreuses capacités qui contribuent à cette approche.\n\nUne taille unique ne convient pas à tous en matière de sécurité, mais il est clair que la sécurité ajoutée est une pratique du passé. La sécurité doit être intégrée dès le début et doit être constamment itérée. Les organisations doivent constamment se demander si elles ont les bons protocoles en place, si elles disposent des bons outils de détection des menaces et si elles peuvent faire confiance à toute la chaîne d'approvisionnement. Tout cela est essentiel.\n\n## Pilier de l'informatique de bord\n\nMaintenant qu'il y a tant de choses qui peuvent être faites avec l'IA, l'apprentissage automatique et différents algorithmes, il est excitant de voir comment nous pouvons exploiter ces éléments sur le terrain et optimiser les architectures pour répondre à ces cas d'utilisation.\n\nIl existe certains modèles vraiment simples où tout vit dans le cloud et seule la collecte des données est effectuée en périphérie. Si la connectivité le permet, les latences peuvent correspondre aux applications. Le gros du traitement peut être centré sur le cloud. Cependant, il existe d'innombrables cas d'utilisation où cela n'a aucun sens en raison de la sensibilité des données ou des exigences de latence. Ainsi, il y a des conversations intéressantes sur la façon de déterminer les architectures optimales pour la périphérie et ce qui se passe avec le cloud et le réseau.\n\nPar exemple, pourrions-nous avoir une architecture transparente de collecte, d'utilisation et de traitement immédiat des données pour fournir des informations ? Pouvons-nous l'incorporer dans la prochaine série de formations afin que le modèle soit continuellement mis à jour ? À quelle vitesse pouvons-nous effectuer cette boucle ? Est-ce réalisable ? Avons-nous besoin de toute notre formation sur le cloud ? Si toute la formation est sur le cloud, quel est l'intervalle approprié pour redescendre ces modèles mis à jour ? Et est-ce que l'edge peut être suffisamment léger pour utiliser les éléments générés sur le cloud ? L'edge reste très complexe, et il y a de nombreuses possibilités et questions fascinantes.\n\n## Pilier de l'intelligence artificielle\n\nl'IA (intelligence artificielle) et l'apprentissage automatique permettent à l'edge (ou à la périphérie) de faire bien plus que ce que quiconque avait précédemment envisagé. Un produit unique n'est pas toujours la réponse appropriée : il s'agit de trouver la solution adaptée à chaque situation. L'utilisation du code source ouvert est essentielle et permet de bénéficier des microservices pour exécuter des algorithmes en périphérie.\n\nPar exemple, si vous avez des algorithmes directement à la source pour effectuer le travail de lecture, vous parlez de flux de trafic. Le rythme des feux rouges, jaunes et verts peut changer en temps réel en fonction du nombre de voitures qui passent, tout en collectant des données à renvoyer à un centre de données plus important qui pourrait ensuite effectuer un nouvel apprentissage. À la fin de la semaine, il peut donc être judicieux d'ajouter quelques microservices supplémentaires ou d'ajuster les algorithmes. Ensuite, ce conteneur retourne à la source et vous pouvez mieux réagir et continuer à apprendre.\n\nAussi, lorsqu'on parle de l'adéquation à l'usage, la bande passante, la latence et le format sont tous des éléments à prendre en compte.\n\nLes rendements des cultures sont un bon exemple. Un client collecte des données qui sont envoyées vers un centre de données. Ils travaillent sur les modèles, mais cela ne se traduit pas par leur permettre de savoir qu'ils ont besoin de plus d'engrais ou qu'il existe des défis actuels liés au soleil et à la pluie. Cela signifie que la formule doit être modifiée. Vous avez besoin des données sur les rendements des cultures, des informations, de l'algorithme et des microservices dans un format beaucoup plus petit. La latence et la bande passante sont différentes, mais vous pouvez avoir une petite unité au milieu d'un champ qui collecte ces données et y répond, par exemple, au flux d'eau ou aux besoins en engrais pour améliorer les rendements des cultures.\n\nEspérons que cette année, davantage de conceptions périphériques deviendront standardisées. Avec FlexRAM et la 5G, il existe des normes, mais tout le reste relève du Far West. De nombreuses personnes conçoivent des choses intéressantes, mais elles ne les conçoivent pas de manière à rendre facile l'utilisation de ces microservices dans un conteneur ainsi que des algorithmes d'apprentissage automatique (ML) et d'intelligence artificielle (AI). Dans certains cas, il est nécessaire d'utiliser plusieurs algorithmes pondérés de manière différente, qui changent chaque semaine en fonction de nouvelles données et d'un nouvel entraînement. Nous devons être en mesure de le faire d'une manière qui n'importe pas qui a construit un appareil. La création de normes non seulement dans les données d'IA et de ML, mais également au niveau périphérique, aidera à développer les capacités.\n\n## Pilier de communication\n\nLe côté commercial de la 5G ouvrira la voie avec la 5G sur les téléphones partout. Cependant, il y a toujours un décalage pour disposer du type d'équipement utilisateur nécessaire pour faire différentes applications pour la périphérie ou l'entreprise, par exemple. Intel met en place ses premiers réseaux 5G avec des partenaires plus avant-gardistes et moins orientés vers le commerce. Bien que leurs partenaires commerciaux utilisent ces réseaux depuis longtemps, les réseaux privés contrôlés à des fins spécifiques sont en train d'être libérés et des applications sont en cours de développement. 2022 est l'année où ces choses deviendront réalité.\n\n## Pilier de gestion des données\n\nLes données qui prenaient autrefois des heures voire des jours pour être ingestées, préparées, analysées et utilisées peuvent aujourd'hui être traitées en quelques minutes voire en nanosecondes. Vous pouvez également exploiter différents modèles, et lorsque les poids changent, vous pouvez agir rapidement et consulter les données afin de fournir de meilleurs services. Le déplacement des données, la gestion de ces données et l'opérationnalisation de votre IA et votre ML font partie de ce que la gestion des données apporte au monde.\n\n## Pilier Multi-Cloud\n\nLe pilier du multi-cloud ne fait pas référence aux fournisseurs de services cloud dans ce contexte. Il fait référence à l'infrastructure en général et à la façon d'abstraire cette infrastructure pour déployer de nouvelles fonctionnalités à travers le réseau, à travers un fournisseur de services cloud, voire à travers votre propre infrastructure de centres de données. L'objectif de l'architecture multi-cloud est que vous connaissiez les utilisateurs clés et, plus important encore, comment les données sont gérées.\n\nDifférents clouds ont des capacités différentes et selon les cas d'utilisation, peuvent utiliser différents clouds à des fins différentes. Intel dispose d'architectes de solutions cloud qui aident les clients à optimiser les charges de travail parmi les offres cloud.\n\nTous ces piliers sont entrelacés et travaillent ensemble. Recherchez les épisodes à venir où Darren, Gretchen et Anna poursuivent cette conversation.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Gretchen Stewart","Anna Scott"],"link":"/episode-EDT77-fr","image":"./episodes/edt-77/fr/thumbnail.png","lang":"fr","summary":"Intelligence d'Intel's Darren Pulsipher, Architecte Principale des Solutions, Dr. Anna Scott, Architecte Principale des Limites et Gretchen Stewart, Scientifique Principale des Données discutent des six piliers de la transformation numérique en 2022 : l'informatique multi-cloud, l'informatique en périphérie, l'intelligence artificielle, l'apprentissage automatique, la cybersécurité, la gestion des données et les communications."},{"id":150,"type":"Episode","title":"Compréhension du modèle de sécurité de responsabilité partagée","tags":["multicloud","compute","cybersecurity","sharedresponsibility","cloudsecurity","cloud","technology","process"],"body":"\r\n\r\n## Sécurité dans le Cloud\n\nLa plupart des gens comprennent la responsabilité de la sécurité sur site, mais la responsabilité devient plus floue dans le cloud. Si les données sont protégées dans le cloud, comment sont-elles protégées ? Qui est responsable de cette sécurité ? Que dire de la mise à jour des machines ?\n\nRécemment, les fournisseurs de services cloud ont commencé à proposer encore plus de services, il y a donc plusieurs modèles. Parfois, la sécurité finit par se perdre au milieu.\n\n## Zones de sécurité\n\nIl y a quatre domaines majeurs de sécurité que nous devons comprendre.\n\n## Physique\n\nLa sécurité physique est la partie facile à reconnaître. Les fournisseurs de services cloud sont responsables de la sécurité physique de leurs centres de données, et vous êtes responsable de la sécurité physique de vos propres centres de données. De plus, vous devez sécuriser votre espace physique. Si vous êtes un fabricant, par exemple, vous devez sécuriser les machines à l'intérieur de votre zone. Une récente intrusion a été réalisée via le système de CVC (Chauffage, Ventilation et Climatisation) qui était connecté au réseau d'une organisation.\n\n## Infrastructure is translated to French as \"l'infrastructure\".\n\nLa sécurité de l'infrastructure n'est pas l'aspect physique, mais le hardware lui-même. Est-ce que vos commutateurs réseau disposent des bons correctifs et mises à jour de sécurité? Les lecteurs et dispositifs de stockage sont-ils mis à jour? Sont-ils protégés? L'infrastructure peut tomber dans des zones grises avec les fournisseurs de services de cloud, donc vous devez savoir qui est responsable de quoi et dans quelles circonstances.\n\n## Translate the following to French: Application\n\nFrench Translation:   Application\n\nPour la sécurité de l'application, il est nécessaire de savoir qui a accès à l'application et si elle est mise à jour avec les bons correctifs de sécurité.\n\n## Les données\n\nProtéger vos données est l'une des choses les plus importantes que vous devez faire. Les données peuvent être utilisées comme une arme lors d'une attaque par ransomware, où les attaquants les prennent ou les chiffrent. Elles sont également utilisées pour obtenir des avantages concurrentiels puissants au sein de différentes organisations. Vous devez savoir où se trouvent vos données et comment les protéger.\n\n## Les modèles de prestation de services cloud\n\nIl existe trois modèles de livraison de base dans le cloud. Différents modèles sont créés tout le temps, mais les trois principaux sont l'Infrastructure en tant que Service (IAAS), la Plateforme en tant que Service (PAAS) et le Logiciel en tant que Service (SAAS). Nous catégoriserons tout le reste comme X en tant que Service (XAAS).\n\n## Infrastructure en tant que Service\n\nIAAS est le fait de louer auprès d'un fournisseur de services cloud. Cela concerne les machines virtuelles et les réseaux virtuels, donc le stockage, le calcul et le réseau. Nous commençons également à voir des choses intéressantes avec des accélérateurs tels que les GPU, voire même les processeurs neuromorphiques. IAAS est l'endroit où vous exécutez vos applications.\n\n## Plateforme en tant que service\n\nLa prochaine couche de la pile est PAAS. C'est là que vous obtenez un certain framework comme Kubernetes pour exécuter des choses. VMware fonctionnant au-dessus de l'IAAS est PAAS. Les pipelines CICD en tant que service proposent de nombreux outils qui s'insèrent dans cet espace. PAAS vous permet de construire et déployer de nouveaux services sur cette plateforme afin de pouvoir facilement déployer et gérer des systèmes volumineux qui sont construits au-dessus de l'IAAS.\n\n## Logiciel en tant que service\n\nLe prochain est SAAS. Il s'agit d'un logiciel spécifique qui est géré par le fournisseur de logiciels ou le fournisseur de services cloud, ou cela pourrait être une offre de SAAS proposée par un tiers pour quelqu'un d'autre. L'élément clé avec SAAS est qu'ils sont responsables de la sécurité de l'application. Ils assurent la disponibilité et gèrent tous les domaines de gestion tels que la fiabilité, la sécurité et l'intégrité. Beaucoup des programmes SAAS sont construits sur des plateformes PAAS.\n\n## X en tant que service\n\nXAAS peut être n'importe quel nouveau service tel que l'intelligence artificielle, la gestion des appareils ou la détection de sécurité.\n\nComprendre ces différents modèles de livraison est important car les modèles de sécurité partagée des fournisseurs de services cloud sont basés sur le modèle de livraison.\n\n## Cyber Domaines\n\nChacun des six piliers de la cybersécurité, tels qu'identifiés par Steve Warren, CTO chez Intel lors d'un podcast précédent, est important que vous soyez dans le cloud, sur site ou en périphérie. Les six piliers sont la détection des menaces, l'intelligence, l'analyse et l'orchestration ; la gestion des identités et des accès ; la sécurité des données et des applications ; la sécurité réseau ; la sécurité de la chaîne d'approvisionnement ; et la sécurité de l'hôte et du système. Ces six domaines s'intègrent dans le modèle de sécurité de responsabilité partagée que les fournisseurs de services cloud prônent.\n\n## Matrice de sécurité de responsabilité partagée.\n\nCette responsabilité partagée est illustrée dans la matrice. Les techniques de livraison du modèle de service se trouvent sur l'axe vertical : SAAS, PAAS, IAAS et sur site. Si vous hébergez vous-même, tout ce qui est à l'extrême droite est de votre responsabilité.\n\nDu côté d'IAAS, vous êtes entièrement responsable de la sécurité des données et des applications, ainsi que partiellement responsable de l'infrastructure, car vous devez toujours assurer le contrôle du réseau et du système d'exploitation. Le fournisseur de services cloud est responsable du réseau physique et de l'hébergement.\n\nSur la couche PAAS, vous êtes toujours responsable de la sécurité des données et partiellement responsable de la sécurité des applications, ainsi que de l'infrastructure d'identité et de répertoire. Il existe des outils disponibles pour vous aider dans ces domaines. Bien que vous soyez responsable des applications et de leurs plateformes, ils sont responsables des structures et des intergiciels qu'ils fournissent. Bien que la couche PAAS prenne en charge la plupart du système d'exploitation et vous fournisse certains outils de niveau supérieur, vous êtes toujours responsable de la configuration des contrôles réseau.\n\nHaut dans la pile sur SAAS, même si vous utilisez le stockage en tant que service, les données en tant que service, ou un CRM tel que Salesforce, vous restez responsable de vos données car vous devez toujours concevoir et chiffrer vos sauvegardes et gérer les comptes et les identités.\n\nUn point clé à travers les modèles est que vous êtes responsable de la sécurité de vos données ; il n'y a jamais de scénario où vous laissez toute votre sécurité aux fournisseurs de services cloud. Vous devez sauvegarder vos données et vous assurer que vous pouvez revenir en arrière en cas d'attaque de ransomware si vous utilisez le stockage d'objets, si vous gérez correctement vos accès et si vous utilisez des outils qui facilite cette tâche.\n\n## Différentes approches de sécurité.\n\nChacun des trois principaux fournisseurs de services cloud adopte une approche différente en matière de sécurité, notamment en ce qui concerne la configuration réseau.\n\n## AWS se traduit en français par \"Amazon Web Services\".\n\nAWS se concentre sur la prévention. Lorsque vous démarrez une machine virtuelle, par défaut, aucun port n'est ouvert, vous devez donc créer des groupes de sécurité. AWS est le plus restrictif, utilisant IAM pour la gestion des identités. AWS est idéal pour les équipes de taille moyenne, mais il ne convient pas aussi bien aux organisations très grandes.\n\n## Azure in French is \"Azur\".\n\nAzure met davantage l'accent sur la facilité d'utilisation ; la sécurité est moins restrictive. Ils utilisent le concept de réseaux virtuels pour la sécurité, de sorte que toutes les machines virtuelles sur le même réseau virtuel peuvent communiquer entre elles sur ce réseau. Cela s'oppose au principe de la confiance zéro, il faut donc décider de ce qui est le plus important pour vous. Azure utilise Active Directory, donc si vous disposez déjà d'un Active Directory mature et important, c'est une bonne option pour la gestion des identités.\n\n## Plateforme Google Cloud\n\nGoogle Cloud Platform se concentre également sur la facilité d'utilisation, mais ils misent sur les machines virtuelles (VMs) et la sécurité du réseau. Vous pouvez avoir des profils qui verrouillent tout sur une VM ou vous pouvez avoir un profil qui les ouvre un peu plus. Ils se situent au milieu du spectre en termes de restrictivité. Bien qu'ils ne soient pas aussi robustes qu'AWS ou Active Directory, GCP dispose d'une bonne gestion des identités.\n\nLes trois fournisseurs de services cloud proposent tous IAAS, PAAS, SAAS, Container as a Service, ainsi que divers XAAS. Vous devez évaluer le modèle de sécurité et comprendre les différences de chacun.\n\nÀ certains égards, comprendre le modèle de sécurité de responsabilité partagée est plus difficile que simplement gérer les choses sur site. Maintenant, plus de parties prenantes sont impliquées et la complexité augmente. La clé réside dans la compréhension des modèles et l'utilisation des outils disponibles pour vous aider à gérer la sécurité sur plusieurs clouds.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT78-fr","image":"./episodes/edt-78/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions, Intel, explique les modèles de sécurité de responsabilité partagée. La responsabilité de la sécurité peut devenir trouble dans le cloud ; la responsabilité dépend des modèles de prestation de services cloud et d'autres facteurs."},{"id":151,"type":"Episode","title":"Intégration des travailleurs à distance","tags":["covid","remoteworker","mobilementor","genz","people","process","compute","cloud"],"body":"\r\n\r\nAprès une carrière internationale chez Nokia, Denis a fondé Mobile Mentor il y a 17 ans. L'entreprise est née de son constat selon lequel les gens n'utilisaient pas la technologie de leurs smartphones. Bien que les smartphones offraient une technologie avancée, les gens ne les utilisaient que pour des fonctions basiques telles que les appels téléphoniques et les messages texte. Un moment décisif s'est produit lorsque Denis essayait de vendre une solution réseau à un PDG, et ce dernier a demandé pourquoi son entreprise devrait acheter davantage d'infrastructures alors que les clients n'utilisaient pas ce qu'ils avaient déjà acheté.\n\nDenis se demandait pourquoi la technologie était si en avance par rapport à ce que les consommateurs voulaient réellement faire avec leurs appareils. Il a quitté Nokia et a commencé Mobile Mentor pour résoudre ce problème. Il a embauché une armée de personnes compétentes en technologie qui s'assoiraient avec les clients professionnels et les aideraient à faire fonctionner leurs smartphones et à être productifs, apprenant ainsi beaucoup sur ce qui motive l'adoption de la technologie et les habitudes.\n\nAujourd'hui, les travailleurs à distance peuvent posséder deux ou trois appareils, en plus de leurs appareils personnels. Ils travaillent depuis chez eux, le bureau, et pendant leurs déplacements. La plupart des services que Mobile Mentor propose actuellement consistent à faire fonctionner ces appareils et, surtout, à s'assurer qu'ils sont sécurisés.\n\nDepuis que le nombre de travailleurs à distance a explosé avec la COVID-19 et que de nombreuses organisations n'ont pas mis en place ou accordé la priorité à une sécurité adéquate, il y a eu, et il continue d'y avoir, une avalanche de piratages et de rançongiciels. Les attaques de rançongiciels ont augmenté de 500% depuis le début de la pandémie, ciblant tragiquement les écoles, les hôpitaux et les organisations municipales.\n\nDe plus, avec la pénurie de puces, de nombreuses entreprises n'ont pas pu obtenir suffisamment de dispositifs pour les travailleurs à distance et ont dû compter sur BYOD (Apportez Votre Propre Appareil). L'utilisation de sources internet domestiques a également augmenté le niveau de risque.\n\nUne chose que les organisations peuvent faire pour réduire les risques est de se débarrasser de l'utilisation des mots de passe. Les mots de passe étaient une idée fantastique en 1961, mais en 2021, les données ont montré qu'ils sont la principale raison pour laquelle les organisations se faisaient pirater. La plupart des attaques commencent par un mot de passe compromis provenant d'une opération de phishing.\n\nLes travailleurs de la connaissance d'aujourd'hui ont un nombre ridicule de mots de passe. Pourtant, seulement 31% des gens utilisent un outil de gestion de mots de passe. 31% écrivent leurs mots de passe de travail dans un journal personnel, et 24% écrivent leurs mots de passe sur une application Notes de leur smartphone. De manière alarmante, selon une étude de la BBC l'année dernière, 15% de la population britannique utilisaient le nom de leur animal de compagnie comme mot de passe, et 6% utilisaient le mot \"password\".\n\nLa première étape pour se débarrasser des mots de passe consiste à adopter la biométrie. Maintenant, un iPhone ou une machine Windows Surface scanne votre visage et vous connecte au système d'exploitation ainsi qu'à toutes les applications de connexion unique et applications tierces où vous possédez cette identité fédérée. C'est un excellent début. L'avenir de la réduction de l'utilisation des mots de passe sera une combinaison de la biométrie et de l'authentification à deux facteurs partout. En attendant, tant qu'il reste des infrastructures d'identité héritées et des applications héritées où la biométrie ne fonctionnera pas, un outil de gestion des mots de passe a du sens.\n\nMobile Mentor a effectué une enquête dans l'industrie par génération et a constaté que la génération Z a le plus de mots de passe. Bon nombre de ces personnes dans la vingtaine ont rejoint la main-d'œuvre et se sont intégrées à distance pendant la pandémie. Dans de nombreux cas, ils n'ont jamais rencontré leur employeur et n'ont pas vécu les liens sociaux qui se créent dans un environnement de travail. Ils ont un regard unique sur la façon dont ils évaluent leur employeur.\n\nLes recherches de Mobile Mentor montrent que les gens préfèrent travailler à domicile, mais dans tous les secteurs, ils croient être plus productifs dans un environnement de bureau. Cela pose un dilemme intéressant et une dichotomie pour l'employeur qui tente de faire venir ces personnes au bureau. Les recherches montrent que 67 % de la génération Z estime que d'autres entreprises font un meilleur travail en matière de fourniture de technologies pour leurs employés. Donc, si un employeur les pousse à venir au bureau, ils pourraient choisir un autre emploi. Changer d'emploi de nos jours ne signifie pas seulement changer de trajet ou autre chose, c'est simplement utiliser un ordinateur portable différent. Cela fait partie de ce qui se passe avec la grande démission.\n\nAvec cette dynamique, l'expérience technologique compte. Les recherches montrent qu'il faut en moyenne trois jours pour configurer entièrement un ordinateur portable pour le travail, contre deux jours pour un travailleur de bureau. Les travailleurs à distance doivent en moyenne ouvrir trois tickets de service pour mettre leur appareil en marche, ce qui peut rendre leur expérience douloureuse. Ils n'aiment pas la stigmatisation de demander de l'aide.\n\nLa meilleure façon de résoudre ce problème est de simplifier le processus. La provision automatique sans intervention est idéale, ce qui implique de configurer la technologie de sorte qu'une entreprise puisse envoyer des appareils à un employé distant, et lorsque celui-ci se connecte avec ses identifiants de travail, les appareils se configurent automatiquement. Tout fonctionne en moins d'une heure et personne dans le service informatique n'a eu à configurer manuellement les appareils, à les emballer à nouveau et à les envoyer à l'employé. Il y a beaucoup de travail préalable pour que cela fonctionne, mais Mobile Mentor peut aider les clients avec ce processus.\n\nDenis croit que les directeurs des systèmes d'information apprendront des concepts importants en étudiant la génération Z et les travailleurs à distance. Les travailleurs de la génération Z ont des attitudes différentes, en particulier en matière de sécurité et de confidentialité. Ils valorisent et donnent la priorité à la vie privée personnelle plutôt qu'à la sécurité, presque quatre fois plus. Pour une génération qui a grandi avec les réseaux sociaux, cela est difficile à comprendre, mais les données sont claires. Ils sont très conscients de la politique de confidentialité de leur employeur, mais presque aveugles aux initiatives de sécurité de l'entreprise.\n\nLe conseil de Denis aux directeurs des systèmes d'information (DSI) pour ce problème est de présenter la vie privée et la sécurité comme les deux faces d'une même pièce. La génération Z peut être sensibilisée à la sécurité si elle est présentée comme la protection de leurs propres données, ainsi que celles de l'entreprise et par extension, celles de leurs clients.\n\nLa recherche de Mobile Mentor montre que l'informatique fantôme est stimulée et accélérée par les travailleurs à distance. Les travailleurs à distance qui vivent loin du siège social peuvent participer à une équipe informatique qu'ils n'ont jamais rencontrée et découvrent des applications, des mécanismes de stockage et des moyens de communication et de collaboration inconnus de leurs entreprises. Les frontières entre personnel et professionnel s'estompent également. Les gens utilisent des appareils personnels pour le travail et près de la moitié laissent leurs membres de famille jouer avec leurs appareils de travail. Le même nombre considère les politiques de sécurité de leur entreprise comme trop restrictives et un tiers affirment avoir trouvé un moyen de contourner ces politiques. Les deux tiers affirment être plus efficaces lorsqu'ils utilisent des applications grand public telles que Gmail et Dropbox.\n\nDenis conseille aux DSI de faire participer les travailleurs à distance dans les décisions futures en matière de produits, car ce sont eux qui testeront plus rapidement les outils de collaboration, les outils de stockage, les applications et les processus d'authentification, plus rapidement que toute personne basée au bureau.\n\nPour plus d'informations sur Mobile Mentor, rendez-vous sur mobile-mentor.com. Il existe un site web distinct, endpointecosystem.com, où ils partagent gratuitement toutes leurs recherches pour éduquer et informer les entreprises sur les problèmes liés aux travailleurs à distance afin d'aider à éviter la prochaine vague d'attaques cybernétiques et d'améliorer l'expérience technologique des employés embarqués.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Denis O&#39;Shea"],"link":"/episode-EDT79-fr","image":"./episodes/edt-79/fr/thumbnail.jpg","lang":"fr","summary":"Darren Pulsipher, architecte en chef des solutions chez Intel, a parlé avec Denis O'Shea, fondateur de Mobile Mentor, de son expérience, de ses recherches et de ses conseils sur l'intégration des travailleurs à distance, en particulier ceux de la génération Z."},{"id":152,"type":"Episode","title":"Sécuriser le télétravailleur Partie 2","tags":null,"body":"\r\n\r\n## Technologies pour la sécurisation du télétravailleur par le client\n\nLa sécurité du côté client commence et se termine par la capacité de vérifier le client grâce à un démarrage sécurisé. Intel propose cette technologie depuis des années dans les systèmes de nos clients avec le démarrage sécurisé avec la technologie d'exécution fiable (TXT), et plus récemment, BootGuard (BtG). Sur les plates-formes les plus récentes, nous avons Intel Hardware Shield, un ensemble de technologies qui sécurisent le système à son niveau le plus bas, au niveau du micrologiciel et du BIOS. C'est ainsi qu'un système de contrôle d'accès d'entreprise peut vérifier qu'un client démarre de manière sécurisée et dispose du micrologiciel correct ainsi que des contrôles de sécurité appropriés avant de lui accorder l'accès à l'entreprise.\n\nIntel a également fait beaucoup de travail au fil des années pour aider les organisations à mieux protéger leurs données. De plus, nous avons permis au client d'activer le chiffrement partout sans impact sur les performances. Nous avons mis en œuvre de nouvelles instructions dans presque chaque génération de nos produits, ajoutant de nouvelles capacités pour sécuriser le travailleur à distance.\n\nNotre autre priorité a été de protéger les applications et leurs données pendant leur utilisation. C'est là que les extensions Intel Software Guard (SGX) entrent en jeu. Cela donne aux organisations la possibilité de placer des parties clés des applications et des données importantes dans des enclaves de mémoire sécurisée et chiffrée. Avec le télétravail, cela signifie que vous pouvez déployer des applications dans des environnements non fiables tout en maintenant un niveau élevé de sécurité.\n\nDe plus, la nouvelle technologie qu'Intel a introduite dans la catégorie de détection des menaces permet aux organisations d'obtenir une visibilité approfondie sur les opérations de la plate-forme afin de surveiller les menaces ; aucun logiciel malveillant ne peut se cacher. Ces technologies révolutionnent la façon dont nous détectons les logiciels malveillants en utilisant l'apprentissage automatique et l'intelligence artificielle.\n\n## Technologies de centre de données pour sécuriser les travailleurs à distance\n\nMême si vous avez sécurisé les clients, vous devez également sécuriser le centre de données. Les deux côtés doivent être protégés car votre système n'est sécurisé que jusqu'au maillon le plus faible. Beaucoup des techniques pour sécuriser le télétravailleur sont similaires à la sécurisation de votre entreprise : le démarrage sécurisé, la sécurité de la virtualisation et les contrôles d'isolement. Encore une fois, des technologies comme Intel TXT et BootGuard vous permettent de démarrer en toute sécurité ces plates-formes et les actifs du centre de données et du cloud. Plus récemment, nous avons introduit une technologie, Intel Select Solutions for Hardened Security, qui intègre bon nombre des technologies de sécurité d'Intel dans une seule plate-forme activée par défaut.\n\nVous devez être en mesure de protéger vos données à grande échelle, ce qui signifie être capable d'utiliser tous vos outils de sécurité sans affecter négativement les performances. Les outils de cryptage accéléré par matériel d'Intel (SHA, AES-2X, VPMADD52) rendent cela possible. Les nouvelles instructions et technologies QuickAssist d'Intel sont spécifiquement adaptées aux besoins de cryptage à grande échelle des entreprises et des services cloud.\n\nEn fin de compte, comment surveillez-vous l'intelligence des menaces et effectuez-vous des audits à grande échelle ? L'architecture de la plateforme d'intelligence cybernétique d'Intel (FPGA, DCPMM, Optane SSD) utilise des technologies de calcul, de stockage et de mémoire haute performance pour agrandir la plateforme d'intelligence cybernétique, même avec le fardeau supplémentaire d'une sécurité externe accrue avec les télétravailleurs.\n\n## Solutions à court terme\n\nLa formation des employés est la première ligne de défense contre les menaces de sécurité liées au télétravail. Il est essentiel de fournir des conseils sur la sécurité à domicile, ainsi que de dispenser une formation en sécurité ou de renforcer la formation précédente dans des domaines tels que l'accès approprié aux données. Soyez proactif en ce qui concerne les correctifs en les diffusant et en exigeant des utilisateurs qu'ils mettent à jour leurs appareils. Si vous disposez d'un contrôle d'accès d'entreprise, de solutions ERM/DRM et de DLP, activez-les et étendez-les. Réévaluez vos politiques pour vous assurer qu'elles répondent à la nouvelle réalité des télétravailleurs. Pour les connexions Web, activez le protocole TLS et assurez-vous qu'il soit appliqué. L'authentification à deux facteurs doit être utilisée. La plupart des organisations pensent peut-être ne pas avoir l'infrastructure nécessaire pour mettre en œuvre cela, mais il existe plusieurs fournisseurs qui peuvent vous aider dans ce domaine sans nécessiter le déploiement d'une grande quantité de nouvelles infrastructures.\n\nDe nombreuses solutions sont simplement des mesures d'hygiène standard : Assurez-vous que vos agents de sécurité du point final sont activés et à jour. Gérez et appliquez des politiques de sécurité pour les différents types d'appareils utilisés par les utilisateurs. Activez le chiffrement du disque dur complet.\n\nLes organisations doivent comprendre que les télétravailleurs évoluent dans un environnement où la probabilité qu'un appareil soit utilisé par d'autres dans différentes circonstances est élevée. De bonnes mesures de sécurité, en particulier l'éducation des employés, peuvent contourner les problèmes et permettre aux employés de travailler sans impacts négatifs.\n\n## Solutions à long terme\n\nUn plan à long terme pour la sécurité dans un environnement avec des télétravailleurs est désormais nécessaire, que ce soit pour une transition permanente vers davantage de travailleurs à distance ou pour faire face à une autre pandémie ou situation similaire. Il y a plusieurs étapes que les organisations devraient prendre dès maintenant afin de soutenir cette réalité dans le futur.\n\nUne des meilleures pratiques est de mettre en place des politiques de confiance zéro. Cela réduit la dépendance envers la nécessité de faire confiance à tous les aspects des utilisateurs et des clients qui arrivent. En plus de cela, l'authentification à plusieurs facteurs avec les utilisateurs et les appareils devrait devenir une norme dans toute l'organisation. Pour ceux qui n'ont pas adopté la gestion des risques de l'entreprise (ERM) et le contrôle d'accès aux données basé sur les politiques, c'est maintenant le moment de le faire pour protéger les données à la fois hors site et sur site. Il est important de mettre en œuvre des solutions de sécurité complètes plutôt qu'au niveau de l'application ou du réseau. Cela inclut le démarrage sécurisé avec attestation, la virtualisation et la sécurité par conteneur, ainsi que la sécurité et la surveillance du micrologiciel. Il est important d'étendre l'audit, la veille des menaces et la surveillance aux environnements de télétravail, malgré les réticences des utilisateurs qui ne veulent pas être davantage surveillés sur leurs systèmes. Il convient également de considérer l'extension de la sécurité au-delà de l'appareil dans les lieux de télétravail chaque fois que cela est possible, par exemple avec les appareils et les réseaux gérés.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT8-fr","image":"./episodes/edt-8/fr/thumbnail.png","lang":"fr","summary":"Avec de nombreux employés travaillant désormais depuis leur domicile, comment vous assurez-vous qu'ils travaillent de manière sécurisée tout en leur offrant la flexibilité dont ils ont besoin pour accomplir leur travail ? Dans cet épisode, Darren et l'invité spécial Steve Orrin, CTO d'Intel Federal, discutent de la manière de tirer parti de la technologie Intel pour aider efficacement à sécuriser le télétravailleur."},{"id":153,"type":"Episode","title":"Réunion des architectes de solutions Cloud d'Intel","tags":["csa","cloudsolutionarchitect","solutionarchitect","people","compute","cloud"],"body":"\r\n\r\nIntel a investi massivement dans le recrutement de certains des meilleurs CSAs de l'industrie, avec environ 80 architectes dans le monde et 22 aux États-Unis. Ce sont de nouveaux rôles avec une grande importance.\n\nStephen Holt est un CSA pour l'Est et gère les autres CSA de cette région. Auparavant chez Intel, il était spécialisé dans les bases de données, mais il a occupé divers postes dans les processus commerciaux, l'analyse et la vente, l'analyse technique et la vente technique chez IBM et diverses startups. Il apporte toute cette expérience chez Intel pour aider l'équipe à travailler ensemble en vue de générer de la valeur pour les clients.\n\nKiran Agrahara est un CSA de la côte Est, relevant de Stephen. Son expérience se situe dans l'infrastructure des centres de données axée sur la virtualisation du stockage des données. Il a travaillé dans les secteurs de la câblodistribution et des services financiers. Au cours des cinq ou six dernières années, il a travaillé avec des start-ups se concentrant sur des domaines tels que la mémoire persistante et le stockage défini par logiciel. Lorsque les clients demandent à Kiran pourquoi ils devraient utiliser l'expertise d'Intel, il répond qu'Intel est partout et qu'il souhaite transmettre ce message aux utilisateurs finaux.\n\nAvant que Sarah Musick ne rejoigne Intel en tant que CSA, elle était partenaire et travaillait sur des logiciels liés à la migration et à l'optimisation de cloud, initialement précipités par son expérience dans l'analyse de données. Avant cela, elle travaillait pour une entreprise d'analyse textuelle basée sur l'apprentissage approfondi et a constaté que son travail dans les données a permis de boucler la boucle avec sa précédente expérience dans la vente technique. Elle met en œuvre des analyses pour faciliter la migration vers le cloud. Elle a rejoint Intel en raison de son intégrité et de son rôle dans le nouveau modèle de collaboration pour tous, y compris les fournisseurs de cloud.\n\nTodd Christ travaille chez Intel depuis 26 ans et possède une expérience de 30 ans dans le domaine de l'informatique et des produits. Il vient récemment du groupe des solutions d'entreprise de cloud d'Intel, qui fait partie du groupe des plates-formes de centres de données. Intel souhaite répondre aux besoins de ses clients, que leurs données soient sur site ou dans le cloud. Todd a conçu Anthos, ce qui fait que les modèles hybrides ou multi-cloud lui tiennent à cœur, et il a travaillé directement avec Microsoft et Google.\n\nUn changement majeur pour aider les clients à passer vers le cloud est la simplicité. Maintenant, les clients n'ont pas besoin de beaucoup réfléchir au matériel. Ils peuvent commencer à effectuer un travail significatif beaucoup plus rapidement, avec moins de frais généraux. Bien sûr, le matériel reste important, mais il est abstrait. Il est cependant important de ne pas perdre de vue les fondamentaux. C'est un peu comme avoir un système de climatisation que l'on ne pense jamais tant qu'il ne fonctionne pas correctement, et alors cela pose problème. Par exemple, il y a des situations où un client constate qu'une charge de travail particulière qu'il a transférée vers le cloud ne fonctionne pas bien, il doit donc revoir le matériel.\n\nIntel a été présent avec les fournisseurs de cloud depuis leur création, et ils se concentrent sur ces charges de travail. Intel met énormément d'efforts dans l'écosystème du cloud qui aide à optimiser au mieux l'exécution de ces charges de travail sur Intel.\n\nIntel a livré 2 milliards de cœurs aux fournisseurs de services cloud (CSP) et plus de 90 % de tout le calcul dans le cloud s'exécute sur Intel.\n\nLes clients veulent une évolutivité rapide et ils veulent les ressources de calcul le plus rapidement possible ; ils se soucient peu du matériel. La vérité, cependant, est que les charges de travail à faible latence fonctionnent beaucoup mieux sur le matériel Intel que sur tout autre concurrent. Le tout dernier processeur Xeon Scalable de troisième génération, Icelake, est incroyablement rapide. Lorsque les clients réalisent qu'ils peuvent économiser considérablement, il se vend tout seul. Une partie du travail d'un CSA consiste donc à informer et à éduquer.\n\nBien que les FCPs puissent sembler vendre des services comme une utilité qui fonctionne immédiatement avec une fiabilité de 100 %, vous ne pouvez pas simplement transférer ou délocaliser des charges de travail critiques dans le cloud.\n\nSi vos applications sont conçues dans un format cloud-native, alors vous n'avez pas à beaucoup réfléchir à leur déploiement sur le cloud. Cependant, si vous avez une application monolithique conçue pour fonctionner dans un centre de données, par exemple, vous ne pouvez pas simplement la déplacer sur le cloud car elle n'est pas optimisée pour fonctionner sur des processeurs spécifiques. En utilisant des outils d'optimisation ou de migration Intel, les clients peuvent prendre des décisions éclairées avant la migration.\n\nCertaines charges de travail peuvent ne pas être adaptées au cloud. C'est pourquoi, surtout vers la fin de 2021, il y a eu plus de discussions autour de la rapatriation. Le pendule revient un peu en arrière alors que les entreprises, en particulier, apprennent à trouver le bon équilibre. C'est là que les CSAs interviennent. Tout ne doit pas aller sur le cloud, et Intel peut aider à déterminer comment optimiser les choses. Certains clients constatent qu'après avoir déplacé des charges de travail vers le cloud en raison de réglementations, ils ne font pas d'économies et dépensent même considérablement plus que s'ils conservaient les choses dans leur propre centre de données. Ou peut-être y a-t-il des problèmes de sécurité car les données résident dans certains endroits et le cloud n'est pas dans le pays approprié.\n\nLes Intel CSAs sont agnostiques, ils ne sont donc intéressés que par ce qui est le mieux adapté aux besoins particuliers des clients. Ils aident à rendre les charges de travail plus mobiles afin que les services informatiques deviennent plus matures, de sorte qu'ils puissent rapatrier les charges de travail dans leurs propres centres de données ou les déplacer vers un autre fournisseur de services Cloud à l'avenir, ou toute autre option la plus rentable. Les Intel CSAs peuvent aider les clients de manière que les fournisseurs de services Cloud n'adressent pas actuellement.\n\nDe nombreux clients sont préoccupés par les données car leur récupération auprès d'un CSP est coûteuse. Intel possède une équipe compétente en mesure d'aider dans cette problématique. Ils sont bien informés non seulement sur les structures permettant de mettre en place des modèles hybrides, mais aussi sur la sécurité, les pare-feu et tous les points d'accès. Une fois que vos données se trouvent derrière un pare-feu, il existe de nombreuses couches de sécurité auxquelles vous devez accéder pour utiliser ces services. La première étape est donc de pouvoir envoyer vos données de manière sécurisée.\n\nMême dans un scénario multi-cloud, extraire les données est coûteux. Si vous vous déplacez simplement entre Azure et AWS, par exemple, les données migrent toujours, et c'est un processus lent. Si les clients ont des téraoctets voire des pétaoctets de données en local qu'ils veulent déplacer vers le cloud, il pourrait y avoir un service semblable à un cloud qui fonctionne en local où ils peuvent obtenir la facilité d'utilisation et les fonctionnalités d'un cloud. Si vous considérez le cloud comme une fonction plutôt qu'un emplacement, il y a plus de possibilités.\n\nIntel est là pour libérer les individus afin qu'ils puissent se consacrer aux tâches les plus significatives pour leur organisation, et l'analyse des données va jouer un rôle primordial dans cette démarche. En 2022, il existe toujours un énorme écart entre la quantité de données dont disposent les individus et la quantité de connaissances qu'ils en tirent réellement. Seulement environ 3% des données sont réellement utilisées pour produire des informations. Il y a donc un trésor énorme et les chipsets Intel présentent de bonnes performances dans les situations nécessitant un traitement de données robuste. Le traitement des données est ce qui nous attend prochainement.\n\nConcevoir intelligemment fait partie de l'avenir car il n'est pas nécessaire de réinventer la roue. D'autre part, il existe des solutions plus récentes qui pourraient convenir à une organisation. L'endroit où se trouve une organisation dans son parcours est également essentiel. Les entreprises plus anciennes et établies qui ont bien fonctionné pendant longtemps, par exemple, peuvent avoir beaucoup de dettes techniques qui pourraient éventuellement être résolues. Elles doivent examiner la technologie sous-jacente pour finalement l'amener à un niveau où elles ont de l'agilité.\n\nLes services CSA d'Intel sont gratuits car Intel souhaite aider les clients à exécuter leurs charges de travail de manière optimale et à profiter de la technologie Intel omniprésente dans le cloud. L'expertise et l'expérience des CSA d'Intel sont vastes et ils travaillent en équipe pour aider les clients avec n'importe quelle connaissance dont ils pourraient avoir besoin. Les clients doivent demander à leur responsable de compte Intel ou à leurs ventes internes de bénéficier de l'aide d'un CSA pour la migration et l'optimisation dans le cloud.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Stephen Holt","Kiran Agrahara","Sarah Musick","Todd Christ"],"link":"/episode-EDT80-fr","image":"./episodes/edt-80/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte en chef de solutions chez Intel, discute avec les principaux architectes de solutions cloud d'Intel, Stephen Holt, Kiran Agrahara, Sarah Musick et Todd Christ, sur la façon dont ils peuvent aider les organisations, gratuitement, à migrer vers le cloud et optimiser leurs charges de travail."},{"id":154,"type":"Episode","title":"Programmation hétérogène avec OneAPI","tags":["cpu","fpga","gpu","heterogeneousprogramming","npu","oneapi","compute","technology","process"],"body":"\r\n\r\nL'objectif de oneAPI est d'aider à abstraire l'énorme diversité qui arrive en matière de matériel, afin que les ingénieurs en logiciel puissent en tirer parti plus haut dans la pile et tirer le meilleur parti du matériel. James, un ingénieur logiciel qui travaille également en étroite collaboration avec le matériel, aime ce que oneAPI peut apporter compte tenu de la tendance à la multiplication des traitements hétérogènes.\n\nLe mot hétérogène signifie essentiellement qu'il existe différents dispositifs dans un système qui peuvent effectuer des calculs, mais ils n'exécutent pas le même ensemble d'instructions. Par exemple, le CPU a une façon d'exécuter les instructions et le GPU a une façon différente. Les FPGA, les autres ASIC et les appareils spécialisés ont chacun des façons différentes d'être programmés. L'objectif est de tirer parti de tous ces dispositifs.\n\nJohn Hennessy et David Patterson, leaders dans le domaine de l'architecture informatique, l'ont qualifié de nouvel âge d'or de l'architecture informatique. Pour les architectes informatiques, c'est une période incroyable pour concevoir toutes sortes de dispositifs spécialisés afin de réaliser des performances accrues dans des domaines spécifiques, mais pour les programmeurs, cela peut être difficile, voire effrayant en raison de la complexité. C'est ici que l'initiative oneAPI et les produits oneAPI interviennent.\n\nLe nom, oneAPI, est à la fois une initiative avec une spécification et une mise en œuvre. L'initiative est un concept simple : les développeurs de logiciels devraient avoir la liberté d'utiliser n'importe quel appareil qu'ils souhaitent avec des performances optimales, et leur codage devrait préserver sa valeur ; ils ne devraient pas avoir à réécrire pour chaque nouvel appareil. Ces qualités peuvent être appliquées aux compilateurs, bibliothèques, débogueurs et tout type d'outil.\n\nUne grande quantité de logiciels pour mettre en œuvre certaines spécifications ont été rendus open source. Une grande partie de ceux-ci provient d'Intel, mais Intel les met également à disposition sous forme de kits d'outils téléchargeables, déjà préconstruits et prêts à l'emploi pour prendre en charge le matériel Intel. D'autres fournisseurs qui participent à la création de parties de la oneAPI ont leurs propres implémentations. Ainsi, tous se réunissent sur les spécifications, mais chacun crée son propre support pour son matériel dans un format multi-fournisseur ouvert.\n\nL'objectif principal est d'écrire du code une fois et qu'il fonctionne bien partout. Cependant, c'est un problème complexe qui nécessitera quelques réglages de performance. Par exemple, si un algorithme fonctionne bien sur un GPU, il fonctionnera également sur presque n'importe quel autre GPU. Si vous passez l'algorithme à une puce FPGA, vous pouvez maximiser la réutilisation de votre programme, mais vous avez la possibilité de recoder certaines parties de l'algorithme pour qu'il fonctionne également bien. C'est une approche complexe qui vous donne la possibilité de tirer parti de n'importe quel matériel avec des degrés d'écriture isolée, idéalement très distincts.\n\nVous pouvez construire un code unique pour différents appareils. Il existe même des approches d'exécution plus sophistiquées qui vous permettent de détecter ce qui est présent et d'exécuter différents morceaux de code, de sorte que vous pouvez réellement avoir un code source commun qui décide dynamiquement. Vous pouvez appliquer un programme de déploiement et il peut déterminer ce qui se trouve sur votre machine et l'utiliser de manière dynamique plutôt que de demander à l'utilisateur d'exécuter une binaire différente en fonction de la machine sur laquelle vous vous trouvez.\n\nC'est excitant car, pendant longtemps, les ingénieurs utilisaient une pile de logiciels qui était la meilleure pour leur machine. De nos jours, ils veulent pouvoir compiler un programme qui peut utiliser des dispositifs de plusieurs fabricants. Le programme doit s'adapter à cela. Pour que cela fonctionne, le compilateur doit être capable de générer du code pour ces différents dispositifs de manière fiable. C'est là que l'ouverture entre en jeu.\n\nCertains soutiendront qu'un système fermé offrira de meilleures performances car les bibliothèques et le langage sont adaptés spécifiquement aux capacités matérielles. Mais la question est de savoir comment obtenir le meilleur des deux mondes ? Par exemple, s'il existe une implémentation spécifique au fournisseur pour une bibliothèque mathématique, il peut y avoir un programme commun. Une grande partie de oneAPI ne consiste pas à réinventer le monde entier, mais à l'organiser de manière à tirer parti du meilleur sur chaque plateforme possible.\n\nOneAPI a les capacités de déplacer la mémoire et de déplacer les données. Différents modèles de programmation reposent sur OneAPI et c'est à vous de décider dans quelle mesure vous souhaitez vous impliquer dans la gestion de la mémoire. Déplacer les données est coûteux et consomme de l'énergie, donc vous ne pouvez pas y échapper, mais OneAPI vous donne les outils pour gérer cela en interrogeant le système et en permettant à votre programme d'effectuer les bonnes décisions dynamiques qui vous donneront les meilleures performances.\n\nIntel a des décennies d'expérience dans la construction d'outils pour aider à l'optimisation et à la migration, et dispose de compilateurs hautement optimisants. V2 a contribué à faire évoluer l'industrie en rendant les compteurs matériels sur les processeurs la norme. Il existe toute une gamme d'autres outils d'analyse pour fournir des commentaires sur la structure de votre programme, trouver des blocages et des applications parallèles, et déterminer où vous devez ajouter des verrous. Intel met tous ces outils à disposition de manière \"oneAPI\" pour plus de polyvalence, au-delà de la simple utilisation d'un CPU.\n\nLes premiers groupes à adopter oneAPI sont ceux travaillant dans le domaine de l'informatique haute performance (HPC). Avec l'explosion de nouvelles idées architecturales en informatique, il y aura une quantité encore plus grande de diversité et d'innovation dans ce domaine. Les codes complexes capables de résoudre les plus grands problèmes d'ingénierie au monde, ou par exemple, de résoudre des problèmes pharmaceutiques et d'évaluer de nouveaux médicaments, nécessitent les matériels les plus récents et performants. Ainsi, ce concept de portabilité des performances est en train de se diffuser dans les laboratoires nationaux, les universités et les centres de recherche.\n\nL'HPC d'aujourd'hui sera les serveurs de département de demain, donc la capacité à utiliser différents matériels nécessite aux ingénieurs logiciels de planifier et de prêter attention à la portabilité du code, car le code ne meurt pas rapidement ; il dure pendant des décennies. Dans quelques années, les systèmes hétérogènes toucheront tout le monde, c'est donc le moment de s'informer à ce sujet.\n\nLes ingénieurs en logiciel n'ont pas besoin de programmer en parallèle pour tirer parti de oneAPI. Il s'agit d'utiliser des éléments qui sont ouverts et compatibles avec plusieurs fournisseurs et plusieurs architectures. Même les ingénieurs qui sont au sommet de la pile doivent comprendre ce qui se trouve dans la pile et ce dont elle est capable en termes de portabilité et de portabilité des performances.\n\nLa communauté de l'IoT est un autre groupe qui a été en avance dans la programmation sur de multiples dispositifs de calcul hétérogènes et l'utilisation de différentes méthodes pour les gérer. Leurs capacités de calcul continuent de croître à mesure que la technologie avance. Ainsi, oneAPI s'applique ici et peut aider à formaliser ou standardiser les choses qui ont été d'abord innovées dans le monde de l'embarqué.\n\nL'idée d'une seule API est passée d'une idée folle dont seules quelques personnes parlaient à une idée de plus en plus reconnue, faisant sens et résolvant les problèmes dans leur organisation.\n\nL'endroit le plus simple pour en apprendre davantage sur l'initiative se trouve sur le site oneapi.io. Pour en savoir plus sur la mise en œuvre, cliquez sur l'onglet \"Implémentation\" pour obtenir un lien vers les mises en œuvre Intel. Vous pouvez y télécharger différents kits d'outils. Recherchez le \"Intel dev cloud\" pour essayer gratuitement les outils dans le cloud, y compris sur différents matériels. Le site oneapi.io propose également une variété de didacticiels et de ressources.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","James Reinders"],"link":"/episode-EDT81-fr","image":"./episodes/edt-81/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte en chef des solutions chez Intel, discute des capacités et de l'avenir de OneAPI, un modèle de programmation unifié intersectoriel, ouvert et basé sur des normes, qui offre une expérience de développement commune sur différentes architectures d'accélérateurs, avec James Reinders, évangéliste en chef de OneAPI chez Intel."},{"id":155,"type":"Episode","title":"Passer d'une infrastructure à des charges de travail.","tags":["cloudmigration","cloud","compute","process","infrastructure","migration","workload"],"body":"\r\n\r\nLes conversations de Sarah avec les clients ont évolué pour parler des charges de travail plutôt que de l'infrastructure. Une raison évidente à cela est le passage du centre de données à la mise en commodité des ressources et à la consommation de tout en tant que service. Une autre raison est l'influence des équipes d'applications dans les décisions architecturales. Les applications sont devenues l'épine dorsale des organisations d'une manière qui n'était même pas le cas il y a cinq ans. La dernière raison est financière : les budgets sont maintenant catégorisés par charges de travail plutôt que par dépenses informatiques généralisées.\n\nUn défi dans ce passage de l'infrastructure à la charge de travail consiste à trouver un équilibre entre les processus et les charges de travail centralisés et décentralisés. Certaines choses, comme la gouvernance de la sécurité, sont gérées par un centre centralisé, tandis que d'autres sont traitées au sein d'équipes locales plus petites. Une grande partie de la prise de décision est déléguée aux équipes qui créent de nouvelles applications et services au sein de l'informatique, ainsi qu'à celles destinées aux clients.\n\nCOVID a été un accélérateur pour promouvoir le changement et inciter les organisations à passer au cloud. Certaines organisations informatiques laissent leurs applications migrer vers le cloud sans contraintes et préfèrent nettoyer le côté technique après plutôt que de ralentir l'innovation en cours. Cet événement « Cygne Noir » est sans précédent et nous constatons encore les retombées de ce changement de paradigme rapide.\n\nLe rôle du CIO est de retour, mais seulement s'ils commencent à réfléchir à nouveau à l'information et aux charges de travail au lieu de gérer un centre de données. Bien sûr, cela dépend de l'organisation. Les organisations natives du cloud, ou celles qui aspirent à le devenir, refondent leurs applications en cours de route car elles veulent être super agiles. Plus elles le font, plus cela se dirige vers l'infrastructure et le service de l'application plutôt que d'accepter les limites qui existaient auparavant dans le centre de données et de travailler dans ces paramètres. Cela a auparavant déclenché une innovation de la part des équipes d'applications, car lorsque l'on travaille avec un ensemble de données préétablies, parfois la nécessité est mère d'invention, contrairement aux possibilités infinies dans le cloud.\n\nC'est une extrémité du spectre. À l'autre extrémité se trouvent les entreprises patrimoniales américaines, les institutions. En général, elles conservent toujours des informations sur le mainframe. C'est une situation de \"si ce n'est pas cassé, ne répare pas\", surtout avec les applications statiques. Le cloud permet aux directeurs informatiques de penser au-delà de la manière traditionnelle de gérer ces applications. Si les directeurs informatiques peuvent adopter ces nouvelles technologies, ils peuvent désormais voir une voie à suivre.\n\nLe traitement qui se déroule sous des applications distinctes est plus pertinent que jamais. Intel joue un rôle majeur dans la résolution des problèmes de performance ou de coûts liés aux offres cloud, en partie parce qu'ils ont contribué au développement de ces technologies et aussi parce qu'ils font partie des plus grandes entreprises de logiciels. Ils possèdent une vaste quantité de connaissances internes. En d'autres termes, toutes les instances dans le cloud ne sont pas équivalentes, donc Intel a optimisé les charges de travail en interne pour tirer le meilleur parti des instances cloud qu'ils utilisent.\n\nPar exemple, de nombreuses organisations transfèrent des éléments vers des clusters Kubernetes et Intel réalise une tonne d'optimisations à ce sujet. Ils peuvent aller au-delà des graphiques standard Helm avec des extensions qui prendraient en compte la santé du nœud en dessous et pas seulement la disponibilité brute. Intel peut faire beaucoup de choses pour aider les clients à améliorer considérablement les performances et les coûts, pas seulement de 2 ou 3 pour cent, mais de 30 ou 40 pour cent ?\n\nToutes les charges de travail ne peuvent pas être hébergées dans le cloud. La structure d'une organisation a un impact sur l'endroit où la charge de travail doit se trouver. La clé est d'être intelligent en matière de cloud.\n\nUne stratégie multi-cloud réussie consiste à avoir un nuage primaire et un nuage secondaire. Lorsque beaucoup parlent de multi-cloud, la motivation derrière cela est la peur de l'enfermement chez un seul fournisseur. L'endroit où réside la majeure partie de vos données influence la stratégie multi-cloud, tout comme l'endroit où les charges de travail s'intègrent le mieux.\n\nPour élaborer une stratégie, les architectes de solutions cloud d'Intel se livreront à une phase de découverte afin de comprendre ce que l'organisation souhaite réaliser et quels sont les problèmes existants. Intel peut souvent remédier à bon nombre de ces problèmes grâce aux outils dont ils disposent à portée de main. Les architectes de solutions cloud contextualiseront également les propositions afin d'accélérer et d'optimiser le processus. Une partie de leur travail consiste à être un formateur, de sorte que tout le monde dispose des informations nécessaires pour avancer.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Musick"],"link":"/episode-EDT82-fr","image":"./episodes/edt-82/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions chez Intel, poursuit sa conversation avec Sarah Musick, architecte des solutions cloud chez Intel, sur le passage de l'infrastructure aux charges de travail. Veuillez le placer dans la playlist Adhérer à la transformation numérique."},{"id":156,"type":"Episode","title":"IDC Directions 2022: Les directives IDC 2022","tags":["data","datacentric","people","process","idc2022"],"body":"\r\n\r\nDavid est basé dans le Maryland, près du centre du gouvernement. Il dispose de plus de 25 ans d'expérience dans le développement technique et commercial dans des infrastructures critiques telles que les soins de santé, l'énergie, les finances et l'industrie. Une grande partie de sa spécialisation se situe à la limites : les choses qui se trouvent en périphérie hyperconnectée et l'espace intégré, mais aussi les problèmes complexes de charge de travail et l'optimisation d'une architecture qui se hisse jusqu'à l'entreprise et le cloud.\n\nDarren, David et certains de leurs pairs ont participé à la conférence IDC pour vérifier si leurs propositions et prédictions étaient conformes aux tendances observées par l'IDC, et ils sont repartis en se sentant validés. Ils sont également allés voir s'ils identifiaient des lacunes qu'ils devaient comprendre et comment ils pouvaient tirer parti des idées et services de l'IDC.\n\nLa plus grande tendance à long terme, sur laquelle toute la conférence était axée, est le numérique en premier, et comment cela impacte virtuellement tous les marchés. Meredith Whalen, Directrice de la recherche chez IDC, a explicité ce que signifie vraiment \"le numérique en premier\". Fondamentalement, toute organisation, entreprise ou gouvernement, doit se demander comment transformer la numérisation en un produit concret. D'ici 2024, il y aura environ 10 billions de dollars à dépenser dans tous les marchés rien que dans les produits numériques. Ce changement de comportement des consommateurs force également le gouvernement à considérer la numérisation, non seulement dans ses services, mais aussi dans la manière dont il aborde le commerce mondial, les économies et les devises.\n\nC'est un défi particulier pour les gouvernements à tous les niveaux car ils se débattent avec des réseaux informatiques sur de nombreux systèmes différents, et dans de nombreux cas, ils sont cloisonnés. Des passerelles efficaces n'existent pas encore pour les relier entre eux. Les systèmes gouvernementaux doivent examiner comment ils peuvent simplifier et rationaliser les choses.\n\nLa prochaine grande tendance est le basculement vers l'informatique en nuage. Chaque fournisseur de services cloud (CSP) a son infrastructure propre, et les constructions d'infrastructures ne sont pas nécessairement transportables, donc des ponts sont nécessaires. Actuellement, les CSP ne sont pas interchangeables. Il est vrai qu'ils veulent seulement que les clients utilisent leur infrastructure, mais avec le développement des environnements de développement de logiciels et des applications logicielles sur leurs places de marché, il est difficile de les déplacer. Les PDG et les DSI se demandent non seulement s'ils utilisent les licences logicielles et l'accès qu'ils ont achetés, mais se concentrent davantage sur les résultats obtenus grâce à leur utilisation.\n\nLe modèle CSP actuel a tendance à être inefficace en ce qui concerne les résultats commerciaux. Les clients ont besoin d'écosystèmes logiciels fonctionnant ensemble pour atteindre ces résultats. Ils recherchent plusieurs écosystèmes qui travaillent ensemble, avec une fluidité entre plusieurs clouds et clouds hybrides. Ainsi, la tendance est que les CSP deviennent interchangeables et les constructeurs de ponts vont jouer un rôle important ici. C'est le retournement de situation.\n\nLes données d'aujourd'hui sont fongibles. Vous pouvez les déplacer, bien sûr, moyennant des coûts associés, mais vous pouvez les copier, les modifier, etc. Cependant, la tendance va vers des données non fongibles. Lorsque ces données, ou actifs numériques, ont une propriété, cela crée d'énormes impacts pour l'avenir sur la façon dont les données sont gérées dans des domaines tels que la sécurité, la confiance et les modèles commerciaux et écosystèmes qui lui sont associés. Il y aura des entités de données que vous devrez accepter et attester de la validité de leur propriétaire, de leur provenance et de toutes les politiques qui les entourent.\n\nDans cette évolution, il y aura des avantages et des inconvénients. D'un côté, les consommateurs ont plus de pouvoir sur leur propre vie privée s'ils ont des droits sur leurs données. Cela s'applique également aux entreprises et aux organisations. D'un autre côté, avec des entités et des actifs de données qui ne sont pas interchangeables, les écosystèmes logiciels et les scientifiques des données doivent les gérer de manière beaucoup plus concise et structurée. À long terme, tout le monde devra gérer cela. Les gouvernements ne font pas face à ce problème pour le moment, mais cela va devenir une partie de plus en plus importante de la manière dont ils gèrent et mélangent les échanges, non seulement pour les biens physiques, mais aussi pour les biens numériques.\n\nÀ quel point le numérique est-il réellement omniprésent ? Un exemple est l'utilisation des jetons numériques en tant que monnaie réelle, notamment dans la communauté des jeux. Les jetons numériques font partie de ce monde par rapport à la manière dont les applications et les écosystèmes utilisent des données non fongibles. Un autre exemple se trouve dans le domaine de la santé. Aujourd'hui, on se rend chez le médecin ou on le consulte virtuellement, et il peut observer ce qui se passe et vous pouvez décrire vos symptômes. Tout cela évolue grâce à des dispositifs portables en temps réel qui peuvent surveiller les taux de glucose, la fréquence cardiaque, les variations de poids, etc. Ces données appartiennent à l'individu, elles doivent donc être sécurisées et authentifiées, mais elles peuvent également être utilisées pour des services qui ne concernent pas les symptômes, mais plutôt des algorithmes personnalisés et des services liés à ce qui se passe réellement dans leur corps pour obtenir les meilleurs diagnostics.\n\nPoints de données à prendre en compte : Meredith Whalen a souligné qu'en 2021, les dépenses de services dépassaient pour la première fois les dépenses de licences régulières dans le domaine des technologies de l'information. Ainsi, étant donné que les services dominent actuellement, la tendance est qu'ils passeront à un modèle basé sur les résultats. En 2023, IDC prévoit que les dépenses numériques seront supérieures aux dépenses non numériques dans les entreprises. Chaque industrie variera, mais à un niveau macro, 2023 marque un point de basculement. En 2024, le bassin de talents suivra. IDC prévoit que la plupart des entreprises dépenseront davantage pour les talents techniques que pour les talents non numériques.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","David Richard"],"link":"/episode-EDT83-fr","image":"./episodes/edt-83/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren, architecte principal des solutions du secteur public chez Intel, et David Richard, architecte principal des solutions du département de la défense chez Intel, reviennent sur les tendances et les idées qu'ils ont tirées de la conférence IDC Directions 2022."},{"id":157,"type":"Episode","title":"Sécuriser votre château avec la confiance zéro.","tags":["cybersecurity","zerotrust"],"body":"\r\n\r\nLe vieux modèle de sécurité pourrait être comparé à un château, avec des gardes le long du chemin et un fossé entourant le château. Toutes les joyaux de la couronne, dans ce cas, les données, seraient centralisés et gérés à l'intérieur du château. L'émergence du zéro confiance a créé un nouveau cadre.\n\nLa plus grande menace pour les données est l'utilisateur final, donc la stratégie numéro un est un cadre qui commence en périphérie avec des piliers d'excellence et des protections internes. Cette nouvelle façon de penser permet aux organisations d'impliquer leur mission et leurs partenaires commerciaux de manière concrète dans la conversation.\n\nLe style de pensée ancien était davantage basé sur une approche à coque dure, avec des protections et des contrôles dans les zones vulnérables. Une partie clé de l'architecture de confiance zéro est une approche basée sur les risques, qui est plus dynamique et repose sur deux éléments : ce qui a fonctionné et ce qui n'a pas fonctionné dans le passé. Donc, si les attaquants sont entrés par la porte gauche la dernière fois, vous renforcerez bien entendu la défense à cet endroit, mais vous en tirerez également des enseignements pour renforcer d'autres endroits en fonction de la nouvelle compréhension de comment cela s'est produit. Une approche basée sur les risques ne se contente pas de résoudre le problème de la dernière attaque, mais anticipe et applique les bonnes mesures de protection pour les menaces actuelles et futures dans l'entreprise.\n\nUne partie de l'approche basée sur les risques consiste à comprendre l'écosystème. Les clients, les partenaires et les utilisateurs font tous partie du calcul de sécurité. La vieille approche à coquille dure ne fonctionne pas. Tout comme un château a des personnes et des provisions qui entrent et sortent, et que les richesses peuvent être situées à divers endroits du royaume, la confiance zéro va encore plus loin en prenant en compte toutes les entrées et sorties de données ou les personnes qui y accèdent.\n\nTraditionnellement, quelqu'un pouvait accéder au château de l'entreprise avec une seule connexion. Il existe plusieurs principes de la confiance zéro, mais les deux bases sont le refus par défaut et la surveillance et l'autorisation continues. La confiance n'est pas automatiquement acquise, et elle n'est pas permanente. Par exemple, si un invité entre dans le château, il est validé à la réception et on lui demande sa raison de visite. Il peut se voir accorder l'accès pour rendre visite à une personne dans une chambre donnée pendant un certain laps de temps, et il sera accompagné lors de ses déplacements. On surveillera également ce qu'il apporte et emporte lors de sa visite.\n\nZero-trust s'applique à l'accès dans tous les endroits : centres de données, clouds, périphériques, environnements professionnels, etc.... Il est axé sur les données et l'accès, associé à une approche fondée sur les risques. Il doit y avoir une stratégie plus développée. L'approche basée sur la confiance zéro résultante ne rejette pas ce qui a bien fonctionné auparavant, mais combine les bons processus, principes et technologies et ajoute un élément temporel.\n\nCet nouvel élément n'est pas aussi difficile que cela a souvent été décrit, mais c'est un problème à la fois de processus et culturel qui peut être délicat.\n\nDe nombreux développeurs ont peur qu'une architecture de confiance zéro les ralentisse, mais les experts en sécurité et les développeurs doivent travailler en partenariat pour surmonter cette perception. Un exemple concret est Log4j. Il y a six mois, les développeurs pouvaient le télécharger sans problème, mais maintenant l'environnement de menace a changé. Sans une approche basée sur les risques, un développeur pourrait télécharger Log4j jusqu'à ce que quelqu'un du service de sécurité intervienne et le bloque. Avec une approche basée sur les risques, en plus des approches d'accès, Log4j ne serait pas disponible et une alternative serait proposée. Un autre exemple serait lorsque Log4j est déjà intégré dans un produit, l'évaluation dynamique de confiance pourrait mettre en place des contrôles supplémentaires plutôt que de le bloquer complètement. Il s'agit des deux côtés du calcul en jeu.\n\nCe partenariat est semblable à la formation croisée et au partage d'informations qui sont nécessaires pour intégrer la sécurité dans le processus de développement. Pendant la construction et les tests d'un produit, la sécurité surveille également et évalue les risques à la fois pour les entités avec lesquelles vous travaillez et pour les vulnérabilités du produit en temps réel. Adopter une approche basée sur les risques dans ce processus permet de tirer parti de l'intelligence qui aborde directement bon nombre des difficultés que nous percevons.\n\nQuelle est la première étape pour les CISOs, les CIOs ou les CTOs pour initier la confiance zéro ? Cameron suggère d'abandonner le \"geekspeak\" et de communiquer en anglais courant. Lancer l'initiative peut être un défi car les dirigeants travaillent généralement avec un résultat ou un objectif en tête. La confiance zéro n'a pas d'objectif défini vers lequel travailler, à part la création d'un environnement avec une assurance accrue pour les utilisateurs. Il existe bien sûr des indicateurs de performance clés et d'autres mesures pour montrer une sécurité renforcée, mais c'est un voyage, pas une destination. Il insiste également sur le financement continu ; ne pas intégrer le budget cybersécurité dans le budget informatique. Il doit être séparé et distinct.\n\nLe meilleur endroit pour trouver des informations de haut niveau avec des conseils pratiques est la publication SP 800-207 du NIST. Elle expose également les cinq piliers de la confiance, qui constituent de bons points de départ.\n\nUn aspect fondamental et premier consiste à disposer d'un bon inventaire des actifs à protéger, tels que les sources de données, les bases de données, les processus métier et les applications de transaction. Fondamentalement, il est important de définir le périmètre de votre château. Il est primordial de ne pas seulement penser à ce que vous possédez, mais également à ce dont vous dépendez, comme l'environnement SAAS, l'infrastructure cloud et les outils tiers.\n\nL'image plus grande consiste à connaître votre chaîne de valeur. Ce n'est pas seulement ce qui se trouve dans votre château, mais aussi comment vous gagnez de l'argent, comment cet argent est distribué, à qui vous payez et vos fournisseurs. Chacun est une pièce essentielle de la chaîne.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin","Cameron Chehreh"],"link":"/episode-EDT84-fr","image":"./episodes/edt-84/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren discute de la sécurité de confiance zéro avec Steve Orrin d'Intel, CTO du secteur public, et Cameron Chehreh, vice-président-directeur général du secteur public."},{"id":158,"type":"Episode","title":"Intelligence artificielle et sécurité","tags":["aiml","cybersecurity","devops","compute","technology","process"],"body":"\r\n\r\n## Échecs de l'IA\n\nRécemment, Darren a demandé à une classe d'étudiants du lycée et de l'université étudiant l'IA de trouver des échecs en matière d'IA. Ils ont trouvé des exemples tels que Microsoft Tay, un chatbot qui apprenait la conversation décontractée à partir de Twitter et qui, en moins de 24 heures, proférait des commentaires racistes et misogynes basés sur la manipulation de fils Twitter. Un autre exemple était un système de suivi de balle du club de football d'Inverness qui devait suivre un ballon de soccer mais qui a suivi la tête chauve d'un arbitre à la place. Plus grave encore, il y a eu une mort lorsqu'un véhicule autonome d'Uber n'a pas reconnu un piéton en dehors d'un passage pour piétons et n'a pas réussi à s'arrêter.\n\n## Déploiement des solutions\n\nTous ces échecs d'IA étaient liés aux données. Lors du déploiement de solutions d'IA, vous devez vous poser des questions cruciales : Où est-ce que je fais mes inférences ? Est-ce que le point d'extrémité est sécurisé ? Si vous effectuez toutes vos inférences au point d'extrémité, par exemple la détection d'objets avec une caméra, vous devez vous assurer que c'est sécurisé ; personne ne devrait pouvoir manipuler les données, la caméra ou le modèle.\n\nUne autre question est la suivante : qu'est-ce qui va être déployé ? Est-ce que je déploie un réseau neuronal ou un algorithme sur le terrain, ou est-ce que je me contente de transmettre des données de retour du terrain vers un centre de données pour y effectuer l'inférence ? De plus, lorsque l'IA est opérationnalisée, il faut se demander à quelle fréquence les modèles ou les algorithmes seront mis à jour.\n\n## Pipeline d'IA\n\nLe processus de développement, d'entraînement, de test, de déploiement et d'inférence de l'IA nécessite trois choses : l'application, le modèle et les données. Ces trois éléments doivent migrer ensemble à travers le pipeline et être protégés simultanément. Il est nécessaire de s'assurer que les données de production ne sont pas manipulées même lors de la production.\n\n## Les menaces de sécurité de l'IA\n\nLes menaces pour l'IA sont importantes, qu'il s'agisse d'espionnage, de sabotage ou de fraude, car la surface d'attaque est vaste.\n\n## Les attaques\n\nD'abord, viennent les modèles. Un modèle peut être manipulé, par exemple en plaçant des autocollants noirs et blancs sur les panneaux d'arrêt pour qu'ils ne soient pas reconnus en tant que tels, ou en perturbant la détection de correspondance de motifs pour que les attaques restent indétectées. Les modèles doivent être protégés non seulement pendant l'entraînement, mais aussi pendant les tests, le déploiement et l'inférence. Le chiffrement, le contrôle d'accès, ainsi que le contrôle des modèles et des versions sont essentiels, tout comme dans le développement d'une application.\n\nLes données de formation et les données de production de la source doivent également être protégées contre toute manipulation.\n\n## Les différents types d'attaques\n\nUn document du Centre Belfer classe les menaces sur un axe de format et un axe de visibilité. L'axe de format va de physique à numérique. L'axe de visibilité va de perceptible à imperceptible.\n\n## Attaques physiques\n\nLes attaques physiques peuvent modifier des éléments physiques tels que l'autocollant sur le panneau d'arrêt. Ces attaques étaient évidentes dès les débuts de la conduite autonome et de la reconnaissance faciale. Nous devons améliorer la formation des algorithmes d'IA pour ces attaques en utilisant l'apprentissage renforcé et les techniques d'apprentissage de cas négatifs.\n\n## Attaques numériques\n\nLes attaques numériques sont plus difficiles à détecter car elles ne sont pas visibles. Une attaque peut être du bruit blanc injecté dans le flux de données pour perturber l'algorithme. Ces attaques sont difficiles à combattre à moins que l'inférence ne soit effectuée en périphérie ou que la détection de schémas ne soit déployée. Pour cette raison, il est essentiel de savoir d'où proviennent les données sources, aussi bien lors de l'entraînement, des tests que de la production.\n\n## Identifier les sources de données\n\nLes sources de données doivent être vérifiées et testées à partir de sources de données publiques. Les données open-source ne sont pas bien protégées. Envisagez de créer vos propres sources de données afin d'avoir un plus grand contrôle. Si vous utilisez une source de données partagée, utilisez un système de contrôle de version tel que GitHub ou GitLab pour vérifier la cohérence. Les données de test nécessitent également un contrôle de version, un contrôle d'accès et d'autres mesures de sécurité, tout comme vous le faites généralement dans un pipeline DevOps.\n\nLa dernière partie, la plus difficile, consiste à protéger les données de production. Rapprocher l'inférence des données est un bon début. Beaucoup de spéculation peut être faite dès le bord avec le traitement neuromorphique et même les jeux d'instructions dans les processeurs Intel pour réduire le risque de manipulation des données pendant le transport. Au lieu de cela, vous pouvez chiffrer les données et les renvoyer au centre de données.\n\nProtéger et gérer les données / Sécuriser le pipeline de l'IA.\n\nUne fois que vous avez identifié toutes vos sources de données, il existe trois aspects essentiels pour la protection : le contrôle, la sécurité et le chiffrement.\n\nLe premier est le contrôle. Vous devriez avoir un contrôle de version, des bibliothèques protégées, ainsi qu'une sauvegarde et une restauration en cas de fichiers de données corrompus. Ce sont des pratiques de sécurité standard que l'IA devrait appliquer, puisqu'elles sont utilisées dans le développement d'applications.\n\nLa sécurité devrait inclure une autorisation d'accès, même certains concepts de confiance zéro tels que donner accès aux personnes qui en ont besoin seulement pendant une courte durée. Assurez-vous que les modèles ne sont pas manipulés et veillez à ce qu'ils soient liés à des applications spécifiques.\n\nLes données doivent être cryptées au repos, en transit et à l'utilisation. Dans le passé, cela était coûteux en termes d'utilisation du processeur et de temps, mais désormais une grande partie du cryptage est réalisée directement dans le silicone et est très rapide avec un minimum, voire aucune perte de performance.\n\n## Appel à l'action\n\nLes données sont la clé du succès et de la sécurité de l'IA, il est donc crucial de les protéger et d'utiliser les meilleures pratiques en matière de sécurité dès le départ. Opérationnalisez les pipelines pour éviter que les humains ne soient impliqués quotidiennement dans le déploiement et les tests des algorithmes d'IA. Automatisez autant que possible et intégrez la sécurité dans le pipeline DevOps de l'IA pour protéger vos données sources, votre modèle et votre application.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT85-fr","image":"./episodes/edt-85/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren aborde les aspects de données de l'intelligence artificielle (IA) et l'importance de sécuriser ces données."},{"id":159,"type":"Episode","title":"Les choses que j'aurais aimé savoir en tant que CTO du gouvernement.","tags":["cto","people","process","change","organizationalchange"],"body":"\r\n\r\nQu'est-ce que Jason et Ron ont appris depuis qu'ils ont rejoint Intel et qu'ils auraient aimé savoir quand ils étaient dans l'armée ?\n\nJason a été surpris de voir à quel point les solutions intégrées de l'industrie étaient meilleures et développaient des capacités incroyables qui manquaient dans l'armée. Il a appris que les cas d'utilisation en entreprise montrent que comprendre l'ensemble des problèmes du client est plus précieux que de simplement pousser la technologie. Les directeurs techniques doivent connaître la technologie et apprendre à changer la culture des membres de l'équipe pour s'y adapter. Rendre l'expérience utilisateur plus conviviale, construire une technologie plus intelligente, plus légère et plus rapide, et impliquer les personnes sont ce qui peut faciliter les transitions. Il aurait aimé voir davantage de résultats et de solutions dans l'armée au lieu de simplement des produits.\n\nRon est d'accord avec l'évaluation de Jason et ajoute que le défi qu'il a observé de l'intérieur du gouvernement était de persuader l'industrie de devenir partenaire de solution de mission afin d'informer au mieux les équipes militaires sur la manière d'appliquer leur technologie à ces missions. Une technologie formidable sans comprendre comment résoudre le problème de mission réel peut être un désastre.\n\nUn autre élément sur lequel ils sont tous d'accord et qui est important est l'expérience utilisateur. L'association de la technologie avec les personnes qui doivent l'utiliser est essentielle. Tout le monde n'a pas besoin d'avoir un doctorat en génie pour utiliser la technologie. L'expérience utilisateur doit être adaptée au niveau de compétence de vos collaborateurs. Le résultat dépend rarement uniquement des machines. Lorsque vous proposez des solutions pour la défense ou l'industrie, vous devez comprendre les besoins technologiques de la mission, ainsi que l'environnement dans son ensemble et comment y travailler. Particulièrement dans le domaine militaire, vous devez prévoir d'exécuter des actions lorsque les choses ne se passent pas bien et les rendre fonctionnelles. La technologie ne peut pas remporter la victoire si vous ne prenez pas en compte l'élément humain ni les circonstances difficiles.\n\nQuelles sont les plus grandes ou les plus excitantes surprises pour Jason et Ron lorsqu'ils sont passés de l'armée à l'industrie privée ?\n\nJason et Ron disent qu'ils ne s'attendaient pas à une culture distincte de travail d'équipe inhérente au service militaire dans le secteur privé chez Intel. Les deux ont trouvé l'expérience d'intégration et le soutien continu encourageants. Ils apprécient l'attitude générale de collaboration et la capacité de créer des choses avec une équipe dynamique qui dépasse tout ce que vous pourriez accomplir seul.\n\nIls apprécient tous les deux qu'il ne leur est pas demandé de vendre uniquement des produits Intel, mais d'aider les clients à résoudre les défis de mission et de donner des commentaires aux clients à Intel. Ils sont chez Intel pour résoudre des problèmes, en particulier dans le secteur public, peut-être avec une technologie qui n'a pas encore été créée.\n\nQuelles sont les lacunes technologiques que Jason et Ron constatent au sein du Département de la Défense ou d'Intel ?\n\nTout d'abord, Jason dit que les opérations de cloud sont primordiales et représentent une dynamique en évolution au sein du DOD. Les commandants sur le champ de bataille évitent les risques. Il n'y a pas de place pour les DDIL. À mesure que les opérations de cloud évoluent, il faut se requalifier et réapprendre tout le travail pour une opération de cloud précise, ainsi qu'apprécier les avantages de la capacité edge to cloud qui offre des informations en temps réel et précises, qui parviennent aux bonnes personnes au bon moment. Tout le monde doit avoir une conscience des situations et une image opérationnelle de ce qui se passe.\n\nRon croit que la prochaine chose sur la liste est d'augmenter la cybersécurité à mesure que la surface de vulnérabilité augmente. Si l'armée ne s'oriente pas vers la confiance zéro lors de sa transition vers un bord compétent et hautement mobile, les résultats en cas de conflit pourraient être désastreux. Le problème du DDIL est énorme, mais il doit être sécurisé contre l'augmentation de la surface de vulnérabilité.\n\nJason croit que la technologie doit avancer malgré les risques, car l'armée gagne toujours grâce à l'information. Que ce soit pour des missions de la FEMA, fournir de l'énergie nucléaire à une ville, établir des hôpitaux de campagne pendant la COVID ou sur le champ de bataille, le progrès technologique, notamment la 5G, est essentiel aux opérations. Les dirigeants du Département de la Défense doivent disposer d'informations et de la capacité de communiquer avec leur quartier général pour obtenir des directives, en particulier lors des opérations de combat où le commandement peut changer en raison des pertes, jusqu'au simple caporal.\n\nRon utilise l'exemple de la plus haute priorité de la défense nationale qui est de ne jamais devoir combattre un adversaire sur le territoire national. Étant donné que les États-Unis pourraient ne pas avoir l'avantage quantitatif loin de chez eux lors d'un conflit contre un adversaire également technologiquement avancé, l'armée doit être plus capable. L'armée doit continuer à fournir une capacité accrue aux forces de défense nationale et le faire de manière sécurisée, malgré l'augmentation de la vulnérabilité. Ces problèmes doivent être résolus afin que les opérateurs puissent avoir confiance en les données et les utiliser efficacement dans un environnement à l'étranger. Il n'y a pas d'autre option que de s'orienter dans cette direction.\n\nJason ajoute qu'un autre domaine dans lequel le DOD a été lent à s'adapter est l'IA, en raison d'un manque de personnes ayant les compétences requises. On ne devient pas expert en IA en sortant de l'école des Rangers de l'Armée. Le même problème existait avec la cybersécurité pendant des années jusqu'à ce que le DOD fasse un investissement important. Certains ressources complexes doivent être consacrées aux opérations d'IA car l'IA peut tout changer.\n\nRon, ayant passé les dernières années au service de l'espace nucléaire, souligne que les États-Unis ne retireront jamais l'élément humain des boucles de décision critique. L'intelligence artificielle sera énormément précieuse car elle peut garantir que la technologie s'adapte de manière dynamique. Elle réduira la charge cognitive et traitera d'innombrables points de données, permettant aux décideurs humains d'avoir une conscience situationnelle plus évidente et d'être mieux préparés à prendre rapidement des décisions éclairées. C'est l'espace dans lequel le Département de la Défense doit s'engager en matière d'IA.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jason Dunn-Potter","Ron Fritzemeier"],"link":"/episode-EDT86-fr","image":"./episodes/edt-86/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren Pulsipher, architecte principal des solutions, secteur public, chez Intel, accueille les invités spéciaux Jason Dunn-Potter, ancien officier en chef des mandats d'officier, US Army, et Ron Fritzemeier, ancien contre-amiral, US Navy. Les deux occupent désormais depuis cinq mois des postes d'architectes de solutions et de spécialistes de mission au sein de l'équipe du département de la défense d'Intel."},{"id":160,"type":"Episode","title":"Calcul confidentiel en DevSecOps","tags":["confidentialcomputing","devops","compute","technology","process","devsecops","cybersecurity","sgx"],"body":"\r\n\r\nLe logiciel d'Anjuna permet aux applications de fonctionner avec la protection SGX d'Intel et résout le problème de la protection des données en cours d'utilisation. La mission d'Anjuna est de rendre les enclaves sécurisées aussi simples que possible. Avec le logiciel d'Anjuna, il n'est pas nécessaire de modifier quoi que ce soit dans l'application ; il suffit de la prendre, de l'exécuter dans une enclave et la technologie SGX fonctionnera immédiatement. Le logiciel fonctionne avec toutes les applications, sur n'importe quelle plateforme cloud et à n'importe quelle échelle.\n\nLa chaîne d'approvisionnement mondiale des logiciels est attaquée. SolarWinds, notamment, a été une attaque contre DevOps, et bien qu'il y ait eu des idées sur la façon de résoudre le problème, cela n'a pas encore été verrouillé. La technologie Anjuna peut être une solution facile. Il n'est pas nécessaire de réorganiser votre logiciel ou de changer de méthodologie. Vous les exécutez dans des enclaves sécurisées.\n\nLorsqu'il a essayé le logiciel d'Anjuna, Darren a créé une pile en utilisant Intel SGX en bas, Red Hat OpenShift, Anjuna pour la partie de calcul confidentiel, et HashiCorp's Vault pour stocker un registre sécurisé. Il a été choqué de voir à quel point la solution était rapidement mise en place et opérationnelle en moins d'une semaine.\n\nDarren appelle ce processus le pipeline sécurisé de DevSecOps, bien qu'il soit composé de plusieurs éléments en mouvement. Ofir est d'accord avec cette terminologie, car ce processus est un nouvel DevOps renforcé avec la technologie matérielle SGX associée au logiciel d'Anjuna.\n\nL'informatique confidentielle, ou enclave sécurisée, résout le problème de la protection des données. Lorsque vous stockez des données dans un stockage persistant, la solution pour les données au repos est déjà présente. Il existe également une solution pour les données en transit avec le protocole TLS. La sécurisation des données en cours d'utilisation n'a pas encore été résolue car lorsque les données sont en cours d'utilisation, l'application doit y accéder en mémoire en clair. Elles ne peuvent pas être simultanément chiffrées et utilisées. Cela pose un problème sans fin. Si un acteur malveillant a accès à une machine sur laquelle l'application est en cours d'exécution, une intrusion est aussi simple que de passer par le dispositif, d'identifier le processus et de créer une copie de la mémoire. Ils auront ainsi accès à tous les secrets et données confidentielles stockées, et celles-ci ne seront pas chiffrées. Cela inclurait également les clés de chiffrement des données au repos et en transit car le logiciel a besoin de les utiliser pour chiffrer les données. L'acteur malveillant aura accès aux clés du royaume.\n\nLe problème est résolu si vous exécutez les différentes applications dans des enclaves sécurisées. Même si quelqu'un parvient à accéder à la machine, il n'aura pas accès à la mémoire de chaque application. Cela ne signifie pas que vous n'avez pas à résoudre les vulnérabilités, mais vous êtes beaucoup moins stressé pour les résoudre le plus rapidement possible. Même s'il existe des vulnérabilités du noyau, lorsque quelque chose s'exécute dans une enclave sécurisée, le noyau ne peut pas accéder à sa mémoire.\n\nLe logiciel Anjuna fonctionne sur d'autres technologies matérielles en plus de SGX. Contrairement à l'exécution du chiffrement en logiciel, où l'impact sur les performances serait élevé, Anjuna peut ajuster finement la configuration pour exécuter votre application avec un impact négligeable sur les performances : moins de cinq pour cent.\n\nDonc, vous ne voudrez peut-être pas tout mettre dans une enclave sécurisée pour le moment, mais c'est l'avenir de la sécurité.\n\nL'un des usages d'une enclave sécurisée est de stocker des données qui couvrent différentes étapes du pipeline DevOps dans un registre sécurisé. Le registre contient toutes les informations liées à la construction, les clés de sécurité et les valeurs de hachage utilisées pour la vérification. Ces valeurs de hachage de vérification doivent rester inchangées tout au long du cycle afin que personne ne puisse injecter de code, de bibliothèques ou de binaires dans le package que vous livrez. Tout devrait s'exécuter dans un conteneur dans le monde moderne.\n\nUn autre candidat à la protection est une clé de signature. Sans des enclaves sécurisées, une fois que vous avez un fichier binaire prêt, vous devez le transférer vers une autre machine dans une pièce sombre à laquelle personne n'a accès. Mais trois personnes avec trois clés différentes le signent là-bas. Les enclaves sécurisées permettent d'accéder à cette clé de signature dans votre environnement familier, mais seule l'enclave y aura accès. Cela sera basé sur l'identité complexe du logiciel en cours d'exécution à l'intérieur de l'enclave SGX, qui est mise en œuvre via la citation d'attestation. En d'autres termes, vous pouvez attester une enclave vers une autre enclave. Vous pouvez également attester des éléments qui s'exécutent en dehors des enclaves. Cela vous donne la possibilité de faire confiance à un logiciel qui s'exécute ailleurs.\n\nLa compilation des binaires est une autre utilisation. L'un des grands problèmes au sein du Département de la Défense, par exemple, est qu'ils veulent être assurés que tout ce qui a été intégré à la construction peut être retracé jusqu'au développeur qui l'a écrit. Surtout dans les systèmes embarqués, où le logiciel contrôle des machines valant des millions de dollars qui peuvent tuer des gens ou sauver des vies. Il doit y avoir une traçabilité complète pour aider à garantir la responsabilité et le bon déroulement du développement sécurisé.\n\nEn plus des attaques par vidage de mémoire, un autre problème d'attaque que Anjuna résout est de s'assurer que dans les cas où vous avez besoin d'accéder au noyau, il protégera tout ce qui doit être couvert dans cette interaction entre l'enclave et le monde extérieur. Il peut également protéger contre l'accès au code et rendre les secrets uniquement disponibles pour l'enclave. De plus, si quelqu'un pénètre dans une machine, il ne pourra pas trouver un certificat TLS en clair ou la clé utilisée pour l'encrypter.\n\nChaque fournisseur de services cloud propose des enclaves sécurisées, et Anjuna les prend tous en charge. Ils prennent également en charge les technologies locales. En plus de l'offre principale, Anjuna peut également permettre la capacité de chiffrer vos données au repos et en transit sans modifier votre logiciel, même dans les applications héritées ou les nouvelles applications qui ne prennent pas en charge le chiffrement de chaque fichier de données.\n\nPour plus d'informations, visitez anjuna.io, ou consultez un livre blanc rédigé par Darren et Ofir sur embracingdigital.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ofir Azoulay-Rozanes"],"link":"/episode-EDT87-fr","image":"./episodes/edt-87/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren Pulsipher, chef architecte des solutions chez Intel, et Ofir Azoulay-Rozanes, directeur de la gestion des produits chez Anjuna, discutent des solutions d'Anjuna pour la confidentialité des données dans le cycle de vie DevOps."},{"id":161,"type":"Episode","title":"Collaborative DevSecOps","tags":["cybersecurity","devsecops","rpa","technology","compute","zerotrust","zta"],"body":"\r\n\r\nL'expérience de Mike en tant qu'ingénieur en cybersécurité dans l'armée de l'air, puis en travaillant dans le développement, la sécurité, les opérations et les services gérés, l'a amené à un objectif de création d'un produit pouvant favoriser la collaboration pour construire une automatisation moderne autour de ce qu'il appelle l'IT en tant que code. Il voulait aborder le DevSecOps de manière globale, en rassemblant tout le monde.\n\nSon produit résultant, Sophos Factory, crée des solutions modernes autour de blocs de construction avec des fonctionnalités, des fonctionnalités et une expérience utilisateur qui peuvent être utilisées sur l'ensemble du spectre des talents techniques. Ce fut un problème complexe à résoudre, y compris des personnes travaillant visuellement, des développeurs qui codent, etc.… Il devait également combler le fossé entre le matériel et le logiciel, en utilisant un processus agile entre les équipes.\n\nSophos Factory n'est pas seulement un pipeline CI/CD. C'est une petite partie du système global, qui fonctionne de bout en bout depuis le développement, la sécurité, les opérations et le déploiement, avec des fonctionnalités telles qu'un constructeur visuel, un DSL et une prise en charge de tout le contenu dans son format d'origine. Il s'intègre également aux systèmes existants. Il rassemble toutes les différentes équipes et les différents outils qu'elles utilisent, donc il va bien au-delà de la simple création d'un pipeline ou de l'automatisation.\n\nLes utilisateurs individuels se voient présenter les éléments avec lesquels ils sont familiers, mais tous avec la même interface. Par exemple, un ensemble de scripts peut être créé à partir d'un format visuel. Une personne chargée de la sécurité peut utiliser la même interface avec les outils et les artefacts auxquels elle s'attend. Un développeur à stack complet ou un ingénieur DevOps peut intégrer et créer tous les artefacts d'une manière que les autres équipes peuvent utiliser. Il ne s'agit pas de la création d'un pipeline pour un élément d'automatisation, mais d'un tissu interconnecté entre des systèmes disparates.\n\nL'intégration signifie le mouvement des données, mais cela signifie aussi des actions. Par exemple, supposons que quelqu'un utilise Jira du côté programmation, ServiceNow du côté opérations informatiques, et une fonction de réponse aux incidents du côté sécurité. Dans ce cas, vous pouvez intégrer toutes ces parties et envoyer quelque chose sur Slack, afin que tout le monde ait une visibilité et puisse répondre quasiment en temps réel.\n\nSophos Factory empaquette des modules dans des pipelines pour assurer leur réutilisation, ce qui en fait des éléments constitutifs. Ils peuvent être construits autour de divers cas d'utilisation, mais l'objectif est de créer quelque chose qui puisse être utilisé encore et encore. Par exemple, supposons que vous utilisiez ServiceNow et que vous souhaitiez créer un ticket. Dans ce cas, vous l'utilisez dans différents autres cas d'utilisation liés à l'automatisation réseau, à l'automatisation de l'infrastructure, au cloud, à la sécurité native, etc. Il s'agit de créer une solution plutôt que de simplement automatiser ces éléments ensemble. Le dernier élément est la pérennité future, pas seulement la répétabilité. Vous pouvez ajouter ou soustraire des éléments du pipeline global, ce qui n'est pas possible avec le matériel mais également très difficile avec les systèmes existants tels que les systèmes CI/CD conçus pour la mise en production de logiciels, pas pour la construction globale d'une solution et le maintien du cycle de vie dans le temps.\n\nAvec la technologie Sophos Factory, vous pouvez regrouper différents outils pour respecter les normes telles que CIF ou NIST 853 et les rendre disponibles sous forme de pipelines à faible code ou sans code. Sophos Factory se distingue des autres technologies d'automatisation grâce au partage via des catalogues de solutions. Vous pouvez publier des blocs de construction d'automatisation, des solutions complètes ou consommer l'automatisation créée par d'autres équipes. Cela crée une quantité considérable de flexibilité.\n\nLe contrôle de version est intégré aux pipelines et aux catalogues de solutions. Si vous utilisez un pipeline de solution à partir d'un enregistrement publié par quelqu'un d'autre, vous pouvez le configurer à la version de votre choix ou tirer la dernière version pour obtenir toutes les mises à jour. L'ABAC fait également partie du système au cas où vous voudriez limiter l'accès à certaines utilisateurs, par exemple, en lecture seule. Grâce à l'interopérabilité de Sophos Factory, vous pouvez également intégrer des outils de numérisation pour maintenir une visibilité dans le pipeline. Vous pouvez également exécuter différents canaux autour des outils de politique.\n\nSophos Factory tisse ensemble les flux de sécurité et d'informatique, créant un excellent point d'intégration parmi les trois têtes du monstre de Dev, Sec et Ops.\n\nPour améliorer la sécurité, Sophos Factory dispose d'un produit de confiance zéro et d'attestation, mais ils travaillent également avec d'autres produits de sécurité tels que HashiCorp Console. La capacité de confiance zéro et d'attestation est l'évolution naturelle pour authentifier entre différents systèmes. Au lieu des identifiants statiques, il existe maintenant de meilleures façons de communiquer et de partager l'attestation de manière sécurisée avec les autres.\n\nL'usine Sophos dispose d'un système d'habilitation intégré pour la gestion des clés, et il prend en charge Vault de HashiCorp ainsi que des modèles natifs du cloud. Ils peuvent également aider à la construction de services de gestion critiques intégrés au cloud et emballés autour d'un pipeline. Il n'y a pas seulement une variable d'habilitation lors de l'exécution, mais aussi une étape d'habilitation qui n'est évaluée qu'à l'exécution. Ils peuvent se superposer à ces outils de sécurité, devenant ainsi naturellement une partie de votre solution de construction.\n\nSophos Factory se situe dans l'espace de la RPA, mais elle va bien au-delà d'un simple exécutant de RPA typique. Ils sont techniquement en RPA car, même si les humains sont toujours impliqués dans la réalisation des tâches, les machines sont utilisées pour automatiser le processus. Les clients cherchent des moyens de mettre à l'échelle et d'obtenir de la valeur à partir des achats IT de manière sécurisée. Sophos Factory encourage le développement des compétences techniques et donne aux employés un accès à des ensembles d'outils, afin d'en tirer davantage et ce, de manière sécurisée.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Mike Fraser","Callen Sapien"],"link":"/episode-EDT88-fr","image":"./episodes/edt-88/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren discute avec Callen Sapien de Sophos, directeur de la gestion des produits chez Sophos Factory, et Mike Fraser, vice-président de DevSecOps, de leur produit qui permet une collaboration véritablement collaborative en SecDevOps."},{"id":162,"type":"Episode","title":"Avenir de la mémoire élevée et CXL","tags":["bigmemory","memverge","optane","technology","data","compute","pmem","cxl","ceo"],"body":"\r\n\r\nUn développement passionnant dans la mémoire est le CXL (Compute Express Link). Un écosystème solide est en train d'être développé du côté hôte, avec le soutien d'Intel et d'autres acteurs pour le CXL 1.1 dans leurs plates-formes de prochaine génération, telles que Sapphire Rapids. Ceux du côté mémoire, y compris la quatrième génération d'Intel Optane, seront compatibles avec le CXL. Intel est un membre moteur d'un consortium CXL fort qui a participé à l'élaboration de la norme. CXL 2.0 a déjà été défini, avec des capacités supplémentaires, notamment l'externalisation et la commutation du CXL, et la version 3.0 est en cours de développement, ce qui permettra de standardiser la façon dont le CXL peut être provisionné et partagé.\n\nIl y aura de nouveaux produits qui sortiront de Samsung, Micron et SK-Hynix avec des capacités CXL. De plus, de nouveaux interconnexions sont en cours de développement qui pourraient potentiellement connecter la mémoire avec le tissu, ce qui permettra d'avoir une mémoire compatible avec le tissu qui pourra être partagée entre plusieurs serveurs.\n\nLa technologie CXL est révolutionnaire. Un nouveau protocole de mémoire fonctionne sur le standard PCIe de cinquième génération et ultérieur. Étant donné qu'il permet de placer la mémoire sur PCIe, elle ne peut pas seulement être à l'intérieur de la boîte, mais à l'avenir, avec la commutation PCIe, la mémoire sur le tissu pourra devenir composable et partageable. Le premier produit CXL devrait être disponible d'ici la fin de l'année.\n\nMemVerge dispose déjà d'un contrôleur de mémoire défini par logiciel et CXL a ouvert un tout nouveau monde pour le logiciel. CXL est à la mémoire ce que le canal à fibre est au stockage. C'est comme un réseau d'espaces mémoire plutôt qu'un réseau de stockage. Il est possible d'accéder directement à la mémoire avec CXL et contourner complètement le processeur.\n\nCXL sera beaucoup plus rapide que les technologies d'interconnexion précédentes. Il aura une latence de 100 ou 200 nanosecondes. C'est là que l'utilisation, la gestion et l'agilité augmentent. Il y aura une disponibilité supérieure et une productivité accrue dans l'utilisation de la mémoire. De plus, vous pourrez allouer dynamiquement de la mémoire ; vous pouvez l'allouer selon vos besoins, et elle n'a pas besoin de s'adapter à la boîte du serveur. Théoriquement, vous aurez toujours suffisamment de mémoire pour tout ce dont vous avez besoin pour être actif.\n\nAvec la technologie de snapshot de MemVerge, vos données sont protégées et persistantes. Cela devient d'autant plus important lorsque votre mémoire devient plus grande. Si vous la perdez, il est plus difficile de la reconstruire.\n\nPour que CXL décolle, trois éléments doivent être en place. Du côté du matériel, les principaux fabricants doivent adhérer aux mêmes normes. Cela s'est produit au cours de l'année dernière, il y a donc une norme unique que tout le monde supporte. Deuxièmement, vous n'avez pas besoin de modifier votre application pour utiliser CXL, tout comme les réseaux de stockage. Troisièmement, du point de vue de la base de données, vous ne devriez pas avoir à réécrire. Cela peut se produire entre ce que les normes fournissent, ce que le système d'exploitation prend en charge et le logiciel MemVerge qui peut faire de l'auto-affectation entre la mémoire DDR et la mémoire CXL.\n\nMemVerge peut offrir cette couche d'abstraction. C'est essentiellement la virtualisation de la mémoire. La mémoire définie par logiciel gère l'emplacement réel de la mémoire physique.\n\nMemVerge rend la mémoire vive transparente pour l'application afin que les programmeurs puissent utiliser une capacité plus importante et ne jamais manquer de mémoire. Ensuite, il y a la protection des données. MemVerge a développé un service de capture instantanée en mémoire qui peut enregistrer l'état complet d'une application en mémoire, et cet état est immuable. Il peut être récupéré à tout moment, n'importe où. Il y a de nombreux cas d'utilisation pour cela, tels que la médiation des rançongiciels et la réduction des délais de cycle dans la recherche en génomique.\n\nLa fonctionnalité de capture d'écran n'est pas seulement utile car elle peut rapidement et facilement capturer un pipeline en cours d'exécution, vous permettant de revenir en arrière et de récupérer à tout moment, mais elle peut également faire économiser de l'argent lors de l'utilisation de services cloud. Les principaux fournisseurs de services proposent des instances temporaires à des prix réduits de 70 à 90 % par rapport au prix demandé, mais il y a un piège : ils peuvent les reprendre à tout moment avec un préavis de seulement 30 secondes à deux minutes. Cela ne laisse pas suffisamment de temps pour réagir, surtout si vous avez beaucoup de données en mémoire, donc cela n'a pas été utile pour de nombreuses charges de travail. Avec la capacité de capture d'écran de MemVerge, vous pouvez prendre périodiquement des captures d'écran de votre charge de travail en cours d'exécution sur n'importe quelle instance. Si l'instance temporaire est retirée, vous disposez d'une image que vous pouvez récupérer et continuer à exécuter. C'est une assurance qui vous permet d'utiliser le service à faible coût en toute sécurité.\n\nPuisque vous prenez une capture instantanée non seulement d'une application mais d'un conteneur ou d'une instance entière, vous pouvez le réinstaurer n'importe où, sur site, dans le même cloud ou dans un autre cloud. Cela vous offre une mobilité maximale et une résilience dans vos opérations, même en cas de panne majeure d'un service cloud. Cette technologie ouvre la porte à de nombreuses possibilités passionnantes.\n\nLa révolution CXL et le logiciel MemVerge Memory Machine sont des combinaisons puissantes pour des possibilités révolutionnaires.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Charles Fan"],"link":"/episode-EDT89-fr","image":"./episodes/edt-89/fr/thumbnail.jpg","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions chez Intel, discute avec Charles Fan, PDG de MemVerge, de la façon dont la révolution CXL et le logiciel de MemVerge représentent l'avenir de la mémoire étendue."},{"id":163,"type":"Episode","title":"Télétravail: Sécuriser votre bureau à domicile","tags":["cybersecurity"],"body":"\r\n\r\n## Conseils de sécurité du système\n\nLa première chose que vous souhaitez faire pour sécuriser votre bureau à domicile est de mettre à jour tous vos systèmes. Cela comprend à la fois les applications et les systèmes d'exploitation de tous vos appareils connectés à Internet. Vous voudrez également mettre à jour le logiciel de sécurité des points d'extrémité et effectuer des analyses régulières lorsque votre appareil est allumé. De nombreuses personnes savent le faire sur leur ordinateur de bureau ou leur ordinateur portable, mais la sécurité des points d'extrémité devrait également être déployée sur les téléphones et les tablettes. De plus, assurez-vous d'activer le pare-feu local et d'activer le pare-feu de votre routeur.\n\nL'une des principales mesures que vous pouvez prendre pour réduire votre risque global lorsque vous êtes sur Internet est de réduire la surface d'attaque en cours d'exécution. Cela signifie que vous devez fermer les applications inutilisées, fermer le navigateur avant d'accéder à de nouveaux sites, et vous déconnecter ou fermer les sessions sécurisées avant de faire des activités telles que la vérification des emails ou la navigation. Vous ne devez pas effectuer différentes activités simultanément pour éviter les attaques croisées.\n\nUn bon site éducatif pour apprendre à naviguer en toute sécurité sur internet est Stop. Think. Connect. https://www.stopthinkconnect.org/\n\n## Conseils de sécurité pour le modem/routeur/wifi.\n\nChanger tous les mots de passe par défaut en mots de passe sécurisés (minimum de 8 à 10 caractères, utiliser des majuscules et des minuscules, des chiffres et des caractères spéciaux). Les mots de passe par défaut sont fournis avec les routeurs, les modems, les portails web des fournisseurs d'accès Internet et le WiFi. Protégez soigneusement l'accès à vos mots de passe. Il est également important de changer le nom du réseau par défaut (SSID) pour quelque chose qui ne contient aucune information d'identification.\n\nActiver l'authentification à deux facteurs chaque fois que possible vous offrira une couche de sécurité supplémentaire. Les routeurs et les modems doivent être mis à jour tout comme votre ordinateur portable, donc assurez-vous d'activer les mises à jour automatiques.\n\nD'autres mesures pour renforcer la sécurité incluent l'activation du WPA et la désactivation du WPS, si possible. Activez la traduction d'adresse réseau (NAT) et le filtrage DNS sur le routeur et le modem. Vous souhaiterez également désactiver le UPnP.\n\nCes techniques empêcheront les personnes non autorisées et vos voisins de \"squatter\" votre WiFi, ce qui crée un risque pour la sécurité.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT9-fr","image":"./episodes/edt-9/fr/thumbnail.png","lang":"fr","summary":"Sécuriser votre appareil et votre centre de données ne vous suffit pas. Avec un nombre croissant de personnes travaillant à domicile, vous devez aider vos employés à sécuriser leur réseau domestique et leur espace de travail. Dans cet épisode, Steve Orrin, CTO fédéral chez Intel, aide Darren à sécuriser son réseau domestique."},{"id":164,"type":"Episode","title":"Barrières à l'adoption des technologies futures","tags":["cto","organizationalchange","change","people","process"],"body":"\r\n\r\nGlenn est le directeur de la technologie chez CACI, une entreprise de six milliards de dollars de taille Fortune 500 axée sur la sécurité nationale. Il travaille dans l'industrie depuis 35 ans, se consacrant aux affaires militaires, au renseignement et à la cybersécurité pour le gouvernement américain. Étant ingénieur, il pense, agit et traite les questions comme un ingénieur.\n\nNouvelle technologie passionnante en IA, neuromorphisme et sécurité n'est parfois pas rapidement adoptée dans le secteur public.\n\nLa première barrière est la complexité. Ceux de la communauté technique sont des innovateurs exceptionnels, mais pas toujours doués pour rendre la technologie facile à consommer, utiliser ou administrer. Cependant, Glenn constate certaines améliorations. Par exemple, au cours des cinq dernières années, la complexité de l'utilisation de l'IA a considérablement diminué, car de plus en plus de personnes utilisent des outils et des kits de développement logiciel.\n\nUne autre barrière peut être le manque de talent pour concevoir, mettre en œuvre et maintenir la technologie dans le secteur public. Il est indispensable d'attirer de nouveaux talents dans l'industrie, ce qui implique également qu'ils doivent pouvoir obtenir des autorisations. L'autre aspect de l'équation du talent est que de nombreux individus résistent à l'adoption de nouvelles technologies. C'est un défi en termes de gestion du changement. Si cela n'est pas abordé correctement, les talents existants deviennent des anticorps face à l'adoption de nouvelles technologies. Au lieu de chercher comment dire \"oui\", ils trouvent mille raisons de dire \"non\".\n\nLe troisième est la cybersécurité. Glenn ne décrit pas cela comme une barrière car ce n'est pas quelque chose qui serait enlevé. Cependant, l'industrie pourrait être plus proactive en déplaçant les décisions davantage vers la gauche et en impliquant la sécurité plus tôt pour accélérer l'adoption.\n\nLe secteur public pourrait en tirer une leçon de la rapidité avec laquelle la technologie des smartphones a été adoptée avec l'avènement de l'iPhone. Les premiers iPhones étaient incroyablement simples à utiliser, et l'entreprise était déterminée à offrir aux clients quelque chose qu'ils pouvaient utiliser immédiatement. De plus, la communauté de développement pouvait instantanément créer et innover avec les kits d'outils et les processus de développement de logiciels. Les enfants de sixième peuvent développer avec les kits d'outils pour iPhone et Android. L'utilisabilité était essentielle et cela a permis d'instaurer un développement de logiciels efficace.\n\nLe secteur public devrait essayer de reproduire ces qualités et d'aborder son marché et ses clients de la même manière, notamment en ce qui concerne l'analyse. Il y a une quantité énorme de données, et nous n'en tirons pas beaucoup de bénéfices. Tout comme Steve Jobs a élargi son secteur d'activité au-delà de la cible moyenne, la même chose doit se produire dans le domaine de l'analyse. Au lieu de cibler les scientifiques des données, peut-être que la cible devrait être un chef d'entreprise ou une personne ordinaire qui gère un foyer.\n\nGlenn croit que la première étape pour envisager l'IA et l'analyse de cette manière consiste à s'engager fermement pour libérer les données. Tant de données sont enfermées dans des conceptions exclusives que cela constitue un modèle économique médiocre. Les clients devraient renouveler les contrats non pas parce que leurs informations sont contrôlées et fermées par un fournisseur, mais parce que le fournisseur propose des solutions, de la valeur et de l'innovation.\n\nL'IA devrait être présentée comme un assistant numérique auprès des clients plutôt qu'un robot mystérieux et magique en arrière-plan, suscitant la méfiance du client quant à sa confiance. Un assistant numérique est simplement un pas en avant par rapport à ce que vous essayez d'accomplir en fournissant des données et des informations grâce à l'apprentissage automatique et l'apprentissage profond qui facilitent votre vie et vous permettent de réaliser le traitement. De cette manière, l'IA ajoute une valeur instantanée et est beaucoup moins effrayante.\n\nGlenn croit que la technologie que le secteur public doit avoir aujourd'hui est, tout d'abord, le cloud et en plus grande quantité. Le cloud élimine beaucoup d'erreurs humaines dans l'administration. Il réduit la surface d'attaque et est basé sur un modèle de consommation à la demande, ce qui peut être économique si vous développez des logiciels pour ce modèle.\n\nDeuxièmement, il existe de bonnes plateformes DevSecOps. Les programmeurs de logiciels peuvent passer plus de temps sur le développement réel grâce aux outils disponibles. DevSecOps en est encore à ses débuts et sa croissance sera explosive.\n\nUne technologie clé de travail à distance pendant le COVID est Commercial Solutions for Classified (CSFC). Cette technologie existe depuis une décennie. Cependant, elle a atteint un stade de maturité où la complexité a été éliminée de son déploiement, de son administration et de sa sécurité, pour être facilement accessible à tous. Ainsi, une personne peut travailler dans un domaine non classifié tout en ayant accès à des informations confidentielles, sans risque de fuites. Il s'agit d'une nouvelle et importante capacité.\n\nTout cela est lié à l'attraction et à la rétention des talents. La difficulté réside non pas dans le fait que le secteur public n'a pas de problèmes passionnants et stimulants, mais dans la perception que le gouvernement avance très lentement. Il y a une part de vérité à cela en raison de l'importance de maintenir une certaine sanctité ou confiance. Pourtant, en réalité, le gouvernement est à la pointe de nombreuses nouvelles technologies telles que la photonique ou la communication basée sur la lumière. Certaines régions avancent rapidement, comme dans le développement logiciel avec l'approche agile et DevSecOps.\n\nGlenn croit que le domaine spatial explosera et sera fondamentalement différent dans cinq ans pour l'avenir de la technologie. La possibilité de mettre plus de choses en orbite de manière économique grâce aux améliorations de la taille, du poids et de la puissance, ainsi que la capacité de communiquer avec la photonique sur des milliers de kilomètres rapidement, permettent la connectivité et la capacité de distribuer et d'utiliser des données sur ces charges utiles.\n\nUn autre domaine qui sera très différent est l'agilité spectrale. Le domaine du spectre électromagnétique sera crucial pour assurer une connectivité résiliente en cas de conflits. Les conflits ultérieurs mettront fortement l'accent sur qui peut communiquer et interférer avec les communications. L'agilité spectrale désigne la capacité de savoir dynamiquement ce qui se passe autour de vous dans le spectre afin de pouvoir vous déplacer rapidement. Cette connectivité est fondamentale pour le commandement et le contrôle conjoints ainsi que pour la vision JADC2.\n\nUne troisième zone concerne ce qui peut être fait avec Kubernetes et le code d'infrastructure. L'automatisation permettra de supprimer les tâches manuelles et de faciliter les choses en général.\n\nAucune conversation sur l'avenir de la technologie ne serait complète sans mentionner le Quantum. Les modèles de programmation quantique en cours de construction sont complètement différents des modèles traditionnels. Nous devons parvenir à un point où nous disposons d'outils pour programmeurs qui automatisent davantage le processus. Former quelqu'un pendant deux ans pour devenir programmeur en informatique quantique ne peut pas être généralisé, donc nous avons besoin que les outils de développement logiciel s'améliorent, tout comme la communauté a abstrait la complexité et développé des outils pour l'intelligence artificielle.\n\nLa dernière zone est l'évolution continue du calcul périphérique. Le nombre de processus de calcul est phénoménal, engendrant la créativité en termes de taille, de poids et de puissance. Le calcul périphérique continuera à se transformer pour être sécurisé et fiable. La communication résiliente peut ne pas être une connexion dédiée, mais un réseau en maillage où les parties du message sont recombines à l'autre extrémité. Cela peut fournir des solutions dans des environnements tactiques et perturbés.\n\nDarren voit l'architecture traditionnelle de Von Neumann décliner à l'avenir, car nous pouvons avoir une persistance des données sans disques durs, et nous pouvons avoir des données qui vivent, se déplacent et migrent avec des fonctions qui y travaillent. Les couches et les limitations du modèle de Von Neumann seront supprimées.\n\nGlenn pense que cela se produira plus rapidement que ce que les gens prédisent généralement en raison d'un biais d'expérience. Mais c'est un monde différent lorsque vous pouvez faire coexister toutes les autres pièces et éliminer les problèmes de latence. Imaginez ce qui pourrait être fait à la vitesse d'inférence en bordure, par exemple, avec des véhicules autonomes. Cette technologie est en train de se produire et les programmeurs envisagent des couches d'abstraction dès le début. Ainsi, vous pourriez, par exemple, prendre un modèle de réseau neuronal déjà développé et le faire passer par une boîte à outils de développement de logiciel pour le placer sur un substrat matériel, une architecture non-Von Neumann, et vous n'avez pas besoin de reprogrammer. Cela accélérera l'adoption et sera transformationnel.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Glenn Kurowski"],"link":"/episode-EDT90-fr","image":"./episodes/edt-90/fr/thumbnail.jpg","lang":"fr","summary":"Darren Pulsipher, Architecte principal des solutions chez Intel, discute des obstacles à l'adoption de nouvelles technologies dans le secteur public et de ce qui nous attend dans le futur avec Glenn Kurowski, directeur technique de CACI."},{"id":165,"type":"Episode","title":"Combattre l'adversaire cyber en sécurisant votre chaîne d'approvisionnement logicielle","tags":["cybersecurity","devsecops","sbom","securesupplychain","policy","process","compute"],"body":"\r\n\r\nDarren et Eric Greenwald, Conseiller juridique général de Finite State, discutent de la sécurisation de la chaîne d'approvisionnement logicielle dans cet épisode.\n\nL'État Fini se concentre sur la recherche de vulnérabilités dans les micrologiciels, le plus souvent des logiciels provenant de tiers qui pourraient déjà comporter des vulnérabilités existantes, avant de les intégrer dans leurs appareils. L'État Fini se concentre principalement sur les dispositifs IoT industriels, les dispositifs médicaux, les automobiles et l'électronique grand public.\n\nPendant sept ans avant de rejoindre Finite State, Eric a travaillé en tant qu'avocat dans le secteur privé, se concentrant sur les tests de sécurité et l'identification des vulnérabilités pour les entreprises de cybersécurité. Avant cela, il a travaillé dans le domaine de la cybersécurité et de la sécurité nationale au sein du gouvernement, pour le FBI et la CIA, et en tant que conseiller principal pour le Comité du Renseignement de la Chambre des représentants. Son travail gouvernemental a culminé lorsqu'il est devenu le directeur principal de la cybersécurité au Conseil de sécurité nationale de la Maison Blanche.\n\nEric croit que l'une des raisons pour lesquelles il y a désormais une insistance sur la sécurisation de la chaîne d'approvisionnement est que la menace a évolué. La nature de plus en plus complexe des logiciels, y compris le fait que de nombreux composants ont des vulnérabilités lorsqu'ils sont créés et ne sont découverts que plus tard, rend plus difficile la recherche de dispositifs construits via la chaîne d'approvisionnement logicielle. De plus, les récentes attaques très médiatisées via une violation de la chaîne d'approvisionnement logicielle, telles que SolarWinds, ont rendu les gens plus conscients du danger.\n\nDans l'attaque SolarWinds, l'auteur a fait preuve de patience, ne causant pas immédiatement de chaos mais attendant un an tout en se baladant à travers les réseaux informatiques américains, cultivant l'accès et les informations. Cette patience est probablement la différence la plus significative entre une attaque nationale et une attaque criminelle. Alors que les attaques criminelles patientes sont parfois plus axées sur un retour sur investissement financier, ceux qui sont derrière une attaque étatique sont prêts à passer des années pour développer leur accès aux renseignements. Cependant, dans les deux cas, les attaques deviennent plus sophistiquées et sont beaucoup mieux placées pour profiter de manière dévastatrice de la complexité de la chaîne d'approvisionnement.\n\nPour combattre ces attaques, de nouvelles réglementations sont en train d'émerger. L'administration Biden a émis l'ordre exécutif 14028 en mai 2021, qui comporte deux points principaux : le développement sécurisé de logiciels et une liste des composants logiciels (SBOM). L'ordre exécutif ne s'adresse qu'aux marchés publics, mais il est probable que l'industrie privée le suive.\n\nLes détails et recommandations techniques concernant le développement sécurisé de logiciels sont encore en cours d'élaboration. Cependant, une partie de ces recommandations stipule que les fournisseurs de logiciels destinés au gouvernement doivent fournir un SBOM (Software Bill of Materials). Le premier projet de loi relatif aux SBOM a été publié en 2014, ce qui a permis le développement de normes plus matures. Un SBOM est essentiellement une liste des composants logiciels utilisés dans un produit logiciel, pas si différente d'une liste d'ingrédients sur un produit alimentaire. Cela offre une transparence dans la chaîne d'approvisionnement, ce qui est essentiel pour évaluer les vulnérabilités ou pour pouvoir identifier une vulnérabilité découverte ultérieurement.\n\nUn excellent exemple de cela est ce qui s'est passé avec Log4j. Lorsque cette vulnérabilité a été découverte, de nombreuses entreprises ne savaient pas si elles l'avaient dans leur pile. Ce ne serait pas une formule magique, mais un SBOM permettrait aux entreprises de découvrir plus facilement si elles ont le composant logiciel problématique dans leur système et d'agir plus rapidement pour mettre en place un correctif.\n\nLes arguments contre la publication des SBOM (Software Bill of Materials) sont qu'ils fourniront une feuille de route pour les attaquants et révéleront des informations exclusives. Bien que ces préoccupations soient légitimes et doivent être discutées, le Département du Commerce et le Département de la Sécurité intérieure considèrent qu'il est bien plus avantageux pour les défenseurs d'avoir de la transparence que de donner un quelconque avantage aux attaquants. Il existe des projets de loi bipartisans en faveur des SBOMs. Il existe des moyens de réduire le risque que les SBOMs tombent entre de mauvaises mains, tels que des contrats sécurisés ou non fongibles. Les débats sur ces préoccupations se poursuivront dans le secteur public et de plus en plus d'entreprises les adopteront.\n\nCette législation est en train de se produire car la FDA a suggéré que les fabricants d'appareils médicaux intègrent des SBOM dans le processus d'examen, de sorte que les SBOM gagnent du terrain auprès de ces fabricants. Le monde physique est de plus en plus affecté par les logiciels dans les appareils médicaux, ainsi que par des systèmes intégrés tels que les systèmes de contrôle des centrales électriques, les systèmes de chauffage, de ventilation et de climatisation, les commandes d'aéroport, etc. Par conséquent, les systèmes opérationnels sont en danger, avec des conséquences beaucoup plus importantes que les attaques sur les systèmes d'entreprise.\n\nLa difficulté pour les professionnels de la technologie opérationnelle (TO) est que bon nombre des composants industriels sont plus anciens et n'ont pas nécessairement été mis à jour. Néanmoins, les pirates informatiques hésitent à se connecter à Internet et à effectuer des mises à jour, car c'est ainsi qu'ils parviennent à s'introduire. La meilleure solution à ce problème est d'essayer d'accéder à la transparence des composants dans la pile, de scanner le système et les appareils faisant partie du réseau TO, et de procéder à une rétro-ingénierie et à une décompilation pour en comprendre les détails. Fondamentalement, vous devez créer votre SBOM (bill of materials) et évaluer les vulnérabilités.\n\nCeci est le domaine principal de travail de Finite State. Ils examinent les systèmes et les appareils et effectuent une analyse. Ils disposent d'une plateforme qui automatise l'analyse du code intégré, fournit une lecture des vulnérabilités et identifie et regroupe les vulnérabilités les plus prioritaires. Parfois, il est possible d'éliminer toute une catégorie de vulnérabilité grâce à une seule correction. La création du SBOM en soi n'est donc pas suffisante. Il doit être intégré à un système de gestion des risques pour trier et filtrer les nombreuses vulnérabilités. Trouver les risques les plus prioritaires est un processus complexe, et Finite State peut aider les équipes de sécurité à hiérarchiser leurs actions pour protéger leurs systèmes.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Eric Greenwald"],"link":"/episode-EDT91-fr","image":"./episodes/edt-91/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte en chef des solutions chez Intel, et Eric Greenwald, avocat général de Finite State, abordent la sécurité de la chaîne d'approvisionnement logicielle."},{"id":166,"type":"Episode","title":"Protéger les murs extérieurs de votre château avec des analyses de vulnérabilité.","tags":["cybersecurity","endpointmanagement","vulnerability","cyberattack","iot","edge","censys"],"body":"\r\n\r\nMatt a passé huit ans en tant qu'officier du renseignement dans l'armée. Dans la dernière partie de son service, il a contribué à la création des forces de mission cybernétique dans le cadre du US Cyber Command. Après avoir terminé son service, il a travaillé avec Army Cyber et quelques startups avant de venir chez Censys.\n\nUne évaluation typique des vulnérabilités nécessite de la part d'une organisation ce qui les intéresse, disons une liste de blocage d'adresses IP pour une analyse des vulnérabilités liées aux identifiants. Ensuite, ces vulnérabilités sont découvertes et peuvent être corrigées. Un scan Censys peut fournir une analyse étendue et approfondie d'Internet à partir d'informations de base telles qu'un domaine et quelques adresses IP, découvrant tout ce que possède l'organisation et surveillant en continu les mêmes agrégations et corrélations entre les ensembles de données. Par exemple, si le département marketing met en place un site web exposé sans TLS, Censys le détectera. Censys adopte une perspective externe pour savoir ce qui est accessible à un attaquant à n'importe quel moment de la journée.\n\nMême si, par exemple, quelqu'un déploie une application dans le cloud, ne la place pas sur le domaine et utilise une sécurité d'obfuscation, Censys le détectera toujours s'il se trouve dans un environnement cloud appartenant à l'organisation. Les connecteurs cloud découvriront de nouvelles instances cloud qui apparaissent. Une certaine connexion, que ce soit des informations WHOIS ou DNS, doit corréler à certaines de ces instances. Censys continue à s'améliorer dans la détection de ces types d'instances.\n\nL'analogie d'un château fonctionne bien ici. Une organisation ne veut pas seulement se fier à ce qu'elle peut voir en interne grâce aux caméras et sentinelles. Elle souhaite une patrouille de sécurité mobile pour repérer les menaces avant même qu'elles n'atteignent les murs du château. La patrouille peut observer le château de la même manière qu'un attaquant le ferait. Censys patrouille l'internet d'un point de vue externe, voyant les choses comme le ferait un potentiel pirate informatique.\n\nUn outil que Censys étudie est JARM de Salesforce, un outil de reconnaissance active des empreintes TLS. Lorsque des éléments sont déployés qui ne correspondent pas aux empreintes sur un serveur spécifique, ils se démarqueront en tant qu'anomalies. Il est essentiel de rechercher des architectures qui devraient être dans une configuration spécifique mais ne le sont pas.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matt Lembright"],"link":"/episode-EDT92-fr","image":"./episodes/edt-92/fr/thumbnail.png","lang":"fr","summary":"Dans cet épisode, Darren parle à Matt Lembright, Directeur des Applications Fédérales chez Censys, de la manière dont Censys évalue la surface d'attaque des appareils connectés à Internet, permettant aux organisations d'éliminer les vulnérabilités."},{"id":167,"type":"Episode","title":"Les quatre superpuissances de la transformation numérique.","tags":["cybersecurity","aiml","multicloud","comms"],"body":"\r\n\r\nIntel se concentre sur l'informatique omniprésente, la connectivité généralisée, le cloud à l'extrémité et l'intelligence artificielle (IA). Les éléments de ces quatre superpuissances sont indispensables dans les stratégies de transformation numérique des entreprises dans le monde entier, donc il existe une grande synergie.\n\n## Calcul omniprésent\n\nL'informatique ubiquitaire est un concept de génie logiciel dans lequel l'informatique est partout. Non seulement cela fait référence aux appareils personnels intelligents tels que les téléphones, les montres et les appareils électroménagers, ou à l'infrastructure des centres de données et aux applications basées sur le cloud, mais c'est aussi l'idée que l'infrastructure d'entreprise qui était historiquement protégée par un pare-feu est maintenant présente dans la vie quotidienne. L'infrastructure d'entreprise est disponible pour les employés sur le campus, ainsi que n'importe où ils ont un appareil. Plus important encore, l'infrastructure peut s'étendre aux clients pour améliorer leur expérience.\n\nIntel examine les éléments fondamentaux qui devraient être promus à travers l'écosystème afin d'atteindre l'objectif que chaque individu sur terre ait accès à un pétaoctet de données ou à un pétaflop de calcul en moins d'une milliseconde. Intel travaille sur les lois de la physique, de l'économie et de la souveraineté des données pour rendre cela possible. Cela fournira aux entreprises de logiciels, aux entreprises de cloud et à toutes les entreprises du monde entier un cadre pour les logiciels qui créeront de la valeur pour leurs clients.\n\n## Connectivité omniprésente\n\nDans une récente interview, Glenn Kurowski, le directeur technique de CACI, a parlé de connectivité et de comment il la voit se répandre dans le domaine spatial. Un excellent exemple est lorsque Elon Musk a fait voler Starlink au-dessus de l'Ukraine pour les empêcher de sombrer dans l'obscurité d'Internet.\n\nAvec cette puissance de calcul immense, la connectivité est cruciale. Il y a la connectivité par satellite, la 5G et la 4G. Cependant, même au sein d'Internet, entre les états et les pays, la question est : comment traversez-vous les réseaux et permettez-vous des ensembles de calcul non interconnectés ? Sans connectivité, la valeur du calcul diminue rapidement.\n\nPour protéger ces données interconnectées, Intel et des centaines d'entreprises technologiques collaborent et innovent ensemble. Les gouvernements doivent aussi comprendre les lois, les règles et les préoccupations. Par exemple, Intel emploie une force considérable pour aider les législateurs à avoir ces conversations et à créer des lois qui protègent les données. L'écosystème d'Intel est vaste, et les gens font attention lorsque Intel affirme que cette sécurité est essentielle.\n\nL'Estonie est un bon exemple de pays qui protège les données de ses citoyens tout en les libérant et en offrant plus de services à moindre coût. Ce modèle évoluera naturellement, à mesure que les gouvernements protègent activement la vie privée et adoptent une approche stratégique.\n\n## Nuage à l'extrémité\n\nDu côté de l'entreprise, il s'agit de l'extension de la surface sur laquelle les entreprises peuvent interagir avec leurs clients via le cloud jusqu'au bord. Un excellent exemple est le commerce omnicanal, où une entreprise connaît l'identité du client, ses tendances et ses besoins. Ils peuvent recommander des services supplémentaires grâce à une connectivité omniprésente du cloud au bord. Ils pourraient indiquer au client où se trouve un article qu'il cherche lorsqu'il arrive en magasin. Les détaillants pourraient étendre leurs réseaux depuis un centre de données dans le cloud ou depuis un centre de données sur site afin de les unifier au bord, créant ainsi un réseau maillé qui parcourt tout le magasin.\n\nNon seulement cela améliorerait l'expérience d'achat, mais les magasins pourraient réduire leurs pertes en détectant les articles qui sont en rupture de stock et en contrôlant le flux à l'intérieur du magasin. Ils pourraient placer leurs produits dans les endroits les plus avantageux et surveiller, par exemple, les produits périssables pour prendre des mesures afin de déplacer rapidement les produits pendant qu'ils sont à leur apogée.\n\n## Intelligence artificielle\n\nL'IA est une extension de l'analyse des données et va inévitablement grandir. Des quantités massives de données sont créées quotidiennement et il est déjà au-delà des capacités des entreprises de les calculer efficacement. Les gens ne considèrent qu'à peine moins de cinq pourcent des données générées.\n\nAvec des algorithmes d'IA, il est possible de trouver des schémas dans ces données pour, par exemple, guérir le cancer. Ces informations pourraient être là, mais l'IA ne peut pas encore les utiliser car elles ne sont pas encore disponibles. Il faudra donc créer un nouveau marché autour de jeux de données centralisés et accessibles. Un courtier en données organisé pourrait rendre des données centralisées disponibles à plusieurs entreprises via le cloud, tout en protégeant la confidentialité des données, telles que l'identification des patients.\n\n## Sécurité des données\n\nLa COVID a rapidement fait de l'informatique omniprésente une nécessité pour les employés travaillant de chez eux. Cela, ainsi que le personnel hybride qui a suivi, a exposé les failles de sécurité de l'industrie. Il y a actuellement une augmentation significative des fonds alloués à la sécurité pour faire face à ces problèmes et suivre l'expansion des grandes puissances. Surtout avec les architectures de l'internet des objets du bord au cloud et l'informatique omniprésente, la surface d'attaque a explosé. L'industrie peut suivre mais cela nécessite des efforts considérables et une pensée avant-gardiste.\n\nIntel a d'importantes innovations dans cet espace avec les extensions de garde logicielle, les fonctionnalités de sécurité intégrées au silicium et l'écosystème permettant de tirer parti de ces éléments. L'écosystème peut créer de nouveaux cas d'utilisation tels que l'analyse distribuée confidentielle pour la recherche sur le cancer ou l'analyse multi-domaines, ce qui signifie qu'elle peut s'appliquer à des données non classifiées, classifiées et hautement confidentielles. Auparavant, ces données ne pouvaient jamais se mélanger. Maintenant, elles peuvent être mélangées en toute sécurité et résoudre des problèmes que nous ne pouvions pas résoudre auparavant.\n\n## Logiciel Intel\n\nBeaucoup pourraient être surpris que Intel compte plus de 19 000 ingénieurs logiciels. Intel parvient à maintenir tous ces ingénieurs pleinement occupés, car ils travaillent à trois niveaux : le logiciel de base ; les langages, frameworks, outils et bibliothèques ; et le travail au niveau des applications.\n\nLa majeure partie de cette technologie au niveau de l'application est offerte dans la communauté open-source, où elle est accessible, sécurisée et optimisée.\n\nAllez sur http://embracingdigital.org pour les ressources Intel liées aux quatre superpuissances.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Ernst"],"link":"/episode-EDT93-fr","image":"./episodes/edt-93/fr/thumbnail.jpg","lang":"fr","summary":"Dans cet épisode, Darren discute des quatre superpuissances de la transformation numérique avec Greg Ernst d'Intel, vice-président des ventes pour les Amériques. Calcul omniprésent, Connectivité généralisée, Du cloud aux périphéries, Intelligence artificielle."},{"id":168,"type":"Episode","title":"La Résurgence du Cloud Privé","tags":["hybridcloud","microcloud","privatecloud","compute","technology","cloud","vergeio"],"body":"\r\n\r\nGreg a commencé à coder sur le Commodore 64 quand il était enfant et n'a jamais arrêté. Il a travaillé sur tout, des moteurs de jeux en 3D aux communications en passant par les moteurs de bases de données, les serveurs web et le chiffrement. Il a développé des logiciels de communication interopérables et des solutions matérielles pour les premiers intervenants après que des problèmes soient survenus lors du 11 septembre. Par exemple, la police ne pouvait pas communiquer avec les pompiers car chacun était sur des systèmes différents.\n\nAprès avoir quitté avec succès cette entreprise, il a créé une nouvelle entreprise pour construire un moteur de recherche vertical à partir de zéro. Il a rapidement réalisé que la partie difficile n'était pas le moteur de recherche ou les algorithmes, mais les problèmes d'infrastructure. Il a constaté que trop de code était spécifique au matériel et qu'il manquait une couche d'abstraction. C'était la base de ce qui est devenu Yottabyte, sa société qui a commencé par résoudre le problème de stockage en rassemblant des disques bon marché et en le présentant comme une plateforme simplifiée sur laquelle vous pouviez écrire des logiciels sans vous soucier de ce qui se trouvait sous le capot. Sa vision s'est ensuite étendue à l'informatique, la mémoire et les réseaux. Yottabyte est récemment devenu Verge.io.\n\nLe succès de Verge.io réside dans le fait que tout a été développé à partir de zéro, même la conception de leur langage de programmation et l'écriture de leurs moteurs de base de données, de sorte que le logiciel puisse tout accomplir. Il n'est pas lié à un matériel spécifique. L'objectif était la simplicité : ce n'est pas seulement un amas de pièces assemblées pour former un cloud privé unique, mais un centre de données virtualisé complet.\n\nCela offre fiabilité et sécurité, notamment grâce à des mises à jour transparentes, aucune dépendance vis-à-vis du matériel spécifique et moins de points d'entrée vulnérables. De plus, les utilisateurs ciblés du système sont des généralistes en informatique, pas des experts en stockage SAN ou des programmeurs.\n\nLa plateforme Verge.io offre la même expérience qu'un client pourrait obtenir d'un nuage public : simplicité, libre-service et agilité, mais elle a un coût considérable et un avantage lié à la gravité des données. Vous payez des choses comme l'IOP ou la sortie de données avec un nuage public. De nombreux petits prix commencent à s'accumuler et vous pouvez devenir responsable vis-à-vis de l'écosystème. En revanche, vous pouvez exécuter le logiciel Verge.io, par exemple, sur des serveurs physiques nus pour économiser des coûts et avoir la possibilité de monter ou de descendre rapidement en puissance. Dans un nuage public, vous renoncez également à beaucoup de contrôle. Avec un nuage privé, vous gardez vos données plus près de l'endroit où elles sont générées et pouvez fonctionner efficacement en périphérie.\n\nUne part de la raison pour laquelle les fournisseurs de services choisissent Verge.io est qu'ils peuvent gérer les charges de travail de leurs clients sans renoncer à leur marge bénéficiaire et sans réduire les complications liées à leur migration vers le cloud public.\n\nLes besoins d'autres clients sont satisfaits car ils peuvent accomplir des choses très difficiles à réaliser avec d'autres logiciels. Par exemple, l'Université du Michigan a constaté une amélioration considérable de l'efficacité et de la facilité d'utilisation. Ils ont des milliers de chercheurs et, lorsqu'ils reçoivent des fonds de subvention, ils ont besoin d'un environnement conforme à la norme HIPAA ou CUI. Auparavant, chaque demande nécessitait de six à neuf mois pour déployer, installer et certifier le matériel. Verge.io a créé un environnement et sa mise en route est aussi simple que d'appuyer sur un bouton. L'utilisateur se voit attribuer une enclave virtuelle entièrement conforme et très sécurisée car l'enclave est encapsulée et isolée.\n\nUn autre avantage est que la fonction de snapshot de Verge.io intègre la continuité des activités et la récupération après sinistre. Vous pouvez prendre le snapshot et le transférer vers une architecture matérielle totalement différente, et il fonctionnera de la même manière. La façon dont fonctionne le snapshot permet de créer une copie clonée, même si elle compte dix pétaoctets, en moins de 30 millisecondes.\n\nParce que le centre de données peut maintenant facilement migrer vers des clouds ou un centre de colocation, les propriétaires d'entreprise ont beaucoup plus de flexibilité pour négocier le prix et les performances du matériel. De plus, il n'y a aucun temps d'arrêt pour les mises à niveau matérielles ou les cycles de rafraîchissement. Le système ne s'arrête jamais.\n\nL'une des zones de développement pour Verge.io consiste à étendre la construction d'un logiciel d'agrégation multi-cloud offrant des avantages tels qu'un panneau de gestion centralisé.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Campbell"],"link":"/episode-EDT94-fr","image":"./episodes/edt-94/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions chez Intel, discute des centres de données définis par logiciel de Verge.io qui simplifient l'informatique et rendent le cloud privé facile et efficace avec le directeur technique de Verge.io, Greg Campbell."},{"id":169,"type":"Episode","title":"Exigences pour les architectures Edge vers Cloud","tags":["data","edge","compute","cybersecurity"],"body":"\r\n\r\nAnna a récemment dirigé un effort visant à rédiger un livre blanc sur les architectures de service de bord à cloud. De la bordure au cloud englobe tous les points forts de la technologie qui concernent Intel : l'IA, la bordure, le cloud et la connectivité réseau. Le but était de rassembler des experts dans ces domaines pour discuter de la manière dont Intel aborde les architectures de cloud de bord et de la façon dont ces architectures peuvent toutes se connecter au cloud. L'accent était mis sur les éléments importants plutôt que sur la technologie actuelle qui résout les problèmes. Cela fournirait un cadre pour discuter et rechercher les désaccords. Une révélation intéressante était que les professionnels de l'informatique, de l'automatisation industrielle et des réseaux parlent des langages techniques différents avec des taxonomies différentes, entre autres défis.\n\nUne leçon tirée de ces discussions est que les communications sont d'une importance vitale, et il est crucial de bien connaître vos communications, surtout en périphérie. Cela détermine la quantité de calculs qui doivent être effectués localement et à quelle fréquence vous pouvez compter sur le cloud. Dans le secteur public, la complexité supplémentaire des cas d'utilisation doit maintenir la fonctionnalité même en cas de conditions de refus, de perturbation, d'interruption et de limitation (DDIL). Nous devons repenser ces architectures de connectivité cloud en fonction de ces limitations.\n\nLa communication et la connectivité constituent la différence la plus significative entre une architecture de cloud, une infrastructure de services et l'edge. De nombreux outils actuels supposent à tort une connectivité constante ; si quelque chose n'est pas connecté, c'est mort. Ce n'est pas le cas pour l'edge. C'est évident dans des organisations edge telles que l'armée. Pourtant, même dans des industries telles que la télémédecine, il faut supposer qu'il n'y aura pas de bonne connectivité, par exemple, dans le cas d'utilisation de la télémédecine où vous pourriez dépendre du WiFi domestique du patient. L'industrie a des exigences similaires. Certains ne peuvent pas être hors ligne en raison des contrôles critiques pour des machines ou des processus spécifiques.\n\nAlors, comment pouvez-vous avoir une informatique centrée sur l'edge qui maintient toutes les fonctionnalités critiques avec une connectivité intermittente vers le cloud ? Il existe des architectures pour cela, mais il reste encore beaucoup à faire pour avoir des opérations sans interruption lorsque la connectivité n'est pas continue. Que pouvez-vous continuer à faire et que se passe-t-il lorsque tout est restauré ? Il y aura une déconnexion avec ce qui s'est passé avec les données. Cela devient complexe lorsque vous devez synchroniser toutes ces données à grande échelle, peut-être avec des milliers d'appareils edge.\n\n## Sécurité\n\nLa sécurité en périphérie est un autre domaine où il y a toujours plus de travail à faire. Les mesures de sécurité traditionnelles telles que l'authentification restent d'une importance capitale, mais les appareils constituent une surface d'attaque considérable, et leur sécurité physique est une question différente. La sécurité des ordinateurs portables tend à être solide, et ces mesures doivent être appliquées aux autres appareils en périphérie. Les nouvelles avancées de l'intelligence artificielle aideront à déterminer si les appareils sont à l'endroit où ils devraient être et à détecter les anomalies parmi, disons, dix mille appareils.\n\n## Développement d'application\n\nDans la création d'applications, les développeurs doivent comprendre l'environnement unique de l'edge et les développer sans avoir besoin de reprogrammer ou d'introduire de nouveaux intergiciels pour fonctionner en edge. Les applications doivent être capables de s'exécuter avec des limitations en termes de calcul, d'énergie et de connectivité. De plus, l'edge peut être dans une nouvelle configuration de calcul distribué, et l'application doit être conçue pour fonctionner sur un réseau maillé avec une distribution de calcul très élevée. Le détachement de l'application du matériel représente un changement important, devenant plus générique et moins spécifiquement conçu. Cependant, l'ensemble du dialogue se dirige vers l'obtention des données de n'importe où et leur utilisation n'importe où.\n\n## Gestion des données\n\nLe volume de données générées et collectées en périphérie est si énorme qu'il n'est pas logique d'envoyer toutes ces données vers un centre de données pour être traitées. Une des raisons est le coût. Aux États-Unis, un réseau privé 5G peut être rentable pour de telles quantités de données, mais le coût serait prohibitif pour la plupart des organisations sans réseau 5G privé.\n\nL'autre problème est que la plupart des données sont inutiles. Par exemple, lors de la surveillance des appareils ou applications, la plupart des données indiquent que tout fonctionne correctement alors que vous vous souciez uniquement des événements suggérant que quelque chose ne va pas. Des algorithmes d'IA sont appliqués en périphérie, réduisant la quantité de données non pertinentes renvoyées vers le cloud pour traitement.\n\nLe mode de fonctionnement traditionnel pour la gestion des données, consistant à tout copier vers le centre de données et à exécuter les analyses là-bas, ne fonctionne pas bien pour le périphérique. Pousser les applications vers le périphérique ne fonctionne pas toujours non plus. Intel a identifié quelques autres architectures de données ou d'opérations de données. L'une d'entre elles s'appelle l'échange de données, où il y a une combinaison de déplacement des données dans des enclaves sécurisées seulement après leur analyse en périphérie, comme dans le traitement par lots. L'autre s'appelle les flux de données intelligents, où SADE et SABR entrent en jeu. Les données sont déplacées uniquement en fonction de règles, et elles sont diffusées en continu. Cela fonctionne dans les environnements DDIL car il peut déterminer les environnements d'exploitation actuels et s'adapter.\n\n## Gestionabilité\n\nLes systèmes doivent être conçus de manière à pouvoir être entretenus. On ne peut pas déployer dix mille appareils, puis envoyer régulièrement une petite armée de personnes pour les vérifier. Les services informatiques ont traditionnellement été prudents en ce qui concerne la mise à niveau d'un système fonctionnel. Pourtant, il est illogique de laisser les systèmes tels quels, surtout compte tenu des craintes d'attaques de ransomware sur les réseaux OT. Les systèmes doivent être conçus de manière à permettre une mise à jour facile de l'ensemble pour faire face à l'environnement de sécurité de manière robuste.\n\n## Disponibilité\n\nSurtout dans des domaines critiques tels que l'armée ou les soins de santé, il est important de concevoir des systèmes avec une redondance suffisante ; c'est davantage une approche systémique. Si les composants individuels échouent, l'objectif final doit quand même être atteint. C'est très différent de ce qui se passe dans le cloud, où il s'agit de maintenir l'infrastructure opérationnelle.\n\nLa technologie n'est pas encore tout à fait prête, mais elle est prise en compte pour être conçue pour plusieurs réseaux. Par exemple, si vous privilégiez l'utilisation du WiFi 6 et que cela échoue, le système peut utiliser le 4G ou un autre réseau disponible. Le réseau doit être solide tout comme le calcul. Une opération autonome sans le côté réseau est fragile. Si vous optez pour des coûts avec connexion filaire, vous avez des limitations plus élevées et vous perdez vos applications mobiles.\n\nVous pouvez trouver le livre blanc \"Exigences essentielles pour les architectures de services Edge to Cloud\" pour plus d'informations sur embracingdigital.org ou intel.org.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Anna Scott"],"link":"/episode-EDT95-fr","image":"./episodes/edt-95/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte en chef des solutions, et le Dr Anna Scott, architecte en chef Edge, secteur public, discutent des exigences essentielles pour les architectures de services du bord au cloud d'Intel."},{"id":170,"type":"Episode","title":"Exploiter les centres de données virtuels avec Verge.io","tags":["privatecloud","cloud","compute","technology","sdi","virtualdatacenter","vergeio"],"body":"\r\n\r\nAaron travaille dans le domaine de l'informatique depuis plus de 20 ans. Il est ingénieur prévente, ce qui signifie qu'il discute avec les clients de la valeur et des fonctionnalités de Verge.io et les aide à définir leurs besoins.\n\nChris est le responsable des ventes chez Verge.io avec 25 ans d'expérience dans les sociétés de logiciels technologiques. Il décrit Verge.io comme ayant une concentration maniaque sur la satisfaction des clients et le succès avec le logiciel.\n\nLe terme \"centre de données virtuel\" est utilisé pour simplifier. La plateforme est multi-locataire, donc il y a des locataires autonomes imbriqués avec toutes les ressources requises à l'intérieur d'un centre de données, de l'unité centrale à la mémoire en passant par le stockage et le réseau, et tout cela est isolé. Cependant, en même temps, vous pouvez construire ceux-ci à la demande. Tout comme vous pouvez créer une machine virtuelle à la demande, souvent avec un modèle, vous pouvez faire la même chose avec un locataire où tout est encapsulé dans le centre de données virtuel. Vous pouvez construire à partir d'un moteur de recettes ou à partir de rien. Vous pouvez facilement cloner ou prendre une capture instantanée d'un de ces locataires.\n\nCes centres de données virtuels peuvent être grands et complexes. Le plus grand client de Verge.io gère plus de 60 nœuds et plusieurs locataires dans son environnement. Les seules restrictions sont celles intégrées dans le cluster Verge.io en coulisses. Par exemple, supposons que vous disposiez de quatre téraoctets de mémoire, de 100 téraoctets de stockage et de 64 cœurs dans votre processeur. Vous pourriez attribuer toutes ces ressources à ce locataire, le diviser comme vous le souhaitez, en moitiés ou en tiers, et construire les locataires de cette manière.\n\nSi, par exemple, vous avez attribué certains nœuds au développement, certains à des tests et le reste à la production, ils seraient différents locataires et vous pourriez effectuer une capture d'écran entre eux et passer en production. Dans ce cas, vous pouvez même avoir plusieurs environnements de production avec des mises à jour en mode bleu-vert.\n\nBeaucoup des clients de Verge.io sont des fournisseurs de services gérés. Ils utilisent la multi-tenancy pour leurs clients finaux et, avec cela, dédient des environnements sécurisés de confiance zéro. Les clients peuvent avoir leur environnement de cloud, et ils peuvent provisionner des charges de travail virtuelles selon leurs besoins.\n\nIl existe également des cas d'utilisation importants pour les clients de type informatique d'entreprise qui souhaitent, par exemple, des environnements de développement et de test en mode bleu-vert ou qui peuvent avoir différents environnements avec différentes exigences de conformité en matière de sécurité telles que SOX ou HIPAA. Verge.io compte plusieurs clients issus d'institutions d'enseignement supérieur qui mènent des recherches conformes. Ils certifient leur grappe une seule fois; ensuite, ils peuvent fournir un environnement de recherche conforme à l'un de leurs chercheurs en moins d'une heure. Auparavant, cela pouvait prendre des mois pour être opérationnel dans un environnement conforme.\n\nPour les mises à jour, bien que les utilisateurs doivent utiliser des outils au niveau de l'application ou à l'intérieur des charges de travail de la machine virtuelle, la \"recette\" de l'environnement pour des éléments tels que les règles du pare-feu ou les paramètres de configuration sur la manière dont les ressources sont attribuées aux charges de travail peuvent être mis à jour de manière transparente. Vous pouvez également prendre une machine virtuelle dans un environnement en cours d'exécution avec une nouvelle conformité et la transférer vers un nouvel environnement.\n\nVerge.io est également utile pour la sécurité. Une des plus grandes sociétés de quant en Europe constitue un bon exemple d'utilisation en matière de sécurité. Ils prennent une photo de tout leur environnement, puis réalisent des exercices d'équipe rouge et bleue contre celle-ci, cherchant des vulnérabilités de sécurité, vérifiant les correctifs, etc.\n\nVDI peut fonctionner dans l'environnement. Verge.io collabore avec une entreprise pour le support VDI. Verge.io contrôle les ressources, le CPU et la mémoire. Ils prennent également en charge le GPU, le GPU passthrough et le GPU physique. C'est un cas d'utilisation important pour certains clients, notamment dans les travaux d'ingénierie ou les charges de travail pétrolières et gazières. Le GPU virtuel permet une économie favorable car les coûts sont répartis entre plusieurs utilisateurs.\n\nUn ajustement parfait pour Verge.io serait les cas d'utilisation de bord. Un exemple typique est le point de vente. Si un client de vente au détail possède une centaine de magasins, il peut avoir besoin de deux ou trois applications de machines virtuelles (VM) dans chaque magasin. Étant donné que Verge.io a une empreinte matérielle réduite, une fois que vous avez au moins deux serveurs, vous pouvez les placer dans le centre de données du cas d'utilisation de bord et créer ces VM. Ensuite, avec les fonctionnalités de capture instantanée et de réplication, ces configurations peuvent être copiées et collées dans tous les environnements différents. Vous pouvez mettre à jour les dernières configurations sur tous, pas seulement les correctifs du système d'exploitation, mais aussi les règles de pare-feu.\n\nUn excitant futur domaine pour Verge.io réside dans les systèmes de conduite automatisée en raison du volume de données. De nombreux fournisseurs testent les véhicules sur des sites éloignés et expédient physiquement les disques durs. Imaginez si les données pouvaient être traitées sur place, entièrement redondantes, avec les coûts attrayants qui y sont associés, puis transportées dans une zone étendue plutôt que sur un disque et un camion.\n\nPour plus d'informations sur Verge.io, visitez http://verge.io.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Aaron Reid","Chris Lehman"],"link":"/episode-EDT96-fr","image":"./episodes/edt-96/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions chez Intel, et Aaron Reid, ingénieur principal des systèmes chez https://www.verge.io/, ainsi que Chris Lehman, vice-président principal des ventes, discutent des cas d'utilisation du logiciel de centre de données virtuel de Verge.io."},{"id":171,"type":"Episode","title":"Les avantages des bases de données graphiques","tags":["data","graphdb","katanagraph"],"body":"\r\n\r\nHadi a obtenu son doctorat en informatique en 2012 et a effectué des recherches en cryptographie et en sécurité des informations réseau. Il a travaillé dans le milieu universitaire pendant quelques années, puis est passé à l'industrie en se concentrant sur différents aspects des solutions de sécurité, notamment la gestion des identités et des accès. En 2015, il a commencé à en apprendre davantage sur la modélisation de graphes et a réalisé comment la modélisation de données par les graphes pouvait résoudre certains des problèmes passionnants et complexes dans son domaine d'étude.\n\nDans les bases de données graphiques, un graphe ne signifie pas des tableaux ou des interfaces graphiques, mais une façon de structurer les données au niveau du stockage afin qu'elles puissent être récupérées et traitées pour résoudre des problèmes complexes, surtout si les données sont interconnectées. Le graphe offre de nombreux avantages et peut compléter les structures de données ou les solutions existantes, telles que les modèles de bases de données relationnelles ou le stockage d'objets.\n\nLa principale différence entre les bases de données graphiques et relationnelles est que, bien que les deux concernent les relations, les bases de données relationnelles déplacent les relations vers le niveau métadonnées et schéma, tandis que les bases de données graphiques se basent sur les relations basées sur les données. En d'autres termes, dans les bases de données relationnelles, vous reliez les colonnes des tables. Pour introduire une nouvelle relation, vous devez modifier votre schéma. Le graphique offre une infrastructure sans schéma où vous pouvez ajouter plus de structure à vos données tout en restant flexible pour pouvoir ingérer n'importe quelle donnée non structurée.\n\nLa moitié des données mondiales a été créée au cours des dernières années, collectées à partir de nombreuses sources différentes, mais moins de deux pour cent ont été analysées, dont la plupart sont des données structurées. Les données sont collectées, mais les informations sont insuffisantes pour être traitées. Il doit exister un moyen d'ajouter de manière flexible une structure dynamique qui puisse changer si vous êtes incertain mais bénéficier quand même du calcul optimisé avancé. Le graphique est un excellent moyen de le faire.\n\nSi vous essayez de travailler avec des données corrélées ou interconnectées, par opposition aux données isolées avec des valeurs critiques, un graphique offrira des avantages en raison des relations. Presque toutes les industries peuvent en bénéficier car les données non structurées proviennent généralement de diverses sources et de natures multiples.\n\nUn exemple serait les solutions de cybersécurité. Il y a des données provenant des journaux et des trains d'audit des environnements réseau, de l'infrastructure cloud, des hôtes de terminaux, etc. Les données proviennent de différentes sources, telles que des répertoires ou des fichiers journaux bruts. Il serait bénéfique de corréler les données, par exemple, car généralement, une identité ou un utilisateur qui pourrait faire partie d'un journal du système de gestion des identités pourrait être le même utilisateur qui déclenche un processus sur un ordinateur portable, tel que le téléchargement d'une pièce jointe à partir d'un e-mail. En analysant ces patterns, vous pouvez utiliser ce lien de corrélation pour obtenir plus d'informations. En d'autres termes, peu importe comment ou d'où proviennent les données, mais fournir ce lien conduit à apprendre sur chaque enregistrement en les examinant dans leur contexte.\n\nUn avantage est qu'il n'y a pas de problème de transformation des données, ce qui augmente la vitesse. Cela réduit également les exigences de stockage.\n\nLes graphes et les structures de base de données relationnelles sont généralement compatibles. Voici un exemple simpliste de réseau social : Les types de nœuds dans le graphe ressemblent à des tables. Ainsi, vous pouvez avoir un nœud appelé \"personne\" et ensuite une table appelée \"personne\". Vous pouvez avoir un autre nœud appelé \"lieu\" et connecter la personne à un lieu spécifique. Vous pouvez avoir une table appelée \"lieu\" et les connecter à des clés étrangères. Ensuite, vous avez cette relation d'amis. Un ami d'une personne dans un modèle de grappe est simplement une boucle de rétroaction. Cela vous permettrait de modéliser ce schéma. Dans une base de données relationnelle, vous devriez créer une nouvelle table appelée \"amis\" et ensuite la connecter. Ainsi, même au niveau du schéma, vous ajoutez de la redondance et une certaine structure par-dessus. Et si vous avez besoin d'ajouter un nouveau concept d'amitié ou de relation, vous devez créer de nouvelles tables, créant ainsi de la redondance et de la complexité.\n\nD'autres avantages des graphiques par rapport aux bases de données relationnelles sont les graphiques basés sur des éléments, l'IA graphique et l'idée de modéliser maintenant les données pour trouver des motifs en fonction de la façon dont les données sont connectées. Vous pouvez réduire l'ensemble de données que vous recherchez ou analysez en raison des relations. Cela utilise la puissance des données pour renforcer encore davantage les données. Les algorithmes dans une base de données graphique sont très différents de ceux des bases de données relationnelles et sont mieux optimisés pour accéder plus rapidement à de grands ensembles de données.\n\nL'un des inconvénients d'une base de données graphes est qu'elle est difficile à mettre à l'échelle. Dans une base de données relationnelle, il est facile de diviser une table et de la répartir sur deux serveurs, par exemple. Les premières bases de données graphes ont été conçues comme une solution unique en son genre, donc si vous vouliez accroître l'échelle, vous devriez ajouter davantage de mémoire et de CPU.\n\nMaintenant, si vous souhaitez travailler avec des pétaoctets de données sous forme de graphiques, vous voulez mettre à l'échelle verticalement autant que possible avec des technologies telles que les machines virtuelles Intel, mais vous voulez également mettre à l'échelle horizontalement. De nouvelles technologies, comme la plateforme graphique de Katana, aident à résoudre ce problème d'échelle grâce au calcul distribué. Vous pouvez diviser ou découper le problème en morceaux et faire travailler chacun sur une petite partie du graphique pour obtenir une solution finale. Katana a démontré que vous pouviez utiliser 256 machines ou plus pour traiter les données, vous permettant ainsi d'obtenir rapidement des dizaines de téraoctets de données en mémoire.\n\nUne base de données à graphes nécessite un écosystème similaire à celui d'une base de données relationnelle. Le graphe est un peu plus dynamique et flexible. Si vous souhaitez passer à une plateforme d'analyse de graphe, qui va au-delà des simples bases de données opérationnelles, vous pouvez profiter d'autres fonctionnalités telles que le data warehousing et les capacités de data lake. Le stockage et le calcul seraient séparés, ce qui signifie que les technologies de traitement de graphes qui effectuent toutes les opérations en mémoire n'ont pas besoin de dépendre du stockage attaché aux services, vous permettant ainsi d'avoir un service de stockage différent.\n\nKatana utilise le stockage d'objets, puis lorsqu'ils veulent effectuer des calculs, ils chargent de manière opportuniste ce qu'ils veulent du graphe dans la mémoire distribuée de toutes les machines. Les données retournent de manière immuable vers le stockage, donc si, par exemple, vous détruisez l'ensemble du cluster, vous ne perdez rien. Toutes les données sont déjà présentes et entreposées. Les bases de données relationnelles sont un domaine plus mature, mais les bases de données graphiques bénéficient d'un soutien croissant dans l'écosystème.\n\nSur la base des huit années d'expérience de Hadi sur le terrain à parler aux parties prenantes et aux clients, tous voient immédiatement les avantages des bases de données graphiques. Les limitations pourraient être qu'elles ne peuvent pas suivre l'évolutivité ou les dépenses. Le travail de Katana et d'autres sociétés de technologie graphique vise à faire des graphiques un outil plus courant que les clients peuvent utiliser pour diverses tâches et moins un luxe dans la base de données. Par exemple, Katana fournit aux clients des solutions de gestion des identités basées sur les graphiques et de gestion des données massives.\n\nDe bons cas d'utilisation des graphes seraient la création précoce de solutions de graphes spécifiques à certaines entreprises telles que LinkedIn et Facebook, qui possèdent leurs graphes sociaux. Aujourd'hui, une application naturelle se trouve dans le domaine du commerce électronique pour les moteurs de recommandation. Trouver des liens entre les clients, les comptes, les achats et d'autres comportements permettra de proposer immédiatement de meilleures recommandations aux acheteurs, d'une manière qui ne peut être réalisée avec des requêtes de base de données relationnelle.\n\nPour en savoir plus sur Katana ou comment contacter Hadi, rendez-vous sur embracingdigital.org.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Madi Ahmadi"],"link":"/episode-EDT97-fr","image":"./episodes/edt-97/fr/thumbnail.jpg","lang":"fr","summary":"Darren Pulsipher, Architecte principal des solutions, Intel, et Dr. Hadi Ahmadi, Directeur de l'architecture des solutions chez Katana Graph, discutent des avantages des bases de données graphiques."},{"id":172,"type":"Episode","title":"L'Émergence du Réseau de Données Mondial","tags":["dataarchitecture","datamanagement","data","technology","cloud","globaldatanetwork","macrometa","multicloud","datamesh"],"body":"\r\n\r\nChetan est un ingénieur devenu spécialiste des opérations et des start-up (Macrometa étant sa quatrième start-up). Il affirme qu'il travaille sur le même problème de gestion des données distribuées et de réduction de la latence depuis vingt ans.\n\nLes données ne sont plus seulement dans un centre de données mais partout : dans le cloud, en périphérie et sur les ordinateurs portables des gens. Gérer efficacement tout cela est un défi.\n\nIl y a environ dix ans, Marc Andreessen a déclaré que le logiciel dévore le monde. À ce stade, le logiciel a tout dévoré et a transformé toutes sortes de contraintes et de barrières en opportunités. L'informatique multitâche est l'une des barrières qui s'est effondrée avec le cloud. Vous pouvez créer des applications qui s'exécutent simultanément dans différentes parties du monde. Un mouvement de développeurs se déroule en parallèle pour rendre tout aussi simple qu'il doit l'être pour la personne moyenne en informatique. Donc d'un côté, il y a une évolution technologique sophistiquée et de l'autre côté, un mouvement de simplicité.\n\nLes architectures telles que Jamstack permettent à l'informatique distribuée de se produire à grande échelle avec une grande simplicité, mais il reste encore une vaste frontière à découvrir et à conquérir. L'opportunité d'une ruée vers des terres étendues se trouve désormais au bord. La gestion des données distribuées et le bord (ou l'edge) sont deux aspects d'une même réalité.\n\nUn gros problème est que certains développements de logiciels deviennent des fonctionnalités en tant que service qui ignorent les données. De plus, il y a une perception selon laquelle les données sont omniprésentes, mais une grande partie de la périphérie n'est pas toujours connectée. Il n'y a aucune garantie qu'une application ait accès à toutes les données. Les réseaux ne sont plus centralisés ; la notion de microservices sans état est venue du mouvement Cloud. Ce caractère sans état peut devenir un énorme obstacle. C'est pourquoi des architectures telles que Jamstack et les fonctions sans serveur considèrent les données comme un problème périphérique plutôt que fondamental.\n\nLes structures de données sans état sont simples. Vous avez un emplacement spécifique où vous enregistrez vos données, puis revenez à l'état sans état. Les structures de données étatiques nécessitent des infrastructures solides avec des structures de données plus complexes car elles prennent en charge l'application pendant qu'elle émet continuellement un état. Alors que nous progressons vers un monde de données en streaming en temps réel où l'état est constamment émis quelque part dans l'écosystème, l'infrastructure devient complexe et difficile à gérer car elle n'est pas conçue. C'est là que Macrometa intervient. Ils ont créé une nouvelle plateforme pour cet état actif continu en temps réel à l'échelle des exaoctets.\n\nTraiter ces données en streaming de manière active et dynamique représente un changement significatif pour de nombreux développeurs de logiciels. Depuis l'apparition des premières infrastructures cloud, puis des plateformes de big data, puis du data-as-a-service, l'industrie s'est efforcée d'ingérer, de traiter et d'analyser efficacement les données historiques. Mais nous vivons désormais dans un monde où les données sont sur un spectre plutôt que d'exister en tant que monolithe. Une qualité nouvellement appréciée est que les données ont des connaissances et une valeur périssables. Certaines données ont une durée de vie limitée. Les échelles de temps actuelles sont trop grandes pour utiliser les données de manière efficiente ; nous avons besoin de systèmes capables de communiquer efficacement en moins de 50 millisecondes et de réduire la surcharge cognitive pour les personnes qui interagissent avec ces systèmes.\n\nLa plupart des gens comprennent mal la latence : ce n'est pas quelque chose qui vous apporte de la joie, mais son absence vous contrarie. Par exemple, combien de temps une personne tolérera-t-elle une vidéo YouTube saccadée ou une diffusion lente sur Netflix ? Cinquante millisecondes pour une machine, c'est une éternité. Une machine peut accomplir un grand nombre de tâches en 50 millisecondes, donc la latence devient essentielle, surtout lorsqu'on considère la valeur périssable des données.\n\nUn autre problème réside désormais dans le fait que, grâce au cloud, à l'interconnectivité et au système mondial, les start-ups deviennent des entreprises multinationales et les données deviennent sensibles à leur emplacement. Certaines données sont réglementées, certaines sont des données à caractère personnel qui ne peuvent pas être extraites dans certaines juridictions, etc. Un excellent exemple de ce problème est la réticence des Européens à ce que leurs données quittent leurs frontières, alors que la majeure partie de l'infrastructure cloud et des applications sont construites aux États-Unis.\n\nUn troisième problème est que les données se trouvent à de nombreux endroits ; il existe des limites entre les systèmes, à la fois physiques et logiques. Les données peuvent être essentiellement statiques et rigides, nous avons donc besoin d'une infrastructure qui permettra aux données de se connecter et de circuler en temps réel avec cohérence et garanties d'ordonnancement. Plus important encore, cela crée une fongibilité pour être consommée rapidement de manières diverses.\n\nUn problème supplémentaire est que les données contiennent beaucoup de bruit et il n'est pas logique de transférer des distances intercontinentales, en payant des frais de transfert, uniquement pour en perdre la majeure partie. Les données perdent de la valeur lorsqu'elles arrivent à leur destination. Il y a également un taux de rafraîchissement élevé, donc les systèmes travaillent souvent avec des données obsolètes.\n\nNous avons besoin de nouvelles méthodes pour résoudre ces types de problèmes de données distribuées. Chetan croit que les dix prochaines années appartiendront à ce domaine des sciences des données.\n\nLa première génération de solutions de données distribuées utilisait la transformation opérationnelle. Google Docs en est un excellent exemple. Cependant, la transformation opérationnelle nécessite une centralisation du contrôle, ce qui ne permet pas une bonne mise à l'échelle. Google a trouvé un moyen de mettre à l'échelle, mais cela ne s'applique pas aux développeurs moyens. Il n'y a peut-être que cinq entreprises dans le monde qui le comprennent à cette échelle, et une grande partie de ces connaissances est enfermée dans ces entreprises et leur technologie propriétaire.\n\nMacrometa travaille avec la communauté et le milieu universitaire pour essayer de créer un nouveau corpus de connaissances, bien plus efficient que ces modèles centralisés, de manière totalement distributive.\n\nActuellement, il existe des infrastructures disponibles qui sont excellentes pour résoudre les problèmes historiques de type registre. Ils cherchent à se tourner vers des données en temps réel, mais leurs architectures ne sont pas fondamentalement faites pour cela. Ces nouveaux problèmes liés à la sensibilité temporelle et spatiale des données, à la valeur d'actionnement, aux taux de rafraîchissement, à la gravité des données et aux interférences de données nécessitent une nouvelle approche, une nouvelle infrastructure. Chetan appelle cela un système d'interaction plutôt qu'un système de registre car les systèmes d'interaction sont des réseaux de données, proches de l'endroit où vous créez et consommez des données, qui les filtrent et les enrichissent ensuite en ligne pour les acheminer vers leurs destinataires prévus. C'est une fonction de mise en réseau.\n\nMacrometa a construit des processeurs réseau qui déplacent les données - un réseau mondial de données. Il s'agit d'un système d'API sans serveur où les développeurs consomment simplement des API pour résoudre des problèmes de données actives et opérationnelles en temps réel. Macrometa est un réseau mondial de données dans la topologie d'un CDM, mais avec une plateforme de données similaire à Snowflake qui produit des primitives de données riches pour traiter des valeurs de données actives et opérationnelles en temps réel.\n\nVous pouvez intégrer des outils analytiques dans le réseau mondial de données et déployer l'analytique près de l'endroit où les données sont générées ou nécessaires. Tout comme Amazon a fondamentalement modifié la distribution commerciale grâce à une architecture et des algorithmes de pointe pour maintenir les entrepôts locaux optimisés en vue des expéditions nocturnes, Macrometa a fait de même pour les données. Ils rapprochent les données et les calculs sur ces données, ce qui permet de les exécuter en millisecondes. Cette capacité à créer des boucles d'informations en temps réel est un puissant facilitateur. Par exemple, les petits détaillants peuvent utiliser les stocks de leurs magasins locaux dans leur commerce électronique sans se surendetter pour concurrencer Amazon.\n\nUn excellent cas d'utilisation de la plateforme de Macrometa se trouve dans la cybersécurité. Certains clients remplacent leurs modèles de données centralisés afin de profiter de la latence réduite, leur permettant de bloquer les menaces en temps réel.\n\nLe réseau mondial de données est une couche de transformation entre vos sources et destinataires de données avec les consommateurs et éditeurs. Il est composé de trois éléments technologiques. Le premier est le maillage mondial des données, qui est la couche d'intégration pour les données. Le deuxième est un tissu informatique mondial qui vous permet d'orchestrer les données et la logique métier sous forme de fonctions et de conteneurs à l'échelle mondiale. Le troisième élément est un tissu mondial de confidentialité : comment sécuriser les données et se conformer aux différents régimes et réglementations en matière de données qui affectent la transmission ou le stockage de vos données.\n\nLe maillage mondial des données est une manière d'intégrer rapidement et facilement des données provenant de différents systèmes, qu'ils soient physiques ou logiques. Tout est incorporé et circule avec cohérence et garanties d'ordre. La valeur la plus significative de ce maillage est qu'il rend les données fongibles et consommables en vous permettant de mettre rapidement des API sur les données. Cela peut être fait en quelques heures, comparé aux mois habituellement nécessaires. Le réseau mondial de données est conçu pour des milliards d'événements par seconde, ce qui lui permet de déplacer des données à grande échelle avec une réduction de coûts de 90% par rapport au cloud.\n\nLe tissu informatique mondial apporte une logique métier et une orchestration pour rapprocher le traitement de vos données de leur origine ou de leur consommation. C'est le schéma anti-cloud. Macrometa déplacera de manière chirurgicale et dynamique ces microservices qui doivent se conformer aux réglementations sur les données, par exemple, aux bons endroits pour l'exécution.\n\nLa dernière pièce est la protection des données. Il s'agit d'un problème complexe et les réponses que nous avons aujourd'hui, par exemple, l'ouverture d'un silo séparé pour cette géographie spécifique afin de respecter les particularités à chaque fois que vous lancez une instance sur votre application, ne sont pas bonnes. La plateforme de Macrometa dispose d'un réseau de données qui intègre déjà et fait circuler vos données à travers toutes les frontières, ainsi que des fonctions de calcul et d'ingestion de données sans limites. Maintenant, elle peut créer des frontières logiques et assigner des données à des régions spécifiques pour les protéger. Ils peuvent définir des affinités et des politiques concernant la façon dont les données vivent et se répliquent dans une région, par exemple s'il faut les anonymiser lorsqu'elles sont copiées hors de la région.\n\nLa technologie de Macrometa permet des cas d'utilisation impossibles à réaliser dans le cloud car les infrastructures cloud sont trop éloignées ou trop lentes. Macrometa a mis en place une infrastructure pour résoudre les problèmes de données en temps réel et les transformer en opportunités plutôt qu'en défis. Pour en savoir plus sur Macrometa, rendez-vous sur macrometa.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Chetan Venkatesh"],"link":"/episode-EDT98-fr","image":"./episodes/edt-98/fr/thumbnail.jpeg","lang":"fr","summary":"Dans cet épisode, Darren se remémore avec Chetan Venkatesh, PDG de MacroMeta. Venkatesh a une longue expérience de gestion des données depuis les premiers jours du calcul en grille et a lancé MacroMeta pour faire face à la gestion des données à travers les centres de données, le cloud et les sites dispersés à travers le monde."},{"id":173,"type":"Episode","title":"Chemin d'attaque cybernétique prédictif avec XM Cyber.","tags":["aiml","cybersecurity","xmcyber","technology","process"],"body":"\r\n\r\nPaul est dans le domaine de la sécurité depuis la fin des années 90, commençant par travailler sur des projets pour le Département de la Sécurité Intérieure et le Département de la Défense. En 2005, il a rejoint Fishnet Security en tant qu'ingénieur commercial et il est depuis impliqué dans l'ingénierie commerciale. Il a rejoint XM Cyber pour se concentrer sur la simulation des violations et des attaques.\n\nPlutôt que la cybersécurité traditionnelle, qui implique la détection, la remédiation et la prévention, XM Cyber est prédictive. Une bonne description serait qu'il s'agit d'une simulation précognitive. XM Cyber crée des incidents imaginaires afin de vous fournir un aperçu de la façon dont vos outils pourraient les gérer et comment vous pourriez travailler pour remédier à des points d'obstruction spécifiques. L'idée est de faire tout cela avant les tests d'intrusion. Vous pouvez corriger les problèmes aujourd'hui, en voir l'impact demain, puis augmenter et améliorer continuellement votre sécurité.\n\nGoogle Maps est une bonne analogie pour expliquer comment fonctionne XM Cyber. Lorsque vous souhaitez vous rendre d'un endroit à un autre, Google Maps vous indique tous les itinéraires possibles pour y arriver, en évitant les péages, en choisissant le trajet le plus direct, etc... XM Cyber fait la même chose, mais avec une simulation d'attaque. Par exemple, supposez que vous disposiez d'un compte utilisateur actif compromis dans un annuaire. Dans ce cas, XM Cyber vous montrera les six étapes nécessaires pour atteindre un contrôleur de domaine local et compromettre cette ressource critique. Il vous montrera également toutes les routes possibles entre ces deux points.\n\nUn cas d'utilisation pour cela est que vous pouvez permettre à une équipe rouge d'être super efficace avec ces informations car ils n'ont pas besoin de chercher et d'essayer de faire des découvertes. Cela peut également aider la section bleue, en leur permettant de prioriser les remédiations sur les points d'étranglement. Par exemple, s'il y a 400 chemins d'attaque allant tous vers différentes zones dans votre DMZ, mais que ces 400 chemins semblent tous devoir exploiter cette entité pour que le chemin se réalise, alors vous pouvez résoudre ce seul problème et détruire les 400 chemins. Les équipes bleues peuvent verrouiller ces points d'étranglement qui pourraient permettre à l'attaquant d'agir.\n\nUn domaine que XM Cyber analyse est la gestion des identités, non seulement dans le centre de données, mais aussi dans le cloud. Parfois, les chemins d'attaque peuvent comporter dix étapes, mais neuf de ces étapes navigueront dans le monde des identités. Par exemple, vous pourriez avoir des permissions pour votre compte d'administrateur, et ensuite ce compte d'administrateur pourrait avoir des permissions supplémentaires. Effectuez six ou sept ajustements de différentes permissions ou réinitialisez des mots de passe et déploiez des GPOs. Vous pourriez parcourir neuf étapes depuis un compte utilisateur standard jusqu'à un administrateur de domaine en exploitant l'annuaire actif.\n\nEn plus de l'identité, XM Cyber examine plus d'une centaine d'entités telles que des machines, des compartiments S3 et des clés SSH. Ces différentes entités peuvent être combinées pour créer un chemin d'attaque. Parfois, cela devient très complexe. Par exemple, une attaque pourrait commencer sur site, passer par Azure, profiter d'Intune, puis revenir compromettre une autre machine permettant une transition vers GCP. Une fois que les attaquants sont dans l'environnement de GCP, ils peuvent profiter de la confiance ou des autorisations entre AWS et GCP pour compromettre AWS. XM Cyber examine tous les différents types d'entités dans des environnements disparates et les connecte pour évaluer ces chemins autour de la façon dont chaque entité joue de manière holistique avec le risque pour toutes les autres.\n\nIl existe deux façons dont XM Cyber se engage avec les clients. La première consiste en une découverte de haut niveau pour évaluer l'environnement, mettre en évidence les vulnérabilités et mesurer comment un attaquant peut exposer de nouvelles vulnérabilités pour mettre en danger les actifs critiques. La deuxième consiste en une évaluation ciblée d'un scénario spécifique qui préoccupe le client. Ces engagements ne se limitent pas à une simple analyse statique des entités. Ils sont dynamiques car ils examinent le trafic et d'autres motifs.\n\nUn cas d'utilisation ciblé typique consiste à déterminer si l'OT est l'actif critique ou le point d'intrusion. XM Cyber joue des scénarios tels que si une machine dans les ressources humaines est le point d'intrusion, existe-t-il un risque pour ce contrôleur logique programmable (PLC) situé dans l'environnement SCADA qui contrôle les interrupteurs de pression pouvant couper l'électricité d'une municipalité. C'est un cas d'utilisation réel que XM Cyber peut simuler. Ce type d'information est essentiel dans un monde où l'OT n'est plus isolé mais connecté aux réseaux.\n\nXM Cyber est une solution SaaS plutôt qu'une solution sur site, ce qui leur permet de rester dynamiques et de fournir le meilleur service. Il peut être effrayant de penser qu'un objet dans le cloud possède toutes vos techniques d'attaque. Cependant, XM Cyber fait beaucoup de travail pour garantir que les données sont complètement isolées, conformes à SOC 2, entre autres certifications, et qu'il n'y a pas de multi-locataires. Ils ne collectent pas non plus de données sensibles. Les informations sensibles sont hachées, et seule une partie est envoyée dans le cloud. Ils n'ont pas besoin de posséder de données réelles.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Paul Giorgi"],"link":"/episode-EDT99-fr","image":"./episodes/edt-99/fr/thumbnail.png","lang":"fr","summary":"Darren Pulsipher, architecte principal des solutions chez Intel, et Paul Giorgi, directeur de l'ingénierie des ventes chez XM Cyber, discutent de la façon dont la technologie de XM Cyber peut aider les organisations à découvrir les chemins d'attaque et à réduire les risques."},{"id":174,"type":"News Brief","title":"2023-2-5","tags":["ai","compute","cybersecurity"],"body":"\n\n## Intelligence Artificielle\nSelon Forbes, d'ici 2030, l'IA pourrait potentiellement apporter 13 000 milliards de dollars à l'économie mondiale, soit 16% du PIB actuel du monde.\n\nChatGPT permet aux escrocs de concevoir des e-mails si convaincants qu'ils peuvent obtenir de l'argent des victimes sans même compter sur les logiciels malveillants.\nCertains craignent que le puissant chatbot ne rende beaucoup plus facile la création de logiciels malveillants pour les non-développeurs et deviennent des criminels du cyberespace\nChatGPT Cybersécurité Ingénierie sociale\n\nGoogle a investi près de 400 millions de dollars dans la start-up d'intelligence artificielle Anthropic, qui teste un rival à ChatGPT d'OpenAI. https://interestingengineering.com/innovation/google-invests-anthropic-battle-chatgpt\nGoogle tiendra un événement sur la façon dont il utilise la puissance de l'IA pour repenser la façon dont les gens effectuent des recherches. Un événement de 40 minutes sera diffusé sur YouTube le 8 février à 8h30 HE.\n\nUn outil de détection de l'IA, qui vérifie le contenu écrit de ChatGPT, indique que Macbeth de William Shakespeare a été généré par l'IA. Je suppose que ChatGPT n'a pas été formé en vieil anglais.\nhttps://venturebeat.com/ai/chatgpt-detection-tool-thinks-macbeth-was-generated-by-ai-what-happens-now/ Je suppose qu'il n'a pas été formé en vieil anglais (aka anglais shakespearien).\n\n## Calcul Ubique\nLes revenus du cloud public continuent d'augmenter, avec une croissance prévue de 26% pour atteindre 525 milliards de dollars cette année, selon Statista.\nLe logiciel en tant que service devrait représenter 253 milliards de dollars de ce chiffre d'affaires. Cependant, à mesure que la croissance du cloud se poursuit, les fournisseurs de services de cloud ressentent la douleur, car il y a eu plusieurs interruptions au cours des 12 derniers mois.\nCe problème a été si important que plusieurs sites web ont vu le jour pour surveiller et signaler la disponibilité du cloud public\ncomme cloudharmony.com et thousandeyes.com.\n\nhttps://cloudharmony.com/status\nhttps://www.thousandeyes.com/outages/\nhttps://www.statista.com/outlook/tmo/public-cloud/worldwide\n\nOracle et Red Hat se sont associés pour fournir des instances de machines virtuelles basées sur RHEL dans le cloud d'Oracle fonctionnant sur des processeurs Intel, AMD et Arm. Cette collaboration entre anciens concurrents offre aux clients un guichet unique pour les charges de travail utilisant la suite de produits Oracle et les charges de travail traditionnelles basées sur Linux. Ce mouvement continue de pousser Oracle à la concurrence avec les grands Hyper-scalers tels que Azure, AWS et Google.\n\nhttps://www.networkworld.com/article/3686513/red-hat-enterprise-linux-arrives-in-oracle-s-cloud.html\n\n## Cybersécurité\nLes sites de commerce électronique en Corée du Sud et aux États-Unis sont attaqués par une campagne de malware GuLoader en cours, selon une entreprise de cybersécurité Trellix, qui l'a révélé à la fin du mois dernier. Les cyberattaquants abandonnent leur approche par document Microsoft Word pour le NSIS (Nullsoft Scriptable Install System), un outil open source pour écrire des installeurs sur le système d'exploitation Windows.\nGuLoader Malware Using Malicious NSIS\n\nLes pirates pro-russes utilisent une nouvelle plateforme DDoS-as-a-Service appelée Passion pour cibler des institutions médicales aux États-Unis, au Portugal, en Espagne, en Allemagne, en Pologne, en Finlande, en Norvège, aux Pays-Bas et au Royaume-Uni. Cette offre de service en tant que service est un service d'abonnement qui permet aux clients de sélectionner leurs vecteurs d'attaque, la durée et l'intensité souhaitées.\nRussian Cybercriminals Launch New Passion Attack Platform\n\nL'Agence américaine de cybersécurité et de sécurité des infrastructures (CISA) a annoncé jeudi la publication d'un guide de catégorisation de vulnérabilité spécifique aux parties prenantes (SSVC) qui peut aider les organisations à prioriser le patching de vulnérabilités en utilisant un modèle d'arbre de décision.\nLe système SSVC a été créé en 2019 par la CISA et l'Institut d'ingénierie du logiciel de l'Université Carnegie Mellon (SEI), et un an plus tard, la CISA a développé son propre arbre de décision SSVC personnalisé pour les faiblesses de sécurité pertinentes pour les organisations du gouvernement et les infrastructures critiques.\nCISA Releases Decsiion Tree Model\n\n## Podcast sur l'adoption de la transformation numérique\n\nCette semaine, Darren Pulsipher interviewe chatGPT.\n\n[Lire la suite](https://www.embracingdigital.org/episodes-EDT122)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW1-fr","image":"./briefs/edw-1/fr/thumbnail.png","lang":"fr","summary":"For the week of February 6, 2023. News from around the world of digital transformation in artificial intelligence, cloud computing, and cybersecurity."},{"id":175,"type":"News Brief","title":"2023-4-9","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Intelligence Artificielle\n\nEncore une autre chose dont les parents doivent s'inquiéter. Le clonage vocal IA ! Lors d'un enlèvement virtuel récent par un cyber-malfaiteur, la mère d'une jeune fille de 15 ans a été appelée et usurpée en utilisant la voix de sa fille pour annoncer son enlèvement. Suivi de \"J'ai votre fille\". En y réfléchissant rapidement, la mère a envoyé un message texte à son mari pour vérifier l'emplacement de leur fille qui était en sécurité. Le FBI enquête sur cet incident ainsi que sur plusieurs autres.\n\n[https://www.azfamily.com/2023/04/10/ive-got-your-daughter-scottsdale-mom-warns-close-encounter-with-ai-voice-cloning-scam/](https://www.azfamily.com/2023/04/10/ive-got-your-daughter-scottsdale-mom-warns-close-encounter-with-ai-voice-cloning-scam/)\n\nEugenia Kuyda est la fondatrice de Replika, une application de chatbot américaine qui prétend offrir aux utilisateurs un « compagnon IA attentionné, toujours là pour écouter et parler, toujours de votre côté ». Lancée en 2017, elle compte aujourd'hui plus de deux millions d'utilisateurs actifs. Chacun a son propre chatbot ou \"replika\", car l'IA apprend de leurs conversations. Les utilisateurs peuvent également concevoir leur propre avatar de dessin animé pour leur chatbot.\n\n[https://www.bbc.com/news/business-65110680](https://www.bbc.com/news/business-65110680)\n\nNVIDIA et Getty Images collaborent sur l'IA générative. Les entreprises visent à développer deux modèles d'IA génératifs en utilisant NVIDIA Picasso, faisant partie des nouveaux services cloud de NVIDIA AI Foundations. Les utilisateurs pourraient utiliser les modèles pour créer une image ou une vidéo personnalisée en quelques secondes, simplement en tapant un concept. L'effort conjoint vise à personnaliser les modèles de base texte-vers-image et texte-vers-vidéo pour produire des visuels étonnants en utilisant du contenu visuel entièrement sous licence.\n\n[https://blogs.nvidia.com/blog/2023/03/21/generative-ai-getty-images/?ncid=so-link-748862&=&linkId=100000197541685#cid=gtcs23_so-link_en-us](https://blogs.nvidia.com/blog/2023/03/21/generative-ai-getty-images/?ncid=so-link-748862&=&linkId=100000197541685#cid=gtcs23_so-link_en-us)\n\n## \"Ubiquitous Compute\" se traduit en français par \"Informatique ubiquitaire\".\n\nEst-ce que COBOL est enfin mort ? GFT et Cloud Frame unissent leurs forces pour aider les organisations à transférer leurs programmes COBOL vers des plateformes plus efficaces. COBOL a été développé en 1959 comme l'un des premiers langages pris en charge par plusieurs fournisseurs pour les utilisateurs d'ordinateurs centraux. Il est devenu de plus en plus coûteux de déboguer et de maintenir les systèmes COBOL en raison du manque de programmeurs qui travaillent encore avec COBOL.\n\n[https://www.cloudcomputing-news.net/news/2023/apr/04/gft-and-cloudframe-help-industries-say-cheerio-to-cobol/](https://www.cloudcomputing-news.net/news/2023/apr/04/gft-and-cloudframe-help-industries-say-cheerio-to-cobol/)\n\nLes grands modèles de langage (LLMs) sont très populaires, avec ChatGPT en tête. Cela profite uniquement aux fournisseurs de services cloud car les LLMs nécessitent une grande quantité de calcul et de stockage de données pour leur déploiement et leur développement. Pour ne pas être laissés pour compte, les fournisseurs de services cloud Azure, AWS et Google ont tous annoncé leurs propres solutions LMS pour rivaliser dans cette nouvelle bataille pour la domination du Cloud. Demandons à ChatGPT qui sera le vainqueur.\n\n[https://www.infoworld.com/article/3693330/large-language-models-are-the-new-cloud-battleground.html](https://www.infoworld.com/article/3693330/large-language-models-are-the-new-cloud-battleground.html)\n\nGoogle a jeté son chapeau dans l'arène de l'informatique confidentielle avec une nouvelle offre offrant une confidentialité totale des données sensibles en cryptant les données au repos, en transit et maintenant en cours d'utilisation. Azure et AWS ont des offres de produits similaires qui fournissent un traitement confidentiel aux clients du secteur public et privé.\n\n[https://www.wired.com/story/google-cloud-confidential-virtual-machines/](https://www.wired.com/story/google-cloud-confidential-virtual-machines/)\n\n## Sécurité informatique (or Sécurité cybernétique)\n\nDans une attaque DDoS intéressante, des acteurs de la menace ont inondé npm, un dépôt de packages open source pour Node.js, en créant des sites Web malveillants et en publiant des packages vides avec des liens vers ces sites malveillants pour profiter de la bonne réputation de l'écosystème sur les moteurs de recherche. Plus de 1,42 million de packages frauduleux ont été téléchargés.\n\n[https://thehackernews.com/2023/04/hackers-flood-npm-with-bogus-packages.html](https://thehackernews.com/2023/04/hackers-flood-npm-with-bogus-packages.html)\n\nPlus de 1 000 000 de sites WordPress sont estimés infectés par une campagne en cours pour déployer le malware Balada Injector. Les attaques se déroulent par vagues, une fois toutes les quelques semaines. Les attaques redirigent des sous-domaines aléatoires vers divers sites d'escroquerie, y compris des sites web avec un faux support technique. La meilleure façon de combattre cela est de mettre à jour vos plugins sur vos sites WordPress. \n\n(Note: The translation was done to the best of my ability as a language model and may not be perfect.)\n\n[https://thehackernews.com/2023/04/over-1-million-wordpress-sites-infected.html](https://thehackernews.com/2023/04/over-1-million-wordpress-sites-infected.html)\n\nLe groupe étatique iranien MuddyWater a mené des attaques destructrices sur les environnements de cloud hybride sous le couvert d'opérations de rançongiciels. Les acteurs de la menace se font passer pour une campagne standard de rançongiciels, mais ils détruisent et perturbent essentiellement les opérations informatiques critiques. Les conclusions ont montré que MuddyWater a collaboré avec DEV-1084 pour mener à bien ces attaques.\n\n[https://thehackernews.com/2023/04/iran-based-hackers-caught-carrying-out.html](https://thehackernews.com/2023/04/iran-based-hackers-caught-carrying-out.html)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW10-fr","image":"./briefs/edw-10/fr/thumbnail.png","lang":"fr","summary":"Résumé"},{"id":176,"type":"News Brief","title":"2023-4-16","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Sécurité informatique\n\nLe FBI a mis en garde le public contre l'utilisation des bornes de recharge publiques pour téléphones en raison du risque d'attaques cybernétiques. Ces bornes de recharge, couramment trouvées dans des lieux publics tels que les aéroports, les centres commerciaux et les hôtels, pourraient potentiellement contenir un logiciel malveillant qui compromet la sécurité des informations personnelles et des appareils des utilisateurs. Apportez votre propre adaptateur secteur USB et vous devriez être prêt à partir.\n\n[https://www.cnbc.com/2023/04/10/fbi-says-you-shouldnt-use-public-phone-charging-stations.html](https://www.cnbc.com/2023/04/10/fbi-says-you-shouldnt-use-public-phone-charging-stations.html)\n\nGoogle a publié une mise à jour urgente pour son navigateur web Chrome afin de remédier à la vulnérabilité critique que les pirates informatiques exploitent. La vulnérabilité, identifiée comme un bogue \"use-after-free\", pourrait permettre aux acteurs malveillants d'exécuter du code arbitraire et potentiellement de prendre le contrôle des systèmes concernés. Google a exhorté les utilisateurs à mettre à jour leur navigateur Chrome vers la dernière version dès que possible pour atténuer le risque d'être ciblé par des cyberattaques.\n\n[https://thehackernews.com/2023/04/google-releases-urgent-chrome-update-to.html](https://thehackernews.com/2023/04/google-releases-urgent-chrome-update-to.html)\n\nLes applications Android et Novi Survey ont été trouvées avec des vulnérabilités de sécurité graves qui pourraient potentiellement exposer les informations personnelles des utilisateurs. Les failles pourraient permettre aux attaquants de contourner les mesures de sécurité et d'accéder de manière non autorisée à des données sensibles. Les utilisateurs sont invités à être prudents et à mettre à jour leurs applications vers les dernières versions pour se protéger contre les menaces cybernétiques potentielles.\n\n[https://thehackernews.com/2023/04/severe-android-and-novi-survey.html](https://thehackernews.com/2023/04/severe-android-and-novi-survey.html)\n\n## Gestion des données.\n\nSelon le directeur de la stratégie de OneTrust, les lacunes en matière de confidentialité et d'éthique des données constituent une menace existentielle pour les organisations. Le fait de ne pas traiter adéquatement ces lacunes peut entraîner des dommages à la réputation, des responsabilités juridiques et financières, ainsi qu'une perte de confiance des clients. Les organisations doivent donner la priorité à la confidentialité et à l'éthique des données afin de réduire les risques et de protéger leurs activités face aux exigences réglementaires en constante évolution et à l'augmentation de la surveillance du public.\n\n[https://deloitte.wsj.com/articles/data-privacy-ethics-gaps-an-existential-threat-says-onetrust-cso-01668542698](https://deloitte.wsj.com/articles/data-privacy-ethics-gaps-an-existential-threat-says-onetrust-cso-01668542698)\n\nAprès un examen approfondi des rapports récents du Bureau de la responsabilité gouvernementale des États-Unis, Andrew Kuoh, un associé chez Capgemini, a identifié trois actions clés sur lesquelles les organisations de données du secteur public doivent se concentrer. Favoriser une culture axée sur les données, établir des cadres de gouvernance des données et tirer parti de technologies modernes telles que l'informatique en nuage et l'intelligence artificielle (IA) pour exploiter la valeur des données. (Les gens, les processus et la technologie vous semblent familiers)\n\n[https://www.informationweek.com/government/3-actions-to-kickstart-data-ecosystems-in-the-public-sector](https://www.informationweek.com/government/3-actions-to-kickstart-data-ecosystems-in-the-public-sector)\n\nUn récent webinaire d'un panéliste de la DBTA a discuté de l'importance de suivre l'évolution des besoins des bases de données et des applications dans le cloud. À mesure que l'informatique en nuage avance, les organisations doivent adapter leurs stratégies de base de données et d'application pour optimiser les performances, la sécurité et la scalabilité. En mettant l'accent sur les technologies natives du cloud, l'automatisation et les meilleures pratiques, les entreprises peuvent rester en tête dans l'environnement cloud dynamique d'aujourd'hui.\n\n[https://www.dbta.com/Editorial/News-Flashes/Keeping-Up-with-the-Evolving-Needs-of-Databases-and-Applications-in-the-Cloud-158166.aspx](https://www.dbta.com/Editorial/News-Flashes/Keeping-Up-with-the-Evolving-Needs-of-Databases-and-Applications-in-the-Cloud-158166.aspx)\n\n## Intelligent Edge \n\nLe bord intelligent\n\nLe salon « Edge Computing Expo North America », prévu les 17 et 18 mai 2023, a récemment annoncé une liste impressionnante de technologues et de leaders d'entreprise dans le domaine de l'IOT. La conférence se tiendra au Centre des congrès de Santa Clara en Californie. Vous devez absolument la noter sur votre calendrier.\n\n[https://www.iot-now.com/2023/04/14/129661-edge-computing-expo-north-america-announces-speaker-line-up-hear-from-leading-experts-in-edge-computing/](https://www.iot-now.com/2023/04/14/129661-edge-computing-expo-north-america-announces-speaker-line-up-hear-from-leading-experts-in-edge-computing/)\n\nLa convergence de la cybersécurité IT et OT devient réelle alors que de plus en plus de solutions de cybersécurité de l'espace IT se concentrent sur la protection des infrastructures IoT et industrielles. Un exemple récent est l'introduction par CrowdStrike de CloudStrike Falcon Insight pour l'IoT. Cette plate-forme propose les mêmes outils pour les points de terminaison IoT, IT et les charges de travail en cloud et dans les centres de données.\n\n[https://www.iot-now.com/2023/04/17/129671-crowdstrike-brings-xdr-for-iot-offering-to-deliver-protection-to-iot-assets/](https://www.iot-now.com/2023/04/17/129671-crowdstrike-brings-xdr-for-iot-offering-to-deliver-protection-to-iot-assets/)\n\nDans un excellent exemple de mise en place de déploiements de l'Industrie 4.0, le Groupe Volvo a entrepris de déployer une infrastructure informatique de maintenance préventive dans son usine de Lyon, en France. Le Groupe Volvo utilise un réseau étendu à longue portée (LoRaWAN) pour permettre la surveillance et l'analyse en temps réel des données des équipements d'usine, ce qui permet une maintenance préventive proactive.\n\n[https://www.edgecomputing-news.com/2023/04/03/volvo-group-uses-lorawan-for-predictive-maintenance-in-lyon-factory/](https://www.edgecomputing-news.com/2023/04/03/volvo-group-uses-lorawan-for-predictive-maintenance-in-lyon-factory/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW11-fr","image":"./briefs/edw-11/fr/thumbnail.png","lang":"fr","summary":"Résumé"},{"id":177,"type":"News Brief","title":"2023-4-23","tags":["ai","edge","cybersecurity"],"body":"\n\n## Intelligence Artificielle\n\nSelon MarketWatch, avec un taux de croissance annualisé attendu d'environ 27%, le matériel d'IA, une industrie de 10 milliards de dollars en 2021, devrait être une industrie de 89 milliards de dollars d'ici 2030.\n\n[https://www.marketwatch.com/story/nvidia-is-ai-hardwares-leader-now-but-intel-amd-and-others-are-closing-fast-8ad6f23f](https://www.marketwatch.com/story/nvidia-is-ai-hardwares-leader-now-but-intel-amd-and-others-are-closing-fast-8ad6f23f)\n\nAgence pour les projets de recherche avancée de défense (DARPA), projet GARD (Garantir la Robustesse de l'IA contre la tromperie) de plusieurs millions de dollars, qui a trois objectifs clés : développer des algorithmes qui protègent l'apprentissage automatique contre les vulnérabilités ; développer des théories pour garantir que les algorithmes d'IA sont défendables contre les attaques ; et partager largement les outils.\n\n[https://www.zdnet.com/in-depth/innovation/these-experts-are-racing-to-protect-ai-from-hackers-time-is-running-out/](https://www.zdnet.com/in-depth/innovation/these-experts-are-racing-to-protect-ai-from-hackers-time-is-running-out/)\n\nTom Brady a menacé de poursuivre en justice les comédiens derrière la vidéo de stand-up AI. En utilisant des données provenant d'interviews avec Tom Brady et des centaines de milliers d'heures de vidéos de stand-up comedy, la branche Dudsey a simulé un spectacle de stand-up d'une durée d'une heure.\n\n[https://nypost.com/2023/04/20/tom-brady-threatened-to-sue-comedians-over-ai-standup-video/](https://nypost.com/2023/04/20/tom-brady-threatened-to-sue-comedians-over-ai-standup-video/)\n\n## Sécurité informatique\n\nUn nouveau rapport de CybelAngel a révélé les cinq principales vulnérabilités en matière de cybersécurité qui posent des risques critiques pour les organisations. Le rapport identifie les menaces principales que sont les erreurs de configuration du cloud, les vulnérabilités de la chaîne d'approvisionnement, les attaques de rançongiciels, les arnaques de phishing et les logiciels non corrigés. Le rapport exhorte les organisations à atténuer ces risques et à protéger proactivement leurs systèmes et leurs données.\n\n[https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures/](https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures/)\n\nL'agence de contrôle du trafic aérien européenne Eurocontrol a révélé avoir été victime d'une cyberattaque en provenance de Russie plus tôt ce mois-ci. L'attaque a visé les systèmes de l'agence, mais Eurocontrol a réussi à contenir et à remédier à l'incident sans perturber le trafic aérien. L'agence a prévenu les autres organisations de rester vigilantes et de renforcer leurs défenses contre les menaces cybernétiques.\n\n[https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures](https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures)\n\nUne vulnérabilité critique a été découverte dans le système de contrôle industriel (ICS) d'INEA, ce qui pourrait permettre à des attaquants à distance de prendre le contrôle des systèmes affectés et de causer des perturbations ou des dommages aux opérations industrielles. La faille, qui affecte toutes les versions du produit, a été découverte par des chercheurs de Claroty. La société a exhorté les organisations utilisant le produit à mettre à jour vers la dernière version dès que possible pour atténuer le risque d'exploitation.\n\n[https://www.securityweek.com/critical-flaw-in-inea-ics-product-exposes-industrial-organizations-to-remote-attacks/](https://www.securityweek.com/critical-flaw-in-inea-ics-product-exposes-industrial-organizations-to-remote-attacks/)\n\n## Intelligent Edge\n\nBord intelligent\n\nKneron, un fournisseur de solutions d'intelligence artificielle (IA) embarquée, a acquis Otus, un fabricant de solutions d'imagerie pour les véhicules autonomes. L'acquisition permettra à Kneron de tirer parti de l'expertise d'Otus dans le développement de caméras compactes et économes en énergie pour les applications d'IA embarquée. L'acquisition devrait accélérer l'adoption de l'IA embarquée dans l'industrie des transports.\n\n[https://www.edgecomputing-news.com/2023/04/20/kneron-buys-otus-for-edge-ai-imaging-in-autonomous-vehicles/](https://www.edgecomputing-news.com/2023/04/20/kneron-buys-otus-for-edge-ai-imaging-in-autonomous-vehicles/)\n\nSolo.io a lancé Gloo Fabric, une plateforme sécurisée de maillage de services multi-cloud pour les applications d'entreprise, prenant en charge les environnements hybrides et multi-cloud, avec un tableau de bord centralisé pour surveiller, gérer et résoudre les problèmes des applications.\n\n[https://www.edgeir.com/gloo-fabric-by-solo-io-promises-secure-multi-cloud-discovery-and-connectivity-for-enterprises-20230421](https://www.edgeir.com/gloo-fabric-by-solo-io-promises-secure-multi-cloud-discovery-and-connectivity-for-enterprises-20230421)\n\nLe gouvernement du Canada et Ericsson investiront 470 millions de dollars canadiens (376 millions de dollars américains) sur cinq ans dans la recherche et le développement de la 5G et de la 6G afin de créer une infrastructure de télécommunications durable et sécurisée, de développer de nouveaux cas d'utilisation et applications, notamment pour les villes intelligentes et les véhicules connectés.\n\n[https://www.edgeir.com/government-of-canada-ericsson-announce-ca470-million-investment-for-5g-6g-rd-20230420](https://www.edgeir.com/government-of-canada-ericsson-announce-ca470-million-investment-for-5g-6g-rd-20230420)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW12-fr","image":"./briefs/edw-12/fr/thumbnail.png","lang":"fr","summary":"Résumé"},{"id":178,"type":"News Brief","title":"2023-4-30","tags":["ai","cybersecurity","edge"],"body":"\n\n## Intelligence Artificielle\n\nLa réaction contre l'IA commence: Les fans de Snapchat expriment leur mécontentement à l'utilisation des bots alimentés par ChatGPT sur la plateforme, marquant une réaction contre l'IA. Les utilisateurs critiquent le manque de réponses semblables à celles d'un humain et l'incapacité de différencier le contenu réel et généré par l'IA. Cet incident souligne des inquiétudes croissantes et la nécessité d'une mise en œuvre équilibrée de l'IA dans les plateformes de médias sociaux.\n\n[https://www.techradar.com/news/the-ai-backlash-begins-snapchat-fans-revolt-against-chatgpt-powered-bot](https://www.techradar.com/news/the-ai-backlash-begins-snapchat-fans-revolt-against-chatgpt-powered-bot)\n\nDes lunettes équipées d'IA lisent la parole non verbale/silencieuse. Des chercheurs de l'Université Cornell ont développé une interface de reconnaissance de la parole silencieuse qui utilise la détection acoustique et l'intelligence artificielle pour reconnaître en continu jusqu'à 31 commandes non vocalisées basées sur les mouvements des lèvres et de la bouche. L'interface portable à faible consommation d'énergie - EchoSpeech - ne nécessite que quelques minutes de données d'entraînement utilisateur avant de reconnaître les commandes et peut fonctionner sur un smartphone. Je jure que mes grands-parents savaient faire cela il y a 40 ans. Ils communiquaient bien avec un simple hochement de tête, un sourire et un clin d'œil.\n\n[https://news.cornell.edu/stories/2023/04/ai-equipped-eyeglasses-can-read-silent-speech](https://news.cornell.edu/stories/2023/04/ai-equipped-eyeglasses-can-read-silent-speech)\n\nLes chatbots d'IA peuvent-ils remplacer les patrons ? Une startup nommée Aesthetic explore le potentiel des chatbots alimentés par l'IA pour prendre en charge les tâches de gestion, offrir des conseils, répondre aux questions et gérer les flux de travail. Bien que efficace, certains soutiennent que les qualités humaines telles que l'empathie sont irremplaçables. La combinaison de l'interaction humaine et de l'IA est-elle la meilleure approche pour un leadership efficace sur le lieu de travail ?\n\n[https://startup.outlookindia.com/sector/saas/can-your-boss-be-replaced-by-an-ai-chatbot--news-8257](https://startup.outlookindia.com/sector/saas/can-your-boss-be-replaced-by-an-ai-chatbot--news-8257)\n\n## CyberSécurité\n\nL'Agence de cybersécurité et de sécurité des infrastructures (CISA) cherche des commentaires du public sur une directive préliminaire exigeant des agences fédérales qu'elles attestent des pratiques de sécurité des logiciels. La directive vise à améliorer la sécurité des chaînes d'approvisionnement en logiciels et la période de commentaires est ouverte jusqu'au 3 mai 2023.\n\n[https://www.cybersecuritydive.com/news/cisa-public-comment-software-security-attestation/648932/](https://www.cybersecuritydive.com/news/cisa-public-comment-software-security-attestation/648932/)\n\nLes pirates informatiques utilisant des rançongiciels ont développé des tactiques, comme le révèle un rapport de CrowdStrike, en ayant recours à des méthodes coercitives pour extorquer des paiements des victimes. En plus de chiffrer les données, ces tactiques impliquent des menaces de violences physiques, la divulgation d'informations volées aux médias et même le ciblage des familles des victimes. Les organisations doivent rester vigilantes et adapter leurs défenses pour contrer ces nouvelles stratégies afin de limiter l'impact des attaques de rançongiciels et protéger leurs données précieuses contre l'exploitation.\n\n[https://cyware.com/news/coercion-in-the-age-of-ransomware-new-tactics-for-extorting-payments-0c31dba6](https://cyware.com/news/coercion-in-the-age-of-ransomware-new-tactics-for-extorting-payments-0c31dba6)\n\nLe fournisseur de stockage réfrigéré Americold a subi une violation de réseau qui a entraîné une panne généralisée affectant plusieurs systèmes. La société a confirmé l'incident mais n'a pas révélé la nature de la violation ni l'ampleur de l'impact. Par mesure de précaution, Americold a temporairement fermé des systèmes spécifiques et a engagé des experts externes en cybersécurité pour enquêter sur la violation. Les données des clients sont considérées comme sécurisées et la société travaille à restaurer la pleine fonctionnalité.\n\n[https://www.bleepingcomputer.com/news/security/cold-storage-giant-americold-outage-caused-by-network-breach/](https://www.bleepingcomputer.com/news/security/cold-storage-giant-americold-outage-caused-by-network-breach/)\n\n## Bord Intelligent\n\nAkamai Technologies a acquis NeoSec, une start-up spécialisée dans la cybersécurité axée sur la sécurité des API, afin de renforcer ses capacités de détection et de réponse aux attaques basées sur les API. L'acquisition améliorera les offres de sécurité d'Akamai et fournira aux clients une meilleure protection contre les menaces visant les interfaces de programmation d'applications. Akamai vise à répondre à l'évolution du paysage de la cybersécurité en élargissant son portefeuille de sécurité et en offrant des solutions améliorées pour lutter contre les risques liés aux API.\n\n[https://www.edgecomputing-news.com/2023/04/24/akamai-acquires-neosec-to-bolster-api-detection-and-response/](https://www.edgecomputing-news.com/2023/04/24/akamai-acquires-neosec-to-bolster-api-detection-and-response/)\n\nZadara et Kasten by Veeam ont établi un partenariat pour proposer une solution complète de protection des données pour les environnements Kubernetes. En combinant zCompute, zStorage de Zadara et la plateforme K10 de Kasten, la solution offre une sauvegarde, une récupération après sinistre et une mobilité d'application. Elle permet un déplacement d'application sans faille entre les clusters Kubernetes, y compris les emplacements Global Edge Cloud de Zadara. Cette collaboration répond au besoin de solutions de stockage agiles et rentables, protégeant les applications natives du cloud dans Kubernetes tout en prenant en charge diverses bases de données. De plus, Zadara a récemment lancé sa plateforme d'infrastructure en tant que service, c9 Flex-N, au Japon en collaboration avec BroadBand Tower.\n\n[https://www.edgeir.com/zadara-kasten-by-veeam-unite-to-provide-multi-tier-data-protection-for-kubernetes-20230428](https://www.edgeir.com/zadara-kasten-by-veeam-unite-to-provide-multi-tier-data-protection-for-kubernetes-20230428)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW13-fr","image":"./briefs/edw-13/fr/thumbnail.png","lang":"fr","summary":"News and stories from the Week of May 01, 2023, in Digital Transformation, including cyberattacks and intelligent edge, non-verbal communication AI, and company merges in the IoT space."},{"id":179,"type":"News Brief","title":"2023-5-7","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Gestion des données\n\nDIQ, un nouvel outil d'indice de confiance des données, promeut la démocratisation des données. Cet outil mesure la fiabilité des sources de données, permettant aux organisations de prendre des décisions éclairées concernant l'utilisation des données. En offrant de la transparence et de la responsabilité, DIQ vise à donner plus de pouvoir aux entreprises dans leurs stratégies axées sur les données et à renforcer la confiance dans la prise de décision basée sur les données.\n\n[https://tdwi.org/articles/2023/05/04/diq-all-data-trust-index-tool-to-drive-data-democratization.aspx\n](https://tdwi.org/articles/2023/05/04/diq-all-data-trust-index-tool-to-drive-data-democratization.aspx\n)\n\nFIATA et le Forum mondial des chargeurs plaident en faveur de normes de gouvernance des données plus solides dans l'industrie du transport maritime. Ils soulignent la nécessité de relever les défis de la qualité, de l'accessibilité et de la sécurité des données pour améliorer l'efficacité de la chaîne d'approvisionnement. L'établissement de normes cohérentes vise à améliorer le partage des données, la collaboration et la prise de décision entre les parties prenantes, ce qui profite finalement à la communauté maritime mondiale.\n\n[https://www.porttechnology.org/news/fiata-global-shippers-forum-push-for-stronger-data-governance-standards/\n](https://www.porttechnology.org/news/fiata-global-shippers-forum-push-for-stronger-data-governance-standards/\n)\n\nDatabricks Ventures a investi dans Immuta; une entreprise axée sur le renforcement de la gouvernance des lakehouse. La plateforme d'Immuta fournit des solutions d'accès et de contrôle des données, permettant aux organisations de gouverner et de sécuriser efficacement leurs environnements de lakehouse. Cet investissement vise à améliorer les capacités de gouvernance des données au sein de l'écosystème de Databricks, en favorisant la confidentialité, la conformité et la sécurité des entreprises utilisant des architectures de lakehouse.\n\n[https://www.databricks.com/blog/strengthening-lakehouse-governance-ecosystem-databricks-ventures-invests-immuta\n](https://www.databricks.com/blog/strengthening-lakehouse-governance-ecosystem-databricks-ventures-invests-immuta\n)\n\n## Intelligence Artificielle\n\nLe principal ingénieur derrière le chatbot avancé de Google, Geoffrey Hinton, a démissionné en raison de préoccupations concernant les risques potentiels de la technologie de l'IA et ses implications éthiques. Hinton pense que le système chatbot qu'il a développé peut être utilisé pour propager des désinformations et des fausses nouvelles, soulignant la nécessité d'un développement et d'un déploiement responsables de l'IA pour protéger la société.\n\n[https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\n](https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\n)\n\nLes entreprises technologiques veulent être rémunérées pour les données alimentant les modèles d'IA. Les chatbots digèrent l'Internet, et les fournisseurs de contenu veulent être payés. Les entreprises d'intelligence artificielle utilisent du contenu créé par des millions de personnes sans leur consentement ni leur compensation. Reddit, une source inestimable pour OpenAI, a récemment annoncé qu'elle commencerait à facturer les entreprises d'IA pour l'accès aux données. Twitter fait de même. OpenAI a refusé de commenter.\n\n[https://www.wsj.com/articles/chatgpt-ai-artificial-intelligence-openai-personal-writing-5328339a\n](https://www.wsj.com/articles/chatgpt-ai-artificial-intelligence-openai-personal-writing-5328339a\n)\n\nL'IA commence à remplacer les humains dans les travaux de bureau. Le PDG d'IBM annonce une pause temporaire dans l'embauche de personnel pour les emplois de bureau alors que l'entreprise investit dans la technologie de l'IA. La décision reflète le changement stratégique d'IBM visant à tirer parti de l'intelligence artificielle pour rationaliser les opérations et améliorer l'efficacité. Cette décision témoigne de l'engagement de l'entreprise à s'adapter aux progrès technologiques et aux exigences évolutives du marché.\n\n[https://fortune.com/2023/05/01/ibm-ceo-ai-artificial-intelligence-back-office-jobs-pause-hiring/\n](https://fortune.com/2023/05/01/ibm-ceo-ai-artificial-intelligence-back-office-jobs-pause-hiring/\n)\n\n## Calcul omniprésent\n\nVMware a introduit Cross-Cloud Managed Services, une nouvelle offre destinée à simplifier et à rationaliser la gestion multi-cloud. Le service offre aux clients une plateforme unifiée pour gérer différents environnements cloud, offrant des fonctionnalités améliorées de visibilité, de sécurité et de gouvernance. Avec cette solution, VMware vise à aider les organisations à surmonter les complexités des opérations multi-cloud et à optimiser leurs stratégies cloud pour une efficacité et une agilité accrues.\n\n[https://www.cloudcomputing-news.net/news/2023/may/04/vmware-unveils-cross-cloud-managed-services/\n](https://www.cloudcomputing-news.net/news/2023/may/04/vmware-unveils-cross-cloud-managed-services/\n)\n\nAWS (Amazon Web Services) a développé un nouveau service qui offre un accès sécurisé aux applications cloud sans les VPN (réseaux privés virtuels) traditionnels. Le service AWS Client VPN utilise l'infrastructure réseau mondiale d'AWS pour établir des connexions sécurisées entre les utilisateurs et les ressources cloud. Cette approche simplifie l'accès à distance tout en maintenant des niveaux élevés de sécurité, ce qui facilite la gestion et la sécurisation efficaces des environnements cloud pour les organisations.\n\n[https://www.networkworld.com/article/3695174/aws-secures-access-to-cloud-apps-without-using-vpns.html\n](https://www.networkworld.com/article/3695174/aws-secures-access-to-cloud-apps-without-using-vpns.html\n)\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW14-fr","image":"./briefs/edw-14/fr/thumbnail.png","lang":"fr","summary":"News in Digital Transformation for the week of May 8, 2023 including "},{"id":180,"type":"News Brief","title":"2023-5-14","tags":["ai","edge","cybersecurity"],"body":"\n\n## Intelligence Artificielle\n\nLe journal Irish Times est tombé victime d'une supercherie lorsqu'il a publié un article rédigé par un programme d'intelligence artificielle (IA). La pièce générée par l'IA est passée inaperçue et a été mise en ligne, mettant en évidence les défis de la détection de contenu généré par machine. Cet incident soulève des préoccupations quant au potentiel de désinformation et à la nécessité de processus de vérification efficaces face au contenu généré par l'IA.\n\n[https://www.lemonde.fr/en/economy/article/2023/05/17/the-irish-times-duped-by-a-hoax-article-written-by-an-artificial-intelligence-program_6026930_19.html\r](https://www.lemonde.fr/en/economy/article/2023/05/17/the-irish-times-duped-by-a-hoax-article-written-by-an-artificial-intelligence-program_6026930_19.html\r)\n\nL'Europe est en tête dans l'établissement de directives pour le développement de l'intelligence artificielle (IA). L'Union européenne développe des règlements exhaustifs pour garantir que l'IA soit transparente, responsable et respecte les droits de l'homme. Ces mesures visent à répondre aux préoccupations éthiques et aux risques potentiels associés à l'IA, faisant de l'Europe un chef de file mondial dans la mise en œuvre d'une IA responsable.\n\n[https://www.nbcnews.com/tech/tech-news/europe-leading-world-building-guardrails-ai-rcna83912\r](https://www.nbcnews.com/tech/tech-news/europe-leading-world-building-guardrails-ai-rcna83912\r)\n\nUne nouvelle étude suggère que la technologie d'intelligence artificielle (IA) générative a le potentiel d'augmenter les heures de travail des professionnels de santé jusqu'à 40%. En automatisant les tâches routinières et en fournissant un soutien décisionnel, l'IA pourrait améliorer l'efficacité et permettre aux médecins et infirmières de se concentrer sur des soins plus critiques et complexes pour les patients.\n\n[https://www.healthcareitnews.com/news/generative-ai-could-augment-40-healthcare-working-hours\r](https://www.healthcareitnews.com/news/generative-ai-could-augment-40-healthcare-working-hours\r)\n\n## Sécurité informatique\n\nL'Agence de cybersécurité et de sécurité des infrastructures (CISA) a averti d'une vulnérabilité critique dans les points d'accès sans fil de Ruckus qui est exploitée pour infecter les réseaux Wi-Fi. Le bogue permet aux attaquants de gagner un accès non autorisé et d'exécuter du code arbitraire à distance. CISA conseille aux organisations utilisant des points d'accès Ruckus d'appliquer les correctifs nécessaires et de mettre en place des mesures d'atténuation pour se protéger contre les attaques potentielles.\n\n[https://www.bleepingcomputer.com/news/security/cisa-warns-of-critical-ruckus-bug-used-to-infect-wi-fi-access-points\r](https://www.bleepingcomputer.com/news/security/cisa-warns-of-critical-ruckus-bug-used-to-infect-wi-fi-access-points\r)\n\nDes données sensibles appartenant à 237 000 employés du gouvernement américain ont été exposées suite à une violation de sécurité. Les informations compromises comprennent les numéros de sécurité sociale, les coordonnées et les informations d'emploi. L'infraction a été attribuée à une personne non autorisée ayant accédé au compte de messagerie électronique d'un employé. Les autorités enquêtent sur l'incident et les personnes touchées sont informées.\n\n[https://www.yahoo.com/news/data-237-000-us-government-232707971.html\r](https://www.yahoo.com/news/data-237-000-us-government-232707971.html\r)\n\nUne grave faille de sécurité a été découverte, exposant plus de 2 millions de données sensibles sur le site WordPress. La vulnérabilité du plugin \"Essential Addons for Elementor\" de WordPress permet un accès non autorisé aux données personnelles, telles que les noms, adresses et informations financières. Cette faille affecte un logiciel largement utilisé et pose un risque important pour la vie privée et la sécurité des utilisateurs. Les développeurs travaillent sur un correctif et il est conseillé aux utilisateurs de mettre à jour leur système rapidement.\n\n[https://thehackernews.com/2023/05/severe-security-flaw-exposes-over.html\r](https://thehackernews.com/2023/05/severe-security-flaw-exposes-over.html\r)\n\n## Calcul en périphérie\n\nUn récent rapport de UL Solutions met en lumière l'intelligence artificielle (IA) et l'Internet des objets (IoT) en périphérie comme des technologies cruciales pour les organisations utilisant des réseaux 5G. La combinaison d'IA et IoT en périphérie permet un traitement de données en temps réel, une analyse avancée et l'automatisation, débloquant de nouvelles possibilités pour les industries de la santé, de la fabrication et des transports. Le rapport souligne le potentiel de ces technologies en matière d'innovation et d'efficacité dans l'ère 5G.\n\n[https://www.edgeir.com/report-reveals-ai-iot-edge-as-key-technologies-for-organizations-leveraging-5g-20230512\r](https://www.edgeir.com/report-reveals-ai-iot-edge-as-key-technologies-for-organizations-leveraging-5g-20230512\r)\n\nKyndryl, une entreprise de services informatiques, élargit son offre en introduisant un service géré Secure Access Service Edge (SASE). SASE combine les capacités de sécurité réseau et de réseaux étendus dans une solution unifiée basée sur le cloud. En intégrant SASE dans son portefeuille, Kyndryl vise à fournir une sécurité et une connectivité améliorées aux entreprises opérant dans un environnement hybride ou multi-cloud. Cette décision est conforme à la demande croissante de solutions de sécurité complètes et simplifiées dans le paysage informatique en constante évolution.\n\n[https://www.sdxcentral.com/articles/analysis/why-kyndryl-is-adding-a-managed-sase-service/2023/05/\r](https://www.sdxcentral.com/articles/analysis/why-kyndryl-is-adding-a-managed-sase-service/2023/05/\r)\n\nLe marché de l'IA Edge devrait connaître une croissance considérable, sa taille étant susceptible d'augmenter significativement dans les années à venir. Des facteurs tels que la prolifération des appareils IoT, les avancées technologiques de l'IA et la nécessité de traitement de données en temps réel stimulent cette croissance. L'IA Edge permet une prise de décision intelligente au bord du réseau, réduisant la latence et améliorant l'efficacité. L'extension du marché offre des opportunités pour diverses industries, notamment la santé, la fabrication et la vente au détail.\n\n[https://finance.yahoo.com/news/edge-ai-market-size-predicted-190000466.html\r](https://finance.yahoo.com/news/edge-ai-market-size-predicted-190000466.html\r)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW15-fr","image":"./briefs/edw-15/fr/thumbnail.png","lang":"fr","summary":"News from the week of May 15, 2023 in digital transformation including stories from Edge Computing, Cybersecurity, and Artificial Intelligence."},{"id":181,"type":"News Brief","title":"2023-5-21","tags":null,"body":"\n\n## Informatique ubiquitaire\n\nAmazon Web Services (AWS) prévoit d'investir 12,7 milliards de dollars dans le marché de l'infrastructure cloud en Inde d'ici 2030, en soutenant la création d'emplois et les projets d'énergie renouvelable. L'investissement vise à répondre à la demande croissante des clients et à contribuer à la transformation numérique de l'Inde tout en abordant les défis d'infrastructure.\n\n[https://www.cloudcomputing-news.net/news/2023/may/22/aws-to-put-13-billion-into-india-cloud-infrastructure-by-2030/\r](https://www.cloudcomputing-news.net/news/2023/may/22/aws-to-put-13-billion-into-india-cloud-infrastructure-by-2030/\r)\n\nSnowflake, un fournisseur de plateforme de données cloud, a ouvert un nouveau bureau au Royaume-Uni pour étendre sa portée dans la région EMEA (Europe, Moyen-Orient et Afrique). Cette initiative fait suite à l'élan positif de l'entreprise sur le marché et vise à mieux servir sa base de clientèle croissante dans la zone.\n\n[https://www.cloudcomputing-news.net/news/2023/may/16/snowflake-opens-uk-office-amid-strong-momentum-across-emea/\r](https://www.cloudcomputing-news.net/news/2023/may/16/snowflake-opens-uk-office-amid-strong-momentum-across-emea/\r)\n\nMicrosoft se prépare à lancer sa solution Microsoft Cloud for Sovereignty, destinée aux organisations gouvernementales et du secteur public. La solution de cloud sectorielle a fait l'objet de préversions privées pour relever des défis spécifiques au secteur. Microsoft met l'accent sur la transparence, les politiques de données et la sécurité, avec des plans pour offrir des solutions de plateforme dans diverses industries.\n\n[https://www.ciodive.com/news/Microsoft-industry-cloud-data-sovereignty-platform/650214/\r](https://www.ciodive.com/news/Microsoft-industry-cloud-data-sovereignty-platform/650214/\r)\n\n## Communications Avancées\n\nIntel a dévoilé son FPGA Agilex, une puce programmable conçue pour les réseaux intelligents. Le FPGA Agilex offre des fonctionnalités avancées, telles que l'accélération de l'IA et une sécurité améliorée, ce qui le rend adapté à diverses applications. On prévoit que la puce renforcera l'infrastructure de réseau avec une flexibilité et une performance accrues.\n\n[https://www.networkworld.com/article/3697156/intel-launches-agilex-fpga-for-smart-networking.html\r](https://www.networkworld.com/article/3697156/intel-launches-agilex-fpga-for-smart-networking.html\r)\n\nEthernet, la technologie de réseau pionnière, célèbre son 50ème anniversaire. Malgré son parcours impressionnant, l'évolution d'Ethernet continue. Avec des avancées telles que des vitesses plus rapides, une capacité accrue et une fiabilité améliorée, Ethernet reste essentiel pour connecter le monde numérique. À mesure que la technologie progresse, Ethernet est en mesure de jouer un rôle crucial dans la formation de l'avenir de la mise en réseau.\n\n[https://www.networkworld.com/article/3697013/ethernet-turns-50-but-its-voyage-has-only-begun.html\r](https://www.networkworld.com/article/3697013/ethernet-turns-50-but-its-voyage-has-only-begun.html\r)\n\nLes solutions open source simplifient la gestion des tissus de réseau complexes, selon Network Computing. Avec la complexité croissante des réseaux modernes, les outils et frameworks open source offrent une flexibilité, une interopérabilité et des capacités d'automatisation. Ces solutions permettent aux organisations de rationaliser la gestion des tissus, de réduire les coûts et d'améliorer l'efficacité. Les approches basées sur l'open source deviennent cruciales pour simplifier les complexités des tissus de réseau dans le paysage numérique actuel.\n\n[https://www.networkcomputing.com/networking/simplifying-complex-fabrics-open-source-based-solutions\r](https://www.networkcomputing.com/networking/simplifying-complex-fabrics-open-source-based-solutions\r)\n\n## Gestion des données\n\nIDM.net.au suggère d'explorer l'automatisation de la gouvernance des données pour cinq raisons : une amélioration de la qualité des données, une conformité rationalisée, une productivité accrue, une visibilité des données accrue et une adaptation au paysage de données en évolution. L'automatisation réduit les erreurs, assure la conformité, stimule l'efficacité, fournit un contrôle et maintient les organisations compétitives.\n\n[https://idm.net.au/article/0014302-5-reasons-explore-data-governance-automation-opportunities\r](https://idm.net.au/article/0014302-5-reasons-explore-data-governance-automation-opportunities\r)\n\nTDWI.org discute comment les experts en données abordent les défis du web scraping. Ils soulignent l'importance de sélectionner des outils et des techniques appropriés pour extraire les données des sites web. Surmonter des obstacles tels que les mesures anti-web scraping et le contenu dynamique nécessite des mandataires, des agents utilisateurs et une expertise en structuration de données. Le web scraping réussi permet une acquisition de données précieuses pour l'analyse et la prise de décision.\n\n[https://tdwi.org/articles/2023/05/18/diq-all-how-data-experts-overcome-web-scraping-challenges.aspx\r](https://tdwi.org/articles/2023/05/18/diq-all-how-data-experts-overcome-web-scraping-challenges.aspx\r)\n\nDans un rapport récent, Dataversity.net met en garde contre une architecture de sécurité médiocre dans les environnements cloud. L'article identifie les erreurs courantes, telles que les contrôles d'accès faibles et les mauvaises configurations, qui peuvent entraîner des violations de données. Mettant l'accent sur l'importance d'une authentification forte et de la surveillance, il souligne la nécessité de mesures de sécurité robustes pour protéger les données sensibles dans les architectures cloud.\n\n[https://www.dataversity.net/cloud-architecture-mistakes-the-perils-of-poor-security-architecture/\r](https://www.dataversity.net/cloud-architecture-mistakes-the-perils-of-poor-security-architecture/\r)\n\n\n\n","guests":null,"link":"/brief-EDW16-fr","image":"./briefs/edw-16/fr/thumbnail.png","lang":"fr","summary":"Digital Transformation news for the week of May 29, 2023. In this episode."},{"id":182,"type":"News Brief","title":"2023-5-28","tags":null,"body":"\n\n## Intelligence Artificielle\n\nLe PDG d'OpenAI, Sam Altman, met en garde contre le possible retrait des services de l'entreprise en UE en raison des inquiétudes concernant la réglementation prévue par l'AI Act. Cela souligne une fracture transatlantique croissante sur le contrôle de l'IA. Les entreprises technologiques américaines, dont Google, se préparent à un potentiel conflit avec les régulateurs européens concernant la réglementation de l'IA.\n\n[https://www.ft.com/content/5814b408-8111-49a9-8885-8a8434022352](https://www.ft.com/content/5814b408-8111-49a9-8885-8a8434022352)\n\nLe président de Microsoft, Brad Smith, a exprimé des préoccupations concernant les deepfakes et a appelé à des réglementations pour lutter contre les opérations étrangères d'influence cybernétique. Il a préconisé la certification d'IA critique, la mise en place de contrôles à l'exportation et la responsabilisation des individus pour les problèmes liés à l'IA. Le PDG d'OpenAI, Sam Altman, a souligné la nécessité d'une coopération mondiale et d'une conformité en matière de sécurité.\n\n[http://https//www.reuters.com/technology/microsoft-chief-calls-humans-rule-ai-safeguard-critical-infrastructure-2023-05-25/](http://https//www.reuters.com/technology/microsoft-chief-calls-humans-rule-ai-safeguard-critical-infrastructure-2023-05-25/)\n\nOpenAI lance un programme de subventions pour explorer les suggestions démocratiques à la prise de décision de l'IA. Ils cherchent des perspectives diverses pour façonner le comportement de l'IA et visent à développer des processus innovants pour la supervision publique. Les équipes sélectionnées recevront des subventions pour mener des expériences et publier des résultats afin d'établir une gouvernance démocratique pour l'IA.\n\n[https://openai.com/blog/democratic-inputs-to-ai](https://openai.com/blog/democratic-inputs-to-ai)\n\n## Sécurité informatique\n\nL'acteur de menace nord-coréen, Kimsuky, a amélioré son logiciel malveillant de reconnaissance, RandomQuery, selon la société de cybersécurité Cyware. Le logiciel malveillant cible les organisations gouvernementales, militaires et de défense, collectant des informations sensibles. Il utilise de nouvelles techniques et échappe à la détection pour mener des opérations d'espionnage. La vigilance et des mesures de sécurité solides sont cruciales pour atténuer la menace.\n\n[https://cyware.com/news/north-korea-actor-kimsuky-updates-its-reconnaissance-malware-randomquery-25cb1d1e](https://cyware.com/news/north-korea-actor-kimsuky-updates-its-reconnaissance-malware-randomquery-25cb1d1e)\n\nDes chercheurs en sécurité ont découvert un logiciel espion Android sophistiqué nommé \"Predator\" qui est actif depuis 2021. Le logiciel espion vise les entités gouvernementales et militaires en Asie du Sud, notamment en Inde, au Pakistan et en Afghanistan. Il peut voler des données sensibles, enregistrer de l'audio et de la vidéo, et geo-localiser en temps réel. Les utilisateurs sont invités à mettre à jour leurs appareils et à être prudents avec les applications suspectes.\n\n[https://thehackernews.com/2023/05/predator-android-spyware-researchers.html](https://thehackernews.com/2023/05/predator-android-spyware-researchers.html)\n\nMicrosoft a capturé des pirates gouvernementaux chinois lors d'une campagne appelée Typhon Volt, visant des organisations d'infrastructures critiques à Guam, un territoire américain. La campagne visait à perturber l'infrastructure de communication entre les États-Unis et l'Asie. CISA a émis un avertissement et Microsoft a conseillé sur l'atténuation de la menace.\n\n[https://www.securityweek.com/microsoft-catches-chinese-gov-hackers-in-guam-critical-infrastructure-orgs/](https://www.securityweek.com/microsoft-catches-chinese-gov-hackers-in-guam-critical-infrastructure-orgs/)\n\n## Informatique de périphérie\n\nDell Technologies lance Dell NativeEdge, une plateforme logicielle qui simplifie et sécurise les déploiements Zero Trust Edge. Elle permet une opération de périphérie rationalisée, un déploiement sans contact et une orchestration d'application multi-cloud. La plateforme vise à améliorer l'efficacité et la connectivité en périphérie, en soutenant diverses industries et cas d'utilisation.\n\n[https://www.edgecomputing-news.com/2023/05/24/dell-nativeedge-software-transforms-edge-operations/](https://www.edgecomputing-news.com/2023/05/24/dell-nativeedge-software-transforms-edge-operations/)\n\nNTT renforce sa collaboration avec Cisco afin de développer des réseaux privés gérés pour l'IoT d'entreprise, en mettant l'accent sur la durabilité et la facilité d'utilisation. Ce partenariat utilise le matériel IoT de Cisco, permettant des insights exploitables grâce à l'IA et au ML. L'accord s'appuie sur leur collaboration précédente pour la solution Private 5G de Cisco, offrant IoT en tant que service, comprenant des services intégrés et une gestion de l'infrastructure.\n\n[https://www.sdxcentral.com/articles/interview/ntt-doubling-down-on-cisco-to-boost-managed-iot/2023/05/](https://www.sdxcentral.com/articles/interview/ntt-doubling-down-on-cisco-to-boost-managed-iot/2023/05/)\n\nCloudflare, un fournisseur de services de pointe, se concentre sur l'intelligence artificielle (IA) après avoir signalé une forte croissance de ses services de periphérie au cours du premier trimestre de 2023. L'entreprise vise à exploiter l'IA pour améliorer ses offres et améliorer l'expérience client, notamment dans la mitigation des DDoS et la sécurité. L'accent de Cloudflare sur l'IA est en phase avec sa stratégie visant à offrir des solutions de périphérie innovantes à une base de clients en croissance.\n\n[https://www.edgeir.com/cloudflare-leaning-into-ai-after-1q23-results-show-good-growth-for-edge-services-20230525](https://www.edgeir.com/cloudflare-leaning-into-ai-after-1q23-results-show-good-growth-for-edge-services-20230525)\n\n\n","guests":null,"link":"/brief-EDW17-fr","image":"./briefs/edw-17/fr/thumbnail.png","lang":"fr","summary":"News for Embracing Digital for the week of May 29, 2023, learn about more regulations for AI, increased nation-state cyber attacks, and edge computing investments."},{"id":183,"type":"News Brief","title":"2023-6-4","tags":["ai","compute","cybersecurity"],"body":"\n\n## Intelligence Artificielle\n\nSelon un rapport, JPMorgan, un géant bancaire, a affiché plus de 3 600 emplois liés à l'IA, reflétant l'intérêt croissant de Wall Street pour cette technologie révolutionnaire. Cette initiative met en évidence la dépendance croissante de l'industrie à l'égard de l'intelligence artificielle pour diverses fonctions et marque un changement dans le secteur financier vers l'adoption de l'IA et de ses avantages potentiels.\n\n[https://www.msn.com/en-us/money/other/banking-giant-jpmorgan-advertised-more-than-3-600-ai-related-jobs-report-says-as-wall-street-starts-to-embrace-the-revolutionary-tech/ar-AA1bYZwL](https://www.msn.com/en-us/money/other/banking-giant-jpmorgan-advertised-more-than-3-600-ai-related-jobs-report-says-as-wall-street-starts-to-embrace-the-revolutionary-tech/ar-AA1bYZwL)\n\nOpenAI a développé une méthode pour améliorer les capacités de raisonnement logique des modèles d'IA afin de réduire les cas d'\"hallucinations\" ou de génération d'informations incorrectes. En modifiant le processus de formation, l'équipe de recherche a pu produire des modèles qui présentent une meilleure cohérence et évitent de faire des détails inventés. Ce développement contribue à créer des systèmes d'IA plus fiables et dignes de confiance.\n\n[https://www.zdnet.com/article/openai-found-a-way-to-make-ai-models-more-logical-and-avoid-hallucinations/](https://www.zdnet.com/article/openai-found-a-way-to-make-ai-models-more-logical-and-avoid-hallucinations/)\n\nLe Japon brave les lois sur le droit d'auteur en permettant la formation de l'IA avec n'importe quelle donnée, dans le but d'accélérer sa progression dans l'IA et de rivaliser à l'échelle mondiale. Les préoccupations des artistes sont contrecarrées par le soutien de l'université et des entreprises, alors que le Japon cherche à accéder aux données occidentales en échange de ses ressources culturelles. Un virage unique dans le débat sur la réglementation émerge.\n\n[https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/](https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/)\n\n## Informatique omniprésente\n\nDell renforce les stratégies multi-cloud avec des avancées dans son portefeuille APEX. Les offres comprennent les plateformes Cloud Dell APEX pour Azure, Red Hat OpenShift, VMware et des solutions de stockage pour les clouds publics. Dell APEX s'étend pour fournir des ressources de calcul et des PC-as-a-Service tout en s'associant à Databricks pour permettre l'analyse de données sur site et dans des environnements de cloud. HPE, de son côté, offre à présent des offres multi-cloud pour sa gamme de produits Green Lake.\n\n[https://www.cloudcomputing-news.net/news/2023/jun/01/dell-apex-portfolio-advancements-help-customers-strengthen-multicloud-strategies/](https://www.cloudcomputing-news.net/news/2023/jun/01/dell-apex-portfolio-advancements-help-customers-strengthen-multicloud-strategies/)\n\nUn rapport récent d'information week prédit que les dépenses mondiales en services cloud atteindront 1,3 billion de dollars d'ici 2025, soit une croissance d'environ 16,9%. Cependant, de nombreuses organisations ont encore besoin d'aide pour le retour sur investissement de leurs investissements. Le rapport identifie les domaines critiques pour améliorer le retour sur investissement dans les déploiements Cloud, notamment le FinOps, l'adoption de stratégies Cloud multi-hybrides, le DevOps et l'ingénierie de plate-forme, et la sécurité Cloud.\n\n[https://www.informationweek.com/cloud/leaders-should-pay-attention-to-these-4-major-cloud-trends](https://www.informationweek.com/cloud/leaders-should-pay-attention-to-these-4-major-cloud-trends)\n\nMicrosoft Azure DevOps a connu une panne de dix heures dans la région du sud du Brésil en raison d'une simple faute de frappe qui a causé la suppression de dix-sept bases de données de production. L'erreur est survenue lors d'une mise à niveau de code et une faute de frappe dans la demande de fusion a conduit à la suppression de l'ensemble du serveur. Les données ont été récupérées, mais le processus de récupération a duré plus de dix heures en raison de diverses complications. Microsoft a mis en place des correctifs et des reconfigurations pour éviter des problèmes similaires à l'avenir.\n\n[https://www.theregister.com/2023/06/03/microsoft_azure_outage_brazil/](https://www.theregister.com/2023/06/03/microsoft_azure_outage_brazil/)\n\n## Nouvelles de sécurité cybernétique\n\nUn expert en cybersécurité met en garde contre les escrocs qui ciblent les utilisateurs de Gmail en exploitant une fonctionnalité de collaboration. Les utilisateurs reçoivent des invitations frauduleuses qui les redirigent vers des sites malveillants. La vigilance, la vérification des demandes, l'activation de l'authentification à deux facteurs et la surveillance des paramètres de compte sont recommandées pour se protéger contre de telles escroqueries.\n\n[https://www.wmur.com/article/nh-cybersecurity-expert-gmail-feature-is-being-hacked-by-scammers/44083493](https://www.wmur.com/article/nh-cybersecurity-expert-gmail-feature-is-being-hacked-by-scammers/44083493)\n\nSelon une enquête menée par le CISO Hall of Fame, la sécurité cloud est la principale préoccupation des professionnels de l'informatique. Le rapport met en évidence la dépendance croissante aux services cloud et la nécessité de faire face aux défis de sécurité tels que les violations de données et les accès non autorisés. Les principaux domaines d'attention comprennent la gestion des identités et des accès, le chiffrement, la détection proactive des menaces pour garantir des mesures de sécurité cloud solides et la recherche de talents en cybersécurité ayant une expérience des services cloud.\n\n[https://thehackernews.com/2023/06/cloud-security-tops-concerns-for.html](https://thehackernews.com/2023/06/cloud-security-tops-concerns-for.html)\n\nDans un article récent, Walmart a ouvert la discussion sur ses meilleures pratiques, espérant ainsi améliorer la connaissance de son écosystème de fournisseurs et de partenaires. Le centre des opérations de sécurité de Walmart et son approche de défense en profondeur illustrent les meilleures pratiques, mais les petites entreprises ont besoin de stratégies pratiques. L'alignement des protocoles de sécurité, la mise en place d'une gestion forte de l'identité et de l'accès, ainsi que la révision des politiques d'accès peuvent améliorer la protection. Les technologies de sécurité de nouvelle génération offrent des solutions rentables pour atténuer les menaces cybernétiques.\n\n[https://betanews.com/2023/06/03/walmart-cybersecurity/](https://betanews.com/2023/06/03/walmart-cybersecurity/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW18-fr","image":"./briefs/edw-18/fr/thumbnail.png","lang":"fr","summary":"Digital transformation news for June 6, 2023. This week more jobs in AI, major cyber security breaches, and cloud technology best practices."},{"id":184,"type":"News Brief","title":"2023-6-11","tags":["ai","compute","cybersecurity"],"body":"\n\n## Intelligence Artificielle\n\nMicrosoft a mis à disposition des clients gouvernementaux américains son puissant technologie OpenAI. Cette initiative permet aux organismes d'utiliser des fonctionnalités avancées à base d'intelligence artificielle en matière de traitement de langage, d'apprentissage machine et de compréhension de langage naturel. Ce partenariat vise à soutenir les organismes gouvernementaux dans leurs efforts de transformation numérique.\n\n[https://www.bloomberg.com/news/articles/2023-06-07/microsoft-offers-powerful-openai-technology-to-us-government-cloud-customers](https://www.bloomberg.com/news/articles/2023-06-07/microsoft-offers-powerful-openai-technology-to-us-government-cloud-customers)\n\nLe Premier ministre de la Roumanie a nommé le tout premier conseiller gouvernemental en intelligence artificielle au monde. Le système d'IA, nommé DORA, aidera dans les processus de prise de décision, analysera les données et fournira des recommandations sur diverses questions de politique. Cette avancée révolutionnaire reflète l'intégration croissante de la technologie de l'IA dans les opérations gouvernementales et vise à améliorer l'efficacité et l'efficience de la gouvernance.\n\n[https://www.euronews.com/next/2023/03/06/romanias-prime-minister-has-hired-the-worlds-first-ai-government-adviser-what-will-it-do](https://www.euronews.com/next/2023/03/06/romanias-prime-minister-has-hired-the-worlds-first-ai-government-adviser-what-will-it-do)\n\nL'IA surpasse les humains dans la découverte d'algorithmes de tri efficaces et leur intégration dans des bibliothèques C ++. AlphaDev, un agent de deep reinforcement learning, a dépassé les références humaines en formulant le problème sous la forme d'un jeu et en sélectionnant des instructions CPU pour créer des algorithmes optimaux. L'étude souligne le potentiel de l'IA dans l'optimisation des algorithmes.\n\n[https://www.nature.com/articles/s41586-023-06004-9](https://www.nature.com/articles/s41586-023-06004-9)\n\n## Informatique ubiquitaire\n\nMicrosoft 365, la populaire suite d'outils de productivité, a subi des pannes généralisées, causant des perturbations pour des millions d'utilisateurs. Les interruptions de service ont affecté plusieurs composants, notamment Outlook, Teams et SharePoint. Microsoft a reconnu le problème et a déclaré que leurs ingénieurs travaillaient à le résoudre.\n\n[https://www.theregister.com/2023/06/06/microsoft_365_outages/](https://www.theregister.com/2023/06/06/microsoft_365_outages/)\n\nLes interruptions des services de cloud augmentent en raison des tensions géopolitiques et des vulnérabilités d'Internet. Les restrictions de flux de données transfrontaliers et les menaces cybernétiques contribuent aux perturbations. Les experts appellent à la coopération et à l'investissement pour renforcer la résilience des services de cloud et protéger la connectivité mondiale.\n\n[https://fortune.com/2023/06/07/cloud-outages-on-the-rise-tech-geopolitics-internet/](https://fortune.com/2023/06/07/cloud-outages-on-the-rise-tech-geopolitics-internet/)\n\nLe marché africain du cloud est en passe de connaître une croissance significative d'ici 2023, soutenue par la transformation numérique et l'augmentation de l'adoption des services de cloud. Une connectivité Internet améliorée et une demande croissante de solutions basées sur le cloud dans différents secteurs contribuent à l'expansion. Les principaux acteurs investissent dans l'infrastructure et les partenariats pour saisir les opportunités offertes par le marché africain du cloud en pleine expansion.\n\n[https://finance.yahoo.com/news/rise-african-cloud-market-2023-082300194.html](https://finance.yahoo.com/news/rise-african-cloud-market-2023-082300194.html)\n\n## Cybersécurité\n\nPour aider à combattre la pénurie de professionnels qualifiés en cybersécurité, Accenture présente \"Skills to Succeed in Cybersecurity\", un programme gratuit visant à pourvoir un million d'emplois en cybersécurité de niveau débutant. L'initiative offre des ressources de formation et de certification complètes, visant à combler l'écart de compétences et à encourager davantage de personnes à poursuivre une carrière en cybersécurité.\n\n[https://fortune.com/education/articles/accenture-launches-free-cybersecurity-upskilling-program-in-effort-to-fill-1-million-entry-level-jobs/](https://fortune.com/education/articles/accenture-launches-free-cybersecurity-upskilling-program-in-effort-to-fill-1-million-entry-level-jobs/)\n\nLa Maison Blanche prolonge les délais d'attestation des logiciels de sécurité et émet des directives éclairantes pour améliorer les pratiques de cybersécurité. Les échéances étendues fournissent plus de temps pour se conformer, tandis que les directives offrent des éclairages sur les processus de mise en œuvre et d'évaluation pour les agences et les entrepreneurs fédéraux. Ainsi, pour le moment, nos applications et services sont-ils vulnérables sans que nous ne le sachions ?\n\n[https://federalnewsnetwork.com/cybersecurity/2023/06/white-house-extends-secure-software-attestation-deadlines-offers-clarifying-guidance/](https://federalnewsnetwork.com/cybersecurity/2023/06/white-house-extends-secure-software-attestation-deadlines-offers-clarifying-guidance/)\n\nLa Stratégie nationale de cybersécurité demande des changements importants pour protéger les infrastructures critiques. La collaboration entre les secteurs public et privé est mise en avant pour lutter contre les menaces cybernétiques en évolution. La gestion des risques, la résilience et l'investissement dans les technologies avancées sont soulignés pour renforcer les infrastructures critiques et assurer la sécurité nationale.\n\n[https://federalnewsnetwork.com/commentary/2023/06/national-cybersecurity-strategy-calls-for-significant-change-in-critical-infrastructure/](https://federalnewsnetwork.com/commentary/2023/06/national-cybersecurity-strategy-calls-for-significant-change-in-critical-infrastructure/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW19-fr","image":"./briefs/edw-19/fr/thumbnail.png","lang":"fr","summary":"La Transformation Numérique pour la semaine du 12 juin 2023 comprend le développement d'une force cybernétique de 1 million, de nombreuses pannes de cloud et l'écriture de code par l'IA pour la bibliothèque standard C++."},{"id":185,"type":"News Brief","title":"2023-2-12","tags":null,"body":"\n\n## Intelligence artificielle\n\nErreurs de l'IA : Mettez cela sur le compte des imprécisions dans les ensembles de données d'entrée.\n\nLes actions d'Alphabet ont chuté de 100 milliards de dollars après que le chatbot d'IA de Google, Bard, ait partagé des informations inexactes dans une vidéo promotionnelle, et qu'un événement de l'entreprise ait échoué à impressionner. Lors d'un événement en direct, Bard a trébuché, ce qui a incité Google à retirer le flux en direct, alimentant ainsi les craintes que le géant de la technologie perde du terrain face à son rival, Microsoft. Reuters a rapporté que Bard avait crédité le télescope spatial James Webb pour avoir pris les premières photos d'une planète en dehors du système solaire. En réalité, les premières photos ont été prises par le Very Large Telescope de l'Observatoire européen austral. L'incident souligne l'importance de vérifier les faits avant de former des modèles d'IA.\n\nPour ne pas être laissé derrière, Meta a lancé BlenderBot, un prototype d'IA conversationnelle qui a bientôt dit aux journalistes qu'il avait supprimé son compte Facebook après avoir appris les scandales de confidentialité de l'entreprise. \"Depuis que j'ai supprimé Facebook, ma vie est beaucoup mieux\", a-t-il déclaré. (oh oh)\n\nEn 2016, Microsoft s'est excusé après qu'un chatbot Twitter, Tay, a commencé à générer des messages racistes et sexistes. Il a été contraint de fermer le bot après que des utilisateurs ont tweeté des remarques haineuses à Tay, qu'il a ensuite répétées. Ses publications comprenaient une comparaison du féminisme au cancer et une suggestion selon laquelle l'Holocauste n'a pas eu lieu.\n\nSatya Nadella de Microsoft a déclaré à CNBC que la recherche alimentée par l'IA est la plus grande chose à avoir eu lieu dans l'entreprise au cours des neuf années où il est PDG.\n\nOtter.ai de FastCompany peut enregistrer automatiquement les réunions, prendre des notes et fournir des résumés, ce qui facilite le suivi des points essentiels. Il peut même effectuer ces tâches sur Zoom. https://www.fastcompany.com/90834773/how-to-use-ai-to-improve-employee-performance-and-engagement\n\n## Cybersécurité\n\nUne nouvelle variante du rançongiciel ESXiArgs apparaît après la sortie de l'outil de décryptage par le CISA.\n\nLes acteurs de la menace derrière l'attaque du rançongiciel ESXiArgs ont publié une version mise à jour qui chiffre plus de données et supprime l'adresse Bitcoin de la note de rançon, demandant plutôt aux victimes de les contacter sur Tox pour obtenir les informations de portefeuille. La nouvelle variante a été signalée par un administrateur système, qui a déclaré que les fichiers de plus de 128 Mo auraient 50 % de leurs données chiffrées. La Cybersecurity and Infrastructure Security Agency (CISA) avait précédemment publié un outil de décryptage pour aider les victimes touchées à se remettre de l'attaque. Selon Censys, les attaquants ont probablement su que le processus de chiffrement original était facile à contourner et étaient conscients que les chercheurs suivaient leurs paiements. https://thehackernews.com/2023/02/new-esxiargs-ransomware-variant-emerges.html\n\nMicrosoft, Google et Apple envisagent de remplacer les mots de passe traditionnels par des passkeys sécurisés pour authentifier les utilisateurs. Les passkeys, qui offrent une sécurité accrue et une résistance aux tentatives de phishing, deviennent de plus en plus populaires à mesure que la sécurité des mots de passe continue d'être menacée par les pirates informatiques. Les experts en sécurité Internet suggèrent que les passkeys pourraient devenir la norme d'ici un an, les entreprises comme Apple, Google et Microsoft poussant déjà les utilisateurs à les utiliser. https://www.cnbc.com/2023/02/11/why-apple-google-microsoft-passkey-should-replace-your-own-password.html\n\n## Intelligent Edge\n\nAT&T collabore avec Ghost Robotics pour utiliser des chiens robots afin d'améliorer la sécurité publique et la défense nationale. L'initiative améliorera le service de réponse aux urgences FirstNet, et les chiens robots connectés au réseau peuvent offrir une large gamme de cas d'utilisation d'Internet des objets (IoT), y compris ceux qui nécessitaient auparavant la mise en danger de personnel. Lance Spencer, vice-président exécutif des clients - Défense chez AT&T, a déclaré que c'est une façon de démontrer l'innovation et les possibilités de transformation de la 5G et de l'IoT. https://www.iottechnews.com/news/2023/jan/26/att-touts-robotic-dogs-public-safety-national-defense/\n\nUne étude de Juniper Research prédit que le nombre mondial de connexions 5G IoT dépassera 100 millions d'ici 2026, en hausse de 1 100% par rapport aux 17 millions de connexions en 2023. Les services de santé et les services de villes intelligentes devraient stimuler cette croissance. Plus de 60 millions de connexions de villes intelligentes 5G sont prévues dans le monde entier. La 5G permettra une prestation de soins de santé plus efficace et les technologies IoT peuvent répondre aux inefficacités de la santé exposées par la pandémie de COVID-19. Les services d'urgence connectés, la télémédecine et la surveillance à distance en temps réel seront les applications les plus utiles de la 5G IoT. https://www.iottechnews.com/news/2023/jan/24/5g-iot-connections-exceed-100m-by-2026/\n\n\n","guests":null,"link":"/brief-EDW2-fr","image":"./briefs/edw-2/fr/thumbnail.png","lang":"fr","summary":"Summary"},{"id":186,"type":"News Brief","title":"2023-6-18","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Informatique omniprésente\n\nInformationWeek explore le débat IA vs low-code/no-code pour les développeurs. Bien que les plateformes low-code/no-code offrent simplicité et rapidité, les outils alimentés par l'IA apportent une automatisation et une intelligence avancées. L'article examine les avantages et les inconvénients de chaque approche, soulignant comment trouver le bon équilibre peut aider les développeurs à innover dans le domaine de la DevOps.\n\n[https://www.informationweek.com/devops/dos-won-t-hunt-is-ai-better-than-low-code-no-code-for-developers-](https://www.informationweek.com/devops/dos-won-t-hunt-is-ai-better-than-low-code-no-code-for-developers-)\n\nCRN relate les 10 plus grosses pannes du Cloud en 2023 (jusqu'à présent) de manière légère. De \"Cloudy with a Chance of Downtime\" à \"The Great Data Storm\", ces pannes ont offert aux utilisateurs de Cloud des opportunités inattendues pour pratiquer leur patience et redécouvrir les plaisirs des activités hors ligne. L'hiver et le printemps ont été difficiles. Espérons des cieux plus dégagés dans le Cloud à venir !\n\n[https://www.crn.com/news/cloud/the-10-biggest-cloud-outages-of-2023-so-far-](https://www.crn.com/news/cloud/the-10-biggest-cloud-outages-of-2023-so-far-)\n\nOracle connait une croissance impressionnante des revenus de cloud dans différents secteurs, grâce à l'adoption de l'IA. Des soins de santé prévisionnels à l'optimisation de la chaîne d'approvisionnement avec l'automatisation intelligente, les solutions cloud d'Oracle permettent aux entreprises de tous secteurs de se développer. Le futur semble prometteur alors que l'IA continue à stimuler l'innovation et à propulser la transformation digitale.\n\n[https://www.ciodive.com/news/Oracle-cloud-revenue-growth-industry-verticals-AI/652885/](https://www.ciodive.com/news/Oracle-cloud-revenue-growth-industry-verticals-AI/652885/)\n\n## Gestion de données\n\nDans la quête d'une IA responsable, TDWI explore les considérations clés. De l'éthique des données et de la réduction des biais à l'interprétabilité et à la responsabilité, une IA responsable nécessite une approche holistique. Les organisations sont encouragées à accorder la priorité à la transparence, à l'équité et à la surveillance humaine pour garantir que les systèmes d'IA servent d'outils fiables et bénéfiques dans notre paysage numérique en constante évolution.\n\n[https://tdwi.org/articles/2023/06/08/adv-all-responsible-ai-0608.aspx](https://tdwi.org/articles/2023/06/08/adv-all-responsible-ai-0608.aspx)\n\nDataStax présente un traducteur de schéma GPT pour sa plateforme de streaming Astra basée sur Apache Pulsar. Cette nouvelle addition améliore les capacités de gestion des données en permettant une intégration transparente entre les schémas et le populaire modèle de langage GPT. Les utilisateurs peuvent désormais exploiter la puissance du traitement du langage naturel dans leurs flux de données, améliorant ainsi l'efficacité et les insights.\n\n[https://www.infoworld.com/article/3699748/datastax-adds-schema-gpt-translator-to-apache-pulsar-based-astra-streaming.html](https://www.infoworld.com/article/3699748/datastax-adds-schema-gpt-translator-to-apache-pulsar-based-astra-streaming.html)\n\nNe pas comprendre les lois complexes sur la vie privée a un coût. Microsoft fait face à une amende de 20 millions de dollars de la part de la FTC pour des violations de la vie privée des enfants sur Xbox. La société aurait omis d'obtenir le consentement des parents pour la collecte de données et manquait de garanties suffisantes. Cette lourde peine rappelle aux organisations de donner la priorité et de respecter les droits à la vie privée des enfants dans le domaine numérique.\n\n[https://www.cpomagazine.com/data-protection/20-million-fine-issued-to-microsoft-by-ftc-over-xbox-childrens-privacy-violations/](https://www.cpomagazine.com/data-protection/20-million-fine-issued-to-microsoft-by-ftc-over-xbox-childrens-privacy-violations/)\n\n## Intelligence Artificielle\n\nLe nouveau rapport de McKinsey met en évidence l'immense potentiel économique de l'IA générative, en la positionnant comme la prochaine frontière pour la productivité. Cette technologie transformative a la capacité de stimuler une croissance significative, de révolutionner les industries et de débloquer des niveaux inédits d'innovation et de créativité. Cependant, quatre secteurs sont à risque de voir une forte suppression des travailleurs humains. À savoir, les opérations client, le marketing et les ventes, l'ingénierie logicielle et la R&D.\n\n[https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier)\n\nL'Europe fait un pas audacieux en régulant l'IA, défiant le pouvoir des géants de la technologie. Les nouvelles réglementations visent à répondre aux préoccupations liées à la transparence, l'équité et la responsabilité. En affirmant son contrôle sur les applications d'IA, l'Europe cherche à façonner une utilisation éthique et responsable de l'intelligence artificielle dans le paysage numérique. En revanche, des pays comme le Japon embrassent l'IA en diminuant la réglementation sur la collecte de données, ne voulant pas être laissés pour compte dans ce grand changement de paradigme.\n\n[https://www.datacenterknowledge.com/artificial-intelligence/europe-moves-ahead-ai-regulation-challenging-tech-giants-power](https://www.datacenterknowledge.com/artificial-intelligence/europe-moves-ahead-ai-regulation-challenging-tech-giants-power)\n\nBCG vient de réaliser l'une des études les plus complètes sur l'IA à ce jour, interrogeant 13 000 personnes, des dirigeants aux employés les moins bien payés, dans plus de 18 pays. Voici leurs conclusions : plus de 80% des dirigeants utilisent l'IA au travail, alors que seulement 20% des travailleurs de première ligne le font. Les gens sont plus optimistes et ne sont pas très préoccupés par l'IA. Les travailleurs de première ligne risquent d'être remplacés s'ils ne mettent pas à jour leurs compétences et commencent à utiliser l'IA dans leur travail quotidien.\n\n[https://www.bcg.com/publications/2023/what-people-are-saying-about-ai-at-work](https://www.bcg.com/publications/2023/what-people-are-saying-about-ai-at-work)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW20-fr","image":"./briefs/edw-20/fr/thumbnail.png","lang":"fr","summary":"Nouvelles de la Transformation Digitale de la semaine du 19 juin 2023 comprenant des histoires de gestionnaires remplaçant les travailleurs par l'IA, des pannes Cloud à nouveau, et des lois sur la vie privée qui mordent les géants de la technologie."},{"id":187,"type":"News Brief","title":"2023-6-25","tags":["ai","edge","cybersecurity"],"body":"\n\n## Intelligence Artificielle\n\nLa relation entre l'IA et les cryptomonnaies évolue en une dynamique complexe alors que les régulateurs luttent avec leur coexistence. Alors que les technologies d'IA offrent un potentiel de détection de fraude et d'évaluation des risques dans l'espace crypto, des défis réglementaires surgissent en raison de la nature décentralisée des cryptomonnaies. Trouver un équilibre entre l'innovation et la supervision demeure un défi crucial.\n\n[https://www.datacenterknowledge.com/artificial-intelligence/ai-and-crypto-are-becoming-regulatory-frenemies](https://www.datacenterknowledge.com/artificial-intelligence/ai-and-crypto-are-becoming-regulatory-frenemies)\n\nLes modèles ChatGPT, Meena de Google, Bing Chat et GPT-3 d'OpenAI se sont affrontés lors d'un test en situation réelle pour fournir des réponses précises et utiles. ChatGPT est ressorti en tête des résultats, démontrant sa capacité à comprendre et générer des réponses cohérentes. Toutefois, des limites ont été observées dans tous les modèles, soulignant les défis continus liés au développement de chatbots répondant aux attentes des utilisateurs.\n\n[https://www.zdnet.com/article/chatbot-showdown-chatgpt-google-bard-and-bing-chat-put-to-a-real-world-test/](https://www.zdnet.com/article/chatbot-showdown-chatgpt-google-bard-and-bing-chat-put-to-a-real-world-test/)\n\nHPE a annoncé le lancement d'un service de cloud supercalculateur d'IA appelé HPE Cray Accelerated Insight. Ce service vise à donner aux organisations un accès facile à des ressources informatiques puissantes pour les charges de travail d'IA. En utilisant l'architecture Shasta de Cray, le service cloud offre des performances élevées et une grande évolutivité pour accélérer la recherche et le développement d'IA.\n\n[https://www.datacenterknowledge.com/cloud/hpe-unveils-ai-supercomputer-cloud-service](https://www.datacenterknowledge.com/cloud/hpe-unveils-ai-supercomputer-cloud-service)\n\n## Cybersécurité\n\nLa guerre cybernétique continue !! Le groupe de pirates informatiques Clop a ciblé les agences gouvernementales américaines, volant des données grâce à une cyberattaque sur le logiciel MoveIT. Cette violation sophistiquée met en évidence la menace croissante des attaques de rançongiciels, Clop utilisant des tactiques avancées telles que le double chantage. L'incident souligne la nécessité d'améliorer les mesures de cybersécurité gouvernementales.\n\n[https://www.wired.com/story/clop-moveit-hack-us-agencies-data-theft/](https://www.wired.com/story/clop-moveit-hack-us-agencies-data-theft/)\n\nLe fabricant chinois de puces Hualan a été ajouté à la Liste des entités des États-Unis en raison de préoccupations liées à la sécurité nationale. Hualan est spécialisé dans les puces de chiffrement, qui jouent un rôle crucial dans la protection des informations sensibles. Cette décision reflète la rivalité technologique en cours entre les États-Unis et la Chine et pourrait avoir des implications significatives pour les chaînes d'approvisionnement mondiales et l'industrie du chiffrement.\n\n[https://www.wired.com/story/hualan-encryption-chips-entity-list-china/](https://www.wired.com/story/hualan-encryption-chips-entity-list-china/)\n\nLe télétravail a accru la surface d'attaque. Une vulnérabilité dans Microsoft Teams a été découverte, permettant la livraison de logiciels malveillants via des comptes externes. Le bug permet aux attaquants d'envoyer des messages malveillants contenant des liens malveillants, compromettant potentiellement les systèmes des utilisateurs. Microsoft a depuis publié un correctif pour résoudre le problème et demande aux utilisateurs de mettre à jour leur logiciel pour assurer leur sécurité.\n\n[https://www.bleepingcomputer.com/news/security/microsoft-teams-bug-allows-malware-delivery-from-external-accounts/](https://www.bleepingcomputer.com/news/security/microsoft-teams-bug-allows-malware-delivery-from-external-accounts/)\n\n## Calcul en bordure de réseau\n\nSpirent, un leader dans la fourniture de solutions de test et de mesure, a introduit une solution de surveillance de la performance via les ondes. Le système permet la surveillance en temps réel et l'analyse de la performance du réseau sans fil, y compris la latence, le débit et la couverture. Cette solution vise à soutenir la demande croissante de connectivité sans fil fiable et de haute qualité, notamment dans le contexte des technologies émergentes telles que la 5G et l'informatique Edge.\n\n[https://www.edgecomputing-news.com/2023/06/19/spirent-launches-over-the-air-performance-monitoring-solution/](https://www.edgecomputing-news.com/2023/06/19/spirent-launches-over-the-air-performance-monitoring-solution/)\n\nZscaler a dépassé Cisco dans la course aux revenus sur le marché de la Secure Access Service Edge (SASE). L'architecture cloud-native de Zscaler, ses offres de sécurité complètes et sa capacité à relever les défis modernes de mise en réseau ont contribué à son succès. La demande croissante d'accès distant sécurisé et de solutions de sécurité dématérialisées a propulsé la croissance de Zscaler, le positionnant comme leader dans l'espace SASE.\n\n[https://www.sdxcentral.com/articles/analysis/how-zscaler-finally-topped-cisco-in-the-sase-revenue-race/2023/06/](https://www.sdxcentral.com/articles/analysis/how-zscaler-finally-topped-cisco-in-the-sase-revenue-race/2023/06/)\n\nAT&T, Dell et VMware collaborent pour simplifier le déploiement du bord de 5G. Leur solution combinée vise à rationaliser la mise en œuvre des réseaux 5G, en exploitant les services d'AT&T, l'expertise en infrastructure de Dell et les capacités logicielles de VMware pour des applications de calcul de bord efficaces et sans faille.\n\n[https://www.networkworld.com/article/3695740/att-dell-and-vmware-team-to-simplify-5g-edge-deployments.html](https://www.networkworld.com/article/3695740/att-dell-and-vmware-team-to-simplify-5g-edge-deployments.html)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW21-fr","image":"./briefs/edw-21/fr/thumbnail.png","lang":"fr","summary":"Nouvelles de la semaine du 26 juin 2023, dans la transformation numérique, y compris l'augmentation des attaques dans la guerre cybernétique, tout le monde sautant sur la vague de l'IA générative et les réseaux radio virtualisés."},{"id":188,"type":"News Brief","title":"2023-7-9","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\nCi troviamo nel pieno della rivoluzione digitale. Durante questo periodo, può essere difficile districarsi tra tutte le promesse e scoprire cosa funziona, cosa sarà ancora valido fra due anni e cosa contribuisce alla tua organizzazione. Molte organizzazioni IT hanno bisogno di aiuto per comprendere messaggi confusi e tecnologie in conflitto. Noi aiutiamo le organizzazioni a fare chiarezza e abbracciare la trasformazione digitale. Il mondo della trasformazione digitale è in continua evoluzione e devi sapere cosa è fattibile oggi. Ascolta il nostro podcast settimanale per le ultime notizie sulla sicurezza informatica, le comunicazioni avanzate, la gestione dei dati, l'intelligenza artificiale, il cloud computing e la tecnologia edge.\n## Intelligence artificielle\n\nSelon la dernière découverte de la Banque d'Amérique, les téléchargements de l'application ChatGPT montrent des signes d'un rythme tranquille. Cet outil de conversation alimenté par l'IA prend une pause, ce qui fait se demander aux analystes si les robots bavards ont épuisé tous les sujets de discussion ou si les utilisateurs aspirent à une interaction humaine à l'ancienne.\n\n[https://www.cnbc.com/2023/07/05/chatgpt-app-downloads-are-slowing-down-bofa-finds-.html](https://www.cnbc.com/2023/07/05/chatgpt-app-downloads-are-slowing-down-bofa-finds-.html)\n\nNew York City déclare la guerre aux algorithmes biaisés ! TechCrunch rapporte que leur loi anti-discrimination pour les algorithmes de recrutement est désormais pleinement en vigueur. Les algorithmes feraient mieux de se méfier car ils ne peuvent plus discriminer sur le marché de l'emploi. C'est comme un film de superhéros, mais nous avons des lignes de code qui se battent pour la justice au lieu de capes !\n\n[https://techcrunch.com/2023/07/05/nycs-anti-bias-law-for-hiring-algorithms-goes-into-effect/](https://techcrunch.com/2023/07/05/nycs-anti-bias-law-for-hiring-algorithms-goes-into-effect/)\n\nIntel dévoile sa dernière création : un modèle d'IA générative en 3D promis à révolutionner le monde virtuel ! Selon l'annonce d'Intel, cette technologie de pointe apporte de nouvelles dimensions à l'IA en créant des modèles 3D époustouflants. Avec cette innovation, Intel pousse la réflexion \"hors des sentiers battus\" à un nouveau niveau. Préparez-vous à une extravagance virtuelle !\n\n[https://www.intel.com/content/www/us/en/newsroom/news/intel-introduces-3d-generative-ai-model.html](https://www.intel.com/content/www/us/en/newsroom/news/intel-introduces-3d-generative-ai-model.html)\n\n## Cybersécurité\n\nL'énergie solaire met en lumière une vulnérabilité cachée ! SecurityWeek révèle qu'une faille exploitée dans un produit d'énergie solaire peut exposer les organisations énergétiques à des cyberattaques. Il semblerait que même le soleil ne soit pas à l'abri des pirates informatiques. Espérons qu'ils trouvent une solution solaire pour sécuriser nos réseaux énergétiques et mettre les hackers à l'ombre !\n\n[https://www.securityweek.com/exploited-solar-power-product-vulnerability-could-expose-energy-organizations-to-attacks/](https://www.securityweek.com/exploited-solar-power-product-vulnerability-could-expose-energy-organizations-to-attacks/)\n\nUh-oh, les agences de cybersécurité lancent l'alerte ! Selon The Hacker News, une nouvelle menace a émergé faisant frissonner nos épines numériques. Il est temps de se préparer et renforcer nos défenses cybernétiques car ces pirates sournois ne sont pas là pour de bonnes intentions. Restez vigilants, chers amis, et contrecarrons leurs plans diaboliques avec nos super pouvoirs cybernétiques !\n\n[https://thehackernews.com/2023/07/cybersecurity-agencies-sound-alarm-on.html](https://thehackernews.com/2023/07/cybersecurity-agencies-sound-alarm-on.html)\n\nCISA détecte une cyber-impulsion dans les rapports de l'agence ! Federal News Network déclare que de plus en plus d'agences font automatiquement des rapports dans le tableau de bord CDM (Diagnostic et Atténuation en Continu). Nos défenses cybernétiques deviennent plus intelligentes, les agences se mobilisent. Félicitations à ceux qui veillent sur la cybersécurité et rendent le monde numérique plus sûr, un rapport à la fois !\n\n[https://federalnewsnetwork.com/cybersecurity/2023/07/cisa-sees-uptick-in-agencies-automatically-reporting-into-cdm-dashboard/](https://federalnewsnetwork.com/cybersecurity/2023/07/cisa-sees-uptick-in-agencies-automatically-reporting-into-cdm-dashboard/)\n\n## Informatique omniprésente\n\nIl semble que les États-Unis dressent une barrière numérique contre la Chine ! Selon Cointelegraph, des rapports font état de plans visant à limiter l'accès de la Chine aux services d'informatique en nuage. C'est comme une partie virtuelle de cache-cache mais avec de graves implications géopolitiques. Voyons qui sortira victorieux de cette lutte acharnée basée sur le cloud. Attrapez votre popcorn, mesdames et messieurs, car la bataille technologique s'intensifie !\n\n[https://cointelegraph.com/news/us-reportedly-plans-to-restrict-china-s-access-to-cloud-computing-services](https://cointelegraph.com/news/us-reportedly-plans-to-restrict-china-s-access-to-cloud-computing-services)\n\nLes terres rares provoquent un sentiment de déjà vu dans le différend commercial entre les États-Unis et la Chine ! Intereconomics met en lumière la situation, soulignant l'importance de ces minéraux cruciaux dans diverses industries. Il semble que l'histoire se répète alors que les terres rares deviennent un point central de cette guerre commerciale en cours. Préparez-vous à une période agitée alors que les États-Unis et la Chine naviguent à nouveau sur ce terrain familier. Trouveront-ils une résolution ou tourneront-ils en boucle sans fin ? Seul le temps le dira !\n\n[https://www.intereconomics.eu/contents/year/2019/number/6/article/rare-earths-in-the-trade-dispute-between-the-us-and-china-a-deja-vu.html](https://www.intereconomics.eu/contents/year/2019/number/6/article/rare-earths-in-the-trade-dispute-between-the-us-and-china-a-deja-vu.html)\n\nIBM est en train de rendre les erreurs en calcul quantique une chose du passé ! Selon Network World, ils font la promotion de techniques de correction d'erreurs qui promettent des performances plus significatives dans le domaine quantique. C'est comme un sortilège magique pour minimiser ces erreurs agaçantes et libérer le véritable potentiel de l'informatique quantique. Préparez-vous à un bond quantique en avant dans le monde de la technologie !\n\n[https://www.networkworld.com/article/3699789/ibm-touts-error-mitigation-for-greater-quantum-computing-performance.html](https://www.networkworld.com/article/3699789/ibm-touts-error-mitigation-for-greater-quantum-computing-performance.html)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW23-fr","image":"./briefs/edw-23/fr/thumbnail.png","lang":"fr","summary":"Voici les dernières nouvelles sur la transformation numérique pour le 10 juillet 2023. La guerre commerciale en cours commence à avoir un impact sur les services cloud. De plus, il y a des spéculations selon lesquelles l'engouement autour de l'IA générative pourrait ralentir. Sur le front de la cybersécurité, il y a des inquiétudes concernant notre capacité à rester en avance sur les acteurs malveillants."},{"id":189,"type":"News Brief","title":"2023-7-16","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Intelligence Artificielle\n\nAI a-t-il maintenant été béni par la religion ? Le Vatican se met au numérique avec un guide divin ! Gizmodo rapporte que le pape François et le Vatican ont publié un document sur l'éthique de l'IA, apportant ainsi une touche sacrée au monde de l'intelligence artificielle. C'est comme un sermon technologique sur l'importance des pratiques éthiques en matière d'IA. Alléluia pour la justice numérique !\n\n[Lire la suite](https://gizmodo.com/pope-francis-vatican-releases-ai-ethics-1850583076)\n\nElon Musk frappe encore, cette fois avec une société d'IA ! Le magnat de la technologie a lancé une nouvelle entreprise axée sur l'intelligence artificielle qui va \"chercher la vérité\". Il semble que Musk ne puisse pas résister à plonger dans des projets futuristes. S'agira-t-il d'un autre bouleversement ou simplement d'une autre plume à son chapeau futuriste ? Seul le temps le dira !\n\n[Lire la suite](https://www.cnn.com/2023/07/12/tech/elon-musk-ai-company/index.html)\n\nL'IA générative devient \"folle\" après quelques itérations de données artificielles ! Tom's Hardware rapporte que l'IA générative commence à produire des résultats extrêmement bizarres lorsqu'elle est entraînée sur des données artificielles pendant plus de cinq itérations. L'imagination de l'IA semble s'être emballée, dévoilant un côté fantasque et inattendu. Espérons qu'elle ne nous offre pas des licornes créées par l'IA qui sèment le chaos !\n\n[Lire la suite](https://www.tomshardware.com/news/generative-ai-goes-mad-when-trained-on-artificial-data-over-five-times)\n\n## Sécurité informatique\n\nDans le domaine des cyber-forteresses, Forbes couronne Intel comme le champion en titre ! La liste 2023 des entreprises américaines les plus cyber-sécurisées met en lumière les compétences de défense inégalées d'Intel. Avec sa maîtrise technologique et une forteresse d'innovation, Intel se dresse fièrement, prêt à affronter tout adversaire numérique. Ils enfilent leurs capes virtuelles et protègent nos données des cyber-vilains avec style et finesse. Il n'est pas étonnant qu'Intel fasse parler de lui dans le monde de la cybersécurité !\n\n[Lire la suite](https://www.forbes.com/sites/hnewman/2023/06/08/meet-americas-most-cybersecure-companies-2023/?sh=dd8bc202cf60)\n\nLe plan national de lutte contre la cybercriminalité a été dévoilé ! Aidez-nous à mettre en œuvre le plan stratégique pour la Stratégie nationale de cybersécurité qui a été publié. C'est un manuel stratégique pour se défendre contre les menaces numériques, en détaillant les mesures pour protéger nos sphères virtuelles. Unissons-nous derrière ce plan et renforçons les défenses cybernétiques de notre nation. En avant vers un avenir numérique plus sûr !\n\n[Lire la suite](https://www.helpnetsecurity.com/2023/07/13/national-cybersecurity-strategy-implementation-plan-published/)\n\nLe champ de bataille de la cybersécurité entre les États-Unis et la Chine s'intensifie au milieu de tensions croissantes. The Hacker News fait état de compromissions alarmantes de comptes de messageries électroniques au sein des agences gouvernementales américaines, alimentant davantage les inquiétudes concernant la guerre cybernétique en cours. C'est un affrontement à enjeux élevés entre des géants numériques, soulignant le besoin urgent de défenses solides et d'une coopération internationale face aux menaces persistantes en matière de cybercriminalité.\n\n[Lire la suite](https://thehackernews.com/2023/07/us-government-agencies-emails.html)\n\n## Le calcul périphérique\n\nInfineon et Edge Impulse s'associent pour libérer la puissance de l'IA à la périphérie ! Edge Computing News déclare que ce partenariat dynamique vise à étendre les capacités d'IA à la périphérie d'Infineon. C'est comme une union parfaite dans le monde de la technologie, combinant l'expertise d'Infineon avec les solutions de pointe d'Edge Impulse. Préparez-vous à l'IA à la périphérie, révolutionnant la façon dont nous traitons les données et débloquant de nouveaux domaines de l'innovation !\n\n[Lire la suite](https://www.edgecomputing-news.com/2023/07/10/infineon-partners-with-edge-impulse-to-extend-its-edge-ai-capabilities/)\n\nAccrochez-vous à vos chapeaux car le marché de l'informatique en périphérie est sur le point d'exploser ! Les actualités sur l'informatique en périphérie révèlent qu'en 2028, cette industrie florissante vaudra la somme colossale de 111,3 milliards de dollars. C'est comme une ruée vers l'or de l'informatique en périphérie, avec des entreprises se précipitant pour capitaliser sur le potentiel immense de la puissance de traitement décentralisée. Préparez-vous à un changement de paradigme alors que l'informatique en périphérie prend la tête de la révolution numérique !\n\n[Lire la suite](https://www.edgecomputing-news.com/2023/07/10/edge-computing-market-to-be-worth-111-3-billion-by-2028/)\n\nSeoul Robotics étend la technologie de transport avec sa technologie avancée de perception 3D. EdgeIR rapporte sur cette innovation révolutionnaire qui apporte une perception de profondeur améliorée aux systèmes de transport. Alors que les véhicules commencent à utiliser cette technologie de pointe de Seoul Robotics, nous pouvons anticiper des trajets plus sûrs et plus intelligents à l'avenir.\n\n[Lire la suite](https://www.edgeir.com/seoul-robotics-develops-3d-perception-tech-to-boost-transportation-systems-20230711)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW24-fr","image":"./briefs/edw-24/fr/thumbnail.png","lang":"fr","summary":"Dans les actualités de la semaine du 17 juillet 2023 sur la transformation numérique, le Pape François offre des orientations sur l'éthique de l'IA, Intel demeure la forteresse cybernétique de l'Amérique et le marché de l'informatique périphérique est sur le point de décoller !"},{"id":190,"type":"News Brief","title":"2023-7-23","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Intelligence Artificielle\n\n\nMeta, le pionnier de l'IA, impressionne la communauté technologique avec une sortie révolutionnaire - en présentant Llama 2, le dernier modèle LLM open-source. En déverrouillant le pouvoir de compréhension du langage pour tous les développeurs, cette innovation révolutionnaire promet de révolutionner les projets d'IA. Dites adieu à la complexité et accueillez une formation d'IA fluide. Préparez-vous à surfer sur la vague des modèles open-source LLM et à assister à l'avenir de l'IA se déployer ! [Lire la suite](https://www.artificialintelligence-news.com/2023/07/19/meta-launches-llama-2-open-source-llm/)\n\n\nLe Forbes Tech Council tire la sonnette d'alarme alors que l'IA génératrice se concentre sur les données personnelles. Leur article captivant plonge dans l'invasion imminente des données et ses conséquences potentielles. Préparez-vous à la montée révolutionnaire du contenu généré par l'IA, offrant des opportunités mais aussi des défis pour la confidentialité des données. Restez informé et vigilant alors que nous naviguons dans ce nouveau monde courageux de l'#IAgénératrice et protégeons notre avenir numérique. [Lire la suite](https://www.forbes.com/sites/forbestechcouncil/2023/07/20/generative-ai-is-coming-for-people-data-are-you-ready/?sh=3e6f95421573)\n\n\nGrève d'Hollywood suite aux inquiétudes liées aux emplois de l'IA. Le film estival phare \"Mission: Impossible - Dead Reckoning\" explore le potentiel et les risques de l'IA. S'agit-il d'un récit d'avertissement ou d'un aperçu de notre avenir? Les acteurs et écrivains craignent que leurs emplois et leurs images ne soient remplacés. Restez à l'écoute pour les mises à jour. [Lire la suite](https://www.wired.com/story/mission-impossible-dead-reckoning-is-the-perfect-ai-panic-movie/)\n\n## CyberSécurité\n\n\nLe Guardian rapporte une importante cyberattaque alors que des pirates chinois ciblent des responsables américains, y compris l'Ambassadeur à Beijing. La violation soulève des préoccupations concernant la cybersécurité et les relations internationales. Les autorités enquêtent sur l'ampleur de l'attaque et ses implications potentielles. Restez à l'écoute pour les mises à jour sur ce développement important et préoccupant. [Lire la suite](https://www.theguardian.com/us-news/2023/jul/20/ambassador-to-beijing-among-us-officials-hit-by-chinese-hackers)\n\n\nDernières nouvelles : Une attaque cybernétique terrifiante se déroule ! Les infrastructures critiques sont sous siège alors que les pirates exploitent une vulnérabilité zero-day dangereuse de Citrix. La panique s'empare des experts en cybersécurité qui se précipitent contre la montre pour contrecarrer les conséquences catastrophiques de l'attaque. Préparez-vous à en subir les retombées et maintenez vos défenses numériques prêtes ! #CitrixZeroDay #CybersecurityEmergency [Lire la suite](https://www.securityweek.com/citrix-zero-day-exploited-against-critical-infrastructure-organization/)\n\n\nCyware apporte des nouvelles alarmantes sur \"HotRAT\", un script caché qui se cache dans les logiciels piratés. Les cybercriminels exploitent cette tactique sournoise pour compromettre les utilisateurs inconscients. Le paysage des menaces s'intensifie, incitant à la prudence lors du téléchargement de programmes piratés. Protégez votre refuge numérique et évitez de tomber victime de cette manœuvre insidieuse. Restez informé et restez en sécurité ! #HotRAT #CybersecurityThreat [Lire la suite](https://cyware.com/news/hotrat-as-hidden-script-in-cracked-software-b2baa5b3)\n\n## Gestion des données\n\n\nTDWI révèle l'aube d'une révolution de la gestion des données avec 'Arch-All Data Fabric'. Dans ce rapport novateur, des experts dévoilent le plan directeur de la gestion des données de nouvelle génération. Assistez à l'intégration de sources de données diverses, propulsant les entreprises à des sommets inédits. Restez à l'écoute pour des insights sur l'avenir axé sur les données ! #DataFabric #GestionDonnéesNouvelleGénération [Lire la suite](https://tdwi.org/articles/2023/07/20/arch-all-data-fabric-how-to-architect-next-generation-data-management.aspx)\n\n\n\nHammerspace sécurise une étape révolutionnaire avec un financement institutionnel initial de 56,7 millions de dollars. Cette réalisation transformative débloque des opportunités commerciales cachées au sein des données non structurées. Découvrez la puissance de leurs solutions innovantes, permettant une gestion transparente des données non structurées pour les entreprises. Préparez-vous à une nouvelle ère d'utilisation des données et de croissance ! #Hammerspace #DataInnovation #FundingSuccess [Lire la suite](https://hammerspace.com/hammerspace-raises-56-7m-in-first-institutional-funding-unlocks-business-opportunities-hidden-in-unstructured-data/)\n\n\nLa grande révélation de Lenovo: de nouvelles solutions de gestion des données conçues pour booster les charges de travail de l'IA ! Datanami rapporte ses offres de pointe, qui donnent aux entreprises un potentiel guidé par l'IA. Plongez dans les dernières innovations qui promettent de révolutionner la manipulation des données pour les projets d'IA. Adoptez une nouvelle ère d'efficacité et d'intelligence dans le paysage technologique ! #Lenovo #GestiondesDonnées #ChargesdeTravailIA [Lire la suite](https://www.datanami.com/this-just-in/lenovo-unveils-new-data-management-solutions-to-enable-ai-workloads/)\n\n## Adoptant le Podcast Numérique\n\nDans cet épisode, Darren revoit une interview qu'il a menée avec chatGPT et fait un suivi avec une interview similaire mettant en vedette Google Bard. La comparaison suscite de l'intrigue, surtout lorsqu'elle est associée aux idées de l'épisode 122 sur l'adoption de la transformation numérique. [Episode 147](https://www.embracingdigital.org/episode-EDT147) [Episode 122](https://www.embracingdigital.org/episode-EDT122)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW25-fr","image":"./briefs/edw-25/fr/thumbnail.png","lang":"fr","summary":"Cette semaine, du 24 juillet 2023, dans les actualités de la transformation numérique, il y a eu des développements dans l'IA générative sur le lieu de travail, ainsi qu'une attaque cybernétique chinoise sur l'ambassade américaine. De plus, les tissus de données commencent à émerger en tant que tendance."},{"id":191,"type":"News Brief","title":"2023-7-30","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Communications avancées\n\n\nIntel et Ericsson s'associent pour créer une meilleure connectivité internet et des expériences sur les appareils. Leur collaboration vise à répondre à la demande croissante d'internet haut débit et de connexions fiables, en promettant des avancées technologiques passionnantes. Grâce à leur expertise combinée, les utilisateurs peuvent s'attendre à des téléchargements plus rapides, des appels vidéo plus fluides et une amélioration globale des performances pour les téléphones, les ordinateurs et autres gadgets. Préparez-vous à un avenir plus connecté et sans faille ! [Lire la suite](http://finance.yahoo.com/news/intel-ericsson-expand-collaboration-advance-201000466.html)\n\n\nPréparez-vous pour l'essor de la 5G ! Selon une nouvelle étude, le marché mondial de l'intégration de systèmes 5G devrait atteindre la somme phénoménale de 115,60 milliards de dollars d'ici 2032. Les entreprises se précipitent pour adopter cette technologie Internet ultra-rapide pour les villes intelligentes, les soins de santé et bien plus encore. Les fabricants se mettent également à niveau avec des technologies innovantes telles que les robots et les données massives. La demande de services d'intégration 5G est en hausse, promettant des moments passionnants à venir ! [Lire la suite](https://finance.yahoo.com/news/global-5g-system-integration-market-100000051.html)\n\n\nNouvelle excitante de T-Mobile ! Leur technologie révolutionnaire 5G promet un internet ultra-rapide, des vitesses de 3,3 GBits/Sec et une connectivité sans faille. Grâce à cette innovation, le téléchargement et le streaming seront ultra-rapides, et les appels vidéo et les jeux en ligne seront plus fluides. Préparez-vous à une nouvelle ère de connectivité et d'expériences internet uniques ! [Lire la suite](https://voip.review/2023/07/26/tmobile-introduces-groundbreaking-5g-tech/)\n\n## Cybersécurité\n\n\nGPT et la cybersécurité : Un futur excitant et inquiétant ! Les experts se passionnent pour GPT, un puissant système d'IA destiné à révolutionner la cybersécurité. La conférence d'Una-May O'Reilly a mis en lumière l'impact de GPT sur la sécurité, montrant qu'il rendra les défenses plus innovantes et efficaces. Mais il y a une tournure : les hackers peuvent également l'utiliser ! L'IA pourrait renforcer les deux côtés dans une course aux armements cybernétiques, nous laissant dans l'incertitude. Préparez-vous pour le futur passionnant et inquiétant de la cybersécurité ! [Lire la suite](https://www.forbes.com/sites/johnwerner/2023/07/28/whos-the-authority-on-gpt-and-cybersecurity-is-it--gpt/)\n\n\nDe nouvelles règles de la Securities and Exchange Commission (SEC) exigent que les entreprises publiques divulguent toute violation de cybersécurité pouvant avoir un impact sur leurs bénéfices dans un délai de quatre jours. Cette mesure vise à protéger les investisseurs et à accroître la transparence des risques liés à la cybersécurité. Des retards sont autorisés en cas de risques liés à la sécurité nationale. La règle exige également que les entreprises partagent des informations sur leur gestion et leur expertise en matière de cybersécurité. [Lire la suite](https://apnews.com/article/sec-cybersecurity-breach-disclosure-risk-hacking-bb6252463637793bfdc8ace5bfcbe7df)\n\n\nDans une actualité de dernière minute, les agences de cybersécurité en Australie et aux États-Unis ont mis en garde contre une faiblesse critique dans les applications web. Les cybercriminels peuvent exploiter la vulnérabilité de référence directe d'objet non sécurisée pour accéder de manière non autorisée à des informations confidentielles. Restez vigilant et protégez vos données avec des protocoles d'authentification et d'autorisation robustes. [Lire la suite](https://thehackernews.com/2023/07/cybersecurity-agencies-warn-against.html)\n\n## Informatique ubiquitaire\n\n\nL'IA est de plus en plus utilisée en programmation pour améliorer l'efficacité et réduire les dépenses. Cependant, la dépendance vis-à-vis du code précédent peut entraîner des erreurs et entraver la créativité dans le développement d'applications. Le code généré par l'IA peut ne pas être optimisé pour la plateforme, il est donc essentiel de trouver un équilibre entre les avantages de l'IA et l'expertise humaine pour des applications efficaces. [Lire la suite](https://www.infoworld.com/article/3703611/the-lost-art-of-cloud-application-engineering.html)\n\n\nMicrosoft domine les charges de travail basées sur l'IA dans le cloud, selon Nadella ! Microsoft prend la tête des charges de travail basées sur l'IA dans le cloud, selon son PDG, Satya Nadella. L'expertise du géant de la technologie dans les tâches pilotées par l'IA fait sensation dans l'industrie. En exploitant la puissance du cloud, Microsoft propose des solutions d'IA de pointe aux entreprises et aux utilisateurs. Grâce à leur approche innovante, ils façonnent l'avenir de la technologie de l'IA. [Lire la suite](https://www.cnbc.com/2023/07/25/microsoft-is-in-the-lead-with-cloud-based-ai-workloads-nadella-says.html)\n\n\nAlibaba s'est associé à l'IA Llama de Meta pour développer un logiciel innovant. Cette collaboration entre Alibaba Cloud, une division du géant chinois, et le modèle d'IA Llama 2, impose de nouvelles tendances dans l'industrie technologique. Ce partenariat fera de la Chine un pionnier dans la programmation sans frais, apportant des innovations passionnantes basées sur l'IA sur le marché. Préparez-vous à assister à l'avenir du développement de logiciels ! [Lire la suite](https://fagenwasanni.com/news/alibabas-cloud-computing-service-utilizes-metas-ai-model-llama-for-software-development/82096/)\n\n## Podcast : Embrasser la Transformation Numérique\n\nDécouvrez l'épisode de cette semaine où Darren participe à une conversation enrichissante avec l'invité spécial Jared Shepard, le PDG de Hypori. L'interview met l'accent sur le sujet crucial de la sécurisation des travailleurs à distance grâce à la virtualisation mobile. Le parcours unique de Jared Shepard, du décrochage scolaire au poste de PDG, ajoute une dimension inspirante à la discussion.\n\n[Episode 148](https://www.embracingdigital.org/episode-EDT148)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW26-fr","image":"./briefs/edw-26/fr/thumbnail.png","lang":"fr","summary":"Voici les dernières actualités dans le domaine de la Transformation Digitale pour la semaine du 31 juillet 2023. Cela comprend des nouvelles sur le partenariat entre Ericsson et Intel dans l'avancement de la technologie 5G, les efforts de ChatGPT pour lutter contre la cybersécurité, et l'émergence du développement de logiciels alimentés par l'IA."},{"id":192,"type":"News Brief","title":"2023-8-6","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Intelligence Artificielle\n\n\nIl y a une inquiétude croissante parmi les experts en technologie concernant l'utilisation d'images améliorées par l'IA. The Guardian a enquêté sur cette question, car il y a une crainte que les visuels modifiés par l'IA puissent influencer l'opinion publique et même déformer des faits importants essentiels au système démocratique. Cela pourrait sérieusement affecter la confiance et la vérité et conduire à un débat sur la façon de protéger les élections de la manipulation de l'IA. [Lire la suite](https://www.theguardian.com/technology/2023/aug/03/ai-enhanced-images-a-threat-to-democratic-processes-experts-warn\r)\n\n\nUn article récent de Yahoo Finance a rapporté que Wendy's utilise l'IA pour révolutionner leurs drive-thrus. Avec des commandes automatisées et des menus personnalisés, le géant de la restauration rapide change la donne pour les repas rapides. Cette approche technologique du service alimentaire offre un aperçu de l'avenir des repas rapides. Il sera intéressant de voir comment les clients réagissent à cette transformation numérique. [Lire la suite](https://finance.yahoo.com/news/wendys-latest-fast-food-company-210910771.html\r)\n\n\nL'impact de l'IA sur les professionnels d'origine asiatique-américaine est une tendance préoccupante. Les pertes d'emplois se profilent à l'horizon, soulevant des questions sur la manière dont la société abordera l'équation complexe de la technologie, de l'ethnicité et de la dynamique de la main-d'œuvre. Une étude plus approfondie des effets de l'IA est nécessaire. [Lire la suite](https://www.nbcnews.com/news/asian-america/asian-american-workers-heavily-affected-ai-rcna98179\r)\n\n## Cybersécurité\n\n\nNBC News a rapporté une préoccupante attaque cybernétique sur un réseau hospitalier qui a entraîné la déconnexion de son infrastructure informatique nationale. La violation a mis en danger les soins aux patients et les autorités travaillent à contenir la situation. Cet incident met en évidence l'importance de la sécurité des données de santé à l'ère numérique. [Lire la suite](https://www.nbcnews.com/tech/security/hackers-force-hospital-system-take-national-computer-system-offline-rcna98212\r)\n\n\nLes développeurs utilisant le système NPM largement répandu sont ciblés par des paquets de code nuisibles qui peuvent compromettre la sécurité des logiciels. Il s'agit d'une menace grave qui doit être identifiée et atténuée afin de prévenir les cyberattaques. La communauté de la cybersécurité est préoccupée et souligne le besoin de chaînes d'approvisionnement de logiciels plus sécurisées pour empêcher une infiltration supplémentaire de ces paquets. [Lire la suite](https://thehackernews.com/2023/08/malicious-npm-packages-found.html\r)\n\n\nLa CISA a publié un plan stratégique appelé \"Plan directeur de la cybersécurité\" pour renforcer les infrastructures numériques nationales. Il met l'accent sur la défense adaptative, la réponse aux incidents et l'innovation pour créer un bouclier numérique impénétrable. Ce plan est un guide essentiel pour ceux qui s'intéressent à la sécurité informatique. [Lire la suite](https://www.securitysystemsnews.com/article/cisa-releases-cybersecurity-strategic-plan\r)\n\n## L'informatique en périphérie\n\n\nLa combinaison de l'informatique quantique et des appareils IoT suscite des inquiétudes en matière de sécurité. La puissance de la technologie quantique et le potentiel de piratage des appareils IoT créent un nouveau défi. La crainte grandit de voir les systèmes de chiffrement être déchiffrés à mesure que l'informatique quantique progresse et que les botnets lancent des attaques DDoS en utilisant des appareils IoT piratés. Les défenses des réseaux doivent être renforcées pour faire face à cette double menace. [Lire la suite](https://www.securitymagazine.com/articles/99604-the-impact-of-quantum-computers-and-iot-devices-on-network-security\r)\n\n\nYahoo Finance a révélé une évolution passionnante dans l'industrie technologique alors que le Edge Computing, l'IoT et l'IA se fusionnent pour créer un marché estimé à 230 milliards de dollars d'ici 2025. Le Edge Computing permet à plus de 75 milliards d'appareils IoT de traiter des données en temps réel, tandis que les capacités cognitives de l'IA sont estimées à augmenter la productivité jusqu'à 40%. Cette révolution est sur le point de remodeler la dynamique des données et la prise de décision dans diverses industries. [Lire la suite](https://finance.yahoo.com/news/edge-computing-iot-ai-revolutionize-011500739.html\r)\n\n\nLa région Asie-Pacifique est en cours d'une transformation numérique significative, entraînée par l'Internet des objets (IdO) et l'intelligence artificielle (IA). Le Mobile Edge Computing (MEC) est à la tête de ce changement, offrant des niveaux de connectivité et d'automatisation inégalés en rapprochant le calcul de la source de données. Le MEC a un immense potentiel pour révolutionner les industries, améliorer les vies et stimuler le destin numérique de la région, mais il pose également des défis en matière de sécurité qui exigent des mesures de protection solides et des réglementations mises à jour. [Lire la suite](https://fagenwasanni.com/news/innovations-in-iot-and-ai-the-role-of-mobile-edge-computing-in-asia-pacifics-technological-advancements/48663/\r)\n\n## Podcast sur l'Adoption de la Transformation Numérique\n\n\nDans le dernier épisode de Embracing Digital Transformation, Darren est rejoint par Leland Brown, ingénieur principal chez Capgemini et ancien invité de l'émission. Ils discutent des progrès à venir de la technologie 5G et de son impact sur le Mobile Edge Computing (MEC) au sein du Département de la Défense des États-Unis. [Lire la suite](https://embracingdigital.org/episode-EDT149\r)\n\nIntelligence Artificielle\n\n\nIl y a une préoccupation croissante parmi les experts en technologie concernant l'utilisation d'images améliorées par l'IA. Le Guardian a enquêté sur cette question, car il y a une inquiétude que les visuels modifiés par l'IA puissent influencer l'opinion publique et même déformer des faits importants essentiels au système démocratique. Cela pourrait sérieusement affecter la confiance et la vérité, et conduire à un débat sur la façon de protéger les élections contre la manipulation de l'IA. [Lire la suite](https://www.theguardian.com/technology/2023/aug/03/ai-enhanced-images-a-threat-to-democratic-processes-experts-warn\r)\n\n\nUn article récent de Yahoo Finance a rapporté que Wendy's, un restaurant de fast food américain, utilise l'intelligence artificielle (IA) pour révolutionner leurs drive-thrus. Avec des commandes automatisées et des menus personnalisés, le géant de la restauration rapide change la donne pour les repas rapides. Cette approche technologique du service alimentaire offre un aperçu de l'avenir des repas rapides. Il sera intéressant de voir comment les clients réagissent à cette transformation numérique. [Lire la suite](https://finance.yahoo.com/news/wendys-latest-fast-food-company-210910771.html\r)\n\n\nL'impact de l'IA sur les professionnels américains d'origine asiatique est une tendance préoccupante. Les pertes d'emplois se profilent à l'horizon, soulevant des questions sur la manière dont la société abordera l'équation complexe de la technologie, de l'ethnicité et de la dynamique de la main-d'œuvre. Une étude approfondie des effets de l'IA est nécessaire. [Lire la suite](https://www.nbcnews.com/news/asian-america/asian-american-workers-heavily-affected-ai-rcna98179\r)\n\n## Cybersécurité\n\n\nNBC News a rapporté une cyberattaque préoccupante sur un réseau hospitalier qui a entraîné la déconnexion de son infrastructure informatique nationale. L'infraction met en danger les soins aux patients, et les autorités travaillent à contenir la situation. Cet incident souligne l'importance de la sécurité des données de santé à l'ère numérique. [Lire la suite](https://www.nbcnews.com/tech/security/hackers-force-hospital-system-take-national-computer-system-offline-rcna98212\r)\n\n\nLes développeurs utilisant le système NPM largement utilisé sont ciblés par des packages de code nuisibles qui peuvent compromettre la sécurité des logiciels. Il s'agit d'une menace grave qui doit être identifiée et atténuée afin de prévenir les cyberattaques. La communauté de cybersécurité est préoccupée et souligne la nécessité de chaînes d'approvisionnement logicielles plus sécurisées pour empêcher toute infiltration supplémentaire de ces packages. [Lire la suite](https://thehackernews.com/2023/08/malicious-npm-packages-found.html\r)\n\n\nCISA a publié un plan stratégique appelé \"Blueprint for Cybersecurity\" pour renforcer l'infrastructure numérique nationale. Il met l'accent sur la défense adaptative, la réponse aux incidents et l'innovation pour créer un bouclier numérique impénétrable. Ce plan est un guide essentiel pour ceux intéressés par la sécurité technologique. [Lire la suite](https://www.securitysystemsnews.com/article/cisa-releases-cybersecurity-strategic-plan\r)\n\n## Edge Computing\n\n## Informatique de périphérie\n\n\nLa combinaison de l'informatique quantique et des dispositifs IoT suscite des préoccupations en matière de sécurité. La puissance de la technologie quantique et le potentiel de détournement des appareils IoT créent un nouveau défi. La crainte de voir le cryptage être cracké augmente à mesure que l'informatique quantique progresse et que les botnets lancent des attaques DDoS en utilisant des dispositifs IoT piratés. Les défenses réseau doivent être renforcées pour faire face à cette double menace. [Lire la suite](https://www.securitymagazine.com/articles/99604-the-impact-of-quantum-computers-and-iot-devices-on-network-security\r)\n\n\nYahoo Finance a révélé un développement passionnant dans l'industrie technologique alors que l'informatique de périphérie, l'Internet des objets (IoT) et l'intelligence artificielle (IA) se fusionnent pour créer un marché estimé à 230 milliards de dollars d'ici 2025. L'informatique de périphérie permet à plus de 75 milliards de dispositifs IoT de traiter les données en temps réel, tandis que les capacités cognitives de l'IA sont estimées à une augmentation de la productivité allant jusqu'à 40%. Cette révolution est sur le point de remodeler la dynamique des données et la prise de décision dans diverses industries. [Lire la suite](https://finance.yahoo.com/news/edge-computing-iot-ai-revolutionize-011500739.html\r)\n\n\nLa région Asie-Pacifique est en train de connaître une transformation numérique significative, sous l'impulsion de l'Internet des objets (IoT) et de l'intelligence artificielle (IA). Le Mobile Edge Computing (MEC) est à l'avant-garde de ce changement, offrant des niveaux inégalés de connectivité et d'automatisation en rapprochant le calcul de la source de données. Le MEC a un potentiel immense pour révolutionner les industries, améliorer les vies et stimuler le destin numérique de la région, mais il pose également des défis en matière de sécurité qui nécessitent des mesures de sauvegarde solides et des réglementations mises à jour. [Lire la suite](https://fagenwasanni.com/news/innovations-in-iot-and-ai-the-role-of-mobile-edge-computing-in-asia-pacifics-technological-advancements/48663/\r)\n\n## Podcast sur la transformation numérique embrace\n\n\nDans le dernier épisode de Embracing Digital Transformation, Darren est rejoint par Leland Brown, un ingénieur principal chez Capgemini et un ancien invité de l'émission. Ils discutent des avancées à venir de la technologie 5G et de son impact sur le Mobile Edge Computing (MEC) au sein du Département de la Défense des États-Unis. [Lire la suite](https://embracingdigital.org/episode-EDT149\r)\n\nIntelligence Artificielle\n\n\nIl y a une inquiétude croissante parmi les experts en technologie concernant l'utilisation d'images améliorées par l'IA. The Guardian a enquêté sur cette question, car il y a une crainte que les visuels modifiés par l'IA puissent avoir un impact sur l'opinion publique et même déformer des faits importants essentiels au système démocratique. Cela pourrait sérieusement affecter la confiance et la vérité, et mener à un débat sur la façon de protéger les élections contre la manipulation de l'IA. [Lire la suite](https://www.theguardian.com/technology/2023/aug/03/ai-enhanced-images-a-threat-to-democratic-processes-experts-warn\r)\n\n\nUn récent article publié par Yahoo Finance a rapporté que Wendy's, un restaurant américain de fast food, utilise l'intelligence artificielle pour révolutionner ses drive-thrus. Avec des prises de commande automatisées et des menus personnalisés, le géant de la restauration rapide change la donne pour les repas rapides. Cette approche technologique du service de restauration offre un aperçu de l'avenir de la restauration rapide. Il sera intéressant de voir comment les clients réagissent à cette transformation numérique. [Lire la suite](https://finance.yahoo.com/news/wendys-latest-fast-food-company-210910771.html\r)\n\n\nL'impact de l'IA sur les professionnels américano-asiatiques est une tendance préoccupante. Les pertes d'emplois se profilent à l'horizon, soulevant des questions sur la façon dont la société abordera l'équation complexe de la technologie, de l'ethnicité et de la dynamique de la main-d'œuvre. Un examen plus approfondi des effets de l'IA est nécessaire. [Lire la suite](https://www.nbcnews.com/news/asian-america/asian-american-workers-heavily-affected-ai-rcna98179\r)\n\n## Sécurité informatique\n\n\nNBC News a rapporté une cyberattaque préoccupante sur un réseau hospitalier qui a entraîné la déconnexion de son infrastructure informatique nationale. La violation a mis en danger les soins aux patients et les autorités travaillent à contenir la situation. Cet incident souligne l'importance de la sécurité des données de santé à l'ère numérique. [Lire la suite](https://www.nbcnews.com/tech/security/hackers-force-hospital-system-take-national-computer-system-offline-rcna98212\r)\n\n\nLes développeurs utilisant le système NPM largement répandu sont visés par des packages de code nuisibles pouvant compromettre la sécurité des logiciels. Il s'agit là d'une menace grave qui doit être identifiée et atténuée afin de prévenir les cyberattaques. La communauté de la cybersécurité est préoccupée et souligne la nécessité de chaînes d'approvisionnement en logiciels plus sécurisées pour empêcher toute infiltration supplémentaire de ces packages. [Lire la suite](https://thehackernews.com/2023/08/malicious-npm-packages-found.html\r)\n\n\nLa CISA a publié un plan stratégique appelé \"Blueprint for Cybersecurity\" pour renforcer l'infrastructure numérique nationale. Il met l'accent sur la défense adaptative, la réponse aux incidents et l'innovation pour créer un bouclier numérique impénétrable. Ce plan est un guide essentiel pour ceux qui s'intéressent à la sécurité technologique. [Lire la suite](https://www.securitysystemsnews.com/article/cisa-releases-cybersecurity-strategic-plan\r)\n\n## Edge Computing\n\n## Calcul Périphérique\n\n\nLa combinaison de l'informatique quantique et des dispositifs IoT suscite des préoccupations en matière de sécurité. La puissance de la technologie quantique et le potentiel de détournement des dispositifs IoT créent un nouveau défi. La crainte grandit de voir les systèmes de chiffrement être piratés à mesure que l'informatique quantique progresse et que les botnets lancent des attaques DDoS en utilisant des dispositifs IoT piratés. Les défenses du réseau doivent être renforcées pour faire face à cette double menace. [Lire la suite](https://www.securitymagazine.com/articles/99604-the-impact-of-quantum-computers-and-iot-devices-on-network-security\r)\n\n\nYahoo Finance a révélé une évolution prometteuse dans l'industrie technologique alors que l'Edge Computing, l'Internet des Objets (IoT) et l'Intelligence Artificielle (IA) fusionnent pour créer un marché dont la valeur est estimée à deux cent trente milliards de dollars d'ici à 2025. L'Edge Computing permet à plus de soixante-quinze milliards d'appareils IoT de traiter des données en temps réel, tandis que les capacités cognitives de l'IA sont estimées à une augmentation de la productivité pouvant aller jusqu'à quarante pour cent. Cette révolution est destinée à remodeler les dynamiques de données et la prise de décision dans diverses industries. [Lire la suite](https://finance.yahoo.com/news/edge-computing-iot-ai-revolutionize-011500739.html\r)\n\n\nLa région Asie-Pacifique connaît une transformation numérique significative, portée par l'Internet des objets (IoT) et l'intelligence artificielle (IA). L'informatique en périphérie mobile (MEC) dirige ce changement, offrant des niveaux inégalés de connectivité et d'automatisation en rapprochant le calcul de la source de données. Le MEC a un potentiel immense pour révolutionner les industries, améliorer la vie quotidienne et stimuler le destin numérique de la région, mais il présente également des défis en matière de sécurité qui exigent des protections solides et des réglementations mises à jour. [Lire la suite](https://fagenwasanni.com/news/innovations-in-iot-and-ai-the-role-of-mobile-edge-computing-in-asia-pacifics-technological-advancements/48663/\r)\n\n## Podcast sur l'adoption de la transformation numérique\n\n\nDans le dernier épisode d'Embracing Digital Transformation, Darren est rejoint par Leland Brown, un ingénieur principal chez Capgemini et un ancien invité de l'émission. Ils discutent des avancées prévues de la technologie 5G et de son impact sur le Mobile Edge Computing (MEC) au sein du Département de la Défense des États-Unis. [Lire la suite](https://embracingdigital.org/episode-EDT149\r)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW27-fr","image":"./briefs/edw-27/fr/thumbnail.png","lang":"fr","summary":"La semaine du podcast du 7 août 2023, il y a eu plusieurs mises à jour importantes concernant la transformation numérique. Notamment, les chaînes de fast-food ont commencé à mettre en œuvre l'IA pour faire face à la pénurie de main-d'œuvre, les hôpitaux sont confrontés à des cyberattaques, et les plateformes de calcul à la périphérie mobile font leur apparition."},{"id":193,"type":"News Brief","title":"2023-8-13","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Informatique omniprésente\n\n\nEn raison d'une récente violation de sécurité sur Microsoft Azure Cloud, le Département de la Sécurité Intérieure examine les systèmes d'identification et d'authentification basés sur le cloud pour prévenir les attaques malveillantes sur les environnements informatiques basés sur le cloud, à la suite de récentes violations de sécurité. Le but est de fournir des recommandations pour améliorer la sécurité des comptes et prévenir les futures violations. [Lire la suite](https://www.reuters.com/technology/us-cyber-safety-review-board-assess-online-intrusion-microsoft-exchange-dhs-2023-08-11/)\n\n\nLe potentiel de l'informatique quantique de révolutionner plusieurs industries, en particulier l'intelligence artificielle, en augmentant la vitesse de traitement est excitant. Cependant, en raison de la nature délicate des qubits, la création d'ordinateurs quantiques est une tâche difficile. Néanmoins, les professionnels de l'industrie restent optimistes quant au potentiel de l'informatique quantique de permettre des avancées majeures dans divers secteurs. [Lire la suite](https://www.scmp.com/magazines/post-magazine/long-reads/article/3230746/quantum-computing-could-give-ai-rocket-fuel-it-needs-become-transformative-its-not-there-yet)\n\n\nLe président Biden a signé un décret présidentiel cette semaine qui interdit les investissements américains dans les secteurs chinois de l'intelligence artificielle, des semi-conducteurs et du calcul quantique afin d'éviter de soutenir involontairement l'avancement militaire et technologique de la Chine. L'industrie des semi-conducteurs, plus particulièrement le développement et la fabrication de puces, est la cible principale. La Chine a exprimé son mécontentement face à ce décret, affirmant qu'il perturbe les liens économiques. Le décret s'applique aux futurs investissements et peut comporter certaines exceptions. [Lire la suite](https://www.tomshardware.com/news/us-order-bans-future-investments-in-chinese-ai-semiconductor-and-quantum-computing)\n\n## Gestion des données\n\n\nLa Chine propose de nouvelles règles pour limiter la technologie de reconnaissance faciale et protéger la vie privée des utilisateurs. Le projet de règles de l'Administration du cyberespace de la Chine indique que la reconnaissance faciale ne devrait être utilisée que lorsque nécessaire et que des solutions non-biométriques devraient être explorées pour atteindre les mêmes objectifs avec moins d'intrusion. Cette démarche fait suite aux préoccupations en Chine concernant l'utilisation éventuelle des informations personnelles grâce à la technologie de reconnaissance faciale. [Lire la suite](https://fagenwasanni.com/news/china-introduces-new-draft-rules-to-regulate-facial-recognition-technology-and-data-management/174996/)\n\n\nDans la transition des serveurs internes aux technologies de cloud et aux centres de données pour la gestion des données, le cloud offre une évolutivité et une flexibilité tout en éliminant les besoins de proximité des serveurs. Cependant, de nombreuses organisations sont surprises par le coût de la gestion des données entre les centres de données et les clouds, car les fournisseurs de services cloud facturent le déplacement des données entre les modalités. Développer une stratégie de gestion des données et une architecture de données hybrides devient crucial pour les organisations afin de réduire les coûts, améliorer la résilience et soutenir les activités dans ces environnements d'exploitation complexes. [Lire la suite](https://devops.com/data-management-cloud-technology-or-data-centers/)\n\n\nLes gouvernements du monde entier adoptent un rôle plus actif dans la régulation de la vie privée des données. L'Oregon a adopté une approche globale, accordant aux consommateurs un plus grand contrôle sur leurs données personnelles. Les entreprises doivent désormais obtenir un consentement explicite pour la collecte et le partage de données, et les individus ont le droit d'accéder, de corriger et de supprimer leurs données. La loi inclut également des obligations de transparence et des mesures de protection contre la discrimination basée sur l'utilisation des données. S'agit-il d'une autre tentative pour l'État de s'emparer de l'argent ou d'une véritable protection de la vie privée des individus ? Le temps nous le dira. [Lire la suite](https://www.reuters.com/legal/legalindustry/oregon-passes-comprehensive-privacy-law-2023-08-11/)\n\n## Cyber sécurité\n\n\nL'Institut national des normes et de la technologie (NIST) prend des mesures significatives pour améliorer les pratiques de cybersécurité en fournissant de nouvelles lignes directrices et des ressources. En plus des piliers précédents du cadre de sécurité consistant à « identifier, protéger, détecter, répondre et récupérer », le NIST a introduit un nouveau pilier appelé « gouverner ». Cette nouvelle addition met l'accent sur le fait que la cybersécurité est une source importante de risques pour les entreprises et aide les organisations à mieux concevoir et exécuter les décisions pour soutenir leur stratégie de sécurité. [Lire la suite](https://www.infosecurity-magazine.com/news/nist-expands-cybersecurity/)\n\n\nLa conférence en cybersécurité Black Hat à Las Vegas a présenté des outils avancés pour la détection de menaces, la réponse aux incidents et la communication sécurisée. Ces outils visent à renforcer la défense en cybersécurité des organisations face aux menaces et aux défis émergents. Comme prévu, l'intelligence artificielle générative a été un sujet central, mettant en évidence son potentiel pour contrecarrer les attaques cybernétiques. [Lire la suite](https://www.crn.com/news/security/20-hottest-new-cybersecurity-tools-at-black-hat-2023)\n\n\nLe défi du Codebreaker de la NSA contribue à remédier à la pénurie de compétences en cybersécurité en mettant les étudiants au défi de résoudre des problèmes complexes et en favorisant la collaboration. Il encourage le développement de nouveaux matériaux éducatifs et de programmes, offrant ainsi à la prochaine génération de professionnels une expérience pratique pour renforcer la défense de l'industrie face aux menaces en évolution. Le programme met en évidence l'importance de promouvoir l'éducation en cybersécurité et de fournir des occasions d'apprentissage pratique. [Lire la suite](https://www.darkreading.com/attacks-breaches/nsa-talks-codebreaker-challenge-success-influence-on-education)\n\n## Podcast sur l'adoption de la transformation numérique\n\nDarren interviewe Sonu Panda, le PDG de Prescriptive Data, dans cet épisode. Ils discutent de la façon dont leur logiciel aide les propriétaires de biens immobiliers commerciaux à transformer leurs bâtiments en espaces intelligents et efficaces. [Lire la suite](https://embracingdigital.org/episode-EDT150)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW28-fr","image":"./briefs/edw-28/fr/thumbnail.png","lang":"fr","summary":"Traduction en français : \nLes actualités de la Transformation Digitale pour le 13 août 2023. Les récents développements en matière de confidentialité des données, de sécurité et d'avancées technologiques, y compris l'informatique quantique et les nouvelles lois sur la vie privée, valent la peine d'être suivis. Les directives du NIST et les outils de Black Hat sont des ressources utiles. Le défi Codebreaker de la NSA favorise l'innovation en matière de cybersécurité."},{"id":194,"type":"News Brief","title":"2023-8-19","tags":["ai","edge","cybersecurity"],"body":"\n\n## Intelligence Artificielle\n\n\nDans une décision récente, un tribunal de D.C. a déterminé que l'art généré par l'intelligence artificielle n'est pas éligible à la protection du droit d'auteur. Cette décision établit un précédent important, suggérant que les œuvres créatives produites uniquement par une intelligence artificielle ne jouissent pas des droits juridiques généralement accordés aux créations humaines. Ce verdict pourrait avoir des conséquences importantes pour l'avenir de la propriété intellectuelle à l'ère numérique. [Lire la suite](https://news.bloomberglaw.com/ip-law/ai-generated-art-lacks-copyright-protection-d-c-court-rules)\n\n\nLes chercheurs du MIT ont découvert que les modèles d'apprentissage automatique utilisés dans le diagnostic et le traitement médicaux peuvent aggraver les inégalités en matière de santé. Ces modèles, lorsqu'ils sont entraînés sur des données biaisées, peuvent entraîner un accès inégal et une qualité de soins différente pour les groupes sous-représentés. Cela souligne le besoin urgent d'une meilleure collecte de données et de développement de modèles afin de garantir des résultats équitables et rémunérateurs en matière de santé. [Lire la suite](https://news.mit.edu/2023/how-machine-learning-models-can-amplify-inequities-medical-diagnosis-treatment-0817)\n\n\nLa demande en pleine explosion pour l'intelligence artificielle (IA) entraîne une pénurie de puces haute-performance, rapporte Fox Business. À mesure que les applications d'IA se déploient dans divers secteurs, le besoin de puces spécialisées pour les alimenter augmente. Cette rareté affecte différents secteurs, notamment l'automobile, la santé et l'électronique grand public, risquant de ralentir l'innovation. Les entreprises se précipitent maintenant pour accélérer la production de puces afin de répondre à cette demande croissante. [Lire la suite](https://www.foxbusiness.com/technology/surging-demand-ai-creating-shortage-high-powered-chips)\n\n## Cybersécurité\n\n\nUne application axée sur les étudiants suscite des préoccupations parmi les parents et les experts en cybersécurité. Les risques potentiels de l'application pour la sécurité des enfants font l'objet d'un examen minutieux, incitant les experts à souligner l'importance de la vigilance parentale et de l'éducation des jeunes utilisateurs en matière de sécurité en ligne. La situation met en évidence le besoin constant d'une utilisation responsable de la technologie et de mesures proactives visant à protéger les enfants à l'ère numérique. [Lire la suite](https://www.live5news.com/2023/08/18/student-geared-app-concerning-lowcountry-parents-cyber-security-experts/)\n\n\nL'industrie de la fintech est grandement affectée par l'IA et la cybersécurité, selon Analytics Insight. L'IA aide à détecter la fraude, le service à la clientèle et à évaluer les risques, mais elle apporte également de nouveaux risques en matière de sécurité. Cette interaction continue entre l'IA et la cybersécurité change le monde de la fintech et conduit à davantage d'innovation et de mesures de sécurité accrues pour protéger les informations financières importantes. [Lire la suite](https://www.analyticsinsight.net/how-ai-and-cybersecurity-shape-fintech-industry/)\n\n\nUn rapport récent de Pew Research révèle que les Américains ont une connaissance limitée en matière d'IA, de cybersécurité et de la grande technologie. Les résultats indiquent qu'une partie substantielle de la population ne comprend pas ces sujets critiques, mettant en évidence la nécessité d'une sensibilisation et d'une éducation accrues du public dans une ère dominée par la technologie et les préoccupations numériques. [Lire la suite](https://www.pewresearch.org/internet/2023/08/17/what-americans-know-about-ai-cybersecurity-and-big-tech/)\n\n\n## Calcul de bord\n\n\nT-Mobile s'associe à Google Cloud pour le traitement en périphérie, selon FierceWireless. Cette collaboration vise à exploiter l'infrastructure en nuage de Google afin d'améliorer les capacités de réseau 5G de T-Mobile et de fournir des services à faible latence, au bénéfice des consommateurs et des entreprises. Cette décision stratégique démontre l'importance croissante du traitement en périphérie dans l'industrie des télécommunications. [Lire la suite](https://www.fiercewireless.com/tech/t-mobile-hooks-google-cloud-edge-compute)\n\n\nLa suppression progressive des réseaux 2G et 3G peut avoir des répercussions sur les services IoT et d'itinérance. De nombreux dispositifs IoT et voyageurs internationaux dépendent toujours de ces anciens réseaux. Les fournisseurs de télécommunications doivent envisager des options et des stratégies de connectivité alternatives pour éviter toute interruption de service pour ces groupes d'utilisateurs au fur et à mesure de leur transition vers des technologies réseau plus avancées. Il est important de traiter ces problèmes afin d'éviter d'accumuler une dette technique à l'avenir. [Lire la suite](https://www.lightreading.com/broadband/sunsetting-2g-and-3g-could-leave-iot-and-roamers-in-dark/d/d-id/786146)\n\n\nLors du Sommet des Télécoms Numériques ET, le CTO d'Airtel a souligné que la convergence de la 5G avec les technologies du cloud et de l'IoT est prête à débloquer des cas d'utilisation novateurs dans diverses industries. Cette combinaison devrait stimuler des solutions transformatrices pour les secteurs verticaux, mettant en valeur le potentiel de la 5G pour révolutionner la connectivité et les services en Inde et au-delà. [Lire la suite](https://telecom.economictimes.indiatimes.com/news/industry/etdigitaltelcosummit-combination-of-5g-with-cloud-iot-to-drive-new-use-cases-for-verticals-airtel-cto/102827792)\n\n## Podcast sur l'adoption de la transformation numérique\n\n\nCette semaine, Darren commence une série sur l'intelligence artificielle générative. Cette semaine, Darren commence une série sur l'intelligence artificielle générative avec des interviews d'experts en éducation, en santé, en cybersécurité et en technologies cloud, et comment l'intelligence artificielle générative joue un rôle dans l'avenir. Les histoires de cette semaine comprennent des interviews du Dr Jeffrey Lancaster parlant de la compréhension de l'IA générative. [Lire la suite](https://embracingdigital.org/episode-EDT151)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW29-fr","image":"./briefs/edw-29/fr/thumbnail.png","lang":"fr","summary":"Dans les dernières actualités d'Embracing Digital du 20 août 2023, il y a des mises à jour concernant l'IA, la cybersécurité et l'informatique en périphérie. Les actualités incluent l'IA qui se dirige vers les tribunaux, les acteurs malveillants ciblant la période de la rentrée scolaire et la dette technique qui est transférée vers l'écosystème 5G par l'informatique en périphérie."},{"id":195,"type":"News Brief","title":"2023-2-19","tags":null,"body":"\n\n## Intelligence Artificielle\n\nLes éthiques et les réglementations relatives à l'utilisation de chatbots génératifs pilotés par une IA commencent à jouer un rôle alors que de plus en plus de personnes utilisent ChatGPT et d'autres. Les chatbots devront désormais inclure un avertissement : Nous pourrions ne pas produire des résultats réels. Croyez à vos propres risques. Vous renoncez à votre droit de nous poursuivre si vous reproduisez des conversations, des images et des vidéos pour des cas d'utilisation à haut risque qui causent de la haine, du harcèlement, de la violence, de l'automutilation, des activités illégales, de la tromperie, de la discrimination ou du spam.\n\n[https://swisscognitive.ch/2023/02/15/honest-lying-why-scaling-generative-ai-responsibly-is-not-a-technology-dilemma-in-as-much-as-a-people-problem/](https://swisscognitive.ch/2023/02/15/honest-lying-why-scaling-generative-ai-responsibly-is-not-a-technology-dilemma-in-as-much-as-a-people-problem/)\n\nAlors que l'IA devient de plus en plus importante dans diverses industries, le rôle de directeur de l'IA (Chief AI Officer ou CAIO) émerge en tant que nouvelle addition à la haute direction. Un bon CAIO devrait clairement définir l'impact souhaité des projets d'IA et se concentrer continuellement sur la réalisation de ce résultat pour l'organisation. Un excellent CAIO devrait également suivre l'impact réel de ces projets et mettre à jour l'entreprise sur leurs performances au fil du temps. Ce rôle élève donc les attentes en matière de transformation et d'impact, ainsi que d'anticipation et de réponse à l'utilisation de l'IA par les concurrents.\n\n[https://applieddatascience.medium.com/the-case-for-the-chief-ai-officer-the-newest-c-suite-role-d55cca40c01](https://applieddatascience.medium.com/the-case-for-the-chief-ai-officer-the-newest-c-suite-role-d55cca40c01)\n\nLe dernier superordinateur d'IBM prend plusieurs nouvelles directions innovantes - c'est le premier superordinateur optimisé pour l'IA et natif du cloud d'IBM, et il est hébergé entièrement dans le Cloud IBM plutôt que sur site comme les superordinateurs traditionnels. Selon IBM, \"il s'agit de l'environnement privilégié des chercheurs d'IBM pour créer les capacités d'IA les plus avancées, notamment en travaillant sur des modèles fondamentaux et en collaborant avec des partenaires pour la création de modèles.\"\n\n[https://futurumresearch.com/research-notes/ibms-latest-supercomputer-is-cloud-based-and-ai-focused/](https://futurumresearch.com/research-notes/ibms-latest-supercomputer-is-cloud-based-and-ai-focused/)\n\n## ## Informatique ubiquitaire\n\nSelon une étude de 451 Research commandée par Oracle Cloud Infrastructure, presque chaque parcours cloud en entreprise devient maintenant un parcours multicloud. L'étude a révélé que 98 % des entreprises interrogées utilisent ou prévoient d'utiliser au moins deux fournisseurs de services cloud. Les principaux moteurs des stratégies multicloud sont la souveraineté des données et l'optimisation des coûts. Les entreprises planifient activement des stratégies multicloud pour l'avenir, avec la redondance des données étant le cas d'utilisation futur le plus anticipé.\n\n[https://www.cloudcomputing-news.net/news/2023/feb/21/98-of-firms-using-public-cloud-adopt-multicloud-infrastructure-provider-strategy/](https://www.cloudcomputing-news.net/news/2023/feb/21/98-of-firms-using-public-cloud-adopt-multicloud-infrastructure-provider-strategy/)\n\nLa \"supercloud\", une architecture de cloud qui permet la migration transparente des applications entre différents fournisseurs de cloud, a été proposée pour la première fois par des chercheurs de l'Université Cornell en 2016. Le concept a refait surface en tant que solution aux défis du multi-cloud, fournissant un réseau homogène pour lier les ressources de cloud ensemble, ce qui entraîne une migration transparente, une sécurité cohérente et des performances optimales.\n\n[https://www.cloudcomputing-news.net/news/2023/feb/22/here-comes-the-supercloud-what-does-it-mean-for-multi-cloud-complexity/](https://www.cloudcomputing-news.net/news/2023/feb/22/here-comes-the-supercloud-what-does-it-mean-for-multi-cloud-complexity/)\n\nLa demande de DRAM pour les serveurs dépassera celle des smartphones en raison de l'utilisation croissante des applications de cloud, d'IA et de HPC ; le contenu DRAM pour les serveurs devrait augmenter de 12,1 % en glissement annuel en 2023, contre 6,7 % pour les smartphones. La mémoire de serveur représentera 37,6 % de la production totale de bits DRAM, contre 36,8 % pour la DRAM mobile. La pandémie de COVID-19 a stimulé la demande de services de cloud, entraînant une forte augmentation des expéditions de serveurs.\n\n## Sécurité informatique\n\nLa société de cybersécurité Resecurity a mis en garde contre une série de cyberattaques qui ont ciblé des centres de données dans le monde entier au cours des 18 derniers mois, entraînant une exfiltration de données et la publication d'informations d'identification d'accès sur le Web sombre. Bien que Resecurity n'ait pas nommé les victimes, Bloomberg affirme que des grandes entreprises, dont Alibaba, Amazon, Apple, BMW, Goldman Sachs, Huawei Technologies, Microsoft et Walmart, ont eu leurs informations d'identification de centre de données volées.\n\n[https://www.csoonline.com/article/3688909/cyberattacks-hit-data-centers-to-steal-information-from-global-companies.html#tk.rss_all](https://www.csoonline.com/article/3688909/cyberattacks-hit-data-centers-to-steal-information-from-global-companies.html#tk.rss_all)\n\nLa firme de cybersécurité Menlo Labs a découvert un groupe d'acteurs de menace inconnus qui cible des entités gouvernementales via une campagne basée sur Discord en utilisant le téléchargeur PureCrypter. La campagne utilise le domaine d'un organisme sans but lucratif compromis comme centre de commande et de contrôle pour livrer une charge utile secondaire, y compris les logiciels malveillants Redline Stealer, AgentTesla, Eternity, Blackmoon et Philadelphia Ransomware.\n\n[https://www.menlosecurity.com/blog/purecrypter-targets-government-entities-through-discord](https://www.menlosecurity.com/blog/purecrypter-targets-government-entities-through-discord)\n\nLe plus grand exercice de cyber-guerre militaire en Europe occidentale s'est récemment déroulé en Estonie, avec la participation de 34 équipes provenant de 11 pays dans une bataille cyber à feu réel. L'événement de sept jours a testé les réponses des participants à des scénarios cyber communs et complexes, y compris des attaques sur des réseaux et des systèmes de contrôle industriel (ICS). Un scénario simule des attaques sur des systèmes robotiques sans équipage. Les équipes d'Italie, d'Estonie et du Royaume-Uni ont été les meilleures performances, jugées en fonction de leur rapidité à identifier et à répondre aux menaces cybernétiques.\n\n[https://www.securityweek.com/11-countries-take-part-in-military-cyberwarfare-exercise/](https://www.securityweek.com/11-countries-take-part-in-military-cyberwarfare-exercise/)\n\n## Podcast sur l'adoption de la transformation numérique.\n\nConsultez l'épisode complet de cette semaine intitulé \"Certifying Autonomous Flight\" dans lequel Darren interviewe LuukVan Dijk de Daedalean.\n\n[https://www.embracingdigital.org/episode-EDT126](https://www.embracingdigital.org/episode-EDT126)\n\n\n\n","guests":null,"link":"/brief-EDW3-fr","image":"./briefs/edw-3/fr/thumbnail.png","lang":"fr","summary":"Summary"},{"id":196,"type":"News Brief","title":"2023-8-26","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\nCi troviamo nel pieno della rivoluzione digitale. Durante questo periodo, può essere difficile districarsi tra tutte le promesse e scoprire cosa funziona, cosa sarà ancora valido fra due anni e cosa contribuisce alla tua organizzazione. Molte organizzazioni IT hanno bisogno di aiuto per comprendere messaggi confusi e tecnologie in conflitto. Noi aiutiamo le organizzazioni a fare chiarezza e abbracciare la trasformazione digitale. Il mondo della trasformazione digitale è in continua evoluzione e devi sapere cosa è fattibile oggi. Ascolta il nostro podcast settimanale per le ultime notizie sulla sicurezza informatica, le comunicazioni avanzate, la gestione dei dati, l'intelligenza artificiale, il cloud computing e la tecnologia edge.\n## Cybersécurité\n\n\nUne nouvelle menace a émergé. Le groupe cybernétique \"Flax Typhoon\", soupçonné d'être lié à la Chine, fait parler de lui. On les suspecte de mener des cyberattaques sophistiquées contre des organisations mondiales. Les experts surveillent attentivement cette situation car elle pourrait avoir des implications importantes pour la sécurité numérique. Restez à l'écoute pour les mises à jour de cette affaire en cours. [Lire la suite](https://thehackernews.com/2023/08/china-linked-flax-typhoon-cyber.html)\n\n\nLes entreprises de cybersécurité lancent un cri d'alarme alors que les attaques de ransomwares ont augmenté de plus de cent cinquante-trois pour cent par rapport à il y a un an et de plus de seize pour cent le mois dernier. Les experts mettent en garde contre la nécessité pour les organisations de renforcer leurs défenses et d'informer leurs employés sur ces menaces afin de se prémunir contre des violations coûteuses de données. [Lire la suite](https://www.securityweek.com/cybersecurity-companies-report-surge-in-ransomware-attacks/)\n\n\nLe secteur de la cybersécurité est confronté à une pénurie de talents sévère, ce qui suscite des attentes en termes de solutions. La demande de professionnels qualifiés dans ce domaine en constante évolution est en plein essor, mais l'écart persiste. Pour remédier à cela, les entreprises explorent des stratégies telles que le renforcement des compétences des employés actuels et la promotion de la diversité dans l'industrie. Il est peut-être temps de retourner à l'école pour obtenir une certification en cybersécurité. [Lire la suite](https://www.helpnetsecurity.com/2023/08/25/cybersecurity-talent-shortage-expectations/)\n\nIntelligence artificielle\n\n\nL'intelligence artificielle a rendu sa voix à une femme paralysée grâce à une avancée incroyable. Les chercheurs de l'UCSF ont utilisé la technologie de l'IA pour décoder les signaux cérébraux de la femme et les traduire en parole. Cette réalisation révolutionnaire pourrait aider de nombreux individus à retrouver leur capacité de communication. Pour en savoir plus sur cette histoire inspirante, consultez un article approfondi sur ucsf.edu. [Lire la suite](https://www.ucsf.edu/news/2023/08/425986/how-artificial-intelligence-gave-paralyzed-woman-her-voice-back)\n\n\nLes lycéens se préparent à l'avènement de l'intelligence artificielle (IA). Les chercheurs du MIT ont lancé une nouvelle initiative pour aider à préparer les étudiants à un avenir basé sur l'IA. Ce programme vise à doter les jeunes esprits des compétences et des connaissances nécessaires pour prospérer dans un monde de plus en plus influencé par la technologie de l'IA. Découvrez l'interview de Darren avec Pete Schmitz dans l'épisode du podcast Embracing Digital Transformation intitulé \"Former la prochaine génération en IA\" pour avoir un aperçu détaillé de ce qui se passe aujourd'hui. [Lire la suite](https://news.mit.edu/2023/how-to-help-high-schoolers-prepare-rise-of-artificial-intelligence-0824)\n\n\nDans une analyse approfondie de DeZeen qui incite à la réflexion, le coût environnemental de l'intelligence artificielle (IA) est examiné de près. À mesure que les applications de l'IA se développent, leur consommation d'énergie augmente, exacerbant potentiellement les préoccupations climatiques. Cet examen critique explore les défis de durabilité posés par l'IA et la nécessité urgente de solutions respectueuses de l'environnement pour le développement et le déploiement de la technologie de l'IA. Restez à l'écoute pour des connaissances approfondies sur cette question environnementale cruciale. [Lire la suite](https://www.dezeen.com/2023/08/26/dezeen-in-depth-examines-the-environmental-cost-of-artificial-intelligence/)\n\n## L'informatique ubiquitaire\n\n\nÀ mesure que nous nous rapprochons de l'ère de l'informatique quantique, la protection des infrastructures critiques devient encore plus cruciale. C4ISRNET explore les défis et les stratégies pour protéger les systèmes vitaux dans ce nouveau paysage technologique. Le potentiel de l'informatique quantique pour briser les méthodes de chiffrement actuelles suscite des inquiétudes, mettant en évidence la nécessité de solutions innovantes en matière de cybersécurité. [Lire la suite](https://www.c4isrnet.com/it-networks/2023/08/21/how-to-protect-critical-infrastructure-in-the-quantum-computing-era/)\n\n\nComme rapporté par les développeurs XDA, Dropbox met fin à sa politique de stockage infini dans le cloud. Cette décision fait suite à des cas d'abus de la part de certains utilisateurs. Par conséquent, Dropbox met en place de nouvelles limitations de stockage, ce qui marque un changement important pour sa base d'utilisateurs. Ce développement souligne les défis continus auxquels sont confrontés les fournisseurs de stockage dans le cloud pour concilier les exigences des utilisateurs avec des modèles économiques durables. [Lire la suite](https://www.xda-developers.com/dropbox-ends-unlimited-cloud-storage-policy-abused/)\n\n\nLa pandémie mondiale de 2020 a revitalisé un secteur technologique souffrant d'une croissance stagnante, l'infrastructure de bureau virtuel. Microsoft et VMware sont en train de remodeler les VDI d'entreprise et la technologie cloud. SiliconANGLE discute de la façon dont ces géants de la technologie collaborent pour redéfinir les VDI et améliorer les solutions cloud. Ce partenariat offrira aux entreprises plus de flexibilité, de dimensionnement et de performances dans leurs environnements informatiques. [Lire la suite](https://siliconangle.com/2023/08/24/microsoft-vmware-redefining-business-vdi-cloud-technology-vmwareexplore/)\n\n## Podcast sur la Transformation Numérique\n\n\n\nCette semaine, la série Embracing Generative AI se poursuit, mettant en vedette des entrevues avec des professeurs de lycée ayant adopté ChatGPT et explorant de nouveaux cas d'utilisation pour l'IA générative. Abonnez-vous à Embracing Digital Transformation pour des discussions approfondies sur les sujets d'actualité. [Lire la suite](http://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW30-fr","image":"./briefs/edw-30/fr/thumbnail.png","lang":"fr","summary":"Voici les dernières mises à jour sur les actualités de la transformation numérique pour la semaine du 27 août 2023. Les histoires de cette semaine abordent la cybersécurité, l'intelligence artificielle et l'informatique omniprésente. Dans cet épisode, vous trouverez des histoires sur une augmentation significative de 150% des attaques de rançongiciels, sur la manière dont l'informatique quantique a mis en évidence les vulnérabilités de sécurité des objets connectés et sur la façon dont l'intelligence artificielle aide à rétablir la voix des personnes paralysées."},{"id":197,"type":"News Brief","title":"2023-9-2","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\nCi troviamo nel pieno della rivoluzione digitale. Durante questo periodo, può essere difficile districarsi tra tutte le promesse e scoprire cosa funziona, cosa sarà ancora valido fra due anni e cosa contribuisce alla tua organizzazione. Molte organizzazioni IT hanno bisogno di aiuto per comprendere messaggi confusi e tecnologie in conflitto. Noi aiutiamo le organizzazioni a fare chiarezza e abbracciare la trasformazione digitale. Il mondo della trasformazione digitale è in continua evoluzione e devi sapere cosa è fattibile oggi. Ascolta il nostro podcast settimanale per le ultime notizie sulla sicurezza informatica, le comunicazioni avanzate, la gestione dei dati, l'intelligenza artificiale, il cloud computing e la tecnologia edge.\n## Cybersecurity\n\n## Cybersécurité\n\n\n\nUne récente cyberattaque, supposée soutenue par la Chine, a suscité des préoccupations pour la sécurité nationale à Guam. Le groupe de pirates informatiques Volt Typhoon a ciblé les télécommunications et le réseau électrique de Guam pour établir une présence discrète. Guam est considéré comme une plaque tournante logistique vitale pour les États-Unis en cas de conflit avec la Chine. Les pirates ont profité d'une vulnérabilité dans le pare-feu Fortinet Fortigate, un produit de protection de cybersécurité de premier plan. Cette attaque met en évidence une nouvelle menace alors que les pirates informatiques nationaux ciblent de plus en plus les outils de cybersécurité cruciaux pour la protection des systèmes. [Lire la suite](https://www.msn.com/en-us/news/other/cybersecurity-tools-are-new-targets-for-nation-state-hackers/ar-AA1g5mib)\n\n\nUne cyberattaque a causé une interruption technologique d'une semaine au système de santé des Sœurs de l'Hôpital, y compris à St. Elizabeth à O'Fallon, Illinois. Le PDG Damond Boatright a confirmé que l'incident a perturbé les services internet, les sites web, les téléphones et les applications informatiques. Bien que des progrès soient en cours pour restaurer les systèmes critiques, le site web de HSHS est toujours inaccessible, et l'accès des patients aux dossiers en ligne et à la communication avec les médecins a été perturbé. Il s'agit de la deuxième attaque contre des groupes hospitaliers aux États-Unis en deux mois. [Lire la suite](https://www.bnd.com/news/local/article278879724.html)\n\n\n\nLes collèges communautaires font face à une menace croissante en matière de cybersécurité : les \"étudiants fantômes\". Ces identités fictives exploitent les processus d'inscription pour accéder aux ressources académiques, compromettant ainsi potentiellement les données des étudiants et l'intégrité institutionnelle. Face à la sophistication croissante des pirates informatiques, les collèges communautaires investissent dans des mesures de cybersécurité pour lutter contre ce fléau. La lutte contre les \"étudiants fantômes\" souligne le besoin d'une sécurité numérique robuste dans les établissements d'enseignement afin de protéger les informations sensibles et garantir un environnement d'apprentissage sécurisé. [Lire la suite](https://www.chronicle.com/article/meet-the-cybersecurity-threat-haunting-community-colleges-ghost-students)\n\n## Intelligence Artificielle\n\n\nL'utilisation de l'IA dans l'enseignement supérieur a suscité un débat entre ceux qui voient son potentiel en termes de personnalisation et d'efficacité, et ceux qui s'inquiètent des questions d'éthique et de perte de lien humain. Une réflexion approfondie est nécessaire pour intégrer l'IA dans les salles de classe universitaires. La professeure universitaire Laura Torres Newey a partagé son approche pour aborder cette question dans un épisode de podcast intitulé \"Adopter la transformation numérique\". Ses idées mettent en évidence les défis et les opportunités de l'intégration de l'IA dans l'enseignement supérieur. [Lire la suite](https://theconversation.com/should-ai-be-permitted-in-college-classrooms-4-scholars-weigh-in-212176)\n\n\nL'industrie du divertissement fait face à des conflits sociaux, avec l'intelligence artificielle (IA) au cœur du conflit. La grève du Screen Actors Guild en est à sa huitième semaine, avec des travailleurs à Hollywood demandant une rémunération équitable et un contrôle créatif dans l'utilisation de la technologie pilotée par l'IA. Ce conflit reflète la lutte de l'industrie pour équilibrer les avantages de l'IA, tels que l'efficacité et l'innovation en termes de coûts, avec les inquiétudes concernant le remplacement des emplois et l'autonomie artistique. Alors que le paysage du divertissement évolue, résoudre ces problèmes de travail liés à l'IA sera crucial pour les travailleurs et l'avenir de l'industrie. Le monde observe ! [Lire la suite](https://www.pbs.org/newshour/show/why-artificial-intelligence-is-a-central-dispute-in-the-hollywood-strikes)\n\n\nLa régulation de l'intelligence artificielle (IA) est une préoccupation mondiale et les pays adoptent des approches diverses pour y faire face. La Chine, Israël et l'Union européenne (UE) sont en tête de la gouvernance de l'IA. La Chine se concentre sur l'éthique de l'IA et la sécurité des données, Israël encourage l'innovation grâce aux « regulatory sandboxes » et l'UE propose des règles strictes en matière d'IA pour garantir la responsabilité. Alors que l'IA continue de façonner diverses industries et aspects de la vie quotidienne, trouver le bon équilibre entre l'innovation et la régulation reste un défi crucial sur la scène mondiale. [Lire la suite](https://www.washingtonpost.com/world/2023/09/03/ai-regulation-law-china-israel-eu/)\n\n## Informatique de bord\n\n\nIntel renforce la sécurité de l'informatique périphérique en introduisant une couche de protection révolutionnaire. La nouvelle technologie de l'entreprise promet de protéger les appareils périphériques et les données, en répondant aux préoccupations critiques en matière de cybersécurité. Avec l'importance croissante de l'informatique périphérique dans diverses industries, ce développement représente une avancée significative pour garantir l'intégrité et la sécurité des environnements informatiques distribués. [Lire la suite](https://www.fool.com/investing/2023/09/01/intel-adds-a-layer-of-protection-to-edge-computing/)\n\n\nAssurer la sécurité de l'Internet des Objets (IdO) est crucial pour protéger les infrastructures critiques telles que les réseaux électriques et les stations de traitement d'eau. Alors que les dispositifs IdO sont de plus en plus intégrés à ces systèmes, les protéger contre les menaces évolutives, y compris les risques potentiels associés à certains pays, est un enjeu urgent pour les États-Unis. Les défis et les implications de la technologie chinoise dans l'IdO font actuellement l'objet d'un examen approfondi, soulignant la nécessité de mesures proactives pour protéger les infrastructures critiques du pays. [Lire la suite](https://www.forbes.com/sites/davealtavilla/2023/09/03/securing-the-iot-from-the-threat-china-poses-to-us-infrastructure/?sh=4325a3f112c0)\n\n\nLe traitement en périphérie ne concerne pas seulement la fabrication et la gestion des infrastructures critiques. Le traitement en périphérie révolutionne le secteur de la santé en permettant le traitement des données en temps réel, ce qui entraîne des délais de réponse plus rapides pour les données essentielles des patients. Il soutient la surveillance à distance et la télémédecine, rendant les soins de santé plus accessibles et réduisant la pression sur les établissements de santé. De plus, le traitement en périphérie améliore la confidentialité et la sécurité des données en traitant localement les informations sensibles des patients, réduisant ainsi le risque de violation de données et garantissant la conformité aux réglementations en matière de santé. [Lire la suite](https://www.ft.com/partnercontent/ntt-ltd/edge-computing-delivers-healthcare-beyond-the-clinic.html?blaid=3846770)\n\nAdopter la transformation numérique\n\n\n\nLa série qui aborde l'IA générative se poursuit avec des interviews sur la préparation des données dans la révolution du GenAI et sur la manière dont l'enseignement supérieur aborde l'IA générative en classe. [Lire la suite](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW31-fr","image":"./briefs/edw-31/fr/thumbnail.png","lang":"fr","summary":"Nouvelles de la semaine du 3 septembre 2023 sur l'adoption du numérique. Cette semaine, des mises à jour sur la cybersécurité, l'intelligence artificielle et le traitement en périphérie. Les cyberattaques de la Chine contre les bases militaires et les infrastructures critiques, les efforts d'Intel pour renforcer la sécurité dans l'espace IoT et les défis rencontrés par les gouvernements pour réglementer l'IA."},{"id":198,"type":"News Brief","title":"2023-9-9","tags":["ai","gpt4","openai","microsoft","generativeai","environmentalimpact","irs","taxevasion","education","cybersecurity","iosspyware","zerodayvulnerability","northkoreanhackers","quantumrandomness","cloudfirst","cloudsmart","embracingdigital","generativeaipolicy","highereducation"],"body":"\n\n## Intelligence Artificielle\n\n\nLe lieu de naissance de GPT-4 a été découvert ! Les champs de maïs de Des Moines, dans l'Iowa, ont été le centre de la dernière avancée d'OpenAI, mais à un coût significatif pour l'environnement. Le dernier rapport environnemental de Microsoft a révélé une augmentation de 34 % de la consommation mondiale d'eau entre 2021 et 2022, ce qui représente près de 1,7 milliard de gallons. Cette hausse est principalement due à l'investissement important de l'entreprise dans l'IA générative et à son partenariat avec OpenAI. L'impact de l'IA générative sur l'environnement est important, et des organismes majeurs tels que Microsoft, Google et OpenAI prennent des mesures pour faire face à ces préoccupations. [Lire la suite](https://apnews.com/article/chatgpt-gpt4-iowa-ai-water-consumption-microsoft-f551fde98083d17a7e8d904f8be822c4)\n\n\n\nL'Internal Revenue Service aux États-Unis prévoit de mettre en place une technologie d'IA pour améliorer la supervision des grands cabinets d'avocats, des fonds spéculatifs, des sociétés de capital-investissement et des investisseurs immobiliers. L'initiative vise à détecter des affaires complexes d'évasion fiscale et à récupérer les revenus fédéraux dus auprès des contribuables fortunés. L'IRS a alloué 80 milliards de dollars de l'Inflation Reduction Act pour s'attaquer aux modèles et tendances dans la réalisation d'audits significatifs et à l'application de sanctions contre les fraudeurs fiscaux de ces industries. [Lire la suite](https://www.nytimes.com/2023/09/08/us/politics/irs-deploys-artificial-intelligence-to-target-rich-partnerships.html)\n\n\nLes éducateurs et les chercheurs sont à l'avant-garde de la prise en compte du rôle de l'IA dans l'éducation. Alors que les étudiants retournent à l'école cet automne, plusieurs articles de revues du monde entier abordent les applications de l'IA, les préoccupations éthiques, l'augmentation par rapport à l'automatisation, la préparation des étudiants au travail intégré à l'IA, la détection des deepfakes, la préservation du sens commun humain et l'utilisation de l'IA génératrice pour l'enseignement. Ces perspectives fournissent des orientations précieuses pour façonner l'avenir de l'IA dans l'éducation. De plus, découvrez l'interview de Darren avec le New York Times et la professeure d'anglais Laura Torres Newey sur la manière dont elle intègre l'IA dans ses cours cet automne. [Lire la suite](https://daily.jstor.org/artificial-intelligence-and-education-a-reading-list/)\n\n## Sécurité informatique\n\n\nLa Chine prend très au sérieux la cybersécurité. Les nouvelles réglementations chinoises en matière de cybersécurité visent à renforcer la sécurité numérique pour l'internet, mais les petites entreprises pourraient rencontrer des difficultés. L'impact complet sur les entreprises de toutes tailles demeure incertain, et il pourrait y avoir des conséquences sur la vie privée en ligne. Malgré tout, ces mesures sont nécessaires afin de protéger les données en ligne et de se prémunir contre les menaces cybernétiques. Les particuliers et les entreprises doivent se tenir informés de ces changements et prendre les précautions nécessaires pour sécuriser leur présence en ligne. [Lire la suite](https://www.scmp.com/news/china/politics/article/3233931/china-steps-cybersecurity-enforcement-smaller-businesses-are-feeling-heat)\n\n\nLe système d'iPhone autrefois immunisé a été attaqué la semaine dernière, avec la détection d'un logiciel espion dans le populaire système d'exploitation iOS qui équipe les iPhones. Apple a agi rapidement après avoir découvert ces menaces. Ils ont publié des mises à jour logicielles essentielles pour garantir la sécurité de vos appareils. C'est comme si un superhéros protégeait votre univers technologique ! Nous examinerons les détails de ces mises à jour, comment elles ont contrecarré les potentielles atteintes à la sécurité et ce que cela signifie pour votre sécurité numérique. Restez à l'écoute pendant que nous dévoilons les derniers développements en matière de technologie et de cybersécurité. [Lire la suite](https://www.washingtonpost.com/politics/2023/09/08/apple-issues-software-updates-after-spyware-discoveries/)\n\n\nDes pirates nord-coréens auraient exploité une vulnérabilité \"zéro-jour\", ce qui leur donne accès à une forteresse de défenses numériques. Cette cyberattaque constitue une menace importante pour les systèmes ciblés, les violations de données et le paysage global de la cybersécurité. Des actions sont en cours pour contrer cette menace, notamment des correctifs et des mesures de protection visant à prévenir des attaques similaires à l'avenir. [Lire la suite](https://thehackernews.com/2023/09/north-korean-hackers-exploit-zero-day.html)\n\n## Informatique ubiquitaire\n\n\nIntégrer des systèmes d'IA générative transforme l'architecture du cloud, l'imprégnant d'un cerveau intelligent capable d'apprendre et de créer de manière autonome. Cette transformation redéfinit l'infrastructure cloud, renforçant l'automatisation, optimisant l'allocation des ressources et améliorant les mesures de sécurité. L'incorporation de systèmes d'IA générative dans le cloud offre des opportunités et des défis prometteurs, et nous entreprendrons un voyage approfondi pour les démêler. [Lire la suite](https://www.infoworld.com/article/3706094/adding-generative-ai-systems-may-change-your-cloud-architecture.html)\n\n\nDans un exploit révolutionnaire, le MIT a pris le contrôle de la randomité quantique pour la première fois. C'est comme apprivoiser le sauvage et l'imprévisible ! Nous plongerons dans les détails de cet exploit remarquable, qui pourrait avoir des implications profondes pour l'informatique quantique et la cryptographie. Rejoignez-nous pour explorer comment la percée du MIT nous permet de tirer parti de l'aléatoire inhérent au monde quantique, ouvrant de nouvelles frontières en matière de technologie et de sécurité. Le temps dira quel impact cela aura sur l'informatique. [Lire la suite](https://scitechdaily.com/harnessing-the-void-mit-controls-quantum-randomness-for-the-first-time/)\n\n\n'Le Cloud en premier' est dépassé, et 'Le Cloud intelligent' est le nouveau mot à la mode dans la technologie. Pensez à passer de 'le cloud est la réponse' à 'le cloud comme allié stratégique'. Nous plongerons dans les détails de ce changement, explorant ce que 'le Cloud intelligent' signifie pour les entreprises et les stratégies informatiques. Rejoignez-nous alors que nous discutons de la manière dont les organisations font évoluer leurs approches pour exploiter le Cloud de manière plus intelligente, en en faisant une partie centrale de leurs parcours de transformation numérique. [Lire la suite](https://www.infoworld.com/article/3705615/cloud-first-is-dead-cloud-smart-is-whats-happening-now.html)\n\n## Adoption de la transformation numérique\n\nCette semaine se poursuit la série \"Embracing Generative AI\" dans le podcast avec une interview fascinante sur le développement d'une politique de génération d'IA pour votre lieu de travail et sur la manière dont l'enseignement supérieur aborde l'IA générative en classe. Branchez-vous sur les derniers épisodes les mardis et jeudis. [Lire la suite](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW32-fr","image":"./briefs/edw-32/fr/thumbnail.png","lang":"fr","summary":"Obtenez les dernières actualités numériques pour la semaine du 10 septembre 2023 couvrant l'intelligence artificielle, la cybersécurité et l'informatique omniprésente. Cette semaine, découvrez une nouvelle menace cybernétique pour l'iPhone, le lieu de naissance du GPT-4 et l'approche du MIT pour faire face à l'incertitude de l'informatique quantique."},{"id":199,"type":"News Brief","title":"2023-9-16","tags":["aisummit","rishisunak","chineseofficialsban","nationalsecurity","ukchinatensions","amazonselfpublishers","aigeneratedcontent","digitalpublishing","columbiauniversityresearch","aireliability","languagemodels","chatbotperformance","cognitivescience","webassembly(wasm)","edgecomputingrevolution","edgedevices","latencyreduction","secureexecution","iotapplications","mobilecomputing","ai","security","edge","embracingdigital"],"body":"\n\n## Intelligence Artificielle\n\n\nLe chancelier britannique Rishi Sunak réfléchit à une interdiction de présence de 50 % des officiels chinois lors du prochain Sommet de l'IA en raison de préoccupations de sécurité nationale. Cette décision reflète les tensions croissantes entre le Royaume-Uni et la Chine concernant la technologie et la propriété intellectuelle. Les détails de l'interdiction potentielle restent à finaliser. Le Sommet de l'IA est un événement majeur de collaboration technologique mondiale. [Lire la suite](https://www.theguardian.com/technology/2023/sep/15/rishi-sunak-considers-banning-chinese-officials-from-half-of-ai-summit)\n\n\nAmazon exige désormais aux auteurs indépendants de mentionner toute utilisation de l'intelligence artificielle dans leurs œuvres afin d'accroître la transparence dans le monde de l'édition numérique. Cette décision fait suite aux inquiétudes concernant la saturation du marché par des contenus générés par l'IA. Ces nouvelles règles vont impacter l'industrie de l'auto-édition sur Amazon. [Lire la suite](https://www.theguardian.com/books/2023/sep/11/self-publishers-must-declare-if-content-sold-on-amazons-site-is-ai-generated)\n\n\n\nLa recherche de l'Université Columbia met en évidence des préoccupations concernant la fiabilité de l'IA dans la prise de décision. L'étude a révélé que même les modèles de langage avancés peuvent confondre les absurdités avec le langage naturel, soulevant des questions sur l'exactitude. Cela offre une opportunité d'améliorer les performances des chatbots et de comprendre le traitement du langage humain. En comblant le fossé entre l'IA et les sciences cognitives, nous pouvons créer des assistants alimentés par une IA plus utiles et efficaces qui répondent mieux à leurs utilisateurs. [Lire la suite](https://www.sciencedaily.com/releases/2023/09/230914114704.htm)\n\n## Calcul en périphérie\n\n\nWebAssembly (Wasm) est sur le point de révolutionner le calcul en périphérie. Cette technologie permet d'exécuter du code provenant de différents langages sur des appareils en périphérie, améliorant ainsi la flexibilité et les performances. En réduisant la latence et en permettant une exécution sécurisée et efficace, Wasm peut transformer la manière dont les applications sont déployées et exécutées en périphérie. Les développeurs peuvent tirer parti du potentiel de Wasm pour créer des solutions de calcul en périphérie plus dynamiques et réactives, améliorant ainsi divers domaines tels que l'IoT et l'informatique mobile. [Lire la suite](https://www.infoworld.com/article/3703052/how-webassembly-will-transform-edge-computing.html)\n\n\n\nLa course pour déplacer l'IA vers le bord s'intensifie ! Cadence Design Systems a dévoilé de nouveaux outils logiciels de propriété intellectuelle de l'IA (IA PI) qui offrent des capacités d'externalisation à partir des processeurs hôtes. Ces outils visent à optimiser les charges de travail de l'IA, améliorant ainsi les performances et l'efficacité énergétique des dispositifs de bord. La technologie permet une exécution plus efficace des tâches d'IA, ce qui la rend adaptée à diverses applications, notamment l'Internet des objets (IdO) et l'informatique de bord, où l'optimisation des ressources est cruciale. Les outils logiciels d'IA PI de Cadence peuvent contribuer à accélérer l'adoption de l'IA dans les dispositifs de bord. [Lire la suite](https://www.edgeir.com/cadences-new-ai-ip-software-tools-promises-offloading-abilities-from-any-host-processor-20230914)\n\n\n\nLes conférences sur le Edge Computing sont au beau fixe cet automne, avec pas moins de sept grandes conférences dans le monde entier. Elles commenceront la dernière semaine de septembre à Amsterdam, passeront par Toronto, Dubaï, Paris, et Londres, pour ne citer que quelques endroits. Dépoussiérez vos sacs de voyage, cet automne sera chargé pour les entreprises d'IoT. Vous pouvez trouver la liste complète sur edgier.com. [Lire la suite](https://www.edgeir.com/edge-computing-events)\n\n## Cybersécurité\n\n\nLe Centre de réponse à la sécurité de Microsoft a identifié des vulnérabilités critiques dans la bibliothèque logicielle open-source NCurses, utilisée dans les systèmes de type Unix (Linux et iOS). Ces failles pourraient permettre aux attaquants d'exécuter du code malveillant ou de causer des attaques de déni de service. Les utilisateurs sont vivement encouragés à mettre à jour leurs installations de NCurses vers la dernière version afin de réduire les risques potentiels. Les vulnérabilités dans des bibliothèques largement utilisées comme NCurses soulignent l'importance des mises à jour régulières de sécurité et de la gestion des correctifs dans l'écosystème logiciel. [Lire la suite](https://thehackernews.com/2023/09/microsoft-uncovers-flaws-in-ncurses.html)\n\n\nLe site Web du Gestionnaire de téléchargement gratuit (FDM), un site très populaire pour les logiciels libres et open source, a été compromis, exposant les utilisateurs à des risques potentiels. Les attaquants ont injecté un code malveillant dans le site, potentiellement affectant les utilisateurs ayant téléchargé le logiciel entre le onze et le douze septembre deux mille vingt-trois. Cette violation souligne l'importance de ne télécharger des logiciels que depuis des sources fiables et de mettre régulièrement à jour les logiciels de sécurité afin de se protéger contre les menaces potentielles. Le FDM a pris des mesures pour résoudre le problème, mais les utilisateurs doivent rester vigilants. [Lire la suite](https://thehackernews.com/2023/09/free-download-manager-site-compromised.html)\n\n\nDes acteurs étatiques iraniens auraient utilisé des attaques de spear-phishing sophistiquées pour cibler des individus aux États-Unis, au Moyen-Orient et en Asie. Les attaques consistent en l'envoi de courriels trompeurs se faisant passer pour des organisations réputées et visant à transmettre des logiciels malveillants et à voler des informations sensibles. Les experts en sécurité soulignent la nécessité de mesures de sécurité robustes pour les courriels et d'une sensibilisation des utilisateurs afin de se protéger contre de telles menaces cybernétiques orchestrées par des acteurs étatiques. [Lire la suite](https://thehackernews.com/2023/09/iranian-nation-state-actors-employ.html)\n\n## En adoptant la transformation numérique\n\nCette semaine se poursuit la série \"Adopter l'IA générative\" avec des invités parlant de l'opérationnalisation de l'IA générative sur le lieu de travail, notamment en l'utilisant pour améliorer la sécurité et générer des rapports en langage naturel. [Lire la suite](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW33-fr","image":"./briefs/edw-33/fr/thumbnail.png","lang":"fr","summary":"Rester à jour avec les dernières nouvelles de la transformation digitale pour la semaine du 17 septembre 2023. Obtenez les dernières mises à jour sur l'intelligence artificielle, le calcul en périphérie et la cybersécurité. Les points forts de cette semaine incluent Microsoft découvrant des vulnérabilités dans les logiciels de leurs concurrents, Amazon mettant en place des exigences de divulgation de l'IA, et une liste complète des prochaines conférences IoT cet automne."},{"id":200,"type":"News Brief","title":"2023-9-23","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Intelligence artificielle\n\n\nLes physiciens développent des modèles génératifs pour l'IA en utilisant des équations physiques bien comprises. Le modèle génératif de flux de Poisson (PFGM), représentant des données avec des particules chargées, peut produire des images de haute qualité dix à vingt fois plus rapidement que les modèles de diffusion. Les chercheurs visent à explorer davantage de processus physiques et à affiner le PFGM en ajustant sa dimensionnalité. [Lire la suite](https://www.quantamagazine.org/new-physics-inspired-generative-ai-exceeds-expectations-20230919/ )\n\n\nEt si vous exécutez un chatbot sur votre PC ? Intel a annoncé une nouvelle puce, prévue pour décembre, qui permettra aux ordinateurs portables d'exécuter des chatbots d'intelligence artificielle générative sans dépendre des centres de données cloud. Cette fonctionnalité, démontrée lors de leur conférence pour développeurs de logiciels, permet aux entreprises et aux consommateurs de tester des technologies AI telles que ChatGPT sans envoyer de données sensibles vers le cloud. [Lire la suite](https://www.reuters.com/technology/intel-says-newest-laptop-chips-software-will-handle-generative-ai-2023-09-19/)\n\n\nLa croissance de la communauté open-source fait sensation dans le développement de l'IA. Pas étranges à l'innovation, les projets open-source tels que Hadoop et Spark ont permis aux développeurs de faire avancer le développement de l'IA depuis les années 1970. Les modèles génératifs d'IA tels que ChatGPT et Llama 2 sont construits sur des bases open-source, remettant en question le statu quo propriétaire des programmes d'IA. Les développeurs continueront à repousser les limites de l'IA en dehors des domaines du développement commercial. [Lire la suite](https://www.zdnet.com/article/why-open-source-is-the-cradle-of-artificial-intelligence/)\n\n## Calcul omniprésent\n\n\nIntel a lancé sa plateforme 'Developer Cloud' pour permettre aux développeurs de tester et déployer des applications d'IA et de calcul à haute performance en utilisant les dernières plates-formes matérielles. Elle prend en charge l'entraînement d'IA, l'optimisation des modèles et les tâches d'inférence, et repose sur un logiciel ouvert avec oneAPI, permettant le choix du matériel et la portabilité du code. D'autres technologies liées à l'IA ont également été présentées lors de l'annonce. Tout le monde peut s'inscrire sur cloud.intel.com. [Lire la suite](https://ummid.com/news/2023/september/20-09-2023/intel-developer-cloud-reaches-general-availability.html)\n\n\nPour aider à limiter les dépenses mystiques et coûteuses de gestion des données, WiMi Hologram Cloud a développé un nouvel algorithme de compression de données holographiques qui améliore l'efficacité du calcul en nuage en réduisant les besoins de transmission et de stockage des données. Ces avancées, ainsi qu'un protocole de stockage d'images distribué, répondent à la demande croissante de technologies holographiques avancées. Cela devrait aider les organisations qui souffrent des coûts de sortie des données dans le nuage. [Lire la suite](https://beststocks.com/wimi-hologram-cloud-revolutionizing-cloud-com/ )\n\n\nLors de l'événement Innovation 2023 d'Intel, la société a dévoilé un CPU à 288 cœurs faisant partie de sa gamme \"Sierra Forest\" pour les serveurs haute densité. Ce CPU aura 144 cœurs sur ses doubles puces, totalisant ainsi 288 cœurs et fils d'exécution. Intel a également confirmé que sa famille de processeurs Xeon de 5e génération, \"Emerald Rapids\", sera lancée le 14 décembre, offrant des améliorations de performance tout en maintenant la consommation électrique. Intel a souligné sa focalisation sur les applications d'intelligence artificielle, avec un supercalculateur pour les tâches d'IA utilisant des processeurs Xeon et des accélérateurs d'IA Gaudi2. [Lire la suite](https://www.techspot.com/news/100221-intel-announces-288-core-sierra-forest-xeon-processor.html)\n\n## Cyber sécurité\n\n\nScattered Spider, un groupe de piratage qualifié, gagne en notoriété grâce à ses attaques par rançongiciel contre des entreprises. Les analystes suggèrent que le groupe est principalement composé de jeunes de 17 à 22 ans provenant de pays occidentaux. Ils utilisent des techniques avancées d'ingénierie sociale, telles que le \"SIM swapping\", ainsi que des tactiques telles que le \"SWATing\" pour identifier des points d'accès privilégiés. Bien que leur motivation ne soit peut-être pas monétaire, leurs attaques ont perturbé divers secteurs et ont entraîné l'ouverture d'enquêtes par les forces de l'ordre. [Lire la suite](https://www.reuters.com/technology/power-influence-notoriety-gen-z-hackers-who-struck-mgm-caesars-2023-09-22/)\n\n\nUn rapport sur les tendances de la cybercriminalité en Inde révèle une hausse des cyberattaques, notamment du phishing, des logiciels malveillants et de la fraude financière, la situation étant exacerbée par la COVID-19. Les conclusions soulignent l'urgence de mesures de cybersécurité renforcées, d'une sensibilisation du public et d'une collaboration entre les forces de l'ordre et les secteurs privés pour lutter efficacement contre les menaces cybernétiques. L'éducation aux meilleures pratiques de cybersécurité est également mise en avant pour protéger les individus de telles attaques. [Lire la suite](https://www.theregister.com/2023/09/21/india_cybercrime_trends_report/)\n\n\nPour aider à lutter contre la menace de la cybersécurité, Microsoft Azure essaie d'améliorer la cybersécurité grâce à l'éducation. Azure a récemment annoncé plus de soixante heures de cours gratuits sur la cybersécurité, comprenant notamment la sécurité du cloud hybride, la configuration du pare-feu, la gestion des correctifs, la gestion de l'annuaire Active Directory et les technologies de chiffrement. Vous pouvez en savoir plus sur learn.microsoft.com. [Lire la suite](https://www.helpnetsecurity.com/2023/09/20/free-microsoft-azure-cybersecurity-resources/)\n\n## Adopter la transformation numérique\n\nCette semaine débute une nouvelle série sur la construction d'une stratégie cloud avec des architectes de solutions cloud d'Intel. Darren interviewe les experts d'Intel en migration cloud, gestion des coûts cloud et évaluation de la charge de travail. Découvrez les dernières informations sur le podcast \"Embracing Digital Transformation\". [Lire la suite](https://www.embracingdigital.org)\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW34-fr","image":"./briefs/edw-34/fr/thumbnail.png","lang":"fr","summary":"Actualités sur la transformation numérique pour le 24 septembre 2023, comprenant l'intelligence artificielle, l'informatique omniprésente et la cybersécurité. Écoutez parler des nouveaux processeurs d'Intel qui transforment l'IA et le calcul haute densité, ainsi que du groupe de hackers composé d'adolescents qui a attaqué les casinos MGM de Las Vegas."},{"id":201,"type":"News Brief","title":"2023-9-30","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","datasciencesolution","polystores","dataarchitecture","aichatbots","cybersecurityawareness","aisecuritycenter","googlechrome","mozillafirefox","digitaltransformationpodcast","intelcloudsolutionarchitects"],"body":"\n\n## Gestion des données\n\nBMLL Vantage, une solution de science des données, a été honorée en remportant le prix de la \"Meilleure solution de science des données\" lors des Data Management Insight Awards. Cette récompense célèbre le travail exceptionnel de BMLL Vantage dans le domaine de la science des données et reconnaît sa contribution significative à l'industrie. [Lire la suite](https://mondovisione.com/media-and-resources/news/bmll-vantage-wins-best-data-science-solution-at-the-data-management-insight-aw/)\n\nLes polystores intègrent de manière transparente des sources de données diverses, en supprimant les silos et en permettant une analyse interfonctionnelle. Ils optimisent les performances en exploitant les forces de diverses bases de données et s'adaptent aux technologies en évolution. Les organisations devraient utiliser des polystores pour libérer le potentiel des données afin de prendre des décisions éclairées à l'ère du big data. [Lire la suite](https://thenewstack.io/polystores-the-data-management-game-changer/)\n\n\nLorsqu'il s'agit de décider d'une architecture de données, il est essentiel de prendre en compte des facteurs tels que la taille des données, la fréquence, l'emplacement de la source, le type, la confidentialité, l'accès et l'urgence. Comprendre les différents types d'architectures et leurs utilisations peut vous aider à vous adapter aux changements dans les moteurs d'entreprise et les conditions environnementales, et choisir la meilleure solution pour votre entreprise. Considérer ces facteurs a été démontré pour améliorer les capacités de prise de décision des architectes de systèmes. [Lire la suite](https://www.intel.com/content/www/us/en/content-details/789953/content-details.html)\n\n## Intelligence Artificielle\n\n\nMark Zuckerberg a dévoilé un ensemble de chatbots à intelligence artificielle lors de Meta Connect 2023, dont un avec le rappeur Snoop Dogg en tant que Maître du Donjon dans le jeu de rôle D&D. Ces bots, inspirés de célébrités, ne sont pas disponibles au public. Meta AI est l'assistant chatbot leader, basé sur un modèle de langue personnalisé similaire à ChatGPT d'OpenAI, mettant l'accent sur l'esthétique visuelle plutôt que sur la reproduction de schémas de conversation. Les plans comprennent l'ajout de fonctionnalités vocales. [Lire la suite](https://kotaku.com/meta-quest-3-ai-chatgpt-snoop-dogg-facebook-chatbot-1850882666)\n\n\nLe Premier Ministre israélien Netanyahu met en garde contre les dangers potentiels de la révolution de l'IA, tels que les perturbations de la démocratie, la manipulation des esprits, la perte d'emploi et les guerres dirigées par l'IA. Il exhorte les nations à aborder ces préoccupations et à empêcher les machines auto-apprenantes de contrôler les humains. Cependant, il reconnaît également les aspects positifs de l'IA, tels que l'assistance robotique pour les personnes âgées et l'amélioration des transports. Israël vise à être un leader mondial dans le domaine de l'IA. [Lire la suite](https://www.foxnews.com/world/netanyahu-warns-potential-eruption-ai-driven-wars-lead-unimaginable-consquences)\n\n\nUn employé d'OpenAI, Lilian Weng, a partagé son expérience émotionnelle en utilisant le mode vocal de ChatGPT pour une conversation personnelle sur le stress et l'équilibre entre vie professionnelle et personnelle. Bien que Weng ait trouvé l'interaction réconfortante, cela soulève des préoccupations quant au rôle de l'IA dans la fourniture de thérapie. Cela reflète une tendance où l'IA cherche à paraître plus humaine mais a rencontré des défis, comme on l'a vu avec les expériences précédentes de thérapie par l'IA qui ont suscité des réponses mitigées et, dans certains cas, causé des préjudices. Les considérations éthiques sont cruciales lors de l'intégration de l'IA dans les contextes de santé mentale. [Lire la suite](https://fortune.com/2023/09/28/generative-ai-cfos-company-strategy/)\n\n## ## Cybersécurité\n\n\nOctobre marque le vingtième Mois de sensibilisation à la cybersécurité pour promouvoir la sécurité en ligne. Le thème de cette année est \"Sécurisons notre monde\". Il met l'accent sur quatre meilleures pratiques critiques en matière de cybersécurité : utiliser un gestionnaire de mots de passe, mettre en place une authentification multifactorielle, reconnaître et signaler les tentatives de phishing, et installer régulièrement les mises à jour. Cette initiative vise à fournir des informations pour aider les individus à rester plus en sécurité en ligne. [Lire la suite](https://www.ktsm.com/local/octobers-cybersecurity-awareness-month-2023/)\n\n\nLa National Security Agency des États-Unis a créé un centre de sécurité basée sur l'intelligence artificielle pour superviser l'IA dans les services de défense et de renseignement. Le directeur, le Général Paul Nakasone, a souligné l'importance de maintenir les États-Unis en tête dans le domaine de l'IA et de prévenir les acteurs étrangers de voler les innovations américaines. Le centre favorisera une adoption sécurisée de l'IA dans les industries de la sécurité nationale et de la défense. L'IA jouera un rôle important dans la sécurité nationale, la diplomatie, la technologie et l'économie. [Lire la suite](https://www.aljazeera.com/news/2023/9/29/us-national-security-agency-unveils-artificial-intelligence-security-centre)\n\n\nUn nouveau problème de sécurité a été découvert dans les navigateurs Google Chrome et Mozilla Firefox. Cela pourrait également affecter d'autres programmes. Il s'appelle \"StrangeU\". Les pirates pourraient l'utiliser pour prendre le contrôle de votre ordinateur. Les entreprises ont corrigé le problème, mais d'autres programmes pourraient encore être exposés à des risques. Assurez-vous de tenir tous vos logiciels à jour et soyez vigilants en ce qui concerne des problèmes similaires dans d'autres applications. Restez en sécurité ! [Lire la suite](https://arstechnica.com/security/2023/09/new-0-day-in-chrome-and-firefox-is-likely-to-plague-other-software/)\n\n## Podcast sur l'adoption de la transformation numérique\n\nCette semaine se poursuit la série sur les architectures multi-hybrides de cloud avec des interviews supplémentaires des architectes de solutions cloud d'Intel. Pour écouter ces interviews de trente minutes, vous pouvez les trouver en adoptant la transformation numérique sur vos sites de podcast préférés. [Lire la suite](https://www.embracingdigital.org/episode-EDT163)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW35-fr","image":"./briefs/edw-35/fr/thumbnail.png","lang":"fr","summary":"Nouvelles sur la transformation numérique pour la semaine du 1er octobre, y compris des histoires sur la gestion des données, l'intelligence artificielle et la cybersécurité. Venez écouter la célébration du mois de sensibilisation à la cybersécurité, une IA qui fait de Snoop Dogg un maître de donjon pour D&D, et découvrez une nouvelle architecture de données nommée Polystore."},{"id":202,"type":"News Brief","title":"2023-10-7","tags":["ai","edge","cybersecurity","precrime","aihealcare","mgmbreach","scatteredspider","aplhav","aiethics","convergeditot","iot","criticalinfrastructure"],"body":"\n\n## Cyber-sécurité\n\n\nLe 5 octobre, MGM Resorts International a annoncé qu'une violation de données et une attaque par ransomware avaient entraîné des perturbations, entraînant une perte de cent millions de dollars au cours du troisième trimestre. Les groupes de hackers AlphaV et Scattered Spider ont revendiqué la responsabilité de la violation, au cours de laquelle ils auraient volé des données du système de MGM et les auraient utilisées pour exercer une extorsion de fonds. [Lire la suite](https://www.reuters.com/business/mgm-expects-cybersecurity-issue-negatively-impact-third-quarter-earnings-2023-10-05/)\n\n\nSecurityWeek a rapporté la découverte de firmware contenant des portes dérobées dans les appareils Android utilisés par les écoles américaines. Les chercheurs ont identifié des vulnérabilités dans les ordinateurs portables et les tablettes distribués aux institutions éducatives, ce qui pourrait potentiellement exposer les données sensibles des étudiants aux menaces cybernétiques. Le firmware compromis pourrait permettre aux cybercriminels d'accéder et de manipuler les appareils à distance s'il est exploité. [Lire la suite](https://www.securityweek.com/android-devices-with-backdoored-firmware-found-in-us-schools/)\n\n\nDans le contexte du Mois de sensibilisation à la cybersécurité de 2023, il est souligné par SC Media que plus que simplement promouvoir la sensibilisation à la cybersécurité est nécessaire. SC Media appelle les organisations à intensifier le partage d'informations sur les attaques cybernétiques ainsi que sur les meilleures pratiques en matière de cybersécurité et démystifie les mythes entourant le partage d'informations précieuses au sein de l'industrie de la cybersécurité. [Lire la suite](https://www.scmagazine.com/perspective/cybersecurity-awareness-month-2023-why-we-need-more-than-just-cybersecurity-awareness)\n\n## Intelligence Artificielle\n\n\nSelon le rapport de 2023 de \"Freedom on the Net\" de Freedom House, les gouvernements du monde entier utilisent l'intelligence artificielle pour la surveillance et la censure. La Chine est à la pointe de la censure alimentée par l'IA. Cela met en évidence la nécessité d'équilibrer l'avancement technologique avec la protection des droits individuels et des libertés numériques. Le rapport souligne la nécessité d'efforts mondiaux pour protéger la liberté numérique des individus. [Lire la suite](https://gizmodo.com/freedom-house-2023-freedom-on-the-net-report-ai-1850887842)\n\n\nUn article récent publié dans Nature discute de la recherche sur l'utilisation d'algorithmes d'intelligence artificielle pour prédire et gérer les crises d'épilepsie. L'étude présente des résultats prometteurs en prévoyant avec précision les crises chez un groupe de patients, offrant ainsi la possibilité d'une gestion plus efficace des crises et d'une amélioration des résultats pour les patients. La prédiction des crises basée sur l'IA pourrait considérablement améliorer la qualité de vie des personnes atteintes d'épilepsie et réduire les risques associés. [Lire la suite](https://www.nature.com/articles/s41746-023-00931-7)\n\n\nSelon un rapport de la région de la baie de NBC, un professeur de l'Université de Californie à Berkeley est en train de mener des recherches novatrices sur l'utilisation de l'intelligence artificielle (IA) pour lutter contre la violence conjugale. Le professeur développe des algorithmes d'IA pour analyser les publications sur les réseaux sociaux et les messages textes à la recherche de signes de violence domestique, dans le but de fournir une intervention précoce et un soutien aux victimes. Cette application innovante de la technologie de l'IA a le potentiel d'avoir un impact significatif dans l'identification et la résolution des problèmes de violence domestique à l'ère numérique. On dirait le début de Pré-Crime dans la région de la baie ! [Lire la suite](https://www.nbcbayarea.com/news/local/uc-berkeley-professor-artificial-intelligence-domestic-violence/3336053/)\n\n## ## Calcul en périphérie\n\n\nDes chercheurs émettent un avertissement concernant l'exposition d'environ cent mille systèmes de contrôle industriel (SCI) sur Internet, selon BleepingComputer. Cette découverte alarmante soulève des préoccupations concernant les éventuelles cyberattaques contre les infrastructures critiques et les installations de fabrication. Les dispositifs de SCI exposés représentent un risque de sécurité important, mettant en évidence le besoin urgent d'améliorer les mesures de cybersécurité pour protéger les infrastructures critiques contre les menaces et les attaques potentielles. [Lire la suite](https://www.bleepingcomputer.com/news/security/researchers-warn-of-100-000-industrial-control-systems-exposed-online/)\n\n\nComme discuté dans un rapport sur Game Is Hard, l'informatique en périphérie émerge comme une alternative notable à la domination de NVIDIA sur le marché des puces d'IA. Avec l'essor de l'informatique en périphérie, de plus en plus d'entreprises explorent des solutions de puces d'IA adaptées au traitement local et à l'analyse des données en temps réel. Cette tendance reflète l'évolution du paysage technologique de l'IA et de la concurrence sur le marché, offrant potentiellement de nouvelles options et innovations pour diverses industries au-delà du bastion traditionnel de NVIDIA. [Lire la suite](https://gameishard.gg/news/the-rise-of-edge-computing-an-alternative-to-nvidias-dominance-in-the-ai-chip-market/282141/)\n\n\nL'article du Conseil technologique de Forbes explore comment l'IA Générative aborde les lacunes en compétences dans la convergence de l'informatique industrielle et de la technologie opérationnelle (IoT). En automatisant les tâches, en analysant les données et en optimisant les processus, l'IA Générative contribue à combler le fossé entre ces domaines traditionnellement distincts. Cette application innovante de la technologie IA a le potentiel d'améliorer l'efficacité et la productivité dans les environnements industriels, permettant une intégration et une exploitation plus fluides des systèmes informatiques et IoT. [Lire la suite](https://www.forbes.com/sites/forbestechcouncil/2023/10/04/how-generative-ai-fills-skills-gaps-for-industrial-itot-convergence/)\n\n## S'adapter et embrasser les nouvelles du numérique de transformation\n\n\n\nCette semaine Darren publie le dernier épisode de la série Embracing Multi-Hybrid Cloud axée sur l'amélioration continue dans l'opérationnalisation d'une stratégie Cloud. Écoutez sur n'importe quel site de podcast à Embracing Digital Transformation. [Lire la suite](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW36-fr","image":"./briefs/edw-36/fr/thumbnail.png","lang":"fr","summary":"Obtenez les dernières nouvelles sur la transformation numérique pour la semaine du 8 octobre 2023. Les faits saillants de cette semaine comprennent un rapport sur la perte de 100 millions de dollars par MGM Resorts suite à une cyberattaque, la manière dont l'IA est utilisée pour aider les victimes de violence domestique, et l'impact de l'informatique de périphérie sur les centres de données d'IA. Restez branché pour rester informé(e)."},{"id":203,"type":"News Brief","title":"2023-10-14","tags":["compute","edge","israelcyberattacks","hamasconflict","redalertsystemapp","powerplantcyberthreats","generativeai","cloudmigration","cmauk","publiccloudinvestigation","avoslocker","ransomeware","criticalinfrastructure","ecybersecurity","embracingdigital","edw37","zerotrustarchitecture"],"body":"\n\n## ## Cybersécurité\n\n\nIsraël lutte contre les cyberattaques pendant le conflit avec le Hamas. L'application 'Red Alert System' a été compromis[e] pour envoyer de fausses alertes et des messages politiques. Les centrales électriques israéliennes ont également été touchées. Le PDG de Secure Cyber Defense, Shawn Waldman, met en garde contre les menaces cybernétiques persistantes, soulignant les récentes informations sur des acteurs étrangers ciblant l'infrastructure critique des États-Unis. Il insiste sur la nécessité de progresser continuellement en matière de cybersécurité. [Lire la suite](https://www.wdtn.com/as-seen-on-2-news/israeli-cyberwarfare-cyberattacks-infrastructure/)\n\n\nIsraël et la Palestine ont connu une hausse du trafic internet après que Israël a déclaré la guerre au Hamas le sept octobre deux mille vingt-trois. Les données de Cloudflare révèlent que les cyberattaques visant Israël ont augmenté, y compris les attaques DDoS sur les journaux israéliens. En Palestine, des perturbations internet ont été observées, potentiellement liées à des pannes de courant. Cloudflare surveille ces tendances et propose des outils pour suivre les motifs de trafic internet. [Lire la suite](https://blog.cloudflare.com/internet-traffic-patterns-in-israel-and-palestine-following-the-october-2023-attacks/)\n\n\nLes attaques de ransomwares deviennent de plus en plus sophistiquées, avec une utilisation croissante des dispositifs de réseau pour leur diffusion. Les informations médicales protégées dans le secteur de la santé représentent une cible de premier choix. Les organisations à revenu élevé, surtout aux États-Unis, sont des cibles préférées. De nouveaux groupes émergent, et des langages comme Rust et GoLang sont adoptés. Les organisations renforcent leurs mesures de cybersécurité, et Cyble Vision est recommandé pour anticiper les menaces de ransomwares. [Lire la suite](https://thehackernews.com/2023/10/ransomware-attacks-doubled-year-on-year.html)\n\n## ## Calcul en périphérie\n\n\nLa bande de ransomwares AvosLocker a été impliquée dans des attaques contre les secteurs des infrastructures critiques aux États-Unis. Leurs tactiques comprennent l'utilisation de logiciels légitimes et d'outils d'administration système à distance open-source pour compromettre les réseaux, suivis de menaces d'extorsion de données. AvosLocker est apparu au milieu de l'année 2021 et utilise des techniques pour désactiver la protection antivirus, affectant les environnements Windows, Linux et VMware ESXi. Le groupe est connu pour utiliser des outils open-source et des tactiques \"living-off-the-land\" (LotL) pour éviter l'attribution. Le CISA et le FBI recommandent des mesures d'atténuation pour les organisations des infrastructures critiques, notamment des contrôles d'application, la limitation des services de bureau à distance, la restriction de l'utilisation de PowerShell et la maintenance de sauvegardes hors ligne. Les attaques de ransomwares ont explosé en 2023, les attaquants déployant rapidement des ransomwares après avoir obtenu un accès initial. [Lire la suite](https://thehackernews.com/2023/10/fbi-cisa-warn-of-rising-avoslocker.html)\n\n\nLa NSA a publié ELITEWOLF, un référentiel de signatures et d'analyses sur son GitHub, afin de renforcer la sécurité des Technologies Opérationnelles (OT) et de contrer les activités cyber malveillantes ciblant les infrastructures critiques. Étant donnée la menace croissante pesant sur les systèmes OT, la NSA recommande aux propriétaires et exploitants d'infrastructures OT d'utiliser ELITEWOLF dans le cadre d'une surveillance systématique vigilante. Cette initiative fait suite à l'Avis de Cybersécurité sur la Protection des Technologies Opérationnelles et des Systèmes de Contrôle contre les Attaques Cyber. [Lire la suite](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3554537/nsa-releases-a-repository-of-signatures-and-analytics-to-secure-operational-tec/)\n\n\nL'OTAN a promis de répondre fermement à toute attaque délibérée confirmée sur les infrastructures critiques de la mer Baltique. L'engagement survient à la suite de dommages causés aux infrastructures sous-marines de la région. La Finlande enquête sur l'incident, qui pourrait avoir impliqué des forces externes, y compris la Russie. S'il est prouvé qu'il s'agit d'une attaque délibérée contre les infrastructures critiques de l'OTAN, l'organisation répondra par un effort uni et déterminé. [Lire la suite](https://news.yahoo.com/attack-alliances-critical-infrastructure-deliberate-073708273.html)\n\n## ## Technologie Cloud\n\n\nL'IA génératrice réduit les efforts de migration cloud de trente à cinquante pour cent, en exploitant des modèles de langage étendus (MLE). Les MLE peuvent évaluer l'infrastructure, déplacer les charges de travail et vérifier l'efficacité de la migration. Bhargs Srivathsan de McKinsey a souligné que l'IA génératrice et le cloud sont mutuellement bénéfiques, car le cloud permet l'IA génératrice, ce qui accélère à son tour la migration cloud. Les MLE peuvent être utilisés pour la génération de contenu, l'engagement client, la création de données synthétiques et le codage. [Lire la suite](https://www.theregister.com/2023/10/11/generative_ai_cloud_migration/)\n\n\nLes entreprises ayant des solutions technologiques sur site font face à des perturbations lors de leur transition vers le cloud complet. Pour faire face à ce défi, l'approche du Cloud Hybride permet aux entreprises de maintenir les services principaux sur site. Cette approche ajoute de nouvelles capacités cloud à l'infrastructure existante, réduisant les perturbations et préservant les technologies éprouvées. Par exemple, un Cloud Hybride peut être utilisé dans les opérations des centres de contact pour superposer les canaux de discussion et de médias sociaux sur l'infrastructure existante, permettant ainsi aux organisations de gérer les fluctuations du volume d'appels sans perturbation significative. [Lire la suite](https://www.techradar.com/pro/why-businesses-should-stop-wondering-about-cloud-and-go-hybrid)\n\n\nL'Autorité de la concurrence et des marchés du Royaume-Uni (CMA) enquête sur le marché du cloud public afin d'assurer une concurrence équitable pour les entreprises et les particuliers. L'enquête examinera les activités des prestataires tels que AWS, Microsoft Azure et Google Cloud afin de répondre aux préoccupations concernant les comportements anti-concurrentiels, l'utilisation des données et les obstacles à l'entrée. La CMA vise à encourager l'innovation, offrir des prix compétitifs et garantir le choix des clients. [Lire la suite](https://www.techrepublic.com/article/cma-investigates-uk-public-cloud-market/)\n\n## ## Podcast sur l'Adoption de la Transformation Numérique\n\n\n\nDans l'épisode de cette semaine du podcast \"Embracing Digital Transformation\", Darren interviewe Rachel Driekosen sur l'utilisation de l'IA pour protéger les enfants en ligne et traduire les coupables en justice. Restez à l'écoute pour une prochaine série sur l'Architecture de Confiance Zéro dans les semaines à venir. [Lire la suite](https://www.embracingdigital.org/episode-EDT167-en)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW37-fr","image":"./briefs/edw-37/fr/thumbnail.png","lang":"fr","summary":"Transformation numérique cette semaine pour le 15 octobre 2023, rapporte des cyberattaques en cours pendant le conflit entre Israël et le Hamas et la compromission de l'application « Red Alert System ». De plus, l'IA générative a montré sa capacité à réduire les efforts de migration vers le cloud tandis que la CMA du Royaume-Uni enquête sur le marché du cloud public."},{"id":204,"type":"News Brief","title":"2023-10-21","tags":["keywords","ai","compute","cybersecurity","iranianhackers","cybersecuritybreach","ironnetclosure","secretmicrosoft365","cloudtrends","healthcareai","chinachipexports","openai","digitalnews","ubiquitouscomputing","digitaltransformation","dataencryption","aiethics","techindustry","emergingtech","intelinterview","aistrategies","quantumcomputing","datasecurity","techtrends"],"body":"\n\n## Cybersécurité\n\n\nDes pirates informatiques iraniens ont réussi à infiltrer un réseau gouvernemental du Moyen-Orient et sont restés cachés pendant huit mois avant d'être découverts. L'attaque a affecté divers organismes et l'étendue des dommages est encore en cours d'évaluation. Des experts enquêtent sur l'incident et mettent l'accent sur la nécessité de mettre en place des mesures de cybersécurité solides pour se prémunir contre de futures attaques. Cette violation rappelle l'importance de donner la priorité à la cybersécurité dans notre monde de plus en plus numérique. [Lire la suite](https://www.securityweek.com/iranian-hackers-lurked-for-8-months-in-government-network/)\n\n\nLe district scolaire du comté de Clark, dans le Nevada, a dû recourir à des devoirs écrits à la plume et au papier après qu'une attaque de ransomware ait perturbé leurs ressources numériques. Le district travaille à résoudre le problème et met en évidence la nécessité de la cybersécurité pour protéger l'infrastructure éducative des menaces cybernétiques. [Lire la suite](https://thenevadaindependent.com/article/clark-county-students-back-to-pen-and-paper-assignments-after-cybersecurity-breach)\n\n\nIronNet, une ancienne entreprise de cybersécurité licorne, a fermé ses portes après avoir fait face à des défis en matière de financement et de concurrence sur le marché. L'industrie de la cybersécurité en est encore à la phase de croissance et doit s'adapter aux menaces changeantes ainsi qu'aux pressions économiques, soulignant ainsi la nécessité de résilience et d'innovation. [Lire la suite](https://www.scmagazine.com/news/more-cybersecurity-firm-closures-expected-after-ironnet-shutters)\n\n## Calcul ubiquitaire\n\n\nLe Pentagone met en œuvre une version secrète de Microsoft 365, appelée DOD365-Secret, pour renforcer les capacités de défense nationale. Le nouveau service fournira des outils de communication et de collaboration sécurisés et est testé depuis un an par la Defense Information Systems Agency (DISA). Cette initiative souligne l'engagement du gouvernement à améliorer la cybersécurité et la protection des données, notamment dans les environnements militaires sensibles. [Lire la suite](https://federalnewsnetwork.com/on-dod/2023/10/secret-level-version-of-microsoft-365-rolls-out-to-top-pentagon-offices-this-month/)\n\n\nForbes prédit dix grandes tendances de l'informatique en nuage qui façonneront l'industrie en 2024. Ces tendances comprennent l'informatique en périphérie, la cybersécurité, l'informatique sans serveur, l'intégration de l'IA, l'adoption hybride et multi-cloud, les pratiques de cloud durables, l'informatique quantique, l'analyse des données et le développement natif du cloud. Il est important pour les entreprises et les professionnels de l'informatique de se tenir au courant de ces tendances pour naviguer dans le paysage en constante évolution du cloud. [Lire la suite](https://www.forbes.com/sites/bernardmarr/2023/10/09/the-10-biggest-cloud-computing-trends-in-2024-everyone-must-be-ready-for-now/?sh=2093527266d6)\n\n## Intelligence artificielle\n\n\nLe Jour-Q est en chemin. Les ordinateurs quantiques pourraient bientôt devenir assez puissants pour décoder les algorithmes de chiffrement qui protègent nos informations numériques aujourd'hui. Cela met en péril la sécurité des données, alors les gouvernements et les entreprises technologiques investissent dans le chiffrement et la technologie résistants aux quantiques pour se prémunir contre cela.L'Organisation mondiale de la santé (OMS) a présenté des réflexions pour réguler l'intelligence artificielle (IA) dans le secteur de la santé, selon leur annonce du dix-neuf octobre deux mille vingt-trois. Cette orientation met l'accent sur l'éthique, la transparence, la confidentialité des données et la responsabilité dans les applications d'IA pour la santé. Elle aborde le rôle croissant de l'IA dans le domaine de la santé et la nécessité d'un cadre assurant son utilisation responsable et éthique. [Lire la suite](https://www.who.int/news/item/19-10-2023-who-outlines-considerations-for-regulation-of-artificial-intelligence-for-health)\n\n\n\nLe New York Times rapporte sur les restrictions de la Chine concernant l'exportation des puces d'intelligence artificielle. Ces restrictions visent à préserver la sécurité nationale et l'indépendance technologique, et ont un impact sur les chaînes d'approvisionnement technologique mondiales. Comme l'intelligence artificielle joue un rôle de plus en plus crucial dans divers secteurs, cette mesure reflète les efforts de la Chine pour affirmer son contrôle sur les technologies avancées et soulève des inquiétudes quant aux dynamiques du commerce technologique mondial. [Lire la suite](https://www.nytimes.com/2023/10/17/business/economy/ai-chips-china-restrictions.html)\n\n\nSelon le journal en ligne The Information, OpenAI aurait apparemment suspendu le développement d'un nouveau modèle d'IA appelé \"Arrakis\", marquant un revers rare pour l'organisation. Cette décision souligne les complexités et les défis de développer des modèles d'IA avancés et met en évidence l'importance de la recherche et du développement d'IA responsables et éthiques. [Lire la suite](https://www.theinformation.com/articles/openai-dropped-work-on-new-arrakis-ai-model-in-rare-setback)\n\n\n## Nouvelles du Podcast sur l'Adoption de la Transformation Numérique\n\nDans sa série « Embracing Generative AI », Darren présente une interview avec Andy Morris, le stratégiste principal en IA d'Intel. L'interview explore le sujet de l'IA génératrice du quotidien et examine divers outils qui peuvent être utiles aux gens dans leur vie quotidienne. C'est une conversation informative qui met en lumière l'impact de l'IA dans notre vie quotidienne et comment nous pouvons en tirer parti pour notre bénéfice. [Lire la suite](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW38-fr","image":"./briefs/edw-38/fr/thumbnail.png","lang":"fr","summary":"En adoptant les actualités numériques de la semaine du 22 octobre 2023, incluant des actualités en matière de cybersécurité, d'informatique ubiquitaire et d'intelligence artificielle. Les points forts de cette semaine incluent le retour au stylo et au papier dans les établissements scolaires, la vigilance pour le Jour Q et les directives sur l'IA dans le domaine de la santé."},{"id":205,"type":"News Brief","title":"2023-10-28","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## La cybersécurité\n\n\nKaspersky rapporte sur un nouveau logiciel malveillant, surnommé \"Élégant,\" qui présente des capacités d'espionnage avancées et présente des similitudes avec des outils de piratage liés à la NSA. La firme de cybersécurité n'a pas attribué ce logiciel malveillant à un acteur de menace spécifique, mais sa découverte souligne la nécessité de mesures de cybersécurité solides face à des cyberattaques parrainées par l'État. [Lire la suite](https://cyberscoop.com/kaspersky-reveals-elegant-malware-resembling-nsa-code/)\n\n\n\nCISA et le HHS ont lancé une trousse à outils en matière de cybersécurité pour les organisations de soins de santé. La trousse vise à renforcer la posture de cybersécurité de l'industrie en offrant des ressources précieuses et des orientations pour atténuer les menaces cybernétiques. L'objectif est de renforcer la sécurité de l'infrastructure de soins de santé critique, compte tenu de l'augmentation du nombre de risques cybernétiques et des attaques de rançongiciel contre les hôpitaux récemment. Pour en savoir plus, rendez-vous sur la page d'actualités de CISA. [Lire la suite](https://www.cisa.gov/news-events/news/cisa-hhs-release-collaborative-cybersecurity-healthcare-toolkit)\n\n\nLa Californie a mis en place de nouvelles réglementations en matière de cybersécurité comprenant des évaluations de risques, des plans de réponse aux incidents, le chiffrement des données et l'authentification à plusieurs facteurs. Il est crucial que les entreprises en Californie se conforment à ces réglementations, car le non-respect peut entraîner des sanctions et des conséquences juridiques. Restez informé en vérifiant les mises à jour sur le site web de la cybersécurité en Californie. [Lire la suite](https://www.armstrongteasdale.com/thought-leadership/californias-newest-cybersecurity-rule-what-you-need-to-know/)\n\n## Calcul en périphérie\n\n\nLe Parlement du Royaume-Uni enquête sur la cybersécurité des infrastructures critiques, y compris l'énergie, les transports et les soins de santé. L'enquête vise à évaluer la préparation et la résilience de ces secteurs face aux cybermenaces potentielles. Cette initiative intervient dans un contexte de préoccupations croissantes concernant la vulnérabilité des services essentiels aux cyberattaques. Elle souligne l'importance de protéger les services vitaux contre les menaces numériques dans un monde interconnecté. Le récent conflit entre Israël et le Hamas met en évidence la nécessité de protéger les infrastructures critiques contre les cybermenaces. [Lire la suite](https://www.bankinfosecurity.com/uk-parliament-probes-critical-infrastructure-cybersecurity-a-23400)\n\n\nTenable et Siemens Energy s'associent pour améliorer la cybersécurité industrielle. La collaboration alliera l'expertise de Tenable en matière de cybersécurité aux connaissances sectorielles de Siemens Energy afin de proposer des solutions de sécurité avancées pour les infrastructures critiques. L'objectif est de renforcer la protection des systèmes énergétiques essentiels contre les menaces cyberévoluantes. L'importance de protéger les infrastructures critiques dans un monde de plus en plus interconnecté ne peut être surestimée. [Lire la suite](https://finance.yahoo.com/news/tenable-siemens-energy-expand-collaboration-130000076.html)\n\n\nLes banques singapouriennes DBS et Citibank ont subi une interruption de service en raison d'une défaillance du système de refroidissement de leur centre de données commun. Cet incident a perturbé les services bancaires et causé des désagréments aux clients. De tels incidents mettent en évidence l'importance d'une infrastructure de centre de données solide pour des services financiers ininterrompus. Les banques ont depuis rétabli leurs services, soulignant le rôle critique de la technologie dans les opérations bancaires modernes. [Lire la suite](https://www.channelnewsasia.com/singapore/dbs-citibank-outage-data-centre-cooling-system-down-3861076)\n\n## Informatique omniprésente\n\n\nAmazon a lancé un service de cloud indépendant pour l'Europe pour répondre aux préoccupations croissantes en matière de protection des données et aux exigences réglementaires. Le nouveau service cloud vise à offrir une souveraineté et une sécurité accrues des données aux clients européens, tout en permettant aux organisations de tirer parti de l'informatique en nuage de manière plus locale et conforme. [Lire la suite](https://www.euronews.com/next/2023/10/25/amazon-rolls-out-new-independent-cloud-for-europe)\n\n\nLes États-Unis et l'Australie collaborent sur le développement de puces quantiques avancées. Ce partenariat met en évidence l'importance croissante de l'informatique quantique dans la sécurité nationale et le progrès technologique. Alors que les pays s'efforcent de maintenir leur leadership dans ce domaine critique de recherche et de développement, des efforts communs comme ceux-ci deviennent de plus en plus courants. Cette collaboration témoigne de la coopération de plus en plus profonde entre les deux alliés dans le domaine de la science et de la technologie. [Lire la suite](https://foreignpolicy.com/2023/10/25/quantum-computing-united-states-australia-cooperation-allies-science-technology-chips/)\n\n\nUn rapport récent a révélé que quarante pour cent des entreprises enregistrent des pertes de revenus en raison de l'indisponibilité technologique et de la gestion des services de cloud. Cela souligne l'importance d'une infrastructure informatique solide et d'une gestion efficace du cloud. S'attaquer à l'indisponibilité et simplifier les opérations de cloud est crucial pour garantir des activités commerciales ininterrompues et maximiser les revenus. Pour en savoir plus, consultez le rapport sur les actualités du cloud computing. [Lire la suite](https://www.cloudcomputing-news.net/news/2023/oct/24/40-of-firms-lose-revenue-from-technology-downtime-and-cloud-complexity/)\n\n## Podcast sur l'adoption de la transformation numérique.\n\nUne nouvelle série intitulée \"Embracing Zero Trust\" commence cette semaine. Le Dr Anna Scott et Dave Marcus, qui sont des invités spéciaux, décriront les six piliers de la Confiance Zéro et comment ils peuvent être utilisés dans l'infrastructure existante. Pendant ce temps, la série de Darren \"Embracing Generative AI\" se poursuit avec Sunny Stueve, une spécialiste des facteurs humains chez Leidos. [Lire la suite](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW39-fr","image":"./briefs/edw-39/fr/thumbnail.png","lang":"fr","summary":"Dans la dernière édition de notre rubrique Embracing Digital This Week, nous vous présentons les derniers développements et les histoires du monde de la cybersécurité, de l'informatique en périphérie et de l'informatique omniprésente. Dans cet épisode, nous abordons un nouveau logiciel malveillant appelé Elegant, les réglementations en matière de cybersécurité en Californie, l'enquête du Parlement britannique sur la cybersécurité des infrastructures critiques, et d'autres actualités intéressantes de l'industrie technologique."},{"id":206,"type":"News Brief","title":"2023-2-26","tags":null,"body":"\n\n## Intelligence Artificielle\n\nOpenAI et Bain & Company ont formé la Global AI Services Alliance pour transformer le potentiel commercial. Bain combinera ses capacités avancées de mise en œuvre numérique et son expertise stratégique avec les outils et plateformes d'IA d'OpenAI, y compris ChatGPT, pour aider ses clients dans le monde entier à identifier et mettre en œuvre la valeur de l'IA pour maximiser leur potentiel commercial. The Coca-Cola Company est la première entreprise à s'engager avec la nouvelle alliance.\n\nL'unité cloud d'Amazon s'est associée à Hugging Face, le créateur de ChatGPT rival. Hugging Face construira la prochaine version de son modèle de langage, appelé BLOOM, sur AWS.\n\n[https://www.seattletimes.com/business/amazons-cloud-unit-partners-with-hugging-face-maker-of-chatgpt-rival/](https://www.seattletimes.com/business/amazons-cloud-unit-partners-with-hugging-face-maker-of-chatgpt-rival/)\n\nL'IA peut également être utilisée pour réellement faire du bien dans le monde. L'IA imagine des médicaments que personne n'a jamais vus. Maintenant, nous devons voir s'ils fonctionnent. Les premiers médicaments conçus avec l'aide de l'IA sont maintenant en essais cliniques, les tests rigoureux effectués sur des volontaires humains pour vérifier si un traitement est sûr - et fonctionne - avant que les régulateurs ne les autorisent à une utilisation généralisée.\n\n[https://www.technologyreview.com/2023/02/15/1067904/ai-automation-drug-development](https://www.technologyreview.com/2023/02/15/1067904/ai-automation-drug-development)\n\n## Intelligence Périphérique\n\nDes vulnérabilités de sécurité ont été trouvées dans des dispositifs industriels IoT sans fil provenant de quatre fournisseurs, créant une surface d'attaque importante pour les acteurs malveillants cherchant à exploiter les environnements technologiques opérationnels (OT). Les failles offrent des points d'entrée d'attaque à distance, permettant aux adversaires non authentifiés de s'implanter et de l'utiliser comme levier pour se propager vers d'autres hôtes, causant ainsi des dommages sérieux.\n\n[https://thehackernews.com/2023/02/critical-infrastructure-at-risk-from.html](https://thehackernews.com/2023/02/critical-infrastructure-at-risk-from.html)\n\nL'Alliance des normes de connectivité (CSA) a formé un groupe de travail dédié à la promotion de l'adoption de \"Matter\" à des fins de santé. Matter est une norme interopérable pour les dispositifs intelligents pour la maison publiée par la CSA l'année dernière. Le groupe de travail sur la santé et le bien-être alignera les principaux contributeurs de l'industrie de la santé autour de la norme Matter pour garantir des appareils de santé et de bien-être sécurisés et interopérables.\n\n[https://www.iottechnews.com/news/2023/feb/10/matter-creator-csa-announces-healthcare-working-group/](https://www.iottechnews.com/news/2023/feb/10/matter-creator-csa-announces-healthcare-working-group/)\n\nLes robo-taxis de Zoox, la filiale de véhicules autonomes d'Amazon, circulent désormais dans les rues de Foster City, en Californie. Les véhicules de Zoox sont spécialement conçus pour une conduite autonome et ne comportent aucun contrôle manuel. Les employés à temps plein de l'entreprise sont les premiers à utiliser le service de navette autonome entre leurs campus. Cela nécessite une confiance absolue dans la qualité du travail de votre équipe !\n\n[https://www.iottechnews.com/news/2023/feb/14/amazon-robotaxis-hit-the-streets-of-california/](https://www.iottechnews.com/news/2023/feb/14/amazon-robotaxis-hit-the-streets-of-california/)\n\n## Gestion des données\n\nDans une étude récente d'Informatica sur la dissémination des données, Informatica a constaté que la dispersion des données et le besoin de gestion unifiée sont des points de douleur courants pour de nombreuses organisations. De plus, la qualité des données et la gouvernance sont des priorités absolues. Cela conduit à une augmentation des investissements dans les technologies de données cette année. La recherche basée sur des sondages a révélé que 78 % des CDO ont déclaré qu'ils devaient s'aligner stratégiquement avec les organisations générant des revenus pour justifier les dépenses supplémentaires visant à améliorer l'analyse et la gouvernance des données.\n\n[https://accelerationeconomy.com/data/informatica-research-highlights-data-sprawl-why-management-needs-to-be-unified/](https://accelerationeconomy.com/data/informatica-research-highlights-data-sprawl-why-management-needs-to-be-unified/)\n\nAlation Inc., un fournisseur de solutions de renseignement sur les données d'entreprise, a lancé Alation Marketplaces. Ce nouveau produit permet aux ensembles de données tiers d'augmenter les données existantes dans le catalogue de données d'Alation, permettant aux utilisateurs d'explorer des données tierces externes et de compléter les ensembles de données existants. De plus, la société a étendu ses solutions Alation Anywhere à Microsoft Teams et Alation Connected Sheets à Microsoft Excel pour permettre aux utilisateurs de données d'accéder aux informations contextuelles du catalogue directement dans leur outil de choix.\n\n[https://www.alation.com/press-releases/alation-launches-data-marketplaces/](https://www.alation.com/press-releases/alation-launches-data-marketplaces/)\n\n\n","guests":null,"link":"/brief-EDW4-fr","image":"./briefs/edw-4/fr/thumbnail.png","lang":"fr","summary":"Summary"},{"id":207,"type":"News Brief","title":"2023-11-11","tags":["homelawncaretools","ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Cybersécurité\n\n\n\nLes ports australiens sont aux prises avec une cyberattaque, touchant les opérations de DP World. L'attaque contre le géant de la navigation a entraîné des perturbations dans les mouvements de conteneurs. Alors que l'ampleur et l'origine de la violation sont en cours d'investigation, cela soulève des inquiétudes quant à la vulnérabilité de l'infrastructure critique face aux menaces cybernétiques. Les autorités travaillent à rétablir la normalité alors que l'incident souligne l'importance croissante de la cybersécurité dans la protection des services essentiels. [Lire la suite](https://www.bloomberg.com/news/articles/2023-11-11/australian-authorities-meet-as-dp-world-shuts-ports-on-cyber-act)\n\n\nLa ICBC, la plus grande banque du monde, a été victime d'une cyberattaque de rançongiciel. La violation a perturbé les opérations, affectant les services à la clientèle. Alors que l'ampleur de l'attaque est en train d'être évaluée, elle souligne la menace croissante de la cybercriminalité pour les institutions financières. La ICBC travaille à contenir la violation et à rétablir la normalité, soulignant la nécessité de mesures de cybersécurité robustes dans le secteur bancaire mondial. [Lire la suite](https://www.cnbc.com/2023/11/10/icbc-the-worlds-biggest-bank-hit-by-ransomware-cyberattack.html)\n\n\nLes cyberattaques soutiennent les attaques cinétiques ! Le groupe de piratage russe SandWorm est impliqué dans une cyberattaque qui a perturbé l'approvisionnement en électricité en Ukraine. L'attaque sophistiquée visait l'infrastructure énergétique, causant des pannes généralisées. Les autorités ukrainiennes enquêtent sur l'incident, mettant en lumière la menace constante que représentent les cyberattaques parrainées par l'État pour les infrastructures critiques. La situation souligne le besoin de renforcer les mesures de cybersécurité pour protéger les services essentiels. [Lire la suite](https://thehackernews.com/2023/11/russian-hackers-sandworm-cause-power.html)\n\n## Intelligence Artificielle\n\n\nLes experts en IA mettent en garde contre une éventuelle pénurie de données, soulignant la diminution des ressources de données mondiales nécessaires pour alimenter l'intelligence artificielle. La croissance exponentielle des applications d'IA et des modèles gourmands en données dépasse la génération de données. Cette rareté pourrait entraver les progrès futurs de l'IA, soulignant la nécessité d'approches innovantes pour la collecte et la synthèse des données. Le défi souligne l'importance d'un développement de l'IA responsable et durable. [Lire la suite](https://www.sciencealert.com/the-world-is-running-out-of-data-to-feed-ai-experts-warn)\n\n\nLa prolifération de l'IA menace la démocratie ! mettent en garde des experts lors de la conférence Reuters Next. Le progrès rapide de l'intelligence artificielle suscite des inquiétudes quant à son utilisation potentielle pour la surveillance, la manipulation et l'érosion des valeurs démocratiques. Les experts appellent à des directives éthiques, des cadres réglementaires et un débat public pour atténuer les risques associés à l'influence croissante de l'IA sur la politique et la société. [Lire la suite](https://www.reuters.com/technology/reuters-next-rapid-ai-proliferation-is-threat-democracy-experts-say-2023-11-08/)\n\n\nDans une interview, Barack Obama a exprimé sa préoccupation concernant l'impact de l'intelligence artificielle (IA) sur l'emploi. Il reconnaît le potentiel de déplacement d'emplois dû à l'automatisation et souligne la nécessité de politiques pour répondre à ces défis. Obama suggère d'investir dans l'éducation et les programmes de reconversion afin de préparer la main-d'œuvre pour les marchés de l'emploi en évolution façonnés par l'IA et l'automatisation. [Lire la suite](https://www.independent.co.uk/tv/news/barack-obama-biden-ai-jobs-b2445354.html)\n\n## Informatique omniprésente\n\n\nUne percée dans l'informatique quantique intègre l'IA et l'apprentissage automatique pour une correction d'erreur de qubit robuste. Des scientifiques ont développé une approche de l'informatique quantique améliorée par l'IA qui améliore la correction des erreurs pour les qubits, un défi crucial dans l'informatique quantique. Cette combinaison innovante de l'IA et de la technologie quantique est prometteuse pour faire progresser la fiabilité et les performances des ordinateurs quantiques. [Lire la suite](https://scitechdaily.com/ai-enhanced-quantum-computing-machine-learning-powers-robust-qubit-error-correction/)\n\n\nGroupe Cloud Software, la société mère de Citrix, met fin à de nouvelles transactions commerciales en Chine, y compris à Hong Kong et à Macao, à compter du trois décembre. Évoquant le \"coût croissant\" de l'exploitation dans la région, la décision fait suite à des mouvements similaires d'autres entreprises technologiques américaines dans un contexte de perspectives économiques plus faibles et de réglementations plus strictes en matière de sécurité des données en Chine. Alors que de plus en plus d'entreprises américaines quittent la Chine, Pékin a proposé d'assouplir les contrôles de sécurité des données transfrontalières pour attirer les investisseurs étrangers. [Lire la suite](https://finance.yahoo.com/news/citrix-owner-cloud-software-becomes-093000327.html)\n\n\nUn article récent de Analytics Insights prévoit les tendances du cloud computing pour deux mille vingt-quatre. Celles-ci comprennent l'importance de l'informatique en périphérie pour un traitement des données plus rapide, la montée des stratégies multi-cloud pour plus de flexibilité, et l'intégration de l'IA/ML pour une analyse de données optimisée. Des mesures de sécurité telles que l'architecture de confiance zéro seront une priorité pour faire face à l'évolution du paysage des menaces. [Lire la suite](https://www.analyticsinsight.net/cloud-trends-for-2024-whats-on-for-cloud-computing/)\n\n## Podcast sur l'Adoption de la Transformation Numérique\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW41-fr","image":"./briefs/edw-41/fr/thumbnail.png","lang":"fr","summary":"Adoptant le numérique Cette semaine du 12 novembre 2023, y compris les actualités en cybersécurité. L'intelligence artificielle et l'informatique omniprésente. Découvrez plusieurs cyberattaques importantes sur des infrastructures critiques, l'IA en manque de données, et des percées dans l'informatique quantique."},{"id":208,"type":"News Brief","title":"2023-11-18","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Cybersécurité\n\n\nL'Agence de Sécurité Cybernétique et de Sécurité des Infrastructures (CISA) prévoit de faire appliquer la responsabilité exécutive en matière de sécurité logicielle pour l'achat du gouvernement. L'initiative vise à faire signer aux dirigeants l'approbation de la sécurité des logiciels vendus au gouvernement. Ce mouvement fait partie des efforts de la CISA pour renforcer les mesures de cybersécurité et garantir l'intégrité des logiciels utilisés dans les systèmes gouvernementaux critiques. [Lire la suite](https://federalnewsnetwork.com/cybersecurity/2023/11/cisa-aims-to-make-executives-sign-off-on-security-of-software-sold-to-government/)\n\n\nLes agences américaines de cybersécurité émettent une alerte concernant une nouvelle menace de rançongiciel visant les infrastructures critiques. L'alerte met en évidence les risques potentiels et exhorte les organisations à renforcer leurs protocoles de cybersécurité. Les agences soulignent l'importance de mesures proactives, notamment la mise à jour régulière des logiciels et l'adoption de pratiques de sécurité robustes pour atténuer le risque d'attaques de rançongiciels. [Lire la suite](https://thehackernews.com/2023/11/us-cybersecurity-agencies-warn-of.html)\n\n\n\nSolarWinds fait face à une poursuite judiciaire de la part de la Securities and Exchange Commission (SEC) suite à la cyberattaque de deux mille vingt. La SEC allègue que l'entreprise a omis de divulguer ses vulnérabilités, induisant ainsi ses investisseurs en erreur. SolarWinds, connue pour son logiciel de gestion informatique, a subi une attaque en chaîne d'approvisionnement très médiatisée. Le procès souligne le regard de plus en plus attentif porté sur les obligations de divulgation en matière de cybersécurité et les potentielles conséquences juridiques pour les entreprises qui ne parviennent pas à gérer de manière adéquate de tels incidents. [Lire la suite](https://www.nytimes.com/2023/11/18/business/dealbook/solarwinds-sec-lawsuit.html)\n\n## Intelligence Artificielle\n\n\nLe PDG d'OpenAI, Sam Altman, démissionne face à des inquiétudes croissantes concernant la direction de l'entreprise. Le départ d'Altman fait suite à des tensions internes et à des démissions concernant l'orientation de l'entreprise vers des applications commerciales plutôt que son engagement initial envers des avantages de l'IA à large spectre. Ce changement soulève des questions sur l'équilibre entre les intérêts corporatifs et les considérations éthiques dans le développement de l'IA. [Lire la suite](https://www.washingtonpost.com/technology/2023/11/17/openai-ceo-resigns/)\n\n\nLe récent événement sur l'IA de Microsoft souligne trois points clés. Premièrement, l'entreprise met l'accent sur son engagement envers le développement responsable de l'IA, en se concentrant sur l'équité, la transparence et la responsabilité. Deuxièmement, Microsoft vise à autonomiser les développeurs avec des outils comme le GPT de OpenAI, qui offre des capacités avancées de langage naturel. Enfin, l'événement souligne la dévotion de Microsoft à l'innovation guidée par l'IA à travers les industries, en mettant en avant les applications pour la santé, la finance et la science du climat. [Lire la suite](https://www.cnbc.com/2023/11/17/here-are-three-key-takeaways-from-microsofts-bullish-ai-event-.html)\n\n\nYouTube est confrontée au défi du contenu deepfake alors que les créateurs d'IA produisent des vidéos de plus en plus convaincantes et trompeuses. La plateforme a du mal à distinguer entre les utilisations inoffensives et malveillantes du contenu généré par l'IA. YouTube reconnaît la nécessité d'une vigilance et d'une approche globale pour lutter contre les deepfakes afin de préserver l'intégrité de son contenu. On demande désormais aux créateurs de contenu de vérifier si le contenu est généré par une IA ou non. [Lire la suite](https://apnews.com/article/youtube-artitifical-intelligence-deep-fake-ai-creaters-0513fd9fddbd93af327f0411dd29ff3d)\n\n## Informatique Ubiquitaire\n\n\n\nAmazon et Microsoft se livrent une concurrence acharnée pour obtenir des contrats lucratifs de cloud computing gouvernemental, car les agences fédérales dépendent de plus en plus des services cloud. Les entreprises convoitent le contrat du Joint Warfighter Cloud Capability (JWCC) du Département de la Défense, un accord valant potentiellement dix milliards de dollars. Ce combat illustre la tendance plus générale des géants de la technologie à se disputer les contrats de cloud gouvernemental et souligne l'importance stratégique des services de cloud dans la modernisation de l'infrastructure gouvernementale. [Lire la suite](https://www.bloomberg.com/news/articles/2023-11-16/amazon-amzn-microsoft-msft-fight-for-cloud-computing-government-contracts)\n\n\nLa Marine américaine améliore ses systèmes de torpilles en exploitant l'informatique en nuage pour les sous-marins. Cette mise à niveau vise à améliorer les performances et les capacités des torpilles. La Marine cherche à améliorer le traitement et l'analyse des données en utilisant l'informatique en nuage, permettant des opérations de torpilles plus sophistiquées et efficaces. Cette démarche reflète les efforts constants de l'armée pour intégrer des technologies avancées, telles que l'informatique en nuage, afin de renforcer les capacités de ses systèmes navals. [Lire la suite](https://www.defensenews.com/naval/2023/11/16/us-navy-upgrading-torpedoes-leveraging-cloud-computing-for-submarines/)\n\n\nLe gouvernement américain met en garde Alibaba en raison de préoccupations liées à l'utilisation potentielle abusive de puces fabriquées en Amérique dans sa technologie de surveillance. Cette démarche fait partie d'un effort plus large visant à restreindre les exportations de technologies sensibles en raison de préoccupations en matière de sécurité nationale. Ce développement souligne la surveillance des entreprises technologiques chinoises. Il met en lumière la focalisation du gouvernement américain sur la prévention de l'utilisation non intentionnelle de la technologie américaine de manière à compromettre la sécurité ou les droits de l'homme. [Lire la suite](https://finance.yahoo.com/news/alibaba-flagged-us-chip-curbs-192349433.html)\n\n## Actualités du Podcast sur l'Adoption de la Transformation Numérique\n\n\n\nJetez un coup d'œil aux trois séries intitulées \"Adopter l'IA générative\", \"Adopter la confiance zéro\", et \"Adopter le multi-hybride cloud\". Dans ces séries, Darren explore le monde de l'IA avec des invités de l'industrie, de l'éducation et des gens ordinaires, reçoit des experts en cybersécurité pour parler de la protection du monde numérique, et cherche dans le ciel les dernières stratégies et technologies cloud. [Lire la suite](https://www.embracingdigital.org/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW42-fr","image":"./briefs/edw-42/fr/thumbnail.png","lang":"fr","summary":"Transformation numérique pour la cybersécurité, l'IA et l'informatique pendant la semaine du 19 novembre 2023. Les sujets incluent les implications juridiques de la cybersécurité, le départ de Sam Altman de OpenAI et la concurrence des fournisseurs de Cloud pour les contrats gouvernementaux."},{"id":209,"type":"News Brief","title":"2023-11-25","tags":["uniquehandmadewoodfurniture.","ai","compute","cybersecurity","samaltman","openai","aibreakthrough","neuralchat7b","intel","responsibleai","meta","hacking","insiderthreats","securitymeasures","eucybersecurityregulations","clearfake","malware","ubiquitouscomputing","macstadium","m2prochip","alibabacloud","cloudcomputing"],"body":"\n\n## Intelligence Artificielle\n\n\nSam Altman a quitté OpenAI après une lettre exprimant des préoccupations concernant une percée en intelligence artificielle non divulguée. La lettre, envoyée par un employé anonyme, a conduit à la destitution d'Altman en tant que PDG. Les détails de la percée restent confidentiels, soulevant des questions sur la dynamique interne d'OpenAI. Altman a été réintégré lorsque sept cents des sept cent soixante-dix employés l'ont suivi chez Microsoft. [Lire la suite](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/)\n\n\nIntel a développé le NeuralChat Sept B, un modèle de chatbot qui priorise la protection des données de l'utilisateur. Le modèle est formé en utilisant l'Optimisation de la Vie Privée Différentielle (DPO) pour renforcer les mesures de protection de la vie privée. En mettant en œuvre la DPO, Intel vise à garantir des interactions sécurisées au sein du chatbot et à atténuer les risques liés à la vie privée. Ces efforts sont en ligne avec les normes en évolution pour le développement responsable de l'IA. [Lire la suite](https://medium.com/@bnjmn_marie/neuralchat-7b-intels-chat-model-trained-with-dpo-e691dfd52591)\n\n\nMeta, la société mère de Facebook, aurait dissout son équipe de l'IA Responsable, soulevant des inquiétudes quant à son engagement envers des pratiques d'IA éthiques. Les critiques soutiennent que la dissolution de l'équipe pourrait compromettre les efforts visant à traiter les préoccupations éthiques et les éventuels biais dans les technologies d'IA de Meta, mettant en évidence le débat en cours autour du développement responsable de l'IA. [Lire la suite](https://www.theverge.com/2023/11/18/23966980/meta-disbanded-responsible-ai-team-artificial-intelligence)\n\n## Cybersécurité\n\n\nUn cadre d'une entreprise de cybersécurité a plaidé coupable de piratage d'hôpitaux dans un retournement de situation choquant. Le dirigeant, auparavant chargé de la protection des systèmes, a admis avoir exploité les vulnérabilités pour son gain personnel. Cette violation soulève de sérieuses questions sur les menaces internes et la nécessité de mesures de sécurité robustes au sein même du secteur de la cybersécurité. Le plaidoyer de culpabilité souligne l'impératif de protocoles de cybersécurité stricts et de conduite éthique, surtout lorsqu'on est chargé de protéger des infrastructures essentielles comme les institutions de santé. [Lire la suite](https://www.bleepingcomputer.com/news/security/cybersecurity-firm-executive-pleads-guilty-to-hacking-hospitals/)\n\n\nL'UE prévoit d'élargir les réglementations en matière de cybersécurité pour couvrir des secteurs critiques tels que l'énergie, les transports et les fournisseurs de services numériques comme Amazon, Google et Microsoft. Ceci dans le but de renforcer la cybersécurité à travers diverses industries en raison de l'augmentation des menaces cybernétiques. Cependant, certains voient ces nouvelles réglementations comme un moyen de capturer des revenus provenant des géants de la technologie. [Lire la suite](https://www.finextra.com/newsarticle/43338/eu-considers-widening-scope-of-cybersecurity-regulation)\n\n\nUn nouveau logiciel malveillant appelé ClearFake a été découvert ciblant les utilisateurs de Mac. Il se déguise en une application d'antivirus légitime, trompant les utilisateurs en téléchargeant un logiciel malveillant. Les experts mettent en garde contre le potentiel de vol de données et de compromission du système. Il est essentiel de faire preuve de prudence et d'utiliser un logiciel de sécurité de bonne réputation pour contrer les menaces évolutives ciblant les plateformes macOS. [Lire la suite](https://cybersecuritynews.com/clearfake-new-malware-mac/)\n\n## Informatique omniprésente\n\n\nLe nouveau Mac Cloud de MacStadium, alimenté par la puce M2 Pro, offre des performances et des capacités avancées pour les développeurs macOS et les entreprises. Il répond à la demande croissante de développement Mac basé sur Kubernetes dans des environnements cloud, répondant aux besoins uniques de la communauté de développement Apple. La puce M2 Pro améliore considérablement les capacités d'hébergement Mac, offrant une vitesse et une efficacité améliorées. [Lire la suite](https://finance.yahoo.com/news/macstadium-unveils-powerful-next-generation-120000257.html)\n\n\n\nAlibaba a ajouté trois nouveaux dirigeants à son unité d'informatique en nuage, indiquant son engagement à stimuler son activité dans le cloud. Les dirigeants apportent une expertise diverse au marché compétitif de l'informatique en nuage et stimuleront l'innovation et la croissance. L'accent mis par Alibaba sur les services cloud s'aligne sur les demandes évolutives du paysage numérique, renforçant sa détermination à maintenir une position de leader dans le secteur mondial de l'informatique en nuage. [Lire la suite](https://www.proactiveinvestors.com.au/companies/news/1034236/alibaba-taps-three-new-cloud-computing-executives-after-nixing-spinoff-ipo-plans-1034236.html)\n\n\nSelon les rapports, l'Europe est à la traîne dans la course mondiale à l'IA et doit se concentrer sur l'informatique quantique pour rester compétitive. Investir dans la technologie quantique transformative est impératif pour que l'Europe puisse regagner son leadership technologique et faire face à l'évolution du paysage mondial de l'innovation. [Lire la suite](https://www.euronews.com/next/2023/11/23/europe-has-lost-the-ai-race-it-cant-ignore-the-quantum-computing-one)\n\n## Podcast sur l'Adoption de la Transformation Numérique\n\n\n\nCette semaine, Darren est revenu après une semaine de congé pour Thanksgiving. Il a interviewé Louis Parks, le PDG de Veridify, où il a révélé les vulnérabilités de l'infrastructure critique de la technologie opérationnelle. De plus, découvrez la nouvelle image de marque et les logos d'Embracing Digital Transformation. [Lire la suite](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW43-fr","image":"./briefs/edw-43/fr/thumbnail.png","lang":"fr","summary":"Nouvelles de la Transformation Numérique pour la semaine du 26 novembre 2023. Des changements transformationnels dans le leadership en intelligence artificielle à l'accent impératif sur la cybersécurité et le dévoilement de solutions informatiques avancées, cette compilation encapsule des moments pivot dans le paysage technologique en constante évolution. Rejoignez-nous alors que nous nous penchons sur les subtilités des développements récents, en mettant en lumière les profonds impacts qu'ils ont à travers les secteurs, et en guidant la trajectoire du futur numérique."},{"id":210,"type":"News Brief","title":"2023-12-2","tags":["ai","compute","datamanagement","aws","siemens","industrialdge","cloudconfiguration","multiaccessedgecomputing","mec","networkarchitecture","iot","realtimeapplications","edgecomputinginhealthcare","healthcaremarket","polarismarketresearch","usda","datastrategy","digitalgovernance","vastdata","aiworkflows"],"body":"\n\n## Calcul en périphérie\n\n\nAmazon Web Services (AWS) et Siemens ont uni leurs forces pour rationaliser la configuration industrielle de la périphérie au cloud. Cette collaboration vise à simplifier l'intégration des appareils de périphérie avec les services cloud, améliorant ainsi l'efficacité dans les environnements industriels. Le partenariat combine AWS IoT Greengrass et la plateforme Industrial Edge de Siemens, fournissant une solution sans faille pour configurer, gérer et optimiser les appareils de périphérie dans le cloud. Cette collaboration devrait faciliter une connectivité plus fluide et plus accessible de la périphérie industrielle au cloud. [Lire la suite](https://www.edgecomputing-news.com/2023/12/01/aws-and-siemens-team-up-for-easier-industrial-edge-to-cloud-configuration/)\n\n\n\nLa Multi-Access Edge Computing (MEC) est une architecture de réseau qui rapproche les capacités de calcul plus près de la périphérie du réseau, réduisant la latence et améliorant l'efficacité. En déployant des ressources de calcul à la périphérie, le MEC permet un traitement plus rapide des données générées par des dispositifs tels que des capteurs IoT et des appareils mobiles. Cette approche décentralisée améliore les applications et les services en temps réel tels que la réalité augmentée et les véhicules autonomes. Le MEC exploite les serveurs de périphérie à proximité des utilisateurs finaux, optimisant le traitement des données pour un calcul de haute performance à faible latence. [Lire la suite](https://www.techopedia.com/what-is-multi-access-edge-computing)\n\n\nSelon Polaris Market Research, le marché de la technologie Edge Computing dans le domaine de la santé devrait dépasser les quarante-trois milliards deux cent quatre-vingt-neuf millions de dollars américains d'ici deux mille trente-deux, affichant une croissance robuste du taux de croissance annuel composé de vingt-six virgule trois pourcent. L'adoption croissante des solutions de technologie Edge Computing dans le domaine de la santé, stimulée par les avancées dans les technologies IoT et AI, devrait améliorer l'efficacité du traitement des données, permettre des applications en temps réel et améliorer les soins aux patients. Le rapport suggère une expansion significative du marché alors que le secteur de la santé adopte la technologie Edge Computing pour une performance améliorée et un traitement décentralisé des données. [Lire la suite](https://www.prnewswire.co.uk/news-releases/edge-computing-in-healthcare-market-expected-to-cross-usd-43-29-billion-by-2032--driving-26-3-cagr-growth-polaris-market-research-301998143.html)\n\n## Gestion des Données\n\n\n\nLe USDA a révélé une nouvelle stratégie de données pour renforcer la gouvernance numérique. L'initiative exploite les données pour améliorer les services, la prise de décision et la responsabilité. Le plan met l'accent sur l'innovation basée sur les données, la collaboration et la modernisation de l'infrastructure de données. Les composants clés incluent la priorisation de la confidentialité et de la sécurité, la promotion de l'accessibilité des données, et la promotion d'une culture d'utilisation responsable des données. L'approche globale de l'USDA vise à maximiser la valeur des données pour soutenir la mission de l'agence et servir le public de manière plus efficace. [Lire la suite](https://www.nextgov.com/digital-government/2023/11/usda-unveils-new-data-strategy/392382/)\n\n\nVAST Data a publié une mise à jour de sa plateforme conçue pour simplifier les flux de travail d'IA et les opérations de cloud hybride sur AWS. La mise à jour se concentre sur l'amélioration de l'efficacité des processus gourmands en données, offrant une intégration transparente avec les services AWS. La plateforme de VAST Data vise à rationaliser la gestion des données et à accélérer les charges de travail de l'IA, offrant une expérience plus conviviale pour les organisations qui exploitent AWS pour leurs opérations de cloud hybride. La mise à jour souligne l'engagement de VAST Data à optimiser les performances et à simplifier les flux de travail complexes d'IA et de cloud hybride. [Lire la suite](https://www.datanami.com/this-just-in/vast-datas-new-platform-update-aims-to-simplify-ai-workflows-and-hybrid-cloud-operations-on-aws/)\n\n\nIntel s'est associé à Granulate pour optimiser les opérations de gestion des données sur Databricks, une plateforme utilisée pour l'analyse des mégadonnées. La technologie d'optimisation continue en temps réel de Granulate, alimentée par le matériel Intel, est conçue pour améliorer les performances et l'utilisation des ressources pour les utilisateurs de Databricks. Cette collaboration reflète les efforts de l'industrie pour améliorer les capacités d'analyse de données, permettant aux organisations de tirer pleinement parti de la puissance des mégadonnées pour une efficacité accrue et des coûts réduits. [Lire la suite](https://www.valdostadailytimes.com/ap/business/intel-granulate-optimizes-databricks-data-management-operations/article_ef0e550d-dbd3-5e5d-9a1d-75ffc7d0be4d.html)\n\n## Intelligence Artificielle\n\n\nLes Californiens exhortent leurs législateurs à prendre des mesures pour protéger les élections et garantir l'intégrité électorale. Les préoccupations croissantes concernant la sécurité électorale ont poussé les citoyens à réclamer une action législative pour protéger le processus démocratique. Cette demande reflète une prise de conscience croissante de la sécurisation des élections contre les menaces potentielles. Elle renforce l'appel aux législateurs à adopter des politiques qui améliorent la transparence, la cybersécurité et la confiance générale dans le système électoral. [Lire la suite](https://news.yahoo.com/californians-want-lawmakers-safeguard-elections-150026166.html)\n\n\nL'OTAN a annoncé une stratégie globale pour l'intelligence artificielle (IA) qui reconnaît son rôle crucial dans la guerre moderne et la sécurité. La stratégie utilise l'IA pour améliorer la prise de décision, la cybersécurité et l'efficacité opérationnelle. La décision de l'OTAN reflète l'importance croissante de l'IA dans les domaines militaires et la nécessité de rester à l'avant-garde des avancées technologiques. La stratégie met en évidence des lignes directrices éthiques et souligne l'importance de la collaboration avec les alliés et les partenaires industriels. [Lire la suite](https://news.yahoo.com/nato-artificial-intelligence-strategy-amid-143228193.html)\n\n\nIntel a récemment annoncé \"Vision AI Everywhere\", une nouvelle technologie intégrant l'IA dans les centres de données et les appareils. Cette intégration vise à améliorer les performances et l'efficacité, à optimiser les charges de travail, à renforcer les capacités des centres de données et à permettre la prise de décision en temps réel. Lors de l'événement HPE Discover, Intel a présenté diverses applications telles que des centres de données écoénergétiques et de l'imagerie médicale avancée. \"Vision AI Everywhere\" est aligné avec l'engagement d'Intel à promouvoir l'adoption de l'IA dans différents secteurs, inaugurant une nouvelle ère de l'informatique intelligente. [Lire la suite](https://siliconangle.com/2023/11/29/data-centers-devices-intels-vision-ai-everywhere-hpediscover-hpediscover/)\n\n## Adopter la Transformation Numérique\n\n\n\nEmbracingdigital.org a récemment fait peau neuve, affichant un nouveau look et un nouveau logo. Jetez un coup d'œil à l'image de marque mise à jour du spectacle, ainsi qu'à une nouvelle boutique proposant les derniers cadeaux pour les passionnés de transformation numérique. De plus, dans l'émission de cette semaine, retrouvez une interview en deux parties avec Shamim Naqvi, directeur général de SafeLiShare, où ils se penchent sur la confiance zéro et le partage de données. [Lire la suite](http://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW44-fr","image":"./briefs/edw-44/fr/thumbnail.png","lang":"fr","summary":"Veuillez consulter les dernières actualités dans le monde de la Transformation Numérique pour la semaine du 3 décembre 2023. Vous trouverez une variété d'histoires intéressantes liées à l'informatique en bordure, à la gestion des données et à l'intelligence artificielle. Cette semaine, AWS et Siemens ont collaboré pour simplifier l'informatique en bordure, tandis qu'Intel aide à améliorer la gestion des données basée sur le cloud. De plus, plusieurs gouvernements développent de nouvelles stratégies pour l'IA."},{"id":211,"type":"News Brief","title":"2023-12-9","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","usarmy","uno","cybersec","iot","aijobs","gemini","nano","pro","ultra","meta","ibm","openai","altman","aws","google","microsoft","mcdonalds","vergeio","nutanix","embracingdt"],"body":"\n\n## Cybersécurité\n\n\nL'armée des États-Unis prévoit d'améliorer sa connectivité et ses capacités de cybersécurité en introduisant un nouveau réseau unifié appelé UNO. Cette initiative vise à fournir à l'armée une communication rapide et sécurisée, qui peut aider à surmonter les défis sur le champ de bataille numérique. Le réseau coopératif est censé simplifier les opérations et renforcer la résilience face aux cybermenaces, renforçant ainsi l'infrastructure technologique de l'armée. [Lire la suite](https://www.c4isrnet.com/battlefield-tech/it-networks/2023/12/04/unified-network-promises-us-army-rapid-connectivity-cybersecurity/)\n\n\nAlors que l'Internet des objets (IoT) et l'intelligence artificielle (IA) continuent de progresser, le risque pour les infrastructures critiques dues aux menaces cybernétiques a considérablement augmenté. Les experts en cybersécurité soulignent la nécessité de meilleures mesures de sécurité pour se protéger contre ces menaces. Ils comptent sur l'intégration de l'IA et de l'apprentissage automatique pour améliorer la détection et la prévention des nouvelles cyberattaques. Ce paysage en constante évolution de la cybersécurité à l'ère de l'IoT et du cloud computing souligne le besoin constant de s'adapter et de sécuriser les écosystèmes numériques. [Lire la suite](https://readwrite.com/the-evolution-of-cybersecurity-in-the-age-of-iot-and-cloud-computing/)\n\n\nEnviron cinquante pour cent des organisations sondées prévoient de réduire leur personnel de cybersécurité, ce qui soulève des inquiétudes compte tenu des menaces cybernétiques en constante évolution. Cette tendance souligne l'importance d'une gestion stratégique de la main-d'œuvre pour maintenir des défenses numériques solides. Pendant ce temps, certaines autres entreprises ont besoin d'aide pour trouver des candidats appropriés pour remplir leurs postes de cybersécurité vacants. [Lire la suite](https://www.csoonline.com/article/1251369/almost-50-organizations-plan-to-reduce-cybersecurity-headcounts-survey.html)\n\n## Intelligence Artificielle\n\n\nGoogle a lancé une nouvelle plateforme d'IA appelée Gemini, qui vise à simplifier la formation et le déploiement de modèles d'apprentissage automatique. Cela permettra aux développeurs de créer des applications d'IA plus évolutives et efficaces. Gemini propose des outils et des ressources conviviaux pour promouvoir l'innovation et les progrès dans la technologie de l'IA. Il cible actuellement trois offres : nano pour les téléphones, pro pour les centres de données sur site et ultra pour un nuage public basé sur le cloud fronté par Bard. [Lire la suite](https://apnews.com/article/google-gemini-artificial-intelligence-launch-95d05d02051e75e20b574614ae720b8b)\n\n\n\nMeta et IBM travaillent ensemble pour développer une norme ouverte pour l'intelligence artificielle, visant à promouvoir l'interopérabilité et à contrer l'influence des grands acteurs de la technologie. En favorisant une approche plus collaborative, ils espèrent créer un paysage concurrentiel encourageant l'innovation et la diversité dans la technologie de l'IA. [Lire la suite](https://www.thestreet.com/technology/meta-and-ibm-team-up-against-dominant-big-tech-players)\n\n\nLe PDG d'OpenAI, Sam Altman, a souligné l'importance d'un développement responsable de l'IA après qu'un employé a été licencié suite à des préoccupations concernant le mauvais usage de la technologie d'IA. L'incident met en lumière les défis continus dans l'industrie technologique en ce qui concerne l'éthique et la gouvernance en intelligence artificielle. [Lire la suite](https://www.foxbusiness.com/technology/openais-sam-altman-opens-up-shock-firing)\n\n## L'informatique omniprésente\n\n\nAWS et Google critiquent les pratiques restrictives de Microsoft en matière de cloud, les accusant de limiter le choix des clients et l'interopérabilité, ce qui étouffe la concurrence sur le marché. Cela met en évidence les tensions persistantes entre les fournisseurs de cloud et les préoccupations concernant les pratiques commerciales équitables et la dynamique du marché. [Lire la suite](https://www.itpro.com/cloud/cloud-computing/aws-joins-google-in-calling-out-restrictive-microsoft-cloud-practices)\n\n\nMcDonald's et Google Cloud s'associent pour utiliser la technologie cloud et les solutions d'IA générative dans les restaurants McDonald's du monde entier. L'objectif est d'améliorer l'expérience client, d'optimiser les opérations et de stimuler l'innovation dans l'industrie de la restauration rapide. En intégrant les technologies avancées de Google Cloud, McDonald's vise à rester à la pointe de la transformation numérique, offrant des services améliorés et une efficacité opérationnelle accrue à travers son réseau mondial de restaurants. [Lire la suite](https://www.prnewswire.com/news-releases/mcdonalds-and-google-cloud-announce-strategic-partnership-to-connect-latest-cloud-technology-and-apply-generative-ai-solutions-across-its-restaurants-worldwide-302006915.html)\n\n\n\nIl a été signalé que l'acquisition de VMware par Broadcom a provoqué des changements qui ont conduit vingt pour cent de ses utilisateurs à chercher d'autres solutions de virtualisation. Cette tendance opportuniste a créé une chance pour d'autres acteurs du marché de s'établir comme alternatives viables. Malgré la dominance de VMware sur le marché de la virtualisation, la concurrence de sociétés comme Nunatix et VergeIO pousse les clients à se détourner de la plateforme établie. [Lire la suite](https://www.channelfutures.com/mergers-acquisitions/20-of-users-looking-to-escape-to-vmware-alternatives)\n\n## Podcast Embrassant la Transformation Numérique\n\n\n\nDans cet épisode de sa série sur l'adoption de la confiance zéro, Darren interviewe le PDG de SafeLiShare, Shamim Naqvi, pour discuter de la sécurité de la confiance zéro et de la collaboration de données à l'aide de l'informatique confidentielle. De plus, Darren aimerait remercier ses auditeurs pour leur soutien. Grâce à vous, le podcast compte plus de trois mille abonnés et quatre mille auditeurs hebdomadaires. [Lire la suite](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW45-fr","image":"./briefs/edw-45/fr/thumbnail.png","lang":"fr","summary":"Les actualités de la Transformation Numérique pour la semaine du 10 décembre 2023, contiennent des histoires sur la cybersécurité, l'intelligence artificielle et l'informatique ubiquitaire. Cette semaine, écoutez le plan de l'armée américaine pour améliorer la cybersécurité par l'unification, la guerre des nuages qui se porte devant les tribunaux au Royaume-Uni, et le drame d'OpenAI et Sam Altman."},{"id":212,"type":"News Brief","title":"2023-12-16","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","innovation","intel","pc","aiworkforce","snowflake","aws","solix","aiintegration","britain","nationalgrid","chineserisks","hackers","infrastructure","vulnerabilities","nsa","russianthreats","digitaltransformation","openziti","zerotrust"],"body":"\n\n## Intelligence Artificielle\n\n\nLa dernière annonce d'Intel marque un moment crucial dans l'avènement des ordinateurs portables alimentés par l'IA. Le lancement des processeurs de la série Core Ultra H et U présente la révolutionnaire architecture Meteor Lake, intégrant des capacités d'IA avancées dans des carnets ultra-minces. Cette sortie améliore les performances et rehausse la plateforme graphique Arc, portant l'expérience utilisateur à de nouveaux sommets. L'engagement d'Intel envers l'innovation signale une ère transformative sur le marché des ordinateurs portables, où l'IA devient intégrale à l'informatique de tous les jours. [Lire la suite](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\nLa conseillère municipale élue de New York City, Susan Zhuang, a reconnu avoir discrètement fait appel à l'IA pour communiquer avec le public et répondre aux demandes des médias. Enfin une politicienne qui dit la vérité ! L'utilisation de l'IA par Zhuang marque une approche unique pour dialoguer avec les électeurs, et les implications de cette stratégie axée sur la technologie pourraient remodeler le paysage de la communication politique à l'ère numérique. [Lire la suite](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\nLa création par Intel d'un PC IA personnalisé a apporté une potentielle énorme perturbation du travail aux entreprises alors qu'elles continuent d'intégrer l'intelligence artificielle (IA) dans leurs opérations. Au fur et à mesure que les technologies IA sont adoptées, les fonctions professionnelles et les journées de travail devraient changer, suscitant des préoccupations quant aux potentiels défis de la main-d'œuvre. Les entreprises se préparent à cette transition transformative et reconnaissent le besoin de stratégies proactives pour naviguer les perturbations potentielles et assurer une transition en douceur vers un futur piloté par l'IA. [Lire la suite](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Gestion des données\n\n\nFlocon de neige, une plateforme de données cloud de premier plan, a obtenu l'Autorisation FedRAMP Haute sur AWS GovCloud US Ouest et US Est. Cette étape significative met en relief l'engagement de Snowflake à respecter les normes de sécurité gouvernementales strictes, permettant aux agences fédérales de tirer parti de ses capacités avancées de gestion des données dans un environnement cloud sécurisé. L'Autorisation FedRAMP Haute souligne la position de Snowflake en tant que partenaire de confiance dans la fourniture de solutions rapides et évolutives pour les entités gouvernementales, favorisant l'innovation et l'efficacité dans la gestion des données. [Lire la suite](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\nSolix a introduit la Plateforme de Données Commune (CDP) trois virgule zéro, qui comprend une gestion avancée de données multi-cloud et des capacités d'IA d'entreprise. Cette sortie reflète l'engagement de Solix à fournir des solutions à la pointe de la technologie pour gérer et tirer des informations de grands ensembles de données. Les fonctionnalités d'apprentissage automatique et d'IA améliorées de la CDP trois virgule zéro permettent aux organisations de prendre des décisions plus éclairées basées sur les données. La tendance de la gestion de données multi-cloud stimule le développement de nombreuses nouvelles offres commerciales, et la dernière version de Solix est prête à être un contributeur significatif dans cet espace. [Lire la suite](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\nDans un rapport récent, Datanami étudie les considérations essentielles pour garantir que votre stratégie de gestion des données est prête pour l'IA avec cinq indicateurs clés. Priorisez la qualité des données, la capacité d'évolution, les capacités d'intégration, la sécurité robuste et la flexibilité. Ces éléments sont essentiels pour optimiser les systèmes alors que l'intelligence artificielle continue de façonner les processus de données. Restez en tête à l'ère de l'IA en évaluant et en améliorant ces aspects pour répondre aux exigences des technologies en évolution. Lisez davantage sur datanami.com pour des informations complètes. [Lire la suite](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Cybersécurité\n\n\n\nLe réseau national britannique aurait rompu les liens avec un fournisseur basé en Chine en raison de préoccupations liées à la cybersécurité, comme l'a rapporté Reuters le dix-sept décembre deux mille vingt-trois. Cette décision reflète l'examen mondial accru des risques potentiels associés aux partenariats technologiques étrangers, en particulier dans les secteurs d'infrastructure critiques. Ce mouvement du réseau national souligne l'accent croissant mis sur le renforcement des mesures de cybersécurité pour protéger les services essentiels contre les menaces potentielles. [Lire la suite](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\nDes pirates informatiques ont ciblé l'approvisionnement en eau des États-Unis, suscitant des alertes sur la vulnérabilité des infrastructures critiques. Axios a rapporté le seize décembre deux mille vingt-trois que cette violation pose des problèmes significatifs en matière de sécurité nationale et de sécurité publique. L'incident souligne le besoin urgent de mesures de cybersécurité robustes pour protéger les services essentiels. Les autorités enquêtent sur la violation, mettant en évidence les défis continus dans la défense des infrastructures critiques contre les menaces cybernétiques. [Lire la suite](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\nLa NSA a mis en garde contre des acteurs russes du cyberespace exploitant une vulnérabilité connue aux implications mondiales. La vulnérabilité identifiée a un impact significatif à l'échelle mondiale, soulignant l'importance d'atténuer les risques de cybersécurité. La NSA conseille aux organisations de s'occuper rapidement et de colmater cette vulnérabilité pour renforcer leur résilience face à d'éventuelles cybermenaces. [Lire la suite](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Adopter la transformation numérique\n\n\n\nDarren se tourne vers l'open source pour une architecture de réseau à confiance zéro avec openZiti avec l'un des organisateurs de la communauté open source, Philip Griffiths. Un grand merci également aux auditeurs d'Embracing Digital Transformation pour avoir partagé la nouvelle, nous avons ajouté plus de quatre mille abonnés à notre chaîne YouTube et avons eu plus de quatorze mille téléchargements ou vues de notre podcast la semaine dernière. Merci encore pour votre soutien. [Lire la suite](http://www.embracingdigital.org)\n\n## Intelligence Artificielle\n\n\n\nLa dernière annonce d'Intel marque un moment crucial dans la naissance des ordinateurs portables alimentés par l'IA. Le lancement des processeurs de la série Core Ultra H et U introduit l'architecture révolutionnaire de Meteor Lake, en incorporant des capacités d'IA avancées dans des ordinateurs portables ultra-minces. Cette sortie améliore les performances et rehausse la plateforme graphique Arc, élevant l'expérience utilisateur à de nouveaux sommets. L'engagement d'Intel envers l'innovation signale une ère transformative sur le marché des ordinateurs portables, où l'IA devient intégrale à l'informatique de tous les jours. [Lire la suite](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nLa conseillère municipale élue de New York, Susan Zhuang, a reconnu avoir discrètement utilisé l'IA pour communiquer avec le public et répondre aux demandes des médias. Enfin une politicienne qui dit la vérité! L'utilisation de l'IA par Zhuang représente une approche unique pour interagir avec les électeurs, et les implications de cette stratégie axée sur la technologie pourraient remodeler le paysage de la communication politique à l'ère numérique. [Lire la suite](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nLa création par Intel d'un PC IA personnalisé a apporté une possible perturbation massive du travail aux entreprises alors qu'elles continuent d'intégrer l'intelligence artificielle (IA) dans leurs opérations. À mesure que les technologies d'IA sont adoptées, les fonctions de travail et les jours ouvrables devraient changer, suscitant des inquiétudes concernant d'éventuels défis pour la main-d'œuvre. Les entreprises se préparent à ce changement transformateur et reconnaissent la nécessité de stratégies proactives pour naviguer les perturbations potentielles et garantir une transition en douceur vers avenir alimenté par l'IA. [Lire la suite](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Gestion des Données\n\n\n\nFlocon de neige, une plateforme de données cloud leader, a obtenu l'Autorisation FedRAMP High sur AWS GovCloud US Ouest et US Est. Cette étape importante souligne l'engagement de Flocon de neige à respecter les normes de sécurité gouvernementales strictes, permettant aux agences fédérales de tirer parti de ses capacités avancées de gestion des données dans un environnement cloud sécurisé. L'Autorisation FedRAMP High souligne la position de Flocon de neige en tant que partenaire de confiance pour fournir des solutions rapides et évolutives pour les entités gouvernementales, favorisant l'innovation et l'efficacité dans la gestion des données. [Lire la suite](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix a introduit la Plateforme de Données Communes (PDC) trois virgule zéro, qui comprend une gestion avancée des données multi-cloud et des capacités d'IA d'entreprise. Cette version reflète l'engagement de Solix à fournir des solutions de pointe pour gérer et tirer des informations de grands ensembles de données. Les fonctionnalités améliorées d'apprentissage automatique et d'IA de la PDC trois virgule zéro permettent aux organisations de prendre des décisions plus éclairées basées sur les données. La tendance de la gestion des données multi-cloud stimule le développement de nombreuses nouvelles offres commerciales, et la dernière version de Solix est sur le point d'être un contributeur significatif dans cet espace.\n [Lire la suite](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n## Intelligence Artificielle\n\n\n\nLa dernière annonce d'Intel marque un moment crucial dans la naissance des ordinateurs portables alimentés par IA. Le lancement des processeurs Core Ultra H et U série introduit l'architecture révolutionnaire Meteor Lake, intégrant des capacités avancées d'IA dans des ordinateurs portables ultra-minces. Cette sortie améliore les performances et élève la plate-forme graphique Arc, élevant l'expérience utilisateur à de nouveaux sommets. L'engagement d'Intel envers l'innovation signale une ère transformative sur le marché des ordinateurs portables, où l'IA devient intégrale à l'informatique de tous les jours. [Lire la suite](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nLa conseillère municipale élue de New York, Susan Zhuang a admis avoir discrètement employé l'IA pour communiquer avec le public et répondre aux demandes des médias. Enfin une politicienne qui dit la vérité ! L'utilisation de l'IA par Zhuang marque une approche unique pour s'engager avec ses électeurs, et les implications de cette stratégie axée sur la technologie pourraient remodeler le paysage de la communication politique à l'ère numérique. [Lire la suite](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nLa création par Intel du PC IA personnalisé a apporté un potentiel bouleversement massif du travail aux entreprises alors qu'elles continuent d'intégrer l'intelligence artificielle (IA) dans leurs opérations. Au fur et à mesure que les technologies IA sont adoptées, on s'attend à ce que les fonctions professionnelles et les journées de travail changent, soulevant des préoccupations au sujet de potentiels défis pour la main-d'œuvre. Les entreprises se préparent à cette transition transformative et reconnaissent le besoin de stratégies proactives pour naviguer à travers les perturbations potentielles et assurer une transition en douceur vers un futur piloté par l'IA. [Lire la suite](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Gestion des Données\n\n\n\nSnowflake, une plateforme de données cloud de premier plan, a obtenu l'autorisation FedRAMP Haute sur AWS GovCloud US Ouest et US Est. Cette étape importante souligne l'engagement de Snowflake à respecter les normes de sécurité gouvernementales strictes, permettant aux agences fédérales d'exploiter ses capacités avancées de gestion des données dans un environnement cloud sécurisé. L'Autorisation Haute FedRAMP souligne la position de Snowflake comme partenaire de confiance dans la fourniture de solutions rapides et évolutives pour les entités gouvernementales, favorisant l'innovation et l'efficacité dans la gestion des données. [Lire la suite](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix a introduit la Plateforme de Données Commune (PDC) trois virgule zéro, qui comprend une gestion avancée des données multi-cloud et des capacités d'IA d'entreprise. Cette version reflète l'engagement de Solix à fournir des solutions à la pointe de la technologie pour gérer et tirer des informations de grands ensembles de données. Les fonctionnalités d'apprentissage automatique et d'IA améliorées de la PDC trois virgule zéro permettent aux organisations de prendre des décisions plus éclairées basées sur les données. La tendance de la gestion des données multi-cloud stimule le développement de nombreuses nouvelles offres commerciales, et la dernière version de Solix est prête à être un contributeur significatif dans cet espace. [Lire la suite](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\n\nDans un rapport récent, Datanami explore les considérations cruciales pour garantir que votre stratégie de gestion des données est prête pour l'IA avec cinq indicateurs clés. Priorisez la qualité des données, la scalabilité, les capacités d'intégration, une sécurité robuste, et la flexibilité. Ces éléments sont cruciaux pour optimiser les systèmes à mesure que l'intelligence artificielle continue de façonner les processus de données. Restez en tête à l'ère de l'IA en évaluant et en améliorant ces aspects pour répondre aux exigences des technologies en évolution. Lisez plus sur datanami.com pour des informations complètes. [Lire la suite](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Cybersécurité\n\n\n\nSelon les informations, le Réseau national britannique a apparemment rompu les liens avec un fournisseur basé en Chine en raison de préoccupations liées à la cybersécurité, comme l'a rapporté Reuters le dix-sept décembre deux mille vingt-trois. Cette décision reflète un examen mondial accru des risques potentiels associés aux partenariats technologiques étrangers, en particulier dans les secteurs d'infrastructure critiques. Cette démarche du Réseau national souligne l'accent croissant mis sur le renforcement des mesures de cybersécurité pour protéger les services essentiels contre les menaces potentielles. [Lire la suite](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\n\nDes pirates informatiques ont pris pour cible l'approvisionnement en eau des États-Unis, suscitant des inquiétudes quant à la vulnérabilité des infrastructures essentielles. Axios a rapporté le seize décembre deux mille vingt-trois que cette violation représente de graves préoccupations pour la sécurité nationale et la sécurité publique. L'incident souligne l'urgence de mesures de cybersécurité robustes pour sauvegarder les services essentiels. Les autorités enquêtent sur la violation, mettant en évidence les défis permanents en matière de défense des infrastructures essentielles contre les menaces cybernétiques. [Lire la suite](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\n\nLa NSA a mis en garde contre les cyberacteurs russes exploitant une vulnérabilité connue aux implications mondiales. La vulnérabilité identifiée a un impact significatif dans le monde entier, soulignant l'importance d'atténuer les risques de cybersécurité. La NSA conseille aux organisations de traiter et de corriger rapidement cette vulnérabilité pour renforcer leur résilience face aux éventuelles menaces cybernétiques. [Lire la suite](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Adopter la transformation numérique\n\n\n\nDarren se tourne vers l'open source pour une architecture réseau de confiance zéro avec openZiti en collaboration avec l'un des organisateurs de la communauté open-source, Philip Griffiths. De plus, un grand merci aux auditeurs Embracing Digital Transformation pour avoir diffusé la nouvelle, nous avons ajouté plus de quatre mille abonnés à notre chaîne YouTube et avons eu plus de quatorze mille téléchargements ou vues de notre podcast la semaine dernière. Merci encore pour votre soutien. [Lire la suite](http://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW46-fr","image":"./briefs/edw-46/fr/thumbnail.png","lang":"fr","summary":"La semaine du 17 décembre 2023, dans l'actualité de la Transformation Numérique, comprend des histoires sur l'IA, la gestion de données et la cybersécurité. Découvrez la naissance du PC IA, la gestion de données Multi-cloud pour les nuages du gouvernement, et des avertissements majeurs pour la cybersécurité de l'infrastructure critique."},{"id":213,"type":"News Brief","title":"2023-12-23","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","wordpress","plugindata","xwpspamshield","ontariohack","healthcarecyber","holidayhacks","ontarioincidents","artificialintel","generativeai","aiinschool","intelnervana","cloudcomputing","cloudcost","pentagoncloud","cisco","isovalent","openziti","podcast","digitaltransf"],"body":"\n\n## Cybersécurité\n\n\n\nUn plugin WordPress voyou a compromis les données des utilisateurs en exposant les adresses e-mail, les noms d'utilisateur et les mots de passe en clair. Les chercheurs en sécurité ont découvert la violation, exhortant les utilisateurs affectés à réinitialiser immédiatement leurs identifiants. Le plugin, nommé \"X-WP-SPAM-SHIELD-PRO\", est connu pour ses activités malveillantes, soulignant l'importance des audits de sécurité réguliers pour les sites WordPress. Les administrateurs de sites Web sont invités à retirer le plugin compromis et à renforcer les mesures de sécurité pour prévenir l'accès non autorisé et l'exposition des données. [Lire la suite](https://thehackernews.com/2023/12/rogue-wordpress-plugin-exposes-e.html)\n\n\n\nLa CBC rapporte une augmentation des cyberattaques ciblant l'Ontario, au Canada, en deux mille vingt-trois. Ces incidents concernent divers secteurs, y compris les soins de santé, les municipalités et l'éducation. Les pirates informatiques exploitent les vulnérabilités, provoquant des perturbations et exposant des données sensibles. Les experts insistent sur l'amélioration des mesures de cybersécurité, la formation et la collaboration pour se protéger contre les cybermenaces évolutives et protéger les infrastructures critiques. [Lire la suite](https://www.cbc.ca/news/canada/toronto/cybersecurity-ontario-incidents-2023-1.7048495)\n\n\n\nLes cyberattaques pendant les vacances de Noël augmentent pour plusieurs raisons. Premièrement, l'activité d'achat en ligne est intensifiée, créant des opportunités pour les cybercriminels. Deuxièmement, les tentatives d'hameçonnage ont augmenté, où les attaquants trompent les victimes en utilisant des emails ou les réseaux sociaux. Troisièmement, les employés travaillant à distance présentent un risque pour la sécurité. Quatrièmement, les attaques d'ingénierie sociale peuvent être plus réussies pendant cette saison. Enfin, une négligence de sécurité potentielle en raison des festivités peut également augmenter le risque d'attaques. Il est vital d'être conscient de ces risques et de prendre des mesures pour les atténuer. [Lire la suite](https://bit-sentinel.com/5-reasons-why-cyber-attacks-increase-during-the-christmas-holidays/)\n\n## Intelligence Artificielle\n\n\n\nEn deux mille vingt-trois, l'IA générative a fait des progrès significatifs dans divers secteurs, y compris la santé, la finance et le divertissement. Jim Cramer de CNBC récapitule la fin de l'année en soulignant les contributions de l'IA à la découverte de médicaments, à la modélisation financière et à la création de contenu. Le potentiel transformateur de l'IA générative est mis en évidence, marquant son influence continue sur l'innovation et les avancées technologiques. [Lire la suite](https://www.cnbc.com/2023/12/21/jim-cramer-recaps-the-year-in-generative-artificial-intelligence.html)\n\n\n\nL'article récent de Future-Ed explore le potentiel de l'intégration de l'IA dans les écoles. Il souligne l'importance des considérations éthiques et d'une mise en œuvre responsable pour une intégration réussie. L'article fournit des idées précieuses pour les éducateurs, les décideurs politiques et les parties prenantes naviguant dans ce paysage en évolution. Le podcast Embracing Digital Transformation présentera également des entretiens avec des étudiants universitaires et leurs professeurs, discutant de leurs pensées. [Lire la suite](https://www.future-ed.org/navigating-the-artificial-intelligence-revolution-in-schools/)\n\n\n\nIntel a dévoilé une nouvelle puce, l'Intel Nervana NCS-un, pour accélérer les applications d'intelligence artificielle (IA). La NCS-un est conçue pour améliorer les performances de l'IA, offrant un débit élevé et une efficacité optimale. Intel vise à faire progresser l'innovation en IA en fournissant aux développeurs les outils pour déployer et dimensionner efficacement les modèles d'IA. La puce fait partie de l'engagement d'Intel à favoriser les applications d'IA dans diverses industries et représente une étape significative dans la stratégie d'IA de l'entreprise. [Lire la suite](https://www.intc.com/news-events/press-releases/detail/1663/intel-accelerates-ai-everywhere-with-launch-of-powerful)\n\n## Informatique omniprésente\n\n\n\nLes institutions financières, y compris Capital One et Arvest Bank, accordent la priorité au contrôle des coûts du cloud face aux défis. L'accent est mis sur l'optimisation des dépenses par un alignement stratégique avec les objectifs commerciaux, en utilisant des outils de gestion des coûts du cloud, et en mettant en œuvre des pratiques de gouvernance. À mesure que l'informatique en nuage évolue, une gestion efficace des coûts devient essentielle pour ces organisations. [Lire la suite](https://www.ciodive.com/news/cloud-cost-control-capital-one-arvest-bank/703025/)\n\n\n\nLe Washington Post rapporte la remise en question par le Pentagone de ses contrats de cloud computing, laissant supposer des changements potentiels de stratégie. L'article explore les implications de ces changements et leur impact sur les grandes entreprises technologiques en lice pour des contrats de défense lucratifs. Alors que le Pentagone navigue dans le paysage complexe du cloud computing, l'article met en évidence la dynamique en évolution et le paysage concurrentiel dans la quête de capacités technologiques avancées par le secteur de la défense. [Lire la suite](https://www.washingtonpost.com/technology/2023/12/21/pentagon-cloud-computing-contracts/)\n\n\n\nCisco effectue un mouvement stratégique dans la sécurité du cloud en acquérant Isovalent, un innovateur dans la mise en réseau et la sécurité natives du cloud. Cette acquisition reflète l'engagement de Cisco à faire avancer ses capacités en matière d'intelligence artificielle (IA) et de sécurité du cloud. L'expertise d'Isovalent en technologie eBPF (filtre de paquets de Berkeley étendu) devrait améliorer la capacité de Cisco à sécuriser les environnements cloud, offrant des solutions avancées pour le paysage évolutif de la cybersécurité dans les infrastructures basées sur le cloud. [Lire la suite](https://www.informationweek.com/it-infrastructure/cloud-computing#close-modal)\n\n## Adopter la Transformation Numérique\n\n\n\nPremièrement, j'aimerais souhaiter à tous mes auditeurs un Joyeux Noël et une Bonne Année. Dans le podcast de cette semaine, Darren explore OpenZiti, une solution de réseau open-source à zéro confiance. De plus, merci à tous ceux qui parlent du podcast : nous avons désormais dépassé dix mille auditeurs hebdomadaires, et votre soutien est grandement apprécié. [Lire la suite](https://www.embracingdigital.org)\n\n## Cybersécurité\n\n\n\nUn plugin WordPress malveillant a mis en danger les données des utilisateurs en exposant les adresses email, les noms d'utilisateur et les mots de passe en clair. Des chercheurs en sécurité ont découvert la brèche, exhortant les utilisateurs affectés à réinitialiser immédiatement leurs identifiants. Le plugin, nommé \"X-WP-SPAM-SHIELD-PRO\", est connu pour ses activités malveillantes, soulignant l'importance des audits de sécurité réguliers pour les sites WordPress. On conseille aux administrateurs de sites Web de supprimer le plugin compromis et de renforcer les mesures de sécurité pour prévenir l'accès non autorisé et l'exposition des données. [Lire la suite](https://thehackernews.com/2023/12/rogue-wordpress-plugin-exposes-e.html)\n\n\n\nLa CBC rapporte une hausse des cyberattaques visant l'Ontario, au Canada, en deux mille vingt-trois. Ces incidents touchent une variété de secteurs, dont la santé, les municipalités et l'éducation. Les pirates informatiques exploitent les vulnérabilités, causant des perturbations et exposant des données sensibles. Les experts insistent sur l'amélioration des mesures de cybersécurité, de la formation et de la collaboration pour se prémunir contre les cybermenaces en évolution et protéger les infrastructures critiques. [Lire la suite](https://www.cbc.ca/news/canada/toronto/cybersecurity-ontario-incidents-2023-1.7048495)\n\n\n\nLes cyberattaques pendant les vacances de Noël sont en hausse pour plusieurs raisons. Premièrement, l'activité d'achat en ligne est accrue, créant des opportunités pour les cybercriminels. Deuxièmement, les tentatives de hameçonnage ont augmenté, où les attaquants trompent les victimes en utilisant les e-mails ou les réseaux sociaux. Troisièmement, les employés en télétravail représentent un risque pour la sécurité. Quatrièmement, les attaques d'ingénierie sociale peuvent être plus réussies pendant cette saison. Enfin, une négligence potentielle de la sécurité due aux festivités peut également augmenter le risque d'attaques. Il est essentiel d'être conscient de ces risques et de prendre des mesures pour les atténuer. [Lire la suite](https://bit-sentinel.com/5-reasons-why-cyber-attacks-increase-during-the-christmas-holidays/)\n\n## Intelligence Artificielle\n\n\n\nEn deux mille vingt-trois, l'IA générative a fait des progrès significatifs dans divers secteurs, y compris la santé, la finance, et le divertissement. Jim Cramer de CNBC récapitule la fin de l'année en soulignant les contributions de l'IA à la découverte de médicaments, à la modélisation financière, et à la création de contenu. Le potentiel transformateur de l'IA générative est mis en évidence, marquant son influence continue sur l'innovation et les avancées technologiques. [Lire la suite](https://www.cnbc.com/2023/12/21/jim-cramer-recaps-the-year-in-generative-artificial-intelligence.html)\n\n\n\nLe récent article de Future-Ed explore le potentiel de l'intégration de l'IA dans les écoles. Il met l'accent sur l'importance des considérations éthiques et de la mise en œuvre responsable pour une intégration réussie. L'article fournit des informations précieuses pour les éducateurs, les décideurs politiques et les parties prenantes qui naviguent dans ce paysage en évolution. Le podcast Embracing Digital Transformation présentera également des interviews avec des étudiants universitaires et leurs professeurs, discutant de leurs pensées. [Lire la suite](https://www.future-ed.org/navigating-the-artificial-intelligence-revolution-in-schools/)\n\n\n\nIntel a dévoilé une nouvelle puce, l'Intel Nervana NCS-un, pour accélérer les applications d'intelligence artificielle (IA). Le NCS-un est conçu pour améliorer les performances de l'IA, offrant un haut débit et une grande efficacité. Intel vise à faire progresser l'innovation en IA en fournissant aux développeurs des outils pour déployer et mettre à l'échelle efficacement les modèles d'IA. La puce fait partie de l'engagement d'Intel à favoriser les applications de l'IA dans diverses industries et représente une étape significative dans la stratégie d'IA de l'entreprise. [Lire la suite](https://www.intc.com/news-events/press-releases/detail/1663/intel-accelerates-ai-everywhere-with-launch-of-powerful)\n\n## Informatique ubiquitaire\n\n\n\nLes institutions financières, y compris Capital One et Arvest Bank, accordent la priorité au contrôle des coûts du cloud face à des défis. L'accent est mis sur l'optimisation des dépenses grâce à un alignement stratégique avec les objectifs commerciaux, l'utilisation d'outils de gestion des coûts du cloud et la mise en place de pratiques de gouvernance. À mesure que l'informatique en nuage évolue, une gestion efficace des coûts devient cruciale pour ces organisations. [Lire la suite](https://www.ciodive.com/news/cloud-cost-control-capital-one-arvest-bank/703025/)\n\n\n\nLe Washington Post rapporte que le Pentagone réexamine ses contrats d'informatique en nuage, laissant entendre des changements éventuels de stratégie. L'article explore les implications de ces changements et leur impact sur les grandes entreprises technologiques qui soumissionnent pour des contrats de défense lucratifs. Alors que le Pentagone navigue dans le paysage complexe de l'informatique en nuage, l'article souligne la dynamique changeante et le paysage compétitif dans la quête du secteur de la défense pour des capacités technologiques avancées. [Lire la suite](https://www.washingtonpost.com/technology/2023/12/21/pentagon-cloud-computing-contracts/)\n\n\n\nCisco fait un mouvement stratégique dans la sécurité du cloud en acquérant Isovalent, un innovateur en matière de réseau et de sécurité natives du cloud. Cette acquisition reflète l'engagement de Cisco à faire progresser ses capacités en matière d'intelligence artificielle (IA) et de sécurité du cloud. L'expertise d'Isovalent en technologie eBPF (extended Berkeley Packet Filter) devrait renforcer la capacité de Cisco à sécuriser les environnements cloud, fournissant des solutions avancées pour le paysage évolutif de la cybersécurité dans les infrastructures basées sur le cloud. [Lire la suite](https://www.informationweek.com/it-infrastructure/cloud-computing#close-modal)\n\n## Adopter la Transformation Numérique\n\n\n\nPremièrement, j'aimerais souhaiter à tous mes auditeurs un Joyeux Noël et une Bonne Année. Dans le podcast de cette semaine, Darren explore OpenZiti, une solution de réseau zéro confiance open source. Aussi, merci à tous ceux qui font passer le mot au sujet du podcast : nous avons maintenant dépassé les dix mille auditeurs hebdomadaires, et votre soutien est très apprécié. [Lire la suite](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW47-fr","image":"./briefs/edw-47/fr/thumbnail.png","lang":"fr","summary":"Pendant la semaine du 24 décembre 2023, il y a eu plusieurs reportages sur la transformation numérique. Ces histoires couvraient des sujets tels que la cybersécurité, l'intelligence artificielle et l'informatique. Parmi les points forts de la semaine figuraient une augmentation des cyberattaques pendant la période des fêtes, les efforts d'Intel pour développer davantage la technologie d'intelligence artificielle, et la tendance vers le contrôle des coûts en informatique en nuage."},{"id":214,"type":"News Brief","title":"2023-12-31","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","edgecompute","digitaltransform","nsacyber","aics","tomtom","microsoft","mitresearch","aiethics","iot","healthcare","malwaredetect","patientdata","aiagriculture","predictive","genai","automation","navigation","malware"],"body":"\n\n## Cybersécurité\n\n\n\nLa solution de cybersécurité alimentée par l'IA de Checkpoint offre une plateforme révolutionnaire avec un taux de précision de quatre-vingt-dix-huit pour cent pour détecter les activités malveillantes. Le système utilise des algorithmes avancés et des analyses prédictives pour fournir aux entreprises une défense puissante contre les risques cybernétiques en évolution, renforçant la sécurité numérique lors de menaces accrues. [Lire la suite](https://fortune.com/2023/12/29/ai-cybersecurity-checkpoint/)\n\n\n\nLes attaques cybernétiques sur les établissements de santé ont fortement augmenté, suscitant des inquiétudes concernant la sécurité des patients à l'Hôpital Liberty, qui lutte contre une attaque depuis les deux dernières semaines. Des mesures de cybersécurité robustes sont nécessaires pour protéger les informations médicales sensibles et garantir la continuité des services de soins de santé. Cela met en évidence les vulnérabilités plus larges des infrastructures critiques face à l'escalade des menaces cybernétiques. [Lire la suite](https://www.kctv5.com/2023/12/30/liberty-hospital-staff-worries-patients-are-jeopardy-if-cyber-security-incident-drags/)\n\n\n\nLe rapport de l'Agence de Sécurité Nationale sur l'année de cybersécurité deux mille vingt-trois offre un aperçu des cybermenaces, des défenses réussies et des tendances émergentes. La publication souligne l'engagement de l'agence à renforcer la résilience nationale en matière de cybernétique et constitue une ressource précieuse pour les professionnels de l'industrie, les décideurs politiques et le public. Il fournit une analyse rétrospective du paysage de la cybersécurité et guide les efforts futurs pour renforcer la sécurité numérique. [Lire la suite](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3621654/nsa-publishes-2023-cybersecurity-year-in-review/)\n\n## Intelligence Artificielle\n\n\n\nDes chercheurs du MIT ont développé des algorithmes qui permettent aux machines de comprendre les nuances contextuelles dans le langage, favorisant des interactions plus précises. Cette percée dans le traitement du langage naturel a des applications potentielles dans divers domaines, comblant le fossé entre la communication humaine et la compréhension par la machine. La recherche du MIT ouvre la voie à des systèmes d'IA plus avancés et sensibles au contexte. [Lire la suite](https://news.mit.edu/2023/leveraging-language-understand-machines-1222)\n\n\n\nL'intelligence artificielle et l'automatisation transforment les gouvernements en améliorant l'efficacité et l'agilité. Ce changement vers une approche axée d'abord sur le numérique reflète un engagement à s'adapter à l'ère moderne. L'impact de ces technologies refaçonne la gouvernance et offre une approche plus avancée pour relever les défis numériques. [Lire la suite](https://federalnewsnetwork.com/commentary/2023/12/navigating-the-era-of-innovation-how-artificial-intelligence-and-automation-are-driving-a-digital-first-government/)\n\n\n\nMichael Cohen admet avoir soumis de faux documents juridiques générés par l'IA pendant qu'il travaillait pour Donald Trump, soulevant des inquiétudes quant à l'éthique de l'IA et à la surveillance dans le système juridique. Cela met en évidence le besoin d'une surveillance vigilante et de mesures de protection pour maintenir l'intégrité des procédures juridiques. L'incident souligne les défis en constante évolution de l'éthique de l'IA et de l'intégrité dans les processus juridiques. [Lire la suite](https://www.nbcnews.com/politics/politics-news/michael-cohen-says-unknowingly-submitted-fake-ai-generated-legal-cases-rcna131631)\n\n## Informatique en périphérie\n\n\n\nTomTom et Microsoft se sont associés pour introduire une solution d'intelligence artificielle générative innovante pour les véhicules connectés. La technologie exploite des algorithmes avancés pour transformer la navigation, promettant une expérience de conduite plus intelligente et plus réactive. Ce développement signifie un pas significatif en avant dans l'intégration de l'IA dans les automobiles. [Lire la suite](https://www.iottechnews.com/news/2023/dec/19/tomtom-microsoft-unveil-generative-ai-connected-vehicles/)\n\n\n\nPanasonic combat le logiciel malveillant IoT en déployant des pots de miel pour détecter les menaces cybernétiques. L'entreprise acquiert des informations précieuses sur l'évolution des tactiques de malwares en créant des cibles simulées pour attirer les activités malveillantes, renforçant ainsi les mesures de cybersécurité. Cette approche proactive reflète l'accent croissant mis sur les techniques innovantes pour protéger les appareils connectés et les réseaux face à l'escalade des défis de sécurité. [Lire la suite](https://www.wired.com/story/panasonic-iot-malware-honeypots/)\n\n\n\nL'intégration des technologies d'Intelligence Artificielle (IA) et de l'Internet des Objets (IdO) transforme rapidement l'industrie de l'agriculture. Le marché de l'agriculture de précision est prévu d'atteindre cinq virgule deux milliards d'euros d'ici deux mille vingt-sept, grâce à l'utilisation innovante de l'IdO, de l'analyse de données et de l'automatisation dans l'agriculture. Cette technologie optimise les rendements des cultures, réduit la consommation de ressources et promeut une agriculture durable et efficace dans le monde entier. [Lire la suite](https://iotbusinessnews.com/2023/12/22/53545-the-precision-agriculture-market-to-reach-e-5-2-billion-worldwide-in-2027/)\n\n## Adopter la Transformation Numérique\n\n\n\nCette semaine, l'Embracing Digital Transformation poursuit sa série Embracing Generative AI, avec des entretiens mettant en vedette une étudiante universitaire, Madeline Pulsipher, qui partage son parcours avec GenAI, suivi de la perspective d'une professeure, Laura Newey, sur l'utilisation de GenAI en classe. Ne manquez pas ces épisodes perspicaces ! [Lire la suite](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW48-fr","image":"./briefs/edw-48/fr/thumbnail.png","lang":"fr","summary":"Actualités de la Transformation Numérique pour la semaine du 1er janvier 2024, couvrant la cybersécurité, l'intelligence artificielle et l'informatique en périphérie. Les sujets comprennent l'IoT et l'IA dans l'agriculture, les politiciens trompés par les hallucinations générées par l'IA, et les cyberattaques visant le secteur de la santé."},{"id":215,"type":"News Brief","title":"2024-1-7","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","intel","articul8","digitalbridge","genai","ethics","china","itot","upskilling","airbus","atos","solix","dataplatform","microsoft","oracle","azure","database","digitaltransformation","generativeai"],"body":"\n\n## Intelligence Artificielle\n\n\n\nIntel a récemment annoncé la scission de la société de logiciels d'IA Articul8, soutenue par DigitalBridge. Le but de cette démarche est d'améliorer les capacités d'IA d'entreprise. Articul8 se concentrera sur le développement de solutions d'IA avancées, en tirant parti de l'expertise d'Intel dans le domaine. Cette décision stratégique témoigne de l'engagement d'Intel à faire progresser les technologies d'intelligence artificielle pour des applications industrielles plus larges. [Lire la suite](https://www.reuters.com/technology/intel-spins-out-ai-software-firm-with-backing-digitalbridge-2024-01-03/)\n\n\n\nUn rapport sur GenAI de Foreign Affairs aborde les préoccupations concernant l'impact de l'intelligence artificielle sur les valeurs démocratiques des États-Unis. Le rapport souligne la possibilité d'une mauvaise utilisation de l'IA dans des activités telles que la désinformation et la surveillance, et insiste sur la nécessité de directives éthiques et de cadres réglementaires pour répondre à ces préoccupations et protéger les principes fondateurs de la démocratie. [Lire la suite](https://www.foreignaffairs.com/united-states/artificial-intelligences-threat-democracy)\n\n\n\nLa Chine a récemment publié des directives à l'intention des chercheurs en intelligence artificielle, les empêchant d'utiliser certains fonds à des fins liées à l'armée. Ces directives visent à garantir que la technologie de l'intelligence artificielle est utilisée à des fins pacifiques et éthiques, avec un accent sur l'évitement de son mauvais usage dans des domaines qui pourraient potentiellement nuire à la sécurité internationale. Cette démarche met en évidence la détermination de la Chine à façonner le développement éthique et l'application de l'IA, soulignant son engagement envers l'utilisation responsable de la technologie de l'intelligence artificielle. [Lire la suite](https://www.scmp.com/news/china/science/article/3247420/china-unveils-new-artificial-intelligence-guidelines-scientists-and-bans-use-funding-applications)\n\n## Cybersécurité\n\n\n\nL'analyse de l'Intelligence de Sécurité souligne l'importité d'intégrer l'IT et l'OT dans la cybersécurité. Cette convergence est cruciale pour protéger les systèmes industriels contre les cybermenaces. L'analyse explore les défis et les avantages et met en évidence le besoin d'une stratégie de sécurité globale. [Lire la suite](https://securityintelligence.com/posts/it-and-ot-cybersecurity-integration/)\n\n\n\nL'apprentissage continu dans le domaine de la cybersécurité est essentiel pour combler le déficit de compétences. Un rapport récent préconise de renforcer les compétences des talents existants et d'investir dans des programmes de formation pour répondre à la demande de professionnels qualifiés. Les organisations doivent favoriser une culture d'amélioration continue des compétences pour renforcer la résilience cybernétique. [Lire la suite](https://www.informationweek.com/cyber-resilience/upskilling-is-the-secret-to-closing-the-cybersecurity-skills-gap-)\n\n\n\nAirbus acquiert l'unité de cybersécurité d'Atos pour deux milliards de dollars afin de renforcer ses capacités en matière de cybersécurité dans les secteurs ICS-OT. Ce mouvement souligne la nécessité de mesures de cybersécurité robustes dans les infrastructures critiques, en particulier dans les industries où la technologie opérationnelle joue un rôle essentiel. L'accord implique une étape stratégique vers l'amélioration de la cybersécurité au sein du cadre opérationnel d'Airbus. [Lire la suite](https://www.darkreading.com/ics-ot-security/airbus-acquire-atos-cybersecurity-unit-2-billion)\n\n## Gestion des Données\n\n\n\nSolix a révélé une plateforme de données d'entreprise pour l'ère de \"Gen AI\". Il vise à offrir des solutions avancées de gestion des données pour répondre aux besoins évolutifs des technologies d'IA. La plateforme de Solix est axée sur l'optimisation des capacités de traitement et de stockage des données pour soutenir les exigences des applications et des analyses pilotées par l'IA. Ce mouvement signifie un pas stratégique vers l'activation d'une utilisation efficace des données dans le domaine rapidement en évolution de l'intelligence artificielle. [Lire la suite](https://venturebeat.com/data-infrastructure/solix-launches-new-enterprise-data-platform-for-the-gen-ai-era/)\n\n\n\nMicrosoft et Oracle se sont associés pour améliorer la compatibilité entre leurs bases de données, Microsoft SQL Server et Oracle Database, sur la plateforme cloud d'Azure de Microsoft. Ce partenariat vise à offrir une expérience sans couture pour les utilisateurs qui exécutent ces bases de données sur Azure, avec une performance et une flexibilité améliorées. La collaboration vise à répondre aux exigences évolutives des solutions de base de données basées sur le cloud, en soutenant l'interopérabilité et la facilité d'utilisation pour les entreprises utilisant à la fois les technologies Microsoft et Oracle sur Azure. [Lire la suite](https://www.infoq.com/news/2024/01/microsoft-oracle-database-azure/)\n\n\n\nEn deux mille vingt-quatre, les données d'entreprise et les technologies d'IA subissent des changements significatifs. Les tendances comprennent l'expansion du rôle de l'IA dans la gestion des données, l'accent renforcé sur l'éthique des données et la confidentialité, l'ascension de l'analytique augmentée, et l'influence de l'informatique en périphérie sur le traitement des données. Les organisations sont exhortées à s'adapter à ces dynamiques en évolution pour rester compétitives et exploiter le potentiel complet des données et technologies d'IA cette année. [Lire la suite](https://tdwi.org/articles/2024/01/05/ta-all-shifting-sands-in-enterprise-data-and-ai-technologies-in-2024.aspx)\n\n## Podcast Embrassant la Transformation Numérique\n\n\n\nCette semaine sur le podcast, Darren poursuit sa série sur l'adoption de l'IA générative quand il interviewe Laura Newey après un semestre d'enseignement de l'anglais à l'université avec Chat GPT et l'IA générative comme outil. Vous devez vérifier cet épisode et le partager avec vos étudiants universitaires. [Lire la suite](https://www.embracingdigital.org/en)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW49-fr","image":"./briefs/edw-49/fr/thumbnail.png","lang":"fr","summary":"Intel a récemment annoncé la scission de l'entreprise de logiciels d'IA Articul8, soutenue par DigitalBridge. Le but de cette manoeuvre est d'améliorer les capacités d'IA en entreprise. Articul8 se concentrera sur le développement de solutions avancées d'IA, en tirant parti de l'expertise d'Intel dans ce domaine. Ce mouvement stratégique signifie l'engagement d'Intel à faire progresser les technologies d'intelligence artificielle pour des applications industrielles plus larges."},{"id":216,"type":"News Brief","title":"2023-3-5","tags":null,"body":"\n\n## Intelligence Artificielle\n\nLes chatbots basés sur l'IA générative sont loin d'être parfaits. ChatGPT, le chatbot de Bing de Microsoft, Galactica de Meta (une IA générative conçue pour aider les scientifiques à des tâches telles que l'annotation des protéines ou l'écriture de code), et d'autres systèmes ont été retirés ou limités après avoir été découverts pour générer des informations peu fiables ou incorrectes, ou avoir sombré dans des réactions émotionnelles apparemment menaçantes.\n\nLes gouvernements américain et européen collaborent sur une étude de recherche en IA pour développer des stratégies de réglementation et de promotion de l'innovation en matière d'IA. L'étude portera sur cinq domaines : la prévision des conditions météorologiques extrêmes et du climat, la gestion des interventions d'urgence, l'amélioration de la santé et de la médecine, l'optimisation du réseau électrique et l'optimisation de l'agriculture. Les participants incluent le Département de l'énergie des États-Unis, le Département de l'Agriculture, le Pentagone, l'Administration nationale océanique et atmosphérique, les agences de santé et de science, ainsi que leurs homologues européens.\n\n[https://www.axios.com/2023/03/01/ai-research-us-eu](https://www.axios.com/2023/03/01/ai-research-us-eu)\n\nAirdot Deploy : Déploie automatiquement des modèles ML et les met à l'échelle. Identifie automatiquement les packages requis, comprend la dépendance des modules, refactorise le code, construit des API REST autour du modèle ML, le met en conteneur, crée l'infrastructure, l'échelle automatiquement et configure des alertes et une surveillance automatisées, tout cela avec juste une seule ligne de code. Fini pour votre équipe DevOps.\n\n[https://www.airdot.io/blog/announcing-airdot-deploy-ml](https://www.airdot.io/blog/announcing-airdot-deploy-ml)\n\n## Sécurité informatique\n\nUn rapport de Tenable révèle que les cyberattaques sont principalement menées en utilisant des vulnérabilités connues pour lesquelles des correctifs sont déjà disponibles. Les auteurs de menaces continuent d'exploiter des vulnérabilités que les organisations n'ont pas réussi à corriger ou à remédier, notamment des failles de gravité élevée dans Microsoft Exchange et les solutions de réseau privé virtuel.\n\n[https://www.helpnetsecurity.com/2023/03/03/known-exploitable-vulnerabilities/?web_view=true](https://www.helpnetsecurity.com/2023/03/03/known-exploitable-vulnerabilities/?web_view=true)\n\nLes CISO municipaux font face à des défis alors que les menaces cybernétiques augmentent, les attaques de rançongiciels étant la plus grande préoccupation. Oakland, en Californie, a récemment déclaré l'état d'urgence après une attaque de rançongiciel par le groupe Play qui a perturbé les systèmes téléphoniques et les services non urgents. Cette attaque fait partie d'une série d'attaques de rançongiciels contre des gouvernements locaux aux États-Unis, notamment à Baltimore, La Nouvelle-Orléans, Pensacola et Atlanta.\n\n[https://www.csoonline.com/article/3688958/municipal-cisos-grapple-with-challenges-as-cyber-threats-soar.html](https://www.csoonline.com/article/3688958/municipal-cisos-grapple-with-challenges-as-cyber-threats-soar.html)\n\nLe président Biden a signé la loi sur la préparation à la cybersécurité de l'informatique quantique, exigeant que les agences fédérales accordent la priorité à l'utilisation de technologies résistantes à l'informatique quantique. La loi oblige à fournir des directives pour évaluer les systèmes critiques avec des normes de cryptographie post-quantique. Cela fait suite aux craintes que la technologie quantique rende les cryptages existants vulnérables à une rupture rapide.\n\n[https://fedscoop.com/biden-signs-quantum-computing-cybersecurity-act-into-law/](https://fedscoop.com/biden-signs-quantum-computing-cybersecurity-act-into-law/)\n\n## Informatique Ubiquitaire\n\nLe rapport annuel d'Accenture sur le cloud bancaire a révélé que les banques ciblant les systèmes centraux pour la migration vers le cloud sont confrontées à un facteur de risque majeur en raison de la difficulté de trouver du personnel qualifié en cloud computing. Les banques perçoivent la migration centrale comme étant à haut risque en raison du manque de personnel ayant une expertise en cloud computing et de la difficulté de recruter les compétences requises dans l'environnement actuel. L'adoption du cloud public représente à la fois une mise à niveau de la sécurité et un facteur de risque, reflété par le manque d'adoption du cloud dans l'industrie bancaire.\n\n[https://www.ciodive.com/news/banks-finance-cloud-migration-skils-gap-cybersecurity/644169/](https://www.ciodive.com/news/banks-finance-cloud-migration-skils-gap-cybersecurity/644169/)\n\nLa société de data warehouse basée sur le cloud, Snowflake, prévoit d'ajouter plus de 1 000 employés au cours de l'exercice fiscal en cours, selon le directeur financier Mike Scarpelli lors de l'appel aux résultats du quatrième trimestre de l'entreprise. Cela fait suite à l'ajout de 1 900 personnes l'année dernière, contrairement à la tendance à la réduction des effectifs chez les autres grandes entreprises technologiques. Snowflake prévoit de donner la priorité à l'embauche dans les domaines de la production, de l'ingénierie et des ventes.\n\n[https://www.ciodive.com/news/Snowflake-expands-workforce-extends-AWS-partnership/644034/](https://www.ciodive.com/news/Snowflake-expands-workforce-extends-AWS-partnership/644034/)\n\nSelon les experts de l'industrie, les directeurs informatiques doivent restructurer leurs départements et stratégies informatiques pour tirer pleinement parti de la transformation cloud. Neil Holden, directeur informatique du groupe Halfords, estime que les départements informatiques doivent fonctionner différemment en raison du cloud et de ce que cela signifie pour l'entreprise. Holden a réorganisé son équipe pour s'assurer qu'elle puisse maximiser les capacités et les opportunités commerciales offertes par le cloud.\n\n[https://www.cio.com/article/463595/transforming-it-for-cloud-success.html](https://www.cio.com/article/463595/transforming-it-for-cloud-success.html)\n\n## Podcast sur la transformation numérique.\n\nDécouvrez l'épisode complet de cette semaine \"Innovation as a Service\" où Darren interviewe Andrew Cohen, Directeur Général chez Netsurit.\n\n[https://www.embracingdigital.org/episode-EDT127](https://www.embracingdigital.org/episode-EDT127)\n\n\n\n","guests":null,"link":"/brief-EDW5-fr","image":"./briefs/edw-5/fr/thumbnail.png","lang":"fr","summary":"Summary"},{"id":217,"type":"News Brief","title":"2024-1-14","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","sandworm","hacking","zyxelsecurity","miraibotnet","sechack","bitcoinhack","cryptojacking","openai","aijobs","airegulation","ainetworks","mitai","aiwarfare","taiwanai","israelai","edgecomputing","iotsecurity","5ginorbit","iridiumnetwork","digitaltransformation","zero5gtrust","dataassurance"],"body":"\n\n## Cybersécurité\n\n\nDe nouvelles recherches menées par Forescout contestent l'attribution cybernétique dans les incidents de piratage, rendant difficile l'identification précise des coupables. Le groupe de piratage Sandworm n'aurait peut-être pas été impliqué dans les attaques cybernétiques visant vingt-deux organisations énergétiques danoises en mai deux mille vingt-trois, qui ont exploité une faille de sécurité dans le pare-feu Zyxel et déployé des variantes de botnet Mirai sur des hôtes infectés via un vecteur d'accès initial inconnu. Les résultats soulignent les défis liés à l'attribution des menaces cybernétiques. [Lire la suite](https://thehackernews.com/2024/01/new-findings-challenge-attribution-in.html)\n\n\nLe compte Twitter de la Commission des opérations de bourse des États-Unis (SEC) a été piraté, avec la publication de fausses informations concernant les fonds négociés en bourse de Bitcoin. Le FBI mène une enquête sur le piratage et la SEC a confirmé qu'il résultait du fait qu'un individu avait pris le contrôle du compte par l'intermédiaire d'un tiers. L'incident soulève des préoccupations quant à la vulnérabilité de la plateforme à la désinformation. Les analystes de sécurité expriment leur préoccupation quant à l'absence d'authentification à deux facteurs, et les législateurs ont critiqué la SEC pour avoir potentiellement sapé les marchés. [Lire la suite](https://www.wired.com/story/sec-x-account-hack-investigation/)\n\n\nLes autorités ukrainiennes, avec l'aide d'Europol et d'un fournisseur de cloud, ont arrêté un homme de vingt-neuf ans à Mykolaiv pour avoir orchestré un schéma sophistiqué de cryptojacking. Le suspect a infiltré mille cinq cents comptes appartenant à une entreprise américaine bien connue en utilisant des outils de force brute personnalisés. Il a créé plus d'un million d'ordinateurs virtuels pour assurer le fonctionnement des malwares, gagnant plus de deux millions de dollars en profits illicites. Le cryptojacking exploite les identifiants compromis sur les plateformes cloud, permettant l'utilisation non autorisée des ressources informatiques pour manipuler des cryptomonnaies. L'arrestation du suspect a eu lieu le neuf janvier. [Lire la suite](https://thehackernews.com/2024/01/29-year-old-ukrainian-cryptojacking.html)\n\n## Intelligence Artificielle\n\n\nLe PDG d'OpenAI, Sam Altman, a exprimé sa préoccupation quant à la pénétration rapide de l'intelligence artificielle (IA) dans la société. Il la décrit comme la révolution technologique la plus rapide, soulignant la vitesse à laquelle la société doit s'adapter. Altman estime que les progrès de l'IA entraîneront des changements dans les emplois, mais il pense également qu'ils créeront de nouvelles et meilleures opportunités. Altman évoque la nécessité d'un organisme régulateur mondial pour superviser les systèmes d'IA robustes, compte tenu de leur impact potentiel sur la société et l'équilibre géopolitique. [Lire la suite](https://www.livemint.com/technology/gadgets/amazon-republic-day-sale-top-5-smartwatches-with-up-to-77-discounts-11705312172154.html)\n\n\nUn des défis liés à l'intelligence artificielle (IA) et aux réseaux neuronaux consiste à comprendre leur fonctionnement. Les chercheurs du MIT ont développé un nouveau système d'IA qui étudie et explique le comportement des réseaux neuronaux pour répondre à cette problématique. Ce système utilise des modèles de langage pré-formés et permet une meilleure compréhension des calculs de réseaux neuronaux. De plus, les chercheurs ont introduit le benchmark FIND pour évaluer la précision des techniques d'interprétation. Malgré certaines limites, le benchmark FIND est un outil précieux pour évaluer l'efficacité des procédures d'interprétabilité. [Lire la suite](https://www.marktechpost.com/2024/01/13/mit-researchers-developed-a-new-method-that-uses-artificial-intelligence-to-automate-the-explanation-of-complex-neural-networks/)\n\n\nTaïwan et Israël utilisent l'intelligence artificielle pour relever différents défis. Taïwan a employé des stratégies innovantes pour contrer l'ingérence étrangère dans leurs élections, en utilisant des outils d'IA pour signaler les contenus trompeurs, en établissant des initiatives anti-désinformation, et en surveillant internet pour la manipulation de l'information. Pendant ce temps, l'armée d'Israël utilise un système d'IA appelé \"l'Evangile\" pour localiser les cibles plus rapidement, réduire les victimes civiles et identifier les tunnels et les lanceurs de missiles du Hamas. Malgré les préoccupations concernant les erreurs algorithmiques et le bilan de morts parmi les citoyens palestiniens, l'utilisation de l'IA par Israël est considérée comme un éventuel élément clé dans la guerre tactique. [Lire la suite](https://lynnwoodtimes.com/2024/01/14/artificial-intelligence-240114/)\n\n## Informatique en périphérie\n\n\n\nLes États-Unis et l'Union européenne ont convenu d'une feuille de route conjointe pour un programme d'étiquetage des consommateurs pour les produits de maison intelligente et les appareils connectés. Le programme comportera un label de confiance cyber sur l'emballage des appareils pour indiquer la conformité aux normes de sécurité. La Federal Communications Commission est à la tête de l'initiative et vise à finaliser la politique et les normes d'ici la fin de deux mille vingt-quatre. Le programme informera les consommateurs sur la cybersécurité des appareils IoT, promouvant la sensibilisation et la sécurité. [Lire la suite](https://www.nextgov.com/cybersecurity/2024/01/eu-signs-iot-safety-label-plan/393328/)\n\n\n\nLa connectivité IoT 5G depuis l'espace est à l'horizon, élargissant l'informatique périphérique au-delà de la fabrication traditionnelle de briques et mortier. Iridium Communications a récemment lancé le Project Stardust, un service réseau 5G exploitant sa constellation de satellites en orbite terrestre basse. Le service est conçu pour supporter les services 5G, les appareils IoT, la messagerie, les services d'urgence et le suivi des actifs. Iridium a commencé à tester le service en deux mille vingt-trois et prévoit de le lancer commercialement en deux mille vingt-six. [Lire la suite](https://www.sdxcentral.com/articles/news/iridiums-project-stardust-satellite-strategy-supports-5g-iot-from-space/2024/01/)\n\n\nEn deux mille vingt-trois, l'IoT a connu des développements significatifs tels que la directive NIS2 sur la cybersécurité de l'UE, les licenciements par les principaux acteurs technologiques affectant l'IoT, la 5G dans l'espace, et les initiatives axées sur la durabilité. Renesas a acquis le fabricant de puces IoT cellulaire Sequans pour deux cent quarante-neuf millions de dollars, tandis que les guerres des nuages IoT se sont intensifiées avec Google mettant fin à son service Core IoT. Pragmatic Semiconductor a levé trois cent quatre-vingt-neuf millions trois cents mille dollars, et Samsara est devenue l'action IoT la mieux performante. L'Inde a entrepris un déploiement national de compteurs intelligents, et des percées en IA générative et en IoT ont été combinées pour fournir une réparation guidée et enseigner aux robots avec des systèmes de vision. Wow! Quelle année! [Lire la suite](https://iot-analytics.com/iot-2023-in-review/)\n\n## Adopter la Transformation Numérique\n\n\n\nCette semaine, Darren hébergera son podcast avec deux interviews. La première interview se concentrera sur la sécurité Zero Trust dans la 5G, et la seconde se concentrera sur l'amélioration de l'assurance des données dans les architectures cloud, en mettant l'accent sur l'adoption de Zero Trust. En vous branchant, vous pouvez vous informer sur les dernières transformations numériques. Nous avons passé un excellent mois le mois dernier, avec plus de soixante mille écoutes du podcast. Merci de partager avec vos amis. [Lire la suite](https://www.embracingdigital.org/en)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW50-fr","image":"./briefs/edw-50/fr/thumbnail.png","lang":"fr","summary":"Actualités de la transformation numérique de la semaine du 15 janvier 2024, y compris des mises à jour sur la cybersécurité, l'intelligence artificielle et le calcul à la périphérie. Cette semaine, le piratage de la SEC suscite des alarmes, l'IA est utilisée pour expliquer l'IA et la 5G est déployée dans l'espace."},{"id":218,"type":"News Brief","title":"2024-1-21","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","tapestorage","filestorage","cloudstorage","sql","nosql","cpra","dataprivacy","cisa","openai","pentagon","ibm","quantumcomputing","nuclearecurity","csuite","deepfake","mcafee","digitaltransformation"],"body":"\n\n## Gestion des Données\n\n\nHammerspace a récemment intégré le stockage sur bande à son système de fichiers global afin d'améliorer les capacités de gestion des données. Cette intégration permet aux utilisateurs d'accéder et de gérer efficacement les données sur différents supports de stockage, favorisant ainsi la flexibilité des solutions de stockage de fichiers. La solution actuelle proposée par Hammerspace met en place un système de fichiers global qui couvre plusieurs centres de données, nuages, et limites traditionnelles de bordure. [Lire la suite](https://www.techtarget.com/searchstorage/news/366566738/Hammerspaces-global-file-system-now-includes-tape)\n\n<hr>\n\n\nLa gestion de données hétérogènes est essentielle pour les entreprises modernes. Toad Data Studio prend en charge SQL, NoSQL, et les bases de données dans le cloud, offrant des capacités complètes de gestion de bases de données. La compatibilité multi-plateformes rationalise les flux de travail, l'optimisation des requêtes améliore les performances de la base de données, et les outils de collaboration permettent un travail d'équipe efficace. Par conséquent, Toad Data Studio est une ressource indispensable pour gérer et administrer des bases de données dans divers environnements. [Lire la suite](https://www.infoworld.com/article/3712250/toad-data-studio-manages-sql-nosql-and-cloud-databases.html)\n\n<hr>\n\n\nSi vous exploitez une entreprise en Californie, il est crucial de vous préparer pour la prochaine loi sur les droits à la vie privée en Californie (CPRA) en affinant vos pratiques en matière de confidentialité des données. En améliorant la gestion des données, les mécanismes de consentement et les stratégies de conformité, vous pouvez répondre aux exigences de la CPRA et rester en tête dans la protection de la confidentialité des consommateurs. Il est recommandé de mettre en oeuvre des mesures proactives pour garantir que votre entreprise respecte pleinement la CPRA. [Lire la suite](https://www.jdsupra.com/legalnews/prepare-for-the-cpra-by-improving-your-7537199/)\n\n<hr>\n\n## Cybersécurité\n\n\nLa CISA a publié un guide complet de réponse aux incidents de cybersécurité adapté aux services des eaux. Le guide est conçu pour aider les services des eaux à répondre efficacement aux incidents de cybersécurité et à atténuer les menaces potentielles du cyberespace. Il fournit des informations précieuses et sert de ressource pour renforcer les capacités de réponse aux incidents des services des eaux. Cette démarche est une étape importante dans la lutte contre les cyberattaques sur les infrastructures critiques du secteur de l'eau. [Lire la suite](https://www.techtarget.com/searchsecurity/news/366566740/CISA-posts-incident-response-guide-for-water-utilities)\n\n<hr>\n\n\nOpenAI s'est associé au Pentagone pour une initiative en matière de cybersécurité, marquant un changement par rapport à sa position précédente. En tirant parti de l'expertise d'OpenAI, le partenariat vise à faire progresser les capacités de cybersécurité pour la défense. Ce développement souligne le rôle en évolution de l'IA dans les efforts de sécurité nationale, avec OpenAI contribuant activement aux projets de cybersécurité en collaboration avec le Pentagone. [Lire la suite](https://www.semafor.com/article/01/16/2024/openai-is-working-with-the-pentagon-on-cybersecurity-projects)\n\n<hr>\n\n\n\nIBM a averti que l'émergence de l'informatique quantique pourrait entraîner une menace sévère pour la cybersécurité. La préoccupation est que les ordinateurs quantiques pourraient briser les méthodes de chiffrement largement utilisées, nécessitant le développement de mesures de sécurité résistantes à la quantique. Les organisations sont exhortées à prendre des mesures proactives pour se préparer aux défis futurs posés par l'informatique quantique en matière de cybersécurité. [Lire la suite](https://www.bloomberg.com/news/articles/2024-01-17/quantum-computing-to-spark-cybersecurity-armageddon-ibm-says)\n\n<hr>\n\n## Intelligence Artificielle\n\n\nL'intégration de l'intelligence artificielle (IA) dans les processus de prise de décision au sein de la sécurité atomique soulève des questions stimulantes sur la manière dont les technologies d'IA pourraient influencer la stabilité, la dynamique de confiance et l'équilibre stratégique global dans le contexte des capacités nucléaires. Il est essentiel de garantir une mise en œuvre responsable de l'IA pour éviter les conséquences non intentionnelles qui pourraient menacer la sécurité et la stabilité mondiales. Le besoin critique pour une intégration fiable de l'IA souligne l'importance de fournir un environnement international sécurisé et stable dans le contexte des capacités nucléaires. [Lire la suite](https://warontherocks.com/2024/01/artificial-intelligence-and-nuclear-stability/)\n\n<hr>\n\n\nL'intelligence artificielle (IA) est en voie de bouleverser la direction exécutive de trois manières significatives. Les dirigeants peuvent s'attendre à des changements dans les processus de prise de décision, la dynamique de la main-d'œuvre et le rôle global des leaders. Alors que l'IA continue d'évoluer, son impact transformateur refaçonne les paradigmes traditionnels du leadership. Les cadres sont encouragés à s'adapter à ces changements et à utiliser stratégiquement l'IA comme un avantage concurrentiel dans l'environnement commercial dynamique. [Lire la suite](https://www.cio.com/article/1293438/3-ways-ai-is-set-to-disrupt-the-c-suite.html)\n\n<hr>\n\n\nMcAfee a lancé un système de détection audio de deepfake alimenté par l'intelligence artificielle pour faire face à la menace croissante du contenu audio manipulé. La technologie est conçue pour identifier et atténuer les risques associés à l'audio deepfake, soulignant la nécessité de solutions de détection et de prévention avancées. Cela reflète l'importance croissante de l'IA pour contrer les menaces émergentes dans les médias manipulés et renforcer les mesures de cybersécurité. [Lire la suite](https://www.artificialintelligence-news.com/2024/01/08/mcafee-unveils-ai-powered-deepfake-audio-detection/)\n\n<hr>\n\n## Adopter la Transformation Numérique\n\n\n\nLe dernier épisode de Darren sur le podcast Embracing Digital Transformation se plonge dans le monde de la gestion des données de fabrication R&D. Il examine les subtilités de la collecte et de l'analyse des données, de la recherche pure innovante au processus de fabrication plus prévisible. C'est une exploration perspicace des défis et des opportunités de la transformation numérique dans ce domaine. [Lire la suite](https://www.embracingdigital.org)\n\n<hr>\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW51-fr","image":"./briefs/edw-51/fr/thumbnail.png","lang":"fr","summary":"Préparez-vous pour les actualités de la Transformation Digitale de cette semaine, 22 janvier 2024, qui présente des histoires sur la gestion des données, la cybersécurité et l'intelligence artificielle. Cette semaine, nous avons des actualités sur les droits à la confidentialité des données en Californie, des conseils sur la protection de votre organisation contre les cyberattaques liées à l'IA et à l'informatique quantique, et un regard sur comment l'IA est utilisée pour aider le gouvernement à prendre des décisions en temps de guerre."},{"id":219,"type":"News Brief","title":"2023-3-12","tags":null,"body":"\n\n## Intelligence Artificielle\n\nMicrosoft confirme qu'une nouvelle version de son chatbot d'IA populaire pourra transformer du texte en VIDÉO. GPT-4 et les développements futurs changeront à jamais notre façon de penser à l'intelligence artificielle. Cette IA puissante peut traiter des textes, des nombres, des vidéos, des images et plus encore. Microsoft a expliqué que GPT-4 serait \"multimodal\". Holger Kenn, directeur de la stratégie commerciale chez Microsoft Allemagne, a expliqué que cela permettrait à l'IA de l'entreprise de traduire le texte de l'utilisateur en images, musique et vidéo.\n\nL'IA va-t-elle remplacer les acteurs ? Imaginez créer un film sans embaucher de vrais acteurs ou écrire un livre entièrement illustré de 200 pages en une seule journée en utilisant l'IA. Oui, vous avez bien lu, GPT-4 rend possible l'utilisation de l'intelligence artificielle pour presque tout ce que vous pouvez imaginer.\n\nSchrodinger : Un simulateur de marché financier entièrement interactif multi-agents intégré à ChatGPT Citadel, l'un des fonds spéculatifs les plus réussis au monde, est en pourparlers pour obtenir une licence ChatGPT à l'échelle de l'entreprise. ChatGPT peut produire un code correct avec une stratégie d'exécution de transactions (simple).\n\n[https://www.youtube.com/watch?v=tvzO79V9uq4](https://www.youtube.com/watch?v=tvzO79V9uq4)\n\n## Gestion de données\n\n\"Zéro\" est le nouveau mot du jour, avec l' \"Alliance des données\" créant le terme \"zéro-copie\" pour les nouvelles architectures de données distribuées. L'intégration zéro-copie est un concept qui permet de partager des données entre différents systèmes sans les copier. L'approche traditionnelle de la copie de données d'un système à un autre peut entraîner des inefficacités, des problèmes de cohérence des données et des vulnérabilités de sécurité.\n\n[http://tdan.com/the-data-centric-revolution-zero-copy-integration/30462](http://tdan.com/the-data-centric-revolution-zero-copy-integration/30462)\n\nLa pandémie de COVID-19 a accéléré la vision des données partout, dans le cloud, les centres de données, les ordinateurs portables et les appareils IoT. Cette explosion de données a amené les organisations informatiques à réévaluer leurs stratégies de gestion des données. De nouvelles stratégies et solutions de gestion de données distribuées doivent être développées qui soient plus complètes pour gérer des types de données hétérogènes, y compris des données structurées, semi-structurées et non structurées.\n\n[https://www.engineeringnews.co.za/article/modern-data-management-platforms-are-vital-for-solving-modern-data-management-problems-2023-03-14/rep_id:4136](https://www.engineeringnews.co.za/article/modern-data-management-platforms-are-vital-for-solving-modern-data-management-problems-2023-03-14/rep_id:4136)\n\nLa complexité de la gouvernance des données entraîne des changements organisationnels, y compris l'émergence du rôle de responsable des données dans les organisations. Les responsables des données commencent à gérer l'accès aux données, les problèmes de confidentialité et la gestion du cycle de vie des données. La place des responsables des données dans l'organisation est encore sujette à un débat sain alors que les organisations renforcent leur orientation centrée sur les données.\n\n[https://www.techtarget.com/searchdatamanagement/tip/Data-stewardship-Essential-to-data-governance-strategies](https://www.techtarget.com/searchdatamanagement/tip/Data-stewardship-Essential-to-data-governance-strategies)\n\n## Sécurité informatique.\n\nUn kit de phishing à adversaire intermédiaire (AiTM) open source a trouvé plusieurs preneurs dans le monde de la cybercriminalité en raison de sa capacité à orchestrer des attaques à grande échelle. Microsoft Threat Intelligence suit l'acteur de menace derrière le développement du kit sous son pseudonyme émergent DEV-1101. Une attaque de phishing AiTM implique généralement un acteur de menace tentant de voler et d'intercepter le mot de passe et les cookies de session d'une cible en déployant un serveur proxy entre l'utilisateur et le site Web.\n\n[https://thehackernews.com/2023/03/microsoft-warns-of-large-scale-use-of.html](https://thehackernews.com/2023/03/microsoft-warns-of-large-scale-use-of.html)\n\nL'Agence de cybersécurité et de sécurité des infrastructures (CISA) a averti les organisations qui exploitent des infrastructures critiques des vulnérabilités aux rançongiciels sur leurs appareils. L'avis comprend une liste d'appareils que les attaquants peuvent cibler et recommande aux organisations d'évaluer leurs réseaux pour les risques possibles. CISA dispose d'un nouveau programme pilote d'avertissement de vulnérabilité aux rançongiciels (RVWP) avec deux objectifs : scanner les réseaux des entités d'infrastructures critiques et aider les organisations vulnérables à corriger les failles avant qu'elles ne soient piratées.\n\n[https://www.bleepingcomputer.com/news/security/cisa-now-warns-critical-infrastructure-of-ransomware-vulnerable-devices/](https://www.bleepingcomputer.com/news/security/cisa-now-warns-critical-infrastructure-of-ransomware-vulnerable-devices/)\n\nUn rapport de Drata met en évidence les tendances clés en matière de conformité pour 2023. Le rapport décrit le passage à une surveillance continue de la conformité et la nécessité d'outils d'automatisation pour atteindre la conformité. Il met également en évidence l'accent croissant sur les réglementations de confidentialité et l'importance croissante de la gestion des risques liés aux fournisseurs. Le rapport recommande aux entreprises de donner la priorité à leurs efforts de conformité, d'utiliser des outils d'automatisation et de rester à jour avec les exigences réglementaires évolutives pour éviter les amendes potentielles et les dommages à la réputation.\n\n[https://drata.com/resources/2023-compliance-trends](https://drata.com/resources/2023-compliance-trends)\n\n## Podcast sur l'Adoption de la Transformation Numérique\n\nDécouvrez l'épisode complet de cette semaine intitulé \"Closing the Digital Skills Gap\" où Darren interviewe Jon Gottfried de Major League Hacking.\n\n[https://www.embracingdigital.org/episode-EDT128](https://www.embracingdigital.org/episode-EDT128)\n\n\n\n","guests":null,"link":"/brief-EDW6-fr","image":"./briefs/edw-6/fr/thumbnail.png","lang":"fr","summary":"Summary"},{"id":220,"type":"News Brief","title":"2023-3-19","tags":["ai","compute","datamanagement"],"body":"\n\n\n## Intelligence artificielle\n\n## Calcul omniprésent\n\n## Bord intelligent\n\n## Gestion de Données\n\n## La Cyber-sécurité\n\n## Communications avancées\n## Intelligence Artificielle\n\nDans la section \"Potentiel de comportements émergents risqués\" du rapport technique de l'entreprise, OpenAI s'est associé au Centre de recherche sur l'alignement pour tester les compétences de GPT-4. Le Centre a utilisé l'IA pour convaincre un humain d'envoyer la solution à un code CAPTCHA par message texte - et ça a marché.\n\n[https://gizmodo.com/gpt4-open-ai-chatbot-task-rabbit-chatgpt-1850227471](https://gizmodo.com/gpt4-open-ai-chatbot-task-rabbit-chatgpt-1850227471)\n\nFonctionnalités de Copilot, les dirigeants de Microsoft ont souligné la propension du logiciel à produire des réponses inexactes, mais ont présenté cela comme quelque chose qui pourrait être utile. Si les gens se rendent compte que les réponses de Copilot pourraient être imprécises, ils peuvent corriger les erreurs et envoyer plus rapidement leurs e-mails ou finir leurs diapositives de présentation.\n\n[https://www.cnbc.com/2023/03/16/microsoft-justifies-ais-usefully-wrong-answers.html?__source=sharebar|linkedin&par=sharebar](https://www.cnbc.com/2023/03/16/microsoft-justifies-ais-usefully-wrong-answers.html?__source=sharebar|linkedin&par=sharebar)\n\nGoogle Health a annoncé Med-PaLM 2, une nouvelle version révolutionnaire de son grand modèle linguistique médical. Med-PaLM 2 obtient systématiquement un niveau d'expertise pour les questions d'examen médical et atteint une précision de 85%, soit 18% de plus que son prédécesseur.\n\n## Calcul Omniprésent\n\nVous pourriez bientôt nager au-dessus d'un centre de données 😊 Une entreprise technologique britannique, Deep Green, prévoit d'installer de petits centres de données dans des piscines publiques à travers le Royaume-Uni. Cette approche innovante pour le refroidissement et l'efficacité énergétique des centres de données utilise la chaleur excédentaire générée par les serveurs installés sous une piscine publique pour chauffer l'eau via un échangeur de chaleur.\n\nLe gouvernement américain a lancé une nouvelle initiative pour aider les organisations à détecter les failles de sécurité dans les services cloud de Microsoft. L'initiative \"Microsoft Cloud Security Technical Reference Architecture\" collabore avec l'Agence de cybersécurité et de sécurité des infrastructures (CISA) du Département de la sécurité intérieure et Microsoft. Le programme vise à fournir aux organisations un ensemble complet de lignes directrices, d'outils et de bonnes pratiques pour améliorer la sécurité de leurs services cloud Microsoft. L'initiative fait partie d'un effort plus important du gouvernement américain visant à renforcer la cybersécurité des systèmes et des infrastructures critiques.\n\n[https://www.techradar.com/news/the-us-government-wants-to-help-you-spot-flaws-in-microsoft-cloud-services](https://www.techradar.com/news/the-us-government-wants-to-help-you-spot-flaws-in-microsoft-cloud-services)\n\nSelon une enquête récente, 83% des directeurs des systèmes d'information (DSI) estiment qu'ils doivent en faire plus avec moins en 2023. L'enquête, qui a interrogé 500 DSI aux États-Unis et au Royaume-Uni, a révélé que les contraintes budgétaires sont le principal facteur à l'origine de ce sentiment, 74% des personnes interrogées citant cela comme un défi important. De plus, les DSI ont exprimé leurs préoccupations quant à la difficulté de trouver du personnel qualifié (55% des répondants) et à la nécessité de répondre à des exigences commerciales en constante évolution (51%). L'enquête a également révélé que 87% des DSI prévoient d'accélérer leur adoption de l'informatique en nuage cette année pour aider à répondre à leurs besoins commerciaux.\n\n[https://www.cloudcomputing-news.net/news/2023/mar/13/83-of-cios-must-do-more-with-less-in-2023/](https://www.cloudcomputing-news.net/news/2023/mar/13/83-of-cios-must-do-more-with-less-in-2023/)\n\n## Gestion des données\n\nUne enquête menée par TDWI a révélé que les organisations adoptent une stratégie de gestion des données pour améliorer la qualité des données et réduire les silos. 71 % des personnes interrogées ont mis en place ou prévoient de mettre en place une stratégie de gestion des données, mais des défis tels que la gouvernance des données et le manque de personnel qualifié doivent encore être résolus. Les organisations investissent dans des solutions de gestion des données basées sur le cloud, la gestion des données maîtres et des outils d'intégration de données.\n\n[https://tdwi.org/articles/2023/03/23/diq-all-data-management-0323.aspx](https://tdwi.org/articles/2023/03/23/diq-all-data-management-0323.aspx)\n\nL'intelligence artificielle (IA) a le potentiel d'aider à résoudre le problème de la surcharge de données informatiques. Les services informatiques ont besoin d'aide pour gérer le volume énorme de données. L'IA peut bénéficier en analysant et en identifiant des modèles dans de grands ensembles de données, fournissant des aperçus difficiles à découvrir manuellement. Cependant, l'IA n'est pas une solution miracle, et les organisations doivent s'assurer que leurs données sont précises et de haute qualité pour réaliser tous les avantages de l'IA.\n\n[https://www.infoworld.com/article/3689668/can-ai-solve-it-s-eternal-data-problem.html](https://www.infoworld.com/article/3689668/can-ai-solve-it-s-eternal-data-problem.html)\n\nUn nouveau rapport révèle l'importance de la gestion des données pour les entreprises. Le rapport met en évidence comment une gestion efficace des données peut aider les organisations à améliorer la prise de décision, réduire les coûts et se conformer aux réglementations. Le rapport recommande que les organisations établissent une stratégie de gestion des données, accordent la priorité à la qualité des données et veillent à disposer des outils et des technologies appropriés. Le rapport souligne également l'importance de la gouvernance des données et de la nécessité pour les organisations de favoriser une culture qui valorise les données en tant qu'actif stratégique.\n\n[https://solutionsreview.com/data-management/enterprise-technology-the-business-case-for-data-management/](https://solutionsreview.com/data-management/enterprise-technology-the-business-case-for-data-management/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW7-fr","image":"./briefs/edw-7/fr/thumbnail.png","lang":"fr","summary":"Résumé."},{"id":221,"type":"News Brief","title":"2023-3-26","tags":["ai","compute","cybersecurity"],"body":"\n\n\n## Intelligence artificielle\n\n## Calcul omniprésent\n\n## Bord intelligent\n\n## Gestion de Données\n\n## La Cyber-sécurité\n\n## Communications avancées\n## Informatique ubiquitaire\n\nGordon Moore, le co-fondateur de la société Intel et inventeur de la loi de Moore, est décédé le 24 mars 2023 à l'âge de 92 ans. Né en 1929, Moore a co-fondé Intel en 1968 et a été son PDG de 1975 à 1987. Il est surtout connu pour sa prédiction, la loi de Moore, selon laquelle le nombre de transistors sur une puce électronique doublerait environ tous les deux ans, entraînant une croissance exponentielle de la puissance de calcul. Cette prédiction a perduré pendant plus de 50 ans et a joué un rôle vital dans l'avancement de l'industrie technologique.\n\nLe 4004, le premier microprocesseur d'Intel, a fait ses débuts en 1971 en tant que premier microprocesseur disponible commercialement. Initialement conçu pour les calculatrices, il a rapidement trouvé des applications dans d'autres domaines, tels que les contrôleurs de feux de circulation et les caisses enregistreuses électroniques. Le 4004 contenait 2 300 transistors sur une seule puce et a aidé à établir Intel comme un acteur majeur de l'industrie des semi-conducteurs. Aujourd'hui, les microprocesseurs sont utilisés dans divers appareils, des smartphones aux superordinateurs.\n\nIntel a annoncé la disponibilité de sa dernière série de processeurs Xeon Max Scalable, basée sur son architecture Sapphire Rapids. La série Xeon Max dispose de 100 milliards de transistors SuperFin 10 nm, d'une accélération de l'IA et de l'encryption, ainsi que d'une prise en charge du PCIe 5.0, offrant des performances et une sécurité améliorées pour les charges de travail informatiques d'entreprise. Cela représente 40 millions de fois plus de transistors dans le dernier processeur que leur premier processeur il y a plus de 50 ans.\n\n[https://www.datacenterdynamics.com/en/news/intel-announces-xeon-max-sapphire-rapids-cpus/](https://www.datacenterdynamics.com/en/news/intel-announces-xeon-max-sapphire-rapids-cpus/)\n\n## Intelligence Artificielle\n\nChatGPT, un grand modèle de langage entraîné par OpenAI, a connu un problème technique qui l'a fait tomber pendant plusieurs heures. Le problème, causé par un bogue dans le système, a affecté la fonction d'historique du modèle, l'empêchant d'accéder à ses conversations précédentes. OpenAI a depuis résolu le problème et ChatGPT est maintenant pleinement opérationnel. L'incident rappelle les risques potentiels de dépendre de l'IA et l'importance d'avoir des systèmes robustes pour résoudre les problèmes techniques.\n\n[https://www.independent.co.uk/tech/chatgpt-down-bug-issue-history-b2306269.html](https://www.independent.co.uk/tech/chatgpt-down-bug-issue-history-b2306269.html)\n\nBard, un modèle de langage d'intelligence artificielle développé par Google en concurrence avec GPT d'OpenAI, est maintenant disponible pour une utilisation publique. Bard, qui utilise une approche différente de la modélisation du langage que GPT, vise à générer un texte plus créatif et diversifié en permettant aux utilisateurs de saisir leurs directives et contraintes. La sortie de Bard pour une utilisation publique devrait accélérer le développement de nouvelles applications de traitement du langage naturel et faire progresser le domaine de la modélisation du langage de l'IA.\n\n[https://www.bloomberg.com/news/articles/2023-03-21/google-chatgpt-rival-bard-now-open-to-public-use](https://www.bloomberg.com/news/articles/2023-03-21/google-chatgpt-rival-bard-now-open-to-public-use)\n\nUne nouvelle méthode agricole, Synecoculture, implique de planter plusieurs espèces végétales ensemble à haute densité. Cependant, cela peut être un processus complexe en raison des différences de croissance et de saisons. Les chercheurs ont développé un robot pour semer, tailler et récolter les plantes dans une végétation dense afin de résoudre ce problème. Le petit corps flexible du robot sera utile pour la Synecoculture à grande échelle, en faisant une étape essentielle vers l'agriculture durable et la neutralité carbone.\n\n[https://www.sciencedaily.com/releases/2023/03/230320102001.htm](https://www.sciencedaily.com/releases/2023/03/230320102001.htm)\n\n## Sécurité informatique (or Cyber-sécurité, but the former is more common in French).\n\nL'agence américaine de cybersécurité et de sécurité des infrastructures (CISA) a publié un nouvel outil pour détecter les activités malveillantes dans les services cloud de Microsoft. L'outil Sparrow peut analyser les environnements Azure et Microsoft 365 à la recherche de signes de piratage et d'autres activités non autorisées. Sparrow utilise des données open-source et des algorithmes d'intelligence artificielle pour identifier les menaces potentielles et fournir aux utilisateurs des recommandations exploitables pour prévenir d'autres dommages. La sortie de Sparrow fait partie des efforts continus de la CISA pour améliorer la sécurité des systèmes basés sur le cloud et protéger contre les cyber-attaques.\n\n[https://www.bleepingcomputer.com/news/security/new-cisa-tool-detects-hacking-activity-in-microsoft-cloud-services/](https://www.bleepingcomputer.com/news/security/new-cisa-tool-detects-hacking-activity-in-microsoft-cloud-services/)\n\nLes véhicules électriques de Tesla ont été piratés deux fois lors du concours d'exploitation Pwn2Own annuel organisé par l'initiative Zero Day. Une équipe de chercheurs a réussi à exploiter une vulnérabilité dans le système d'infodivertissement d'un Model 3 pour prendre le contrôle des phares, des haut-parleurs et d'autres fonctions du véhicule. Une autre équipe a utilisé un bug dans la même approche pour exécuter du code et accéder aux données de la voiture. Tesla a depuis publié un correctif via une mise à jour sans fil pour corriger les vulnérabilités.\n\n[https://www.securityweek.com/tesla-hacked-twice-at-pwn2own-exploit-contest/](https://www.securityweek.com/tesla-hacked-twice-at-pwn2own-exploit-contest/)\n\nIntel a annoncé sa nouvelle plateforme Core vPro de 13e génération, qui comprend des fonctionnalités de sécurité basées sur le matériel conçues pour réduire la surface d'attaque de la plateforme. La nouvelle plateforme inclut Intel Hardware Shield, qui utilise une détection de menaces au niveau du processeur pour fournir un processus de démarrage sécurisé, et la technologie de contrôle de flux d'exécution d'Intel, qui aide à prévenir les attaques de programmation orientée retour (ROP). La plateforme inclut également la technologie de détection de menaces d'Intel, qui utilise l'apprentissage automatique pour identifier les menaces potentielles à la sécurité. La plateforme Core vPro de 13e génération est destinée aux clients professionnels recherchant des fonctionnalités de sécurité avancées pour se protéger contre des cyberattaques de plus en plus sophistiquées.\n\n[https://www.securityweek.com/intel-boasts-attack-surface-reduction-with-new-13th-gen-core-vpro-platform/](https://www.securityweek.com/intel-boasts-attack-surface-reduction-with-new-13th-gen-core-vpro-platform/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW8-fr","image":"./briefs/edw-8/fr/thumbnail.png","lang":"fr","summary":"Résumé"},{"id":222,"type":"News Brief","title":"2023-4-2","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Sécurité informatique\n\nNouveau malware macOS MacStealer vole les données et mots de passe du trousseau iCloud. Un nouveau malware de vol d'informations a jeté son dévolu sur le système d'exploitation macOS d'Apple pour aspirer des informations sensibles à partir d'appareils compromis. MacStealer est le dernier exemple d'une menace qui utilise Telegram comme plateforme de commande et de contrôle (C2) pour exfiltrer des données.\n\nMicrosoft publie un correctif pour une faille de confidentialité aCropalypse dans les outils de capture d'écran Windows. Microsoft a publié une mise à jour hors bande pour résoudre une faille qui compromet la vie privée dans son outil d'édition de capture d'écran pour Windows 10 et 11. Le problème, appelé aCropalypse, pourrait permettre aux acteurs malveillants de récupérer des portions modifiées de captures d'écran, révélant potentiellement des informations sensibles qui auraient pu être rognées.\n\nOpenAI révèle qu'un bug dans la bibliothèque open source Redis était responsable de l'exposition des données utilisateur de ChatGPT. OpenAI a divulgué que le bug était à l'origine de l'exposition d'informations personnelles et de titres de discussion d'autres utilisateurs sur le service ChatGPT plus tôt cette semaine. Le problème, qui a été découvert le 20 mars 2023, a permis à certains utilisateurs de voir des descriptions brèves de conversations d'autres utilisateurs dans la barre latérale de l'historique des discussions, incitant l'entreprise à agir.\n\n## Intelligence artificielle.\n\nC'est GRAND : Elon Musk, Bill Gates et d'autres leaders technologiques demandent une pause dans la course à l'IA « hors de contrôle ». Plus de 1 100 signataires notables ont signé une lettre ouverte demandant à « tous les laboratoires d'AI de faire une pause pendant au moins 6 mois immédiatement ». La lettre, qui a également été signée par le PDG d'OpenAI, a déclaré que la pause devrait s'appliquer aux systèmes d'IA « plus puissants que GPT-4 ». Elle a également déclaré que des experts indépendants devraient utiliser la pause proposée pour développer conjointement et mettre en œuvre un ensemble de protocoles partagés pour les outils d'IA qui sont sûrs « au-delà de tout doute raisonnable ».\n\n[https://techcrunch.com/2023/03/28/1100-notable-signatories-just-signed-an-open-letter-asking-all-ai-labs-to-immediately-pause-for-at-least-6-months/](https://techcrunch.com/2023/03/28/1100-notable-signatories-just-signed-an-open-letter-asking-all-ai-labs-to-immediately-pause-for-at-least-6-months/)\n\nCerebras Systems publie sept nouveaux modèles GPT entraînés sur des systèmes à l'échelle de la plaquette CS-2. C'est la première fois qu'une entreprise utilise des systèmes d'IA non basés sur GPU pour former des LLM jusqu'à 13 milliards de paramètres et partage les modèles, poids et recettes d'entraînement via la licence Apache 2.0 standard de l'industrie. Une série de sept modèles GPT avec 111M à 13B de paramètres. Normalement, cela prendrait plusieurs mois, mais ce travail a été accompli en quelques semaines. Les sept modèles Cerebras-GPT sont immédiatement disponibles sur Hugging Face et Cerebras Model Zoo sur GitHub.\n\n[https://www.marketwatch.com/press-release/cerebras-systems-releases-seven-new-gpt-models-trained-on-cs-2-wafer-scale-systems-2023-03-28](https://www.marketwatch.com/press-release/cerebras-systems-releases-seven-new-gpt-models-trained-on-cs-2-wafer-scale-systems-2023-03-28)\n\nGoogle Bard avec Wordle - cela aurait pu mieux se passer. Après avoir corrigé le bot et lui avoir rappelé de ne deviner que des mots de cinq lettres, il s'est excusé et est revenu avec SLANTS, puis a continué avec différentes variations du mot SLANT ☹\n\n[https://www.techradar.com/news/i-tried-to-use-google-bard-to-help-me-with-wordle-but-it-didnt-go-well](https://www.techradar.com/news/i-tried-to-use-google-bard-to-help-me-with-wordle-but-it-didnt-go-well)\n\n## Intelligent Edge translates to ## Bord intelligent in French.\n\nBT s'est associé à Amazon Web Services (AWS) pour tester le calcul en périphérie 5G à Manchester en utilisant AWS Wavelength. L'essai examinera comment le calcul en périphérie peut améliorer les performances du réseau 5G de BT dans trois domaines clés : la réalité augmentée, les jeux immersifs et l'automatisation industrielle. L'objectif est d'améliorer l'expérience client en offrant des services plus rapides et plus fiables. L'essai fait partie d'une collaboration plus large entre les deux entreprises visant à explorer le potentiel du calcul en périphérie au Royaume-Uni.\n\n[https://www.edgecomputing-news.com/2023/03/29/bt-uses-aws-wavelength-for-5g-edge-trial-in-manchester/](https://www.edgecomputing-news.com/2023/03/29/bt-uses-aws-wavelength-for-5g-edge-trial-in-manchester/)\n\nComcast a annoncé des partenariats étendus visant à améliorer la connectivité dans les bâtiments et environnements intelligents. Les partenariats comprennent des accords avec le fournisseur d'infrastructure de comptage avancé Sensus et BuildingIQ, un fournisseur de solutions de gestion de l'énergie basées sur l'intelligence artificielle. L'objectif est de fournir une gestion plus intelligente de l'énergie, une meilleure gestion de l'eau et une automatisation du bâtiment améliorée. Ces partenariats font partie des efforts de Comcast pour améliorer la connectivité pour les clients commerciaux grâce à son réseau de hotspots Wi-Fi, de capteurs et d'autres dispositifs IoT.\n\n[https://www.edgeir.com/comcast-focuses-connectivity-on-smarter-buildings-environments-with-expanded-partnerships-20230331](https://www.edgeir.com/comcast-focuses-connectivity-on-smarter-buildings-environments-with-expanded-partnerships-20230331)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW9-fr","image":"./briefs/edw-9/fr/thumbnail.png","lang":"fr","summary":"Résumé"}]