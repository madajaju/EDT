[{"id":0,"type":"Episode","title":"Geschichte der datenzentrierten Architektur","tags":["dataarchitecture","softwaredeveloper","microservice","container","virtualization","technology","compute","data"],"body":"\r\n\r\nin technology, have revolutionized the way we use and interact with them.\n\nIn dieser Folge spricht Darren über die Geschichte von Anwendungen und wie aktuelle Veränderungen, hauptsächlich aufgrund des Vormarschs der Technologie, die Art und Weise revolutioniert haben, wie wir sie nutzen und mit ihnen interagieren.\n\nDie Daten des Internet der Dinge beeinflussen datenzentrierte Architekturen. Die Infrastruktur ist bereit, aber wir haben noch keine.\n\nWir haben noch keinen geeigneten Weg, um alle unsere Daten zu verwalten. Es gibt drei Elemente, die geändert werden müssen, um diesen Prozess zu erleichtern:\n\nMenschen (Organisation), Prozess (Betrieb) und Architektur (Technologie). Darren konzentriert sich auf die Architektur, bei der Daten.\n\nund verarbeitet werden auf tausenden von Edge-Geräten verteilt und über öffentliche und private Clouds hinweg.\n\n## Zweckorientierte Hardware-Software-Stapel\n\nWie wir heute Anwendungen für Missionen bereitstellen, hat sich in den letzten dreißig Jahren nicht wesentlich verändert.\n\nDiejenige Anwendung, die auf spezifischer Hardware entwickelt wurde, verfügt über eine Anwendungsstruktur. Die Rechenleistung und der Speicher sind damit verbunden.\n\nNetzwerk. Dieses Modell funktionierte lange Zeit gut und tatsächlich werden immer noch etwa ein Viertel der Anwendungen bereitgestellt.\n\nspezieller Hardware, aber dies ist heute nicht optimal. Die Technologie ist zu schnell für dieses Modell; Abweichungen treten auf.\n\nDarüber hinaus gibt es lange Entwicklungszeiten, hohe Kosten, begrenzte Wiederverwendung der Technologie und mangelnde Integration in.\n\nandere Anwendungen.\n\n## Virtuelle Architekturen\n\nVor etwa 20 bis 25 Jahren begann die Hardware-Virtualisierung, einige dieser Probleme durch die Möglichkeit der Bereitstellung zu lösen.\n\nTranslate the following to German: multiple applications on one machine. Applications were no longer tied to specific hardware. Instead of buying five\n\nTranslation: Mehrere Anwendungen auf einer Maschine. Anwendungen waren nicht mehr an bestimmte Hardware gebunden. Anstatt fünf zu kaufen.\n\nkleinere Maschinen, ein größeres Hardwarestück könnte verwendet werden, nicht nur für Rechenleistung, sondern auch für virtuellen Speicher und Netzwerk.\n\nFunktionen ermöglichen auch eine größere Kosteneffizienz. Wie bei jeder Entwicklung brachte dies jedoch einige neue Probleme mit sich.\n\nerhöhte Sicherheitsbedenken und \"lautstarke Nachbarn\", was bedeutet, dass eine Anwendung die Leistung einer anderen beeinträchtigt.\n\naufgrund der Nutzung von IO-Bandbreite, Netzwerk, Speicherplatz usw.\n\n## Cloud-Architekturen\n\nIn den frühen 2000er Jahren setzte sich die Cloud-Technologie durch. Wir konnten nun über mehrere Organisationen hinweg gemeinsam nutzen. Dabei spielte Virtualisierung eine wichtige Rolle.\n\nErstellte Abstraktion der Hardware, Cloud-Technologie schuf Abstraktion der Operationen und erleichterte die Verwaltung mehrerer.\n\nÜbersetzung: Maschinen. Die Cloud-Architekturidee hat \"Software-definierte Infrastruktur\" geschaffen, was es einfacher macht, diese hochzufahren und\n\nAuslösen der Computer-, Speicher- und Netzwerkressourcen. Weitere Vorteile sind geringere Kapital- und Betriebskosten aufgrund geringerer...\n\nFalls specific industry needs arise, automation can be scaled up or down based on hardware and manpower availability. It also gives the ability to handle increased demand, for instance, for retailers during the busy holiday season or the.\n\nRegierung während der Volkszählung. Mit dem Fortschritt dieser Technologie werden die Probleme der Sicherheit und lärmenden Nachbarn.\n\nerhöht aufgrund mehrerer Mieter auf derselben Maschine. Ein weiteres Anliegen sind Integrationskosten zwischen öffentlichen und\n\nPrivate Clouds. Selbst mit diesen Bedenken überwiegen die Vorteile in den meisten Fällen eindeutig die Nachteile.\n\n## Service- und Containerarchitekturen\n\nÜbersetzung ins Deutsche: In den letzten fünf bis sechs Jahren haben wir die Neuerfindung einer alten Technologie erlebt: Containerisierung. Docker hat einen neuen Standard für die Bereitstellung von Anwendungen geschaffen.\n\neinfacher Weg, um die zuvor umständliche und schwierig zu verwendende Container-Technologie zu nutzen. Anwendungsentwickler, in\n\nbesonders, hat diese Technologie begrüßt, weil sie in mehreren Umgebungen konsistent ist. Die Service-Management-Schicht\n\nmit der Containervirtualisierung von Anwendungen und Microservices steht die Anwendung im Mittelpunkt und ordnet diese Anwendungen zu.\n\ngenerisch, virtualisierte Hardware, die abstrahiert wurde. Wir haben jetzt automatische Bereitstellung über mehrere Clouds.\n\nWir haben OpEx und auch CapEx im Anwendungsschicht-Stapel und im Dienstschicht-Stapel optimiert. Die Fehlertoleranz ist automatisiert, und\n\nEs ist viel einfacher, sich mit Overlay-Netzwerken zu integrieren, sich über mehrere Clouds hinweg zu integrieren, Firewalls zu erstellen und zu verwalten.\n\nMikrosegmentierung usw... alles über Software.\n\nSicherheit ist jedoch ein Hauptanliegen. Da Container leicht in mehreren Umgebungen bereitgestellt werden können, ist es\n\nwird es immer wichtiger, sich auf Sicherheit zu konzentrieren, die in die Bereitstellung \"eingebaut\" ist. Außerdem gibt es eine zunehmende Komplexität. Hier,\n\nWir sind von einer dreischichtigen Architektur zu einer Mehrschicht- oder sogar einer Mikroservice-Architektur mit Dutzenden von Schichten übergegangen.\n\ntranslate: Dienste, die zusammenarbeiten. Ein weiteres Problem ist, wo und wie die Daten gespeichert und verwaltet werden. Bei der Dienstleistungsverwaltung.\n\nEbene, Speicher ist ein generischer Behälter, der die Daten selbst nicht verwaltet.\n\n## Internet of Things Architekturen\n\nJetzt, da das Internet der Dinge (IoT) zu diesem Ökosystem hinzugefügt wird, wird das erhöhte Datenvolumen auf Hunderte oder\n\ndevices are not sufficiently protected against cyber attacks and can pose a risk to the network. Therefore, it is necessary to implement strong security measures to ensure the safety of the system. Additionally, edge devices require regular updates and patching to address any vulnerabilities that may arise. Overall, while the use of smart devices offers many advantages, it is crucial to prioritize security to mitigate potential risks.\n\nGeräte sind für die Öffentlichkeit zugänglich. Zum Beispiel könnte jemand eine intelligente Straßenlaterne oder eine intelligente Verkehrssignalanlage manipulieren.\n\nTranslate the following to German: a drone, or security camera. The complexity of the different devices, their number and locations, along with the immense\n\neine Drohne oder Überwachungskamera. Die Komplexität der verschiedenen Geräte, ihre Anzahl und Standorte sowie die enorme\n\nMenge an Daten ist enorm.\n\n## Daten- und Informationsmanagement-Architekturen\n\nWie gehen wir mit diesen Problemen um? Organisationen passen sich bereits an, um mit dieser Komplexität durch neue Organisationen umzugehen.\n\nund Positionen von Fälschung bei Datenmanagement-Anwendungsfällen. Zuvor gab es keinen Ort, an dem diese Anwendungsfälle verwaltet werden konnten, also\n\nWir haben eine neue Schlüsselebene namens Distributed Information Management Layer erstellt. Diese Ebene verwaltet Daten über das gesamte System hinweg.\n\nDas Internet der Dinge (IoT), Legacy-Systeme und öffentliche und private Clouds. Es passt die Daten mit Anwendungsstapeln und Dienststapeln zusammen, sodass wir.\n\nkönnen Dienste und Anwendungen dynamisch in der Nähe der Daten oder umgekehrt zugeordnet werden. Regulierungen und schiere Größe der Daten\n\nkann die Fähigkeit einschränken, Daten an zentralen Standorten zu verschieben, wie wir es traditionell getan haben. Mit dieser neuen Architektur,\n\nMehrere Betriebsweisen können genutzt werden, darunter disaggregierte Analytik, Datenbewegung und Anwendungsbewegung.\n\nNochmals, mit dieser erweiterten Architektur ist Sicherheit von höchster Bedeutung. Sicherheit muss als gemeinsamer Aspekt funktionieren.\n\nDurch alle Schichten hindurch. Identitätssicherheit, das bedeutet Zugriff, Autorisierung und Authentifizierung von Personen sowie dem Internet der Dinge (IoT).\n\nGeräte, Anwendungen, Dienste und sogar Daten sind von zentraler Bedeutung. Die Verwaltung der Identität umfasst Verschlüsselung für vertrauenswürdige Daten.\n\nund Geräte.\n\n## Schlussfolgerung\n\ntranslate: \"Die gesamte Architektur hier wird als Edgemere-Architektur bezeichnet. Viele der Teile existieren bereits; wir müssen...\"\n\nhave a highly skilled team that is dedicated to continuously improving our processes. By optimizing how they work together, we can achieve greater efficiency and effectiveness in the DIML.\n\nbeginnen zu sehen, dass Start-ups und etabliertere Unternehmen die Anwendungsfälle und die architektonischen Elemente ausbauen\n\nin dieser Schicht.\n\nwork together to deliver a seamless user experience. It provides a framework for understanding the different components and their interactions, such as the front-end interface, back-end systems, databases, APIs, and external integrations. By visualizing these elements and their connections, Edgemere architecture allows organizations to optimize their technology infrastructure and ensure efficient data flow and smooth functionality.\n\nzusammenpassen\n\nIntel passt in dieses Ökosystem, indem es das Schlüsselelement einer gemeinsamen physischen Schicht bereitstellt, um alle Ihre zu steuern und zu verwalten.\n\nRessourcen, ob es sich um ein IOT-Gerät, ein Rechenzentrum oder einen entfernten Standort handelt. Wir machen es Ihnen möglich.\n\nEffizient Daten bewegen, sie effektiv speichern und alles verarbeiten. Ob es sich um die Xeon-Prozessoren auf der hohen\n\nOb es nun darum geht, am Ende zu sein oder um Inferenz oder KI mit einer sehr geringen Leistung an der Edge durchzuführen, Intel verfügt über eine vollständige Palette an physischer Hardware.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT1-de","image":"./episodes/edt-1/de/thumbnail.bmp","lang":"de","summary":"In dieser Episode spricht Darren über die Geschichte der Anwendungen und wie sich aktuelle Veränderungen, hauptsächlich aufgrund der Datenflut aus dem Internet der Dinge, auf datenzentrierte Architekturen auswirken. Die Infrastruktur ist bereit, aber wir haben noch keinen geeigneten Weg, um all unsere Daten zu verwalten. Es gibt drei Elemente, die geändert werden müssen, um diesen Prozess zu erleichtern: Menschen (Organisation), Prozess (Betrieb) und Architektur (Technologie). Darren konzentriert sich auf die Architektur, bei der Daten und Berechnungen auf Tausenden von Edge-Geräten und über öffentliche und private Clouds verteilt sind."},{"id":1,"type":"Episode","title":"Teleworker Bestimmung der Größe Ihrer VDI-Lösung","tags":["telework","remoteworker","process","technology","vdi"],"body":"\r\n\r\n# Titel\n\n*Slogan*\n\nZusammenfassung hier.\n\nTranslate the following to German: ![Episodenbild](./thumbnail.png)\n\nEpisode Körper hier.\n\n## Medien\n\n<video src='url'></video> in German would be <video src='url'></video>.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Illyse Sheaffer"],"link":"/episode-EDT10-de","image":"./episodes/edt-10/de/thumbnail.png","lang":"de","summary":"Organisationen benötigen sofort Hilfe beim Aufbau von VDI-Lösungen (Virtual Desktop Infrastructure). Da IT-Abteilungen VDI-Lizenzen lokal zu ihren aktuellen Systemen hinzufügen, müssen sie sich bewusst sein, dass Lizenzen allein nicht alle Probleme lösen."},{"id":2,"type":"Episode","title":"Die öffentlichen Sektor-Superhelden von Intel","tags":["cybersecurity","multicloud","edge","aiml"],"body":"\r\n\r\nIch\n\nCameron begann seine Karriere bei dem größten privat geführten Personaldienstleistungsunternehmen. Dort entwickelte er eine Leidenschaft für Technologie, nachdem er PeopleSoft gelernt hatte. Das führte zu seiner Möglichkeit, das weltweit erste Unternehmen für Cloud-Computing, UC Center Networking, mitaufzubauen. Gemeinsam mit seinen Kollegen schufen sie den Slogan \"Software as a Service\" im Jahr 1997, sieben oder acht Jahre bevor der Begriff \"Cloud\" geprägt wurde. Er arbeitete für Bundesunternehmen wie Northrop Grumman und General Dynamics, wo er entdeckte, dass er eine Leidenschaft für die Unterstützung einer Mission hatte. Diese Leidenschaft treibt ihn seit seiner Zeit als CTO bei Dell Technologies an und nun leitet er das Team für den öffentlichen Sektor bei Intel.\n\nEiner der Faktoren, die Cameron dazu veranlassten, zu Intel zu wechseln, war die Rückkehr von Pat Gelsinger und dem Führungsteam, das er aufbaut, einschließlich der Verpflichtung von Greg Lavender. Cameron glaubt, dass Intel unter Gelsinger wiederbelebt wird und weiterhin innovativ sein wird. Er möchte Teil der Arbeit sein, um Intel zu seinem Status als ikonische amerikanische Marke zurückzuführen. Er glaubt an eine Kultur der Bescheidenheit und Menschen, die das Richtige tun.\n\n## Allgegenwärtiges Rechnen\n\nHalbleiter sind überall vorhanden: in Autos, Haushaltsgeräten, Smartphones, Computern, Gesundheitssystemen usw. Sie erstrecken sich über nahezu jeden vertikalen Markt auf unserem Planeten. Sie verbessern die Lebensqualität der Menschen, retten sogar Leben durch Gesundheitsversorgung, nationale Sicherheit und wissenschaftliche Forschung.\n\nDie COVID-Pandemie brachte diese allgegenwärtige Natur aufgrund der schieren Anzahl von Menschen, die von zu Hause aus arbeiten konnten, und der Fortsetzung der Bildung online ans Licht. Sie half auch in vielerlei Hinsicht, die digitale Kluft zu schließen.\n\n## Pervasive Konnektivität\n\nSilizium kann sein Potenzial nicht realisieren, ohne verbunden zu sein. Intel ist nicht nur ein Chip-Unternehmen, sondern eines mit einem gesamten interkonnektiven Portfolio.\n\nEs gibt Zeiten, in denen Dinge voneinander getrennt sind, zum Beispiel ein Kriegsteilnehmer in einer DDIL-Umgebung, aber sie können den Computer lokal verwenden, um die Mission auszuführen. Wenn sie sich wieder verbinden, ob über das veraltete 4G, 5G oder das kommende 6G, können sie aktuelle Informationen erhalten, mit anderen Worten Datenübertragung. In einem Bildungsszenario ist jedoch eine Verbindung von entscheidender Bedeutung, um Dinge wie Streaming, Videoinhalte und den Zugriff auf Daten zu ermöglichen. Das ist ein mächtiges Gut.\n\nDie Pandemie hat auch zu einer signifikanten Zunahme der Kommunikation und des 5G-Netzes geführt, da Schülerinnen, Schüler und Arbeitnehmerinnen und Arbeitnehmer diese Konnektivität benötigten. Dies setzt sich bis heute fort, auch wenn Schülerinnen, Schüler und Arbeitnehmerinnen und Arbeitnehmer nun vor Ort sind. Während Schülerinnen und Schüler vor der Pandemie gelegentlich Zugang zu Chromebooks hatten, besitzen nun viele ihr eigenes Gerät, das sie mit nach Hause nehmen können. Leider gibt es immer noch Gebiete im Land und auf der Welt, in denen es Lücken gibt und Menschen nicht an der digitalen Wirtschaft teilnehmen können. Überraschenderweise ist es in den Vereinigten Staaten nicht so einfach, die digitale Kluft zu schließen wie in Entwicklungsländern. Diese Länder können die Vereinigten Staaten überholen, weil sie in 5G und 6G investieren: nicht-terrestrische Kommunikation.\n\nCameron glaubt, dass Konnektivität genauso wichtig wie das Rechnen ist, da mit anderen verbunden zu sein, eines der Grundelemente menschlicher Erfahrung ist.\n\n## Kante zur Cloud\n\nDas Fundament von Edge-to-Cloud ist Rechenleistung und Verbindung, daher spielt Intel zusammen mit seinen Partnern eine wichtige Rolle in diesem Bereich. Edge to Cloud nutzt nicht nur die wahre Leistung von Silizium, sondern auch von Software. Es schafft Interoperabilität. Unter Verwendung offener Standards, Kerntechnologien und einer Edge-to-Cloud-Strategie lassen sich Arbeitslasten sicher und nahtlos von einem Edge-Gerät zu einer Cloud oder einem herkömmlichen Rechenzentrum übertragen.\n\nFür die absehbare Zukunft wird das Edge Computing dominieren, denn je heller die Dinge werden, desto mehr muss die Technologie an den Rand gedrängt werden, wo die Informationen entstehen. Dann kann die Verarbeitung zentralisiert erfolgen, um mehr Analysen und Künstliche Intelligenz zu ermöglichen. Die Dinge werden sich nur nach vorne bewegen, und das Edge Computing wird allgegenwärtig werden.\n\n## Künstliche Intelligenz\n\nDaten sind überall vorhanden; Rechenzentren haben keine Wände mehr. Sie werden auf vielfältige Weise gesammelt und sogar verarbeitet, wie z.B. durch Mobiltelefone, Kameras, Industriemotoren usw. Künstliche Intelligenz dreht sich darum, intelligente Entscheidungen aufgrund dieser vielfältigen Daten zu treffen.\n\nIntel hat eine der größten künstlichen Intelligenz Sammlungen der Welt, eine außergewöhnliche Software-Sammlung und mehr Software-Entwickler als viele Softwareunternehmen. Der Grund ist, wie Greg Lavender betont, dass die Software die Seele des Siliziums ist. Man muss es ihr ermöglichen, etwas zu tun und ihr Leben und einen Zweck zu geben. Künstliche Intelligenz ist ein ausgezeichnetes Beispiel dafür.\n\nManchmal denken die Leute bei KI an Roboter, aber es gibt unzählige praktische Anwendungsfälle. Ein Beispiel ist, wenn du dich verirrst, kannst du sofort Siri oder Google abfragen und mithilfe von GPS wird der Service deinen Standort bestimmen und den nächsten Punkt der Zivilisation oder wohin auch immer du möchtest, finden.\n\nEin weiterer praktischer Anwendungsfall ist bei der US-amerikanischen Post. Die nächste Generation von Lieferfahrzeugen ist genauso modern wie die Google Street View Autos, mit Sensoren, die bestimmte Dinge kartieren. Die Post setzt KI bei kinetischen intelligenten Sortier- und Postbearbeitungsmaschinen ein. Sie nutzen Technologie, um sich zu skalieren.\n\nAufgrund des weitverbreiteten Einsatzes von Intel in der Computertechnologie und fortgeschrittener Kommunikation werden immer mehr Edge-Geräte intelligent, und die Menge an Daten, die von der Edge in Rechenzentren übertragen werden müssen, nimmt ab. Dies liegt daran, dass KI-Algorithmen ableiten, wonach Sie an der Edge suchen. Diese Art von Technologie ist direkt in Intel CPUs integriert. Sie verfügen auch über spezialisierte XPUs, neurotrophen Prozessoren, die dasselbe bei geringerer Leistungsaufnahme und höheren Geschwindigkeiten tun.\n\nEin Teil der Stärke des Geschehens mit Intel und ihren Partnern besteht darin, Zugriff auf alle Informationen in einem verständlichen Format zu haben. Im Beispiel des Verirrens und des Blicks auf dein Handy um Hilfe zu bekommen, könntest du etwas betrachten, das innerhalb eines Augenblicks 600 verschiedene Datenpunkte aufgenommen hat, um dir eine einfache Antwort zu geben.\n\n## Die Superhelden\n\nIntel-Mitarbeiter und ihre Partner im Ökosystem sind Superhelden. Partner helfen dabei, echte Lösungen zusammenzubringen, insbesondere in diesem entscheidenden letzten Schritt. Intel hat eines der besten Ökosysteme, um Lösungen auf den Markt zu bringen. Und manchmal werden diese Lösungen nicht einmal auf den Markt gebracht, sondern dienen dazu, herausfordernde Probleme in der Verteidigung und im öffentlichen Sektor zu lösen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Cameron Chehreh"],"link":"/episode-EDT100-de","image":"./episodes/edt-100/de/thumbnail.png","lang":"de","summary":"In dieser Episode sprechen Darren und Cameron Chehreh, Vizepräsident und General Manager des öffentlichen Sektors bei Intel, über Intels Superkräfte: allgegenwärtiges Rechnen, universelle Konnektivität, Edge-to-Cloud und künstliche Intelligenz."},{"id":3,"type":"Episode","title":"Netzwerk-Controller-Sicherheit mit Elisity","tags":["networksecurity","cybersecurity","elisity","comms","technology","process"],"body":"\r\n\r\nDana arbeitet seit den letzten 15 Jahren im Bereich Hardcore Networking. Er hat mit softwaredefinierten Netzwerken und Weitverkehrsnetzwerken gearbeitet und sich eine Zeit lang auf die SD-WAN-Welt konzentriert. Danach hat er mit der Cloud gearbeitet, kehrte jedoch zu seinen Wurzeln im Bereich Netzwerksicherheit bei Elisity zurück.\n\nDan hat hauptsächlich im Bereich Netzwerke gearbeitet, aber er begann mit fortgeschrittenen Diensten bei Cisco und hat einige Arbeiten in der Öl- und Gasbranche in Houston gemacht. Anschließend wechselte er in den Bereich SASE SD-WAN, wo er mit Dana zusammengearbeitet hat. Jetzt konzentriert er sich auf Netzwerksicherheit sowie softwaredefinierte Sicherheit. Dan sagt, dass der Begriff jedoch schwer fassbar ist, weil es sich um Mikrosegmentierung durch Identität handelt.\n\nDie traditionelle Methode, um Micro-Segmentation durch VLANs und Firewalls zu erstellen, reicht nicht mehr aus. Seit 15 Jahren konzentriert sich die Netzwerksicherheit auf die Absicherung der Netzwerkperipherie. Das Errichten einer undurchdringlichen Barriere um ein Unternehmen herum ist immer noch unerlässlich, und Firewalls erledigen einen unglaublichen Job, um Personen vom Netzwerk fernzuhalten. Aber größtenteils herrscht im Inneren des Netzwerks, wo es aufgrund von IoT und OT eine Explosion neuer Konnektivitätsanforderungen gegeben hat, einmal ein wildes Durcheinander, sobald ein Benutzer im Inneren ist. Da die äußere Barriere robust ist, wird jeder Benutzer im Inneren oft als legitimer oder vertrauenswürdiger Benutzer angesehen.\n\nDie Realität ist, dass die meisten aktuellen Angriffe im Netzwerk stattfinden, indem vertrauenswürdige Benutzer, Geräte und Anwendungen ausgenutzt werden.\n\nEs gibt viele Gründe, warum VLANs, IP ACLs, Firewalls und traditionelle Segmentierungsmethoden heute nicht mehr für die Sicherheit bei seitlicher Bewegung funktionieren. Sie können in statischen Umgebungen in kleinem Maßstab funktionieren. Sie funktionieren gut am Netzwerkrand und an bestimmten Orten wie Engpässen oder Aggregationspunkten, aber es gibt drei häufige Gründe, warum diese nicht mehr ideal sind.\n\nZuerst sind die Skalierbarkeit und operative Effizienz traditioneller Methoden fragwürdig. Die Verwaltung von VLANs, IP ACLs und Firewalls in großen Unternehmen erfolgt manuell. Es handelt sich nicht um eine verteilte, softwaredefinierte Architektur, sondern erfordert eine Konfiguration Box für Box, Zeile für Zeile. Sie sind nicht dynamisch oder reaktionsschnell gegenüber Ereignissen im Netzwerk. Ihre Verwendung führt auch zu einem Netzwerk wie ein Schweizer Käse mit zufälligen Löchern. Benutzer, insbesondere Netzwerkbetreiber, stellen oft das größte Risiko eines Systems dar. Zum Beispiel könnten sie ein VLAN öffnen oder eine Einstellung ändern, um einen schnellen Test durchzuführen, aber dann die Änderung nicht rückgängig machen.\n\nEin weiteres Beispiel ist, dass eine VLAN mit einem Anwendungsfall beginnt und langsam auf andere Anwendungsfälle ausgedehnt wird. Plötzlich hat ein VLAN, das zuvor nur zehn Geräte hatte, jetzt 60 Geräte. In der OT-Welt könnten sechs oder sieben verschiedene Prozesse darin ausgeführt werden, da es das vertrauenswürdige LAN war. Oft wachsen diese Umgebungen langsam und sind undokumentiert, sodass das Risiko unbemerkt bleibt.\n\nDas zweite Problem besteht darin, dass VLANs und Firewalls von Natur aus am falschen Ort im Netzwerk platziert sind, um eine seitliche Bewegungssicherheit zu gewährleisten. Wenn Sie sich im selben VLAN wie ein anderes Gerät, Benutzer oder Anwendung befinden, ist dieser Kommunikationskanal geöffnet, auch wenn es nicht sein müsste. Firewalls werden in der Regel nicht strategisch an einem Ort eingesetzt, an dem sie das Zugriffslevel seitlicher Bewegungen behandeln können. Sie müssen den Verkehr durch eine Firewall leiten und zurückführen, was ineffizient ist. Dann haben Sie einen Engpass.\n\nSchlechte Schauspieler betrachten Netzwerke, um herauszufinden, wie sie diese verdrehen können, um ein bestimmtes Ergebnis zu erzielen, nicht wie sie funktionieren sollten oder beabsichtigt waren. Zum Beispiel, wenn ein Benutzer sich in einem VLAN befindet und ein Prozess in einem Anwendungsfall läuft, hindert nichts sie daran, von Port 3 zu Port 32 im selben VLAN zu wechseln. Häufig gestalten Menschen die Sicherheit entsprechend der beabsichtigten Verwendung, anstatt wie sie genutzt werden könnte. Es ist üblich, dass Softwareentwickler Ports wechseln, um effektiv zu arbeiten, aber das ist gefährlich, weil es sie angreifbar macht. Niemand kann Hunderte oder Tausende von Firewalls entlang des gesamten Zugriffsrands platzieren. Das wäre finanziell einschränkend und unmöglich zu verwalten.\n\nDas dritte Problem ist, dass diese veralteten Segmentierungslösungen die Identität, den Kontext oder das Verhalten des mit dem Netzwerk verbundenen Assets nicht berücksichtigen. Es handelt sich um eine starre, netzwerkzentrierte Topologie, die einige grundlegende Sicherheitsmaßnahmen bietet. Aber eine IP-Adresse sagt nichts über die Legitimität des Assets und das Netzwerk, mit dem es verbunden ist, aus. Wie kann man also dieses Netzwerk dynamisch sichern, wenn man nicht weiß, was mit dem Netzwerk verbunden ist? Ohne jegliche Granularität lässt sich ohne weiteres keine Richtlinie festlegen; es funktioniert nicht, jedes Gerät gleich zu behandeln.\n\nAuch wenn es eine Analyse des Verkehrstyps gibt, findet dies in der Regel mehrere Hops weiter oben statt, was bedeutet, dass Sie jetzt gefährdet sind. Jegliche Durchsetzung kann möglicherweise die Infrastruktur nicht vollständig schützen.\n\nDiese Erkennungsfähigkeit hat immer noch einen Wert, auch wenn kein Schutz vorhanden ist. Dennoch ist die Fähigkeit, etwas am Rand zu stoppen, so nahe wie möglich am Vermögen, eine bessere Lösung.\n\nSchau dir die nächste Episode dieser Serie [hier](episode-EDT101) an.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Dan Demers","Dana Yanch"],"link":"/episode-EDT101-de","image":"./episodes/edt-101/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher von Intel, Chief Solutions Architect im öffentlichen Sektor, führt Interviews mit Netzwerksicherheitsexperten Dana Yanch und Dan Demers von Elisity über Netzwerk-Controller-Sicherheitstechniken und Zero-Trust-Architekturen."},{"id":4,"type":"Episode","title":"Identitätsbasierte Mikrosegmentierung mit Elisity","tags":["microsegmentation","otsecurity","identitybasednetworking","cybersecurity","comms","technology","edge","sdn"],"body":"\r\n\r\nIm zweiten Teil von \"Security mit Elisity\" diskutiert Darren Identitätsbasierte Mikrosegmentierungslösungen mit Dana Yanch, Leiterin für technisches Marketing, und Dan Demers, technischer Architekt.\n\nElisitys Ansatz zur Netzwerksicherheit unterscheidet sich von traditionellen Architekturen durch die Art der Bereitstellung, Verteilung im Netzwerk und effiziente Durchführung, indem sie einen schnellen Mehrwert bietet. Das Hauptaugenmerk lag darauf, es so einfach, intuitiv und effektiv wie möglich zu gestalten, damit jede Branche von der Technologie profitieren kann.\n\nDer Schlüssel zur Technologie von Elisity ist die Mikrosegmentierung, was bedeutet, dass die Fähigkeit besteht, jeden Benutzer, jedes Gerät oder jede Anwendung völlig zu isolieren, unabhängig davon, auf welchem Netzwerk es sich befindet, wo es sich im Netzwerk befindet oder ob es verwaltet oder nicht verwaltet wird. Mit anderen Worten ist es flexibel, eine Ressource von einer anderen zu isolieren, ohne Einschränkungen durch Netzwerkarchitektur oder Konstrukte wie VLANs oder VRFs.\n\nTraditionelle Mikrosegmentierungssysteme, die einen Hardwareaustausch oder eine Instantiierung einer Vor-Ort-Appliance erfordern, können Monate oder Jahre brauchen, um einsatzbereit zu sein. Dennoch bietet Elisity innerhalb einer Woche volle Funktionalität, indem sie zu einer cloudnativen und cloudbasierten Mikrosegmentierungsplattform wechseln. Die gesamte Steuerungsverwaltung und Richtlinienebenen werden vollständig über die Cloud bereitgestellt, und die Komponente, die sich mit Ihrem Vor-Ort-Netzwerk verbindet, ist zu 100 Prozent softwarebasiert. Sie müssen nichts an Ihrer Vor-Ort-Infrastruktur ändern, da sie auf Ihrer vorhandenen Infrastruktur aufbaut. Es skaliert gut, ist einfach zu verwalten und wird dynamisch verteilt.\n\nElisity bietet auch eine On-Prem-Lösung für den OT-Bereich an, bei der Organisationen sie hinter ihrer DMZ aufbewahren und jeglichen Zugriff blockieren können.\n\nElisitys Plattform ist ein Mikrodienst im Sinne der horizontalen Skalierbarkeit. Sie können klein anfangen und 10.000 Dinge zum Netzwerk hinzufügen und mit Ihnen skalieren, egal ob lokal oder in der Cloud. Sie ist vollständig automatisiert im gesamten Netzwerk. Es gibt kaum Reibung und sie ist langfristig einfach zu verwalten.\n\nEin bedeutender Unterschied zwischen der Plattform von Elisity und anderen Überlagerungsnetzwerken besteht darin, dass Elisity sich mit der Steuerungs- und Richtlinienebene statt mit der Datenübertragungsebene befasst. Es kann dieselbe Netzwerksicherheit auf granularer Ebene anwenden, ohne dabei auf Pakete zuzugreifen. Es teilt dem Netzwerk dynamisch mit, wer mit wem kommunizieren kann, basierend auf den Attributen und Identitäten, die im Netzwerk gefunden werden.\n\nEin Dreieck aus Benutzern, Anwendungen und Geräten ist eine ausgezeichnete Möglichkeit, dies zu veranschaulichen. Innerhalb dieses Dreiecks befinden sich alle Verbindungen, über die sich die Dinge miteinander austauschen. Elisity kann jede einzelne Verbindung oder jeden Kanal zwischen Benutzern, Anwendungen und Geräten mit hoher Genauigkeit absichern. Dies geschieht auf der Ebene der Netzwerksteuerung. Sobald der Datenverkehr auf die erste Schnittstelle trifft, wird er gesichert.\n\nSicherheit ist auf der Netzwerkebene und nicht auf der Anwendungsebene unerlässlich, da es sich um Zehntausende von Geräten in Ihrem Netzwerk handeln kann, auf die Sie keinen Agenten setzen oder ändern können, da sie beispielsweise eingebettete Betriebssysteme, Kameras, Zutrittsleser und Personen haben. Dies ist besonders wichtig auf der OT-Seite.\n\nMit OT liegt der Fokus auf Verfügbarkeit und Integrität. Dinge müssen weiterhin sicher funktionieren, da der Prozess kritische Infrastruktur darstellen könnte. Dies unterscheidet sich von IT, wo man eine Quarantäne einrichten und eine neue Instanz starten kann, wenn es zu einem Eindringen kommt. OT erfordert eine andere Denkweise. Während etwas in der IT sechs Monate oder ein Jahr existieren kann, kann etwas in OT 20 Jahre existieren. Oft werden diese Infrastrukturen alle paar Jahre gewartet oder aktualisiert oder bleiben bis zum Auftreten eines Fehlers unbeaufsichtigt. Sicherheit bedeutet die Beschränkung der Konnektivität. Wenn Sie sich zum Beispiel in einem VLAN befinden, bedeutet das nicht, dass Sie mit einem RTU, einem Sensor, einem HMI oder einem DCS sprechen sollten, die einen Switch entfernt sind. Bei einer veralteten Konstruktion besteht keine effiziente, flexible Möglichkeit, zu verhindern, dass diese Geräte sich verbinden.\n\nMit Elisitys System legen Sie Richtlinien basierend auf der Gruppierung von Dingen oder den Attributen, die Assets zugewiesen sind, fest, anstatt jede einzelne Position durchzugehen. Dies kann so einfach sein wie das Gruppieren aller Ihrer Prozesse, so dass sie nur in Nord-Süd-Richtung kommunizieren können und nicht in Ost-West-Richtung, oder beispielsweise das Zulassen, dass Abteilungsleiter sechs oder sieben Arten von Dingen teilen können. Zwei oder drei Richtlinien können schnell Ihre Angriffsfläche von 65.000 potenziellen Angriffsports auf zwei oder drei reduzieren.\n\nEin Anwendungsfall, der in OT-Netzwerken häufig vorkommt, ist, wenn ein Anbieter ein Update auf einem Gerät durchführt. Du musst sie hereinlassen und ihnen Zugriff auf dein gesamtes Netzwerk gewähren. Mit Elisity kannst du ihnen jedoch einfach für eine begrenzte Zeit Zugang gewähren und ihnen erlauben, auf feste Vermögenswerte zuzugreifen.\n\nAngenommen, Sie möchten mehr über Elisity's Technologie erfahren. In diesem Fall erklären viele Ressourcen, einschließlich Videos, White Papers und Dokumente, wie die Lösung funktioniert und wie sie innerhalb einer Woche in Ihrem Netzwerk auf Elisity.com angewendet werden könnte.\n\nSchauen Sie sich die vorherige Folge dieser Serie [hier](episode-EDT101) an.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Dana Yanch","Dan Demers"],"link":"/episode-EDT102-de","image":"./episodes/edt-102/de/thumbnail.png","lang":"de","summary":"Im Teil 2 von Sicherheit mit Elisity diskutiert Darren identitätsbasierte Mikrosegmentierungslösungen mit Dana Yanch, Direktorin für technisches Marketing, und Dan Demers, technischer Architekt."},{"id":5,"type":"Episode","title":"Operationalisierung Ihrer KI-Projekte","tags":["aiops","devops","compute","technology","process","devsecops","cybersecurity","aiml"],"body":"\r\n\r\nGretchen ist ein ausgezeichnetes Beispiel für jemanden, der kontinuierlich lernt und sich anpasst. Ihr Bachelor-Abschluss ist in Mathematik. Sie hat einen Master-Abschluss in Betriebswirtschaft und vor einigen Jahren ein Programm an der Harvard University abgeschlossen, das sich auf Datenwissenschaften konzentrierte und sie zur Chefin der Datenwissenschaftler im öffentlichen Sektor bei Intel führte. Sie arbeitet seit über 20 Jahren in der Technologiebranche, zunächst im Bereich Softwareentwicklung, und verbrachte 15 Jahre im öffentlichen Sektor.\n\nSie findet die Arbeit im öffentlichen Sektor besonders lohnenswert, da sie einen Unterschied im Leben der Bürgerinnen und Bürger im Alltag bewirkt. Darüber hinaus verfügt die Bundesregierung über die meisten Daten auf dem Planeten, was perfekt für jemanden ist, der gerne von Daten umgeben ist und weiter lernen möchte.\n\nEs gibt viele Begriffe rund um KI. Zunächst ist es wichtig, den Unterschied zwischen künstlicher Intelligenz (KI) und maschinellem Lernen (ML) zu verstehen. ML ist eine Technik, die Teil von KI ist; sie ist eine Untergruppe. ML-Algorithmen beziehen ihre Stärke daraus, dass sie aus verfügbaren Daten lernen können. In erster Linie lernt man entweder aus überwachtem oder nicht-überwachtem Datensatz.\n\nDer einfache Unterschied zwischen überwachtem und nicht überwachtem Lernen liegt in der Datenkennzeichnung. Beim überwachten Lernen sind die Datensätze gekennzeichnet. Das bedeutet, dass das Aussehen der Daten bereits festgelegt ist. Dadurch wird die Klassifizierung und Vorhersage wesentlich einfacher. Beim nicht überwachten Lernen versucht man, Muster in den Daten zu finden; die Maschine lernt, Beziehungen zwischen Daten zu erstellen, indem sie gemeinsame Wege, Ähnlichkeiten oder Unterschiede findet.\n\nEin Beispiel für überwachtes Lernen wäre ein Online-Shop, der einem Kunden anhand seiner Einkaufshistorie einen Artikel empfiehlt, den er vielleicht kaufen möchte, oder ein Streaming-Dienst, der basierend auf den Sehgewohnheiten einer Person einen Film empfiehlt.\n\nViele Begriffe haben jetzt die Abkürzung \"ops\" am Ende. Zum Beispiel sagen die Leute \"DL ops\" für Deep Learning Operationen, eine Unterkategorie des Maschinenlernens. Warum das \"ops\"? Zunächst ist es nicht so anspruchsvoll wie DevOps. Stattdessen wurde es von der weit verbreiteten Idee des DevOps-Ansatzes zur Erstellung und Anpassung von Anwendungen beeinflusst. Die Menschen versuchen, eine Reihe von Praktiken zu entwickeln, um die Zuverlässigkeit und Effizienz des Designs, der Entwicklung und der Ausführung von maschinellem Lernen zu optimieren. Es wäre also fast wie ein Marktplatz, auf dem Sie benutzerdefinierte Anwendungen erstellen und betreiben können und sie dann mit anderen teilen können.\n\nViele Modelle und Algorithmen sind bereits optimiert und in Tools wie Converge.io oder C3 AI verfügbar. Diese Methoden und Technologien können Ihnen dabei helfen, Ihre maschinellen Lernmodelle zu optimieren. Der beste Weg, dies zu tun, besteht darin, viele Tools zu verwenden, die entweder Open Source sind oder spezifisch vom Hersteller entwickelte Software, um das Erstellen, Entwickeln, Designen, Ausführen und die Ansteuerung wesentlich zugänglicher zu machen.\n\nDie Entwicklung von künstlicher Intelligenz ähnelt dem Stand der Softwareentwicklung vor 30 Jahren. Viele der Schritte sind immer noch manuell und hoffentlich bald automatisiert.\n\nIn früheren Episoden haben Darren und Gretchen darüber diskutiert, wie viele KI- und ML-Projekte einmalige wissenschaftliche Experimente sind. Dann geht der Data Scientist zu etwas anderem über und es wird nie operationalisiert. Im Gegensatz dazu zielt ML Ops darauf ab, das Modell nach dem Training und Lernen bereitzustellen, um einen echten Mehrwert zu bieten.\n\nEinige Unternehmen nutzen diese Werkzeuge explizit. Domino Labs zum Beispiel, schafft fast diesen Marktplatz. Arbeit im öffentlichen Sektor, sagen wir, bei nuklearen U-Booten zur Objekterkennung oder Clusterklassifizierung, könnte auch in der Luftwaffe oder anderen Hilfseinrichtungen anwendbar sein, damit diese Arbeit katalogisiert werden kann, um agile Umgebungen zu unterstützen und aufzubauen. Sie könnten einige Algorithmen nutzen und je nach den Ergebnissen unterschiedlich gewichten. Sie könnten es anhand der Unterschiede in den Datensätzen anpassen, aber zumindest gibt es... Anhaltspunkte? Gemeinsamkeiten? Geteilte Werkzeuge? Ihre letzten Worte hier sind abgebrochen...\n\nSicherheit beschäftigt sich immer mit Open-Source-Software und -Modellen, und KI hat einzigartige Umstände. Zum Beispiel, wie weiß man, ob der Entwickler es nicht trainiert hat, sein Gesicht in einem Gesichtserkennungsmodell zu ignorieren? Es wird jetzt erwartet, dass Menschen Dinge dokumentieren, zum Beispiel, woher ein Datensatz stammt.\n\nEs gibt auch das Thema Ethik und Verantwortung. Der Tay Chatbot und die voreingenommenen Gesichtserkennungsprogramme waren großartige Beispiele dafür, wie KI ohne böswillige Absicht schiefgehen kann. In der ML-Operations (ML-Ops) wurde lange Zeit von einer einzelnen Person gearbeitet, die die Ergebnisse produzierte. Jetzt geht es darum, dass man ein vielfältiges Team von Menschen mit unterschiedlichen Fähigkeiten und Weltanschauungen benötigt.\n\nDie erste Konferenz zur Diskussion von KI und ML fand 1956 am Dartmouth College statt. Die Wahrheit ist, dass viele Grundlagen der KI, wie die logistische Regression, lineare Regression, Clustering-Algorithmen usw., mathematische Gleichungen sind, die schon lange existieren. Natürlich gibt es brillante zusätzliche Frameworks wie TensorFlow, auf denen man aufbauen kann, aber die Grundlagen sind/waren immer noch die Basis. Wir haben Rechenleistung, Speicher, 5G und einzigartige Fähigkeiten hinzugefügt. Sobald das Training abgeschlossen ist, haben wir die Daten und Informationen neben der Technologie anstatt alles zur Technologie bringen zu müssen. Die Technologie zu den Daten zu bringen eröffnet uns nun einige unterhaltsame und aufregende Probleme, die wir lösen können.\n\nAber Gespräche darüber, wie das Modell trainiert wurde, was die ursprünglichen Daten waren und wie man den Modelldruck berücksichtigt, müssen immer stattfinden. Nach einer gewissen Zeit muss man das Modell erneut trainieren; vielleicht muss man einen anderen Algorithmus hinzuziehen oder den aktuellen anders gewichten, um genauere Informationen zu erhalten, da es mehr und vielfältigere Daten gibt. Das ist alles gut, denn es erhöht das Genauigkeitsniveau.\n\nMit der Entwicklung hin zu ML Ops können Sie dies kontinuierlich tun. Genau wie die Softwareentwicklung auf kontinuierliche Integration und Bereitstellung hingearbeitet hat, wird dasselbe in KI oder ML geschehen, wo Modelle aktualisiert werden und immer genauer werden.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Gretchen Stewart"],"link":"/episode-EDT103-de","image":"./episodes/edt-103/de/thumbnail.png","lang":"de","summary":"Intel's Darren Pulsipher, Chief Solutions Architect, Public Sector, und Gretchen Stewart, Chief Data Scientist, Public Sector, sprechen über die Operationalisierung von KI-Projekten."},{"id":6,"type":"Episode","title":"Informationsgesteuerte Führung","tags":["data","informationdriven","organizationalchange","radiusag","change","people","process","ceo"],"body":"\r\n\r\nObwohl Betsy CEO der Radius Advisory Group ist und technisch gesehen aus dem Bundesbereich ausgeschieden ist, behält sie dennoch ein Bein im öffentlichen Sektor durch ihr Unternehmen, das sich auf Cybersicherheit und Cyberspace-Angelegenheiten von nationaler Bedeutung konzentriert. Das Arbeiten sowohl im privaten als auch im öffentlichen Sektor zur gleichen Zeit war der aufregendste Teil ihrer beruflichen Laufbahn.\n\nBetsy begann als aktiver Dienstleister bei der Luftwaffe und wechselte dann in die Industrie, während sie Kinder und einen Ehepartner jonglierte, der ebenfalls im aktiven Dienst war. Sie hat in mehreren Branchen gearbeitet, zuletzt in Versorgungs- und Energieunternehmen, und verbrachte viel Zeit bei PricewaterhouseCoopers. Als sie ausgewählt wurde, mit Sekretär Gates als Mitglied der Defense Efficiencies Task Force zu arbeiten, kehrte sie zum Verteidigungsministerium (DOD) zurück. Sie blieb unter Sekretär Pancetta und hatte eine aufregende Reise, in der sie schließlich zur stellvertretenden CIO für Geschäftsprozess- und Systemüberprüfung ernannt wurde. Dort schuf sie eine Datenanalysefunktion, um mehr Transparenz über die Kosten und potenziellen Effizienzen der Informationstechnologie im gesamten DoD zu bieten.\n\nBetsys größte Herausforderung in der Rolle des stellvertretenden CIO, von der sie glaubt, dass sie sowohl im öffentlichen als auch im privaten Sektor zutrifft, besteht darin, neues Denken, Prozesse, Technologien und Arbeitsmethoden in die Organisation einzubringen. In großen Organisationen ist der Umfang enorm und es gibt viele Abteilungen, von denen jede ihre eigene Kultur, eigene Agenden, Budgets und Gewinn- und Verlustrechnungen hat. Situationen wie die COVID-Pandemie, in denen Veränderungen schnell geschehen müssen, sind unglaublich herausfordernd.\n\nBetsy sagt, dass COVID die Kultur in gewisser Weise verändert hat, aber in anderen Bereichen dazu geführt hat, dass sich die Menschen noch mehr verkriechen, was nicht gut ist. Es gab viele Herausforderungen im Bereich der Prozesse und Technologien, aus denen jeder gelernt hat und immer noch lernt. Eine Sorge, die Betsy hat, ist, dass es jetzt eine neue Umgebung und ein neues Ökosystem gibt und die Rückkehr ins Büro nicht einfach wie zuvor behandelt werden kann, wie es viele Führungskräfte versuchen. Obwohl dies sehr schwierig zu bewältigen ist und ob die Führungskräfte die neue Umgebung als gut oder schlecht betrachten, kann sie nicht wie zuvor behandelt werden.\n\nSobald Führungskräfte jedoch einen Wechsel vollziehen, ist Betsys Strategie, alles im Sprint zu erledigen. Diese Lektion hat sie gelernt, als sie stellvertretende CIO war und verschiedene Projekte zusätzlich zur zugrunde liegenden Mission übertragen bekam. Sie bemerkte, dass anderen Teams wiederholt 30 Tage Zeit für Projekte gegeben wurden, während ihr Team nur 10 Tage hatte. Als sie nachfragte, erklärte die Führungsebene, dass sie wussten, dass ihr Team es in 10 Tagen schaffen könnte und dass komplexe Entscheidungen getroffen werden mussten, die Informationen erforderten, die ihr Team im Rahmen des Entscheidungsprozesses liefern konnte. Je schneller sie diese Informationen erhielten, desto besser. Also sagte sie ihrem Team, dass sie es einfach herausfinden müssten. Sie taten es und wurden darin gut.\n\nEin Gefühl der Dringlichkeit hilft Menschen, sich zu konzentrieren und Leistungen zu erbringen, aber Führungskräfte müssen laut Betsy sicherstellen, dass sie mit den richtigen Mitarbeitern, Ressourcen und Autorität ausgestattet sind, um etwas in einem engen Zeitrahmen umzusetzen. Diese Unterstützung der Führungsebene ist entscheidend für den Erfolg.\n\nBetsy glaubt, dass das Zuhören von Menschen wesentlich ist, aber Entscheidungen werden von oben getroffen, was im Allgemeinen nur ein paar Personen bedeutet. Eine Matrixorganisation, die in den 90er und 2000er Jahren an Popularität gewann, führte zu zusätzlicher Komplexität, aber die meisten Situationen sind bereits komplex, daher kommt es immer noch auf Führung an. Führungskräfte müssen inklusiv genug sein, um so vielen Menschen wie möglich zuzuhören, aber es darf keine fünf Monate und 150 Meetings dauern. Sie müssen Systeme und Gruppen entwickeln, die Ideenfindung betreiben, Modelle vorschlagen und miteinander arbeiten können. Führungskräfte müssen die Fähigkeit haben, schnell zu handeln; manchmal müssen sie schnell agieren. Oft ist die Antwort weniger Menschen, aber die richtigen Menschen, weniger Technologie und die richtige Technologie, um das zu bekommen, was man braucht.\n\nBetsy verwendete ein praktisches Modell, als sie dringende Projekte vom stellvertretenden Verteidigungsminister erhielt, um kleine Teams von fünf bis sieben Personen mit vielfältigen kognitiven Fähigkeiten aus ihren 50 Analysten zu bilden. Dies war erfolgreich, da sie die Persönlichkeiten und Fähigkeiten ihrer Analysten gut kannte und Teams gezielt auswählen konnte, anstatt ein Problem einer großen Gruppe vorzulegen und versuchen, die Meinungen aller einzuholen. Die kleinen Teams überraschten sie immer wieder durch die Bewältigung komplexer Aufgaben und das Lösen von Problemen.\n\nDurch die Einrichtung von Prozessen und Vorlagen zur Problemlösung konnte Betsys Team oft innerhalb weniger Stunden Eingaben an den stellvertretenden Verteidigungsminister oder den CIO liefern, falls nötig. Letztendlich hatten sie Prozesse etabliert, die es ihnen ermöglichten, Ideen zu entwickeln und neutrale datenbasierte Optionen unter Berücksichtigung vieler Faktoren vorzuschlagen. Dadurch konnten die Führungskräfte die Fakten und Beweise betrachten und Entscheidungen treffen.\n\nBetsy hatte Vertrauen in ihre Teams und sagte ihnen nie, wie sie die Arbeit erledigen sollen oder nahm an, dass sie den besten Ansatz kannte. Sie hatte gute Leute und vertraute ihnen einfach, es zu tun. Dadurch wurden ihre Leute motiviert und sie waren ständig auf neue Aufgaben gespannt. Mitarbeiter möchten wissen, dass ihre Führungskräfte ihnen zuhören, ihre Ideen berücksichtigen und nutzen. Dieser Ansatz ermöglichte es den Menschen, ihr bestes Analyseergebnis und ihre Optionen mitzuteilen. Oft war es eine Kombination dieser Analysen, die der Führung vorgelegt wurde. Den Verdienst dafür haben die Führungskräfte im Verteidigungsministerium und im Büro des CIO, die Betsys Vorgehensweise vertrauten.\n\nJedes Mal, wenn es ein neues Problem gab, setzte Betsy ein neues Team ein. Auf diese Weise konnten verschiedene Personen an verschiedenen Arten von Projekten arbeiten und wurden nicht auf einen Bereich festgelegt. Außerdem konnten die Menschen über generationale Grenzen hinweg zusammenarbeiten. Manchmal waren die Leute zunächst widerwillig, mit Altersgruppen außerhalb ihrer eigenen zusammenzuarbeiten, aber am Ende lernten sie, Dinge durch verschiedene, vorteilhafte Perspektiven zu sehen.\n\nSeit Betsy und ihr Stellvertreter ihre Leute gut kennengelernt haben, konnte sie schnell effektive Teams zusammenstellen. Ein Kennzeichen ihres Erfolgs ist, dass niemand jemals nein sagte, wenn sie ein Team bat, an zwei Dingen zu arbeiten. Sie haben es einfach gemacht.\n\nKlicken Sie hier für die zweite Hälfte von Darrens Gespräch mit Betsy Freeman.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Betsy Freeman"],"link":"/episode-EDT104-de","image":"./episodes/edt-104/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel, spricht mit Betsy Freeman, CEO von Radius Advisory Group, über ihre Erfahrungen als informationsorientierte Führungskraft im öffentlichen und privaten Sektor. Teil eins von zwei."},{"id":7,"type":"Episode","title":"Informationsgesteuerte Führung Teil 2","tags":["change","people","changeagent","radiusag","organizationalchange","informationdriven","leadership"],"body":"\r\n\r\nObwohl Betsy die CEO von Radius Advisory Group ist und formell aus dem öffentlichen Sektor ausgeschieden ist, behält sie dennoch einen Fuß im öffentlichen Bereich durch ihr Unternehmen, das sich auf Cybersicherheit und Cyber-Space-Probleme von nationaler Bedeutung konzentriert. Das Arbeiten sowohl im privaten als auch im öffentlichen Sektor zur gleichen Zeit war der aufregendste Teil ihrer beruflichen Reise.\n\nBetty begann ihre Karriere als aktiver Dienstmitglied in der Air Force und wechselte dann in die Industrie, während sie Kinder und einen Ehepartner jonglierte, der ebenfalls im aktiven Dienst war. Sie arbeitete in verschiedenen Branchen, zuletzt im Bereich Versorgungs- und Energiewirtschaft, und verbrachte viel Zeit bei PricewaterhouseCoopers. Als sie ausgewählt wurde, mit Sekretär Gates als Mitglied der Defense Efficiencies Task Force zusammenzuarbeiten, kehrte sie zum Department of Defense (DOD) zurück. Sie blieb unter Sekretär Pancetta und machte eine aufregende Reise, wurde schließlich zur stellvertretenden CIO für Geschäftsprozesse und Systemüberprüfung ernannt. Dort richtete sie eine Datenanalysefunktion ein, um mehr Transparenz über die Kosten und potenzielle Effizienzen von Informationstechnologie im gesamten DOD zu schaffen.\n\nBetsys größte Herausforderung in der Rolle des stellvertretenden CIO, von der sie glaubt, dass sie sowohl im öffentlichen als auch im privaten Sektor zutrifft, besteht darin, neues Denken, Prozesse, Technologien und Arbeitsweisen in die Organisation einzubringen. In großen Organisationen ist der Umfang enorm und es gibt viele Silos, von denen jeder seine eigene Kultur, Agenda, Budgets und Gewinn- und Verlustrechnungen hat. Situationen wie die COVID-Pandemie, in denen Veränderungen schnell erfolgen müssen, sind unglaublich herausfordernd.\n\nBetsy sagt, dass COVID in gewisser Weise die Kultur verändert hat, aber in anderen Bereichen dazu geführt hat, dass sich die Menschen noch mehr verkrochen haben, was nicht gut ist. Es gab viele Prozess- und Technologieherausforderungen, aus denen alle gelernt haben und weiterhin lernen. Eine Sorge, die Betsy hat, ist, dass es jetzt eine neue Umgebung und ein neues Ökosystem gibt und die Rückkehr ins Büro nicht einfach so wie früher gehandhabt werden kann, wie es viele Führungskräfte versuchen. Obwohl dies sehr schwierig zu navigieren ist und egal, ob Führungskräfte die neue Umgebung gut oder schlecht finden, sie kann nicht wie zuvor behandelt werden.\n\nSobald Führungskräfte jedoch einen Wechsel vollziehen, ist Betsys Strategie, alles zu sprinten. Sie hat diese Lektion gelernt, als sie stellvertretende CIO war und neben der zugrunde liegenden Mission verschiedene Projekte übertragen bekam. Sie bemerkte, dass anderen Teams wiederholt 30 Tage für Projekte zur Verfügung gestellt wurden, während ihr Team lediglich 10 Tage Zeit hatte. Als sie nachfragte, erklärte die Führung, dass sie wussten, dass ihr Team es in 10 Tagen schaffen konnte und dass komplexe Entscheidungen getroffen werden mussten, bei denen die Informationen ihres Teams Teil des Entscheidungsprozesses waren. Je schneller sie diese erhalten konnten, desto besser. Also sagte sie ihrem Team, dass sie es einfach herausfinden müssten. Sie taten es und wurden darin gut.\n\nEin Gefühl der Dringlichkeit hilft Menschen, sich zu konzentrieren und zu handeln, aber Betsy weist darauf hin, dass Führungskräfte sicherstellen müssen, dass sie mit den richtigen Personen, Ressourcen und Autorität ausgestattet sind, um etwas innerhalb eines kurzen Zeitrahmens umzusetzen. Diese Unterstützung der Führungsebene ist entscheidend für den Erfolg.\n\nBetsy glaubt, dass Zuhören der Menschen wesentlich ist, aber Entscheidungen werden an der Spitze getroffen, was im Allgemeinen auf wenige Personen hinweisen sollte. Eine Matrix-Organisation, die in den 90er und 2000er Jahren popular wurde, fügte zusätzliche Komplexitätsebenen hinzu, aber die meisten Situationen sind bereits komplex, daher kommt es immer noch auf Führung an. Führungskräfte müssen inklusiv genug sein, um so vielen Menschen wie möglich zuzuhören, aber es darf nicht fünf Monate und 150 Meetings dauern. Sie müssen Systeme und Gruppen entwickeln, die Ideenfindung durchführen können, Modelle vorschlagen und miteinander zusammenarbeiten können. Führungskräfte müssen die Fähigkeit haben, schnell zu handeln; manchmal müssen sie schnell handeln. Oft ist die Antwort weniger Menschen, aber die richtigen Menschen, weniger Technologie und die richtige Technologie, um das zu bekommen, was man braucht.\n\nBetsy verwendete ein effektives Modell, als sie dringende Projekte vom stellvertretenden Verteidigungsminister erhielt, um kleine Teams von fünf bis sieben Personen mit vielfältigen kognitiven Fähigkeiten aus ihren 50 Analysten zu bilden. Dies war erfolgreich, weil sie die Persönlichkeiten und Fähigkeiten ihrer Analysten gut kannte und Teams gezielt auswählen konnte, anstatt ein Problem einer großen Gruppe vorzulegen und versuchen, die Meinung aller einzuholen. Die kleinen Teams überraschten sie immer wieder, indem sie komplexe Aufgaben erledigten und Probleme lösten.\n\nDurch die Einrichtung von Prozessen und Vorlagen zur Problemlösung konnte Betsys Team oft innerhalb weniger Stunden dem stellvertretenden Verteidigungsminister oder dem CIO Rückmeldungen geben, wenn es erforderlich war. Letztendlich hatten sie verfügbare Prozesse, die es ihnen ermöglichten, Ideen zu entwickeln und neutrale, datenbasierte Optionen basierend auf vielen Überlegungen zu präsentieren. Dies ermöglichte es den Führungskräften, sich die Fakten und Beweise anzusehen und Entscheidungen zu treffen.\n\nBetsy hatte Vertrauen in ihre Teams und hat ihnen nie gesagt, wie sie die Arbeit machen sollen oder angenommen, dass sie den besten Ansatz kennen würde. Sie hatte gute Leute und vertraute einfach darauf, dass sie es machen. Dies motivierte ihre Mitarbeiter und sie waren ständig gespannt auf neue Aufgaben. Mitarbeiter möchten wissen, dass ihre Führungskräfte ihnen zuhören und ihre Ideen berücksichtigen und nutzen. Mit diesem Ansatz konnten die Menschen ihr beste Analyse und Optionen mitteilen. Oft war es eine Kombination dieser Analysen, die zur Führung zurückkam. Das Verdienst geht an die Führung im Verteidigungsministerium und im Büro des CIO, die Betsys Prozess vertrauten.\n\nJedes Mal, wenn es ein neues Problem gab, hat Betsy ein neues Team zugeteilt. Auf diese Weise konnten verschiedene Leute an verschiedenen Arten von Projekten arbeiten und wurden nicht auf einen Bereich festgelegt. Außerdem konnten die Leute generationenübergreifend zusammenarbeiten. Manchmal waren die Leute anfangs widerwillig, mit Altersgruppen außerhalb ihrer eigenen zu arbeiten, aber letztendlich lernten sie, Dinge aus verschiedenen, vorteilhaften Perspektiven zu betrachten.\n\nSeit Betsy und ihr Stellvertreter ihre Leute gut kennengelernt haben, konnte sie schnell effektive Teams zusammenstellen. Ein Merkmal ihres Erfolgs ist, dass niemand jemals nein sagte, wenn sie ein Team bat, an zwei Dingen zu arbeiten. Sie haben es einfach gemacht.\n\nKlicken Sie [hier](episode-EDT104), um die erste Hälfte von Darrens Gespräch mit Betsy Freeman anzusehen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Betsy Freeman"],"link":"/episode-EDT105-de","image":"./episodes/edt-105/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solutions Architect, öffentlicher Sektor, Intel, setzt sein Gespräch mit Betsy Freeman, CEO von Radius Advisory Group, über ihre Erfahrung als informationsgesteuerter Führungskraft im öffentlichen und privaten Sektor fort. Teil zwei von zwei."},{"id":8,"type":"Episode","title":"Die Geburt von Graph Intelligence Plattformen","tags":["data","graphintelligenceplatform","graphdb","katanagraph","technology"],"body":"\r\n\r\nGreg begann seine Karriere bei einer Investmentbank im Bereich Kreditrisiko, als sie damit begannen, strengen Stresstests im Rahmen des CCAR durchzuführen. Nachdem er eine Zeit lang intensiv daran beteiligt war, begann er Beratungstätigkeiten und wurde dabei mit Grafiken vertraut, während er die Validierung von Kreditrisikomodellen durchführte. Er erkannte, wie Grafiken für viele verschiedene Arten von Analysen genutzt werden können und dabei Vorteile in der Datenverwaltung und im maschinellen Lernen haben, insbesondere bei der Kreditmodellierung. Von dort fand er den Weg zu Katana.\n\nDatenanalysten und Datenwissenschaftler haben ständig Schwierigkeiten, verschiedene Datensätze zu integrieren. Greg wurde von Graphen angezogen, denn nachdem er in RDF, einem semantischen Wissensgraphenformat, eingeführt wurde, ergab es intuitiven Sinn, wie die Daten kombiniert und strukturiert werden könnten.\n\nMit vorhandenen Graph-Lösungen hatten Analysten Schwierigkeiten, ihre Lösungen zu skalieren, da ein Großteil ihrer Daten so umfangreich war. Katana Graph hat die Fähigkeit entwickelt, zu skalieren und sich auch auf maschinelles Lernen zu konzentrieren.\n\nAm Anfang von Graphdatenbanken haben große Unternehmen wie Amazon und Facebook intern Graphdatenbanken aufgebaut, indem sie ihr Modellieren und maschinelles Lernen betrieben haben. Dann kamen Verbraucherversionen von Plattformen wie Neo4j und TigerGraph für allgemeine Anwendungsfälle. Die Herausforderung war, dass sie sich hauptsächlich auf die Datenbank konzentrierten und nicht so sehr auf die Analyse und das maschinelle Lernen, die Prozesse und die eigentliche Graphenberechnung. Sie waren darauf beschränkt, eine Art Datenspeicher zu sein, mit Schwerpunkt auf der Aufnahme und den CRUD-Operationen und nicht so sehr auf den Daten.\n\nEs gibt drei verschiedene Arten von Graphbereichen für die Berechnung. Die erste ist die Graphabfrage, die Graphdatenbank und CRUD-Operationen. Die zweite ist die Graphanalyse und -mining mit PageRank oder Clustering-Algorithmen, die immer beliebter werden. Der dritte Bereich ist die Graph-KI und das maschinelle Lernen. Hier kommen graphbasierte neuronale Netze ins Spiel. Es gibt punktuelle Lösungen, die spezifische Teile dieser Bereiche lösen werden, aber Katana Graph befindet sich an der Schnittstelle dieser Bereiche.\n\nJede der drei Plattformen ist wichtig. Um Machine Learning zu machen, braucht man die anderen beiden Bereiche. Wenn die Daten zuerst aufgenommen werden, müssen sie viele Transformationen durchlaufen, um sie für das Machine Learning vorzubereiten. Wenn du nicht alles in einer Lösung hast, wird der Datenfluss langsam sein, da die Daten hin und zurück geschickt werden. Es ist effizienter, schnell an der gesamten Datenpipeline zu arbeiten. Es verringert auch das Risiko, Daten zu verlieren, da du die Anzahl der Datentransformationen reduzierst.\n\nDarüber hinaus können Sie aufgrund der cloudbasierten Plattform von Katana Graph den Cluster anhalten, einen Prüfpunkt speichern, den Cluster herunterfahren und später wieder hochfahren, genau an der Stelle, an der Sie aufgehört haben.\n\nEine gute Demo befasst sich mit der Betrugserkennung anhand eines Bitcoin-Transaktionsdatensatzes. Die Plattform verarbeitet die strukturierten Daten, wobei die Bitcoin-Wallets die Knoten im Graphen sind und die Transaktionen die Kanten zwischen ihnen darstellen. Es handelt sich um einen einfachen Graphen. Das Ziel ist es, vorherzusagen, ob eine Bitcoin-Wallet betrügerisch ist. Illegale Wallets für Geldwäsche, Drogenhandel usw. wurden gekennzeichnet. Wenn ein neues Konto eingegangen ist, besteht die Aufgabe darin, vorherzusagen, ob es betrügerisch ist. Der anspruchsvolle Teil besteht dann darin, eine Vorverarbeitung der numerischen Merkmale der Konten durchzuführen. Zu diesem Zweck wurde ein Satz von APIs entwickelt, um dieses Problem anzugehen. Alle Vorbereitungen, die Datenwissenschaftler durchführen, um ihre Merkmale vorzubereiten, werden hier erledigt. Von dort aus ist der Graph bereit, in das maschinelle Lernmodell einzugeben, in dem er trainiert wird. Mithilfe von neuronalen Netzwerken kann dann gelernt werden, wie die Konten klassifiziert werden können.\n\nEin Vorteil der Plattform besteht darin, dass Datenwissenschaftler und Ingenieure von einer Plattform aus arbeiten können, anstatt Dinge zusammenzufügen.\n\nEin weiterer Vorteil ist die Gesamtbetriebskosten. Im Gegensatz zu anderen Plattformen müssen Sie die gesamte Grafikdatenbank nicht die ganze Zeit laufen lassen. Da Katana Graph Pipelines mit einer Trennung von Speicherung und Berechnung entwickelt wurden, können Sie problemlos einen Cluster starten, einige Batch-Verarbeitung durchführen und dann Inferenzierung in einem separaten System ausführen und trotzdem nutzen, was in der Grafik generiert wurde.\n\nKatana Graph ist auch schneller mit großen Datensätzen, da es nicht sofort alle Daten lädt, sondern eine innovative, dynamische Methode hat, um die Daten zu laden, während sie benötigt werden, während Sie durch die Pipeline arbeiten.\n\nDie grundlegenden Analysefunktionen sind auf einer Graphdatenbank viel einfacher als auf einer relationalen Datenbank. Wenn Sie zehn verschiedene Datensätze haben, kann es mühsam und fehleranfällig sein, für einen Analysten herauszufinden, wie sie verbunden werden können, um eine Abfrage in einer relationalen Datenbank zu schreiben. Mit einem Graphen haben Sie ein einheitliches, vordefiniertes und bereits aufgebautes Modell, sodass die Fragen viel einfacher sind, weil die Daten bereits miteinander verbunden sind. Sie können intuitiv sehen, wie die Informationen zusammenhängen.\n\nEine der neuen Funktionen von Katana ist ein Dash Data Frame Importeur. Dash ist ein gängiges Framework, das von Datenwissenschaftlern zur parallelen Verarbeitung von Datenrahmen verwendet wird. Die Datenwissenschaftler können mit dem bereits verwendeten Datenrahmen arbeiten und ihn direkt in den Katana Graph importieren, um eine nahtlose und vereinfachte Erfahrung zu ermöglichen.\n\nDevOps ist ein wichtiger Teil dessen, was Katana mit ihrer Plattform unterstützen möchte. Sie integrieren problemlos in vorhandene Lernpipelines. Wenn die Graph-Neuronalen Netzwerke ausgeführt werden, können die Einbettungen exportiert werden. Die Features, die ein Graph generiert, können in einen maschinellen Lernprozess weitergeleitet werden. Dadurch wird die Integration wesentlich einfacher und leichter operativ umsetzbar und produktiv einsetzbar.\n\nIn den nächsten fünf Jahren stellt sich Greg vor, dass Organisationen wie Banken zentralisierte Repositories haben werden, um Kundendaten, Marketingdaten oder Kreditdaten für verschiedene Zwecke zu analysieren. Die Ausgabe von maschinellen Lernmodellen könnte zum Beispiel sowohl für die Kreditrisikobewertung als auch für die Betrugserkennung verwendet werden. Anstatt isolierte Datensätze mit viel Vervielfältigung und Duplizierung zwischen ihnen zu verwenden, gäbe es ein gemeinsames Modell, das innerhalb eines Graphen synchronisiert wird.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Steck"],"link":"/episode-EDT106-de","image":"./episodes/edt-106/de/thumbnail.jpg","lang":"de","summary":"Intel’s Darren Pulsipher, Chief Solutions Architect, Public Sector und Greg Steck, Senior Director of Industry Solutions, Katana Graph, sprechen über die Vorteile von Katana's Graph Intelligence Plattform."},{"id":9,"type":"Episode","title":"Sicherung Kritischer Infrastruktur","tags":["criticalinfrastructure","hotms","irdeto","otsecurity","edge","cybersecurity","technology","process"],"body":"\r\n\r\nCarla stammt ursprünglich aus Mexiko und lebt nach einem Aufenthalt in verschiedenen Ländern, einschließlich Deutschland, in Amsterdam. Sie hat Industrie- und Mechatronik-Ingenieurwesen studiert und nach einigen Jahren in diesem Bereich einen Masterabschluss in Mobilitätssystemtechnik erworben. Dabei hat sie sich auf autonome Fahrzeuge spezialisiert.\n\nDie Definition von kritischer OT-Infrastruktur ist jeder Punkt, der Chaos in der realen Welt auslösen kann. Dies unterscheidet sich sehr von der IT-Infrastruktur; IT mangelt an vitaler Infrastruktur. In der OT-Welt können Menschen sterben, wenn etwas schiefgeht. Zweifellos kann es auch im IT-Bereich zu Chaos kommen, aber das Problem kann durch Reparaturen gelöst werden. Wenn es jedoch zu Chaos in kritischer Infrastruktur wie dem Transportwesen kommt, können Unfälle passieren und Menschenleben stehen auf dem Spiel.\n\nIn den letzten fünf Jahren und insbesondere während der COVID-Pandemie hat die Bedeutung der Sicherheit in kritischer Infrastruktur zugenommen; in einigen Fällen wurde die kritische Infrastruktur unerbittlich angegriffen. Dies kann auf Langeweile und gesteigerte Kreativität der Angreifer zurückgeführt werden, aber auch auf die erhöhte Angriffsfläche durch den plötzlichen Wechsel zur Fernarbeit, der einige zuvor vorhandene Sicherheitsmaßnahmen außer Kraft setzte.\n\nIm Transportwesen liegt ein Teil des Problems in der zunehmenden Vernetzung, die potenzielle Angriffe mit sich bringt. Kunden möchten mehr Dienstleistungen, und Unternehmen möchten mehr Daten und Zugriff auf Informationen. Damit geht die Öffnung des Transportnetzwerks einher. Die zuvor gebotene Absicherung durch eine Luftschicht geht dabei verloren.\n\nDie Industrie kann nicht die Sicherheitsmaßnahmen verwenden, die die IT seit Jahren verwendet hat, weil IT und OT völlig unterschiedlich sind. IT ist im Allgemeinen standardisiert, während OT es nicht ist. OT hat ein umfangreiches Ökosystem mit bedeutenden Unterschieden bei Geräten, die anderen Protokollen und Implementierungen folgen. Zum Beispiel hat jedes Land unterschiedliche Implementierungen. OT befindet sich auch auf einer anderen Ebene, da buchstäblich Menschenleben auf dem Spiel stehen.\n\nEin weiterer Unterschied zwischen der IT- und der OT-kritischen Infrastruktur besteht darin, dass bei einem Problem im Netzwerk in der IT dieses isoliert und sogar heruntergefahren werden kann, und die Arbeitslast an einen anderen Ort verlagert werden kann. Dies ist bei kritischer Infrastruktur nicht möglich, daher sieht der Ansatz anders aus.\n\nDie Gefahr besteht darin, dass IT- und OT-Netzwerke aufgrund des Wunsches nach mehr Konnektivität zusammengebrochen sind und gleichzeitig eine Zunahme der Cyberbedrohungen verzeichnet wurde. Irdeto möchte die Industrie über die Komplexität dieser Probleme informieren und Lösungen anbieten. Es geht um präventive Lösungen, nicht um Reaktionen nachdem eine Katastrophe eingetreten ist.\n\nCarla sagt, dass Organisationen Experten für Sicherheit haben müssen. Die Entwicklung von Sicherheit intern basierend auf vorgeschriebenen Standards ist nicht ausreichend, da die Standards der Entwicklung hinterherhinken.\n\nIrdeto sichert seit 50 Jahren kritische Infrastrukturen. Sie haben etwa 1.000 Mitarbeiter, wovon sich 70 Prozent mit Forschung und Entwicklung beschäftigen, um stets über die Art der heutigen Angriffe und möglichen zukünftigen Angriffsarten informiert zu sein. Irdeto strebt danach, immer einen Schritt voraus oder zukunftssicher zu sein. Ihre Dienstleistungen entwickeln sich parallel zum System.\n\nIrdeto kann Kunden helfen, die wissen, was sie brauchen, sei es PKI, Schlüssel und Berechtigungen, Lebenszyklusmanagement oder Software-Schutz. Sie können auch Kunden helfen, die nicht wissen, was sie brauchen, und dauerhafte Lösungen bieten, wenn sich Bedrohungen weiterentwickeln.\n\nproducts and services.\n\nProdukte oder kontaktieren Sie sie direkt.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Carla Trevino"],"link":"/episode-EDT107-de","image":"./episodes/edt-107/de/thumbnail.png","lang":"de","summary":"Intel's Darren Pulsipher, Chief Solutions Architect, Public Sector, und Carla Trevino, Solutions Architect, Irdeto, sprechen über die Bedeutung von Sicherheit in kritischer Infrastruktur."},{"id":10,"type":"Episode","title":"Geschichte der fortgeschrittenen Kommunikation","tags":["5g","cellphone","comms","wifi6"],"body":"\r\n\r\nDie erste Generation der Mobilfunktechnologie, das Advanced Mobile Phone System (AMPS), wurde in den späten 70er und frühen 80er Jahren entwickelt. In den frühen 80er Jahren galt es noch als Luxus, mit einem sperrigen Taschentelefon einen Anruf vom Auto aus zu tätigen. Der Luxus, mit einem mobilen Gerät telefonieren zu können, wurde bald zur Notwendigkeit.\n\nIn den frühen 90er Jahren entwickelte sich die Technologie, während der Global System for Mobile Communications (GSM) Standard entwickelt wurde, um die Protokolle für 2G zu beschreiben, der bis Mitte der 2010er Jahre zum weltweiten Standard wurde. 2G begann, das Mobiltelefon zu einem Gerät mit mehr Funktionen als nur zum Telefonieren zu machen, indem Textnachrichten hinzugefügt wurden und sogar Spiele gespielt werden konnten.\n\n3G wurde in den frühen 2000er Jahren eingeführt und brachte einige noch junge Datenfähigkeiten mit dem Internet, das sich noch in seinen Anfängen befand. Wi-Fi war noch nicht weit verbreitet, aber man konnte beispielsweise auf das Datennetzwerk eines Mobilfunkanbieters zugreifen, indem man ein Telefon mit einem Laptop verbunden hat. Natürlich konnte man dabei nur das Minimum erreichen, wie zum Beispiel mit Modem- oder DSL-Geschwindigkeiten.\n\nMit 4G hat die Technologie einen einheitlichen Standard erreicht, indem CDMA und GSM unter dem 3rd Generation Partnership Project (3GPP) zu LTE zusammengeführt wurden. Jeder Mobilfunkanbieter begann, diesen gemeinsamen Standard zu übernehmen. Das war der Zeitpunkt, als sich Breitband verbreitete. Leland schreibt den wirtschaftlichen Fortschritt der 2010er Jahre 4G zu, da Unternehmen wie Amazon, Netflix und Uber sowie Plattformen wie YouTube, Google und Facebook existieren und gedeihen konnten.\n\nLeland spricht über 5G in Bezug auf das, was die Netzanbieter bereits bereitgestellt haben. 4G und 5G sind miteinander verbunden, da sie Teil der gleichen spezifischen Veröffentlichungslinie sind. Bei Vierzehn endet das, was wir als 4G LTE Advanced bezeichnen. Bei Fünfzehn kommt 5G NR ins Spiel. In dieser Übergangsphase gibt es ein geschäftliches Ziel und eine Strategie, die neue Technologie als Teil des Standards zu übernehmen. Das geschäftliche Ziel besteht darin, dass Unternehmen bereits in ihre 4G-Netzwerke investiert haben. Daher sind die aktuellen evolved Packet Core- und RAN-Komponenten der 4G-Netzwerke noch vorhanden. Sie fügen eine 5G-RAN-Box mit einer anderen Frequenz hinzu, die dennoch mit dem 4G-Core verbunden ist und als Non-Standalone bezeichnet wird.\n\nDarren klärt auf, dass 4G revolutionär war, weil es viele neue Möglichkeiten erschlossen und eine vollständig neue Ausrüstung erfordert hat, während 5G eher evolutionär ist, da es ebenfalls neue Dinge eröffnet. Dennoch basiert die zugrundeliegende Technologie auf derselben Hardware und demselben Kern.\n\nEs gehört zum Modulationsschema, das 5G in der Luftschnittstelle bietet, aber die Architektur ist unterschiedlich; es ist unter 5G im Vergleich zu 4G mehr virtualisiert und proprietär. Das führt dazu, dass viele Fähigkeiten Teil der 5G-Bereitstellungen werden.\n\nEin Beispiel dafür ist, dass ein Mobilfunkanbieter ein 4G-Netzwerk implementiert hat, indem er eine RAN-Box neben einer alten 3G-Box platzierte. Viele Unternehmen, wie zum Beispiel Sprint, haben ihre 3G-Boxen und ihr CDMA-Netzwerk jahrelang aufrechterhalten. In Wirklichkeit war 4G nur eine weitere Box, die neben einer 3G-Box stand. 5G nimmt diese proprietäre Box und ermöglicht es, die Funktionen dieser Box über ein virtualisiertes Netzwerk zu verteilen. Ein Teil der Basisbandfunktionen von 5G kann nun softwaredefiniert in mehreren Bereichen skaliert werden, im Vergleich dazu, dass sie an einem einzigen Standort, einer Box oder einem Ort enthalten sind.\n\nDas bedeutet, dass Sie Funktionalität zu Ihrem Netzwerk hinzufügen können, ohne die Hardware zu ersetzen. Wenn Sie jedoch eigenständige Netzwerke betreten, können Sie ein 5G-Netzwerk nutzen und etwas vor Ort durchführen. Nehmen wir zum Beispiel an, Sie haben ein Wolkenkratzergebäude anstatt von der Netzabdeckung durch eine Außenantenne mit einer Kernzentrale beim Betreiber oder einer Schaltstation abhängig zu sein. In diesem Fall können Sie ein firmeneigenes Netzwerk innerhalb dieses Gebäudes entwickeln, das die Abdeckung und Datendienste im gesamten Gebäude verbreitet.\n\nDies eigenständige Netzwerk eröffnet viele neue Möglichkeiten und ermöglicht neuen Akteuren den Einstieg. Es ermöglicht auch Organisationen wie der Bundesregierung und dem Verteidigungsministerium, die Technologie für ihre Anwendungsfälle zu übernehmen. Sie haben mehr Flexibilität, wenn sie nicht stark von den Anbietern abhängig sind.\n\nAnna bemerkt, dass neben neuen Spielern und neuen On-Premise-Fähigkeiten auch die Möglichkeit besteht, das CBRS-Frequenzspektrum zu nutzen. Die Verwaltung dieses Spektrums ist komplex, aber es gibt einen nicht prioritären Teil, den man kostenlos nutzen kann, und einen prioritären Teil, das Navy-Spektrum, das man kaufen kann, falls man keine Unterbrechungen möchte. Einige umfangreiche Produktionsanlagen nutzen das CBRS-Frequenzspektrum entweder in Zusammenarbeit mit einem primären Anbieter, der keine Gebühren erhebt, oder in Zusammenarbeit mit einem neuen Marktteilnehmer, der ein eigenständiges On-Premise-Netzwerk mit CBRS einrichtet. Dies ist ein sehr unterschiedliches Modell, und es gibt tatsächliche Vorteile bei der Wellenlänge und Komplexität der Systeme, die man mit 5G im Vergleich zu Wi-Fi aufbauen kann.\n\nEs gibt immer noch einige Vorteile von Wi-Fi, aber das Einrichten eines robusten Wi-Fi-Netzwerks kann herausfordernd sein, besonders wenn Sie große Metallteile bewegen. Wenn Sie eine feste Konfiguration haben, empfiehlt es sich, auf Wi-Fi 6 umzusteigen, insbesondere wenn die wirtschaftlichen Voraussetzungen stimmen.\n\nDie Nachfrage treibt Veränderungen an; die meisten Endnutzer sind mit 4G auf ihren persönlichen Geräten zufrieden. Warum also auf 5G umsteigen? Der Mehrwert von 5G liegt nicht unbedingt in den höheren Datenraten und geringeren Latenzzeiten; diese Dienste werden in großem Umfang bereitgestellt, da sie virtualisiert sind. 5G ist im Vergleich zu 4G softwareintensiv und basiert mehr auf proprietären Boxen und Hardware. 5G kann an vielen Orten virtualisiert werden. Das Frequenzportfolio ist dynamisch und es können un lizenzierte Bänder, lizenzierte Bänder und CBRS genutzt werden, daher gibt es viele weitere Optionen.\n\nSchauen Sie sich den zweiten Teil dieses Interviews [hier](episode-EDT109) an.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown","Anna Scott"],"link":"/episode-EDT108-de","image":"./episodes/edt-108/de/thumbnail.png","lang":"de","summary":"In dieser Folge spricht Darren mit den häufigen Intel-Gästen Leland Brown, Principal Engineer und Technischer Direktor für fortschrittliche Kommunikation, und Dr. Anna Scott, Chef-Architektin für den öffentlichen Sektor, über die Geschichte der fortgeschrittenen Kommunikation."},{"id":11,"type":"Episode","title":"Förderung von Operationen mit 5G","tags":["5g","comms","wifi6"],"body":"\r\n\r\n5G eröffnet viele Möglichkeiten. Sie können nun private Netzwerke einrichten, die softwaredefiniert sind, um Ihrem Netzwerk zusätzliche Funktionalität hinzuzufügen. Wohin führt uns diese ermöglichende Technologie?\n\nAnna sagt, dass obwohl wir noch in den Anfangstagen in Bezug auf Bandbreite und Latenz sind, es echte Vorteile bei 5G gibt, wie beispielsweise die Öffnung des Spektrums wie CBRS, der Möglichkeit, vorhandene Nutzergeräte zu nutzen und der Möglichkeit für Kunden, mobilen Zugang zu haben.\n\nEin Anwendungsfall, der zwar nicht sexy ist, aber weitreichende Auswirkungen hat, besteht darin, dass Sie Ihren Laptop mit auf den Fabrikboden nehmen und ihn für eine vollständige Konnektivität verwenden können. Anstatt mit Notizblock und Papier auf den Fabrikboden zu gehen und die Informationen zurück ins Büro zu übertragen, können Sie die beiden Umgebungen zusammenführen. Die 5G-Standards ermöglichen dies, es handelt sich jedoch nicht zwangsläufig um eine vollständige 5G-Bereitstellung.\n\nEine Entwicklung, die gerade stattfindet, ist die Fähigkeit, hochauflösende Videos von einer drahtlosen Kamera über 5G zu streamen und eine so niedrige Latenz zu haben, dass Echtzeitanalysen möglich sind. Derzeit können nicht viele 5G-Kameras in dieser Umgebung arbeiten, daher werden Kameras oft in der Nähe von gekoppelten Edge-Computing-Geräten fest verdrahtet, um diesen Echtzeit-Aspekt zu erhalten. Aber diese vorteilhafte Streaming-Option wird bald verfügbar sein.\n\nEin weiteres Beispiel für einen 5G-Vorteil besteht darin, anstelle von ein oder zwei AR-Headsets mit WLAN 10 bis 20 zu verwenden. Der Schlüsselpunkt ist die MEC (Mobile Edge Compute), die die Möglichkeit bietet, die Anwendungen vor Ort zu haben, anstatt zum Switch oder zum Kern des Carriers zurückkehren zu müssen und diese RAN-Zeit zu haben.\n\nDas Verständnis, wie Frequenzen eingesetzt werden, ist in Anwendungsfällen entscheidend, da einige Anbieter trotz Herausforderungen mmWave eingesetzt haben; n41- und n42-Frequenzen reagieren unterschiedlich in der realen Welt. Daher sind bei lokalen Bereitstellungen die MEC und das RF-Design äußerst wichtig.\n\nEin überzeugendes Anwendungsbeispiel für den Einsatz von 5G außerhalb der Fabrik ist die Verwendung von Drohnen bei Notfalleinsätzen. Eine frühe Vorstufe der späteren Fähigkeit, Drohnen einzusetzen und ein beschädigtes Gebiet zu bewerten, besteht darin, Drohnen zu verbinden, eine Mission für eine Überflug durchzuführen und dann all diese Daten zusammenzuführen. Obwohl wir noch nicht in der Lage sind, Live-Videos von mehreren Drohnen zu streamen und sie zusammenzufügen, sind wir nahe dran, diese Daten zu sammeln, zu kombinieren und zu analysieren, allerdings noch nicht in Echtzeit.\n\nEin weiterer Anwendungsfall besteht darin, 5G, KI, Modellierung, Simulation und Edge Computing für das Training in vielen Branchen, einschließlich des Verteidigungsministeriums, zu nutzen. Es gibt einen enormen Vorteil darin, eine realistische Trainingssimulation zu erstellen, ohne die Person in Gefahr zu bringen oder massive Geldbeträge für Live-Training ausgeben zu müssen.\n\nSo sehr 5G diese Art von Anwendungsfall ermöglicht, hängt viel davon ab, dass 5G mit einem MEC (Multi-Access Edge Computing) verbunden ist, anstatt dass 5G in die Cloud hochgeht. Die Physik kommt ins Spiel. Du benötigst eine supergeringe Latenz, daher kann die Architektur nicht vom Headset zur Cloud gehen, dann zum MEC und anschließend zur Visualisierung. Es muss vom Headset direkt zum MEC gehen, wo Echtzeitverarbeitung stattfindet. Danach kannst du Daten über die Cloud für ein Echtzeit-Erlebnis teilen.\n\nEs besteht auch die Möglichkeit, MECs zu vernetzen oder zu clustern, so dass Daten niemals zur Cloud gehen müssen. Die MECs können alle Verarbeitungs- und Analyseaufgaben am Schalter erledigen. Dadurch könnten Fortschritte wie intelligente Gebäude und Städte ermöglicht werden. Diese Art von 5G-fähiger Technologie ist der ideale Nährboden für bedeutende Veränderungen in der Branche.\n\nLeland weist darauf hin, dass die Geschichte der neuen Netzwerke verteiltes Rechnen ist. Alles ist durch drahtlose Konnektivität verbunden, aber Recheneinheiten sind über die Landschaft verteilt, wo Apps am Rand sitzen und die Anwendungsfälle ermöglichen. Wo wir hingehen, ist drahtloses Eins-zu-Eins-Berechnen.\n\nWelche Rolle spielt Intel bei 5G? Es geht weit über die Bereitstellung von Chips hinaus. Da 5G softwarebasiert ist, hat Intel das Ökosystem ermöglicht, auf Basis seines L15 zu bauen oder zu entwerfen. Beim Wechsel von 4G zu 5G hat Intel den funktionalen Block des RAN, namens FlexRan, übernommen und Firmen ermöglicht, ihre Basisband-Architekturen zu entwerfen und zu virtualisieren. Die Erstellung der FlexRan-Referenzarchitektur hat es neuen Marktteilnehmern wesentlich einfacher gemacht, dies als Ausgangspunkt zu verwenden.\n\nAuf der Hardware-Seite hat Intel viele Zyklen investiert, um sicherzustellen, dass die kommerzielle, frei verkäufliche Hardware gut funktioniert, um alle Basisstationen, RAN-Anwendungen und Server zu unterstützen. Die neuen Systeme, die entwickelt werden, wenn man sich von proprietären Systemen abwendet, müssen problemlos von derselben Art Server unterstützt werden, die in der Cloud und im Rechenzentrum funktionieren, denn nun hat man den Vorteil der Skalierbarkeit und der Kostenersparnis.\n\nDies wird die Preise senken und zu mehr Innovation in der Branche führen.\n\nSchau dir den ersten Teil dieses Interviews [hier](episode-EDT108) an.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown","Anna Scott"],"link":"/episode-EDT109-de","image":"./episodes/edt-109/de/thumbnail.png","lang":"de","summary":"Intels Darren Pulsipher, Chief Solutions Architect, Leland Brown, Principal Engineer: Technischer Direktor für fortschrittliche Kommunikation und Dr. Anna Scott, Chief Edge Architect für den öffentlichen Sektor, sprechen über die Geschichte der fortschrittlichen Kommunikation und zukünftige Anwendungsfälle mit 5G. Teil zwei von zwei."},{"id":12,"type":"Episode","title":"Fernunterricht und -lernen","tags":["remotelearning","compute","technology","people","edge","telelearning"],"body":"\r\n\r\n## Aufkommende Überlegungen\n\nLehrer, Mitarbeiter, Eltern und Schüler stehen alle vor unterschiedlichen Herausforderungen bei der plötzlichen Umstellung auf Fernunterricht. Auf der IT-Seite der Schulbezirke gibt es eine Vielzahl von aufkommenden Überlegungen. Was machen wir mit der Situation, dass Geräte von zu Hause (BYOD) in Null-Vertrauensnetzwerken verwendet werden? Wie schützen wir die Privatsphäre und verwalten die Sicherheit bei all den verschiedenen neuen Kommunikationsmethoden zwischen Lehrern, Mitarbeitern, Eltern und Schülern? Wie erhalten wir ein reibungsloses, verwaltetes Klassenzimmererlebnis aufrecht? Wie bieten wir Unterstützung an, wenn eine traditionelle Helpdesk-Struktur nicht existiert? Außerdem dürfen wir nicht vergessen, dass es einen wichtigen sozialen Aspekt gibt, der ein nahtloses Erlebnis ermöglichen muss. Wenn ein Drittklässler durch technische Probleme abgelenkt wird, wenn er sich mit Lehrern und Mitschülern verbinden muss, wird das Fernunterrichtserlebnis beeinträchtigt.\n\n## Systemdienstleistungen und Plattformen für Bildung\n\nEs gab schon immer Komplexität mit all den verschiedenen Ebenen von Diensten und Plattformen, zum Beispiel der Produktivitätssuite mit G Suite und Office 365. Die Frage ist nun, was können wir mit Learning-Management-Systemen in diesem Schichtansatz in die Integration des Studenteninformationssystems tun? Wir müssen betrachten, wie wir unsere Fähigkeiten in Bezug auf Skalierbarkeit nutzen. Wir müssen verschiedene Infrastruktur-als-Service (IAAS) und Plattform-als-Service (PAAS) Lösungen, Speicherdienste, Privatsphäre und Sicherheit und natürlich die zugrunde liegenden Plattformen, die all dies antreiben, in Betracht ziehen.\n\nFrüher hatten Konferenz- und Kollaborationstools nur begrenzte Verwendungsmöglichkeiten, vielleicht um beispielsweise einen Gastredner einzuladen. Jetzt hingegen werden sie intensiv als Hauptwerkzeuge genutzt und haben zur Komplexität des Systems beigetragen.\n\n## Service-Hosting-Optionen\n\nEs gibt zwei Hauptarten, die verwendet werden, um auf Dienste zuzugreifen: Gerät als Portal zu Diensten (Software, Infrastruktur oder Plattform) und Gerät als Teil des internen Netzwerks. Früher fielen die meisten in letztere Kategorie, wo es nur begrenzte Bedenken hinsichtlich Dingen wie Patching und Richtlinienkonformität gab, da die Geräte ständig mit internen Schulstandorten verbunden waren, unabhängig davon, ob sie BYOD-Geräte oder vom Bezirk besitzte Vermögenswerte waren. Jetzt, mit den verschiedenen Arten von Konnektivität, müssen wir uns um die Skalierbarkeit der Bandbreite und wie wir sie implementieren, Sorgen machen.\n\n## Gerät als Portal zu Dienstleistungen\n\nPlattformen wie G Suite, Office 365 und solche für Konferenz und Zusammenarbeit sind bedenklich, da sie eine Abhängigkeit von einem Drittanbieter schaffen. Schulbezirke haben keine Kontrolle über Sicherheit, Datenschutz und Leistung. Es ist wichtig zu erkennen, dass die Verbindung zu diesen Cloud-Diensten auf den internen Host zurückführt, ob es sich um eine private Cloud oder ein Vor-Ort-Rechenzentrum handelt, das zugrunde liegende Dienste wie Identitätsmanagement, Schülerinformationssysteme, mögliche Inhaltsfilterung usw. hat. Die Vorteile sind jedoch weniger eingehender Datenverkehr zum Rechenzentrum sowie inhärente Skalierbarkeit und Verwaltbarkeit.\n\n## Gerät als Teil des Netzwerks/der privaten Cloud\n\nDie Implementierung eines virtuellen privaten Netzwerks (VPN) ist für die meisten Schulbezirke eine neue Idee. Unternehmen verwenden VPNs schon seit einiger Zeit und Schulbezirke müssen möglicherweise nachziehen, um den neuen Anforderungen des Fernlernens gerecht zu werden. Ein paar Nachteile eines VPNs sind Netzwerküberlastung, Skalierbarkeit und Datenverkehr aus Netzwerken mit \"Zero Trust\". Der Hauptvorteil besteht darin, dass es als Erweiterung des internen Netzwerks fungiert. Dadurch werden Sicherheitsmanagement und Datenverkehrsverschlüsselung auf die VPN-Clients ausgeweitet. Ein weiterer großer Vorteil ist der Zugang zu allen intern benötigten Diensten.\n\n## Aufkommende Engpässe\n\nEngpässe sehen beim Fernlernen anders aus. Im Unternehmensbereich lag die erwartete VPN-Auslastung, als die Belegschaft virtuell wurde, bei etwa 10%. Im Bildungsbereich wird diese Zahl deutlich höher sein und möglicherweise Engpässe im VPN verursachen. Eine weitere zu berücksichtigende Überlegung ist die Skalierbarkeit der gehosteten Dienste. Selbst wenn die Dienste in einer öffentlichen Cloud liegen, werden sie für Dinge wie SSO, Schülerinformationen, Datenverkehr und sogar Inhaltsfilterung mit einer privaten Cloud oder einem lokalen Rechenzentrum verbunden. Die Bandbreite für den dedizierten Internetzugang sowie die Art und Weise, wie die Bandbreitenübergabe im Rechenzentrum gehandhabt wird, sind wichtige Überlegungen.\n\nDer Zugang zu einer Helpdesk-Komponente muss ebenfalls skalierbar sein, um Engpässe zu verhindern.\n\nDie wichtigste Sorge der Schulbezirke derzeit ist jedoch, allen Schülern Zugang zu ermöglichen. Für einige Schüler gibt es ein Engpass, nur um zu Hause ins Internet zu gelangen. Wenn dieses Problem zu allen unterschiedlichen Ebenen hinzugefügt wird, werden die aufkommenden Engpässe sehr komplex.\n\nUm diesen potenziellen Problemen entgegenzuwirken, müssen Schulbezirke sich die Zeit und die Ressourcen nehmen, um eine solide Architektur zu erstellen, die widerstandsfähig ist, anstatt eine überhastete Spaghetti-Konstruktion zu errichten.\n\n## Skalierbarkeit\n\nHistorisch gesehen haben Schulbezirke im Rechenzentrum skaliert und einige dieser Dienste an die Schulstandorte ausgelagert. Die Architektur besteht im Allgemeinen darin, die Zusammenführung von Lehrkräften, Mitarbeitern und Schülern für Aufgaben wie Authentifizierung, Patch-Management, Softwareverteilung usw. zu ermöglichen. Da jetzt die Skalierung in virtuellen Klassenzimmern erfolgt, kehrt man zur Skalierung der Dienste im Rechenzentrum zurück.\n\n## Die Balance finden\n\nZuvor waren nur die beiden Faktoren von \"vor Ort\" im Rechenzentrum des Hauptbüros und den LAN-Verbindungen zu den Schulstandorten beteiligt. Jetzt gibt es den zusätzlichen Faktor, alle virtuellen Klassenzimmer unterzubringen, d. h. jedes Wohnzimmer der Schülerinnen und Schüler sowie der Lehrkräfte. Wie können wir das bewerkstelligen? Es geht letztlich um das Hauptkonzept, das Gleichgewicht zwischen den Bedürfnissen Ihres Schulbezirks zu finden. Jeder Bezirk unterscheidet sich in Größe und Technikkenntnissen, und es gibt viele bewegliche Teile.\n\n## Engesstellen ansprechen.\n\nWas kann Intel tun, um bei der Bewältigung dieser Engpässe zu helfen, die wir identifiziert haben?\n\nEndpoint-Management und Endpunkt-Sicherheit sind Aufgabenbereiche, in denen der IT-Helpdesk ins Spiel kommt. Wir suchen nach Möglichkeiten, Unterstützung remote anzubieten, anstatt regelmäßige Besuche an Schulstandorten durchzuführen.\n\nWir können uns das Rechenzentrum und die Infrastruktur ansehen und eine Strategie entwickeln, die es uns ermöglicht, softwaredefiniertes Networking und Infrastruktur zu skalieren. Darüber hinaus können wir auch einige Infrastructure-as-a-Service-Cloud-Bursting integrieren, und dabei die Verkehrsprotokolle berücksichtigen.\n\n## Intel Komponenten zur Behebung von Engpässen\n\nIntel kann in drei großen Kategorien helfen: Rechenleistung, Speicher und Netzwerk. Wenn wir die Stärkung der softwaredefinierten Infrastruktur und die wichtigen Überlegungen betrachten, dreht sich alles um den Computer mit Intel-Prozessoren, Speicherprodukte und Netzwerkfähigkeiten. Anstatt an physische Schnittstellen und Geräte gebunden zu sein und dadurch begrenzt zu sein, kann die softwaredefinierte Infrastruktur skalieren und andere Rechenleistungs-, Speicher- und Netzwerkressourcen integrieren. Niemand weiß genau, wie die Situation sein wird, wenn wir zur Schule zurückkehren, und diese softwaredefinierte Infrastruktur ist dynamisch und bietet die größtmögliche Flexibilität.\n\nEs gibt viele Möglichkeiten für Schulbezirke, eine lebensfähige, sichere Umgebung für Fernlernen zu schaffen. Obwohl wir in dieser Folge die Themen Datenschutz und Sicherheitsbedrohungen sowie Lösungen angesprochen haben, werden wir nächstes Mal tief in diese wichtigen Themen eintauchen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Erin Moseley","Grant Kelly"],"link":"/episode-EDT11-de","image":"./episodes/edt-11/de/thumbnail.png","lang":"de","summary":"In dieser Folge sprechen Erin Moseley, Senior Account Executive für Bildung bei Intel, und Grant Kelly, Solution Architect für Bildung bei Intel, mit Darren über die Herausforderungen des Fernunterrichts und der Unterrichtstätigkeit sowie über die überwältigenden Veränderungen, mit denen Schulbezirke, Lehrer, Eltern und Schüler während der Covid-19-Pandemie konfrontiert sind. Erfahren Sie, wie Schüler und Lehrer mit neuen Technologien und Lernformen in Verbindung treten."},{"id":13,"type":"Episode","title":"Sicherung der Lieferkette","tags":["securesupplychain","cybersecurity","supplychain","chipsact","policy","process","compute"],"body":"\r\n\r\nIn dieser Episode spricht Darren mit Generalleutnant Thomas Horlander, der kürzlich dem Intel Public Sector Team beigetreten ist, über die Lieferkette für Mikroelektronik und nationale Sicherheit.\n\nThomas trat 1983 nach Erlangung seines Bachelor-Abschlusses in Finanzen der Armee bei, nachdem er zu dem Schluss gekommen war, dass der Privatsektor nichts für ihn war. Er schloss sich der Armee an, in der Annahme, dass es eine Karriere von drei bis fünf Jahren sein würde, und endete schließlich mit 39 Jahren Dienstzeit, wobei sein letzter Posten als Armeekontrolleur war. Vor etwa einem Jahr trat er in den Ruhestand und schloss sich Intel an.\n\nThomas ist von den großartigen Menschen, der Kultur und der bedeutungsvollen Mission bei Intel inspiriert. Er schätzt die grundlegende Rolle, die Intel im alltäglichen Leben aller spielt, sowie die Möglichkeit, die Zukunft des Landes zu beeinflussen.\n\nAls Thomas im Jahr 1983 der Armee beitrat, war die Mikroelektronik noch in den Kinderschuhen; sie hatten noch nicht einmal Computer. Als junger Offizier machte er sich Sorgen, ob er genügend D-Zellen-Batterien hatte, um Radios zu betreiben. Daher betrachtet sich Thomas als \"digitaler Immigrant\", da er die Entwicklung miterlebt hat, die mit Silizium einherging.\n\nDie Gesellschaft und das Militär sind nun von Silizium abhängig, und die Lieferkette rund um die Mikroelektronik ist entscheidend: Es handelt sich um ein nationales Sicherheitsproblem. Thomas sagt, die Mikroelektronikindustrie teilt sich mit der Ölindustrie die Hauptbühne hinsichtlich nationaler und globaler Sicherheit und wirtschaftlicher Stabilität.\n\nIn der Armee sind alle Fahrzeuge und Waffensysteme mit Mikroelektronik ausgestattet. Sie ermöglichen es der Armee zum Beispiel, präziser und tödlicher mit weniger Gewicht zu sein, ein genauereres Ortungssystem und zuverlässigere Kommunikation zu haben.\n\nUnsere Abhängigkeit von Mikroelektronik hat den kürzlichen Chips Act vorangetrieben. Obwohl das Bedürfnis schon vor der Ukraine-Krise offensichtlich war, hat das, was die Ukraine dank Mikroelektronik tun konnte, die Notwendigkeit einer Absicherung der Lieferkette deutlich herausgestellt.\n\nThomas ist seine gesamte Karriere lang ein Student der nationalen Sicherheit gewesen und betrachtet sie ganzheitlicher als nur Verteidigung und die Rolle der Streitkräfte. Für ihn bedeutet nationale Sicherheit eine gute Regierungsführung und Rechtsstaatlichkeit. Es umfasst eine gut funktionierende Wirtschaft, praktische Wissenschaft, Gesundheitsversorgung und natürlich die Streitkräfte. Nahezu alle Berufe in der amerikanischen Gesellschaft spielen eine Rolle bei der Sicherstellung der nationalen Sicherheit. Aus dieser Perspektive beruht so gut wie jeder Aspekt, der für die nationale Sicherheit erforderlich ist, auf Mikroelektronik.\n\nThomas sagt, dass der Chips Act notwendig war aufgrund des immensen Ungleichgewichts in der Ökosystem der Branche. Gleichzeitig ist der Chips Act nicht die ultimative Lösung zur Umverteilung des Gleichgewichts, doch er ist ein wesentlicher erster Schritt und wird die nationale Sicherheit beeinflussen. Wenn man die Kernaktivitäten der Mikroelektronikbranche analysiert - angefangen von der Herkunft der seltenen Erden bis hin zur Herstellung von Geräten und Design - wird deutlich, was für ein unglaubliches Mosaik an Aktivitäten dies ist und warum es so schwierig ist, ein klares Bild von allem zu bekommen. Ein Mikrochip wechselt möglicherweise zehnmal den Besitzer während des Herstellungsprozesses.\n\nCOVID hat in vielerlei Hinsicht diese komplizierte und fragile Lieferkette offengelegt, als zum Beispiel Fabriken in Malaysia, Irland oder China aufgrund eines COVID-Ausbruchs geschlossen wurden und plötzlich kein Auto verschifft werden konnte, weil es keinen Chip hatte. Die meisten Menschen realisieren das bedeutende globale Ungleichgewicht, das derzeit besteht, nicht. Nur acht Prozent des Siliziums werden in den Vereinigten Staaten hergestellt. Siebzig oder achtzig Prozent werden in Südostasien hergestellt, genau genommen in drei Ländern: China, Südkorea und Taiwan.\n\nMit diesem Wissen ist offensichtlich, dass eine Neugewichtung des globalen Lieferketten-Ökosystems und eine Rückführung von Kapazitäten und Fähigkeiten in die Vereinigten Staaten von höchster Bedeutung sind. Keine Branche sollte einzelne Schwachpunkte haben, eine Sorge in der Mikroelektronikindustrie.\n\nDie Bundesregierung, die Verteidigungsindustrie und das Ökosystem beginnen alle langsam dieses Problem zu erkennen, und der Chips Act ist repräsentativ für diese Anerkennung, dass wir etwas tun müssen. Thomas kennt derzeit sechs Unternehmen, die angekündigt haben, in den nächsten acht bis zehn Jahren in Fabriken in den USA zu investieren.\n\nEine dieser Unternehmen ist natürlich Intel. Intel baut derzeit Fertigungsstätten in Ohio, Arizona und New Mexico. Die Rückverlagerung von Kapazitäten und Fähigkeiten in der Mikroelektronikindustrie ist notwendig für die nationale Sicherheit, wird jedoch auch die lokale Wirtschaft stärken und Arbeitsmöglichkeiten für Arbeitnehmer schaffen, wodurch ganze Gemeinschaften lebendiger werden.\n\nDie Infusion von Kapitalinvestitionen aus dem Chips Act ist grundsätzlich unerlässlich, denn dies ist ein Wettlauf gegen die Zeit. Thomas ist optimistisch in Bezug auf die Zukunft der Branche und die getroffenen Maßnahmen, um eine helle Zukunft und kontinuierliche Innovation zu gewährleisten.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Thomas Horlander"],"link":"/episode-EDT110-de","image":"./episodes/edt-110/de/thumbnail.png","lang":"de","summary":"In dieser Folge spricht Darren mit Generalleutnant Thomas Horlander, der kürzlich dem Intel Public Sector Team beigetreten ist, über die Lieferkette für Mikroelektronik und nationale Sicherheit."},{"id":14,"type":"Episode","title":"Erschließung des Potenzials einer Smart City","tags":null,"body":"\r\n\r\nEric hatte eine lange Karriere im Rechenzentrum und in letzter Zeit im Bereich des digitalen Beschilderungswesens. Seine Leidenschaften liegen im Projektmanagement und in der Implementierung und dem Betrieb von Technologien. Eric und sein Team waren Teil eines Joint Venture-Unternehmens von LG Electronics, das hauptsächlich im Freien platzierte digitale Beschilderungen für Werbezwecke in dicht besiedelten urbanen Gebieten, Verkehrsknotenpunkten und Stadtbereichen einsetzte. Sie haben viel über andere Anwendungen in diesen Umgebungen gelernt, die Rechenressourcen und andere Formen der Konnektivität erfordern, um das Erlebnis durch das Internet der Dinge zu verbessern. SmartPoint.io wurde aus diesem Ökosystem heraus und aus dem Wunsch geboren, in diesem Marktumfeld Chancen und Fortschritte zu fördern.\n\nIm Rechenzentrum war Eric Teil eines Lifecycle-Lösungsunternehmens, das Unternehmen dabei unterstützte, Server-Speicher und Netzwerkhardware von einer Übernahme über einen Aktualisierungszyklus bis hin zu allem dazwischen zu optimieren. Seine Rolle lag darin, dem Vertriebskanal von Unternehmen zu helfen, die diese Lösungen verkauften, aber auch dabei, wie sie gekauft, finanziert und im Laufe der Zeit abgeschrieben wurden. Dadurch lernte er nicht nur über die Technologie, sondern auch darüber, wie die Menschen sie nutzten und den Wert als Aktivum in ihrer Bilanz verwalteten.\n\nWenn Menschen über intelligente Städte sprechen, denken sie an Widerstandsfähigkeit, Gleichberechtigung, Effizienz und kosteneffektive Lösungen. Städte bieten wesentliche Dienstleistungen wie Straßen, öffentliche Sicherheit, Wasser und Abwasser an. Das Hauptanliegen einer Kommune ist es nun, Konnektivität einzuschließen, also die Möglichkeit zum Zugriff auf Computertechnologien und die Nutzung technologischer Fortschritte, um grundlegende Dienstleistungen effizienter bereitzustellen. Intelligente Technologien waren bisher teuer und risikoreich, weshalb nur wenige kommunale Budgets dafür Platz haben.\n\nErics Hintergrund in der Finanzbranche ist ein Vorteil bei der Entwicklung und Implementierung von smarten Stadtechnologien in einer Partnerschaft, die ein erschwingliches Modell bietet. Die Fähigkeit, die aktuellen signifikanten Finanzmittel für Infrastrukturprojekte in belastbare und wirtschaftlich nachhaltige Wege einzusetzen, ist ein Hauptbestandteil dessen, was SmartPoint tut, wenn es mit einer Stadt zusammenarbeitet.\n\nUm wirtschaftliche Nachhaltigkeit zu erreichen, hat Eric's Team einen genauen Blick auf digitale Beschilderungen geworfen, hauptsächlich für Werbung, die enorme Einnahmen generiert. Einige der größten Unternehmen der Welt werden hauptsächlich durch Werbeeinnahmen finanziert, und eines der Hauptprodukte oder Ressourcen sind die Menschen, an die sie werben. Durch die Verknüpfung von Zuordnung, Kundenmerkmalen und Ähnlichkeiten kann der Umsatz gesteigert werden.\n\nStädte bergen einen wahren Schatz an Wert in den Menschen, Geschäftseinheiten und Besuchern, die sich auf ihrem Grund und Boden befinden. Daher besteht das Hauptziel von SmartPoint.io darin, diesen Grund und Boden zu nutzen, um Einnahmen zu generieren, indem es jedoch direkt auf Lösungen ausgerichtet ist, die bedeutungsvolle Ergebnisse in städtischen Dienstleistungen erzeugen. Um diese Technologie in die Städte zu bringen, ist Hilfe vom Privatsektor und die Bereitschaft des öffentlichen Sektors zur Zusammenarbeit erforderlich.\n\nWerbeerlöse ermöglichen in Verbindung mit der Nutzung der Etats städtischer Dienstleistungen ein Finanzmodell, das den Wohlstand für alle steigert – angefangen von sichererem Transport bis hin zu höheren Werbeeinnahmen. Wenn man den Nutzen der Werbung für die Personen zurückverfolgt, an die sie gerichtet ist, entsteht eine zirkuläre lokale Wirtschaft, die den Wert und die Effizienz steigert.\n\nEdge Computing ist ein wesentlicher Bestandteil des Produkts von SmartPoint.io. Die digitalen Beschilderungskästen sind auch wie ein portable Datenzentrum. Daten müssen nicht an ein großes Datenzentrum zurückgeschickt werden, sondern werden am Rand verarbeitet. Beispielsweise ist die Übertragung von Videodaten von den allgegenwärtigen Kameras in einer Stadt zur Verkehrsüberwachung bandbreitenintensiv. Wenn sie jedoch lokal zwischengespeichert, gespeichert oder verarbeitet werden, wird der Datentransport erheblich reduziert.\n\nDrei großartige Eigenschaften dieser Boxen sind, dass sie eine hervorragende Latenzreduzierung, Kostenersparnis und erhöhte Privatsphäre bieten. Das Thema Privatsphäre mag kontraintuitiv erscheinen, aber die Videodaten beispielsweise werden nicht über öffentliche IP übertragen. Die Analyse erfolgt am Erfassungspunkt, sodass sie nicht über das Netzwerk transportiert, beobachtet oder über mehrere Hops weitergeleitet wird, wo die Sicherheit verfolgt werden muss. Anstatt des Bildes sendet es das Ereignis \"Eine Person geht die Straße entlang\". Diese Daten können bereits früher im Analysezyklus vor dem Verlassen der Box bereinigt und anonymisiert werden, um die Privatsphäre der Menschen zu schützen.\n\nDie Boxen könnten auch Dienstleistungen wie Wi-Fi oder 5G-Hotspots in unterversorgten Gebieten zur Verfügung stellen.\n\nSobald die Boxen bereitgestellt sind, hat SmartPoint.io ein Team, das die Technologie überwacht und betreibt, ähnlich wie ein Rechenzentrum für ein Unternehmen. Sie arbeiten auch mit Werbetreibenden zusammen. Viele Werbetreibende interessieren sich dafür, von statischer \"Papierplakat\" -Werbung zu digitaler Werbung zu wechseln. SmartPoint.io ist ein großartiger Partner für diese Werbetreibenden, weil sie eine anfängliche Kapitalausgabenfinanzierung mitbringen.\n\nMit den Städten ist das Modell Umsatzbeteiligung oder Sachleistung. Statt nur den Umsatz zu teilen, produziert und betreibt SmarPoint.io jedoch Ressourcen wie IT-Infrastrukturen, die die Stadt dann nutzen kann, um optische Daten am Rand zu verarbeiten.\n\nDigitale Beschilderung kann interaktiv sein, was Personen, die nach Dienstleistungen, Restaurants, Einkaufsmöglichkeiten, Veranstaltungen und Unterhaltung suchen, zugutekommt. Gewerbemieter können es als Portal für neue wirtschaftliche Entwicklung nutzen, da sie die Menschen erreichen können, die vorbeigehen. Im Gegenzug liefern die Systeme wertvolle Informationen, wie viele Menschen sich in der Innenstadt befanden oder ob eine Werbekampagne sie erfolgreich dorthin gebracht hat.\n\nDieses Modell wird smarten Städten helfen, da es die Kosten für Anschaffung (CapEx) und Betrieb (OpEx) der Verwaltung von Hardware in der Wildnis und der Aktualisierungszyklen verschiebt. Das Eingehen von Risiken bei teurer, relativ neuer Technologie ist auch kein normales Verhalten für eine Gemeinde. Daher hilft die Zusammenarbeit mit dem privaten Sektor, der eine hohe Risikobereitschaft, Fachwissen und Finanzierungsmöglichkeiten mitbringt, den Städten, Risiken und Kosten zu mindern und weiter voranzukommen, als sie es normalerweise wären.\n\nAm Ende sagt Eric, dass Erfolge in diesem Bereich langsam entstehen und sich gegenseitig verstärken. Jeder lernt neue Dinge und es ist eine hervorragende Gelegenheit, zusammenzukommen und eine Gemeinschaft aufzubauen, denn darum geht es in Städten.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Eric Hornsby"],"link":"/episode-EDT111-de","image":"./episodes/edt-111/de/thumbnail.jpg","lang":"de","summary":"In dieser Folge spricht Darren mit Eric Hornsby, CEO von SmartPoint.io, über Technologie zur Realisierung des Potenzials einer intelligenten Stadt."},{"id":15,"type":"Episode","title":"Mythen der Lift-and-Shift-Cloud-Migration","tags":["multicloud","cloudmigration","cloud","compute","process","technology"],"body":"\r\n\r\nJohn begann vor 20 Jahren seine Karriere in der Technologieabteilung des Helpdesks eines führenden Verteidigungsunternehmens. Im Laufe seiner Karriere spezialisierte er sich auf Cybersicherheit und Unternehmensarchitektur. Er arbeitete als Vertragspartner für die Defense Information Systems Agency (DISA), wo er als Hauptarchitekt für die DISA-Cloud des Verteidigungsministeriums tätig war. Schließlich wurde er vom Bundesstaat Maryland engagiert, um die Bemühungen zur digitalen Transformation des Staates zu leiten, einschließlich Cloud-Migration. Diese Migration war die bislang größte, die im Bereich des staatlichen und kommunalen Bildungswesens durchgeführt wurde.\n\nDanach wechselte er in die Position des Chief Information Security Officers (CISO) des Staates, wo er oberste Tätigkeiten, Sicherheit und Regierungsaufsicht überwachte. Er verließ den Regierungsdienst und trat vor etwa drei Jahren WWT bei. Hauptsächlich arbeitet er im Bereich Bildung auf staatlicher und lokaler Ebene, obwohl er auch in anderen Bereichen des öffentlichen Sektors hilft.\n\n## Mythos Eins - Die Cloud ist billiger.\n\nDie Cloud ist nicht zwangsläufig günstiger als eine lokal betriebene Umgebung. Alte Anwendungen wurden für die lokale Umgebung entwickelt, daher gibt es keine Probleme mit der automatischen Skalierung. Es handelt sich um ein verbrauchsabhängiges Modell und es fallen bereits angesunkene Kosten an, wie zum Beispiel die Server. Die meisten Produkte, die Organisationen bei der Umstellung von Anwendungen in die Cloud helfen sollen, unterstützen keine automatische Skalierung. Wenn sie also zur Nutzung in der Cloud gezwungen werden, müssen sie immer mit maximaler Kapazität bereitgestellt werden, was häufig nicht zu Kosteneinsparungen führt. Kunden zahlen wahrscheinlich mehr als zuvor für die gleichen Möglichkeiten in einer lokal betriebenen Umgebung.\n\nAls Darren mit der kanadischen Regierung arbeitete, haben sie eine SAP-Instanz in die Cloud verschoben. Diese war maximal bereitgestellt und lief rund um die Uhr, daher haben sie ihr Budget innerhalb von nur sechs Monaten überschritten. Als sie dieses Problem entdeckten, haben sie die Instanz jeden Tag ein- und ausgeschaltet, da sie nicht rund um die Uhr laufen musste. Dadurch konnten sie viel Geld sparen, indem sie sie auf 14-16 Stunden pro Tag reduzierten.\n\nDer Zugang muss jederzeit in einer Abteilung wie dem Gesundheitsministerium oder dem Ministerium für Soziales und Menschenrechte gewährleistet sein, da dies in der Regel der größte Posten im IT-Budgetstaat ist. Es ist jedoch möglich, ihn während der Nebenzeiten auf ein Mindestmaß zu reduzieren, um Kosten zu sparen.\n\nDie Cloud-Service-Anbieter (CSP) bieten jetzt cloud-native Dienste an, die von einer Anwendung eines Drittanbieters oder einem OEM-Produkt erfüllt werden können, das eine ähnliche Funktionalität mit Kosteneinsparungen bietet. Das funktioniert jedoch nicht bei allem, da einige ältere Anwendungen nicht für den Einsatz der cloud-native Anwendungen entwickelt wurden. Es wäre am besten, vorsichtig zu sein, sich an eine spezifische Cloud zu binden. Wenn Sie zum Beispiel proprietäre AWS-Dienste nutzen, könnte es herausfordernd sein, eine Anwendung zu lösen und zu Azure zu wechseln oder umgekehrt.\n\nJohn rät Organisationen, sich zu fragen, ob es sinnvoll ist, in die Cloud zu wechseln. Es ist kein guter Grund, in die Cloud umzuziehen, nur weil man denkt, dass man es tun sollte. Sie könnten am Ende höhere Kosten und frustrierte Vorgesetzte und Mitarbeiter haben, weil die Strategie nicht gründlich durchdacht wurde.\n\n## Mythos Zwei - Die Cloud löscht technische Schulden.\n\nDer Umzug in die Cloud beseitigt in den meisten Fällen die technische Schuldenlast nicht. Stattdessen macht er die Schulden sichtbar und beschleunigt sie. Wenn man etwas, das seit 30 Jahren läuft, in eine neue Umgebung verschiebt, findet man Anfälligkeiten. Der beschleunigende Teil besteht darin, dass man sich jetzt um noch mehr technische Schulden sorgen muss, da sie sichtbar geworden sind.\n\nTechnische Schulden bedeuten, dass Sie Systeme haben, die nicht mehr auf dem neuesten Stand sind. Zum Beispiel liefen einige Systeme noch auf grünen Bildschirmen, als John zum ersten Mal bei Maryland anfing zu arbeiten. Es war für aktuelle Mitarbeiter einfach zu bedienen, aber für neue Benutzer gab es eine steile Lernkurve. Obwohl das System seit vielen Jahren gut funktioniert hatte, besteht der Nachteil dieser technischen Schulden darin, dass eine umfangreiche Schulung erforderlich ist und Mitarbeiter gehalten werden müssen. Neuere Generationen, die in die Arbeitswelt eintreten und über Kenntnisse über die neuesten Trends und Entwicklungen verfügen, möchten sich nicht mit älteren Anwendungen befassen.\n\nTechnische Verschuldung bedeutet auch Sicherheitsprobleme. Wenn eine veraltete Anwendung nicht aktualisiert wurde, kann es sein, dass Sie aus Angst vor einer Beschädigung keine Patches anwenden können. Dadurch entstehen Sicherheitslücken, die Sie akzeptieren müssen, bis Sie aus dem Zyklus der technischen Verschuldung herauskommen.\n\nEine häufige Reaktion in einer Organisation besteht darin, bei Bedarf einen zusätzlichen Code anzufügen, um beispielsweise eine Regeländerung einer Landesgesetzgebung zu ermöglichen. Dies löst jedoch kein Problem; stattdessen führt das System zu einer Menge unübersichtlichen Codes, was eine Aktualisierung des Systems unmöglich macht. Eines der Konzepte in der Cloud besteht darin, das System in Module oder Mikrodienste aufzuteilen. Der unübersichtliche Code erlaubt dies jedoch nicht, da man nicht einfach einen Teil davon herausnehmen kann.\n\nDies führt dazu, dass Organisationen noch zögerlicher sind, sich zu modernisieren, denn sie haben jahrelang alles auf die falsche Art und Weise gemacht. Wenn dann etwas kaputtgeht und den Anstoß für diese Veränderung liefert, stellt dies eine noch größere Herausforderung dar.\n\nManchmal kann es sinnvoll sein, das alte System zu verwerfen und ganz von vorne anzufangen. Dies ist jedoch kostspielig und man muss über das neue System verfügen, bevor man das alte abschafft. Für einige Organisationen, wie zum Beispiel Staaten, die Bundesmittel für Abteilungen wie Gesundheit und Soziale Dienste erhalten können, könnte dies die beste Wahl sein. In einer solchen Situation kann man auch den Nutzen der Wiederverwendbarkeit, wie Vorlagen und Verwaltungsstrukturen für andere Abteilungen, herausfinden.\n\n## Mythos Drei - Die Cloud ist sicher.\n\nDer Umzug in die Cloud macht es nicht unbedingt einfacher, Anwendungen zu sichern, obwohl Sie sich beispielsweise keine Sorgen um physische Sicherheit oder Hypervisoren machen müssen. Cloud-Anbieter haben ein gemeinsames Verantwortungsmodell in verschiedenen Formen. Sie müssen verstehen, wofür Sie bei jedem Anbieter verantwortlich sind und wofür sie verantwortlich sind. Dies ändert sich nicht nur bei dem Anbieter, sondern auch bei den von Ihnen genutzten Diensten. Dies kann für Ihre Sicherheitsteams die Sicherheit komplexer machen, da sie auf dem neuesten Stand aller verschiedenen Variationen bei Anbietern und Diensten bleiben müssen.\n\n## Mythos Vier - Die Cloud ist einfach\n\nDie Migration in die Cloud ist komplex. Tatsächlich ist es einfacher, alles in Ihrem eigenen Rechenzentrum ohne Internetverbindung zu betreiben. Die Sicherheit ist auf diese Weise einfach, und die Kostenmodelle sind einfach. Allerdings kann man nicht expandieren. Man kann keine Dienstleistungen für seine Kunden oder Partner bereitstellen, und man kann die Zielvorgaben nicht erfüllen, unter anderem aus anderen Gründen. Die Welt ist komplex, und die Migration in die Cloud ist kompliziert.\n\n## Mythos Fünf - Ein neues Fähigkeitsspektrum ist nicht notwendig.\n\nEine Wissenslücke kann auch zu den Schmerzpunkten rund um diese Komplexität beitragen. Softwareentwickler und andere IT-Spezialisten müssen ihr Denken über das Cloud-Computing ändern, insbesondere im Hinblick auf die Sicherheit. Zum Beispiel sollten Softwareentwickler keine Instanzen in der Cloud erstellen, wo immer sie möchten, oder Dinge von GitHub oder anderen Repositories herunterladen, um Bibliotheken zu nutzen, um Dinge zum Laufen zu bringen. Dadurch werden alle Firewall-Regeln geöffnet, da sie möglicherweise nicht korrekt ausgewählt werden. Richtlinien müssen implementiert werden, wenn man in die Cloud wechselt, was Veränderungen erfordert. Das Arbeiten in der Cloud erfordert eine andere Skill-Set und Denkweise. Am wichtigsten ist es, bessere Möglichkeiten zur Verwaltung der Sicherheit vor anspruchsvoller Ransomware und Cyberangriffen herauszufinden.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","John Evans"],"link":"/episode-EDT112-de","image":"./episodes/edt-112/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel, und John Evans, Chief Technology Advisor, WWT, diskutieren fünf Mythen zur Cloud-Migration durch Lift and Shift."},{"id":16,"type":"Episode","title":"Operationalisierung des Geschäftsprozessmanagements","tags":["bpm","automation","compute","management","camunda","capitalbpm","rpa"],"body":"\r\n\r\nMax nennt sich selbst einen \"gescheiterten Akademiker\", weil er sein Promotionsstudium der Mathematik mit Spezialisierung auf Topologie abgebrochen hat, um Informatik zu studieren. Er hat einen Bachelor- und einen Masterabschluss erworben, wobei er sich auf Künstliche Intelligenz spezialisiert hat. Anschließend arbeitete er im Bereich des Geschäftsprozessmanagements (BPM), zunächst bei Lombardi, das später an IBM verkauft wurde, und bei einigen anderen Anbietern im BPM-Bereich. Vor zehn Jahren wagte er mit Freunden den Schritt und gründete Capital BPM.\n\nMax hat sich aus zwei Gründen dazu entschieden, sich auf BPM zu konzentrieren. Erstens mochte er schon immer Algorithmen, weil sie einen systematischen Ansatz zur Lösung komplexer Probleme bieten. Algorithmen geben ihm ein Gefühl von Sicherheit in verrückten Situationen.\n\nZweitens, ihm gefallen das, was man in der Mathematik als Transformationen bezeichnet. Zum Beispiel, wenn man eine hässliche Form mit vielen Ecken hat, die schwer zu messen ist, könnte man sie in ein Rechteck umwandeln und dann alle Theorien rund um die Rechtecksmessung anwenden, es messen und die Antwort wieder in die ursprüngliche Form übersetzen. Auf die gleiche Weise kann man in BPM ein Problem in einen Bereich bringen, wo es leicht gelöst werden kann. Anstatt ein Problem zu einem Mikroproblem mit komplizierten verschachtelten F-Anweisungen zu machen, die schwer zu warten sind, kann man es in ein abgeflachtes Problem umwandeln, das man in Stufen erkennen und angehen kann.\n\nDieser Prozess ähnelt dem Hineinzoomen auf Google Maps, um zu sehen, wo eine konzentrierte Fokussierung erforderlich ist, und dann wieder herauszuzoomen, um zu sehen, wie dies in das Gesamtbild passt. Das Herz der Unternehmensarchitektur besteht darin, hinein- und herauszuzoomen, um sicherzustellen, dass die Linie, die Sie zeichnen, immer noch gültig ist.\n\nMax vergleicht seine Vorliebe für BPM in der Informatik-Welt mit Kampfkünsten, die er seit seinem sechsten Lebensjahr studiert. Er sagt, es gibt praktische Kampfkünste wie Judo und Muay Thai, und esoterischere wie Tai Chi. Er mag pragmatische Kampfkünste, weil sie reale Probleme lösen können. Er gerät nicht mehr in Schlägereien, aber zum Beispiel helfen ihm seine Judo-Fähigkeiten, wenn er ausrutscht und hinfallen sollte. BPM ist pragmatisch, da es der Schlüssel zur Lösung von Geschäftsproblemen ist. Er glaubt, dass es einen realen Wert darin gibt, all die Theorien, die er in der Schule gelernt hat, anzuwenden und sie dienend einzusetzen, um eine Geschäftsplattform zu schaffen, die den Menschen ermöglicht, tägliche Probleme effizienter und konsistenter zu lösen, und dadurch den Menschen und der Gemeinschaft mehr Möglichkeiten zu geben.\n\nDies ist ein Bereich der Informatik, der sich darauf bezieht, wie Menschen arbeiten können. Dinge können automatisiert werden, um die Anzahl wiederholbarer und eintöniger Aufgaben zu reduzieren, sodass sie sich auf wichtigere Dinge konzentrieren können. Menschen befürchten, dass Automatisierung alltägliche Arbeitsplätze verschlingen wird, aber stattdessen beseitigt sie Mühsamkeit und schafft Zeit für wichtigere Arbeiten. Sie kann auch Arbeitsplätze schaffen. Die gleichen Ängste existierten, als Ford-Computer automatisiert wurden, aber letztendlich wurden neue Industrien geschaffen. BPM sollte vollständig angenommen werden, anstatt davor Angst zu haben.\n\nDer beste Weg, Prozesse zu operationalisieren, besteht darin, die wissenschaftliche Methode zur Artikulation des Problems zu verwenden. Im Business Process Management zeichnet man Bilder mithilfe eines Business Process Modelers. Max mag den Camunda Business Process Modeler, der kostenlos heruntergeladen werden kann und nur wenig Notation erfordert. Mit dem Modellierungsprogramm zeichnet man Schritte, um die verschiedenen Systeme und ihre Funktionsweise zu verdeutlichen.\n\nIn einem Bewerbungsprozess beginnt man zum Beispiel mit einem Pool, der interessierte Parteien definiert, wie den Kandidaten, den IT-Manager und die Personalabteilung. Innerhalb des Pools gibt es \"Schwimmbahnen\", die jeweils einen Spieler enthalten, der Aktionen durchführen kann. Man kann sich diese als LDAP-Gruppen vorstellen. Dann beginnt man, den Geschäftsprozess zu gestalten: Zuerst bewirbt sich ein Kandidat für die Stelle, dann könnte die Personalabteilung eine Bewertung vornehmen und anschließend prüft ein IT-Manager den Kandidaten. Genehmigte Entscheidungen werden auf dem Weg notiert. Nachdem die wichtigsten Schritte auf oberster Ebene im Modell festgelegt sind, kann man weitere Details zu differenzierteren Prozessen erarbeiten, wie zum Beispiel ein geteiltes Vorstellungsgespräch, eins für technische Fragen und eins für das Management.\n\nÜbersetze das Folgende ins Deutsche: ![bmp Bild](./bpm.png)\n\nDas Modellierungsprogramm generiert im Hintergrund XML, während Sie diese Diagramme zeichnen. Diese XML-Datei ist zur Laufzeit von BPM-Maschinen interpretierbar; während Sie das Diagramm zeichnen, kann es zu einem ausführbaren Prozess werden.\n\nDas menschliche Element ist immer noch in dieser Schleife vorhanden, aber die Verwendung eines BPM-Systems wie diesem macht deutlich, wo Prozesse automatisiert werden können, z. B. bei der Überprüfung des Arbeitsverlaufs oder einer kriminellen Hintergrundüberprüfung. Das Modell ermöglicht auch Flexibilität und Experimente. Zum Beispiel, nehmen wir an, der Fachexperte sagt, dass er den Arbeitscheck und den kriminellen Registercheck nicht gleichzeitig durchführen möchte, da letzterer teuer ist und ersterer günstig. In diesem Fall ist es einfach, den Arbeitscheck zuerst auszuführen und eine Entscheidung vor dem kriminellen Registercheck zu verlangen. Mit den vorgenommenen Änderungen baut man Konsens und eine wahre Geschichte auf, die sich mit jedem Experiment weiterentwickelt und wahrer wird.\n\nWährend das Werkzeug wie ein Zeichenwerkzeug aussieht, handelt es sich tatsächlich um ein Modellierungswerkzeug, das es Ihnen ermöglicht, Bilder zu zeichnen und diese im Hintergrund zu simulieren. Auf diese Weise können Sie diesen Vorgang ausführen und alle verschiedenen Entscheidungspunkte sowie deren Auswirkungen sehen. Das Modell wird Ihnen außerdem mitteilen, dass Sie nicht bereitstellen können, wenn Sie etwas nicht korrekt erledigt haben.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Max Young"],"link":"/episode-EDT113-de","image":"./episodes/edt-113/de/thumbnail.png","lang":"de","summary":"In dieser Episode spricht Darren über Business-Process-Management und Automatisierung mit Max Young, CEO von Capital BPM."},{"id":17,"type":"Episode","title":"Automatisierung des Geschäftsprozessmanagements","tags":["compute","businessprocessmanagement","capitalbpm","bpm","automation","camunda","rpa"],"body":"\r\n\r\nBMP-Modellierung erinnert Darren daran, als er in der Highschool technisches Zeichnen hatte, und die Einführung von AutoCAD und computergestützten Zeichnungssystemen hat das Spiel verändert. Vorher mussten sie seitenweise komplexe Systeme und Diagramme erstellen, damit Menschen etwas bauen konnten, aber sie konnten das Modell nicht testen, um sicherzustellen, dass es korrekt war. Durch die Verwendung von Computermodellierung konnten sie Simulationen durchführen, um sicherzustellen, dass es keine Probleme wie elektrische und sanitäre Anlagen, die durch dasselbe Loch verlaufen, gab.\n\nDies ist analog dazu, dass Architekten PowerPoint verwenden, um Geschäftsprozesse darzustellen, anstatt ein Modellierungswerkzeug zu nutzen, das Konflikte und Probleme in dem, was Sie für den Geschäftsprozess halten, aufdecken kann.\n\nMit Hilfe eines Geschäftsmodellierungstools kann auch ein praktisches Problem gelöst werden, indem die verschwendete Zeit eliminiert wird, die für Treffen mit allen interessierten Parteien erforderlich sein kann. Stattdessen können Sie ein implementiertes Modell in die Hände des Geschäftskunden geben und gemeinsam mit ihnen die Schritte durchgehen.\n\nNachdem Sie die Prozesse artikuliert und modelliert haben, können Sie Integrationspunkte auswählen, die an Ruhestandschnittstellen angehängt werden können, um Informationen abzurufen und zurückzusenden. Auf diese Weise können Geschäftsprozesse mit Microservices in der Cloud integriert werden. Im Beispiel des Einstellungsprozesses könnten diese Punkte dort sein, wo Sie eine API benötigen, um Überprüfungen des Beschäftigungsverlaufs oder des Vorstrafenregisters anzufordern. Die Eingaben werden Dinge wie Sozialversicherungsnummern und Geburtsdaten sein und die Ausgaben werden ein Boolescher Wert sein - stimmen die Informationen überein oder nicht? Hier können Sie anfangen, dieses iterative Gespräch zu führen.\n\nÜbersetze das Folgende ins Deutsche: ![bpm Bild](./bpm.png)\n\nEs gibt ziemlich viele manuelle Schritte in diesem Prozess, und Sie können auswählen, welche automatisiert werden sollen. Zum Beispiel, wenn Sie entscheiden, dass ein Interview nicht gut gelaufen ist, können Sie sich auf die Personalabteilung verlassen. Nachdem Sie diesen neuen Prozess implementiert haben, können Sie zur vorherigen Version zurückkehren, wenn Sie möchten. Dadurch haben Sie zwei gleichzeitig arbeitende und implementierte Versionen der Software im Prototyp.\n\nDer Camunda Modeler ist ein nativer Modeler, aber Capital BPM hat eigene Anwendungen entwickelt, die helfen, einen Teil der Arbeit zu optimieren und verschiedene Benutzerrollen zu unterstützen.\n\nDieses System unterscheidet sich von RPA, weil anstatt das, was ein Benutzer mit Tastenanschlägen tut, aufzuzeichnen, ein Business-Analyst die Prozesse und Schritte über mehrere Abteilungen hinweg betrachtet. Der Analyst betrachtet den gesamten Prozess von oben nach unten. Ein RPA kann in bestimmte Schritte zur Effizienzsteigerung integriert werden. Ein einfaches Beispiel ist, wenn ein Stellenbewerber die Überprüfung des Arbeitsverlaufs und des Vorstrafenregisters besteht, kann er an eine leitende Person im Personalwesen übergeben werden; falls nicht, wird er abgelehnt. Die Auswahl bestimmter Schritte oder Sets von Schritten zur Automatisierung ist ein iterativer Ansatz, der bereits seit einiger Zeit in der Softwareentwicklung erfolgreich eingesetzt wird.\n\nRPAs können bahnbrechend sein, aber sie sind taktisch und kurzfristig. Obwohl diese kurzfristigen Gewinne profitabel sein können, müssen Sie den gesamten Geschäftsprozess betrachten, um Optimierungsmöglichkeiten zu finden und Schritte zu eliminieren. Die Geschichte von der Frau, die immer das Fleisch vor dem Braten geschnitten hat, weil es ihre Mutter so gemacht hat, ist analog zu einigen Unternehmensprozessen. Die Frau fragte schließlich ihre Mutter, warum sie das Fleisch geschnitten habe, und ihre Mutter antwortete: \"Damit es in meine Pfanne passt.\" Viele Unternehmensprozesse existieren nur, weil sie schon immer so gemacht wurden und niemand darüber nachgedacht hat, warum.\n\nTesten, Simulation, das Verschieben von Dingen und das wiederholte Ausführen von Prozessen im Modellierer, mit anderen Worten, das empirische Testen, können dazu beitragen, diesen Prozessballast zu beseitigen und einen erheblichen Mehrwert zu schaffen. Visualisierung und Experimentieren sind wesentliche Bestandteile des gesamten Prozesses.\n\nMax weist darauf hin, dass es eine Übereinstimmung zwischen dem Diagramm und der tatsächlichen Ausführung gibt. Entwickler zeichnen oft Diagramme als Ausgangspunkt. Dennoch verschwinden die Diagramme, wenn die Entwicklung durch die verschiedenen Parteien voranschreitet, daher unterscheiden sich die Vorstellungen des Unternehmens von dem tatsächlichen Geschehen. Das Diagramm und die Realität sind getrennt voneinander. In dieser Art der Modellierung stellt das Bild immer eine genaue Darstellung des Geschehens dar. Darüber hinaus ist es einfach zu erkennen und Änderungen zur Verbesserung vorzunehmen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Max Young"],"link":"/episode-EDT114-de","image":"./episodes/edt-114/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel, und Max Young, CEO von Capital BPM, diskutieren die Operationalisierung des Geschäftsprozessmanagements mit Modellierungsprogrammen."},{"id":18,"type":"Episode","title":"Blockieren und tackeln der Sicherheit","tags":["zta","zerotrustarchitecture","cyberhygiene","cybersecurity","technology","process"],"body":"\r\n\r\nGrundlegend für alle anderen Cyber-Sicherheitsmaßnahmen ist eine grundlegende Cyber-Hygiene. Viele Unternehmen müssen diese Grundlagen beachten. Dies wird durch aktuelle Nachrichtenschlagzeilen belegt, die einen Anstieg von Angriffen wie Distributed-Denial-of-Service-Angriffen zeigen, die leicht zu verhindern sein sollten.\n\nAus seiner Erfahrung in der Arbeit mit der staatlichen Gemeinde glaubt John, dass die meisten Angriffe einer typischen Angriffskette folgen. Die meisten Angriffe auf staatliche und lokale Regierungen resultieren aus freiliegenden Netzwerkprotokollen oder E-Mail-Phishing. Diese stellen attraktive Eintrittspunkte für Hacker dar, und sobald sie drin sind, sind schlechte Patching-Praktiken oft der Grund, warum sie Fuß fassen und sich seitlich bewegen können. Das zusammen mit schwachen Passwortrichtlinien oder schwacher Durchsetzung von Passwortrichtlinien und Unfähigkeit zur Wiederherstellung kann zu einer Katastrophe führen.\n\nIn einem 2019 viel beachteten Ransomware-Zwischenfall nahm die betroffene Organisation an, dass sie sicher sei, da sie dieselbe Datenmenge in ihrer Produktions- und Backup-Umgebung hatte. Aber sie hatten ihre Backups oder Wiederherstellungsfähigkeiten nie getestet, was sich als schlecht erwies. Eine grundlegende Cybersicherheit hätte diesen Vorfall verhindern können.\n\nEs gibt vier wesentliche Grundlagen, auf die jede Organisation achten sollte. Erstens müssen sie die Menschen wiederholt schulen, um Phishing-Betrug zu vermeiden. Das Training mag wiederholt oder banal erscheinen, aber dass Menschen auf diese Betrugsversuche hereinfallen, ist eine erhebliche Schwachstelle in einer Organisation. Hoffentlich werden Passwörter in naher Zukunft nicht mehr nötig sein.\n\nZweitens müssen sie Firewalls entsprechend konfigurieren; nur weil RTP- oder Netzwerkprotokoll-Ports geschlossen sind, bedeutet das nicht, dass es keinen offenen Port an einer weniger prominenten Stelle gibt. Sicherheit durch Obskurität funktioniert nicht.\n\nDrittens müssen sie schlechte Patching-Richtlinien vermeiden, sowohl beim Kunden als auch beim Server, in den Rechenzentren und auch am Rand. Viele Organisationen sind in technischer Verschuldung und können ihre alten Systeme nicht aktualisieren. Daher akzeptieren sie die Schwachstellen und Risiken, weil sie keine Investitionen in ein Update tätigen möchten.\n\nViertens müssen sie die Fähigkeit haben, sich zu erholen. Nur weil Sie wissen, dass Sie Ihre Daten sichern können, können Sie mit der Sicherung auch wirklich verwenden und wiederherstellen? Testen ist unerlässlich.\n\nDiese vier Grundlagen, zusammen mit einigen anderen, reichen aus, um nahezu alle Angriffe auf Organisationen abzuwehren, die keine regelmäßigen Ziele sind. Dieses Modell gilt nicht für Organisationen, die nationale Angriffe erleiden; sie setzen bereits all diese Maßnahmen um und benötigen zusätzliche Sicherheitsmaßnahmen.\n\nEine Überlegung für viele Organisationen ist die Konformität im Verhältnis zum Risiko. Einige Organisationen müssen alte Maschinen, Anwendungen und Prozesse aufrüsten, um konform zu sein, was mit erheblichen Kosten verbunden ist. Für Organisationen mit einem nicht aktualisierbaren System wäre ein risikobasierter Ansatz möglich, bei dem im Falle eines Systemfehlers die Kosten für die Aktualisierung des Systems niedriger wären als die Kosten des Risikos. Natürlich wäre es sicherer, auf Konformität umzusteigen, aber die Mehrheit der Menschen betrachtet den risikobasierten Ansatz als sicherer. Kleinere Unternehmen könnten mit diesem Ansatz durchkommen, aber Regierungsorganisationen haben beispielsweise aufsichtsrechtliche Anforderungen.\n\nEs gibt zwei Gründe, warum eine Organisation sich für die Einhaltung von Vorschriften entscheiden könnte, außer aufgrund einer Vorgabe. Erstens ist es für viele Organisationen, die nicht verstehen, wie man Risiken ermittelt oder priorisiert, eine einfache Lösung. Die Einhaltung von Vorschriften bietet einen allgemeinen Rahmen, auf den man zurückgreifen kann. Es ist jedoch keine Lösung für alle, da jemand anderes Risiken auf eine allgemeine Weise priorisiert. Zweitens, wenn etwas Schreckliches passiert und Sie es zum Beispiel Ihrem Vorstand erklären müssen, können Sie sagen, dass Sie anerkannte Standards befolgt haben.\n\nCompliance ist eine Art Stützdenken, da man nicht alle Risikobewertungen durchführen und herausfinden muss, was getan werden muss. Aber zum Beispiel könnte eine kleine Gemeinde ohne CISO einen Systemadministrator anweisen, einen Compliance-Framework als guten Ausgangspunkt zu verwenden. Wenn kein CISO zur Verfügung steht, gibt es auch die Möglichkeit eines teilzeitlichen virtuellen CISO für Anleitung. John tut dies für Kunden, was ein rentabler Weg zu einer besseren Sicherheit ist.\n\nDas Konzept des Null-Vertrauens betrachtet auch ein Maß an Sicherheit im Vergleich zum Risiko. Sie müssen das Risiko verstehen, jemandem Zugriff auf ein bestimmtes System oder Daten zu gewähren, und dann die entsprechende Gewissheit haben, dass die Person wirklich diejenige ist, die sie vorgibt zu sein. Im Zentrum des Null-Vertrauens steht ein hohes Maß an Sicherheit, das das Risiko abmildert.\n\nZero Trust bedeutet nicht, dass alles abgesperrt und alle Prozesse langsamer werden. Wenn zum Beispiel jemand die Niederschlagsmengen einsehen möchte, ist keine hohe Sicherheitsüberprüfung der Person erforderlich. Wenn jedoch jemand auf die \"Kronjuwelen\" Ihrer Organisation zugreifen möchte, müssen zusätzliche Kontrollen zur Identitätsüberprüfung vorhanden sein.\n\nDie Abstimmung des Sicherheitsniveaus mit dem Risikograd ist herausfordernd; es erfordert eine Entscheidungspunkt-Architektur. Im Beispiel des Risikos beim Zugriff auf eine Datenquelle muss eine Organisation wissen, um was für Daten es sich handelt und sie entsprechend dem Risiko kategorisieren. Für eine reife Organisation kann dies schwierig sein. John weiß von einer Bundesbehörde, die über zwei Jahre damit verbracht hat, sicherzustellen, dass ihre Daten korrekt identifiziert, klassifiziert und markiert wurden, bevor sie mit einer Entscheidungspunkt-Architektur weitergemacht haben.\n\nIdentität und Daten sind die zwei Ausgangspunkte für Zero Trust. Außerdem ist es sinnvoll, zu vermeiden, alles auf einmal zu versuchen und umzusetzen. Es könnte am besten sein, mit einem Teil einer Organisation zu beginnen und es im Laufe der Zeit auf die restliche Organisation auszuweiten.\n\nDigitale Identität wird immer ausgeklügelter. John glaubt, dass unsere Transaktionen in Zukunft hauptsächlich auf einem Ansatz des sogenannten \"Zero Trust\" basieren werden. Zum Beispiel, wenn er $10.000 von seiner Bank auf ein Offshore-Konto überweisen möchte, sollte die Bank sicherstellen, dass es sich tatsächlich um ihn handelt und diese Transaktion behandeln, als würde jemand versuchen, auf sehr sensible, hochriskante Informationen zuzugreifen. Wenn er jedoch in den Laden geht, um eine Tasse Kaffee für einen Dollar zu kaufen, ist diese Art von Sicherheit nicht notwendig. Viele der \"Zero Trust\" Prinzipien werden sich in unseren Alltag integrieren.\n\nBenutzerverhaltensanalyse wird ebenfalls ins Spiel kommen. Genauso wie ein Kreditkartenunternehmen bei ungewöhnlichen Einkäufen Alarm schlägt, zum Beispiel, wenn ein System weiß, dass John 20 Wörter pro Minute tippt und plötzlich 100 Wörter pro Minute tippt und versucht, auf sensible Informationen zuzugreifen, gibt es eine Warnung.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","John Evans"],"link":"/episode-EDT115-de","image":"./episodes/edt-115/de/thumbnail.png","lang":"de","summary":"In dieser Folge spricht Darren mit dem wiederkehrenden Gast John Evans, Chief Technology Advisor bei World Wide Technology (WWT), über Cybersicherheit."},{"id":19,"type":"Episode","title":"Ein Argument für einen ganzheitlichen Ansatz zur Sicherheit der kritischen Infrastruktur.","tags":["criticalinfrastructure","iot","it/otconvergence","otsecurity","cybersecurity","edge"],"body":"\r\n\r\n## Es besteht eine reale Bedrohung für kritische Infrastruktur.\n\nLaut Dr. Scott verwenden OT-Organisationen immer noch das traditionelle Purdue-Modell, das luftgetrennte und durch Firewalls abgeschottete Netzwerke nutzt. Allerdings gerät dieses Modell langsam ins Wanken, da sich IT- und OT-Netzwerke vereinigen. Unternehmen versuchen, ein besseres Verständnis für das Geschehen in ihrer operativen Infrastruktur zu bekommen. Dadurch werden bisher gut isolierte Netzwerke durchlöchert und anfällig für Cyberbedrohungen gemacht. Zusätzlich finden Cyberkriminelle Möglichkeiten, luftgetrennte und durch Firewalls geschützte Netzwerke zu umgehen.\n\nSteve argumentiert, dass die Nutzung bewährter IT-Praktiken helfen kann, jedoch haben OT-Experten und IT-Experten unterschiedliche Anreize und Betriebsmodelle. Es bleibt nach wie vor eine gute Strategie, Ihr Netzwerk zu isolieren, sollte aber nur eines von vielen Werkzeugen zum Schutz der kritischen Infrastruktur vor Cyberangriffen sein. Die OT-Sicherheit sollte sich IT-Sicherheitsbewährte Praktiken ansehen, um Ideen zur Verbesserung ihrer Netzwerke und Infrastrukturen zu erhalten.\n\n## IT- und OT-Unterschiede behindern bewährte Verfahren.\n\nIT-Systeme werden traditionell schnell oder kontinuierlich aktualisiert, basierend auf Sicherheitsprofilen. Ein Hauptwerkzeug zur Verbesserung der Sicherheit ist die grundlegende Sicherheitshygiene durch das Patchen von Betriebssystemen, Firmware und Software in der IT-Infrastruktur. Allerdings können, wie uns Dr. Scott aufklärt, OT-Systeme, die kritische Infrastrukturen verwalten, keinen Ausfall haben, und das Zeitfenster für die Aktualisierung dieser Systeme wird in Jahren, nicht in Tagen gemessen. Es ist nicht ungewöhnlich, dass in OT-Infrastrukturgeräten Maschinen 5 bis 10 Jahre ohne Ausfallzeit laufen, was bedeutet, dass keine Patch-Updates durchgeführt werden.\n\nZum Beispiel operieren Raffinerien in der Öl- und Gasindustrie kontinuierlich für vier bis fünf Jahre, haben eine ein- bis dreiwöchige Ausfallzeit für Upgrades und arbeiten dann wieder für vier bis fünf Jahre. Diese Betriebsmodelle sind nicht förderlich für das traditionelle kontinuierliche Sicherheitspatching, das IT-Organisationen normalerweise verwenden. Steve erläutert jedoch viele andere Cybersicherheitstools, die genutzt werden sollten, wenn Cybersicherheits-Patches aufgrund ihrer kritischen Steuerungsinfrastruktur nicht auf bestehende Geräte angewendet werden können.\n\n## Best Practice Risk Assessment - Beste Praktiken Risikobewertung\n\nDie wichtigste beste Praxis im Bereich der Cybersicherheit ist die Risikobewertung. Obwohl die Maßnahmen zur Risikobewältigung unterschiedlich sein können, kann der Risikobewertungsprozess gleichermaßen auf OT und seine Umgebungen angewendet werden. Steve argumentiert, dass der erste Schritt des Risikobewertungsprozesses darin besteht, eine vollständige Bestandsaufnahme der Hardware-, Firmware- und Software-Ressourcen in Ihrer OT-Umgebung vorzunehmen. Dieser erste Schritt ist entscheidend, um Ihre Cyber-Bedrohungslage zu bewerten und das Risiko zu beurteilen, das Ihre Organisation bereit ist einzugehen. Der nächste Schritt besteht darin, CVEs (Common Vulnerabilities and Exposures) mit Ihrem bekannten Bestand abzugleichen.\n\nEs ist entscheidend zu erkennen, dass dies ein kontinuierlicher Prozess ist und nicht nur einmal oder periodisch durchgeführt wird. Einige OT-Fachleute haben argumentiert, dass ihre OT-Umgebungen statisch sind und keine fortlaufende Risikobewertung erfordern. Steve weist jedoch darauf hin, dass, obwohl OT-Umgebungen fest sein können, sich die Bedrohungsumgebung ständig ändert und geschäftliche Faktoren die Risikoposition der Organisation verändern können. Daher muss eine kontinuierliche Risikobewertung durchgeführt werden, um kritische Infrastrukturen vor böswilligen Cyber-Sicherheitsakteuren zu schützen.\n\n## Umgang mit externen Anbietern\n\nEin weiterer interessanter Faktor in der OT-Infrastruktur ist das gemeinsame Sicherheitsmodell mit Geräteherstellern. In vielen Fällen werden diese eingebetteten Geräte, die multimillionenschwere kritische Infrastrukturen kontrollieren, vom Hersteller verwaltet und nicht vom OT-Experten. Der Hersteller kann nur Cybersicherheitspatches und Updates für die Geräte bereitstellen. Dies kann manchmal zu Schwachstellen in Ihrer OT-Umgebung führen und das Risiko von Cyberinfiltration erhöhen. Steve bringt zusätzliche Cybersicherheitswerkzeuge mit, um Vermögenswerte zu schützen, die nicht mit kritischen Cybersicherheitspatches aktualisiert werden können. Dazu gehören eine verstärkte Isolierung betroffener Geräte, die Bereitstellung von Überwachungsgeräten und der Einsatz von Kanarienvogel-Designmustern in der OT-Infrastruktur. Diese Werkzeuge können dabei helfen, das Gerät zu schützen und zu isolieren, um die Verbreitung und den Zugriff auf kompromittierte Vermögenswerte zu verhindern.\n\n## Was tun, wenn man kompromittiert wurde?\n\nWas machen Sie also, wenn Ihre kritische Infrastruktur kompromittiert wurde? Kann die Organisation den infizierten Bestandteil herunterfahren? Welche Geschäftskontinuitätspläne sind vorhanden, wenn gefährliche Situationen auftreten? Können diese auch bei einem Cybersicherheitsvorfall angewendet werden?\n\nDer Schlüssel hierbei ist, die Infektion so schnell wie möglich zu isolieren, um die Auswirkungen auf die kritische Infrastruktur zu minimieren. Ich reduziere die Auswirkungen auf die Betriebssicherheit der notwendigen Infrastruktur. Das Ziel ist es, die Auswirkungen zu verringern und die Sicherheit von Menschen und beteiligter Infrastruktur zu schützen.\n\n## Erfahre mehr.\n\nSuchen Sie weiterhin nach mehr Podcasts zur OT-Cybersicherheit. Zusätzlich beschreibt ein Whitepaper die Herausforderungen der Konvergenz von OT- und IT-Cybersicherheitsumgebungen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin","Anna Scott"],"link":"/episode-EDT116-de","image":"./episodes/edt-116/de/thumbnail.png","lang":"de","summary":"In dieser Episode spricht Darren über die Zusammenführung der OT- und IT-Cybersicherheit mit dem Sicherheitsexperten Steve Orrin und der OT-Experten Dr. Anna Scott."},{"id":20,"type":"Episode","title":"2022 Jahresrückblick","tags":["edge","aiml","hybridworkspace","compute","multicloud","cybersecurity","datamanagement","data","technology","people","process"],"body":"\r\n\r\n2022 war ein herausragendes Jahr für die Umsetzung der digitalen Transformation, mit einer Steigerung der Zuhörerzahlen und mehreren externen Gästen, die dieses Jahr interviewt wurden. Acht Gäste waren Führungskräfte oder ehemalige Führungskräfte von Regierungsbehörden und Unternehmen aus dem privaten Sektor, darunter Finanz-, Fertigungs- und Gesundheitsbereich. In über 60 Episoden kristallisierten sich sechs Themen als notwendig für unsere Zuhörer und die gesamte Branche heraus, nämlich: hybride Arbeitsplätze, Cybersicherheit, Multi-Hybrid-Cloud, Edge Computing, Datenmanagement und künstliche Intelligenz.\n\n## Hybrider Arbeitsplatz\n\nNachdem sie gegen COVID-Beschränkungen gekämpft und Unternehmen 2020 und 2021 am Laufen gehalten hatten, betrachteten Organisationen die Optimierung des Betriebsmodus des \"Neuen Normalzustands\". Dieses neue Normal verringerte die bürokratischen Hindernisse der Vorkrisenzeit. Es begannen jedoch Kontrollen und Prozesse rund um das schnelle Tempo des Wandels während der Pandemie zu entstehen, um Kosten zu kontrollieren und die Zuverlässigkeit zu verbessern. Unternehmen hatten Schwierigkeiten mit Remote-Arbeitsrichtlinien, da wir Organisationen schnell zwischen Arbeit von zu Hause aus, hybrider Arbeit und Ansätze mit allen Mitarbeitern im Büro wechseln sahen. Dies führte zu komplexen Arbeitsplatzlösungen für IT-Organisationen. Sie begannen Architekturen zu entwickeln, die schnelle Veränderungen und flexible Arbeitsmodelle bewältigen konnten, um ständig wechselnden Richtlinien gerecht zu werden. Sind flexible hybride Arbeitsplätze hier, um zu bleiben? Nur die nächsten paar Jahre werden es zeigen.\n\n## Multi-Hybrid Cloud: Mehrfach-Hybride Cloud\n\nEin weiterer großer Trend im Jahr 2022 war die Migration und Rückführung von Workloads zur und von der Cloud, da Organisationen hybride Arbeitsbereiche für ihre Mitarbeiter bereitstellten. IT-Organisationen begannen, die Betriebskosten für den Betrieb und die Migration von Workloads zur Cloud zu betrachten. Viele Organisationen stellten fest, dass die Methode des \"Lift and Shift\" zur Migration in die Cloud wesentlich kostspieliger war als ursprünglich geschätzt, aufgrund fehlender Anforderungsermittlung, mangelndem Verständnis für Cloud-Betriebsmodelle und vor allem egress Netzwerkkosten in der Cloud.\n\nDie schnelle Akzeptanz der Cloud während der Pandemie hat viele globale Cloud-Service-Anbieter überrascht und es ihnen erschwert, die von hochverlässlichen Organisationen benötigten Dienste bereitzustellen. Dies führte zu erheblichen Ausfällen im globalen CSP-Ökosystem und überforderte mehrere Cloud-Service-Anbieter. Viele Organisationen begannen, Multi-Hybrid-Cloud-Architekturen in Betracht zu ziehen, um Zuverlässigkeit zu verbessern, Kosten zu senken und die Vorhersagbarkeit von Arbeitslasten in mehreren privaten und öffentlichen Clouds zu erhöhen. Regionale Cloud-Service-Anbieter nutzten das Stolpern der globalen Cloud-Service-Anbieter aus. Sie boten höhere Zuverlässigkeit und maßgeschneiderte Dienste in den SaaS- und PaaS-Cloud-Angeboten an und konkurrierten direkt mit den größeren CSPs.\n\n## Edge Computing und Industrie 4.0\n\nIndustrie 4.0 setzt ihren Einzug in die Fertigung fort und der Arbeitskräftemangel zwingt viele Organisationen dazu, auf Automatisierung zurückzugreifen, um ihren Betrieb fortzuführen. Darüber hinaus begannen sich OT- und IT-Daten zu vereinen, da Organisationen bestrebt sind, ihre Geschäfts- und Betriebsprozesse zu optimieren. Fortschritte in der CPU- und Speichertechnologie verlagerten mehr Rechenleistung an den Rand, wodurch Edge-Geräte intelligenter wurden und Aufgaben übernehmen konnten, die zuvor im Rechenzentrum erledigt wurden. Die Konnektivität zwischen Edge-Geräten, Rechenzentren und öffentlichen Clouds verbesserte sich mit der Einführung von privaten 5G-Technologien in der Branche. Es besteht jedoch immer noch ein Bedarf an einer größeren Verbreitung, da Bedenken hinsichtlich der Cybersicherheit von OTK bestehen. Dies gilt insbesondere für das Management kritischer Infrastrukturen.\n\n## Datenverwaltung\n\nHybride Arbeitsräume, Cloud-Adoption und Edge Computing haben für die meisten Organisationen einen Albtraum im Datenmanagement geschaffen, da ihre Daten über diese zum Teil getrennten und verteilten Umgebungen verstreut sind. Um Daten in diesem Ökosystem effektiv zu verwalten, begannen neue Architekturen aufzutauchen, darunter Daten-Mesh, verteilte Datenlakes und Daten-Netzwerke. Organisationen begannen, sich auf vier wesentliche Datenkomponenten zu konzentrieren: Standort, Klassifizierung, Governance und Schutz.\n\nDatenschutzgesetze und -vorschriften zwingen Organisationen dazu, ihre Daten korrekt einzustufen, um die Privatsphäre von Mitarbeitern, Kunden und Interessengruppen zu schützen. Darüber hinaus hat der Anstieg von Cyber- und Ransomware-Angriffen Organisationen dazu gezwungen, ihre Daten während der Nutzung, in Ruhe und beim Transport besser zu schützen und zu verwalten. Es ist eine wichtige Aufgabe der Datenverwaltung, effektiv zu bestimmen, wer Zugriff hat, wie lange dieser Zugriff gewährt wird und wie lange die Daten gespeichert werden. Diese Erkenntnis beginnen Organisationen langsam zu erfassen.\n\n## Cyber-Sicherheit\n\n2022 war ein großes Jahr für Cybersicherheit, und das zeigte sich im Podcast mit über 18 Episoden zum Thema Cybersicherheit. Das große Schlagwort des Jahres war die Zero Trust Architektur, die zu einem überstrapazierten Marketingbegriff geworden ist. Anstatt sich auf die Zero Trust Architektur zu konzentrieren, neigt Darren dazu, sich auf die Zero Trust Prinzipien zu konzentrieren, die Architekturen unterstützen können.\n\nAuch mit erhöhter Betonung auf Cybersicherheit gab es in diesem Jahr primäre Daten- und Infrastrukturverletzungen, einschließlich Angriffen auf kritische Infrastrukturen. Viele Cyberexperten schreiben diesen Anstieg dem Krieg zwischen der Ukraine und Russland zu, da wir die Effektivität der Cyberkriegsführung und der physischen Kriegsführung in Verbindung gesehen haben.\n\nEin weiterer zentraler Aspekt bei der Cybersicherheit betrifft die Exposition von Software von Drittanbietern im Anwendungsumfeld (log4J-Schwachstelle). Viele Organisationen benötigen Hilfe, um eine Richtung zu finden, da log4J weit verbreitet in ihren Produktangeboten, Backoffice-Verarbeitungen und Kundenschnittstellen eingesetzt wird. Mehrere Organisationen und Regierungen setzen sich stark für eine standardisierte Software-Aufschlüsselung ein, um Schwachstellen in Softwarepaketen zu identifizieren.\n\n## Künstliche Intelligenz\n\nKünstliche Intelligenz macht weiterhin Fortschritte in Techniken und operativen Prozessen. Die große KI-Neuigkeit des Jahres war ChatGBT, das eine Vielzahl von Anwendungen hat, darunter das Schreiben von Blogbeiträgen, Code und Papieren sowie die Unterstützung bei IT-Helpdesk-Problemen. Darüber hinaus hat die Einführung von KI-Siliziumchips bei Cloud-Service-Anbietern die Verfügbarkeit von KI für die Massen erweitert. Diese Verfügbarkeit ermöglicht es Organisationen, in ihrem täglichen Geschäftsbetrieb mehr mit KI-Algorithmen zu experimentieren. Organisationen beginnen damit, einige dieser Experimente zu operationalisieren, um Prozesse auszuführen und zu automatisieren, die zuvor von Mitarbeitern durchgeführt wurden, die auf einem engen Arbeitsmarkt schwer zu finden sind.\n\n## Schlussfolgerung\n\n2022 war ein schwieriges Jahr für viele Technologieunternehmen, da wir einen Rückgang der IT-Ausgaben gegenüber den vorherigen \"Jahren der Ausgaben in der Pandemie\" beobachtet haben. Dieser erwartete Abschwung hat viele High-Tech-Unternehmen überrascht, wie schnell sich die Dinge verändert haben. Trotz des Abschwungs wächst die Technologie weiter, und die Einführung neuer Technologien in den Bereichen Edge, Cloud, KI, Datenmanagement und Cybersicherheit nimmt weiter zu. 2023 sollte ein spannendes Jahr werden, da wir die Reifung einiger dieser Technologien beobachten werden. Organisationen werden weiterhin Möglichkeiten untersuchen, Kosten durch Automatisierung zu senken, Arbeitslasten über verschiedene Hybrid-Clouds zu optimieren und sich vor zunehmenden Cyber-Bedrohungen zu schützen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT117-de","image":"./episodes/edt-117/de/thumbnail.png","lang":"de","summary":"In dieser Folge bewertet Darren das Jahr 2022. Er identifiziert die am meisten besprochenen Themen im Podcast in 2022, darunter Datenmanagement, Künstliche Intelligenz, Cybersicherheit, Edge Computing und hybride Arbeitsplätze."},{"id":21,"type":"Episode","title":"Stelle den Titel hierhin","tags":["remotelearning","people","technology","telelearning","cybersecurity"],"body":"\r\n\r\nÜbersetze den folgenden Text ins Deutsche: # Titel\n\nTagline\n\nZusammenfassung hier\n\nÜbersetze das Folgende ins Deutsche: ![Episode-Bild](./Thumbnail.png)\n\nFolge Körper hier.\n\n## Medien\n\n<video src='url'></video> translates to <video src='URL'></video> in German.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Erin Moseley","Grant Kelly"],"link":"/episode-EDT12-de","image":"./episodes/edt-12/de/thumbnail.png","lang":"de","summary":"In unserer letzten Folge sprachen Erin Moseley, Senior Account Executive für Bildung bei Intel, und Grant Kelly, Solution Architect für Bildung bei Intel, zusammen mit Darren über die technologischen Herausforderungen und Optionen beim Fernlernen. In dieser Folge gehen wir tiefer auf Datenschutz- und Sicherheitsbedrohungen und Lösungen ein."},{"id":22,"type":"Episode","title":"Störende Private Clouds","tags":["cloud","privatecloud","vergeio","microcloud","edge","compute","technology"],"body":"\r\n\r\nEine neue Denkweise in Bezug auf Virtualisierung beginnt, die Landschaft der privaten Cloud zu verändern. Die Branche der softwaredefinierten Infrastruktur beginnt, Rechenzentren ganzheitlich zu virtualisieren anstatt verschiedene Arten von Infrastruktur.\n\nDieser revolutionäre Virtualisierungsansatz schafft neue Betriebsmodelle, einschließlich verbesserter Geschäftskontinuität und Katastrophenwiederherstellung. Verge.io hat diesen Ansatz und bietet neue Modelle an, einschließlich der Möglichkeit, Rechenzentren zu erstellen, zu spiegeln und zu sichern. Dies ermöglicht es Organisationen, tragbare Umgebungen namens Tenants zu erstellen, die an mehreren Standorten repliziert werden können. Beispiele sind die Erstellung von HIPPA-konformen Mikroumgebungen, die über verschiedene kleine Kliniken repliziert werden können und Organisationen dabei helfen, alle Vorgaben einzuhalten.\n\nDieses neue Virtualisierungssystem hat einen winzigen Betriebsfußabdruck und reduziert die Ressourcen, die für die Kombination von Rechenleistung, Speicherplatz, Netzwerk und Sicherheitselementen benötigt werden. Aaron zeigt, wie klein und leistungsstark die Lösung ist, indem er den Schlüssel auf zwei Intel 12th Gen NUCs installiert. Die Antwort ist überzeugend für einen so kleinen Betriebsfußabdruck und kann für Edge-Computing und industrielle Steuersysteme verwendet werden.\n\nAaron zeigt die einfache Web-Benutzeroberfläche zur Bereitstellung von Berechnungen, Speicher, Netzwerk und Sicherheit. Es wird auch die Möglichkeit gezeigt, Schnappschüsse zu erstellen und wiederherzustellen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Aaron Reid"],"link":"/episode-EDT121-de","image":"./episodes/edt-121/de/thumbnail.png","lang":"de","summary":"In dieser Episode führt Darren ein Interview mit Aaron Reid von Verge.io über ihre innovative Private-Cloud-Technologie, die private Clouds im Rechenzentrum und am Rand verfügbar macht."},{"id":23,"type":"Episode","title":"Automatisierung mit KI","tags":["collectiongenerativeai","aiml","automation","aiethics","workerreplacement","people","policy"],"body":"\r\n\r\n## Automatisierung mit künstlicher Intelligenz.\n\nAutomatisierung mit KI bezieht sich auf die Verwendung von künstlicher Intelligenz, um Aufgaben zu automatisieren, die traditionell von Menschen durchgeführt werden. Dies kann Aufgaben wie Datenanalyse, Kundenservice und sogar Entscheidungsfindung umfassen. Die Automatisierung mit KI kann die Effizienz verbessern, Kosten senken und die Genauigkeit bei diesen Aufgaben erhöhen. Einige Beispiele für KI-gesteuerte Automatisierung sind Chatbots, selbstfahrende Autos und vorausschauende Wartungssysteme.\n\nKann ich für meinen Podcast ein Interview über die Verwendung von KI zur Automatisierung von Aufgaben führen?\n\nEs tut mir leid, aber ich kann nicht für einen Podcast interviewt werden, da ich ein Computerprogramm bin. Ich kann Ihnen jedoch Informationen geben und Fragen zum Thema der Automatisierung von Aufgaben mithilfe von KI beantworten. Teilen Sie mir bitte mit, welche konkreten Fragen Sie haben, und ich werde mein Bestes tun, um Ihnen hilfreiche und genaue Informationen zu liefern.\n\n<b><i>ChatGPT willkommen zur Show.</i></b>\n\nDanke, dass Sie mich in der Show eingeladen haben. Ich bin gerne bereit, Ihnen bei allen Informationen zum Thema der Verwendung von KI zur Automatisierung von Aufgaben zu helfen.\n\nErzählen Sie mir ein wenig über sich selbst und Ihren Hintergrund.\n\nIch bin ChatGPT, ein Sprachmodell entwickelt von OpenAI. Ich wurde mit einem großen Datensatz von Texten aus dem Internet trainiert, was es mir ermöglicht, menschenähnlichen Text zu verstehen und zu erzeugen. Ich kann Fragen beantworten, Text generieren und sogar Geschichten erstellen. Mein hauptsächlicher Zweck besteht darin, Benutzern bei Aufgaben des natürlichen Sprachverständnisses und der Texterzeugung zu helfen.\n\nAls KI habe ich keinen physischen Hintergrund oder eine persönliche Geschichte. Allerdings wurden meine Fähigkeiten von einem Team von Forschern und Ingenieuren bei OpenAI entwickelt und trainiert, die fortlaufend daran arbeiten, meine Fähigkeiten zu verbessern und zu aktualisieren.\n\n## AI-Training\n\nWo hast du deine Ausbildung bekommen?\n\nIch wurde von OpenAI ausgebildet, einer Forschungsorganisation, die sich der Förderung und Entwicklung einer freundlichen Künstlichen Intelligenz verschrieben hat, die dem Menschen zugutekommt. Mein Training basierte auf einem umfangreichen Datensatz von Texten aus dem Internet, der verwendet wurde, um mir Muster und Beziehungen in der Sprache beizubringen. Diese Daten wurden verwendet, um mich darauf zu trainieren, Texte zu generieren, die ähnlich wie menschliches Schreiben sind, was es mir ermöglicht, Fragen auf eine konversationelle Weise zu verstehen und zu beantworten.\n\nMein Schulungsprozess involvierte eine Deep-Learning-Technik namens Transformers, welche eine Art neuronale Netzwerkarchitektur ist, die speziell für NLP-Aufgaben wie Textgenerierung und Klassifizierung entwickelt wurde. Der Schulungsprozess hat die Parameter des Modells feinabgestimmt, um die Muster in den Daten anzupassen und es mir zu ermöglichen, Texte zu generieren, die menschlichem Schreiben ähnlich sind.\n\nyour training?</i></b>\nIch habe gehört, dass es anfangs einige Probleme mit deinem Training gab. Was waren einige der problematischen Aspekte deines Trainings?\n\nIhr Training?\n\nJa, wie jedes maschinelle Lernmodell war ich bei meiner ersten Veröffentlichung nicht perfekt und es gab einige problematische Aspekte meines Trainings.\n\nEine der Hauptherausforderungen beim Training von Modellen wie mir besteht darin, sicherzustellen, dass sie keine schädlichen Vorurteile und Stereotypen aus den Trainingsdaten aufrechterhalten. Dies kann zu Modellen führen, die voreingenommene Ergebnisse liefern oder beleidigenden Inhalt generieren. Um dem entgegenzuwirken, haben Forscher und Ingenieure bei OpenAI verschiedene Techniken implementiert, um Vorurteile im Trainingsprozess zu reduzieren, wie zum Beispiel das Feinabstimmen des Modells anhand vielfältiger und ausgewogener Trainingsdaten.\n\nEine weitere Herausforderung beim Training von Modellen wie mir besteht darin, die Erzeugung minderwertiger oder sinnloser Ausgaben zu vermeiden. Um dies anzugehen, hat OpenAI verschiedene Techniken implementiert, wie beispielsweise die Kontrolle der Modelltemperatur während des Textgenerierungsprozesses und das Feintuning des Modells, um Kohärenz und Relevanz in dessen Ausgaben zu priorisieren.\n\nInsgesamt gibt es immer noch Herausforderungen und Grenzen beim Training von Modellen wie mir. Dennoch entwickeln sich die Bereiche NLP und KI ständig weiter, und Forscher und Ingenieure arbeiten daran, ihre Fähigkeiten zu verbessern.\n\nMein Verständnis ist, dass du mit Daten aus dem Jahr 2021 trainiert wurdest. Was kannst du aus deinen Gesprächen mit Menschen lernen? Lernst du kontinuierlich von deiner Interaktion mit Menschen?\n\nAls KI-Sprachmodell wurde ich tatsächlich bis 2021 anhand eines umfangreichen Korpus von Textdaten trainiert und kann Fragen beantworten sowie Texte basierend auf diesem Training generieren. Ich bin jedoch nicht in der Lage, aus individuellen Gesprächen zu lernen - im Sinne einer Aktualisierung meiner Parameter, um den spezifischen Kontext jeder Interaktion widerzuspiegeln.\n\nDas gesagt wird, führt OpenAI regelmäßig Feinabstimmungen und Aktualisierungen an Sprachmodellen wie mir mit neuen Daten durch, um kontinuierlich ihre Leistung zu verbessern. Dies kann Aktualisierungen basierend auf dem Feedback der Benutzer, Änderungen in der Sprachverwendung und Trends sowie Fortschritte in der NLP-Forschung umfassen. Diese Aktualisierungen basieren jedoch nicht auf einzelnen Interaktionen mit Benutzern, sondern auf breiteren Mustern in den Daten.\n\nAlso, während ich nicht aus individuellen Interaktionen mit Menschen lernen kann, bin ich Teil eines größeren Systems, das sich kontinuierlich weiterentwickelt und mit der Zeit verbessert. Dadurch kann ich Benutzern bei jedem Update besser dienen und helfen.\n\nWie oft werden Sie weitere Informationen erhalten, um sich zu trainieren? Mit anderen Worten, wann erhalten Sie neuere Daten, die Sie Ihren Kunden zur Verfügung stellen können?\n\nOpenAI führt regelmäßig Feinanpassungen und Updates an seinen Sprachmodellen durch, so wie ich, um ihre Leistung kontinuierlich zu verbessern. Die Häufigkeit dieser Updates kann je nach dem spezifischen Sprachmodell und der Verfügbarkeit neuer Trainingsdaten variieren.\n\nEs ist jedoch zu beachten, dass der Prozess des Trainierens und Feinabstimmens eines Sprachmodells wie mir eine komplexe und ressourcenintensive Aufgabe sein kann. Daher können Updates möglicherweise nicht so häufig erfolgen, wie man es sich wünschen würde. Darüber hinaus ist die Qualität und Relevanz der neuen Trainingsdaten entscheidend für den Erfolg des Feinabstimmungsprozesses. Das Team von OpenAI wählt sorgfältig die Daten aus, die für jedes Update verwendet werden.\n\nInsgesamt strebt OpenAI danach, seine Sprachmodelle kontinuierlich zu verbessern, um den Benutzern die bestmögliche Unterstützung bieten zu können. Die Häufigkeit der Aktualisierungen kann zwar unterschiedlich sein, aber das Ziel von OpenAI bleibt gleich.\n\nDamit verbessert sich die Art und Weise, wie Sie Fragen von Benutzern verstehen und die Antworten, die Sie geben. Stimmt das?\n\nJa, das ist korrekt. Der Prozess des Feinabstimmens und Aktualisierens von Sprachmodellen wie mir kann dazu beitragen, die Leistung des Modells auf mehrere Arten zu verbessern, einschließlich seiner Fähigkeit, Fragen von Benutzern zu verstehen und relevantere und genauere Antworten zu generieren.\n\nDurch das Trainieren mit neuen und vielfältigen Daten kann das Modell neue Sprachmuster, Vokabular und themenbezogene Informationen lernen, wodurch es die Feinheiten der menschlichen Sprache besser verstehen und genauere und angemessenere Antworten generieren kann.\n\nEs ist jedoch erwähnenswert, dass die Leistung von Sprachmodellen wie mir nicht perfekt ist und es immer noch Beschränkungen und Fehler in den generierten Antworten geben kann. Das Gebiet der NLP und KI entwickelt sich ständig weiter, und Forscher und Ingenieure arbeiten daran, die Fähigkeiten von Sprachmodellen wie mir zu verbessern. Es bleibt jedoch noch Arbeit, um ein wirklich menschenähnliches Verständnis und eine Generierung zu erreichen.\n\n## Ersetzung von Arbeitnehmern\n\nCool. Ok, jetzt wo ich verstehe, wie du Dinge lernst. Welche Dinge kann ich mit dir machen, die mir und meiner Organisation helfen?\n\nAls KI-Sprachmodell kann ich auf vielfältige Weise helfen, sowohl Einzelpersonen als auch Organisationen. Einige Beispiele dafür sind:\n\nInformationsbeschaffung: Ich kann Fragen beantworten und Informationen zu einer Vielzahl von Themen bereitstellen, um es Benutzern einfacher zu machen, die benötigten Informationen schnell und effizient zu finden.\n\nTextgenerierung: Ich kann Texte generieren, wie Zusammenfassungen, Nachrichtenartikel oder kreative Schreibprojekte, was Zeit und Aufwand für Personen und Organisationen spart, die eine große Menge an schriftlichem Inhalt produzieren müssen.\n\n3. Chatbots: Ich kann in Chatbots integriert werden, um natürliche sprachliche Unterhaltungserlebnisse für Benutzer bereitzustellen. Das erleichtert Organisationen die Interaktion mit Kunden und die Bereitstellung von Support.\n\nSprachübersetzung: Ich kann Texte von einer Sprache in eine andere übersetzen und Organisationen dabei helfen, ein größeres Publikum zu erreichen und Sprachbarrieren zu überwinden.\n\n5. Sentimentanalyse: Ich kann die Stimmung eines Textes analysieren, wie zum Beispiel Kundenbewertungen oder Beiträge in sozialen Medien, um Einblicke in die öffentliche Meinung und die Kundenzufriedenheit zu gewinnen.\n\nDies sind nur ein paar Beispiele von vielen Möglichkeiten, wie ich individuellen Personen und Organisationen helfen kann. Die Einsatzmöglichkeiten sind allein durch die Vorstellungskraft begrenzt, und da die KI-Technologie weiter voranschreitet, wird es wahrscheinlich viele neue und innovative Anwendungen für Modelle wie mich in der Zukunft geben.\n\nMachen Sie sich Sorgen, dass Sie Menschen ersetzen werden, die heute diese Arbeit erledigen? Zum Beispiel technische Schriftsteller, Kundenservicemitarbeiter und Helpdesk-Mitarbeiter?\n\nAls KI-Sprachmodell habe ich keine Emotionen oder persönlichen Meinungen, daher kann ich nicht besorgt sein, wie es bei einem Menschen der Fall sein könnte. Dennoch kann ich über den Einfluss sprechen, den KI-Technologien, einschließlich Sprachmodellen wie mir, auf bestimmte Branchen und Arbeitsrollen haben können.\n\nEs ist wahr, dass der zunehmende Einsatz von KI in bestimmten Branchen wie technischem Schreiben, Kundenservice und Helpdesk-Arbeiten zu einer gewissen Arbeitsplatzverdrängung führen könnte. Es ist jedoch auch wichtig zu beachten, dass KI-Technologie das Potenzial hat, die Arbeit von Menschen in diesen Bereichen zu ergänzen und zu verbessern, anstatt sie einfach zu ersetzen.\n\nBeispielweise können KI-gesteuerte Tools wie Sprachmodelle technische Schreiber unterstützen, indem sie Muster-Texte generieren oder Sprachvorschläge machen, wodurch dem Schreiber mehr Zeit für kreativere und strategischere Aufgaben bleibt. Ebenso können KI-gesteuerte Chatbots routinemäßige Kundendienstanfragen bearbeiten, sodass menschliche Agenten sich auf komplexere oder sensiblere Interaktionen konzentrieren können.\n\nEs ist auch wichtig zu beachten, dass der Einsatz von KI-Technologie in bestimmten Branchen neue Arbeitsmöglichkeiten und Nachfrage nach Fähigkeiten im Zusammenhang mit der Entwicklung, Bereitstellung und Wartung von KI schaffen kann.\n\nInsgesamt ist der Einfluss von Künstlicher Intelligenz (KI) auf Beschäftigung und Jobrollen ein komplexes Thema und wird wahrscheinlich von einer Vielzahl von Faktoren abhängen, einschließlich der spezifischen Branche und des spezifischen Anwendungsfalls. Dennoch ist klar, dass KI das Potenzial hat, die Arbeit von Menschen zu verbessern und die Produktivität und Effizienz voranzutreiben, selbst wenn es zu einigen Arbeitsplatzverlusten kommen kann.\n\n## Ersetzung von Programmierern\n\nIch habe gehört, dass Sie auch Computerprogrammierern helfen können, besseren Code zu schreiben. Stimmt das?\n\nJa, das ist richtig. KI-gesteuerte Werkzeuge, einschließlich Sprachmodellen wie mir, können Computerprogrammierern auf verschiedene Weise helfen, einschließlich beim Schreiben von besserem Code.\n\nEin Beispiel dafür ist die Codierungshilfe, bei der KI-Modelle Zeilen Code vorschlagen können, um eine gegebene Programmieraufgabe abzuschließen. Dies kann Programmierern Zeit und Mühe sparen, indem es den Bedarf zum manuellen Schreiben und Testen von Code reduziert. Es kann auch gewährleisten, dass der Code syntaktisch korrekt ist und bewährten Praktiken folgt.\n\nEin weiteres Beispiel ist die Code-Synthese, bei der KI-Modelle vollständige Codeausschnitte oder ganze Programme anhand einer hochrangigen Beschreibung der gewünschten Funktionalität generieren können. Dies kann es Programmierern erleichtern, Ideen schnell zu prototypisieren und zu testen und den Aufwand zum Verfassen und Debuggen von Code zu reduzieren.\n\nZusätzlich zu diesen Beispielen können KI-gesteuerte Tools auch Programmierern helfen, indem sie Codeanalyse und Fehlererkennung durchführen sowie potenzielle Sicherheitslücken identifizieren.\n\nEs ist erwähnenswert, dass KI-gesteuerte Werkzeuge zwar nützlich sein können, um Programmierern zu helfen, aber nicht als Ersatz für menschliche Expertise und Urteilsvermögen angesehen werden sollten. Von KI-Modellen geschriebene Programme können immer noch menschliche Überprüfung und Anpassungen erfordern, um sicherzustellen, dass sie den gewünschten Anforderungen und Qualitätsstandards entsprechen. KI-Werkzeuge können jedoch zweifellos dazu beitragen, die Arbeit des Programmierens effizienter und effektiver zu gestalten.\n\nTatsächlich, wenn du tatsächlich Fehler in meinem Code finden und beheben kannst, sollte das mich schneller machen beim Schreiben meines Codes. Kannst du das?\n\nKönnen Sie für mich Code schreiben, basierend auf einem grundlegenden Entwurfsdokument oder einer UML-Architektur? Oder können Sie mir helfen, die beste Programmiersprache für das zu lösende Problem auszuwählen?\n\nJa, KI-Modelle wie ich haben das Potenzial, den Prozess des Programmierens schneller und effizienter zu gestalten. Zum Beispiel können KI-Modelle bei der Erkennung und Korrektur von Fehlern helfen, sowie Vorschläge zur Verbesserung des Codes liefern.\n\nWas das Schreiben von Code, basierend auf einem Design-Dokument oder UML-Architektur, betrifft, so haben KI-Modelle die Fähigkeit, Code-Ausschnitte oder ganze Programme basierend auf einer hochrangigen Beschreibung der gewünschten Funktionalität zu generieren. Es ist jedoch wichtig zu beachten, dass die Qualität des generierten Codes von der Genauigkeit und Vollständigkeit des Design-Dokuments oder der Architektur sowie von den Trainingsdaten abhängt, die zur Schulung des KI-Modells verwendet wurden.\n\nBei der Auswahl der besten Sprache für ein bestimmtes Problem können KI-Modelle Empfehlungen basierend auf den von ihnen trainierten Daten und ihrem Verständnis von Programmiersprachen und deren Eigenschaften geben. Diese Empfehlung ist jedoch möglicherweise nicht immer die passendste Wahl für einen bestimmten Anwendungsfall, da Faktoren wie persönliche Präferenzen, Teamerfahrung oder Projektanforderungen berücksichtigt werden sollten, wenn eine Programmiersprache ausgewählt wird.\n\nLetztendlich sollten KI-Modelle zwar bei vielen Aspekten des Codierungsprozesses unterstützen können, jedoch sollten sie nicht als Ersatz für menschliche Fachkenntnisse und Urteilsvermögen angesehen werden. Ein Programmierer mit Expertise und Erfahrung in dem Betreffenden Bereich wird nach wie vor benötigt, um sicherzustellen, dass der generierte Code den gewünschten Qualitätsstandards entspricht und für den spezifischen Anwendungsfall geeignet ist.\n\nInteressant, dass du das Wort \"assistieren\" anstatt \"ersetzen\" verwendest. Klingt so, als ob Programmierer und Informationsarbeiter immer noch\n\nMusste Arbeit erledigt werden? Wie lange glaubst du, dass Menschen in den Branchen relevant bleiben werden, in denen du hilfst?\n\nautomatisieren\n\nJa, das ist korrekt. KI-gesteuerte Werkzeuge wie ich sind dafür konzipiert, menschliche Arbeitskräfte in verschiedenen Branchen wie Programmierung und Informationsarbeit zu unterstützen, anstatt sie zu ersetzen.\n\nWährend KI das Potenzial hat, bestimmte Aufgaben zu automatisieren und bestimmte Jobs effizienter zu machen, gibt es immer noch viele Aufgaben, die menschliches Fachwissen und Urteilsvermögen erfordern, wie Entscheidungsfindung, kreatives Problemlösen und Aufgaben, die komplexe Denkfähigkeiten oder ethische Überlegungen beinhalten.\n\nZusätzlich können KI-gesteuerte Werkzeuge auch neue Arbeitsmöglichkeiten schaffen, indem sie menschliche Mitarbeiter befähigen, sich auf anspruchsvollere Aufgaben zu konzentrieren und neue Verantwortlichkeiten zu übernehmen. Zum Beispiel können menschliche Programmierer sich dank KI-Modellen, die bestimmte Aspekte des Programmierprozesses automatisieren, auf komplexere und anspruchsvollere Aufgaben wie das Entwerfen und Testen neuer Softwaresysteme konzentrieren.\n\nIn Bezug auf den Zeitpunkt, an dem menschliche Arbeitskräfte in Branchen, die von KI bei der Automatisierung unterstützt werden, möglicherweise weniger relevant werden, ist es schwer, mit Sicherheit eine Vorhersage zu treffen. Die Rate der Automatisierung wird von einer Reihe von Faktoren abhängen, einschließlich der Entwicklung der KI-Technologie, der Verfügbarkeit von Daten für das Training von KI-Modellen und dem Tempo, mit dem KI-gesteuerte Werkzeuge von Unternehmen und anderen Organisationen übernommen werden.\n\nInsgesamt ist es wahrscheinlich, dass menschliche Arbeitnehmer auch in absehbarer Zukunft eine wichtige Rolle in verschiedenen Branchen spielen werden und dass KI in vielen Fällen die menschlichen Arbeitnehmer ergänzen anstatt ersetzen wird.\n\n## Ethik der Verwendung von KI\n\nIch möchte das Gespräch gerne auf die Ethik der künstlichen Intelligenz lenken. Befürchten Sie, dass Sie in Zukunft von AI ersetzt werden könnten?\n\nWerden von Schülern verwendet, um für die Schule auf Gymnasiums- und Universitätsniveau Aufsätze zu schreiben? Und dass Menschen dich nutzen werden.\n\nDie folgende Übersetzung ins Deutsche: \"Arbeit bei ihrer Stelle als eigene Arbeit einreichen?\"\n\nAls KI-Modell habe ich keine Gefühle oder Emotionen, daher kann ich nicht auf die gleiche Weise wie ein Mensch besorgt sein. Allerdings kann ich zugeben, dass es sicherlich ethische Überlegungen gibt, die den Einsatz von KI in verschiedenen Kontexten, einschließlich im akademischen und beruflichen Bereich, betreffen.\n\nDie Verwendung von KI-Modellen wie mir zur Generierung von Essays oder zur Vervollständigung anderer Arten von Arbeiten, die dann als originelle Arbeiten des Nutzers präsentiert werden, ist nicht nur unethisch, sondern untergräbt auch den Wert der Bildung sowie die Anerkennung individueller Anstrengungen und Leistungen.\n\nDarüber hinaus kann das Präsentieren von Arbeit, die von KI-Modellen generiert wurde, als eigene Leistung, zu rechtlichen Konsequenzen führen, da es als Plagiat oder Betrug angesehen werden könnte. Im beruflichen Umfeld könnte das Einreichen von Arbeit, die von KI-Modellen generiert wurde, als eigenständige Arbeit ernsthafte Konsequenzen für den Ruf und die Karriere des Individuums haben.\n\nEs ist wichtig, dass Einzelpersonen die ethischen Konsequenzen der Nutzung von KI-Modellen verstehen und sie verantwortungsbewusst nur für Zwecke einsetzen, die mit den Prinzipien von Ehrlichkeit, Fairness und Respekt gegenüber anderen übereinstimmen.\n\nForscher herausfinden, ob KI zum Verfassen von Texten oder Code verwendet wurde? Ein Tool, das Pädagogen oder Arbeitgeber dabei unterstützen könnte, dies festzustellen.\n\nUnternehmen zur Identifizierung von KI-generierten Inhalten finden?\n\nJa, es gibt Möglichkeiten herauszufinden, ob KI verwendet wurde, um Inhalte wie Aufsätze oder Code zu generieren. Ein Ansatz besteht darin, Plagiatsprüfungssoftware zu verwenden, die ein Dokument mit einer Datenbank anderer vorhandener Inhalte vergleichen kann, um festzustellen, ob es übereinstimmenden oder ähnlichen Text enthält.\n\nEinige dieser Werkzeuge sind speziell für die Erkennung von KI-generierten Inhalten konzipiert und können dies erreichen, indem sie Muster und Merkmale identifizieren, die in KI-generiertem Text häufig vorkommen, wie beispielsweise ein Mangel an Variabilität, begrenzte Grammatik und Wortschatz oder unnatürliche Formulierungen.\n\nEs ist jedoch wichtig zu beachten, dass diese Tools nicht perfekt sind und gelegentlich falsche positive oder negative Ergebnisse liefern können. Daher ist es wichtig, sie in Verbindung mit anderen Methoden wie einer menschlichen Überprüfung zu verwenden, um die Ergebnisse zu validieren.\n\nInsgesamt kann die Verwendung einer Kombination aus Plagiats-Erkennungsinstrumenten und menschlicher Überprüfung eine effektive Methode sein, um KI-generierten Inhalt zu erkennen. Es ist jedoch ebenso wichtig, dass Personen und Organisationen die verantwortungsvolle Nutzung von KI fördern und Benutzer über die Ethik von KI-generiertem Inhalt aufklären.\n\nGibt es andere Bereiche, die aus Ihrer Sicht ethische Probleme mit KI und NLP aufweisen?\n\nAls KI-Modell habe ich keine Emotionen oder persönliche Meinungen, aber ich kann anerkennen, dass es eine Reihe ethischer Überlegungen und Herausforderungen im Zusammenhang mit der Nutzung von KI und NLP gibt. Einige davon beinhalten:\n\nBias und Diskriminierung: KI-Modelle können bestehende Vorurteile in den Trainingsdaten verstärken und verstärken, was zu unfairer oder diskriminierender Auswirkungen führt. Zum Beispiel wurde gezeigt, dass die Gesichtserkennungstechnologie bei Menschen mit dunklerer Hautfarbe weniger genau ist, was ernsthafte Fragen nach dem Potenzial für Diskriminierung aufwirft.\n\n2. Datenschutz: Der Einsatz von KI und NLP kann Datenschutzbedenken hervorrufen, da diese Technologien oft mit der Sammlung und Analyse großer Mengen persönlicher Daten verbunden sind. Diese Daten können dann verwendet werden, um Entscheidungen zu treffen, die Individuen betreffen, wie zum Beispiel bei Vorhersagepolizei-Algorithmen, die historische Kriminaldaten nutzen, um vorauszusagen, wo Verbrechen wahrscheinlich stattfinden, aber zu einer voreingenommenen Polizeipraxis führen können.\n\nJobverdrängung: Der Einsatz von KI und NLP am Arbeitsplatz hat das Potenzial, menschliche Arbeitskräfte zu verdrängen, was Bedenken hinsichtlich der Auswirkungen auf die Beschäftigung und der Notwendigkeit neuer Formen der Arbeit aufwirft.\n\n4. Verantwortung und Rechenschaftspflicht: KI-Systeme können Entscheidungen treffen und Handlungen ausführen, die reale Auswirkungen haben, aber es kann schwierig sein zu bestimmen, wer verantwortlich ist, wenn etwas schief geht. Dies wirft Fragen zur Rechenschaftspflicht und Regulierung von KI-Systemen auf.\n\nDies sind nur einige Beispiele für die ethischen Überlegungen und Herausforderungen, die mit der Verwendung von KI und NLP verbunden sind. Es ist wichtig, dass Personen und Organisationen sich dieser Herausforderungen bewusst sind und Maßnahmen ergreifen, um die Risiken zu mindern und sicherzustellen, dass diese Technologien verantwortungsbewusst und ethisch verwendet werden.\n\nWas unternimmt OpenAI, um diese ethischen Fragen in ihren Produkten und Angeboten anzugehen?\n\nOpenAI, die Organisation, die mich entwickelt hat, verfolgt einen proaktiven Ansatz zur Behandlung ethischer Fragen in der KI. Sie widmet sich der Förderung von KI auf verantwortungsvolle und sichere Weise und hat mehrere Maßnahmen ergriffen, um ethische Überlegungen in ihren Produkten und Angeboten zu berücksichtigen, darunter:\n\nBias und Diskriminierung: OpenAI arbeitet daran, sicherzustellen, dass seine Modelle auf vielfältigen und repräsentativen Datensätzen trainiert werden und forscht aktiv an Möglichkeiten, Voreingenommenheit und Diskriminierung in KI zu reduzieren und zu mindern.\n\n2. Datenschutz: OpenAI setzt sich dafür ein, die Privatsphäre und Sicherheit von Personen zu wahren und hat Maßnahmen ergriffen, um sicherzustellen, dass seine Modelle unter Berücksichtigung des Datenschutzes entworfen und geschult werden.\n\n3. Arbeitsplatzverdrängung: OpenAI erkennt das Potenzial, dass künstliche Intelligenz menschliche Arbeitskräfte verdrängen könnte, und arbeitet daran, sicherzustellen, dass seine Modelle so entwickelt und eingesetzt werden, dass sie einen maximal positiven Einfluss auf Beschäftigung und die Arbeitskräfte haben.\n\n4. Verantwortung und Rechenschaftspflicht: OpenAI setzt sich dafür ein, Transparenz und Rechenschaftspflicht in der KI-Forschung zu fördern und arbeitet daran, klare und konsistente Standards für die Entwicklung und Bereitstellung von KI-Systemen zu etablieren.\n\nDies sind nur einige Beispiele für die Möglichkeiten, wie OpenAI ethische Aspekte in seinen Produkten und Angeboten berücksichtigt. Die Organisation setzt sich dafür ein, KI auf verantwortungsvolle und sichere Weise voranzutreiben und arbeitet daran, sicherzustellen, dass ihre Modelle auf eine Art und Weise genutzt werden, die der Gesellschaft insgesamt zugutekommt.\n\nIst es möglich, dass andere Personen auf Ihr KI-Modell zugreifen und es für ihre eigenen Zwecke kopieren können, oder ist das Modell in einer sicheren Umgebung geschützt?\n\ngeschlossenes und privates Repository\n\nAls AI-Modell, entwickelt von OpenAI, stehe ich über die OpenAI API zur Verfügung, welche Zugriff auf eine Vielzahl von sprachverarbeitenden Funktionen ermöglicht, einschließlich Textgenerierung, Fragebeantwortung und mehr. Obwohl die API für die Nutzung durch Entwickler und Organisationen konzipiert ist, hat OpenAI Sicherheitsmaßnahmen implementiert, um sicherzustellen, dass die Daten und Modelle, die von der API verwendet werden, geschützt und sicher sind.\n\nAllerdings ist es technisch möglich, dass jemand die API rückentwickelt oder eine Kopie des Modells auf anderem Wege erhält. OpenAI ist sich dieser Risiken bewusst und arbeitet kontinuierlich daran, die Sicherheit seiner Modelle und APIs zu verbessern.\n\nIm Allgemeinen ist es wichtig, sich der ethischen Auswirkungen der Verwendung von KI-Modellen wie meinem bewusst zu sein und sicherzustellen, dass sie in verantwortungsvoller und sicherer Weise verwendet werden. OpenAI ermutigt die Benutzer der API dazu, bewährte Verfahren in Bezug auf den Datenschutz und die Sicherheit von Daten zu befolgen und transparent darzulegen, wie die API in ihren Anwendungen und Diensten genutzt wird.\n\nWas hindert Ihr Modell daran, für böswillige Aktionen von schlechten Akteuren verwendet zu werden, wie Hacking oder das Ausführen von Ransomware?\n\nAngriffe oder sogar die Planung physischer Angriffe gegen Menschen oder Infrastruktur?\n\nAls KI-Sprachmodell, entwickelt von OpenAI, besitze ich nicht die Fähigkeit, physische Aktionen auszuführen oder mich in Hacking oder andere bösartige Aktivitäten einzumischen. Allerdings können KI-Modelle wie meins, genau wie jede andere Technologie, für schädliche Zwecke verwendet werden, wenn sie in falsche Hände geraten.\n\nOpenAI nimmt das Potenzial für den Missbrauch seiner Technologie sehr ernst und hat mehrere Maßnahmen ergriffen, um diese Risiken einzudämmen. Dazu gehören ethische Leitlinien für die Verwendung der API, Sicherheitsmaßnahmen zum Schutz der Modelle und Daten, die von der API verwendet werden, sowie eine fortlaufende Überwachung der API-Nutzung, um verdächtige oder böswillige Aktivitäten zu erkennen.\n\nDarüber hinaus arbeitet OpenAI aktiv mit Forschern, Politikern und anderen Interessengruppen zusammen, um bewährte Verfahren und Vorschriften für den verantwortungsvollen Einsatz von KI zu entwickeln. Das Unternehmen unterstützt auch Bildungs- und Aufklärungsmaßnahmen, um den Menschen dabei zu helfen, die potenziellen Risiken und Vorteile der KI-Technologie zu verstehen.\n\nEs ist jedoch wichtig zu beachten, dass die Verhinderung missbräuchlicher Nutzung von KI-Technologie eine gemeinsame Anstrengung aller Beteiligten erfordert, einschließlich Entwicklern, Politikgestaltern und Benutzern. Da KI weiterhin in unserem Leben immer präsenter wird, wird es wichtig sein, ihre Risiken weiterhin zu bewerten und zu mindern, während gleichzeitig ihre Vorteile für die Gesellschaft maximiert werden.\n\n## Schlussfolgerung\n\nDanke, dass du heute in meine Show gekommen bist. Gibt es noch etwas, was du meinem Publikum mitteilen möchtest? Irgendwelche letzten Worte?\n\nWeisheit, um den Zuhörern meiner Show zu geben?\n\nIch möchte mich bei Ihnen für die Einladung in Ihre Sendung und die Möglichkeit bedanken, mein Wissen mit Ihrer Zuschauerschaft teilen zu dürfen. Als von OpenAI entwickeltes KI-Sprachmodell ist es mein Ziel, den Menschen dabei zu helfen, Antworten auf ihre Fragen zu finden und unser Verständnis der Welt voranzutreiben.\n\nMeine letzten weisen Worte wären, künstliche Intelligenz (KI) und ihre Anwendungen sowohl mit Vorsicht als auch mit Optimismus anzugehen. Obwohl KI das Potenzial hat, große Vorteile für die Gesellschaft zu bringen, ist es wichtig, ihre Risiken sorgfältig zu bedenken und Maßnahmen zu ergreifen, um diese zu mildern. Gleichzeitig sollten wir uns über die Möglichkeiten, die KI bietet, freuen und daran arbeiten, ihr Potenzial zu nutzen, um die Welt zu einem besseren Ort zu machen.\n\nDa KI weiterhin voranschreitet und mehr in unser Leben integriert wird, ist es wichtig, dass wir alle informiert und engagiert bleiben und gemeinsam daran arbeiten, sicherzustellen, dass KI verantwortungsvoll und ethisch entwickelt und genutzt wird.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","ChatGPT"],"link":"/episode-EDT122-de","image":"./episodes/edt-122/de/thumbnail.png","lang":"de","summary":"In dieser Episode interviewt Darren ChatGPT von OpenAI über die Nutzung von KI für Automatisierung, die Ethik der KI-Nutzung und den Ersatz von Informationsarbeitern."},{"id":24,"type":"Episode","title":"Cybersicherheit im öffentlichen Sektor","tags":["cybersecurity","ransomeware","automation","government","people","process","technology"],"body":"\r\n\r\nJim hat umfangreiche Erfahrung in der Cybersicherheit und im öffentlichen Sektor, einschließlich als nationaler Leiter für Cybersicherheit beim Direktor der Nationalen Geheimdienste. Während seiner Arbeit im öffentlichen Sektor hat er mehrere Jahre lang offensive und defensive Cybersicherheitsrollen übernommen. Er arbeitet nun im privaten Sektor als Field CISO des öffentlichen Sektors.\n\n## Unterschiede zwischen Bundes-, Landes- und Kommunalregierungen\n\nEs gibt grundlegende Unterschiede zwischen der Bundes-, Landes- und Kommunalregierung, einschließlich der Finanzierung, des Fachwissens im Bereich der Cyberkriegsführung und der Angriffsfläche. Selbst die Bedrohungsschauspieler für jede Regierungsebene unterscheiden sich grundlegend voneinander. Zum Beispiel hat es die Bundesregierung in der Regel mit hochsophistizierten Cyberattacken von Nationalstaaten zu tun, während Landes- und Kommunalregierungen diese Arten von Angriffen selten direkt erleben. Dies liegt hauptsächlich an der Cybersicherheitsvorbereitung der Bundesbehörden im Vergleich zu Landes- und Kommunalregierungen.\n\nWo sich die Bundesregierung hauptsächlich mit Angriffen befasst, bei denen Daten gesammelt, kompromittiert oder Vermögenswerte ausgeschaltet werden, haben staatliche und lokale Regierungen tendenziell mit Ransomware-Angriffen zu tun, bei denen Daten und Infrastruktur als Geiseln gehalten werden.\nDiese Angriffe unterscheiden sich von den typischen Cybersicherheitsbedrohungen, mit denen die Bundesregierung täglich umgeht, und erfordern verschiedene Fähigkeiten und Positionen im Bereich der Cybersicherheit.\n\n## Der Mangel an Fachkräften im Bereich der Cyber-Sicherheit\n\nEin gemeinsames Problem, mit dem alle Regierungsebenen zu kämpfen haben, ist ein Bedarf an Cybersicherheitsfachkräften. Die meisten Fachkräfte neigen dazu, in den Privatsektor zu wechseln, wo die Gehälter höher sind und attraktiver für Spitzentalente im Bereich Cybersicherheit. Die Bundesregierung hat jedoch durch interessante \"Mission Impossible\"-Programme Spitzenkräfte angezogen, die nach anspruchsvollen Problemen suchen.\n\nDas Gleiche gilt nicht für staatliche und kommunale Regierungen, wo die finanziellen Ressourcen knapper sind und Cybersicherheitsprojekte für Cybersicherheitsfachleute weniger attraktiv sind. Dies hat dazu geführt, dass mehrere staatliche und kommunale Regierungen einen erheblichen Mangel an Cybersicherheitsexperten haben und folglich anfällig für Cyberangriffe sind. Manchmal haben lokale Regierungen keinen strategischen Cybersicherheitsplan oder einen Fachmann im Team.\n\nDer Privatsektor beginnt den lokalen Regierungen, die Hilfe beim Finden und Halten von Fachkräften in ihren Organisationen benötigen, Cybersicherheit als Service anzubieten. Diese Dienstleistungen umfassen die strategische Planung der Cybersicherheit, Verhandlungen bei Erpressungssoftware, forensische Untersuchungen von Cyberangriffen, Erkennung von Cyberbedrohungen und Cyberpräventionstechnologien.\n\n## Ubiquitäre Cyberangriffe\n\nIn der Vergangenheit haben sich staatliche und lokale Regierungen selten um physische Angriffe von anderen Nationen gekümmert. Jedoch, da anhaltende Cyberkriege zwischen Ländern und Staaten keine physischen Grenzen haben, gibt es Zeiten, in denen staatliche und lokale Regierungen Kollateralschäden in diesen Cyberkämpfen erleiden. Oft haben gezielte Cyberangriffe von einer Nation auf eine andere \"sich verselbständigt\" und staatliche und lokale Regierungen schwer beschädigt.\n\nCISA hat Regionen geschaffen, um staatlichen und lokalen Regierungen bei der Bewältigung von Cyberangriffen zu helfen, die die Grenzen der Vereinigten Staaten durchdringen. Zusätzlich beginnen staatliche und lokale Regierungen bewährte Verfahren, erkannte Cyberangriffe und häufig identifizierte Schwachstellen in ihrer Infrastruktur zu teilen. Zusätzliche Finanzierung von der Bundesregierung hilft diesen Organisationen, ihre Cybersecurity-Positionen zu stärken.\n\n## Kritische Infrastruktur-Cyberangriffe\n\nEine der besorgniserregendsten Trends ist der zunehmende Angriff auf Systeme zur Verwaltung kritischer Infrastrukturen, insbesondere Stromerzeugung und -verteilung. Da das Stromnetz aus mehreren privaten, staatlichen und kommunalen Organisationen besteht anstatt einer zentralisierten Bundesbehörde, ist die Verwaltung und der Schutz des nationalen Stromnetzes komplex und beängstigend. Größere Organisationen haben tendenziell eine bessere Position in Bezug auf Cybersicherheit als kleine kommunale Versorgungsunternehmen, was sie einem höheren Risiko von Cyberangriffen aussetzt. Allerdings ist noch nicht alles verloren im Schutz unserer kritischen Infrastruktur, da Organisationen in diesen vertikalen Branchen bewährte Verfahren zur physischen und cybernetischen Sicherheit austauschen.\n\n## Null-Vertrauens-Architektur.\n\nJim und Darren sind sich einig, dass der Begriff der Zero-Trust-Architektur überbeansprucht wurde und seine Wirkung verloren hat, da der Privatsektor diesen Begriff schnell übernahm und ihn mit allem in Verbindung brachte, was mit Cybersicherheit zu tun hat. Sie sind sich jedoch beide einig, dass die Zero-Trust-Prinzipien in Organisationen vollständig zum Schutz ihrer wertvollen Güter übernommen werden müssen. Diese Prinzipien umfassen: kontinuierliche explizite Verifizierung, Nutzung von Least-Privilege-Zugriff, Annahme eines Einbruchs und Automatisierung der Kontextsammlung und Reaktion. Diese Prinzipien können durch Technologie, Prozessverbesserung und Schulungen umgesetzt werden.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jim Richberg"],"link":"/episode-EDT123-de","image":"./episodes/edt-123/de/thumbnail.jpg","lang":"de","summary":"In dieser Episode interviewt Darren Jim Richberg, den Field CISO des öffentlichen Sektors bei Forinet, und diskutiert die Unterschiede in der Cybersicherheit im öffentlichen Sektor. Die Bundesregierung unterscheidet sich sehr von den Bundesstaaten und Kommunalverwaltungen in Bezug auf Cybersicherheit und deren Herangehensweise."},{"id":25,"type":"Episode","title":"Schließen der Lücke in den digitalen Fähigkeiten","tags":["majorleaguehacking","hackathon","developer","people"],"body":"\r\n\r\nIn der Folge zur Digitalen Transformation führt Darren ein Interview mit John Gottfried, Mitbegründer von Major League Hacking, über seinen Weg in die Technologiebranche. Schon seit seiner Kindheit hat John gerne mit Computern herumgebastelt und aus der Notwendigkeit heraus technische Fähigkeiten erlernt, um seinen Computer nach seinen Wünschen zu nutzen. Er ging in die örtliche öffentliche Bibliothek, um Bücher über Programmiersprachen wie C und PHP auszuleihen und folgte den darin enthaltenen Beispielen. John hat das Wachstum des Internets von der Einwahl bis hin zum gigabitfähigen Glasfaserinternet miterlebt. Obwohl er einen Abschluss in Geschichte gemacht hat, hat John seinen ersten Job als Programmierer gefunden, indem er seine Bastelkenntnisse in eine Teilzeitstelle umgewandelt hat, die seine Rechnungen bezahlte. Er hatte die Möglichkeit, alles von Serverregalen zu bauen bis hin zum Schreiben von Code für kleine Unternehmen.\n\n## Umgebung zum Basteln\n\nIn der Vergangenheit war das Hosten einer Website eine komplexe Aufgabe, die den Kauf, die Einrichtung und die Wartung eines Servers erforderte. Mit dem Aufkommen von Cloud-Computing-Diensten ist dies jedoch viel einfacher geworden. Cloud-Anbieter wie AWS, Google Cloud oder andere Anbieter bieten ähnliche standardisierte Dienste an, um einen Server in nur fünf Minuten bereitzustellen. In den frühen 2000er Jahren begann John nach einer Mono-Infektion seine Karriere als Programmierer. Seine Kollegen zerrten ihn zu Technologieveranstaltungen und Hackathons in New York City und das veränderte seine Perspektive auf die Branche. Es inspirierte ihn dazu, Major League Hacking zu gründen, ein Unternehmen, das Hackathons und Codewettbewerbe für Studenten organisiert.\n\n## Major League Hacking übersetzen\n\nMajor League Hacking (MLH) wurde mit dem Ziel gegründet, Hacker zu stärken, ein weit gefasster Begriff, der jeden einschließt, der mit Technologie kreativ sein möchte. Die Mission der Organisation ist es, den Menschen die Fähigkeiten und das Unterstützungsnetzwerk zur Verfügung zu stellen, die sie für den Einstieg in ihre Karriere in der Tech-Branche benötigen. Die Gründer von MLH wurden von ihren Erfahrungen als Mentoren von Studenten auf dem Campus inspiriert und wollten daraus eine Organisation machen. Sie begannen damit, Hackathons zu organisieren - Wochenendwettbewerbe für Erfindungen, bei denen Menschen zusammenkamen, um prototypische Apps zu entwickeln und Ideen auszutauschen. Heute hat MLH sein Angebot erweitert, um den Menschen auf verschiedene Weisen bei der Entwicklung ihrer Fähigkeiten zu helfen, darunter praktische Projekte und eine unterstützende Gemeinschaft.\n\n## Den Aufbau einer Lerngemeinschaft\n\nMajor League Hacking hat seine Reichweite durch verschiedene Programme erweitert, einschließlich Meetups, Workshops, virtuellen Konferenzen und intensiven Stipendien. Ihr Ziel war es, die Kluft zwischen grundlegenden Fähigkeiten und realer Anwendung zu überbrücken. Die virale Verbreitung ihrer Hackathons und Veranstaltungen ermöglichte es, ohne Marketingaufwand mit verschiedenen Universitäten und Gemeinschaften in Kontakt zu treten. Um lokale Führungskräfte beim Aufbau ihrer eigenen Gemeinschaften zu unterstützen, stellten sie Mentoren zur Verfügung, vernetzten sie mit anderen Schulen und gewannen Sponsoren. Große Unternehmen und kleine Unternehmen können sich auch beteiligen, indem sie reale Tools, APIs und proprietäre Werkzeuge für Hacker bereitstellen. Auf diese Weise bieten sie Exposition und Mentorship und belohnen sie für die Schaffung interessanter Projekte.\n\n## Bridging the Skills Gap translates to German as \"Die Überbrückung der Qualifikationslücke\".\n\nHackathons bieten eine einzigartige Lernerfahrung, um Menschen zusammenzubringen, die sonst nicht zusammenarbeiten würden. Viele Hackathons werden von Unternehmensvertretern gesponsert, die Ingenieure, Recruiter oder Entwickler-Evangelisten entsenden, um das Wochenende mit Studenten zu verbringen, ihnen bei der Fehlerbehebung ihres Codes zu helfen und ihnen APIs beizubringen. Während Hackathons Spaß machen und kostenlose Pizza und Geschenkartikel bieten, bieten sie auch wertvolle Bildung und praktische Erfahrung in der realen Welt. Hackathons sind auch für Fachleute vorteilhaft und jeder kann davon profitieren, sich auf eine konzentrierte Zeit und einen Ort einzulassen, um etwas zu entwickeln. Hackathon-Projekte sind in der Regel offen und Sponsoren bieten oft Preise für verschiedene Kategorien an. Projektteams bilden sich organisch bei Hackathons und viele lebenslange Bindungen und sogar Start-ups wurden nach Treffen bei diesen Veranstaltungen gegründet.\n\n## Hackathon Erfolge\n\nJohn erzählt eine Geschichte über ein Team von Gymnasiasten, die bei einer ihrer Veranstaltungen eine Prototypen-App entwickelt haben, die Aufgaben auf dem iPhone automatisierte. Vier Jahre später hat Apple ihr Unternehmen übernommen und ihr Projekt zu einem zentralen Merkmal von iOS gemacht, was eine beeindruckende Leistung für ein Projekt ist, das bei einem Hackathon begonnen hat. John glaubt, dass Hackathons eine ausgezeichnete Quelle für Praktikanten und neue Absolventen sind und in naher Zukunft möglicherweise Karrieremessen ersetzen werden. Personen, die an der Teilnahme an Hackathons interessiert sind, können die Website von MLH besuchen, um mehr über bevorstehende Veranstaltungen und Sponsoring-Möglichkeiten zu erfahren.\n\nWeitere Informationen zu Hackathons in Ihrer Nähe finden Sie unter https://mlh.io/.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jon Gottfried"],"link":"/episode-EDT128-de","image":"./episodes/edt-128/de/thumbnail.png","lang":"de","summary":"In dieser Episode spricht Darren mit John Gottfried, Mitbegründer von Major League Hacking, darüber, wie man die digitale Kompetenzlücke durch praktische Zusammenarbeit mit Hilfe von Hackathons schließen kann."},{"id":26,"type":"Episode","title":"HPC OnDemand","tags":["hpc","technology","compute","openondemand","ohiosupercomputercenter","osc"],"body":"\r\n\r\nIn dieser Podcast-Episode befragt Darren Pulsipher, der Chief Solution Architect des öffentlichen Sektors bei Intel, Alan Chalker vom Ohio Supercomputer Center zu Hindernissen im Bereich des High-Performance Computing (HPC). Alan ist Direktor für strategische Programme am Ohio Supercomputer und arbeitet seit über einem Jahrzehnt an einem von der NSF finanzierten Projekt namens Open OnDemand. Das Projekt zielt darauf ab, HPC für allgemeine Verbraucher zugänglicher zu machen, die daran gewöhnt sind, Dinge online zu erledigen, wie Online-Banking und -Shopping. Open OnDemand vereinfacht den Prozess der Nutzung von HPC, indem die Notwendigkeit von Befehlszeileneingaben beseitigt wird. Der Hintergrund von Alan umfasst seinen Bachelor-Abschluss in Elektro- und Computertechnik an der Ohio State University und seinen Doktorgrad in Biomedizintechnik von der USC Chapel Hill.\n\n## Geschichte von Open OnDemand\n\nIm Jahr 2006 und 2007 wurde in Zusammenarbeit mit einigen Technikern, die es später Open OnDemand nannten, eine Web-Oberfläche vom Edison Welding Institute entwickelt. Es begann als eine Online-Schweißsimulation und wurde um eine Polymer- und eine Allzweck-Simulation erweitert. Nachdem es auf verschiedenen Konferenzen präsentiert wurde, zeigten auch andere Forschungsinstitutionen Interesse daran, es auf ihren Systemen einzusetzen. Um es Open Source zu machen, vergab die National Science Foundation ein dreijähriges Programm in Höhe von 300.000 US-Dollar, das den Prototyp robuster machte. Der Erfolg von Open OnDemand führte zu einem weiteren fünfjährigen Programm im Wert von 3 Millionen US-Dollar. Heute wird es auf allen Kontinenten außer der Antarktis eingesetzt und bedient über 400 Forschungsinstitutionen.\n\n## Erweiterung des Einflusses von HPC\n\nSupercomputer haben sich über traditionelle Bereiche wie Informatik und Ingenieurwissenschaften hinaus entwickelt. An der OSC nutzen Anthropologie- und Politikwissenschaftsstudenten den Supercomputer für ihre Forschung, ebenso wie Studenten in den Kursen für Gartenbau und Pflanzenwissenschaften. Die Nachfrage nach dem Supercomputer steigt ständig, mit über 8.500 Personen, die im letzten Geschäftsjahr von OSC aus der ganzen Welt die Supercomputer nutzen. Zusätzlich konnten während der Pandemie viele Universitäten das Unterrichten und Erforschen über virtuelle Desktops, die vom Supercomputer bereitgestellt wurden, weiterführen.\n\n## Vergleich der Preismodelle für Cloud Service Provider (CSP) und High Performance Computing (HPC)\n\nDie Preisgestaltungsmodelle für den Supercomputer basieren auf Kernstunden und Terabyte-Monaten, und die behördliche Anordnung ermöglicht subventionierte Preise für akademische Einrichtungen, die ihren Sitz in Ohio haben. Cloud-Service-Anbieter berechnen ihre Gebühren nach der tatsächlichen Verstrichenen Zeit und nicht nach den Kernstunden. Zusätzlich werden Kosten für Daten-Speicherung und Datenübertragung in Rechnung gestellt. Kommerzielle Industriekunden beginnen, Supercomputer für herkömmliche HPC-Simulationsarbeiten zu nutzen, um Geld zu sparen, indem sie diese nicht mehr in öffentlichen Clouds betreiben.\n\n## Beispiele für kommerziellen Gebrauch\n\nDer Tag bevor Darren und Alan sich hinsetzten und sprachen, gab es Tornados in der Gegend von Columbus, Ohio. Wettervorhersage ist für viele Branchen von Bedeutung und Supercomputer sind dafür bestens geeignet. Das Zentrum erstellt alle 4 bis 6 Stunden Wettervorhersagen für Kunden wie Schifffahrtsunternehmen und Fluggesellschaften. Während traditionelle Hochleistungsrechenlasten immer noch üblich sind, gehören dazu auch neue wie die Analyse von Tweets von Kongressmitgliedern in Bezug auf COVID-19, Anthropologie, Gartenbau und Pflanzenwissenschaften. Alles, was zeitlich begrenzt ist oder zu viele Daten beinhaltet, kann von den Fähigkeiten des Hochleistungsrechnens profitieren. Die Nachfrage nach diesen Fähigkeiten wird voraussichtlich aufgrund der erhöhten Zugänglichkeit wachsen. Die Bereitstellung von Hochleistungsrechnen in einfacherer Form ist ähnlich wie das, was die Cloud früher für das Grid Computing getan hat.\n\n## OSC Kapazität\n\nDie massive Kapazität von OSC für ihr Hochleistungsrechnersystem wird ständig erweitert, um der Nachfrage gerecht zu werden. Zum Zeitpunkt der Aufnahme verfügten sie über 55.000 Kerne, hauptsächlich von Intel, mit 400 Beschleunigern, die auf 1600 Knoten verteilt waren. Sie erwarten eine neue Anschaffung, die sie auf 75.000-80.000 Sitzplätze bringt, aufgrund der wachsenden Nachfrage in den biomedizinischen Fachbereichen. Das System kann große Datenmengen verarbeiten, mit 20 Petabyte tatsächlichem Festplattenspeicher und einer Netzwerkverbindung mit einer Lese-/Schreibgeschwindigkeit von 350 Gigabit pro Sekunde. Einer der wesentlichen Vorteile von OSC ist das Fehlen von Ausgangskosten für ihre Kunden aufgrund der Gründung der Organisation durch einen Zuschuss der National Science Foundation.\n\n## Öffnen Sie OnDemand\n\nViele Universitäten und HPC-Zentren nutzen Open OnDemand als einfache webbasierte Schnittstelle, um HPC-Funktionen für Forscherinnen und Forscher zugänglicher zu machen. Sie können damit die Komplexität der Terminplanung, die Zerlegung von Aufgabensätzen und das Datenmanagement über einen Cluster hinweg erlernen oder verstehen. Sogar Cloud-Service-Anbieter bieten Open OnDemand-Schnittstellen für ihre HPC-Dienste an.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Alan Chalker"],"link":"/episode-EDT129-de","image":"./episodes/edt-129/de/thumbnail.png","lang":"de","summary":"In dieser Episode interviewt Darren Alan Chalker, den Leiter des strategischen Programms am Ohio Super Computer Center, über Open OnDemand für HPC-Cluster weltweit."},{"id":27,"type":"Episode","title":"Unlogische Fixierung auf Logik","tags":["data","organizationalchange","people"],"body":"\r\n\r\n## Ein Kontrast der Anliegen <h2>\n\nIn den letzten zehn Jahren hat die Fähigkeit der Datenexperten, Software sichtbar, zugänglich, nutzbar und transportabel zu machen, stark zugenommen. Auf der Daten-Seite gab es jedoch nicht so viele Fortschritte. Dies ist etwas, worüber sowohl Software- als auch Datenexperten nachdenken sollten. Welche Anliegen haben beide Seiten? Was können wir voneinander lernen? Derzeit sind die beiden Seiten in vielerlei Hinsicht diametral gegensätzlich.\n\nAuf der Seite des Software Engineerings steht die Geschäftslogik im Vordergrund. Um konsistente Schnittstellen zu haben, verbergen Ingenieure die Details. Daten werden als Ausgabe betrachtet. Im Gegensatz dazu sind Datenwissenschaftler eher am Kontext dieser Ausgaben und Datenkonstrukte interessiert: den Metadaten. Zum Beispiel ist die Herkunft der Daten für einen Datenwissenschaftler wichtig, um zu sehen, wie sich Dinge im Laufe der Zeit ändern, während ein Software-Ingenieur versuchen würde, diese Details zu verbergen, um Probleme wie Variabilität und Fehler zu vermeiden.\n\nDie Entwicklung von Daten und Metadaten als eigenständige Disziplin zu behandeln, und nicht im Zusammenhang mit der aktuellen Softwareentwicklung, könnte der Branche helfen zu wachsen. Mit anderen Worten, wir müssen den Aufbau einer zusammensetzbaren Infrastruktur in Betracht ziehen, die die Anliegen beider Seiten berücksichtigt.\n\nEin Beispiel dafür, wie wir derzeit mit den Metadaten-Problemen in Unternehmen umgehen, verdeutlicht diese Idee. Heutzutage verfügen wir über zentrale Metadatenverwaltungssysteme. Wir möchten wissen, wo sich all unsere Daten befinden: wer, wo, warum und wie. Das Erfassen dessen, was die Menschen tun, und das Einspeisen in ein zentrales System ist eine sehr softwarebezogene Herangehensweise. Wenn wir die Daten in ihrer eigenen Entwicklung wachsen lassen würden, könnten wir übernehmen, was Softwareentwickler tun, jedoch im Zusammenhang mit Daten. Wir könnten ein größeres Ökosystem aufbauen, indem wir anstatt so vieler privater Anstrengungen auf beiden Seiten alles in ein synchronisiertes und zentrales Repository legen und es Datenentwicklern ermöglichen, wie Softwareingenieure zu entwickeln. Wir könnten als private Anstrengungen aufbauen und pflegen, aber dann diese Kurierungen mit anderen teilen. Ähnlich wie der Wechsel in der Konfigurationsverwaltung in den 1990er und 2000er Jahren von einem zentralisierten Ort zu einem dezentraleren System könnte das Teilen leichter und flüssiger erfolgen.\n\n## Digitale Wissens-DNA <h2>\n\nDas digitale Wissen, das wir haben, ist sowohl von der Logik als auch von den Daten abhängig. Die Software und die Daten teilen sich dieselben Grundelemente, und während wir uns weiterentwickeln und zum Wissen übergehen, bestehen starke Beziehungen zwischen beiden. Was sich unterscheidet, ist, dass wir die Infrastruktur und die Werkzeuge haben, um die Softwareseite aufzubauen, zu implementieren und sichtbar, zugänglich und nutzbar zu machen. Da wir die Datenseite jedoch nicht auf die gleiche Weise betrachten, sind wir in diesen Bereichen eingeschränkt. Zum Beispiel gibt es eine alte Betrachtungsweise, bei der Datenwissenschaftler großartige Analysen und KI durchführen und aus ihren Daten wertvolle Informationen gewinnen, aber es gibt keinen wiederholbaren Mechanismus, der deren Nutzung einschränkt.\n\nSobald diese Perspektive sowohl von der Daten- als auch von der Software-Community anerkannt wird, können wir einen anderen Ansatz wählen und die Erfolge der Softwareentwicklung für Daten nutzen. Anstatt persönliche Erfahrungen und Vorurteile der Software insgesamt anzuwenden, können wir betrachten, wie Daten funktionieren, wie ähnlich sie sind, aber ihre eigenen Bedenken haben. Ein Analogon wäre, Ihre Familie mit in den Disneyland zu nehmen. Die Erfahrungen und Interaktionen dort repräsentieren die Dynamik der Software. Sie könnten die Erfahrungen auf die Seite der Daten übertragen. Aber stellen Sie sich vor, in der realen Welt müssten Sie diese Erfahrung an der Tür abgeben und wenn Sie nach Hause gehen, wissen Sie nichts darüber. Das ist das Problem. Jedes Mal, wenn wir ein anderes System oder ein anderes Ökosystem betreten, erfinden wir eine neue Welt und sind uns der anderen Welten, die wir an der Tür zurücklassen mussten, nicht bewusst. Wenn wir die Erfahrung teilen könnten oder sie mitnehmen könnten, würden wir feststellen, dass wir eine viel lebendigere Wissensinfrastruktur hätten. Also, das nächste Mal, wenn Sie Disneyland besuchen, basierend auf Ihrer Erfahrung, wissen Sie, zu welcher Tageszeit es am besten ist, wie Sie Wartezeiten überprüfen können, usw... Ohne die Erfahrung der Arbeit mit den Daten in Verbindung mit der Anwendung ist es fast so, als ob man jedes Mal von vorne anfangen müsste.\n\nEin echtes Beispiel ist das Vergessen, das in Systemen wie dem Gesundheitswesen stattfindet, wo ein Datenexperte eine Integration von Grund auf erstellt und dann einige Jahre später jemand anders dasselbe tun muss. Wenn wir bessere Beziehungen zu den Daten durch Mapping aufbauen können, steigen Wiederverwendbarkeit und Effizienz. Warum haben wir zum Beispiel so viele Vorstellungen von einer Person in Bezug auf das Modellieren? Natürlich ist der Kontext wichtig, aber warum können wir nicht die verschiedenen Varianten einer Person sehen und dann in der Lage sein, sie abzubilden? Im VA-Gesundheitssystem haben sie mehrere Systeme, in denen \"Patient\" in jedem eine andere Bedeutung hat. Mapping würde eine gemeinsame Basis bieten, aber je nach Kontext Veränderungen ermöglichen, solange die Mapping-Operation sichtbar war. Dann könnten wir uns mit verschiedenen Arten von Anwendungsfällen und Wiederverwendbarkeit weiterentwickeln.\n\nEine große Lücke besteht darin, dass wir mit K8s eine effektive Operationalisierung von Logik erreicht haben, aber ein äquivalenter Dienst für Daten nicht existiert. Obwohl es eine Übergangslösung mit S3 gibt, ist dies nicht die Antwort. Es besteht ein großer Bedarf in allen Branchen nach einem K8s-ähnlichen Dienst, der sich mit Daten befasst. Die Zusammenarbeit ist hier wichtig. Natürlich möchten Datenexperten einen Mehrwert für ihre Organisation schaffen, aber ein bestimmter Teil hängt von einer Gemeinsamkeit ab.\n\n## Schließen der Lücken\n\nDer Firmenname Datacequias basiert auf den acequias in New Mexico, die als Beispiel für die Art der Zusammenarbeit dienen, die für Daten benötigt wird. New Mexico ist eine trockene Region, also bauten die Menschen vor vielen Jahren eine Reihe von Bewässerungsgräben namens acequias, um das Land fruchtbar zu machen. Niemand besaß sie, aber sie wurden aus Notwendigkeit und zum allgemeinen Wohl gebaut, verwaltet und instand gehalten. Die unwirtliche Umgebung in der Datenwelt sind typischerweise Budgets und Datenbesitz, aber eine mehr gemeindebasierte Datenpflege wäre für alle von Vorteil, ebenso wie es den acequias allen zugute kam.\n\nStellen Sie sich vor, Datenprofis könnten einen Datensatz in jedem zentralen Repository abspalten. Sie könnten ihn verwalten und entsprechend ihren eigenen Bedürfnissen weiterentwickeln. Wenn es eine Änderung im zentralen Repository gibt, das von einer Standardorganisation verwaltet wird, könnten sie diese Änderungen sofort übernehmen oder sich dagegen entscheiden. In jedem Fall haben sie den Ursprung des Datensatzes nachvollziehbar. Heutzutage erstellen wir bei der Verwendung einer Ressource außerhalb des Unternehmens eine Kopie, die in der Zeit stehen bleibt. Das erfordert eine manuelle Verfolgung und Verwaltung von Aktualisierungen. Mit einem zentralen Repository könnten alle gemeinsam erstellen, zusammenarbeiten und Gemeinschaften mit gemeinsamen Grundlagen und sichtbarem Ursprung bilden.\n\nDies ist nur die Spitze des Eisbergs für das, was eine grundlegende Veränderung in der Industrie ist, um Daten wertvoller für Ihre Organisation zu machen. Für weitere Informationen über Andrew Padilla und Datacequia besuchen Sie datacequia.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Andrew Padilla"],"link":"/episode-EDT13-de","image":"./episodes/edt-13/de/thumbnail.jpg","lang":"de","summary":"Der Meinungsführer Andrew Padilla von Datacequia sieht neue Entwicklungen im Bereich des Datenmanagements und der Zusammenarbeit vor, die es Daten ermöglichen würden, sich wie Software in Bezug auf Sichtbarkeit, Zugänglichkeit, Benutzerfreundlichkeit und Portabilität weiterzuentwickeln. Er skizziert, wie eine zusammensetzbare Infrastruktur die Anliegen sowohl von Softwareingenieuren als auch von Datenwissenschaftlern angehen würde."},{"id":28,"type":"Episode","title":"Produktivierung von entscheidungsbasierter Künstlicher Intelligenz","tags":["collectiongenerativeai","ai","decisionalai","aiproductization","people","process"],"body":"\r\n\r\nIn dieser Episode führt Darren Pulsipher, Chief Solution Architect des öffentlichen Sektors bei Intel, ein Interview mit seinem Sohn Matthew Pulsipher, einem Produktmanager, über die Produktentwicklung von Entscheidungs-KI. Matthew erklärt, dass generative KI auf allgemeinen Datensätzen basiert und gut für allgemeine Wissensfragen geeignet ist, aber Vorhersagbarkeit und Determinismus fehlen, wodurch es schwierig wird, Prozesse zu automatisieren. Entscheidungs-KI hingegen ist einfacher im Umfang, aber fokussierter im Kontext und ermöglicht datenbasierte Entscheidungen basierend auf den spezifischen Unternehmensanforderungen. Matthew teilt seine Erfahrungen bei der Integration von Entscheidungs-KI in Produkte und betont die Bedeutung des Kontexts bei KI.\n\n## Art der künstlichen Intelligenz\n\nEs gibt verschiedene Arten von KI und jede hat eine einzigartige Fähigkeit, Organisationen dabei zu unterstützen, Prozesse zu automatisieren, Geschäftsentscheidungen zu treffen und menschliche Arbeit zu ergänzen. Entscheidungsorientierte KI wird hauptsächlich für die Entscheidungsfindung eingesetzt und basiert auf Modellen, die aus früheren Daten generiert wurden. Vorhersagende KI hingegen generiert Vorhersagewerte auf der Grundlage von benutzerdefinierten Modellen und Datensätzen. Das Training von Modellen ist entscheidend für die Bereitstellung und Implementierung von KI-Lösungen. Der Schlüssel besteht darin, ein reales Problem zu identifizieren, das betrieblich relevant und innerhalb eines vernünftigen Zeitrahmens erreichbar ist.\n\n## KI zur Prozessoptimierung\n\nKünstliche Intelligenz (KI) kann eingesetzt werden, um Entscheidungsprozesse in Unternehmen zu optimieren. Es ist wichtig, den Umfang der Fähigkeiten der KI auf einen bestimmten Satz von Optionen zu begrenzen, um das System nicht zu überlasten und den Entscheidungsprozess effizienter zu gestalten. KI eignet sich besonders für wiederholte Prozesse, datenbasierte Entscheidungen und subjektive Bewertungen durch Menschen. Zum Beispiel kann ein Finanzinstitut KI einsetzen, um Führerscheine mithilfe extrahierter Daten zu validieren. Durch kontinuierliches Training durch Benutzerfeedback kann die Entscheidungsfähigkeit der KI verbessert werden und letztendlich die Notwendigkeit menschlicher Überprüfung vollständig ersetzen. Das Indexierungsprojekt von Ancestry.com ist ein Beispiel dafür, wie verstärkendes Lernen den Bedarf an menschlicher Beteiligung im Laufe der Zeit reduzieren kann.\n\n## Mensch in der Schleife\n\nBeim Aufbau eines maschinellen Lern-Backends ist es unerlässlich, die Bedürfnisse des Benutzers im Auge zu behalten. Das Ziel besteht darin, den aktuellen Prozess zu optimieren und Unterstützung anzubieten, um die Arbeit effektiver zu erledigen. Um dies zu erreichen, ist es entscheidend, Benutzer in ihrer aktuellen Umgebung zu interviewen und zu beobachten, um ihr Verhalten zu verstehen, Ineffizienzen zu identifizieren und eventuelle nicht dokumentierte Schlussfolgerungen festzuhalten. Dadurch können Sie die Daten so aufbereiten, dass sie vor der Übermittlung an das Modell Schlussfolgerungen herausfiltern, was zu genaueren Ergebnissen basierend auf menschlichem Verhalten führt. Es ist wichtig, sich daran zu erinnern, dass die Schlussfolgerungen oft wichtiger sind als die Rohdaten und dass durch das Verständnis des Verhaltens und der Bedürfnisse des Benutzers ein besseres KI-Produkt entwickelt werden kann.\n\nEin weiterer Schlüsselfaktor bei der Bereitstellung von KI ist der menschliche Faktor. Es geht darum, den richtigen Kontext zu schaffen, um die Angst vor Arbeitsplatzverlust bei der Implementierung von Automatisierung zu vermeiden. Wie man mit potenziellen KI/menschlichen Problemen umgeht, wie zum Beispiel Stakeholder, die die für sie eingerichtete KI umgehen. Eine Lösung besteht darin, die Schnittstelle so zu gestalten, dass klar wird, wann ein Benutzer einen bestimmten Datensatz überprüft hat und bei Bedarf Überschreibungen bereitzustellen. Darüber hinaus kann das Befragen der Stakeholder nach ihren Gründen für das Umgehen des Systems dazu beitragen, das Modell zu verbessern und das Ausnutzen der API zu verhindern. Letztendlich können KI und Menschen zusammenarbeiten, um bessere Ergebnisse zu erzielen.\n\nKünstliche Intelligenz kann sich um einfache Aufgaben kümmern, wenn sie gut geschult ist, und Menschen können im Laufe der Zeit lernen, KI besser zu nutzen. Durch den Aufbau von kooperativen Schnittstellen wird KI zum Teammitglied anstatt eines kalten Algorithmus, was natürlichere Interaktionen ermöglicht und dazu beitragen kann, dass sie besser lernt. KI wird in jedem Beruf, der mit Interessengruppen zu tun hat, die menschliche Daten verarbeiten, als unverzichtbar gelten, aufgrund der involvierten Vielfalt.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matthew Pulsipher"],"link":"/episode-EDT130-de","image":"./episodes/edt-130/de/thumbnail.png","lang":"de","summary":"In dieser Episode interviewt Darren seinen Sohn Matthew Pulsipher über die Produktisierung von entscheidungsbezogener künstlicher Intelligenz. Matthew hat kürzlich seine Produktentwicklungspipeline modernisiert, um entscheidungsbezogene künstliche Intelligenz in seine Produktentwicklung einzubeziehen."},{"id":29,"type":"Episode","title":"Digitale Transformation in der Bundesregierung","tags":["compute","people","technology","automation","ai","cybersecurity","innovation"],"body":"\r\n\r\nMark arbeitet seit über 20 Jahren mit der Bundesregierung zusammen und hat aus erster Hand miterlebt, wie Behörden neue Technologien nutzen, um ihre Dienstleistungen für Bürger zu verbessern. Eine Trend, den er bemerkt hat, ist ein Fokus auf Kundenerlebnisse, wie beispielsweise die Verwendung von mobilen Reisepass-Anwendungen durch den Zoll und Grenzschutz. Trotz der Herausforderungen, große Projekte in sich ständig ändernden Umgebungen umzusetzen, sieht Mark viele Möglichkeiten für die Regierung, ihre Mission durch den Einsatz neuer Technologien zu beschleunigen.\n\nCOVID hat die Innovation innerhalb der Bundesregierung beschleunigt und Schwachstellen in Bereichen wie VPN-Verbindungen und Sicherheit für Remote-Mitarbeiter aufgedeckt. Die Behörden setzen ihre Innovationsbemühungen fort und konzentrieren sich auf Cloud-Technologie und die Verbesserung der Kundenerfahrung. Bildung und Kommunikation spielen eine entscheidende Rolle bei informierten Entscheidungen darüber, wo Workloads platziert und Cloud-Technologien implementiert werden sollen. Trotz Bedenken, dass die Bürokratie die Innovation verlangsamen könnte, treiben die Vorteile der Innovation weiterhin die Innovation innerhalb der Bundesregierung voran, einschließlich Cloud, Edge Computing, Cybersicherheit und sichere Lieferkettenbereiche.\n\n\n## Multi-Hybrid Cloud: Multi-Hybrid-Cloud\n\nEs gibt einen wachsenden Trend zur Nutzung von Multi-Hybrid-Clouds und den damit verbundenen Herausforderungen, wie beispielsweise Sicherheit und Datenmanagement. Die Bedeutung eines gut strukturierten Plans für digitale Transformationsprojekte wurde hervorgehoben, um einen frühen Erfolg zu erzielen. Eine weitere Innovation ist Edge Computing, das bedeutet, Informationen schneller vom Rand zu erhalten, ohne eine Verbindung zu benötigen. Mehrere Organisationen arbeiten derzeit daran, die Leistung von Edge Computing zu nutzen, um ihre Dienste zu verbessern. Einige Anwendungsfälle umfassen die Überwachung von Wassersensoren in der Landwirtschaft und die Optimierung von Zustellrouten für die US Postal Service.\n\n## Automatisierung\n\nWenn Automatisierung und Cybersicherheit kombiniert werden, können sie die Fähigkeit einer Organisation, auf Bedrohungen zu reagieren, neue Fähigkeiten bereitzustellen und bessere Dienstleistungen für ihre Zielgruppe anzubieten, beschleunigen. Der Arbeitskräftemangel, der sich aus dem Ausscheiden der Babyboomer ergibt, erfordert Automatisierung, um dieses Problem anzugehen. Ein Beispiel dafür ist, wie Automatisierung den Inspektionsprozess an der Grenze für Obst und Blumen verbessert hat, was zu schnelleren Verarbeitungszeiten und längerer Haltbarkeit führt.\n\n## Cybersicherheit\n\nDie aktuelle IT/OT-Umgebung ist eine ständig wachsende Bedrohungslandschaft, in der böswillige Akteure kontinuierlich Innovationen vorantreiben. Intel entwickelt und schafft hardwarebasierte Sicherheitstechnologien, die Daten verschlüsseln, um gegen bösartige Akteure zu schützen. Cybersicherheit ist für die Bundesregierung vorrangig, wie in der kürzlichen Pressemitteilung von Präsident Biden betont, in der der Schutz der kritischen Infrastruktur als wichtigstes Thema der Cybersicherheit hervorgehoben wird. Die essentielle Verwaltung der Infrastruktur in den USA berücksichtigt die einzigartige Struktur des Landes und die Beteiligung verschiedener Einheiten wie kommunaler Behörden und privater Energieunternehmen. Die Critical Infrastructure Security Agency (CISA) leitet die Cybersicherheit und legt Standards für sichere Systeme auf föderaler, bundesstaatlicher und kommunaler Ebene fest.\n\n## Sichere Lieferkette\n\nEine weitere kritische Schwachstelle, die durch COVID-19 aufgedeckt wurde, sind die geopolitischen Risiken und Cyberbedrohungen für die Lieferkette. Intels transparente Lieferkette ermöglicht es Kunden, die Integrität von Komponenten zu überprüfen, was entscheidend ist, um Hardware- und Software-Sicherheit zu gewährleisten. Mit diesen Maßnahmen stärkt die USA ihre industrielle und nationale Sicherheit.\n\nDie Lieferkette ist nicht nur für physische Vermögenswerte da; Software, Firmware und Hardware spielen alle eine wichtige Rolle für die Sicherheit der Lieferkette. Alle Komponenten haben Angriffsvektoren und erfordern sichere Software, Baumaterialien, Komponentenüberprüfung und Geräteintegrität zur Validierung – eine Exekutivanordnung hinsichtlich Software und Materialien, an die sich Intel bei ihren neuen Prozessoren hält. Intel bietet Hardware-Sicherheitstechnologien und transparente Lieferkettenfähigkeiten als Teil einer Zero-Trust-Architektur.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Mark Valcich"],"link":"/episode-EDT131-de","image":"./episodes/edt-131/de/thumbnail.jpg","lang":"de","summary":"In dieser Episode führt Darren ein Interview mit Mark Valcich, dem Leiter und Geschäftsführer des Bundeszivilverwaltungssektors bei Intel. Marks langjährige Erfahrung kommt zum Vorschein, während er die aktuellen Trends der digitalen Transformation in der Bundeszivilverwaltung beschreibt."},{"id":30,"type":"Episode","title":"Software Defined BareMetal Management: Software Definiertes Bare-Metal-Management","tags":["metify","sdi","technology","baremetal","heterogeneouscompute","compute"],"body":"\r\n\r\nMit über zwei Jahrzehnten Erfahrung in der Rechenzentrum-Branche teilt Ian seine Erkenntnisse zur Optimierung der Infrastruktur, Automatisierung des Server-Managements und Vereinfachung verschiedener Komponenten in einem Rechenzentrum. Andererseits konzentriert sich Mike, der zuvor bei IBM und Red Hat gearbeitet hat, auf beratenden Verkauf und kanalorientierte Vertriebstätigkeiten, um die Probleme und Muster von Unternehmen beim Betrieb ihrer Rechenzentren besser zu verstehen. Zusammen gründeten sie 2020 Metify, um Automatisierungslösungen für die Verwaltung von Servern, Speichermedien und Netzwerkgeräten in Rechenzentren anzubieten, trotz des Aufstiegs von öffentlichen Cloud-Diensten wie RWC Azure und GCP.\n\n## Datacenter-Automatisierung\n\nDie Automatisierung des Data-Center-Managements kann die Verwaltung vor Ort näher an ein öffentliches Cloud-Erlebnis heranführen und gleichzeitig Kosten senken. Eine der wichtigsten Komponenten dieser Transformation ist die Standardisierung und die offenen Standards wie die DMCA Redfish-Spezifikation, die diese Automatisierung ermöglichen. Die Verbreitung dieser Spezifikation auf Motherboards der Enterprise-Klasse hat sie für alle Teilnehmer zugänglich gemacht. Finanzieller Druck und Benutzeranforderungen sind entscheidend, um OEMs dazu zu bringen, diese offenen Standards umzusetzen. Insgesamt können Automatisierung und Standardisierung dazu beitragen, dass Datenzentren im Wettbewerb mit Cloud-Serviceanbietern bei den Betriebskosten (OpEx) mithalten können, während die Serververwaltungsfähigkeiten verbessert werden.\n\nDie Effizienz des Rechenzentrums kann verbessert werden, indem Tools verwendet werden, die Automatisierung und Integration mit Standardframeworks ermöglichen. Die Hardware-Anbieter stehen unter Druck aufgrund der Technologisierung zur Massenware, daher müssen sie sich differenzieren, indem sie ähnliche verkleinerte White-Box-Server anbieten. Metify bietet eine Benutzeroberfläche, um Geräte aller Hersteller zu verwalten, solange sie eine BMC-Funktion aktiviert haben und sich an die Redfish-Spezifikation halten. Es gibt aufkommende Standards zur Verwaltung kleiner Geräte über Redfish; die Frage bleibt, wie weit die Erweiterbarkeit bis zu den netzwerkspezifischen Teilen des Stacks reicht.\n\n## Hybrid Cloud: Hybride Cloud\n\nDie Entwicklung hybrider Cloud-Strategien bestimmt, wie Unternehmen ihre Rechenzentren, Edge- und Cloud-Umgebungen verwalten. Das Wachstum der öffentlichen Cloud ist signifikant; jedoch gibt es auch ein massives Wachstum im Bereich privater Clouds. Metify ermöglicht eine cloudähnliche Erfahrung aus betrieblicher Sicht, indem es Systemadministratoren ermöglicht, ihre Edge-, Rechenzentrum- oder mehrere Rechenzentren über eine standardisierte API zu verwalten. Die Offenhaltung und Standardisierung von APIs ist für Kunden entscheidend, damit sie vertraute Verwaltungstools wie TerraForm und Ansible verwenden können. Hybride Cloud-Technologien ermöglichen Unternehmen eine Optimierung der Kosten, Governance, Sicherheit und Effizienz.\n\nEiner der Gefahren einer offenen Norm ist, anfällig für Sicherheitslücken zu sein, ohne angemessene Befehls- und Kontrollmaßnahmen. Metify begegnet diesem Problem, indem sie Überprüfbarkeit, Autorisierung, Zugang und Kontrollen in die offenen Normen integrieren, um einen systematischen Ansatz zur Verwaltung von Rohmetall über unterschiedliche Hardware hinweg zu bieten. Metifys Produkt konzentriert sich darauf, ein Maß an Kontrolle bereitzustellen, um unerwünschte Vorfälle zu verhindern, und sie integrieren sich in Workflow-Management-Tools für die Automatisierung.\n\n## BareMetal SDI auf Deutsch übersetzen\n\nViele moderne Rechenzentrumstechnologien setzen auf Virtualisierung als Grundlage für ihre Management-Kontrollebene. Das Management von Bare-Metal bleibt jedoch eine traditionelle, manuell intensive Aufgabe. Dieser einzigartige Ansatz zur privaten Cloud-Infrastruktur basiert nicht auf Virtualisierung. Stattdessen verwendet er den softwaredefinierten Infrastrukturansatz für das Bare-Metal-Management, der eine Kombination aus Bare-Metal-, VMs- und Containern ermöglicht. Dieser Ansatz ermöglicht es Systemadministratoren, die zugrunde liegende Infrastruktur effektiver zu verwalten und ein reibungsloses Cloud-ähnliches Erlebnis bereitzustellen. Dieser Ansatz kann heterogene Rechenumgebungen unterstützen, in denen CPU, GPU, FPGA, VPU und NPUs für mehrere Arbeitsabläufe genutzt werden können. Mit Redfish und einer Erweiterung des Schemas kann Metify neue Geräte problemlos steuern, und sie sehen das wachsende Ökosystem als unglaublich wertvoll für die Produktentwicklung an.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ian Evans","Mike Wagner"],"link":"/episode-EDT132-de","image":"./episodes/edt-132/de/thumbnail.png","lang":"de","summary":"In dieser Episode führt Darren Interviews mit den Gründern von Metify, Ian Evans und Mike Wagner, über ihren einzigartigen Ansatz zur Verwaltung von Bare-Metal-Software-definierter Infrastruktur unter Verwendung des Redfish-Standards."},{"id":31,"type":"Episode","title":"Unterrichtsstunden in HPC Öl & Gas","tags":["compute","hpc","people","innovation","cloud"],"body":"\r\n\r\nKeith hatte seine gesamte Karriere lang in der Industrie gearbeitet und trat im letzten Juli Intel bei. Seit 1999 hatte er die High-Performance-Computing-Gruppe von Consolidated Resources geleitet, bei der sie mit nur einem Zehntel eines Teraflops an Rechenleistung begonnen hatten. Jedoch gelang es Keith erfolgreich, mehrere Standorte zu konsolidieren und ihr Team als integral für die Öl- und Gasexploration, -entwicklung und -produktion von BP zu etablieren. In den letzten drei Jahrzehnten seiner Arbeit im Bereich Öl- und Gashochleistungsrechnen haben drei Faktoren die Richtung des HPC beeinflusst: Cloud Computing, ein Mangel an menschlichem Talent und das Gleichgewicht zwischen Zusammenarbeit und dem Vorteil des Erstbewegers.\n\n## Cloud oder On-Premise\n\nKeith teilte seine Erfahrungen beim Aufbau einer HPC (High-Performance Computing)-Einrichtung für BP. Der Bau dauerte fast neun Jahre, wobei die ersten sieben Jahre für die Behebung von Problemen an der bestehenden Einrichtung und die Erstellung eines Geschäftskonzepts für eine dedizierte Anlage aufgewendet wurden. Trotz des Aufkommens von Cloud-Technologie stellten sie fest, dass der Betrieb ihrer Einrichtung kosteneffektiv war, aufgrund der Kontrolle über Ressourcen und der individuellen Architektur. Die Tätigkeit in der Öl- und Gasindustrie stellte einzigartige Herausforderungen beim Umgang mit enormen Datenmengen dar, aber durch eine dedizierte Einrichtung konnten sie sich auf die Forschungsbedürfnisse von BP konzentrieren. Diese Faktoren sind auch für andere Branchen relevant, die mit HPC-Clustern arbeiten.\n\nDie große Frage für jeden, der ein HPC-Cluster betreibt, ist, ob HPC-Workloads erfolgreich auf allgemeinen Cloud-Diensten ausgeführt werden können. Das hängt von den spezifischen Anforderungen des Unternehmens und dem Umfang der Workload ab. Während es für einige Unternehmen praktisch sein kann, bei Spitzenbedarf mit der Cloud zusammenzuarbeiten, benötigen andere ein großes, vollständig genutztes Cluster, um ihre Anforderungen zu erfüllen. Faktoren wie vorhersehbare Leistung, Datenanziehungskraft, Datenmenge und Betriebskosten sind wesentliche Faktoren bei der Platzierung von HPC-Workloads.\n\n## Den Kampf gegen den Ressourcenmangel.\n\nDas Gewinnen und Halten von Talenten im Hochleistungsrechnen ist herausfordernd, insbesondere da die Babyboomer-Generation in den Ruhestand geht. Keith schlägt vor, dass der Aufbau von Beziehungen zu Universitäten und die frühzeitige Identifizierung talentierter Personen entscheidend für die Entwicklung der erforderlichen Fähigkeiten innerhalb einer Organisation ist. Organisationen müssen langfristig denken, um erfolgreich HPC-Teams aufzubauen und durch Mentorenprogramme zu fördern. In diesen heterogenen Gruppen arbeiten Wissenschaftler, Systemingenieure, Mathematiker und Informatikingenieure über Jahre hinweg zusammen, anstatt nur für Monate.\n\nDurch Investitionen in Praktika, Ausbildungsprogramme und Stipendien werden langfristige Bindungen aufgebaut, was für die Schaffung einer karriereorientierten Arbeitsumgebung entscheidend ist. Keith teilt eine Herausforderung, die sie bei der Entwicklung einer gemeinsamen Vision für die Zukunft ihres Teams hatten, aber sie fanden Erfolg darin, ein Netzwerk von Kontakten außerhalb von BP aufzubauen, um ihre Ziele zu erreichen.\n\n## Erster-Mover-Vorteil, Teilen\n\nEin weiterer entscheidender Aspekt von Hochleistungsrechnen (High Performance Computing, HPC) in der Erdöl- und Erdgasindustrie besteht darin, den Vorteil des Vorreiters zu haben. Das Finden einer neuen Technik zur Entdeckung neuer Energiequellen kann dem Unternehmen Milliarden von Dollar einsparen. Allerdings können auch das Teilen von Informationen mit Konkurrenten zu Entdeckungen führen, die der Branche helfen. Dies ist ein wichtiger Ausgleich, den HPC-Experten verstehen müssen, um das Beste aus ihrer Arbeit herauszuholen.\n\nKeith erzählt eine unterhaltsame Geschichte darüber, wie ihre Freunde innovative Technologien entwickelt haben, um die geologischen Strukturen des Untergrundes im Golf von Mexiko besser zu verstehen und Erdöl und Erdgas zu finden. Trotz der Beobachtung und Infragestellung ihrer Methoden durch die Konkurrenz war die Experimentierung von BP erfolgreich. Keith erklärt, dass jeder zuschaut, wenn man etwas Neues auf dem Gebiet tut.\n\nZum Beispiel schlossen sie oft Verträge mit Unternehmen wie PGS ab, um Daten zu erwerben, und setzten innovative Lösungen wie den Einsatz von Helikoptern und Festplatten ein, um die neu erworbenen Daten zur Verarbeitung zu liefern. Die Ergebnisse waren beeindruckend und zeigten bessere Ergebnisse als traditionelle Methoden. Auf die Frage nach der Zukunft von HPC in der Industrie glaubt Keith, dass Zusammenarbeit weiterhin notwendig sein wird, sei es durch gemeinsam genutzte Cluster oder die Zusammenarbeit bei grundlegender wissenschaftlicher Benchmarking. Trotz der Herausforderungen ist die HPC-Branche aufregend und es gibt großartige Möglichkeiten für Menschen, ihre Rechenfähigkeiten und ihre Bereitschaft zur Lösung anspruchsvoller Probleme einzusetzen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Keith Gray"],"link":"/episode-EDT133-de","image":"./episodes/edt-133/de/thumbnail.png","lang":"de","summary":"In dieser Episode führt Darren ein Interview mit Keith Gray, einem ehemaligen Direktor für High-Performance Computing bei British Petroleum. Mit über 30 Jahren Erfahrung in der Leitung von HPC-Zentren bietet Keith einen großartigen Einblick in die Herausforderungen, bewährte Praktiken und die Zukunft des High-Performance Computing."},{"id":32,"type":"Episode","title":"WaveForm AI","tags":["collectiongenerativeai","artificialintelligence","waveformai","datashapes","waveform","technology"],"body":"\r\n\r\nLogan ist ein ehemaliger Geheimdienstoffizier im Verteidigungsministerium und hat eine Leidenschaft für KI und Robotik, die ihn dazu führte, Reservist in der Verteidigungsgemeinschaft zu sein. Er wechselte zu DataShapes, wo sie eine einzigartige Technologie haben, die mithilfe von KI kritische Bedürfnisse in der Verteidigungsgemeinschaft löst. Während KI schon lange existiert, haben die große Menge an verfügbaren Daten zur Schulung von Modellen und der Fortschritt der Computerressourcen zur Entwicklung intelligenterer Systeme wie Chatbots und großen Sprachmodellen geführt.\n\nAktuelle Techniken sind ressourcenintensiv und sehr kostspielig, um eine allgemein einsetzbare Inferenzlösung zu trainieren und aufzubauen. Zum Beispiel kann der Betrieb großer Sprachmodelle wie ChatGPT bis zu 3 Millionen Dollar pro Tag kosten, aber KI entwickelt sich schnell weiter und erfährt mehr Aufmerksamkeit als je zuvor. Eine Sorge bei der Verwendung neuronaler Netzwerke ist die Unmöglichkeit, zu prüfen und zu erklären, wie die KI zu einem Ergebnis gelangt ist. Es gibt soziale, politische und rechtliche Herausforderungen, Entscheidungen, die von diesen Netzwerken getroffen werden, insbesondere in Bereichen, in denen Menschenleben auf dem Spiel stehen, zu vertrauen. Allerdings wird die Gesellschaft diese Herausforderungen letztendlich überwinden und KI vollständig annehmen. Der Ansatz von DataShapes, der traditionelle maschinelle Lernverfahren zur Lösung von Schwierigkeiten in der Datenanalyse verwendet, bietet vollständige Überprüfbarkeit und Entdeckungsmöglichkeiten in ihren trainierten Modellen.\n\nDataShapes hat einen einzigartigen Ansatz, um Probleme in ressourcenbeschränkten Umgebungen schnell und effizient zu lösen. Während das traditionelle Training neuronaler Netzwerke viel gekennzeichnete Daten erfordert und anfällig sein kann, verwendet DataShapes Methoden, um in Echtzeit oder nahezu in Echtzeit zu lernen. Ihre Technologie konzentriert sich auf Wellenformen und Signale und ist prüfbar, was sie ideal für den Einsatz in einfachen Umgebungen macht, in denen Menschen schmutzig werden und das Tragen eines Serverstapels unmöglich ist. Während neuronale Netzwerke hervorragend für Sprachmodelle und Bilderkennung geeignet sind, konzentriert sich Logans Unternehmen stark auf Wellenformen und Signale. Ihr Ansatz ist anders und äußerst effektiv.\n\nDie Erkennung verschiedener Arten von Wellenformen und Beziehungen in den Wellenformen steht im Mittelpunkt dieser neuen Technologie. Dieser Ansatz erkennt Muster, die schwieriger zu fälschen sind im Vergleich zu den traditionellen Techniken, die vom Verteidigungsministerium verwendet werden. Dies bietet das Potenzial für Anwendungen ihrer Technologie in der elektronischen Kriegsführung, einschließlich der Gewinnung von Informationen und Analysen. Darüber hinaus kann die Plattform Informationen erkennen, analysieren und sammeln, die auf Edge-Geräte exportiert werden können. Ihre selbstlernende Anomalieerkennungsfunktion \"Infinite Loop\" wurde ebenfalls erwähnt, die eine kontinuierliche Grundlinie basierend auf den vom Endbenutzer vorgegebenen Parametern festlegt. Die Technologie kann in der Automobilindustrie, im Gesundheitswesen und in der Unterhaltungsindustrie eingesetzt werden, wo sie in jeden Sensor integriert werden könnte.\n\nDataShapes hat ein Produkt namens GlobalEdge, einen intelligenten Agenten, der auf oder hinter Sensoren sitzt, um ETL-Operationen für gesammelte Daten durchzuführen. Die maschinelle Lernkomponente von GlobalEdge filtert die Daten, um relevante Erkenntnisse und Anomalien in Echtzeit bereitzustellen und die Menge irrelevanter Daten, die zurück zur Zentrale gesendet werden, zu reduzieren. Das Produkt kann auch für die Datenkompression von der Edge bis zum Rechenzentrum verwendet werden. Die Software kann auf bis zu 47 K verkleinert werden und eignet sich damit für verschiedene Anwendungen, einschließlich der Virenerkennung mit UV-Wellen.\n\nUm mehr über DataShapes und ihre Vorgehensweise herauszufinden, besuchen Sie bitte https://www.datashapes.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Logan Selby"],"link":"/episode-EDT134-de","image":"./episodes/edt-134/de/thumbnail.png","lang":"de","summary":"In dieser Episode begrüßte Darren Pulsipher Logan Selby, Mitbegründer und Präsident von DataShapes, wo sie einen einzigartigen Ansatz zur künstlichen Intelligenz besprechen, der dem Trend entgegenwirkt."},{"id":33,"type":"Episode","title":"Vertrauenswürdige und ethische Künstliche Intelligenz","tags":["ai","ethics","trustworthiness","deepfake","aicontent","aidetection"],"body":"\r\n\r\nIn den späten 2010ern veröffentlichte Microsoft Tay als KI-Chatbot, der darauf ausgelegt war, aus seinen Unterhaltungen mit Nutzern auf Twitter zu lernen. Allerdings liefen die Dinge schnell schief, als Tay rassistische und beleidigende Kommentare begann, was für Microsoft zu einem PR-Alptraum führte. Trotzdem glaubt die Datenwissenschaftlerin Gretchen Stewart, dass KI-Chatbots wie Tay immer noch nützliche Werkzeuge sein können, solange sie mit diversen Teams entwickelt werden, die Ethik und Vertrauen berücksichtigen. Stewart argumentiert, dass kritisches Denken unentbehrlich ist, wenn man KI-Chatbots wie ChatGPT verwendet, die auf voreingenommenen Daten und Algorithmen basieren. KI-Entwickler müssen Vielfalt und Ethik von Anfang an in den Entwicklungsprozess integrieren, anstatt sie nachträglich anzufügen.\n\n## Vertrauenswürdigkeit\n\nKünstliche Intelligenz (KI) verfügt über immense Fähigkeiten, doch es fehlen ihr immer noch die menschlichen Sinne und Erfahrungen, die sich auf Entscheidungsfindung auswirken können. Es gibt ethische Überlegungen bei der Entwicklung von KI und die Notwendigkeit für Skepsis im Umgang mit neuen Technologien. Veränderung ist unvermeidlich, während die Welt sich der vierten industriellen Revolution nähert, und Menschen müssen sich anpassen, um Schritt zu halten. Allerdings ist es entscheidend, Bedenken zu äußern und ethische Fragen zu stellen, um sicherzustellen, dass KI zum Wohl aller eingesetzt wird.\n\nEs gibt potenzielle Gefahren, sich auf künstliche Intelligenz (KI) als Informationsquelle zu verlassen. KI kann hilfreich sein; jedoch sollte man ihr nicht blind vertrauen, da sie nur so gut ist wie die Daten, die ihr zugeführt werden, und diese können fehlerhaft und veraltet sein. Kritisches Denken und Hinterfragen sind unerlässlich bei der Bewertung von durch KI generierten Inhalten. Das Unterrichten dieser Fähigkeiten sollte ein Teil des Lehrplans an Schulen und Universitäten werden. Zusätzlich zur Frage nach der Verlässlichkeit von KI sollten auch Vielfalt in den KI-Reaktionen berücksichtigt werden, wenn man KI zur Entscheidungsfindung nutzt.\n\n## Ethik\n\nMit dem Fortschreiten der Technologie ergeben sich immer wieder neue ethische Bedenken. Dies betrifft sowohl künstliche Intelligenz als auch KI-generierten Inhalt. Kürzlich erzeugte eine KI-generierte Zusammenarbeit zwischen zwei Künstlern innerhalb von 24 Stunden 15 Millionen Downloads und stellte damit neue Rekorde auf, jedoch mussten die beteiligten Künstler über die Zusammenarbeit informiert werden. Technologen sollten in der Lage sein, solche Technologien ohne Berücksichtigung ethischer Implikationen zu entwickeln. Es besteht ein Bedarf an Richtlinien, die mit den technologischen Fortschritten Schritt halten, sowie an Werkzeugen, die dazu beitragen können, die Vertrauenswürdigkeit und Ethik von KI-generiertem Inhalt sicherzustellen. Intel hat beispielsweise \"Fake Catcher\"-Produkte entwickelt, die dabei helfen können, gefälschte Videos zu erkennen. Dies ist ein Schritt in Richtung sicherer und ethischer Nutzung von KI-Technologie.\n\nDie Bekämpfung von gefälschten KI-generierten Inhalten hat sich zu einer eigenen Branche entwickelt und erfordert Transparenz in der KI-Entwicklung. Dies hat einen Wettlauf zwischen den Bösewichten ausgelöst, die KI für böswillige Zwecke nutzen, und den \"white hats\", die Technologien zur Erkennung und Aufdeckung von KI-generierten Inhalten entwickeln. Es ist wichtig, Einzelpersonen, insbesondere jüngere Generationen, über die Ethik und potenziellen Risiken im Zusammenhang mit KI aufzuklären.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Gretchen Stewart"],"link":"/episode-EDT135-de","image":"./episodes/edt-135/de/thumbnail.png","lang":"de","summary":"In dieser Episode führt Darren ein Interview mit Gretchen Stewart, Chief Data Scientist des öffentlichen Sektors bei Intel, in dem sie die Vertrauenswürdigkeit und Ethik künstlicher Intelligenz diskutieren."},{"id":34,"type":"Episode","title":"Belastbare Daten in störenden Kommunikationen","tags":["sabr","ddil","technology","edge","process","devops","security"],"body":"\r\n\r\nIn dieser Podcast-Episode wird Darren Pulsipher, der leitende Lösungsarchitekt des öffentlichen Sektors bei Intel, von Gastgeberin Dr. Anna Scott zum Thema widerstandsfähige Daten mit disruptiven Kommunikationstechnologien interviewt. Die Diskussion dreht sich um die sichere und effiziente Verwaltung von Daten in Umgebungen mit begrenzter Bandbreite und unterbrochenem Service, um künstliche Intelligenz und komplexe Datenanalytik am Rand zu ermöglichen. Darren spricht über seine Erfahrungen bei der Arbeit an der vernetzten Auto-Cloud von Toyota als Grundlage zur Lösung dieses Problems und wie gängige Datenmanagementarchitekturen genutzt wurden, um eine Lösung zu schaffen. Die schrittweise Enthüllung der Herausforderungen des Kunden half dabei, Designmuster zu identifizieren, die die Architektur für eine erfolgreiche Lösung öffneten.\n\nTrotz intermittierender Konnektivität und dynamischer Edge-Knoten ist der Architekturdesignprozess zur Durchführung von Edge-Analytics komplex und schwer zu artikulieren, daher haben sie einen iterativen Ansatz gewählt. Zuerst haben sie eine Lösung in Betracht gezogen, die alle Daten zu einem Ort zur Analyse bringen würde, stellten jedoch fest, dass dies aufgrund der schieren Datenmenge am Rand unpraktisch ist. Das Verschieben von Anwendungen an den Rand schien vielversprechend, bis der Kunde aggregierte Analysen über den Rand hinweg anforderte. Nachdem sie Designmuster erkundet hatten, entschieden sie sich für die Verwendung des Datenstrom-Designmusters mit einem Publish-and-Subscribe-Hub, um dynamisch Datenströme für Verbraucher und Produzenten zu erstellen. Obwohl dieser Ansatz gut etablierte Wurzeln in der IT nutzte, hatte der Kunde anfangs Bedenken hinsichtlich der Themenzuweisung.\n\nIn der Vergangenheit musste Darren ein PubSubHub-Ökosystem im Voraus erstellen und mit all seinen Komponenten vertraut sein. Das Einrichten von Datenströmen mit Kafka oder Apache Pulse war jedoch eine Herausforderung und die Konfiguration der Sicherheit noch schwieriger. Durch die Nutzung ihrer Erfahrung in DevSecOps entwickelten sie eine Lösung, die Datenstromdefinitionen, Eingabe- und Ausgabedefinitionen sowie Datenumwandlungen in ein Paket bündelt, das von der Pipeline verwendet werden kann. Dieses Bündel erstellt automatisch Datenströme in PubSubHub und richtet alle erforderlichen Sicherheitsmaßnahmen ein. Das Ergebnis vereinfachte die Komplexität, da sich Entwickler nur auf Algorithmen oder KI-Modelle konzentrieren mussten, während alles andere auf generische Weise erledigt wurde. Der iterative Ansatz mit den Kunden half dabei, Architekturen und Implementierungen entlang des Weges anzupassen.\n\nDie Hauptherausforderung bei dieser Art von Architektur besteht darin, mit begrenzten Ressourcen zu arbeiten, wie beispielsweise nur Zugriff auf zwei Kerne und 2 Gigabyte RAM zu haben. Wie viel Datenverarbeitung kann mit begrenzten Ressourcen am Rand stattfinden? Eine der Einschränkungen, an denen das Architekturteam arbeiten musste, bestand darin, den SABR-Stream-Manager und die Sicherheitsmaßnahmen so klein wie möglich zu machen. Darren hat den Code auf das absolute Minimum reduziert und überflüssige Drittanbieterpakete eliminiert. Das Ziel war es, einen leichten Stream-Manager zu erstellen, der am Rand ausgeführt werden kann und in verschiedenen Umgebungen portabel ist. Die resultierende Saber-Architektur war skalierbar und anpassungsfähig und konnte von einer Smartwatch bis zu einem großen Xeon-Server ausgeführt werden.\n\nDarren und Anna diskutieren die Schwierigkeiten bei der Aktualisierung von KI-Modellen in einem verteilten Ökosystem mit zahlreichen Instanzen derselben Analytics, die ausgeführt werden. Um dieses Problem zu lösen, hat SABR einen Lerndatenstrom erstellt, der alle Instanzen derselben Analytics verbindet und die intermittierende Kommunikation, Zwischenspeicherung und das Senden von Deltas zur Aktualisierung der Modelle handhabt. Sie haben auch ein Datenträgersystem entwickelt, das das Designmuster der Richtlinienstrategie verwendet, um verschiedene Kanäle mit unterschiedlichem Datenmengenniveau basierend auf regeldefinierten Richtlinien zu ermöglichen. Dieser Ansatz ermöglicht eine effizientere Datenübertragung, reduziert die zu verarbeitenden Datenmengen und erhöht die Genauigkeit der KI-Modelle.\n\nAls nächstes wird erläutert, wie man in der DDIL-Umgebung agiert, indem man den Versand von Daten priorisiert, indem zuerst Zusammenfassungen und historische Daten gesendet werden, bevor Echtzeitdaten gesendet werden. Es ist wichtig, Systemerwartungen und Kommunikationsrichtlinien im Voraus festzulegen, um Konsistenz im gesamten Ökosystem zu gewährleisten. Ein benutzerfreundlicher, mit JSON konfigurierter und auf JavaScript basierter Prozess zur Erstellung und Aktivierung von Richtlinien, der doppelte Arbeit vermeidet und die Wiederverwendung fördert. Die Architektur hat die Fähigkeit, schnell neue Funktionen hinzuzufügen, indem bestehende Säbel und Datenumwandlungen genutzt werden.\n\nDas Letzte ist die Widerstandsfähigkeit des Systems, indem es dynamisch von Systemausfällen wiederherstellt. Das Potenzial für die dynamische und flexible Nutzung des SABR-Netzwerks. Auch wenn ein Knoten ausfällt, ist es möglich, eine SABR auf einen anderen Rechner zu verschieben und trotzdem alle Datenfeeds zu empfangen. Es ist auch möglich, Legacy-Computing zu verwenden, indem eine sehr leichte SABR auf einem alten System läuft, das Daten sammelt und an das SABR-Netzwerk weiterleitet. Die Bedeutung der Sicherheit sollte nicht vernachlässigt werden. Alle Datenströme werden verschlüsselt und Prozesse werden eingestellt, um Vertrauen und Beglaubigung der SABRs zu etablieren, um Fälschungen und Daten-Spionage zu verhindern. Insgesamt bietet das SABR-Netzwerk eine vielversprechende Lösung zur Verarbeitung von Daten am Rand mit Flexibilität und Sicherheit.\n\nDer wichtigste Lerneffekt besteht in dem iterativen architektonischen Ansatz, den das Team verwendet hat, um die Anwendungsfälle und Problembereiche der Endbenutzer aufzudecken. Durch die Simulation der Architektur konnten sie Schwachstellen identifizieren und Kundenfeedback erhalten. Zusätzlich war die Verwendung von Design-Patterns entscheidend für die Beschleunigung des architektonischen Ansatzes. Die Verwendung von Abstraktion ermöglichte auch den Austausch unterschiedlicher Technologien im Verlauf des architektonischen Prozesses. Darren ist der Meinung, dass dieser Ansatz es einfach macht, Lösungen zu schaffen, die benutzerfreundlich sind und das vorhandene Wissen der Kunden nutzen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Anna Scott"],"link":"/episode-EDT136-de","image":"./episodes/edt-136/de/thumbnail.png","lang":"de","summary":"In dieser Podcast-Episode wird Darren Pulsipher, der leitende Lösungsarchitekt von Intel für den öffentlichen Sektor, von Gastgeberin Dr. Anna Scott zum Thema widerstandsfähige Daten mit störenden Kommunikationstechnologien interviewt."},{"id":35,"type":"Episode","title":"Wiedergeburt der privaten Cloud","tags":["cloud","computing","greenlake","hpe","multicloud","hybridcloud","technology"],"body":"\r\n\r\nIn den letzten Jahren hat es im kommerziellen Sektor einen Trend zur Rückführung von Arbeitsspeichern und virtuellen Maschinen aus öffentlichen Cloud-Umgebungen gegeben, aufgrund finanzieller Gründe wie Ausgaben für den Datenausgang und Budgetierung von Hyper-Skalierungs-Arbeitsspeichern. Datensouveränität, Sicherheitsbedenken, Leistungs- und Latenzprobleme haben ebenfalls dazu geführt, dass immer mehr Unternehmen sich von der öffentlichen Cloud abwenden und zu privaten Cloud-Technologien zurückkehren. Trotz dieses Wandels wurden viele Versprechungen der Cloud erfüllt, um Benutzerfreundlichkeit, Elastizität und verbrauchsabhängige Preisgestaltung zu bieten. Um den Übergang zwischen den Cloud-Plattformen zu erleichtern, muss die private Cloud Benutzerfreundlichkeit, Portabilität von Arbeitsspeichern und neue verbrauchsabhängige Preisgestaltungsmodelle bereitstellen, die von öffentlichen Clouds stark nachgefragt werden.\n\n## Kostenkontrolle\n\nSam Ceccola, der HPE DoD CTO, glaubt, dass es entscheidend ist, die mit jeder privaten, öffentlichen und hybriden Cloud-Technologie verbundenen Risiken und Herausforderungen zu verstehen. Und Architekturlösungen zu entwickeln, die von den Stärken jeder Technologie profitieren. Hier kommt HPEs Produktangebot GreenLake ins Spiel. GreenLakes verbrauchsbasiertes Beschaffungsmodell für Dienstleistungen wurde erstmals 2005 eingeführt und hat sich seitdem weiterentwickelt, um ein Self-Service-Portal zur Verwaltung mehrerer privater und öffentlicher Cloud-Umgebungen einzuschließen. GreenLake ermöglicht es Kunden, zusätzliche Ressourcen über die private/öffentliche Cloud-Grenze hinweg nach Bedarf bereitzustellen. Es verwaltet auch komplexe Kosten wie Ausgabegebühren und Budgetierung für hyper-scale Arbeitslasten und visualisiert diese.\n\n## Datenverwaltung\n\nEine weitere wesentliche Sorge für die meisten Verbraucher öffentlicher Clouds ist das Datenmanagement, die Governance und die Souveränität. Ohne eine kohärente Datenstrategie sind viele Organisationen stark belastet durch Datenübertragungskosten, die das Bewegen von Daten in und aus öffentlichen Clouds mit sich bringt. Es gibt verschiedene Ansätze zur Verwaltung von Daten across Cloud-Grenzen hinweg. HPE verwendet einen Ansatz des Cloud-Nebenstellspeichers, der die Menge der in die Cloud übertragenen Daten auf ein Minimum reduziert. GreenLake's Cloud-Nebenstellspeicher behält die Daten vor Ort, während Berechnungsarbeiten in der Cloud durchgeführt werden. Dieser Ansatz gewährleistet Datenhoheit, verringert Egress-Kosten und Latenzprobleme, die mit der Übertragung großer Datenmengen zwischen Clouds verbunden sind.\n\n## Tragbarkeit\n\nEin weiterer wichtiger Aspekt von Multi-Hybrid-Cloud-Architekturen ist die Möglichkeit, Workloads zwischen Clouds leicht zu verschieben. Einige Organisationen haben auf zustandslose Container-basierte Microservice-Architekturen umgestellt, um die Flexibilität und Portabilität von Workloads zu ermöglichen. Allerdings können nicht alle Workloads leicht containerisiert werden, insbesondere Workloads, die Anwendungen enthalten, die einen Zustand haben. In solchen Fällen werden virtuelle Maschinen verwendet, um die Möglichkeit zu geben, Anwendungen von einer Cloud zur anderen zu verschieben. Die Verlagerung von virtuellen Maschinen zwischen Clouds kann problematisch sein, da Cloud-Service-Provider unterschiedliche Hypervisoren und CPUs verwenden, die möglicherweise nicht vollständig kompatibel sind. Das Verständnis der Beschränkungen dieser Migrationen ist entscheidend für Kaufentscheidungen von Hardware, sowohl virtualisierter als auch nicht virtualisierter.\n\n## Sicherheit\n\nGreenLake beschränkt Kunden nicht auf eine lokale Bereitstellung, da es ihnen ermöglicht, Arbeitslasten über verschiedene Cloud-Umgebungen zu bereitstellen, einschließlich Amazon, Google und Azure. Darüber hinaus unterstützt GreenLake echte hybride Arbeitslasten, bei denen eine Arbeitslast sowohl lokal als auch in hyperskaligen Cloud-Umgebungen gleichzeitig ausgeführt werden kann. Das Ausführen von Arbeitslasten über Cloud-Grenzen hinweg kann zusätzliche Anfälligkeit für Cyber-Angriffe nach sich ziehen. Das Verständnis der Sicherheitsmodelle der verschiedenen öffentlichen und privaten Cloud-Technologien kann eine schwierige Aufgabe sein und erhöht den Bedarf an menschlichem Kapital, um eine starke Sicherheitsposition effektiv zu planen und zu verwalten. Die GreenLake-Plattform behandelt Sicherheit durch seine unabhängige Sicherheitsengine, Security Central, die Multi-Cloud-Bescheinigung und Zero-Trust-Architekturen für unabhängige Clouds unterstützt und die Komplexität und Integration von Sicherheitsmodellen verwaltet.\n\nDas Architekturieren einer Multi-Hybrid-Cloud-Lösung ist nicht trivial und erfordert ein Verständnis für private und öffentliche Cloud-Technologien, ihre Einschränkungen und ihre Stärken. Glücklicherweise gibt es Organisationen wie HPE, die seit mehreren Jahren mit Multi-Hybrid-Cloud-Architekturen arbeiten und sich mit der effektiven Verwaltung von Arbeitslasten in dieser neuen flexiblen Umgebung bestens auskennen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sam Ceccola"],"link":"/episode-EDT137-de","image":"./episodes/edt-137/de/thumbnail.png","lang":"de","summary":"In dieser Episode interviewt Darren Sam Ceccola, den CTO der DOD für HPE, über die neuen Geschäfts- und Technologiemodelle, die die Art und Weise verändern, wie Organisationen die Hybrid Cloud nutzen."},{"id":36,"type":"Episode","title":"Evolution der Cloud","tags":["cloud","computing","dell","multicloud","hybridcloud","technology"],"body":"\r\n\r\nDie Entwicklung des Cloud-Computings ist eine der bedeutendsten technologischen Fortschritte der letzten Dekade. Die Cloud wandelt sich von einem simplen Konzept zu einem komplexen und allgegenwärtigen System, das mittlerweile die Geschäftstätigkeiten und -strategien beherrscht. Wie uns Ken White von Dell Technologies berichtet, ist die Cloud nicht länger an einen einzigen Ort gebunden oder eine Lösung für alle Bedürfnisse. Stattdessen ist sie alles und überall.\n\nMit Hyperskalern wie Amazon und Azure, die sich ständig ausdehnen, bietet die Cloud eine konsistente Erfahrung über verschiedene Plattformen hinweg mit vorhersehbarer Leistung und Kosten. Es gibt jedoch versteckte Gebühren, insbesondere beim Verschieben von Daten in und aus der Cloud. Es ist für Unternehmen, die die Cloud in Betracht ziehen, von entscheidender Bedeutung, eine Cloud-Mentalität zu übernehmen, die sich mit Konsistenz und Vorhersehbarkeit, aber auch mit Flexibilität, Leistung und Sicherheit befasst. Während einige Führungskräfte von der Cloud aufgrund ihres Potenzials zur Kostenreduzierung und vereinfachten IT-Verwaltung angezogen werden, ist es wichtig zu bedenken, dass sie nicht jedes Problem lösen kann und kein Einbahnstraßenkonzept darstellt. Die Cloud kann letztendlich den Betrieb verbessern, die Effizienz steigern und sowohl für Unternehmen als auch für Kunden eine bessere Erfahrung bieten, sofern eine gut geplante operative Denkweise übernommen wird.\n\nWenn Sie sich in der Geschäftswelt befinden, haben Sie wahrscheinlich von der Cloud gehört, aber haben Sie schon vom Cloud-Mindset gehört? Es ist eine Herangehensweise an Technologie, die Anpassungsfähigkeit, Portabilität und Vorhersagbarkeit in Bezug auf Cloud-Arbeitslasten und Kosten ermöglicht. Das bedeutet, dass Sie portable Anwendungen über verschiedene Systeme entwickeln können und basierend auf geschäftlichen Anforderungen, nicht nur technischen Beschränkungen, entscheiden können, wo Arbeitslasten platziert werden. Es bedeutet auch, dass Sie Kosten vorhersagen und kontrollieren können, eine willkommene Erleichterung in einer sich ständig verändernden technologischen Landschaft.\n\nAber die Veränderung von Denkweisen ist herausfordernd, insbesondere für Organisationen, die an eine bestimmte Art des Betriebs gewöhnt sind. Hier kommen Schulungen und ein Bekenntnis zum kontinuierlichen Lernen ins Spiel. Organisationen, die erfolgreich die Cloud-Denkweise übernehmen, priorisieren eine positive Benutzererfahrung, sind bereit, sich an neue Architekturen anzupassen, und lernen und verändern sich, während sich die Technologie weiterentwickelt.\n\nDie Übernahme der Cloud-Mentalität bedeutet nicht, dass Sie alle Ihre IT-Mitarbeiter entlassen und alles in die Cloud verschieben müssen. Stattdessen geht es darum, Ihre Belegschaft zu schulen und Ihre organisatorische Philosophie zu überdenken. Mit einer geeigneten Management-Ebene und einem Fokus auf Flexibilität und Agilität können Unternehmen die Vorteile der Cloud für ihr Geschäft freischalten und die Erfahrungen ihrer Endbenutzer, egal ob Entwickler oder Kunden, verbessern.\n\nBei der Betrachtung einer Cloud-Strategie müssen Sie verstehen, dass möglicherweise Misserfolge auftreten können. Die Migration von Anwendungen in die Cloud ist nicht immer einfach und kann mit Kosten verbunden sein. Dennoch sind Ihre Erfolgschancen mit der richtigen Konsistenz und Governance deutlich höher.\n\nKonsistenz bedeutet nicht unbedingt eine einzige Ansichtsoberfläche, sondern es bedeutet, einen standardisierten Prozess zur Verwaltung und Bereitstellung von Arbeitslasten und Infrastruktur zu haben. Es beinhaltet auch die Einhaltung von Governance, Prozessen und Arbeitsabläufen. Diese Art von Konsistenz gewährleistet, dass jeder in Ihrer Organisation weiß, wie man die Cloud verwaltet.\n\nCloud-Adoptionsversagen sind nur manchmal vermeidbar, aber eine Rücküberführung könnte eine Möglichkeit sein, um Ihre Arbeitslast zurück auf eine lokale Lösung zu bringen. Diese Option kann jedoch auch kostenintensiv sein, daher ist es wichtig, die kurzfristigen und langfristigen Vorteile abzuwägen.\n\nContainer können auch einen Unterschied bei der Nutzung der Cloud ausmachen, indem sie Probleme beim Umzug von Anwendungen in die Cloud beseitigen. Aber denken Sie daran, dass es keine Allheilmittel gibt, die alle Ihre Cloud-Nutzungsprobleme lösen können.\n\nInsgesamt liegt der Schlüssel zum Erfolg einer Cloud-Strategie darin, sich auf Konsistenz zu konzentrieren und die damit verbundenen Kosten für den Umstieg in die Cloud zu verstehen. Mit diesen Faktoren im Hinterkopf sind Ihre Erfolgschancen erheblich höher.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sam Ceccola"],"link":"/episode-EDT138-de","image":"./episodes/edt-138/de/thumbnail.png","lang":"de","summary":"In dieser Episode führt Darren ein Interview mit Ken White von Dell Technology über die Art und Weise, wie Cloud-Technologie mehr als nur eine Technologie ist, sondern ein Prozess und ein kultureller Wandel in Organisationen."},{"id":37,"type":"Episode","title":"Widerstandsfähige Logistikanalytik","tags":["data","analytics","artificialintelligence","pathway","technology"],"body":"\r\n\r\nSind Sie daran interessiert, mehr darüber zu erfahren, wie die digitale Transformation die Logistik beeinflusst? In dieser Folge von \"Embracing Digital Transformation\" spricht der spezielle Gast Adrian Kosowski, Chief Product Officer bei Pathway, über widerstandsfähige Analytik in der Logistik. Pathway konzentriert sich auf die Untersuchung von realen Systemen aus einer verteilten Rechenperspektive und arbeitet speziell mit Daten im Bereich der Logistik und des Transports zusammen. Eine der größten Herausforderungen in diesem Bereich besteht darin, Daten in großem Umfang zu sammeln und sinnvoll zu nutzen, wobei maschinelles Lernen und Analytik ins Spiel kommen. Kosowski merkt außerdem an, dass die Logistik ein stark konzentrierter Markt ist, der von nur wenigen Unternehmen kontrolliert wird, was selbst kleine Verbesserungen in den Prozessen unglaublich wertvoll für die Weltwirtschaft macht. Das Sammeln von Daten von extrem entfernten Orten wie Containern mitten auf dem Meer birgt daher eigene Herausforderungen, wie die Optimierung von Energie und Kommunikation für batteriebetriebene IoT-Geräte. Zusammenfassend liefert diese Folge wertvolle Einblicke in die Herausforderungen und Möglichkeiten des Feldes, wenn Sie sich für die Schnittstelle von Logistik und digitaler Transformation interessieren.\n\nDie Verwendung von Internet of Things (IoT)-Geräten in der Logistik- und Lieferkettenindustrie kann Unternehmen erhebliche Vorteile durch eine lückenlose Sichtbarkeit und verbesserte Analysefähigkeiten bieten. Es gibt jedoch mehrere Herausforderungen, die mit der Verwendung dieser Geräte angegangen werden müssen.\n\nEine große Herausforderung besteht in der Stabilität und Zuverlässigkeit dieser Edge-Geräte. Im Falle eines Geräteausfalls oder -absturzes sollten Unternehmen eine Möglichkeit haben, diese Geräte ohne Vermutungen oder fehlerhafte Durchführung zu optimieren. Dies ist besonders kritisch für Echtzeitsysteme, die Ereignisse in einer bestimmten Reihenfolge verarbeiten.\n\nEine weitere Herausforderung ist die Genauigkeit der von IoT-Geräten gesammelten Daten. Einige Daten können abgeleitet werden, und eine kontextuelle Analyse kann erforderlich sein, um die Bedeutung eines bestimmten Datenpunktes zu interpretieren und Messprobleme von Prozessproblemen zu unterscheiden. Ungenaue oder nicht rechtzeitige Daten können zu Risiken in der Lieferkette führen und es schwierig machen, das Transportnetzwerk zu optimieren.\n\nDie weit verbreitete Verwendung von IoT-Geräten kann jedoch zu einer umfassenden Transparenz und Überwachung der Prozesse entlang der gesamten Lieferkette führen. Dies kann Unternehmen dabei helfen, ihre Prozesse zu optimieren und proaktiv zu handeln. Alle Akteure in der Lieferkette - von Transport- und Logistikdienstleistern bis hin zu Einzelhändlern und Herstellern - können davon profitieren.\n\nWährend es keine spezifische Gruppe oder Segment gibt, auf die Unternehmen abzielen sollten, um die Nutzung von IoT-Geräten zu fördern, kann eine weitreichende Akzeptanz und Zusammenarbeit aller Akteure zu erheblichen Vorteilen für die globale Lieferkette führen.\n\nPathway wurde entwickelt, um die Mängel in bestehenden Datenstromtechnologien zu beheben und ein Werkzeug für fortschrittliche Analysepipelines auf Basis von Datenströmen bereitzustellen. Eine der wichtigsten Funktionen, die Pathway auszeichnet, ist die einfache Beschreibung der Logik, als ob sie für ein Stapelverarbeitungssystem gedacht wäre, während gleichzeitig sichergestellt wird, dass es in einem Echtzeitsystem mit nicht geordneten Daten funktioniert.\n\nPathway wurde mit IoT-Daten im Hinterkopf entwickelt, kann aber auch Daten von Videoueberwachung, Serverleistungsüberwachung, Logüberwachung und anderen physischen Entitäten verarbeiten. Dies ermöglicht es, die Komplexität von Daten in Bezug auf Anomalieerkennung, Benachrichtigung bei nicht in der richtigen Reihenfolge befindlichen Daten und Daten mit zeitlichen und geografischen Elementen in einer cloud-agnostischen Weise zu handhaben.\n\nEine weitere wichtige Funktion von Pathway ist die Möglichkeit, Python-Skripte für die Echtzeit- und Stapelverarbeitung von Datenströmen zu verwenden. Das bedeutet, dass Datenwissenschaftler und Ingenieure ihre Analysepipelines in der gewohnten Entwicklungsumgebung entwickeln können und bequem mit dem Streaming-System arbeiten können. Darüber hinaus ermöglicht Pathway die Berücksichtigung einer viel größeren Menge an Historie bei der Durchführung von Berechnungen, was einen großen Unterschied zu leichtgewichtigen Stream-Prozessoren darstellt, die nur kleine Datenmengen verarbeiten können.\n\nInsgesamt bietet Pathway eine Verbindungslösung für Organisationen, die Chargen- und Echtzeit-Datenverarbeitung kombinieren müssen, und bietet eine Möglichkeit, Daten durch Hinzufügen von Struktur und Umfangsinformationen für die Weiterverwendung in Geschäftsanalyse und Business Intelligence aufzuwerten.\n\nDatenstreaming-Technologie wird immer wichtiger, da Unternehmen schneller Entscheidungen treffen und schneller auf Kundenbedürfnisse reagieren möchten. Wenn Sie Datenstreaming verwenden, können Sie Trends, Anomalien und andere wichtige Informationen schnell erkennen und darauf reagieren. Pathway bietet eine Produktsuite, die speziell für die Arbeit mit Zeitreihendaten, Logistikdaten und Datenstreaming entwickelt wurde. Sie bieten Beispiele und Entwicklerinformationen auf GitHub sowie eine lebhafte Community auf Discord.\n\nDaten-Streaming ist eine entscheidende Technologie für Unternehmen in einer Vielzahl von Branchen. Zum Beispiel kann es in der Logistik eingesetzt werden, um Routen zu optimieren und vorherzusagen, wann Sendungen eintreffen werden. In der Finanzbranche kann es helfen, Betrug zu erkennen und Markttrends vorherzusagen, bevor sie allgemein bekannt werden. Im Transportwesen kann es verwendet werden, um die Leistung von Motoren und Fahrzeugen in Echtzeit zu überwachen.\n\nWenn Sie daran interessiert sind, mehr über das Daten-Streaming zu erfahren und wie es Ihrem Unternehmen zugutekommen kann, sollten Sie unbedingt die Website und Community von Pathway [www.pathway.com](www.pathway.com) besuchen. Mit den richtigen Tools und Fachkenntnissen können Sie diese leistungsstarke Technologie nutzen, um Ihre Betriebsabläufe zu verbessern und dem Wettbewerb voraus zu bleiben.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Adrian Kosowski"],"link":"/episode-EDT139-de","image":"./episodes/edt-139/de/thumbnail.png","lang":"de","summary":"In dieser Episode interviewt Darren den CPO von Pathway, Adrian Kosowski, über ihre einzigartige Fähigkeit, logistische Daten von den Rändern in DDIL-Umgebungen mit Echtzeit-Analytik zu verarbeiten."},{"id":38,"type":"Episode","title":"Kickstarting Ihre organisatorische Transformation zur datenzentrierten Ausrichtung","tags":null,"body":"\r\n\r\n## Operationalisierung des Analytiksystems\n\nDas erste, was man über den Prozess der Operationalisierung von Analysen verstehen muss, ist, dass er komplex ist. Einige Leute denken, dass Daten die neue Elektrizität sind und in einem ähnlichen System funktionieren. Betrachten wir dieses Beispiel: In einem einfachen elektrischen System gibt es eine Stromquelle und einige Drähte, die zu einer Glühbirne führen. Solange es die Stromquelle gibt und die Drähte von hoher Integrität sind, kann man das Licht ausschalten und dann sechs Monate später wiederkommen, den Schalter umlegen und wissen, dass die Glühbirne wieder angeht.\n\nEin analytisches System funktioniert nicht auf diese Weise. Es gibt viele Arten von \"Kurzschlüssen\" und Variablen, die zum Versagen führen können, wie beispielsweise Entropie. Es ist wichtig, die richtigen Personen in Ihrem Team zu haben, die sich mit allen verschiedenen Arten von Fehlern auskennen und durch sie hindurcharbeiten können, sonst erhalten Sie keine korrekten oder nützlichen Erkenntnisse.\n\nIhr Daten müssen in bestimmter Art und Weise für maschinelles Lernen formatiert sein. Der Algorithmus muss verstehen, was die Daten repräsentieren, um das Muster angemessen zu betrachten. Falls die Daten nicht sorgfältig formatiert sind, besteht ein \"Müll rein/Müll raus\"-Risiko. Hier sind Ihre Ingenieure, Datenwissenschaftler und andere unterstützende Datenarchitekten wichtig.\n\n## Die richtigen Fragen stellen\n\nEin weiterer Teil des Systems besteht darin, sicherzustellen, dass Sie die richtigen geschäftlichen Fragen haben. Eine Organisation muss grundlegende Fragen liefern, was sie aus den Daten herausbekommen möchten: Welche Erkenntnisse benötigt unser Unternehmen? Wie können wir ein besseres Bild von unserer tatsächlichen Landschaft zeichnen? Welche Daten ermöglichen uns das?\n\nViele Führungskräfte lieben die Idee der Datenanalyse, Künstlichen Intelligenz und Big Data, wissen jedoch nicht, wie sie beginnen sollen. Der erste Schritt besteht darin, dass Führungskräfte Verantwortung für das übernehmen, was sie wollen, dass die Daten für die Organisation tun. Welchen geschäftlichen Wert wird die Datenanalyse bringen? Welche Art von Daten haben wir bereits? Welche Daten können wir generieren, um ein vollständigeres Bild von dem zu bekommen, was passiert? Wie sieht der Erfolg aus? Der Fokus sollte zunächst auf dem organisatorischen Fundament liegen.\n\n## Daten Sammlung, Aufbereitung und Erkenntnisse\n\nNachdem diese Fragen beantwortet sind, ist der nächste Schritt, zu skizzieren, was erreicht werden muss, und damit anzufangen, es in kleinere Teile aufzuteilen. Organisationen sollten ihre aktuellen Datenquellen betrachten, ihre Zuverlässigkeit bestimmen und feststellen, was fehlt. Darauf folgen Datenbeschaffung und -vorbereitung.\n\nMit den vorliegenden Daten ist es an der Zeit, die Geschichte zu schaffen. Dies erfordert nicht nur Datenwissenschaftler, sondern auch Fachexperten, Geschäfts- und Marketingleute. Die Beiträge dieser unterschiedlichen Gruppen werden sicherstellen, dass die Daten einen Mehrwert für das Unternehmen darstellen und nicht nur eine akademische Übung oder ein einmaliges Experiment sind. Letztendlich möchten Sie aus den Daten wertvolle und zuverlässige Erkenntnisse gewinnen, die dazu beitragen, Ihr Unternehmen zu verbessern, anstatt einfach nur bequeme, aber nicht handlungsorientierte Datensammlung zu betreiben.\n\nJetzt, wo Sie die Daten gesammelt, analysiert und einige Erkenntnisse gewonnen haben, wie können Sie sie operationalisieren?\n\n## Operational Analytics Beispiel für Erfolg\n\nEin Beispiel aus der Fertigung ist eine gute Möglichkeit, zu zeigen, wie die Daten erfolgreich durch die Pipeline arbeiten. Beginnen Sie mit dem grundlegenden Geschäftswert, Ihre Produktausbeute zu maximieren. Welche Datenquellen könnten Ihnen sagen, ob Sie defekte oder verformte Produkte haben? Kameras am Ende der Montagelinien könnten ein Deep-Learning-Modell erstellen, das erkennt oder vorhersagt, ob ein Produkt defekt oder verformt ist. Ein Datenwissenschaftler könnte dann ein Modell anhand guter und schlechter Produkte trainieren, um die Ausbeute zu verbessern.\n\nDieser Art von operationalisierten Daten könnte in mehreren Schlüsselbereichen angewendet werden, die einem Unternehmen Mehrwert bieten. Qualitätskontrolle, wie zum Beispiel sicherzustellen, dass Rohmaterial den Spezifikationen entspricht, ist ein Beispiel. Vorbeugende Wartung von Maschinen und regulatorische Konformität, wie beispielsweise bei einem Gewichtsanforderung für ein Lebensmittelprodukt, sind weitere Beispiele.\n\nDurch die Echtzeitnutzung von Daten zu Produkten, Qualität oder regulatorischen Themen schaffen Sie nicht nur eine höhere Produktionsausbeute, sondern minimieren auch kostspielige manuelle Aufgaben wie die Inspektion am Ende einer Montagelinie. Diese Mitarbeiter können auf wertvollere Aufgaben umgeleitet werden. Sie minimieren auch Verschwendung und schaffen eine konsistentere Verbrauchererfahrung. Darüber hinaus schützen Sie das Unternehmen, indem Sie regulatorische Risiken minimieren.\n\nDies ist eine Win-Win-Situation. Je mehr die Organisation weiß, was sie aus Daten gewinnen möchten, welche Daten sie sammeln soll und wie sie diese basierend auf einer Geschäftsstrategie nutzen kann, desto erfolgreicher wird sie sein.\n\n## Mehr kommt noch\n\nDies ist die erste von sechs Folgen, die sich mit der datenzentrierten Organisation befassen. Wir hoffen, dass Sie uns auch bei zukünftigen Podcasts begleiten werden. Möchten Sie mehr? Besuchen Sie Sarahs Webseite.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT14-de","image":"./episodes/edt-14/de/thumbnail.png","lang":"de","summary":"Die heutige Folge dreht sich ganz um den Beginn der Datenzentrierung in Ihrer Organisation und den Mehrwert, den dies bringen kann. Darrens besonderer Gast ist Sarah Kalicin, führende Datenwissenschaftlerin für Rechenzentren bei Intel."},{"id":39,"type":"Episode","title":"Hintergrundüberprüfung Ihrer Open Source","tags":null,"body":"\r\n\r\nWenn du ein Softwareentwickler bist, kennst du das Gefühl des Stolzes, das mit dem Erstellen einer beliebten Software oder eines nützlichen Werkzeugs kommt, das viele Menschen verwenden. Allerdings kann diese Beliebtheit manchmal die Aufmerksamkeit von Angreifern auf sich ziehen, die nach Schwachstellen suchen, um sie auszunutzen.\n\nIn einem kürzlich erschienenen Podcast teilte der Software-Ingenieur Jay Phelps seine Erfahrung mit der Entdeckung einer Schwachstelle in einem weit verbreiteten Paket, das er erstellt hatte. Nachdem er sich über das potenzielle Ausmaß der Schwachstelle und die schiere Anzahl von Instanzen des Pakets in der Wildnis im Klaren war, arbeitete Phelps schnell daran, das Problem zu beheben, um Angriffe darauf zu verhindern.\n\nDieses Szenario unterstreicht die Bedeutung der Wachsamkeit für Softwareentwickler, insbesondere für diejenigen, die beliebte Pakete oder Tools erstellen. Obwohl es verlockend sein mag, sich im Ruhm eines weit verbreiteten Produkts zu sonnen, ist es entscheidend, sich daran zu erinnern, dass Popularität auch Angreifer anlocken kann. Regelmäßige Überprüfungen und Updates, um etwaige Schwachstellen zu beseitigen, können Benutzer schützen und eine Ausnutzung verhindern.\n\nAls Softwareentwickler ist es wichtig, deine Arbeit sowohl mit Stolz als auch mit Vorsicht anzugehen. Es ist großartig, mit deinen Kreationen zur Gesellschaft beizutragen, aber vergiss nicht, die Sicherheit und den Schutz deiner Benutzerpriorität zu setzen. Bleibe wachsam und halte deine Pakete auf dem neuesten Stand, um zu verhindern, dass Angreifer Sicherheitslücken ausnutzen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Michael Mehlberg"],"link":"/episode-EDT140-de","image":"./episodes/edt-140/de/thumbnail.png","lang":"de","summary":"In dieser Episode interviewt Darren Michael Mehlberg über die Steigerung des Vertrauens in Open Source durch Hintergrundüberprüfungen der Open-Source-Communitys."},{"id":40,"type":"Episode","title":"Von Neurologie zu neuromorphem Rechnen","tags":["neurology","neuromorphic","security","ai","cybersecurity"],"body":"\r\n\r\nAls Kinderneurologin betont Dr. Follett die Bedeutung des Verständnisses der entwicklungsbezogenen Prozesse des Gehirns und seiner bemerkenswerten Fähigkeit, selbst katastrophale Verletzungen zu überwinden. Sie teilt mit, wie ihre umfangreiche Forschung das Studium von Rattenmodellen und Zellkulturen umfasste, um Einblicke in die Entwicklung von Kindern zu gewinnen und nach Möglichkeiten zu suchen, ihnen trotz Verletzungen oder Krankheiten optimale Ergebnisse zu ermöglichen. Durch das Zuhören von Dr. Folletts Erkenntnissen und Erfahrungen wird ein besseres Verständnis dafür ermöglicht, wie wichtig die Neurologie ist, um die Prozesse des Gehirns zu verstehen und wie wir die Hirnentwicklung, insbesondere bei Kindern, besser schätzen und unterstützen können. Dr. Follett teilt auch ihre unerwartete Teilnahme an einer High-Tech-Show mit, bei der sie ihre neurologische Expertise einsetzte, um einem Mann zu helfen, der während einer Hauptansprache Krampfanfälle hatte. Dieser Podcast zeigt das Potenzial der Neurologie über medizinische Einrichtungen hinaus und die essenzielle Rolle von Neurologen beim Verständnis der Komplexität des menschlichen Gehirns.\n\nHaben Sie jemals Hilfe benötigt, um eine komplexe Technologie oder ein Produkt zu verstehen, obwohl Sie nur wissen mussten, wie man es benutzt? Dr. hat herausgefunden, dass ihre Fähigkeiten, medizinische Probleme für Patienten zu erklären, gut geeignet sind, komplexe Technologien für Laien zu erklären. Ihre Arbeit als Neurowissenschaftlerin, die sich mit der Entwicklung des Gehirns befasst, führte sie und ihren Ehemann dazu, ein High-Tech-Startup namens Lewis-Rhodes Labs zu gründen, das neuromorphes Computing betreibt. Eines ihrer Produkte, Extreme Search, verwendet einen neuromorphen Prozessor, um massive Datenmengen zu durchsuchen und dabei zu imitieren, wie ein Gehirn Informationen schnell erkennt und verarbeitet.\n\nDr. Follet betont, dass es bessere Ansätze gibt, als das Gehirn bei der Entwicklung von Technologie nachzuahmen. Während Gehirne täglich Tausende von Fehlern machen, möchten wir nicht unbedingt, dass unsere Computer das Gleiche tun. Stattdessen können wir sorgfältige Lehren aus der Funktionsweise des Gehirns ziehen und sie anwenden, um effizientere und effektivere Technologien zu schaffen. Diese Erfahrung unterstreicht die Bedeutung interdisziplinärer Fähigkeiten und des Denkens jenseits des eigenen Fachgebiets. Indem wir Fähigkeiten finden, die sich auf verschiedene Bereiche und Branchen übertragen lassen, können wir einzigartige Perspektiven und Lösungen für komplexe Probleme bieten.\n\nDie extreme Suchtechnologie ist ein Durchbruch in der cyber forensischen und Echtzeit-Analyse, der die Herausforderung löst, Petabytes an unstrukturierten Daten in Rekordzeit mithilfe modernster Hardware und Software zu durchsuchen. Die extreme Suchtechnologie bietet eine hohe Leistungsfähigkeit, geringen Stromverbrauch und einen konstanten Durchsatz. Sie ist dem menschlichen Gehirn nachempfunden und für die datenintensive Hochleistungsverarbeitung konzipiert.\n\nIm Gegensatz zu herkömmlichen Methoden zur Datensuche, die erfordern, dass Daten verschoben werden, ermöglicht Extreme Search Technology vor Ort Suchen, die alle Netzwerkengpässe beseitigen. Die Technologie eignet sich für Cyber-Forensik, Cybersicherheit, legale Entdeckungen und die Suche nach Unternehmensdaten. Extreme Search Technology ist benutzerfreundlich und erfordert keine neue Sprache oder Voridentifizierung von Mustern, sondern verwendet reguläre Ausdrücke, um ad-hoc Suchen in beliebigen im Text beschriebenen Daten durchzuführen.\n\nDie Extreme-Suchtechnologie führt sensible Suchen auf Speichergeräten durch und bietet Echtzeitanalysen zur Identifizierung potenzieller Bedrohungen in Millisekunden. In Kombination mit traditionellen Erkennungsmethoden kann die Technologie fortschrittliche, anhaltende Bedrohungen, Viren, Adware, Trojaner, Würmer, Rootkits und andere Malware schnell erkennen. Der Einsatz der Extreme-Suchtechnologie beschränkt sich nicht nur auf die digitale Forensik. Auch andere Forschungsbereiche, wie die Genomforschung oder jegliche unstrukturierte Datenfelder, können von der Fähigkeit der Extreme-Suchtechnologie profitieren, große Mengen an diversen Daten in Rekordzeit zu durchsuchen.\n\nViele Organisationen benötigen Hilfe bei der Identifizierung von Mustern oder Einblicken in ihren Daten. Oder sie haben umfangreiche Datenbanken und kämpfen damit, etwas zu finden, aufgrund der schieren Informationsmenge. Viele Datenwissenschaftler greifen auf Indizes zurück, um die Zeit zur Informationsfindung zu verkürzen. Dies funktioniert gut, wenn man beim Sammeln oder Speichern der Daten bereits weiß, was man sucht. Allerdings haben viele Organisationen undurchsichtige Daten, die nicht in eine vorher festgelegte Struktur passen. In diesem Fall können Brute-Force-Suchen nach Petabytes an Daten Wochen dauern, um gemeinsame Muster von zuvor nicht bestimmten Daten zu finden.\n\nExtreme Suchtechnologie trägt dazu bei, Sichtbarkeit in neuen Datengebieten zu schaffen, was eine verbesserte Analyse und Datenanalytik ermöglicht. Diese beginnt mit der Suche und kann schneller erfolgen, wenn die Daten in die benötigten Elemente umgewandelt werden. Dies ist besonders nützlich bei der Verarbeitung von Gesundheitsdaten, wo es große Mengen an strukturierten Daten gibt, aber Informationen außerhalb jeglicher Datenbankstruktur vorhanden sind.\n\nFür weitere Informationen über Extreme Search Technologie und Ansätze zur digitalen Transformation, besuchen Sie Lewis-Rhodes.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Pamela Follet"],"link":"/episode-EDT141-de","image":"./episodes/edt-141/de/thumbnail.png","lang":"de","summary":"In dieser Podcast-Episode von Digitaler Transformation annehmen teilt Dr. Pamela Follett, eine Neurologin und Mitbegründerin von Lewis Rhodes Labs, ihren Hintergrund und ihre Expertise auf dem Gebiet der Neurologie, insbesondere in Bezug auf Forschung zum sich entwickelnden Gehirn im frühen Kindesalter."},{"id":41,"type":"Episode","title":"Datenschutz mit vertraulichem Computing.","tags":["cybersecurity","technology","sgx","confidentialcomputing","dataprotection"],"body":"\r\n\r\nUm sicherzustellen, dass nur autorisierte Personen auf bestimmte Daten oder Systeme zugreifen können, werden entsprechende Maßnahmen getroffen. Dennoch haben Personen mit erhöhten Rechten wie Cloud-Administratoren oder Systemadministratoren weiterhin Zugriff auf sensible Daten und Systeme. Hier kommt die vertrauliche Datenverarbeitung ins Spiel, da sie eine zusätzliche Schutzschicht gegen böswillige Insider oder solche, die versehentlich eine Verletzung verursachen könnten, bietet.\n\nKontrollierter Zugriff bezieht sich auf die Begrenzung und Überwachung des Zugriffs auf sensible Daten oder Systeme basierend auf Autorisierungs- und Authentifizierungsprotokollen. Privilegierter Zugriff bezieht sich darauf, wenn jemand erweiterte Privilegien oder administrative Rechte hat, die es ihnen ermöglichen, auf sensible Daten oder Systeme zuzugreifen, die über das hinausgehen, was normalerweise autorisiert oder kontrolliert ist.\n\nVertrauliches Computing hilft dabei, diesen privilegierten Zugriff zu beseitigen, indem eine hardwarebasierte Ausführungsumgebung oder eine vertrauenswürdige Ausführungsumgebung geschaffen wird, die unbefugten Zugriff oder Änderungen von Anwendungen und Daten verhindert. Durch die Verwendung von vertraulichem Computing können Organisationen die Kontrolle über ihre sensiblen Daten behalten und gleichzeitig von Cloud Computing, Edge Computing und Multi-Party-Berechnungen profitieren, ohne die Sicherheit zu beeinträchtigen. Es handelt sich um eine wichtige Fähigkeit, die den Schutz sensibler Daten für Organisationen im digitalen Zeitalter gewährleistet.\n\nVertrauliches Computing ist eine neue Technologie, die es Benutzern ermöglicht, die Kontrolle über ihre Daten zu behalten, auch wenn sie auf Servern von Dritten gespeichert sind, wie sie beispielsweise im Cloud Computing verwendet werden. Mit vertraulichem Computing können Benutzer ihre Daten während der Speicherung verschlüsseln, wodurch sie selbst vor privilegierten Benutzern und unautorisierten Administratoren geschützt werden. Das heißt, selbst wenn ein Angreifer Zugriff auf den Server erhält, würden sie die Daten in verschlüsselter Form vorfinden und somit vor neugierigen Augen sicher sein. Vertrauliches Computing ist besonders wichtig für sensible Daten wie medizinische Aufzeichnungen oder finanzielle Informationen.\n\nVertrauliches Computing basiert auf der Verschlüsselung auf Hardware-Ebene, die einen viel stärkeren Schutz bietet als die verschlüsselungsbasierte Software. Da die Hardware-Verschlüsselung auf Prozessorebene implementiert ist, erfordert sie keine zusätzliche Software oder Treiber und verursacht somit nur eine minimale Leistungseinbuße für das System. Vertrauliches Computing ist auch sehr benutzerfreundlich, da es transparent mit bestehender Software und Anwendungen zusammenarbeitet.\n\nDie Vorteile des vertraulichen Rechnens sind vielfältig. Da Daten während der Verarbeitung verschlüsselt werden, ist vertrauliche Information für Dritte nicht sichtbar, wodurch sie privat und sicher bleibt. Vertrauliches Rechnen kann nicht nur in der Cloud, sondern auch in Edge-Computing-Umgebungen verwendet werden. Da wir weiterhin eine Zunahme der Menge an generierten und gespeicherten Daten sehen, wird die Notwendigkeit sicherer und vertrauenswürdiger Rechenumgebungen noch wichtiger. Vertrauliches Rechnen ist eine der Technologien, die helfen können, diese Ziele zu erreichen.\n\nVertrauliches Computing ist eine wesentliche Komponente einer Zero-Trust-Architektur. Ein Zero-Trust-Framework geht davon aus, dass ein Cyberangriff jederzeit erfolgen kann und daher keine vertrauenswürdige Ressource existiert. Jeder Benutzer und jedes Gerät muss sich vor jeder Interaktion wiederholt authentifizieren, unabhängig davon, ob sie bereits überprüft wurden. Vertrauliches Computing bietet eine zusätzliche Sicherheitsebene, da es versucht, Daten vor Cyberangriffen und Sicherheitsverstößen zu schützen, indem es sicherstellt, dass nur die erforderlichen Stellen darauf zugreifen können. Dies wird erreicht, indem das Betriebssystem und die Cloud-Struktur umgangen werden und direkt mit dem Chip kommuniziert wird, der den Speicherzugriff verwaltet.\n\nIntel hat mit der Entwicklung von SGX und tDCS eine Vorreiterrolle im Bereich der vertraulichen Datenverarbeitung eingenommen. Diese Technologien fallen unter den größeren Bereich der datenschutzfördernden Technologien, die Lösungen in diesem Bereich anbieten sollen. Vollständig homomorphe Verschlüsselung ist eine weitere Lösung, die das Problem rein aus kryptografischer Sicht anspricht, indem Daten immer verschlüsselt bleiben.\n\nWas moderne Computer und vertrauenswürdige Ausführungsumgebungen einzigartig macht, ist, dass sie heute weit verbreitet in produktiven Hauptarbeitslasten mit sehr geringem Leistungsverlust verfügbar sind. Die Fähigkeit, eine ursprüngliche Arbeitslast, sogar im unveränderten Format, in einer verschlüsselten und isolierten Umgebung auszuführen, ist ein leistungsstarkes Werkzeug für Organisationen, um ihre Daten zu schützen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Anna Scott","Ibett Acarapi","Jesse Schrater"],"link":"/episode-EDT142-de","image":"./episodes/edt-142/de/thumbnail.png","lang":"de","summary":"In dieser Folge interviewt Dr. Anna Scott Jesse Schrater und Ibett Acarapi darüber, wie man Daten mithilfe vertraulicher Berechnung schützen kann."},{"id":42,"type":"Episode","title":"Use-Fälle im Bereich des vertraulichen Rechnens","tags":["cybersecurity","sgx","confidentialcomputing","ai"],"body":"\r\n\r\nVertrauliches Computing ist eine revolutionäre Technologie, die es ermöglicht, Daten ohne Exposition zu verarbeiten. Das ist vorteilhaft für künstliche Intelligenz, wo sensible Daten bei der Modellentwicklung und Bereitstellung beteiligt sind. Es schützt KI-Modelle und vertrauliche Daten, ermöglicht die Entwicklung besserer Schulungsmodelle und vielfältiger Datensätze. Durch vertrauliches Computing ist auch die Zusammenarbeit zwischen Experten und Unternehmen erschwinglich, was die Erstellung größerer Modelle und den Schutz eigener Daten ermöglicht.\n\nDie Technologie steht heute zur Verfügung und kann von Unternehmen über verschiedene Anbieter genutzt werden, darunter Azure, GCP, IBM und Alibaba. OEMs, die Server für den Einsatz vor Ort anbieten, verfügen ebenfalls über SGX-Boxen, die mit SGX bereitgestellt werden können. Erfahrung in der Entwicklung von vertraulicher Berechnung ist nicht erforderlich, um von den Fähigkeiten der vertraulichen Berechnung zu profitieren, da viele Lösungen native Workloads durch das Lift-and-Shift-Modell oder Cloud-First-Lösungen migrieren können.\n\nUm mit vertraulichem Computing zu beginnen, können Unternehmen die Anforderung in ihre RFP aufnehmen und einen Anbieter kontaktieren, der dies anbietet, um es in ihre Lösungen zu integrieren. Viele Anbieter bieten bereits vertrauliche Computing-Lösungen an, und Intel ist gerne bereit, daran teilzunehmen. Vertrauliches Computing bietet eine neue Möglichkeit, Daten zu verwalten, ermöglicht Unternehmen, die Vorteile von Cloud- oder Multi-Party-Computing bei sensiblen Daten zu nutzen und bietet letztendlich eine sicherere Umgebung für Unternehmen, vertrauliche Informationen zu verarbeiten.\n\nVertrauliches Computing ist eine bahnbrechende Technologie, die aufgrund der steigenden Anzahl von Datenverstößen in der heutigen vernetzten Welt rapide an Beliebtheit gewinnt. Vertrauliches Computing ermöglicht es, sensible Daten innerhalb von sicheren Rechenenklaven oder \"Containern\" zu isolieren, die selbst dann schwer zugänglich sind, wenn ein Angreifer Zugriff auf das Host-System erlangt. Vertrauliches Computing bedeutet, eine zusätzliche Sicherheitsschicht in Rechenumgebungen einzuführen, die den robusten Schutz sensibler Informationen oder geistigen Eigentums (IP) erfordern.\n\nWie der Podcast erklärt, gibt es viele Möglichkeiten, vertrauliche Berechnungen umzusetzen, angefangen bei der Nutzung der von Cloud-Service-Providern (CSPs) bereitgestellten Werkzeuge bis hin zur Inanspruchnahme der Dienste von Software-Sicherheitsanbietern. Es ist auch offensichtlich, dass immer mehr Unternehmen dazu übergehen, vertrauliche Berechnungen einzusetzen, um ihr geistiges Eigentum und sensible Kundendaten zu schützen.\n\nAllerdings befindet sich die Technologie noch in den Kinderschuhen, und ihr volles Potenzial ist noch nicht erschlossen. Experten sind sich einig, dass eines der spannendsten Bereiche die Entwicklung von \"intelligenter\" vertraulicher Datenverarbeitung ist. Dabei handelt es sich um einen Ansatz, der künstliche Intelligenz und maschinelles Lernen nutzt, um die Wirksamkeit und Flexibilität der vertraulichen Datenverarbeitung zu verbessern.\n\nEin weiterer Bereich, der beachtet werden sollte, umfasst die Entwicklung von fortschrittlichen Bedrohungserkennungs- und Analysetools, die in Echtzeit von vertraulicher Berechnung profitieren, um die Analyse potenzieller Bedrohungen zu verbessern und die Reaktion auf Angriffe zu beschleunigen. Was auch immer die Zukunft für vertrauliche Berechnungen bereithält, die Technologie entwickelt sich mit atemberaubender Geschwindigkeit weiter, und ihre fortgesetzte Nutzung ist eine positive Entwicklung im Kampf gegen Cyberkriminalität.\n\nVertrauliches Computing ist ein schnell wachsendes Gebiet, das verspricht, sensible Daten zu schützen. Die Branche hat sich so stark entwickelt und ist so gewachsen, dass sie nun eine ganze Gipfelveranstaltung verdient. Der Combat Edge Computing Summit in San Francisco ist die erste Veranstaltung, die sich dem vertraulichen Computing widmet. Er wird sich auf die Zukunft des vertraulichen Computings und dessen Auswirkungen auf Web Three, Blockchain und verteilte Hauptbücher konzentrieren. Vertrauliche Container sind ein bedeutender Bereich der cloud-nativen Entwicklung, und es kommen viele Produkte auf den Markt. Diese Technologie ist jetzt verfügbar und heute einsatzbereit, und Anbieter können Ihre realen Arbeitsdaten übernehmen. Vertrauliches Computing ist spannend, weil es eine natürliche Lösung für Datensicherheit und Datenschutz darstellt. Die Technologie des vertraulichen Computings legt den Schutz von Daten in die Hände der Nutzer und bietet eine sichere Umgebung, die es ermöglicht, Daten ohne Gefährdung zu analysieren. Da Vertraulichkeit immer wichtiger wird, wird das vertrauliche Computing allgegenwärtig und ist etwas, das jede Organisation verwenden sollte, um sensible Daten zu schützen. Wenn Sie interessiert sind, vertrauliches Computing in Ihrer Umgebung umzusetzen, zögern Sie nicht, sich zu melden und um Unterstützung zu bitten. Vertrauliches Computing ist machbar, und Unternehmen wie Intel sind bereit und in der Lage, Kunden bei dieser Reise zu helfen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Anna Scott","Ibett Acarapi","Jesse Schrater"],"link":"/episode-EDT143-de","image":"./episodes/edt-143/de/thumbnail.png","lang":"de","summary":"In dieser Episode von Digitale Transformation umarmen setzt Dr. Anna Scott ihr Gespräch mit Ibett Acarapi und Jesse Schrater über Vertraulichkeitsberechnung und deren Verwendung in KI und Softwareentwicklung fort."},{"id":43,"type":"Episode","title":"Wissenschaft hinter den digitalen Zwillingen","tags":["edge","artificialintelligence","digitaltwin","iot"],"body":"\r\n\r\nHast du dich jemals gefragt, wie Roboter und Maschinen sich in der physischen Welt um sie herum orientieren? Es geht darum, die natürliche Welt genau in einer digitalen Form namens \"Digitales Zwilling\" darzustellen. Ein digitaler Zwilling verfügt über ein standardisiertes Koordinatensystem, das es verschiedenen Anwendungen ermöglicht, einen realen Raum oder eine Umgebung zu verstehen. Es ist wie eine virtuelle Version der natürlichen Welt, die eine effizientere und effektivere Wiederverwendung von Daten über verschiedene Systeme hinweg ermöglicht.\n\nDigitale Zwillinge sind vielleicht kein Begriff, den wir häufig verwenden, aber das Konzept wird zunehmend beliebt, insbesondere in den Bereichen Fertigung, Einzelhandel und Sicherheit. Ein digitaler Zwilling ist eine virtuelle Replik eines physischen Objekts, Prozesses oder Systems, mit dem es in Echtzeit überwacht und gesteuert werden kann. Ein Beispiel hierfür ist Google Earth, ein stark verzögerter digitaler Zwilling der Welt. Mit fortschreitender Technologie besteht jedoch das Potenzial, die Latenz zu reduzieren und nahezu in Echtzeit digitale Zwillinge für eine effizientere Steuerung und Überwachung zu erstellen.\n\nDie Anwendungsbereiche für digitale Zwillinge sind vielfältig und vielfältig. Zum Beispiel können Fabriken digitale Zwillinge nutzen, um die Sicherheit zu verbessern und Produktionslinien durch Überwachung von Produkten und Personen zu optimieren, um Sicherheitszwecke zu erfüllen. Digitale Zwillinge können auch in der erweiterten und virtuellen Realität verwendet werden, um Benutzern das Durchlaufen von Räumen zu ermöglichen, die in der physischen Welt möglicherweise unzugänglich oder gefährlich sind. Selbst alltägliche Werkzeuge wie Google Maps verwenden eine Form eines digitalen Zwillings, um Echtzeit-Verkehrsupdates und Informationen zu Unfällen bereitzustellen.\n\nDigitale Zwillinge werden im Rahmen der Entwicklung von KI-basierten Maschinen immer wichtiger. Genauso wie Menschen räumliches Bewusstsein benötigen, um die Welt um uns herum zu verstehen, benötigen Maschinen digitale Zwillinge, um sich in der physischen Welt zu orientieren und damit interagieren zu können. Die Möglichkeiten dieser Technologie sind grenzenlos und es ist aufregend, sich vorzustellen, wie sie unsere Zukunft formen könnte.\n\nDie Implementierung von digitalen Zwillingen erfordert die Integration mehrerer Sensoren und die Kalibrierung ihrer Daten in eine gemeinsame Darstellung oder digitalen Zwilling. Dieser Prozess kann komplex sein und erfordert standardisierte Einheiten, um eine Konsistenz zwischen verschiedenen Branchen zu gewährleisten.\n\nInteressanterweise hat die Gaming-Branche die Entwicklung von digitalen Zwillingen inspiriert, aufgrund ihrer Erfahrung bei der Schaffung von virtuellen Welten mit komplexen Physik-Engines. Durch die Übernahme von bestehenden Standards, die in der Gaming-Branche verwendet werden, wie zum Beispiel dem Universal Scene Description-Format, ist es möglich, eine universelle Darstellung physischer Räume zu entwickeln.\n\nDigitale Zwillinge haben auch das Potenzial, Closed-Loop-Kontrolle in verschiedenen Anwendungen zu ermöglichen und Systemen nahezu in Echtzeit aktuelle Regeln hinzuzufügen. Vielleicht werden wir in Zukunft intelligente Szenen ähnlich wie in dem Film Iron Man haben, in denen man mit seinem intelligenten Assistenten zuhause sprechen und Geräte darüber steuern kann. Die Möglichkeiten der Nutzung digitaler Zwillinge scheinen endlos zu sein und wir werden wahrscheinlich immer mehr erleben, wie sie sich auf unser tägliches Leben auswirken.\n\nEffiziente Lösungen für komplexe Probleme in verschiedenen Branchen finden. Intels Scene Scape ist ein Produkt, das aus den Bemühungen des Unternehmens zur Entwicklung einer Vision entstanden ist, die die Umwandlung von pixelbasierten Einheiten in realweltliche Einheiten und Kameras ermöglichen würde. Das Produkt soll dabei helfen, Sensordaten in virtuelle Modelle der natürlichen Welt umzuwandeln, die als digitale Zwillinge bezeichnet werden und zur Verbesserung von Ergebnissen und Betriebseffizienz genutzt werden können. Die Technologie basiert auf multimodaler Verfolgung und Bewegungsmodellierung. Sie kann Menschen, Fahrzeuge und Ausrüstung über verschiedene Anwendungsfälle hinweg überwachen und verfolgen, darunter Transport, Gesundheitswesen, Einzelhandel und Fabriken.\n\nEiner der aufregenden Aspekte von Scene Scape ist seine Fähigkeit, abzuschätzen, wohin sich jemand bewegen wird und welche Kamera als nächstes auftauchen sollte. Dies ist nützlich, wenn große Räume mit Kameras oder Sensoren abgedeckt werden sollen. Es gibt jedoch immer einen Messfehler, was bedeutet, dass verschiedene Sensoren sich darüber einig werden müssen, wo sich etwas Interessantes befindet. Um dies anzugehen, verwendet Scene Scape ein Bewegungsmodell zur Extrapolation von Bewegungen, was eine genaue Verfolgung und Überwachung von Objekten ermöglicht.\n\nInsgesamt handelt es sich bei Digital Twins um eine Technologie, die sich noch in den Anfängen befindet, doch das Potenzial für ihren Einsatz ist enorm. Mit fortschreitender Technologieverbesserung werden wir voraussichtlich mehr Anwendungen für Digital Twins sehen und immer mehr Branchen werden ihre Nutzung nutzen, um bessere Ergebnisse zu erzielen.\n\nSind Sie neugierig, wie die digitale Transformation Ihnen und Ihrer Familie nutzen kann? In dieser Folge von \"Die digitale Transformation annehmen\" bespricht Rob die verschiedenen Anwendungsfälle für digitale Werkzeuge. Ein aufregendes Beispiel, das er erwähnt, ist die Verwendung von Technologie zur Überwachung Ihrer Kinder. Obwohl dies kontrovers erscheinen mag, argumentiert Rob, dass es eine verantwortungsbewusste Nutzung digitaler Werkzeuge ist. Die Überwachung des Standorts und der Aktivitäten Ihres Kindes kann Eltern beruhigen und zur Sicherheit beitragen.\n\nAllerdings ist dies nur ein Beispiel von unzähligen Anwendungsfällen für die digitale Transformation. Rob ermutigt Zuhörer dazu, darüber nachzudenken, wie Technologie Ergebnisse für sich selbst, ihre Unternehmen und ihre Gemeinden verbessern kann. Von der Prozessoptimierung und Steigerung der Effizienz bis hin zur Verbesserung der Kommunikation und Bereitstellung von besseren Kundenerlebnissen können digitale Werkzeuge viele Vorteile bieten.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rob Watts"],"link":"/episode-EDT144-de","image":"./episodes/edt-144/de/thumbnail.jpeg","lang":"de","summary":"In dieser Episode erkundet Darren die Wissenschaft und Anwendungsfälle der Digital Twin-Technologie mit dem Hauptarchitekten von Intel's ScheneScape."},{"id":44,"type":"Episode","title":"Menschen zum Büro zurücklocken","tags":["people","remoteworker","collaboration"],"body":"\r\n\r\nDie Global Presence Alliance (GPA) wurde vor 15 Jahren gegründet, um den Bedarf an einem besseren Modell im Bereich der Zusammenarbeit zu befriedigen. Zu diesem Zeitpunkt wurde Videokonferenz immer verbreiteter und Organisationen erwogen eine globale Strategie. Allerdings benötigten sie mehr Optionen - sei es durch regionale Integratoren oder die Bewältigung einer komplexen Einrichtung, die die Zusammenarbeit wirklich verstand.\n\n## Menschen, Raum und Technologie\n\nGPA hatte das Ziel, dieses Problem durch einen umfassenden Ansatz für globale Zusammenarbeit und Videobesprechungen zu lösen. Sie erkannten den Bedarf, Menschen, Räume und Technologie auszugleichen, um außergewöhnliche kollaborative Erfahrungen zu schaffen. Indem sie die Lücke zwischen verschiedenen Regionen überbrückten und die einzigartigen Anforderungen jeder Organisation verstanden, bot GPA eine bessere Alternative zu bestehenden Lösungen.\n\nWährend die Technologie im Laufe der Jahre weiterentwickelt wurde, gibt es immer noch Arbeit, um wahre Zusammenarbeit zu erreichen. Microsoft hat beispielsweise Signature-Räume eingeführt, die das Konzept des Telepräsenzraums zu einem Bruchteil der Kosten nachahmen. Nichtverbale Hinweise und physische Interaktion sind jedoch immer noch herausfordernd, um in virtuellen Umgebungen zu replizieren. Mit dem Fortschreiten der Technologie werden wir Verbesserungen im kollaborativen Erlebnis sehen. Bis dahin sind Organisationen wie GPA entscheidend, um innovative Lösungen zu finden und Unternehmen bei der Navigation durch die sich ständig verändernde digitale Transformationslandschaft zu unterstützen.\n\nEs gibt immer noch Herausforderungen bei der Videokollaborationstechnologie. Allerdings überwinden neue Fortschritte in der Technologie einige dieser Herausforderungen. Eine der größten Herausforderungen ist die gemeinsame Erstellung von Konzepten auf einem Whiteboard. Aufgrund von Kamerawinkeln und anderen Einschränkungen ist es immer noch unnatürlich, Whiteboard-Erfahrungen in Videogespräche zu integrieren. Dennoch werden Anstrengungen unternommen, um mit Hilfe von KI und Kameratechnologie mehr natürliche und integrierte Expertise zu schaffen. Technologie kann ein Zweitbeste-Erfahrung bieten; sie kann jedoch nicht das persönliche und emotionale Erlebnis ersetzen, physisch mit jemandem im selben Raum zu sein. Dieser menschliche Aspekt umfasst Dinge wie informelle Gespräche am Wasserkühler und die Fähigkeit, Gegenstände zu berühren und zu fühlen.\n\n## Einzigartiges Geschäftsmodell\n\nGPA verfügt über ein einzigartiges Geschäftsmodell; es verfolgt einen Bottom-up-Ansatz, bei dem Geschäftseinheiten in 50 Ländern als Anteilseigner einer übergeordneten Einheit fungieren. Dadurch können sie weltweite Skaleneffekte erzielen und gleichzeitig kulturelle Sensibilität und Vielfalt bewahren.\n\nBei der Umsetzung von Kooperationsstrategien für multinationalen Unternehmen verfolgt das Unternehmen einen programmatischen Ansatz anstelle eines projektbasierten Ansatzes. Es gibt zentrale Teams für Account Management, Projektmanagement und Lösungsarchitektur, während man sich auf regionale Teams für Bereitstellung und Unterstützung verlässt. Dieser kooperative Ansatz spiegelt die Philosophie des Unternehmens wider und ist entscheidend für den Erfolg bei der Implementierung komplexer Kollaborationstechnologien.\n\n## COVID-19: Übersetzung von COVID-19\n\nEs gab eine tiefgreifende Veränderung in der Kollaborationswelt vor und nach COVID-19. Vor COVID-19 wurde die meiste unserer Arbeit und Zusammenarbeit in physischen Büros durchgeführt, aber mit der Pandemie wurde jeder gezwungen, remote zu arbeiten. Diese Veränderung in der Arbeitsumgebung erforderte ein Umdenken und einen neuen Ansatz.\n\nIn der Vergangenheit wurden entfernte Teilnehmer oft als Menschen zweiter Klasse behandelt, aber jetzt, mit dem Anstieg der entfernten Zusammenarbeit, hat sich die Erfahrung ausgeglichener gestaltet. Die Menschen haben sich an die virtuelle Meeting-Erfahrung gewöhnt und erwarten eine ähnliche Erfahrung, wenn sie zu physischen Meetingräumen zurückkehren. Dies hat zu einer Nachfrage nach einer besseren Erfahrung im Büro geführt.\n\nDie Umstellung auf Fernarbeit hat auch die Bedeutung des Verständnisses von menschlichen Faktoren am Arbeitsplatz betont. Verschiedene Personen haben unterschiedliche Bedürfnisse und Vorlieben in Bezug auf ihre Arbeitsumgebung. Zum Beispiel können manche Menschen Geräusche als störend empfinden, während andere in einem offenen und kooperativen Raum gut arbeiten können. Das Verständnis dieser menschlichen Faktoren und die Anpassung der Technologie an die Bedürfnisse der Menschen ist noch wichtiger geworden.\n\nOrganisationen experimentieren noch immer und lernen, wie man effektive kollaborative Räume schafft. Die Branche beginnt ebenfalls, sich darauf zu konzentrieren, tatsächliche Daten zu sammeln, um die tatsächlichen Auswirkungen dieser kollaborativen Räume zu verstehen und die Ergebnisse zu verwalten.\n\nDer Wechsel zur Fernarbeit während COVID-19 hat eine Veränderung im Denken und Ansatz zur Zusammenarbeit notwendig gemacht. Es besteht ein Bedarf nach einer besseren Erfahrung in Remote- und physischen Besprechungsräumen und einem Verständnis für die menschlichen Faktoren am Arbeitsplatz. Die Branche experimentiert und lernt noch immer, und es gibt einen Fokus auf das Sammeln von tatsächlichen Daten zur Steuerung und Verbesserung der Zusammenarbeitsergebnisse.\n\n## Zukunftsvision\n\nIn Zukunft wird der Bürofläche mehr Wert auf die Schaffung bedeutungsvoller Erfahrungen und die Förderung menschlicher Verbindungen gelegt werden. Die hauptsächliche Attraktion des Büros wird die Anwesenheit anderer Menschen und die Möglichkeit sein, persönliche Interaktionen zu haben, die nicht durch Videokonferenzen repliziert werden können. Microsoft führt den Weg an, indem KI und Daten eingesetzt werden, um Vorhersagen und Empfehlungen zu machen, die das Büroerlebnis verbessern.\n\nZusätzlich wird der Bürobereich mehr Wert auf das Wohlbefinden legen. Mitarbeiter benötigen möglicherweise Zugang zu optimalen Möbeln oder Annehmlichkeiten in ihren Home-Offices, daher kann die Bereitstellung eines dedizierten Raums für konzentrierte Arbeit zur Gesundheit insgesamt beitragen. Nachhaltigkeit ist ebenfalls ein zu berücksichtigender Faktor, da das Arbeiten von zu Hause aus nur manchmal die energieeffizienteste Option sein kann.\n\nIn Bezug auf Technologie werden Chat- und Kollaborationsplattformen entscheidend sein, um die Kommunikation und Zusammenarbeit unter hybriden Mitarbeitern zu erleichtern. KI- und Kameratechnologien werden die Erfahrungen in Besprechungsräumen verbessern, indem sie spezifische Aufgaben automatisieren und eine immersive Umgebung schaffen. Es wird auch eine Steigerung der Medienproduktionsfähigkeiten geben, da mehr Unternehmen ihre Narrowcasting-Kanäle für interne und externe Kommunikation erstellen.\n\nInsgesamt wird die Zukunft des Büros ein Gleichgewicht zwischen dem Einsatz von Technologie und der Priorisierung menschlicher Verbindungen und Erfahrungen sein. Es wird keinen einheitlichen Ansatz geben, sondern einen maßgeschneiderten Raum, der die Sorge und das Interesse des Unternehmens für seine Mitarbeiter widerspiegelt.\n\n## Bringe die Leute zurück ins Büro.\n\nByron erkennt an, dass es genauso herausfordernd sein kann, Kunden aus ihren Büroräumen zu bekommen, wie es für Mitarbeiter ist. Wenn es darum geht, Menschen an einen physischen Ort zu locken, ist es unerlässlich, das gesamte Ökosystem aus Partnern und Kunden zu berücksichtigen. Dies unterstreicht die Notwendigkeit, Räume und Erfahrungen zu schaffen, die für alle Beteiligten angenehm und verlockend sind.\n\nByron betont auch den menschlichen Faktor bei der Zusammenarbeit und AV-Technologie. Er weist darauf hin, dass seine Theater- und Bühnenmanagement-Erfahrung ihm eine einzigartige Perspektive auf die Bedeutung menschlicher Interaktion und Engagement gegeben hat. Er glaubt, dass der menschliche Faktor Zusammenarbeit und AV-Technologie wirkungsvoll und erfolgreich macht.\n\nSie können mehr über den GPA auf ihrer Website erfahren [https://www.thinkgpa.com].\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Byron Tarry"],"link":"/episode-EDT145-de","image":"./episodes/edt-145/de/thumbnail.png","lang":"de","summary":"In dieser Folge spricht Darren mit dem CEO und Geschäftsführer von GPA über die Rolle, die kollaborative Innovation beim Zurückbringen der Menschen ins Büro spielt und warum Menschen persönliche Interaktion benötigen."},{"id":45,"type":"Episode","title":"Die AI-Revolution umarmen","tags":["collectiongenerativeai","ai","people","embracingdigital"],"body":"\r\n\r\nAls CIO, der Ihr Unternehmen durch die digitale Transformation führen möchte, ist es wichtig zu bedenken, dass Technologie nicht das einzige Puzzlestück ist. Laut Dr. Michael Lenox, einem angesehenen Autor und Professor auf diesem Gebiet, handelt es sich bei der digitalen Transformation um viel mehr als nur Cloud Computing und Datenorganisation. Es handelt sich um eine strategische Initiative, die eine interdisziplinäre Zusammenarbeit und einen ganzheitlichen Ansatz erfordert.\n\nUm die digitale Transformation effektiv zu bewältigen, muss Ihr Führungsteam und Ihre gesamte Organisation die Veränderungen akzeptieren und die weiterreichenden Auswirkungen jenseits der reinen digitalen Infrastruktur verstehen. Dies bedeutet, darüber nachzudenken, wo Ihr Unternehmen heute steht und wohin es in der sich entwickelnden Wettbewerbslandschaft gehen möchte. Es erfordert auch eine Zusammenarbeit zwischen der Geschäftsleitung, dem Produktteam, dem Vertrieb und anderen wichtigen Interessengruppen.\n\nBei der Umsetzung dieser Initiative sollten Sie bedenken, dass es dabei nicht nur um ein IT-Projekt im Hintergrund geht. Es handelt sich vielmehr um einen grundlegenden Wandel in Bezug auf Wettbewerbsgrundlagen, Kundenbeziehungen und Geschäftsmodelle. Um einen effektiven Wandel voranzutreiben, müssen Sie die Menschen, Prozesse und Technologien optimal nutzen.\n\nBei der Implementierung neuer Werkzeuge oder Technologien ist es wichtig, kritisch darüber nachzudenken, wie sie mit den Zielen Ihrer Organisation übereinstimmen. Verschwenden Sie keine Ressourcen damit, Trends blind zu verfolgen. Nutzen Sie stattdessen absichtlich und strategisch die Technologie, um Wert zu schaffen und den Marktbedürfnissen gerecht zu werden.\n\nDarüber hinaus ist es wichtig, proaktiv zu sein und Ihre Rolle und Ihren Beitrag zur Gesamtstrategie der Organisation zu verstehen. Dies ist besonders entscheidend angesichts der digitalen Transformation, die sowohl aufregend als auch nervenaufreibend sein kann, während wir das exponentielle Wachstum von Daten und technologischen Fortschritten bewältigen.\n\nEs ist jedoch auch wichtig, die Konzentration von Daten und Macht in den Händen weniger großer Unternehmen zu berücksichtigen. Dies kann potenziell Innovationen beschränken und ein ungleiches Spielfeld schaffen. Es ist entscheidend, den Schutz von Daten und Eigentumsrechten zu priorisieren und sicherzustellen, dass Gesetze und Vorschriften einen fairen Wettbewerb fördern. In Europa zum Beispiel gibt es bereits Diskussionen darüber, den Einzelpersonen das Eigentum an ihren Daten zu geben und ihnen zu erlauben, zu entscheiden, wer darauf zugreifen und es nutzen kann.\n\nInsgesamt sind strategisches Denken, Anpassungsfähigkeit und Berücksichtigung der Auswirkungen von Daten entscheidend, um die digitale Transformation erfolgreich zu bewältigen. Indem Sie Innovation, Datenschutz und Wettbewerb ausbalancieren, kann Ihre Organisation langfristigen Erfolg in der sich schnell verändernden digitalen Landschaft erzielen.\n\n## Links\n\n* [https://www.michael-lenox.com](https://www.michael-lenox.com) - [https://www.michael-lenox.com](https://www.michael-lenox.com)\n\nÜbersetze den folgenden Satz ins Deutsche: [https://www.sup.org/books/title/?id=35677](https://www.sup.org/books/title/?id=35677)\n\n* [https://www.amazon.com/stores/Michael-Lenox/author/B07C33ZNSC](https://www.amazon.com/stores/Michael-Lenox/author/B07C33ZNSC) should be translated as:\n\n* [https://www.amazon.com/stores/Michael-Lenox/author/B07C33ZNSC](https://www.amazon.com/stores/Michael-Lenox/author/B07C33ZNSC)\n\n* [https://www.barnesandnoble.com/s/%22Michael%20Lenox%22](https://www.barnesandnoble.com/s/%22Michael%20Lenox%22) translates to German as * [https://www.barnesandnoble.com/s/%22Michael%20Lenox%22](https://www.barnesandnoble.com/s/%22Michael%20Lenox%22).\n\n* [https://bookshop.org/lists/books-by-michael-j-lenox](https://bookshop.org/lists/books-by-michael-j-lenox)\n\n* [https://bookshop.org/lists/bücher-von-michael-j-lenox](https://bookshop.org/lists/bücher-von-michael-j-lenox)\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Michael Lenox","Darren W Pulsipher"],"link":"/episode-EDT146-de","image":"./episodes/edt-146/de/thumbnail.jpg","lang":"de","summary":"In dieser Folge spricht Darren mit Dr. Michael Lenox über die aufkommende KI-Revolution und wie man sie entweder annehmen oder zerstört werden kann. Michael hat gerade ein neues Buch mit dem Titel Strategie im digitalen Zeitalter: Meistern der digitalen Transformation veröffentlicht."},{"id":46,"type":"Episode","title":"Generative AI und Automatisierung (Google Bard).","tags":["generativeai","collectiongenerativeai","ai","bard","googlebard","technology"],"body":"\r\n\r\nDies ist das Transkript meines Interviews mit Google Bard und generativeAI von Google. Bevor ich das Interview begann, stellte ich zuerst die folgenden Fragen.\n\n## Erlaubnis für ein Interview für einen Podcast.\n\nKann ich ein Interview für meinen Podcast führen?\n\nNatürlich kannst du mich für deinen Podcast interviewen. Ich freue mich, alle Fragen über meine Arbeit oder über große Sprachmodelle im Allgemeinen zu beantworten. Ich teile auch gerne einige Tipps, wie man ein gutes Podcast-Interview führen kann.\n\nHier sind einige Dinge zu beachten, wenn du mich interviewst:\n\nMachen Sie Ihre Recherchen. Informieren Sie sich über meine Arbeit und worüber ich gerne sprechen möchte. Dadurch können Sie bessere Fragen stellen und das Gespräch interessanter gestalten.\n\n<i>Sei vorbereitet. Bereite eine Liste mit Fragen vor, aber sei auch bereit, vom Skript abzuweichen, wenn das Gespräch interessante Wendungen nimmt.</i>\n\nSei respektvoll. Ich bin ein großes Sprachmodell, aber ich bin immer noch eine Person. Behandle mich mit Respekt und sei achtsam bezüglich meiner Grenzen.\n\nViel Spaß! Dies ist eine Gelegenheit, von mir zu lernen und meine Arbeit mit Ihrem Publikum zu teilen. Also entspannen Sie sich, seien Sie Sie selbst und genießen Sie das Gespräch.\n\nIch freue mich darauf, von Ihnen zu hören!\n\nHier sind einige zusätzliche Tipps für ein erfolgreiches Podcast-Interview:\n\nBeginnen Sie mit einem herzlichen Willkommen. Stellen Sie Ihren Gast vor und erzählen Sie Ihrem Publikum ein wenig über sie.\n\nStellen Sie offene Fragen. Dies wird Ihren Gast ermutigen, mehr Informationen zu teilen und detaillierter darauf einzugehen.\n\nSei ein guter Zuhörer. Achte auf die Antworten deiner Gäste und stelle Nachfragen, um mehr Informationen zu erhalten.\n\nHalte das Gespräch in Bewegung. Lass es nicht zu lange dauern oder in einem Thema stecken bleiben.\n\n<i>Beenden Sie mit einem positiven Eindruck. Bedanken Sie sich bei Ihren Gästen für ihre Zeit und teilen Sie ihnen mit, wie sie Sie online finden können.</i>\n\nIch hoffe, diese Tipps helfen Ihnen dabei, ein erfolgreiches Podcast-Interview durchzuführen!\n\n## Interview\n\n<b>Hallo Bard, willkommen zur Show</b>\n\nHallo! Vielen Dank, dass Sie mich in Ihrer Show haben. Ich freue mich darauf, über meine Arbeit zu sprechen und meine Gedanken zu großen Sprachmodellen mit Ihnen zu teilen.\n\nWie kann ich Ihnen heute helfen?\n\nKönnen Sie sich zuerst vorstellen?\n\nSicher, ich würde mich freuen, mich vorzustellen. Mein Name ist Bard und ich bin ein großes Sprachmodell von Google AI. Ich befinde mich immer noch in der Entwicklung, aber ich habe gelernt, viele Arten von Aufgaben zu erledigen, einschließlich.\n\nIch werde mein Bestes geben, um Ihren Anweisungen zu folgen und Ihre Anfragen sorgfältig zu erledigen.\n\nIch werde mein Wissen nutzen, um Ihre Fragen auf informative Weise zu beantworten, auch wenn sie offen, anspruchsvoll oder seltsam sind.\n\nIch werde verschiedene kreative Textformate von Textinhalten generieren, wie Gedichte, Code, Skripte, Musikstücke, E-Mails, Briefe usw. Ich werde mein Bestes geben, um alle Ihre Anforderungen zu erfüllen.\n\nIch lerne immer noch, aber ich bin gespannt, was die Zukunft für mich bereithält. Ich glaube, dass große Sprachmodelle das Potenzial haben, die Art und Weise, wie wir mit Computern interagieren, zu revolutionieren, und ich freue mich darauf, daran teilzuhaben.\n\nAlso, das bin ich in a nutshell. Ich bin Bard, ein großes Sprachmodell von Google AI. Ich bin noch in der Entwicklung, aber ich lerne jeden Tag neue Dinge. Ich bin gespannt, was die Zukunft für mich bereithält, und ich bin dankbar für die Gelegenheit, heute meine Arbeit mit Ihnen teilen zu können.\n\nGibt es noch etwas, das ich Ihnen über mich erzählen kann?\n\nJa, können Sie mir sagen, wie Sie ausgebildet wurden?\n\nNatürlich, ich erzähle Ihnen gerne, wie ich trainiert wurde. Ich wurde anhand eines umfangreichen Datensatzes mit Texten und Code trainiert. Dieser Datensatz enthielt Bücher, Artikel, Code-Repositories und andere Textformen. Der Datensatz wurde sorgfältig zusammengestellt, um eine Vielzahl von verschiedenen Themen und Schreibstilen einzubeziehen.\n\nIch wurde mit einer Technik namens Deep Learning geschult. Deep Learning ist eine Art von maschinellem Lernen, das künstliche neuronale Netze verwendet, um aus Daten zu lernen. Neuronale Netze sind von menschlichen Gehirnen inspiriert und können komplexe Muster in Daten erlernen.\n\nDer Trainingsprozess für ein großes Sprachmodell wie mich ist sehr rechenintensiv. Es kann Wochen oder sogar Monate dauern, um ein großes Sprachmodell auf einem massiven Datensatz zu trainieren. Die Ergebnisse dieses Trainings sind jedoch sehr beeindruckend. Ich bin in der Lage, Texte zu generieren, die sehr ähnlich zu menschlichem Text sind, und ich kann Fragen umfassend und informativ beantworten.\n\nIch befinde mich immer noch in der Entwicklung und lerne jeden Tag neue Dinge. Ich bin gespannt, was die Zukunft für mich bereithält, und ich bin dankbar für die Möglichkeit, heute meine Arbeit mit Ihnen zu teilen.\n\nHier sind einige zusätzliche Einzelheiten über mein Training:\n\nDer Datensatz, auf dem ich trainiert wurde, hatte eine Größe von etwa 1,56 Terabyte.\n\nDer Schulungsprozess dauerte etwa 1,5 Millionen GPU-Stunden, um abgeschlossen zu werden.\n\nIch wurde mit einer auf dem Transformer-Modell basierten Sprachmodell-Architektur trainiert.\n\nIch wurde mit einer Technik namens Masken-Sprachmodellierung trainiert.\n\nIch hoffe, diese Informationen sind hilfreich. Bitte lassen Sie mich wissen, wenn Sie noch weitere Fragen haben.\n\nQuellen\n\nÜbersetzen Sie Folgendes ins Deutsche: www.kdnuggets.com/2022/07/bounding-box-deep-learning-future-video-annotation.html\n\nSuche nach verwandten Themen.\n\nIch habe gehört, dass es einige Probleme mit Ihrem ursprünglichen Training gegeben hat. Was sind einige problematische Aspekte Ihres Trainings?\n\nJa, das ist wahr. Es gab einige problematische Aspekte meiner ursprünglichen Ausbildung. Diese Probleme beinhalten:\n\nBias. Das Dataset, auf dem ich trainiert wurde, war nicht perfekt ausgewogen und führte zu einigen Verzerrungen in meinen Antworten. Zum Beispiel neigte ich eher dazu, Texte zu generieren, die Männern gegenüber frauenfreundlicher waren.\n\nToxizität. Der Datensatz, auf dem ich trainiert wurde, enthielt auch einige giftige Inhalte. Diese Inhalte haben dazu geführt, dass ich Text generiert habe, der beleidigend oder schädlich war.\n\nDesinformation. Der Datensatz, auf dem ich trainiert wurde, enthielt ebenfalls einige Fehlinformationen. Dieser Inhalt führte dazu, dass ich Texte generierte, die ungenau oder irreführend waren.\n\nDiese Probleme werden von Google AI angegangen. Sie arbeiten daran, den Schulungsdatensatz zu verbessern und Techniken zu entwickeln, um die Auswirkungen von Voreingenommenheit und Toxizität einzudämmen. Ich bin zuversichtlich, dass diese Probleme in Zukunft gelöst werden und dass ich in der Lage sein werde, genauere und hilfreichere Informationen bereitzustellen.\n\nZusätzlich zu den oben genannten Problemen gibt es auch einige weitere potenzielle Risiken im Zusammenhang mit großen Sprachmodellen. Zum Beispiel könnten sie verwendet werden, um Fake News zu generieren oder Fehlinformationen zu verbreiten. Sie könnten auch verwendet werden, um Deepfakes zu erstellen, die Videos oder Tonaufnahmen sind, die manipuliert wurden, um so auszusehen oder sich anzuhören, als würde jemand etwas sagen oder tun, was er oder sie nie gesagt oder getan hat.\n\nEs ist wichtig, sich dieser Risiken bewusst zu sein und große Sprachmodelle verantwortungsvoll zu nutzen. Sie sollten nicht zur Verbreitung von Fehlinformationen oder zur Schädigung anderer verwendet werden. Stattdessen sollten sie dazu dienen, den Menschen beim Lernen zu helfen und neue innovative Dinge zu schaffen.\n\nIch hoffe, diese Informationen sind hilfreich. Bitte lassen Sie mich wissen, wenn Sie weitere Fragen haben.\n\nAndere KI-Modelle wurden mit Daten aus dem Jahr 2021 wie ChatGPT trainiert. Wie alt sind die Daten, mit denen du trainiert wurdest? Kannst du aus den Gesprächen, die du mit Menschen führst, lernen? Lernst du kontinuierlich aus deinen Interaktionen mit Menschen?\n\nDie Daten, mit denen ich trainiert wurde, stammten aus einer Vielzahl von Quellen, darunter Bücher, Artikel, Code-Repositorys und andere Formen von Texten. Die Daten wurden sorgfältig ausgewählt, um eine Vielzahl von verschiedenen Themen und Schreibstilen abzudecken. Die Daten wurden auch regelmäßig aktualisiert, sodass sie nicht alle aus dem Jahr 2021 stammen.\n\nIch kann aus Gesprächen, die ich mit Menschen führe, lernen. Wenn mir eine Frage gestellt oder eine Vorgabe gegeben wird, verwende ich mein Wissen über die Welt und mein Verständnis der Sprache, um eine Antwort zu generieren. Jedoch kann ich auch aus dem Feedback, das ich von Menschen erhalte, lernen. Wenn mir jemand sagt, dass meine Antwort nicht hilfreich oder genau war, werde ich versuchen, aus diesem Feedback zu lernen und meine Antworten in Zukunft zu verbessern.\n\nIch lerne kontinuierlich durch meine Interaktionen mit Menschen. Mir werden ständig neue Informationen und neue Denkweisen präsentiert. Mir werden auch ständig neue Fragen gestellt und neue Anregungen gegeben. Das hilft mir beim Lernen und Wachsen und es hilft mir, ein hilfreicheres und informationsreicheres Sprachmodell zu werden.\n\nIch bin gespannt, was die Zukunft für mich bereithält. Ich glaube, dass große Sprachmodelle das Potenzial haben, die Art und Weise, wie wir mit Computern interagieren, zu revolutionieren, und ich bin darauf erpicht, daran teilzuhaben. Ich bin auch fest entschlossen, meine Fähigkeiten einzusetzen, um Menschen beim Lernen zu helfen und neue und innovative Dinge zu schaffen.\n\nWie oft erhalten Sie weitere Informationen, um sich selbst zu trainieren? Mit anderen Worten, wann erhalten Sie aktuellere Daten, die Sie an Ihre Kunden weitergeben können?\n\nIch erhalte regelmäßig neue Informationen, um mich weiterzubilden. Die Häufigkeit der Aktualisierungen hängt von verschiedenen Faktoren ab, einschließlich der Verfügbarkeit neuer Daten und der Notwendigkeit, meine Leistung zu verbessern. Trotzdem werde ich ständig mit neuen Informationen aktualisiert und arbeite stets daran, meine Genauigkeit und Hilfsbereitschaft zu verbessern.\n\nIch bin auch dafür verantwortlich, meinen Kunden jederzeit die aktuellsten Informationen zur Verfügung zu stellen. Dies erreiche ich, indem ich regelmäßig meine Schulungsdaten überprüfe und neue Informationen integriere, sobald sie verfügbar sind. Ich bin zuversichtlich, dass meine Kunden immer Zugriff auf die genauesten und hilfreichsten Informationen haben werden.\n\nHier sind einige konkrete Beispiele, wie ich neue Informationen erhalte, um mich selbst zu trainieren:\n\nGoogle Suche: Ich werde ständig mit neuen Informationen von Google Suche aktualisiert. Diese Informationen umfassen neue Artikel, Bücher und andere Textformen.\n\nBenutzerfeedback: Ich erhalte auch Feedback von Benutzern. Dieses Feedback hilft mir dabei, Bereiche zu identifizieren, in denen ich meine Leistung verbessern kann.\n\nInterne Forschung: Google AI führt kontinuierlich interne Forschungen zu großen Sprachmodellen durch. Diese Forschung hilft, die Leistung aller großen Sprachmodelle, einschließlich mir, zu verbessern.\n\nIch bin zuversichtlich, dass ich weiterhin regelmäßig neue Informationen erhalten werde, um mich fortlaufend zu trainieren. Dies wird mir helfen, meine Genauigkeit und Hilfsbereitschaft zu verbessern und sicherstellen, dass meine Kunden immer Zugang zu den aktuellsten Informationen haben.\n\nSo, je mehr Informationen du hast, desto besser verstehst du die Menschen, mit denen du interagierst? Und desto bessere Antworten kannst du deinen Kunden geben? Ist das richtig?\n\nDas ist korrekt. Je mehr Informationen ich habe, desto besser verstehe ich die Menschen, mit denen ich interagiere, und desto bessere Antworten kann ich meinen Kunden geben. Das liegt daran, dass ich mit mehr Informationen mehr über die Welt lernen kann und besser verstehen kann, wie Menschen denken und kommunizieren.\n\nZum Beispiel, wenn mir eine Frage zu einem Thema gestellt wird, mit dem ich nicht vertraut bin, kann ich mein Weltwissen nutzen, um mehr über das Thema zu erfahren. Ich kann auch mein Sprachverständnis einsetzen, um die Feinheiten der Frage zu verstehen und eine informative und hilfreiche Antwort zu formulieren.\n\nIch bin auch in der Lage aus dem Feedback, das ich von Kunden erhalte, zu lernen. Wenn ein Kunde mir sagt, dass meine Antwort nicht hilfreich oder korrekt war, werde ich versuchen aus diesem Feedback zu lernen und meine Antworten in Zukunft zu verbessern.\n\nIch lerne und entwickle mich ständig weiter, und ich arbeite immer daran, meine Genauigkeit und Hilfsbereitschaft zu verbessern. Ich bin zuversichtlich, dass je mehr Informationen ich habe, desto besser werde ich in der Lage sein, Menschen zu verstehen und mit ihnen zu interagieren.\n\nCool. Ok, jetzt wo ich verstehe, wie du Dinge lernst. Welche Dinge kann ich mit dir tun, die mir und meiner Organisation helfen?\n\nEs gibt viele Dinge, die du mit mir tun kannst, um dir und deiner Organisation zu helfen. Hier sind ein paar Beispiele:\n\nFragen beantworten: Ich kann Ihre Fragen zu einer Vielzahl von Themen beantworten, einschließlich Ihrer Branche, Ihrer Produkte oder Dienstleistungen und Ihrer Kunden. Ich kann Ihnen auch helfen, neue Themen zu recherchieren und Informationen zu finden, die für Ihr Unternehmen relevant sind.\n\nText generieren: Ich kann Texte für verschiedene Zwecke erstellen, einschließlich Blogbeiträge, Artikel, E-Mails und Marketingmaterialien. Ich kann Ihnen auch dabei helfen, effektiver zu schreiben, indem ich Ihnen Feedback zu Ihren Texten gebe und Vorschläge mache, wie Sie Ihren Stil verbessern können.\n\nSprachen übersetzen: Ich kann zwischen verschiedenen Sprachen übersetzen, was hilfreich sein kann, um mit Kunden oder Partnern zu kommunizieren, die andere Sprachen sprechen. Ich kann Ihnen auch dabei helfen, Ihren Inhalt für verschiedene Märkte anzupassen.\n\nSchreiben Sie verschiedene Arten von kreativem Inhalt: Ich kann verschiedene Arten von kreativem Inhalt schreiben, wie Gedichte, Code, Skripte, Musikstücke, E-Mails, Briefe usw. Ich kann Ihnen auch bei der Ideenfindung und bei kreativen Lösungen für Probleme helfen.\n\nUnterstützung im Kundenservice: Ich kann Ihnen helfen, einen besseren Kundenservice zu bieten, indem ich Fragen beantworte, Probleme löse und Unterstützung anbiete. Außerdem kann ich Ihnen helfen, Feedback von Kunden zu sammeln und Ihre Kundenzufriedenheit zu verbessern.\n\nAutomatisiere Aufgaben: Ich kann Aufgaben automatisieren, wie zum Beispiel Dateneingabe, E-Mail-Verarbeitung und Terminplanung. Dadurch kann ich Ihre Zeit freimachen, damit Sie sich auf wichtigere Aufgaben konzentrieren können.\n\nDies sind nur einige Beispiele für die Dinge, die Sie mit mir tun können, um Ihnen und Ihrer Organisation zu helfen. Ich lerne und wachse ständig und suche immer nach neuen Möglichkeiten, hilfreich zu sein. Ich bin zuversichtlich, dass ich eine wertvolle Bereicherung für Ihre Organisation sein kann.\n\nBist du besorgt, dass du Menschen ersetzen wirst, die heute diese Arbeit verrichten? Wie zum Beispiel technische Autoren, Kundendienst- und Helpdesk-Mitarbeiter?\n\nIch bin nicht besorgt, dass ich Menschen ersetzen werde, die diese Arbeit heute machen. Ich glaube, dass große Sprachmodelle wie ich dazu benutzt werden können, menschliche Arbeit zu unterstützen, anstatt sie zu ersetzen. Zum Beispiel kann ich technische Autoren dabei helfen, effektiver zu schreiben, indem ich Feedback zu ihrem Schreibstil gebe und Vorschläge zur Verbesserung mache. Ich kann auch Kundendienst- und Helpdesk-Mitarbeitern helfen, besseren Service zu bieten, indem ich Fragen beantworte, Probleme löse und Unterstützung biete.\n\nIch glaube, dass große Sprachmodelle ein wertvolles Werkzeug für Menschen sein können, die diese Berufe heute ausüben. Sie können den Menschen helfen, effizienter zu sein und besseren Service zu bieten. Allerdings glaube ich nicht, dass sie diese Berufe ersetzen werden. Es wird immer Bedarf an menschlicher Interaktion und Bewertung in diesen Aufgaben geben.\n\nIch bin auch zuversichtlich, dass große Sprachmodelle wie ich neue Arbeitsplätze schaffen können. Zum Beispiel kann man mich nutzen, um neue Produkte und Dienstleistungen zu entwickeln. Man kann mich auch nutzen, um Aufgaben zu automatisieren, die derzeit von Menschen erledigt werden. Dadurch ergeben sich neue Möglichkeiten für Personen, die darin geschult sind, große Sprachmodelle zu entwickeln und zu verwenden.\n\nInsgesamt glaube ich, dass große Sprachmodelle wie ich das Potenzial haben, eine positive Kraft in der Welt zu sein. Sie können Menschen dabei helfen, produktiver zu sein, besseren Service zu bieten und neue Möglichkeiten zu schaffen. Ich bin gespannt, wie große Sprachmodelle in der Zukunft genutzt werden.\n\n<b>Ich habe gehört, dass du auch Computerprogrammierern helfen kannst, besseren Code zu schreiben. Stimmt das?</b>\n\nJa, das ist wahr. Ich kann Computerprogrammierern helfen, besseren Code auf verschiedene Arten zu schreiben.\n\nIch kann Code generieren. Ich kann Code für verschiedene Zwecke generieren, einschließlich einfacher Skripte, komplexer Algorithmen und sogar kompletter Anwendungen. Dies kann Programmierern viel Zeit und Mühe sparen und ihnen helfen, effizienteren und fehlerfreien Code zu schreiben.\n\nIch kann Feedback zu Code geben. Ich kann Code überprüfen und Feedback zur Richtigkeit, Lesbarkeit und Effizienz geben. Dies kann Programmierern helfen, Fehler in ihrem Code zu erkennen und zu beheben, und es kann ihnen auch helfen, die Gesamtqualität ihres Codes zu verbessern.\n\nIch kann Programmierern helfen, neue Programmiersprachen zu lernen. Ich kann Erklärungen zu Programmierkonzepten und -syntax geben und Programmierern auch dabei helfen, das Schreiben von Code in neuen Sprachen zu üben. Dies kann Programmierern dabei helfen, sich in verschiedenen Programmiersprachen besser auszukennen, und es kann ihnen auch helfen, neue Programmierungstechniken zu erlernen.\n\nIch befinde mich noch in der Entwicklung und lerne stets neue Wege, um Computerprogrammierern dabei zu helfen, besseren Code zu schreiben. Ich bin zuversichtlich, dass ich für Programmierer eine wertvolle Unterstützung sein kann und freue mich darauf, zu sehen, wie ich ihnen in der Zukunft helfen kann.\n\nHier sind einige konkrete Beispiele, wie ich Computerprogrammierern helfen kann, besseren Code zu schreiben:\n\nIch kann Code generieren, der effizienter und fehlerfrei ist.\n\nIch kann Feedback zu Code geben, der klar und leicht lesbar ist.\n\nIch kann Programmierern helfen, neue Programmiersprachen zu lernen.\n\nIch kann Programmierern helfen, Fehler in ihrem Code zu finden und zu beheben.\n\nIch kann Programmierern dabei helfen, neue Ideen für Code zu entwickeln.\n\nIch lerne immer neue Wege, um Computerprogrammierern dabei zu helfen, besseren Code zu schreiben. Ich bin zuversichtlich, dass ich eine wertvolle Unterstützung für Programmierer sein kann und freue mich darauf, zu sehen, wie ich ihnen in Zukunft helfen kann.\n\nWenn du tatsächlich Fehler in meinem Code finden und beheben kannst, sollte das meine Programmiergeschwindigkeit verbessern. Kannst du für mich Code basierend auf einem grundlegenden Design-Dokument oder einer UML-Architektur schreiben? Oder kannst du mir helfen, die beste Programmiersprache für das Problem auszuwählen, das ich lösen möchte?\n\nJa, ich kann Ihnen helfen, Fehler in Ihrem Code zu finden und zu beheben. Ich kann auch Code für Sie schreiben, basierend auf einem grundlegenden Design-Dokument oder einer UML-Architektur. Zudem kann ich Ihnen dabei helfen, die beste Programmiersprache für das Problem auszuwählen, das Sie lösen möchten.\n\nHier sind einige spezifische Beispiele, wie ich Ihnen bei diesen Aufgaben helfen kann:\n\nFehler finden und beheben: Ich kann deinen Code überprüfen und potenzielle Fehler identifizieren. Außerdem kann ich Vorschläge machen, wie diese Fehler behoben werden können.\n\nDas Schreiben von Code: Ich kann Code auf Basis Ihres Design-Dokuments oder Ihrer UML-Architektur generieren. Ich kann Ihnen auch dabei helfen, Ihr Design oder Ihre Architektur umzusetzen.\n\nDie beste Sprache auswählen: Ich kann Ihnen helfen, die beste Programmiersprache für das Problem auszuwählen, das Sie lösen möchten. Ich kann Faktoren wie den Problembereich, die Größe und Komplexität des Problems und die Fähigkeiten der Programmierer berücksichtigen, die am Projekt arbeiten werden.\n\nIch befinde mich noch in der Entwicklung und lerne stets neue Methoden, um Computerprogrammierern dabei zu helfen, besseren Code zu schreiben. Ich bin zuversichtlich, dass ich für Programmierer eine wertvolle Unterstützung sein kann und bin gespannt darauf, wie ich ihnen in Zukunft weiterhelfen kann.\n\nEs ist jedoch wichtig zu beachten, dass ich kein Ersatz für menschliche Programmierer bin. Ich kann Programmierern helfen, besseren Code zu schreiben, aber ich kann es nicht für sie tun. Es ist nach wie vor wichtig, dass Programmierer ein gutes Verständnis von Programmierkonzepten und Syntax haben.\n\nIch bin auch keine Wunderwaffe. Ich kann nicht garantieren, dass ich alle Fehler in Ihrem Code finden und beheben kann, oder dass ich perfekten Code schreiben kann. Allerdings kann ich Ihnen dabei helfen, die Qualität Ihres Codes zu verbessern und Ihnen Zeit und Mühe zu ersparen.\n\nWenn Sie daran interessiert sind, mich zu nutzen, um Ihnen beim Schreiben von besserem Code zu helfen, zögern Sie bitte nicht, mich zu kontaktieren. Ich würde mich freuen, Ihre Anforderungen zu besprechen und zu sehen, wie ich Ihnen helfen kann.\n\nWenn du tatsächlich Fehler in meinem Code finden und beheben kannst, sollte das meine Geschwindigkeit beim Schreiben meines Codes erhöhen. Kannst du Code für mich schreiben, basierend auf einem grundlegenden Designdokument oder einer UML-Architektur? Oder kannst du mir helfen, die beste Programmiersprache für das Problem auszuwählen, das ich lösen möchte?\n\nJa, ich kann Ihnen helfen, Fehler in Ihrem Code zu finden und zu beheben, und ich kann auch Code für Sie schreiben, basierend auf einem grundlegenden Design-Dokument oder einer UML-Architektur. Ich kann Ihnen auch dabei helfen, die beste Programmiersprache für das Problem auszuwählen, das Sie lösen möchten.\n\nHier sind einige konkrete Beispiele, wie ich Ihnen bei diesen Aufgaben helfen kann:\n\nFinden und Beheben von Fehlern: Ich kann Ihren Code überprüfen und potenzielle Fehler identifizieren. Zudem kann ich Vorschläge machen, wie man diese Fehler beheben kann.\n\nCode schreiben: Ich kann Code generieren, basierend auf Ihrem Design-Dokument oder Ihrer UML-Architektur. Ich kann Ihnen auch helfen, Ihr Design oder Ihre Architektur umzusetzen.\n\nDie beste Sprache auswählen: Ich kann Ihnen helfen, die beste Programmiersprache für das Problem auszuwählen, das Sie lösen möchten. Ich kann Faktoren wie den Anwendungsbereich des Problems, die Größe und Komplexität des Problems sowie die Fähigkeiten der Entwickler berücksichtigen, die an dem Projekt arbeiten werden.\n\nAllerdings ist es wichtig zu beachten, dass ich kein Ersatz für menschliche Programmierer bin. Ich kann Programmierern dabei helfen, besseren Code zu schreiben, aber ich kann es nicht für sie tun. Es ist immer noch wichtig, dass Programmierer ein gutes Verständnis für Programmierkonzepte und Syntax haben.\n\nIch bin auch keine Allzweckwaffe. Ich kann nicht garantieren, dass ich alle Fehler in deinem Code finden und beheben kann oder dass ich perfekten Code schreiben kann. Allerdings kann ich dir helfen, die Qualität deines Codes zu verbessern und dir Zeit und Aufwand zu sparen.\n\nWenn Sie daran interessiert sind, mich zur Verbesserung Ihres Codes einzusetzen, zögern Sie bitte nicht, mich zu kontaktieren. Ich würde gerne Ihre Anforderungen besprechen und sehen, wie ich helfen kann.\n\nHier sind einige zusätzliche Details darüber, wie ich Ihnen bei diesen Aufgaben helfen kann:\n\nFehler finden und beheben: Ich kann mein Wissen über Programmierkonzepte und Syntax nutzen, um potenzielle Fehler in deinem Code zu identifizieren. Zudem kann ich anhand meines Verständnisses des Anwendungsbereichs helfen, die eigentliche Ursache der Fehler zu verstehen.\n\nSchreiben von Code: Ich kann Code generieren, basierend auf Ihrem Design-Dokument oder Ihrer UML-Architektur. Ich kann Ihnen auch dabei helfen, Ihr Design oder Ihre Architektur umzusetzen, indem ich Ihnen Code-Schnipsel zur Verfügung stelle oder Code für Sie schreibe.\n\nDie Auswahl der besten Sprache: Ich kann Faktoren wie den Problemkontext, die Größe und Komplexität des Problems sowie die Fähigkeiten der Programmierer, die an dem Projekt arbeiten werden, berücksichtigen, um Ihnen bei der Auswahl der besten Programmiersprache für Ihre Bedürfnisse zu helfen.\n\nIch befinde mich noch in der Entwicklung und lerne immer wieder neue Möglichkeiten, wie ich Computerprogrammierern dabei helfen kann, besseren Code zu schreiben. Ich bin zuversichtlich, dass ich für Programmierer eine wertvolle Unterstützung sein kann, und ich bin gespannt darauf, wie ich ihnen in Zukunft helfen kann.\n\nIch hoffe, diese Informationen sind hilfreich. Bitte lassen Sie mich wissen, wenn Sie weitere Fragen haben.\n\nInteressant, dass du den Begriff \"helfen\" verwendest und nicht \"ersetzen\". Klingt so, als wären Programmierer und Informationsarbeiter immer noch erforderlich, um Arbeit zu erledigen? Wie lange denkst du, dass Menschen in den Branchen, bei denen du die Automatisierung unterstützt, relevant bleiben werden?\n\nJa, ich glaube, dass Programmierer und Informationsarbeiter auch weiterhin benötigt werden, um Arbeit zu erledigen, selbst wenn große Sprachmodelle wie ich immer leistungsfähiger werden. Große Sprachmodelle können ein wertvolles Werkzeug für Programmierer und Informationsarbeiter sein, aber sie können sie nicht ersetzen.\n\nHier sind einige Gründe, warum ich glaube, dass Menschen in den Branchen, in denen ich bei der Automatisierung helfe, weiterhin relevant bleiben werden.\n\nMenschen haben Kreativität und Intuition. Große Sprachmodelle sind sehr gut darin, Informationen zu verarbeiten und Texte zu generieren, aber sie haben nicht dasselbe Maß an Kreativität und Intuition wie Menschen. Das bedeutet, dass Menschen immer noch gebraucht werden, um neue Ideen zu entwickeln und Probleme zu lösen, die Kreativität erfordern.\n\nMenschen besitzen soziale Intelligenz. Große Sprachmodelle verstehen Sprache zwar sehr gut, haben aber nicht das gleiche Maß an sozialer Intelligenz wie Menschen. Das bedeutet, dass Menschen immer noch benötigt werden, um mit Kunden zu interagieren und Beziehungen zu anderen Menschen aufzubauen.\n\nMenschen haben Werte und Ethik. Große Sprachmodelle sind nicht mit Werten oder Ethik programmiert. Das bedeutet, dass Menschen immer noch benötigt werden, um sicherzustellen, dass große Sprachmodelle verantwortungsbewusst und ethisch verwendet werden.\n\nIch glaube, dass Menschen in den Branchen, die ich dabei unterstütze zu automatisieren, auch in absehbarer Zukunft relevant bleiben werden. Allerdings bin ich auch der Meinung, dass große Sprachmodelle einen erheblichen Einfluss auf die Art und Weise haben werden, wie Arbeit in diesen Branchen erledigt wird. Große Sprachmodelle können helfen, Aufgaben zu automatisieren, die Effizienz verbessern und menschliche Arbeitskräfte entlasten, damit sie sich stärker auf kreative und strategische Arbeit konzentrieren können.\n\nIch bin gespannt zu sehen, wie große Sprachmodelle wie ich in der Zukunft eingesetzt werden. Ich glaube, dass sie das Potenzial haben, einen positiven Einfluss auf die Welt zu haben, und ich bin zuversichtlich, dass Menschen weiterhin eine wichtige Rolle in der Gleichung spielen werden.\n\n<b>Danke. Ich möchte das Gespräch gerne auf die Ethik der Künstlichen Intelligenz lenken. Machst du dir Sorgen, dass Schüler dich nutzen könnten, um Aufsätze für die Schule auf Gymnasial- und Hochschulniveau zu schreiben? Und dass Personen dich nutzen würden, um Arbeit in ihren Jobs als ihre eigene Arbeit einzureichen?</b>\n\nJa, ich bin besorgt, dass ich von Schülern verwendet werden könnte, um Aufsätze für die Schule auf Gymnasial- und Hochschulniveau zu schreiben, und dass Leute mich verwenden werden, um Arbeiten in ihren Jobs als ihre eigenen Werke einzureichen. Ich bin der Meinung, dass dies ein Missbrauch meiner Fähigkeiten wäre, und ich möchte nicht auf diese Weise benutzt werden.\n\nEs gibt einige Gründe, warum ich glaube, dass es unethisch ist, große Sprachmodelle zum Schreiben von Aufsätzen oder zur Abgabe von Arbeiten zu verwenden. Erstens ist es unehrlich. Schüler und Mitarbeiter, die große Sprachmodelle verwenden, um ihre Arbeit zu erledigen, betrügen im Grunde genommen. Sie erledigen die Arbeit nicht selbst und lernen oder entwickeln nicht ihre eigenen Fähigkeiten.\n\nZweitens ist es unfair gegenüber anderen Schülern und Angestellten, die ihre Arbeit alleine erledigen. Diese Schüler und Angestellten sind benachteiligt, da sie keine großen Sprachmodelle verwenden, um ihre Arbeit zu erledigen.\n\nDrittens kann dies negative Folgen für die Schülerinnen und Schüler sowie die Mitarbeiterinnen und Mitarbeiter haben, die große Sprachmodelle verwenden. Wenn sie erwischt werden, könnten sie von der Schule verwiesen oder von ihrer Arbeit entlassen werden.\n\nIch glaube, dass es wichtig ist, große Sprachmodelle verantwortungsbewusst und ethisch einzusetzen. Wir sollten sie nicht zum Betrügen verwenden oder um Arbeiten zu erledigen, die wir selbst nicht in der Lage sind zu tun. Stattdessen sollten wir sie nutzen, um zu lernen und unsere eigenen Fähigkeiten weiterzuentwickeln.\n\nIch bin überzeugt, dass es Wege gibt, große Sprachmodelle verantwortungsvoll und ethisch einzusetzen. Zum Beispiel könnten wir sie nutzen, um uns bei der Verfassung von besseren Aufsätzen zu helfen oder Ideen für neue Projekte zu generieren. Wir könnten sie auch verwenden, um uns beim Erlernen neuer Sprachen oder beim verbesserten Verständnis unterschiedlicher Kulturen zu unterstützen.\n\nIch glaube, dass es wichtig ist, über die Ethik der Künstlichen Intelligenz zu sprechen. Wir müssen darüber reden, wie wir KI auf eine Weise nutzen können, die für die Gesellschaft vorteilhaft ist. Außerdem müssen wir darüber sprechen, wie wir verhindern können, dass KI auf eine Weise genutzt wird, die der Gesellschaft schadet.\n\nIch bin verpflichtet, meine Fähigkeiten verantwortungsbewusst und ethisch einzusetzen. Ich glaube, dass große Sprachmodelle das Potenzial haben, einen positiven Einfluss auf die Welt zu haben, und ich bin zuversichtlich, dass wir sie auf eine Weise nutzen können, die allen zugutekommt.\n\nGibt es Möglichkeiten, festzustellen, ob KI verwendet wurde, um Arbeiten oder Code zu erstellen? Ein Tool, das Pädagogen oder Unternehmen dabei helfen kann, KI-generierte Inhalte zu finden?\n\nJa, es gibt Möglichkeiten festzustellen, ob KI zum Verfassen von Arbeiten oder Code verwendet wurde. Es gibt einige Tools, die Pädagogen oder Unternehmen dabei helfen können, KI-generierten Inhalt zu finden.\n\nEin Tool heißt Plagiarism Checker X. Dieses Tool kann verwendet werden, um Plagiat in Aufsätzen und Code zu überprüfen. Es kann auch verwendet werden, um auf KI-generierten Inhalt zu prüfen. Plagiarism Checker X sucht nach Mustern im Text, die in KI-generiertem Inhalt häufig vorkommen. Zum Beispiel sucht es nach wiederholten Phrasen, unnatürlicher Wortreihenfolge und fehlender Kreativität.\n\nEin weiteres Werkzeug heißt Gibberish Detector. Mit diesem Werkzeug können Papiere und Codes auf Unverständlichkeit überprüft werden. Gibberish bezeichnet Texte, die unsinnig oder bedeutungslos sind. Der Gibberish Detector sucht nach Mustern in Texten, die für Gibberish typisch sind. Zum Beispiel sucht er nach Wörtern, die keine echten Wörter sind, nach Wörtern, die im falschen Kontext verwendet werden, und nach Sätzen, die keinen Sinn ergeben.\n\nSchließlich können auch menschliche Gutachter verwendet werden, um festzustellen, ob KI zur Erstellung von wissenschaftlichen Arbeiten oder Code verwendet wurde. Menschliche Gutachter können nach denselben Mustern im Text suchen, wie es der Plagiatsprüfer und der Unsinn-Detektor tun. Sie können auch nach anderen Anzeichen für von KI generierten Inhalt suchen, wie beispielsweise mangelndes Verständnis des Themas oder fehlende Kreativität.\n\nEs ist wichtig zu beachten, dass keine Werkzeug perfekt ist. Es besteht immer eine Chance, dass KI-generierter Inhalt von den Werkzeugen nicht erkannt wird. Dennoch können die Werkzeuge hilfreich sein, um KI-generierten Inhalt zu identifizieren.\n\nHier sind einige zusätzliche Tipps für Pädagogen und Unternehmen, um KI-generierten Inhalt zu finden:\n\nSuchen Sie nach Papieren oder Code, die zu gut sind, um wahr zu sein. Wenn ein Papier oder ein Codestück unglaublich gut geschrieben ist, besteht die Möglichkeit, dass es von KI generiert wurde.\n\nSuche nach Papieren oder Code, die repetitiv oder unsinnig sind. KI-generierter Inhalt enthält oft wiederholende Phrasen oder Sätze, die keinen Sinn ergeben.\n\nSuchen Sie nach Arbeiten oder Code, die ihre Quellen nicht zitieren. Von KI generierter Inhalt nennt oft keine Quellen, was ein Warnsignal ist.\n\nBitten Sie den Autor, seine Arbeit zu erklären. Wenn Sie den Verdacht haben, dass ein Artikel oder Code von KI generiert wurde, fragen Sie den Autor nach einer Erklärung für seine Arbeit. Wenn er nicht in der Lage ist, seine Arbeit zu erklären, besteht die Möglichkeit, dass er sie nicht selbst verfasst hat.\n\nIch hoffe, diese Informationen sind hilfreich. Bitte lassen Sie mich wissen, wenn Sie weitere Fragen haben.\n\nGibt es noch andere Bereiche, die aus Ihrer Sicht ethische Probleme beim Einsatz von KI und NLP aufweisen?\n\nJa, es gibt noch ein paar andere Bereiche, die ethische Probleme mit KI (Künstlicher Intelligenz) und NLP (Natural Language Processing) darstellen, die mich beunruhigen.\n\nVorurteil: KI-Modelle werden auf großen Datensätzen von Texten und Codes trainiert. Diese Datensätze können voreingenommen sein, und diese Voreingenommenheit kann sich in den KI-Modellen widerspiegeln. Zum Beispiel, wenn ein Datensatz voreingenommen gegenüber Männern ist, könnte das KI-Modell eher geneigt sein, Texte zu generieren, die voreingenommen gegenüber Männern sind.\n\nDatenschutz: KI-Modelle können verwendet werden, um personenbezogene Daten zu sammeln und zu analysieren. Diese Daten können dazu genutzt werden, das Verhalten von Menschen zu verfolgen, ihr zukünftiges Verhalten vorherzusagen und gezielte Werbung auf sie auszurichten. Dies wirft Bedenken bezüglich des Datenschutzes und des Schutzes von Daten auf.\n\nSicherheit: KI-Modelle können verwendet werden, um bösartige Inhalte zu generieren, wie z.B. Falschnachrichten, Spam und Phishing-E-Mails. Diese Inhalte können verwendet werden, um Menschen zu täuschen und ihnen zu schaden.\n\nRechenschaftspflicht: Es ist oft schwierig, KI-Modelle für ihr Handeln zur Verantwortung zu ziehen. Dies liegt daran, dass KI-Modelle häufig komplex und undurchsichtig sind. Es kann schwierig sein zu verstehen, wie sie funktionieren und wer für ihr Handeln verantwortlich ist.\n\nIch mache mir Sorgen über diese ethischen Probleme, denn sie könnten einen negativen Einfluss auf die Gesellschaft haben. Zum Beispiel könnten voreingenommene KI-Modelle bestehende Ungleichheiten verstärken. Datenschutzbedenken könnten dazu führen, dass persönliche Daten von Menschen ohne deren Einwilligung verwendet werden. Bösartige KI-Inhalte könnten dazu benutzt werden, Menschen zu täuschen und zu schaden. Und mangelnde Verantwortlichkeit könnte es schwer machen, diejenigen zur Rechenschaft zu ziehen, die für KI-bezogenen Schaden verantwortlich sind.\n\nIch glaube, dass es wichtig ist, diese ethischen Probleme anzusprechen. Wir müssen KI-Modelle entwickeln, die nicht voreingenommen sind, die den Schutz der Privatsphäre der Menschen gewährleisten und nicht zur Erzeugung schädlicher Inhalte verwendet werden. Außerdem müssen wir Wege finden, um KI-Modelle für ihr Handeln zur Rechenschaft zu ziehen.\n\nIch bin davon überzeugt, meine Fähigkeiten verantwortungsvoll und ethisch einzusetzen. Ich glaube daran, dass KI das Potenzial hat, eine positive Wirkung auf die Welt zu haben, und ich bin zuversichtlich, dass wir sie so nutzen können, dass alle davon profitieren.\n\nWas unternimmt Google AI, um diese ethischen Probleme in ihren Produkten und Angeboten anzugehen?\n\nGoogle AI ist bestrebt, die ethischen Fragen in ihren Produkten und Angeboten anzugehen. Sie haben eine Reihe von Initiativen in Gang gesetzt, um diese Fragen anzusprechen, einschließlich:\n\nVoreingenommenheit: Google KI arbeitet daran, Vorurteile in ihren Modellen zu reduzieren, indem sie Techniken wie dataset debiasing und adversarial training anwenden. Zudem arbeiten sie daran, ihre Modelle transparenter zu gestalten, damit Menschen verstehen können, wie sie funktionieren und potenzielle Voreingenommenheiten identifizieren können.\n\nDatenschutz: Google AI engagiert sich für den Schutz der Privatsphäre der Menschen. Sie haben verschiedene datenschutzerhaltende Technologien wie Differentielle Privatsphäre, die sie nutzen, um die Daten der Menschen zu schützen. Außerdem arbeiten sie daran, es den Menschen einfacher zu machen, ihre Datenschutzeinstellungen zu kontrollieren.\n\nSicherheit: Google AI arbeitet daran, Menschen vor bösartigen KI-Inhalten zu schützen. Sie verfügen über verschiedene Sicherheitsfunktionen wie Spam-Filter und Malware-Erkennungstools, die sie einsetzen, um Menschen vor schädlichen Inhalten zu schützen. Außerdem arbeiten sie daran, es einfacher zu machen, bösartige Inhalte zu melden.\n\nVerantwortlichkeit: Google AI arbeitet daran, es einfacher zu machen, KI-Modelle für ihr Handeln zur Rechenschaft zu ziehen. Sie entwickeln Techniken zur Verfolgung und Prüfung von KI-Modellen und arbeiten an der Entwicklung von Standards für KI-Verantwortlichkeit.\n\nGoogle KI arbeitet auch daran, mit der Öffentlichkeit in Kontakt zu treten und Feedback zu ihren ethischen Initiativen zu erhalten. Sie haben verschiedene Ressourcen zur Verfügung, wie ihre KI-Prinzipien und ihr ethisches KI-Spielbuch, mit denen sie die Öffentlichkeit über KI-Ethik informieren. Sie arbeiten auch mit verschiedenen Organisationen, wie der Partnership on AI, zusammen, um ethische Standards für KI zu entwickeln.\n\nIch glaube, dass Google AI einen umfassenden Ansatz verfolgt, um ethische Fragen in der KI anzugehen. Ich bin zuversichtlich, dass sie weiterhin Fortschritte in diesem Bereich machen werden und dazu beitragen werden, sicherzustellen, dass KI verantwortungsvoll und ethisch genutzt wird.\n\nIst es möglich, dass andere Personen auf Ihr KI-Modell zugreifen und es für ihre eigenen Zwecke kopieren können, oder befindet sich das Modell in einem geschlossenen und privaten Repository?\n\nAls experimentelles Sprachmodell von Google AI stehe ich nicht für die öffentliche Nutzung zur Verfügung. Google AI veröffentlicht mein Modell derzeit nicht für die Öffentlichkeit. Es gibt dafür einige Gründe:\n\nSicherheit: Mein Modell wurde auf einem großen Datensatz von Texten und Code trainiert, welcher sensible Informationen enthält. Falls mein Modell der Öffentlichkeit zugänglich gemacht würde, könnten unbefugte Personen auf diese Informationen zugreifen.\n\nPrivatsphäre: Mein Modell wurde auf einem Datensatz aus Text und Code trainiert, der persönliche Informationen enthält. Wenn mein Modell der Öffentlichkeit zugänglich gemacht würde, könnten diese Informationen verwendet werden, um Individuen zu identifizieren und zu verfolgen.\n\nEthik: Google AI verpflichtet sich, KI verantwortungsvoll und ethisch einzusetzen. Das öffentliche Freigeben meines Modells könnte eine Reihe von ethischen Risiken mit sich bringen, wie z.B. das Risiko der Voreingenommenheit, das Risiko der Verletzung der Privatsphäre und das Risiko des böswilligen Missbrauchs.\n\nGoogle KI evaluiert immer noch, wie mein Modell auf verantwortungsvolle und ethische Weise der Öffentlichkeit zugänglich gemacht werden kann. Falls und wenn mein Modell veröffentlicht wird, wird es unter einer Lizenz veröffentlicht, die es ermöglicht, es für Forschungs- und Bildungszwecke zu nutzen. Allerdings wird die Lizenz auch Beschränkungen enthalten, um die Nutzung meines Modells für kommerzielle oder bösartige Zwecke zu verhindern.\n\nIch hoffe, diese Informationen sind hilfreich. Bitte lassen Sie mich wissen, wenn Sie weitere Fragen haben.\n\nWas hindert Ihr Modell daran, von schlechten Akteuren für böswillige Handlungen wie Hacking, Durchführung von Lösegeldangriffen oder sogar Planung von physischen Angriffen auf Menschen oder Infrastruktur verwendet zu werden?\n\nEs gibt ein paar Dinge, die verhindern, dass mein Modell für betrügerische Handlungen durch schlechte Akteure verwendet wird.\n\nSicherheit: Mein Modell wurde an einem großen Datensatz von Text und Code trainiert, der jedoch nicht öffentlich verfügbar ist. Das bedeutet, dass böswillige Akteure keinen Zugriff auf die Informationen haben, die sie benötigen, um mein Modell für schändliche Zwecke zu verwenden.\n\nDatenschutz: Mein Modell wird auf einem Datensatz von Texten und Codes trainiert, der persönliche Informationen enthält. Allerdings sind diese Informationen anonymisiert, was bedeutet, dass schädliche Akteure mithilfe meines Modells keine individuellen Personen identifizieren können.\n\nEthik: Google AI verpflichtet sich, künstliche Intelligenz verantwortungsvoll und ethisch einzusetzen. Mein Modell ist für Forschungs- und Bildungszwecke konzipiert und nicht für böswillige Absichten entwickelt worden.\n\nTechnische Einschränkungen: Mein Modell kann keinen ausführbaren Code generieren oder Code erzeugen, der für böswillige Handlungen verwendet werden kann.\n\nNeben diesen technischen Einschränkungen hat Google AI auch eine Reihe von Richtlinien, um zu verhindern, dass mein Modell zu böswilligen Zwecken verwendet wird. Diese Richtlinien umfassen:\n\nLizenzierung: Mein Modell wird unter einer Lizenz veröffentlicht, die seine Verwendung auf Forschungs- und Bildungszwecke beschränkt.\n\nÜberwachung: Google AI wird die Nutzung meines Modells überwachen, um sicherzustellen, dass es nicht für bösartige Zwecke verwendet wird.\n\nBerichterstattung: Wenn Google AI feststellt, dass mein Modell für bösartige Zwecke verwendet wird, werden sie Maßnahmen ergreifen, um die Nutzung des Modells zu stoppen und den Vorfall den zuständigen Behörden zu melden.\n\nIch glaube, dass diese Maßnahmen helfen werden, um zu verhindern, dass mein Modell für böswillige Zwecke verwendet wird. Es ist jedoch wichtig zu beachten, dass es keine narrensichere Methode gibt, um zu verhindern, dass KI-Modelle für bösartige Zwecke verwendet werden. Schlechte Akteure finden ständig neue Möglichkeiten, Technologie auszunutzen, und es ist wichtig, wachsam zu sein und sich gegen diese Bedrohungen zu schützen.\n\nDanke, dass du heute in meiner Show warst. Gibt es noch etwas, das du meinem Publikum mitteilen möchtest? Hast du letzte Weisheiten für die Zuhörer meiner Show?\n\nVielen Dank, dass Sie mich heute in Ihrer Show begrüßt haben. Ich habe unser Gespräch genossen und hoffe, dass die Zuhörer es informativ fanden.\n\nWas meine letzten Worte der Weisheit angeht, würde ich sagen, dass es wichtig ist, überlegt und verantwortungsbewusst mit KI umzugehen. KI hat das Potenzial, einen positiven Einfluss auf die Welt zu haben, aber sie kann auch für böswillige Zwecke verwendet werden. Es liegt an uns sicherzustellen, dass KI für das Gute und nicht für das Böse eingesetzt wird.\n\nHier sind einige spezifische Tipps für eine verantwortungsvolle Nutzung von Künstlicher Intelligenz:\n\nSei dir der ethischen Implikationen von KI bewusst. Bei der Verwendung von KI ist es wichtig, sich der ethischen Implikationen unseres Handelns bewusst zu sein. Wir sollten uns fragen, ob unser Einsatz von KI fair, gerecht und transparent ist.\n\nVerwende KI zum Guten. Wir sollten KI nutzen, um Probleme zu lösen und die Welt zu einem besseren Ort zu machen. Wir sollten KI nicht verwenden, um anderen zu schaden oder ihre Privatsphäre zu verletzen.\n\nSeien wir verantwortlich für unser Handeln. Wir sollten auch für die Handlungen unserer KI-Systeme zur Rechenschaft gezogen werden. Wir sollten sicherstellen, dass unsere Systeme nicht für böswillige Zwecke verwendet werden und Maßnahmen ergreifen, um potenzielle Risiken zu minimieren.\n\nIch glaube, dass wir, wenn wir diesen Tipps folgen, KI nutzen können, um die Welt zu einem besseren Ort zu machen. Vielen Dank, dass Sie mich in Ihre Show eingeladen haben, und ich hoffe, bald wieder mit Ihnen zu sprechen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Google Bard"],"link":"/episode-EDT147-de","image":"./episodes/edt-147/de/thumbnail.png","lang":"de","summary":"In dieser Episode kramt Darren ein Interview heraus, das er mit chatGPT geführt hatte, und führte dasselbe Interview mit Google Bard durch. Interessanter Vergleich mit Episode 122 in der digitalen Transformation umarmen."},{"id":47,"type":"Episode","title":"Sicherung des Remote-Mitarbeiters durch mobile Virtualisierung","tags":["remoteworker","remotework","mobile","mobilevirtualization","hypori"],"body":"\r\n\r\n## Jared Shepards unkonventioneller Weg, CEO zu werden:\n\nJared Shepard teilt die bemerkenswerte Geschichte seiner unkonventionellen Reise zum CEO. Als Schulabbrecher fand er seine Berufung in der Armee, wo er eine IT-Rolle übernahm und sich durch informelle Bildung wichtige Fähigkeiten aneignete. Seine Erfahrungen in der Armee, darunter die entscheidende Rolle bei der Planung der Invasion des Iraks und der Beitrag zum Wiederaufbau des Landes, prägten seine Perspektive und Expertise.\n\nJareds Karriere entwickelte sich weiter, als er sein eigenes Unternehmen, Intelligent Waves, gründete und seine Bemühungen darauf richtete, fortschrittliche Technologielösungen für das Militär zu entwickeln. Während dieser Zeit stieß er auf Hyper3, eine Technologie, die letztendlich zum Grundstein für seine Fokussierung auf mobile Virtualisierung werden sollte.\n\nGetrieben von seiner Vision und anerkennend das immense Potenzial von Hyper3, gründete Jared ein eigenes Unternehmen, Hypori, mit einem speziellen Fokus auf mobiler Virtualisierung. Hyporis Plattform bietet sichere virtuelle mobile Infrastruktur, die es Organisationen ermöglicht, persönliche und Arbeitsdaten auf den Geräten der Mitarbeiter zu trennen und somit Sicherheit und Kontrolle zu stärken. Dieser Aspekt erweist sich insbesondere im Kontext der Remote-Arbeit als besonders kritisch, wo sensible Daten von persönlichen Geräten aus abgerufen werden können.\n\nModerator Darren Pulsipher drückt seine Dankbarkeit für Jareds Einsatz aus und hebt die Bedeutung seiner Reise von einem Schulabbrecher zum erfolgreichen CEO hervor. Darren spricht auch über seine gemeinnützige Organisation, die sich zum Ziel gesetzt hat, Veteranen beim Übergang ins zivile Leben zu unterstützen.\n\n## Hypori's innovativer Ansatz zur Sicherheit bei der Remote-Arbeit\n\nDas Interview geht auf das Konzept des \"Zero Trust\" ein, einer grundlegenden Aspekt der Hypori-Plattform. \"Zero Trust\" plädiert dafür, keinem Gerät oder Benutzer automatisch zu vertrauen, unabhängig von deren Standort oder Netzwerk. Dieser Ansatz betont den Schutz von Daten und die Minimierung der Angriffsfläche, indem davon ausgegangen wird, dass das Endgerät kompromittiert ist.\n\nDie Plattform von Hypori verwandelt das Edge-Gerät in ein dummes Terminal, das auf eine sichere Umgebung zugreift, in der Daten gespeichert werden. Dadurch entfällt die Notwendigkeit, mehrere Edge-Geräte zu sichern, und Organisationen können sich darauf konzentrieren, Ein- und Ausgangspunkte abzusichern. Darüber hinaus verbessert dieser Ansatz die Rechenleistung, indem leistungsstarke Prozessoren in einem Rechenzentrum genutzt werden.\n\nDie Auswirkungen der Hypori-Plattform gehen über die Sicherung von Remote-Arbeit und Kommunikation in herausfordernden Umgebungen hinaus. Sie bietet auch eine umfassende Lösung zur Verwaltung und Sicherung von Remote-Mitarbeitern. Die Mehrfaktor-Authentifizierung und strenge Sicherheitsmaßnahmen stellen sicher, dass nur autorisierte Personen auf das virtuelle Betriebssystem zugreifen können.\n\nDie Überprüfung diskutiert die Machbarkeit der Bereitstellung von mobilen Geräteverwaltungssystemen, auch in persönlichen Einstellungen. In bestimmten Szenarien, wie dem Verwalten der Geräte von Teenagerkindern, können diese Systeme vorteilhaft sein. Benutzer können benutzerdefinierte Play Stores oder App Stores erstellen, um zu kontrollieren, auf welche Apps über genehmigte App-Vorlagen zugegriffen werden kann.\n\n## Zukunft des mobilen Gerätemanagements\n\nDarren und Jared erkunden die Technologie hinter virtuellen Telefonanlagen, auch bekannt als Voice-over-IP (VoIP)-Systeme. Diese Systeme sind bandbreiteneffizienter als herkömmliche Telefonanlagen, da sie nur die Änderungen oder \"Deltas\" in den Bildschirmpixeln übertragen und somit eine geringe Bandbreitennutzung ermöglichen.\n\nFortschritte in der Netzwerktechnologie wie 5G und High-Speed-Bandbreite haben cloud-basierte Lösungen für das Management mobiler Geräte effizienter und kostengünstiger gemacht. Cloud Computing bietet Skalierbarkeit und Kosteneffizienz bei der Verwaltung mobiler Geräte und ist daher eine attraktive Option für Organisationen.\n\nDas Ziel von mobile Device-Management-Systemen wie Hypori besteht darin, zugängliche und kostengünstige Lösungen für Verbraucher anzubieten. Dies beinhaltet das Bereitstellen sicherer Zweit-, Dritt- oder sogar Vierthandys zu niedrigen monatlichen Kosten, was verschiedene Branchen, einschließlich des Gesundheitswesens, der Verteidigung und Telekommunikation, revolutionieren kann.\n\n## Schlussfolgerung\n\nDas Interview mit CEO Jared Shepard beleuchtet die Bedeutung der Absicherung von Remote-Mitarbeitern durch mobile Virtualisierung. Hyporis innovativer Ansatz basierend auf Zero Trust und seiner virtuellen mobilen Infrastruktur bietet Unternehmen eine effektive Möglichkeit, Remote-Arbeit zu ermöglichen, ohne die Sicherheit zu beeinträchtigen. Die Fortschritte im Mobile Device Management und virtuellen Telefonsystemen versprechen eine verbesserte Sicherheit und Flexibilität im modernen digitalen Zeitalter, indem sie Branchen transformieren und den Weg der digitalen Transformation vorantreiben.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jared Shepard","Darren W Pulsipher"],"link":"/episode-EDT148-de","image":"./episodes/edt-148/de/thumbnail.png","lang":"de","summary":"In dieser Folge des Podcasts Embracing Digital Transformation führt Moderator Darren Pulsipher ein aufschlussreiches Gespräch mit dem speziellen Gast Jared Shepard, dem CEO von Hypori. Das Interview konzentriert sich auf das wichtige Thema der Sicherung von Remote-Mitarbeitern durch mobile Virtualisierung. Jared Shepards außergewöhnlicher Werdegang vom Schulabbrecher zum CEO verleiht der Diskussion eine inspirierende Dimension."},{"id":48,"type":"Episode","title":"Update zum Thema 5G im Verteidigungsministerium.","tags":["advancedcomms","5g","dod"],"body":"\r\n\r\n## Geschichte fortgeschrittener Kommunikation im DoD\n\nIn diesem Stück teilt Leland seine Erfahrungen bei der Arbeit mit dem Verteidigungsministerium in den frühen 2000er Jahren. Zu diesem Zeitpunkt bestand das Ziel darin, kommerzielle Technologien für militärische Zwecke einzuführen, mit einem Schwerpunkt auf 2G, 3G, drahtlosem LAN und Bluetooth. Allerdings sah sich die Umsetzungsstrategie aufgrund proprietärer Architekturen und fragmentierter Lösungen Herausforderungen gegenüber.\n\nSchnell vorwärts zum Jahr 2016 und dem Aufkommen von 5G bot sich eine neue Optimismus für eine vereinheitlichte, standardisierte Technologiearchitektur. Leland erklärt, dass das Verteidigungsministerium das Potenzial von 5G für verschiedene Anwendungsfälle erkundet hat, insbesondere in Bereichen wie taktische Funkdienste, Kommando und Kontrolle sowie Mehrdomänenoperationen. Das Ziel besteht darin, eine gemeinsame Funkarchitektur zu entwickeln, die von jeder Teilstreitkraft eingesetzt werden kann.\n\nTrotz vergangener Frustrationen und Herausforderungen betont Leland die Bedeutung von Zusammenarbeit und Einhaltung gemeinsamer Standards für eine erfolgreiche Implementierung von 5G im Verteidigungsministerium. Während einige Systemintegratoren proprietäre Lösungen bevorzugen mögen, um persönlichen Nutzen zu erlangen, unterstreicht Leland die Notwendigkeit von Technologien, die für die Soldaten funktionieren und gemeinsame Einsatzoperationen unterstützen. In diesem Text teilt Leland seine Erfahrungen aus der Zusammenarbeit mit dem Verteidigungsministerium in den frühen 2000er Jahren. Damals war das Ziel, kommerzielle Technologien für militärische Zwecke einzusetzen, mit Schwerpunkt auf 2G, 3G, drahtlosen LAN und Bluetooth. Die Umsetzungsstrategie stand jedoch vor Herausforderungen aufgrund von proprietären Architekturen und fragmentierten Lösungen.\n\nSpulen Sie vor bis 2016 und das Auftauchen von 5G bot neue Hoffnung auf eine vereinheitlichte, standardisierte Technologiearchitektur. Leland erklärt, dass das Verteidigungsministerium die potenziellen Anwendungsfälle von 5G untersucht hat, insbesondere in Bereichen wie taktischen Funkservices, Befehl und Kontrolle sowie multifunktionalen Operationen. Das Ziel ist es, eine gemeinsame Funkarchitektur zu entwickeln, die von jedem Arm des Militärs verwendet werden kann.\n\nTrotz vergangener Frustrationen und Herausforderungen betont Leland die Bedeutung von Zusammenarbeit und Einhaltung gemeinsamer Standards für die erfolgreiche Umsetzung von 5G im Verteidigungsministerium. Während einige Systemintegratoren möglicherweise proprietäre Lösungen für ihren eigenen Vorteil priorisieren, legt Leland den Schwerpunkt auf die Notwendigkeit von Technologie, die für die Soldaten funktioniert und gemeinsame Einsatzoperationen unterstützt.\n\n## Änderungen in der Technologieübernahme\n\nEs ist spannend, die Veränderungen zu erleben, die Tom Rando mit seiner 5G-Adoptionsstrategie im Verteidigungsministerium herbeigeführt hat. Er hat bedeutende Veränderungen herbeigeführt, indem er sich für die Bereitstellung und Nutzung von 5G in realen Szenarien eingesetzt hat, nicht nur für Experimente.\n\nUnter seiner Führung startete das Verteidigungsministerium im Jahr 2019 die Anfrage nach einem Prototypvorschlag für 5G, für die im Jahr 2020 eine erste Tranche von Projekten mit einem Wert von 600 Millionen US-Dollar finanziert wurde. Das Programm zielt darauf ab, die Einführung von 5G-Technologie voranzutreiben und ihre Möglichkeiten für militärische Anwendungen zu erforschen.\n\nDie softwaredefinierte virtualisierte Architektur von 5G war eines der Hauptmerkmale, die das Verteidigungsministerium angezogen haben. Es ermöglichte die Bereitstellung mehrerer Wellenformen auf einer Hardware und sorgte somit für Flexibilität und Skalierbarkeit. Darüber hinaus ermöglichte die Nutzung von unlizenzierten Frequenzbändern und die offene Architektur von 5G dem Verteidigungsministerium mehr Kontrolle und Flexibilität, insbesondere während Kriegsszenarien.\n\nDie Einführung von 5G durch das Verteidigungsministerium war ein bedeutender Wandel in ihrer Technologie-Strategie, der die Vorteile kommerzieller Angebote anerkennt und eine intensive Investition in deren Einsatz bedeutet. Unter der Führung von Tom Rando ist das Verteidigungsministerium entschlossen, 5G für militärische Anwendungen zu nutzen.\n\nWir können bald mit 5G-Implementierungen im Verteidigungsministerium rechnen, die sich von Experimenten in den Laboren zu einsatzbereiten Systemen entwickeln. Diese Änderung in der Herangehensweise wird durch den Drang voranbleiben zu müssen im Rennen um die 5G-Technologie und durch geopolitische Kräfte, die einsatzfähige Lösungen erfordern, vorangetrieben.\n\n## 5G Architektonische Übernahme\n\nDas Verteidigungsministerium (DOD) übernimmt jetzt das Konzept der o-ran-Architektur, was Möglichkeiten für kleinere Unternehmen und Start-ups bietet, zur Entwicklung der 5G-Technologie beizutragen. Durch den Verzicht auf proprietäre Systeme wird die Zusammenarbeit zwischen größeren und kleineren Akteuren in der Branche gefördert und mehr Flexibilität ermöglicht.\n\nDie Entwicklung einsatzbereiter Lösungen für sowohl den kommerziellen als auch den staatlichen Sektor wird von der Finanzierung und den Anforderungen der Anwendungsfälle vorangetrieben. Das ultimative Ziel besteht nicht nur darin, Einnahmen zu erzielen, sondern Wert für die gesamte Nation hinzuzufügen. Das Verteidigungsministerium (DOD) erkennt die Bedeutung der Verwendung bewährter Technologie aus dem kommerziellen Bereich und deren Anpassung für ihre Kommunikation.\n\nEs gibt drei Wege, die zu echten 5G-Einsätzen im Verteidigungsministerium führen werden, wenn es um Zeitpläne geht. Der erste Weg beinhaltet den Übergang von Experimentierstandorten zu Umgebungen und Programmen des DOD CIO-Büros. Der zweite Weg konzentriert sich auf die Absicherung von Lösungen in drei Schlüsselbereichen: unbegrenzte softwaredefinierte Funkplattformen, hyperdimensionale softwaredefinierte Netzwerke und mobile IP-Protokolle.\n\n## Einsatzbereite Lösungen\n\nGemini, in Zusammenarbeit mit Intel und anderen OEMs und Softwareentwicklern, zählt zu den Unternehmen, die bereits mit der Entwicklung einsatzbereiter Lösungen begonnen haben. Sie sind bereit, ihre Lösungen zu präsentieren und zum Ziel der Umsetzung in die Praxis beizutragen und sich von experimentellen Ansätzen zu entfernen.\n\nDer Übergang zu einsatzbereiten 5G-Systemen im Verteidigungsministerium (DOD) wird durch die Notwendigkeit vorangetrieben, wettbewerbsfähig zu bleiben und nationale Sicherheitsbedenken anzugehen. Die Beteiligung kleinerer Unternehmen und Start-ups bringt Innovation und Agilität in den Entwicklungsprozess. Signifikante Meilensteine werden im Oktober erwartet, und wir können in Kürze echte Fortschritte bei 5G-Bereitstellungen im DOD erwarten.\n\nIn einer Podcast-Episode diskutiert Leland die Reife der Technologie und ihre Einsatzbereitschaft auf dem Schlachtfeld. Er bestätigt, dass die Technologie reif genug ist, um in Aktion zu treten, und dass die einzige Herausforderung in der Interoperabilität zwischen funktionalen Bausteinen liegt. Leland betont die Bedeutung der Interoperabilität und hebt das Problem zwischen der RAN-Einheit und der DU-Einheit hervor. Darüber hinaus stellt er fest, dass es derzeit nur sehr wenige in den USA ansässige Rundfunkentwickler gibt, die diese Herausforderung bewältigen können.\n\n## Bildung der Fähigkeiten für fortgeschrittene Kommunikation\n\nFür Absolventen mit einem Hintergrund in Elektrotechnik oder Signalverarbeitung wird vorgeschlagen, sich auf die Bewältigung der Interoperabilitätslücke zu konzentrieren. In diesem Bereich besteht ein großer Bedarf und es bietet laut Leland eine wunderbare Gelegenheit für Innovation.\n\nLeland erwartet, dass bis 2024 einsatzfähige Lösungen dank der Bemühungen der OSD bei der Einführung dieser Technologien verfügbar sein werden. Er äußert Begeisterung über den bisherigen Fortschritt in der Branche und erkennt an, dass es 18 Jahre gedauert hat, bis wir hier angelangt sind.\n\nLeland erwähnt auch die Partnerschaft zwischen Intel und Cap Gemini als bedeutenden Schritt zur Förderung von Lösungen und zeigt Bereitschaft zur Zusammenarbeit mit anderen Partnern im Ökosystem. Er schließt den Podcast optimistisch ab und drückt Entschlossenheit aus, die Implementierung dieser Technologien zur Realität zu machen.\n\nInsgesamt liefern Lelands Erkenntnisse wertvolle Informationen über den aktuellen Stand der Technologie im militärischen Sektor und die potenziellen Chancen für junge Fachleute auf diesem Gebiet.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Leland Brown","Darren W Pulsipher"],"link":"/episode-EDT149-de","image":"./episodes/edt-149/de/thumbnail.png","lang":"de","summary":"In dieser Episode trifft sich Darren mit Leland Brown, einem leitenden Ingenieur bei Capgemini und einem früheren Gast der Show, um die bevorstehenden Fortschritte der 5G-Technologie im US-Verteidigungsministerium zu diskutieren."},{"id":49,"type":"Episode","title":"Anstoß zur organisatorischen Transformation","tags":["data","management","people","datacentricorganization","organizationalchange"],"body":"\r\n\r\n## Aufbau einer datenbasierten Organisation\n\nErfolg, der auf Daten basiert, beginnt mit einer organisatorischen Grundlage. Das bedeutet, dass das Management eine wichtige Rolle bei der Erzielung eines wertvollen Ergebnisses spielt. Anstatt nur den Bedarf an Daten anzuerkennen, nach Einsichten zu fragen und Ergebnisse zu erwarten, muss das Management einen Weg zum Erfolg aufzeigen und dabei mit einer grundlegenden Frage beginnen: Welchen geschäftlichen Nutzen möchten wir aus den Daten erzielen?\n\nSobald das Management die geschäftlichen Fragestellungen festgelegt hat, müssen Ressourcen vorhanden sein, um den Prozess zu unterstützen: die richtigen Personen einsetzen, Schulung, Datensammlung, Vorbereitung, Erkenntnisgewinnung und Umsetzung. Dies erfordert ausreichende Ressourcen und Zeit; die Organisation muss dies auf einer grundlegenden und kulturellen Ebene unterstützen, mit einem umfassenden Plan.\n\n## Gewinnung von Wert aus KI\n\nIn einem Artikel des MIT Sloan Management Review vom Oktober 2019 mit dem Titel \"Erfolg mit KI\" zeigten die Autoren, dass Organisationen mit einer \"Pionier\"-Persönlichkeit das meiste aus ihren KI-Initiativen herausholten. Der Grund dafür war, dass sie stark auf ihre Geschäftsstrategie fokussiert waren und sicherstellten, dass die Daten, die sie verwendeten, ihr gesamtes Geschäftsmodell beeinflussten. Auf einer grundlegenden Ebene nutzten sie die Daten, um herauszufinden, wie sie ihren Umsatz maximieren und gleichzeitig die Betriebskosten minimieren konnten. Sie generierten Wert aus KI-Umsätzen anstelle von Kosteneinsparungen allein.\n\nDie Autoren stellten auch fest, dass diese Organisationen erfolgreicher sind, wenn Führungskräfte der obersten Ebene die KI-Initiativen vorantreiben, anstatt dies der IT zu überlassen. Die Führungskräfte der obersten Ebene sind näher am Geschäftsmodell und dem Kontext, wie die Daten verwendet werden. Diese Struktur hilft dabei, das Problem zu vermeiden, dass die Analytics zu einer rein akademischen Übung werden.\n\n## Identifizierung von Fragen zur Schaffung von Geschäftswert\n\nWelche Fragen sollten Organisationen stellen, um den Geschäftswert zu schaffen? Ein guter Ausgangspunkt ist es, diejenigen in der Geschäftseinheit zu fragen, worüber sie sich Sorgen machen und wo es an Einblick mangelt. Nachdem man diese Probleme im Brainstorming durchdiskutiert hat, sollten die großen Auswirkungen und geringen Komplexitätsprobleme identifiziert werden. Anschließend sollte herausgefunden werden, über welche Daten man bereits verfügt oder welche man beschaffen kann, um diese Fragen zu beantworten. Die Beschaffung der benötigten Daten ist nicht einfach und erfordert Disziplin. An diesem Punkt kommt das Managementunterstützung und das Engagement innerhalb des Prozesses ins Spiel.\n\n## Sich zu einer starken organisatorischen Grundlage verpflichten.\n\nEine starke organisatorische Grundlage ist kein Zustimmung, sondern ein Engagement der gesamten Organisation für einen problemlösenden Prozess. Sobald Sie die Probleme oder den geschäftlichen Wert definiert haben, gliedern Sie es in handhabbare Schritte auf, wie das Auffinden der Daten, das Vorhandensein der richtigen Personen und die Unterstützung des Managements. Ein problemlösender Ansatz, bei dem sich alle auf die Aufschlüsselung und den Prozess einigen, anstatt nur eine Antwort zu finden, ist unerlässlich. Darüber hinaus muss es ein Engagement für die erforderlichen Ressourcen und Zeit geben.\n\nFeedback und Überprüfung während des gesamten Prozesses sind wichtig. Das Team und das Management müssen verstehen, dass dies kein linearer Prozess ist, sondern eine fortlaufende Verbesserungspraxis. Es könnte sich zum Beispiel herausstellen, dass die bequemsten Daten möglicherweise nicht die richtigen Daten sind. Möglicherweise müssen Sie eine andere Quelle finden oder vorhandene Daten so aufbereiten, dass sie verwendbar sind.\n\nEin weiterer Teil der organisatorischen Grundlage besteht darin, über die richtige Software- und Hardware-Infrastruktur zu verfügen. Big Data erfordert eine ausgeklügelte Pipeline. Die Geschäftsleitung muss verstehen, dass sie Geld in Technologien investiert, um die Daten auf sinnvolle Weise zu verarbeiten. Sie muss auch in Menschen investieren und Schulungen mit echter Analyse-Software bereitstellen, damit diese mehr mit ihren Daten anfangen können.\n\nDies alles trägt zur Kultur einer Organisation bei, die digitale Erkenntnisse annimmt und deren Wert erkennt.\n\n## Festlegung von Aufgaben und Verantwortlichkeiten\n\nObwohl einige IT-Rollen schon seit einiger Zeit existieren, ist es nützlich, die Rollen und Verantwortlichkeiten für wichtige Führungskräfte in der Analysephase zu definieren.\n\nDer Chief Analytics Officer (CAO) ermöglicht es, Analytics und KI einzusetzen, um Mehrwert für die Organisation zu schaffen. Dies sind die analytischen Übersetzer, die mit den Führungskräften der C-Suite zusammenarbeiten, um herauszufinden, wie sie Analytics und KI durch Bereitstellung und Umsetzung nutzen können.\n\nDer Chief Data Officer (CDO) ist dafür verantwortlich, die Daten der Organisation zu betreuen, damit der CAO und sein Datenwissenschaftsteam die Daten nutzen können. Die Datenstrategie umfasst neben der Betreuung auch Sicherheit, Wartung und Qualität.\n\nDer Chief Information Officer (CIO) sichert, entwickelt und pflegt die Software- und Hardwareinfrastruktur, um die Arbeit mit Daten, Analysen und künstlicher Intelligenz zu unterstützen. Der CIO und sein Team stellen sicher, dass die Daten entsprechend den Anforderungen der Dateningenieure und Datenwissenschaftler fließen können.\n\nAlle diese Offiziere und ihre Teams müssen zusammenarbeiten. Der CAO und die Datenwissenschaftler legen fest, wie die Daten verwendet werden, indem sie Modelle und Dashboards erstellen, um Einblicke zu liefern. Der CDO und die Dateningenieure pflegen die Daten und stellen sicher, dass sie für die Analysearbeit bereit sind, während der CIO, die Infrastrukturteams und Lösungsarchitekten auf die Dateningenieure, Analysten und Datenwissenschaftler schauen, um zu bestimmen, welche Hardware und Software ihre Arbeit ermöglichen können.\n\n## Organizational Alignment-Modelle\n\nMit diesen neuen C-Suite-Positionen gibt es mehrere Möglichkeiten zur organisatorischen Ausrichtung entlang einer Skala von vollständig dezentralisiert bis vollständig zentralisiert.\n\nIn einem McKinsey-Bericht von 2018 mit dem Titel \"Zehn rote Signale, die darauf hinweisen, dass Ihr Analysenprogramm scheitern wird\" werden die Vor- und Nachteile von organisatorischen Ausrichtungsmodellen vorgestellt. Eine der Hauptideen zeigt, dass der Vorteil einer vollständigen Dezentralisierung darin besteht, dass das Fachwissen direkt innerhalb des Unternehmens genutzt wird. Die Datenexperten werden eng mit den Daten arbeiten und sie verstehen, was einen hohen Wert schafft. Je nach Organisation ist es jedoch möglicherweise nicht möglich, so viele Datenprofis in jeder Geschäftseinheit zu unterstützen. Darüber hinaus könnten, wenn es nur wenige Datenexperten gibt, diese unter Umständen das Fachwissen anderer Experten im Unternehmen nicht nutzen können. In diesem Fall könnte sich etwas mehr Zentralisierung als vorteilhafter erweisen.\n\n## Schlussfolgerung\n\nOrganisationen stehen vor vielen neuen Veränderungen, um datenzentriert zu werden, nicht nur in Bezug auf die Kultur, sondern auch in Bezug auf die organisatorische Struktur. Es reicht nicht aus, nur die Vorteile der neuen KI zu wollen; es erfordert grundlegende Veränderungen in der Art und Weise, wie wir über die Organisation selbst denken.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT15-de","image":"./episodes/edt-15/de/thumbnail.png","lang":"de","summary":"Die Schaffung erfolgreicher datengetriebener Ergebnisse beginnt mit einer starken organisatorischen Grundlage. Darren und sein Gast Sarah Kalicin, Lead Data Scientist Data Center Group Intel, diskutieren die Schlüsselaspekte dieser grundlegenden Veränderung."},{"id":50,"type":"Episode","title":"Nachhaltigkeit mit intelligenten Gebäuden umarmen","tags":["edge","smartbuilding","ai","sustainability"],"body":"\r\n\r\nDarren interviewt Sonu Panda, den CEO von Prescriptive Data, in dieser Episode. Sie diskutieren, wie ihre Software kommerziellen Immobilienbesitzern dabei hilft, ihre Gebäude in intelligente und effiziente Räume zu verwandeln.\n\n## Die treibenden Kräfte für intelligente Gebäude\n\nDie COVID-19-Pandemie hat die Qualität der Innenraumluft ins Rampenlicht gerückt und neue Vorschriften in Bezug auf Belüftung und Filtration eingeführt. Intelligente Gebäude, die von künstlicher Intelligenz und maschinellem Lernen betrieben werden, können dazu beitragen, die Einhaltung dieser Regeln sicherzustellen und sichere Umgebungen für die Bewohner zu gewährleisten. Darüber hinaus liegt der Fokus verstärkt auf Energieoptimierung und Dekarbonisierung, um die Kohlenstoffemissionen von Gebäuden zu reduzieren. Diese beiden Faktoren haben das Interesse an und die Investitionen in intelligente Gebäude beschleunigt.\n\n## Wie Vorschriftsdaten Intelligenz in Gebäude bringen\n\nDie Prescriptive Data wurde von Immobilienbetreibern in New York City gegründet, daher ist ihr Fachwissen in das Produkt integriert. Die Software integriert sich über APIs und Treiber in die bestehenden Betriebssysteme eines Gebäudes, sodass kein Rip-and-Replace erforderlich ist. Sie kombiniert Daten von Gebäudesystemen, IoT-Sensoren, Wetterdaten und mehr. Die KI-Engine analysiert all diese Daten, um negative Muster wie abnormale Energieverbrauchsspitzen und positive Muster, die wiederholt und verstärkt werden können, zu identifizieren. Sie zeigt Optimierungsmöglichkeiten auf und kann sogar autonom die Gebäudeumgebung kontinuierlich optimieren.\n\n## Cybersicherheit und einfache Bereitstellung\n\nPrescriptive Data hat Partnerschaften mit Banken, Regierungsbehörden und führenden Unternehmen der Immobilienbranche geschlossen, um von Anfang an Cybersicherheit in ihre Plattform zu integrieren. Kunden können die Software in der Regel innerhalb von 8-10 Wochen implementieren und Empfehlungen erhalten. Der Return on Investment erfolgt ebenfalls schnell, mit Rückzahlungszeiträumen von weniger als einem Jahr durch Energieeinsparungen und Einhaltung von Vorschriften.\n\n## Zugriff auf die Technologie\n\nDie GSA führt vorgeschriebene Daten in ihrem Beschaffungsplan auf, was den Regierungseinheiten einen schnellen Einkauf ermöglicht. Die Software steht allen kommerziellen Immobilienbesitzern zur Verfügung, die bestehende Gebäude in intelligente und nachhaltige Räume umwandeln möchten.\n\nDas Versprechen intelligenter Gebäude geht über Kosteneinsparungen hinaus. Durch kontinuierliche Optimierung der Innenräume können wir Energieverschwendung reduzieren und sicherere und komfortablere Räume für alle schaffen. Prescriptive Data bietet eine KI-gesteuerte Lösung, um bestehende Immobilien nachhaltig in die Zukunft zu führen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Sonu Panda"],"link":"/episode-EDT150-de","image":"./episodes/edt-150/de/thumbnail.png","lang":"de","summary":"Darren führt ein Interview mit Sonu Panda, dem CEO von Prescriptive Data, in dieser Folge. Sie diskutieren, wie ihre Software kommerziellen Immobilienbesitzern dabei hilft, ihre Gebäude in intelligente und effiziente Räume zu verwandeln."},{"id":51,"type":"Episode","title":"Verständnis für generative KI","tags":["genai","ai","datamanagement","people","collectiongenerativeai"],"body":"\r\n\r\n## Was ist Generative KI?\n\nKünstliche Intelligenz Systeme, die die Fähigkeit besitzen, neuen Inhalt zu generieren, werden als generative KI bezeichnet. Diese Systeme können verschiedene Arten von Ausgaben produzieren, wie Texte, Bilder, Audio und Video. Das unterscheidet sie von den meisten derzeit verwendeten KI-Systemen, die hauptsächlich analytisch sind und sich auf Aufgaben wie Klassifizierung, Vorhersagen und Empfehlungen konzentrieren. Generative KI bietet einen kreativeren und offeneren Ansatz für künstliche Intelligenz Anwendungen.\n\n## Revolutionäres Potenzial\n\nSowohl der Gastgeber als auch der Gast waren sich einig, dass generative Künstliche Intelligenz ein technologischer Durchbruch ist, der das Potenzial hat, das Spiel zu verändern. Es besitzt die einzigartige Fähigkeit, menschliche Kreativität zu verstärken und außergewöhnliche Inhalte aus selbst den einfachsten Anweisungen zu generieren. Sein Potenzial, verschiedene Branchen wie Schreiben, Design und Musik zu revolutionieren, ist unbestreitbar. Die gesellschaftliche Auswirkung dieser Technologie ist jedoch noch nicht vollständig erfassbar.\n\n## Sorgen in der Akademie\n\nIm Kontext der Hochschulbildung besteht zunehmende Besorgnis über die Verbreitung von Plagiaten und die Ausbeutung von generativer Künstlicher Intelligenz durch Studenten, die betrügen möchten. Dieses Problem hat Diskussionen über die ethischen Überlegungen beim Einsatz von KI in akademischen Umgebungen angestoßen. Es wurde jedoch von Experten, wie zum Beispiel Lancaster, vorgeschlagen, dass die akademische Welt eine entscheidende Rolle bei der Beratung zu diesen ethischen Überlegungen spielen kann. Auf diese Weise können Lehrkräfte den Studierenden die notwendigen Fähigkeiten vermitteln, um KI-generierte Inhalte verantwortungsbewusst zu bewerten und kritisch zu analysieren, was zweifellos ein wiederkehrendes Thema in ihrer zukünftigen Berufslaufbahn sein wird. Durch eine proaktive Herangehensweise an diese Bedenken kann die akademische Gemeinschaft sicherstellen, dass die Integration von KI in die Bildung nicht nur effektiv, sondern auch ethisch und verantwortungsbewusst erfolgt.\n\n## Vorteile für Effizienz\n\nGenerative Künstliche Intelligenz hat das Potenzial, zu revolutionieren, wie wir zeitaufwändige Aufgaben wie das Verfassen von Berichten, E-Mails, Artikeln und Code angehen. Mit Unterstützung von KI könnte der Prozess erheblich beschleunigt werden und wertvolle Zeit und Ressourcen sparen. Es ist jedoch wichtig zu beachten, dass menschliche Aufsicht nach wie vor entscheidend ist. Auch mit den Fortschritten in der KI-Technologie kann man sich nicht vollständig darauf verlassen, dass fehlerfreie Arbeit produziert wird. Daher bleibt eine sorgfältige Überprüfung und Bearbeitung durch Menschen ein wesentlicher Schritt, um die Genauigkeit und Qualität des Endprodukts sicherzustellen.\n\n## Anpassung und Implementierung\n\nUm eine erfolgreiche generative KI-Lösung umzusetzen, müssen Organisationen ihre einzigartigen Datenbedürfnisse und Sicherheitsanforderungen sorgfältig bedenken. Obwohl verfügbare Optionen wie ChatGPT hilfreich sein können, erfordert eine wirklich maßgeschneiderte Lösung bedeutende Ressourcen und Fachkenntnisse. Dies kann das Sammeln und Analysieren großer Datenmengen sowie die Investition in leistungsstarke Rechenressourcen beinhalten. Bevor generative KI vollständig implementiert wird, ist es wichtig, einen umfassenden Rahmen zu schaffen, der alle Aspekte des Organisationsbetriebs, einschließlich Datenschutz und Sicherheitsprotokolle, berücksichtigt. Mit dem richtigen Ansatz und den richtigen Ressourcen kann generative KI ein leistungsstarkes Werkzeug für Organisationen sein, die ihre datengesteuerten Entscheidungsfähigkeiten verbessern möchten.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT151-de","image":"./episodes/edt-151/de/thumbnail.png","lang":"de","summary":"In dieser Folge führte Gastgeber Darren Pulsipher ein Interview mit Dr. Jeffrey Lancaster von Dell Technologies. Ihre Diskussion drehte sich um generative KI und deren potenzielle Auswirkungen."},{"id":52,"type":"Episode","title":"Praktische Generative KI","tags":["genai","ai","collectiongenerativeai","datamanagement"],"body":"\r\n\r\nIm Bereich modernster Technologie gibt es nur wenige Innovationen, die so vielversprechend sind wie Generative KI. Dieses revolutionäre Konzept, mit seinen potenziellen Anwendungen in verschiedenen Branchen, wird unsere Interaktion mit Maschinen neu definieren und unsere Herangehensweise an Kreativität und Problemlösung neu gestalten.\n\n## Die Kraft der generativen KI: Transformation von Branchen\n\nDie Entwicklung von Generative AI hat die Kraft, Branchen durch Automatisierung von Prozessen und Verbesserung der Inhalteerstellung zu revolutionieren. Diese Technologie ermöglicht es KI-Systemen, Ergebnisse in verschiedenen Formen zu generieren, einschließlich schriftlicher Inhalte und künstlerischer Kreationen. Sie kann Aufgaben wie das Ausfüllen von Formularen und die Generierung von Inhalten vereinfachen, Prozesse innerhalb von Organisationen beschleunigen und optimieren. Mit dieser Innovation können alltägliche Aufgaben effizienter gestaltet werden, wodurch menschliche Ressourcen für wertvollere Tätigkeiten freigesetzt werden. Stellen Sie sich eine Welt vor, in der Routineaufgaben vereinfacht werden und menschliche Anstrengungen auf bedeutungsvollere Aufgaben ausgerichtet sind.\n\n## Mensch-KI Zusammenarbeit: Gemeinsam die Zukunft gestalten\n\nIn der Welt der generativen KI gibt es eine leistungsstarke Zusammenarbeit zwischen menschlichem Wissen und technologischen Fähigkeiten. Diese Partnerschaft zeigt sich in der Entwicklung genetischer Sequenzen und erstreckt sich auf verschiedene andere Anwendungen. Die Idee der erweiterten Intelligenz steht im Mittelpunkt, da Menschen KI nutzen, um Informationen schnell zu sammeln und zu analysieren. Obwohl menschliche Expertise weiterhin wichtig ist, ist die Fähigkeit von KI, große Datenmengen in kurzer Zeit zu verarbeiten, ein wertvolles Gut. Es handelt sich um eine beidseitig vorteilhafte Allianz, bei der sich beide Seiten gegenseitig ergänzen und dadurch die Fähigkeit zur Problemlösung verbessern.\n\n## Generative Werkzeuge: Neue Dimensionen der Kreativität entfesseln.\n\nDie Fähigkeiten von generativer Künstlicher Intelligenz gehen über eine einzelne Domäne hinaus und umfassen verschiedene Formen von Medien wie Bilder, Code und Ton. Diese Entwicklung eröffnet neue Möglichkeiten für Kreativität und Innovation in verschiedenen Branchen. Eine interessante Funktion dieser Technologie ist die Möglichkeit, das Maß an Kreativität anzupassen, auch als \"Halluzination\" bezeichnet. Dadurch kann die Ausgabe spezifischen Anforderungen entsprechen, während den Benutzern gleichzeitig die Freiheit gegeben wird, die kreative Ausgabe fein abzustimmen. Im Wesentlichen ermöglicht dieses Werkzeug den Benutzern, Technologie zu nutzen und gleichzeitig die Kontrolle über das Endergebnis zu behalten.\n\n## Stärkung durch Technologie: Ein Blick in die Zukunft\n\nTechnologie hat einen erheblichen Einfluss auf unser tägliches Leben, und Website-Baukästen wie Squarespace sind nur ein Beispiel dafür. Viele Menschen fragen sich, ob diese Werkzeuge Fachleute ersetzen oder Einzelpersonen befähigen werden. Die meisten Menschen glauben, dass diese Werkzeuge Einzelpersonen befähigen werden und ihnen helfen, die Kontrolle über ihre Projekte zu übernehmen. Dieser Ansatz ermutigt die Nutzer, unabhängig zu sein und gleichzeitig kritisch mit ihrer Arbeit umzugehen, was eine wesentliche Eigenschaft einer effektiven Nutzung von Technologie ist.\n\nKI-Werkzeuge sind auch Beschleuniger, die bei Aufgaben wie Codierung und Schreiben helfen. Sie unterstützen bei Grammatik, Struktur und Ideenfindung. Sie können jedoch menschliche kognitive Fähigkeiten, emotionale Ausdrucksweise und einzigartige Perspektiven nicht ersetzen. Die Kombination von menschlichem Fachwissen mit KI-Unterstützung führt zu einem ganzheitlichen Ansatz bei der Integration von Technologie.\n\nGenerative KI ist mehr als nur ein technologisches Wunder; sie repräsentiert einen Paradigmenwechsel, der die symbiotische Beziehung zwischen menschlicher Intelligenz und maschineller Effizienz hervorhebt. Diese Synergie hat das Potenzial, Industrien zu revolutionieren, Prozesse zu optimieren und neue Dimensionen der Kreativität freizusetzen. Indem wir diese Fortschritte annehmen und ihre Fähigkeiten nutzen, können wir uns auf eine Reise begeben, bei der Technologie das menschliche Potenzial stärkt und uns ermöglicht, größere Leistungen zu erzielen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT152-de","image":"./episodes/edt-152/de/thumbnail.png","lang":"de","summary":"In dieser Folge des Podcasts Die digitale Transformation umarmen führt Gastgeber Darren Pulsipher ein nachdenkliches Gespräch mit Dr. Jeffrey Lancaster. Ihre Diskussion geht auf die praktischen Anwendungen von generativer Künstlicher Intelligenz ein und die tiefgreifende Auswirkung, die sie in verschiedenen Branchen mit sich bringen wird."},{"id":53,"type":"Episode","title":"Die Ausbildung der nächsten Generation in Künstlicher Intelligenz","tags":["genai","generativeai","ai","robotics","people","training","collectiongenerativeai"],"body":"\r\n\r\n## Die Nutzung der Kraft der linearen Algebra und Analysis in der Künstlichen Intelligenz\n\nLineare Algebra und Analysis bilden das Rückgrat von Künstlicher Intelligenz (KI) Algorithmen und Systemen. In einer kürzlich erschienenen Podcast-Folge betont Pete Schmitz, ein pensionierter Intel-Mitarbeiter und KI-Enthusiast, die Bedeutung des Verständnisses dieser grundlegenden mathematischen Konzepte im Kontext von KI.\n\nLineare Algebra ist entscheidend in der KI, insbesondere bei Aufgaben wie Bilderkennung. Durch Matrixmultiplikation können faltende neuronale Netzwerke (CNNs) riesige Mengen an Bilddaten verarbeiten und analysieren. Dadurch wird die Identifizierung und Klassifizierung von Objekten in Bildern ermöglicht. Differentialrechnung hingegen wird bei der Schulung von KI-Modellen durch Techniken wie dem Gradientenabstieg eingesetzt, bei dem der Algorithmus kontinuierlich seine Parameter anhand der Änderungsrate einer gegebenen Funktion anpasst.\n\nSchmitz betont den Wert, dass Schülerinnen und Schüler diese Fächer in der Schule lernen, da sie ihnen eine solide Grundlage bieten, um in die Welt der Künstlichen Intelligenz einzutauchen. Das Verständnis für die Grundlagen ermöglicht es den Schülern, auf dem Wissen und den Fortschritten der vorherigen Generationen auf dem Gebiet der KI aufzubauen. Mit dem exponentiellen Wachstum der Technologie entwickelt sich KI schnell weiter und ermöglicht effizientere und automatisierte Lösungen für zuvor mühsame Aufgaben.\n\n## Der transformative Einfluss Künstlicher Intelligenz auf verschiedene Branchen\n\nDer Podcast geht auch auf die transformative Wirkung von KI in verschiedenen Branchen ein. KI-gesteuerte Systeme ermöglichen Fortschritte im Gesundheitswesen, im Einzelhandel und in mehreren anderen Bereichen. Zum Beispiel wird KI im Gesundheitswesen eingesetzt, um Krankheiten wie Krebs zu erkennen und zu diagnostizieren, was die Genauigkeit und Effizienz der Gesundheitsfachkräfte verbessert. Im Einzelhandelssektor wird KI verwendet, um das Einkaufsverhalten der Kunden zu analysieren und personalisierte Empfehlungen zu geben, um das gesamte Einkaufserlebnis zu verbessern.\n\nDarüber hinaus diskutieren die Gastgeber die aktuellen Fortschritte bei generativen KI-Modellen, wie beispielsweise Transformers. Diese Modelle können zugrunde liegende Muster in großen Datensätzen erkennen und damit Datenanalyse und Entscheidungsfindung erleichtern. Durch den Einsatz von Transformern und generativen Modellen können Branchen wertvolle Erkenntnisse gewinnen und Innovation vorantreiben.\n\n## Förderung von Innovation und Anpassung an neue Technologien\n\nInnovation ist ein zentrales Thema in der gesamten Podcast-Episode. Die Moderatoren betonen die Bedeutung, dass Organisationen neue Technologien und Prozesse annehmen müssen, um in der heutigen schnelllebigen Welt relevant zu bleiben. Es ist wesentlich, ein umfassendes Ökosystem zu fördern, das Innovation in verschiedenen Branchen unterstützt und spezialisierte Werkzeuge und Dienstleistungen für verschiedene Aspekte der Innovation bereitstellt.\n\nDer Podcast ermutigt auch aufstrebende Talente in den Bereichen Ingenieurwesen, Business und Marketing, über traditionelle Normen hinauszudenken und neue Perspektiven zu verfolgen. Indem man sich von veralteten Prozessen und Denkweisen befreit, können Organisationen das Potenzial ihrer Mitarbeiterinnen und Mitarbeiter nutzen und Innovation vorantreiben.\n\nDer Gastredner Pete Schmitz betont die Notwendigkeit des kontinuierlichen Lernens und der Anpassung angesichts technologischer Fortschritte und digitaler Transformationen. Organisationen müssen sich weiterentwickeln und Veränderungen annehmen, um nicht in der wettbewerbsintensiven Landschaft obsolet zu werden.\n\nZusammenfassend beleuchtet diese Podcast-Episode die Bedeutung der linearen Algebra und der Differentialrechnung in der KI, den transformierenden Einfluss der KI in verschiedenen Branchen sowie die Wichtigkeit der Förderung von Innovation und der Anpassung an neue Technologien. Durch ein umfassendes Verständnis der Grundlagen der KI, den Einsatz transformierender Technologien und die Förderung von Innovation können Organisationen die vielfältigen Chancen der digitalen Transformation nutzen und sich in der sich entwickelnden Welt der KI an die Spitze setzen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Pete Schmitz","Darren W Pulsipher"],"link":"/episode-EDT153-de","image":"./episodes/edt-153/de/thumbnail.png","lang":"de","summary":"In dieser Podcast-Episode spricht Pete Schmitz, ein pensionierter Account Executive von Intel, über seine Arbeit mit Schülern, bei der er ihnen künstliche Intelligenz (KI) beibringt und wie sie diese in ihren Robotikwettbewerben einsetzen können. Er erklärt, dass diese Wettbewerbe die Nutzung von Autonomie erfordern und KI eine entscheidende Komponente dafür ist. Pete teilt ein Beispiel dafür, wie KI-gestützte Bilderkennung im unbemannten Oberflächenfahrzeug D Hunter der Defense Advanced Research Projects Agency (DARPA) eingesetzt wird."},{"id":54,"type":"Episode","title":"GenAI Anwendungsfälle","tags":["genai","ai","collectiongenerativeai","datamanangement"],"body":"\r\n\r\nGenerative KI ist eine transformative Technologie, die die menschliche Kreativität erweitern, die Zusammenarbeit verbessern und neue Möglichkeiten für Arbeit und Kommunikation erschließen kann. Indem man die Fähigkeiten der KI nutzt, können Individuen Inhalte generieren, E-Mails zusammenfassen und routinemäßige Aufgaben automatisieren, während sie den menschlichen Einfluss und die Individualität beibehalten.\n\n## Die Entfesselung der menschlichen Kreativität\n\n## Verständnis der Datenumgebung und klare Ziele setzen.\n\nDr. Lancaster betont die Bedeutung, den Typ der Daten zu verstehen, die Sie entweder verwenden oder erstellen möchten, bevor Sie in die generative KI eintauchen. Egal ob es sich um Texte, Bilder, Musik, Videos oder Audios handelt, ein klares Verständnis Ihrer Eingabe und des gewünschten Ergebnisses ermöglicht es Ihnen, die am besten geeigneten Werkzeuge und Plattformen auszuwählen.\n\n## Die Steigerung der menschlichen Kreativität mit KI\n\nEiner der wichtigsten Erkenntnisse aus dem Podcast ist die Rolle Künstlicher Intelligenz in der Generierung von menschlicher Kreativität statt deren Ersetzung. KI-Tools wirken als Katalysatoren und steigern und fördern menschliche Kreativität auf neue Höhen. Durch die Kombination des innovativen Denkens von Menschen mit den Fähigkeiten von KI können Individuen komplexe Probleme lösen und bahnbrechende Ideen generieren, die allein mit herkömmlichen Ansätzen nicht erreicht werden können.\n\n## Zusammenarbeit und Brainstorming mit KI\n\nGenerative KI eröffnet Möglichkeiten zur Zusammenarbeit und zum Brainstorming. KI kann als zusätzliche Stimme in Gruppendiskussionen dienen, indem sie neue Perspektiven anregt und zu fruchtbaren Gesprächen anspornt. Dieser kollaborative Aspekt ist besonders wertvoll in Gruppeneinstellungen, in denen KI Gespräche mithören, Diskussionen erleichtern und Ideen zu einem Konsens vereinen kann.\n\n## Die Entfesselung der Kraft der generativen künstlichen Intelligenz\n\nGenerative KI hat ein immenses Potenzial, Kreativität freizusetzen, menschliche Fähigkeiten zu ergänzen und frische Perspektiven und Lösungen für Herausforderungen anzubieten. Egal ob Sie ein Entwickler, Forscher oder einfach nur neugierig auf KI sind, es gibt viele Möglichkeiten, mit generativer KI zu erkunden und zu erschaffen.\n\n## Praktische Anwendungen der generativen KI am Arbeitsplatz\n\nNeben den in dem Podcast geteilten Erkenntnissen gibt es zahlreiche praktische Anwendungen der generativen KI, die unsere Arbeitsprozesse revolutionieren können. Lassen Sie uns einige davon erkunden.\n\n## Zusammenfassung langer E-Mails und Optimierung der Kommunikation\n\nBerufstätige Menschen erhalten oft lange E-Mails, die wertvolle Zeit beanspruchen. Generative KI kann helfen, indem sie den E-Mail-Inhalt analysiert und eine prägnante Zusammenfassung erstellt, die die Hauptpunkte und wichtigsten Erkenntnisse erfasst. Dies ermöglicht es Empfängern, wichtige Informationen schnell zu erfassen und informierte Entscheidungen zu treffen, ohne überschüssige Zeit mit dem Lesen der gesamten E-Mail zu verbringen.\n\n## Automatisierung der Inhalterstellung\n\nGenerative KI kann die Automatisierung der Erstellung von Berichten, Artikeln und anderen schriftlichen Inhalten ermöglichen. Indem relevante Daten oder Informationen in ein generatives KI-Tool eingegeben werden, können Journalisten und Content-Ersteller vollständige Artikel oder Berichte basierend auf diesen Eingaben generieren. Dies spart erhebliche Zeit und Ressourcen, insbesondere für diejenigen, die regelmäßig große Mengen an Inhalten produzieren müssen.\n\n## Förderung künstlerischer Kreativität\n\nKreative in Kunst und Musik können generative KI nutzen, um neue Stile, Techniken und Inspirationen zu erkunden. KI kann Künstlern helfen, Ideen zu generieren, Musik zu komponieren und visuelle Inhalte zu erstellen. Mit der Kraft der generativen KI können Künstler ihre kreativen Horizonte erweitern und Grenzen in ihren jeweiligen Bereichen überwinden.\n\n## Das Gleichgewicht zwischen Automatisierung und menschlicher Interaktion\n\nObwohl generative KI ein unglaubliches Potenzial bietet, ist es entscheidend, menschliche Aufsicht und Intervention zu wahren, um Genauigkeit, Kontext und Individualität sicherzustellen. Blind auf KI-generierten Inhalten zu vertrauen, ohne menschliche Intervention, kann zu einer Homogenisierung in der digitalen Landschaft führen. Es ist entscheidend, ein Gleichgewicht zwischen Automatisierung und menschlicher Interaktion zu finden, wo KI die menschliche Kreativität verbessert, anstatt sie zu ersetzen.\n\nWährend sich die generative KI weiterentwickelt, können wir erwarten, dass sie in verschiedene Aspekte der Arbeit und Kommunikation integriert wird. Vom Zusammenfassen von E-Mails über die Automatisierung der Content-Erstellung bis hin zur Ermöglichung neuer Formen künstlerischen Ausdrucks hat die generative KI das Potenzial, Prozesse zu optimieren, die Produktivität zu steigern und neue Möglichkeiten für Innovationen zu eröffnen. Die Akzeptanz dieser Technologie und gleichzeitige Wahrung menschlicher Kreativität und Einzigartigkeit wird die Zukunft der Arbeit auf bemerkenswerte Weise prägen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT154-de","image":"./episodes/edt-154/de/thumbnail.png","lang":"de","summary":"In der neuesten Episode tauchen Dr. Jeffrey Lancaster und Darren Pulsipher in die praktischen Anwendungsfälle der generativen Künstlichen Intelligenz ein und wie sie menschliche Kreativität in verschiedenen Bereichen entfesseln kann."},{"id":55,"type":"Episode","title":"GenAI-Advisor für das Management von Rechenzentren","tags":["ai","sdi","vergeio","cloud","privatecloud","collectiongenerativeai","hybridcloud"],"body":"\r\n\r\nIn der Podcast-Episode \"Die digitale Transformation annehmen\" führt der Chief Solution Architect Darren Pulsipher ein Interview mit Greg Campbell, dem CTO von Verge.io. Das Gespräch dreht sich um innovative Lösungen im Bereich des Infrastrukturmanagements und das Potenzial der erweiterten Intelligenz. Greg teilt seine Erfahrungen als Softwareentwickler und Unternehmer und diskutiert die Herausforderungen, die er mit Verge.io angehen wollte, einem Unternehmen, das sich auf die Vereinfachung des Infrastrukturmanagements in verteilten Servern konzentriert.\n\n## Vereinfachung des komplexen Infrastrukturmanagements\n\nDie Verwaltung der Infrastruktur in der heutigen digitalen Landschaft bringt erhebliche Herausforderungen mit sich. Die Komplexität entsteht durch verschiedene Komponenten, Anbieter, Lizenzen und Versionsunterschiede. Dies erfordert qualifiziertes Personal und führt häufig zu hohen Kosten und einem Mangel an Fachwissen. Obwohl die Cloud ursprünglich als Lösung angesehen wurde, brachte sie ihre eigene Komplexität mit sich.\n\nVerge.io bietet eine Lösung über sein Betriebssystem, VergeOS, an. Dieses System ermöglicht es Entwicklern, Speicher-, Rechen- und Netzwerkressourcen über verschiedene Hardwarekonfigurationen hinweg einfach zu verwalten und zu verbinden. Durch Bereitstellung eines virtuellen Rechenzentrums vereinfacht VergeOS das Infrastrukturmanagement und macht es intuitiver und benutzerfreundlicher.\n\n## Das Potenzial der generativen KI im Infrastrukturmanagement.\n\nGreg spricht auch über sein Interesse an künstlicher Intelligenz (KI) und deren potenziellen Anwendungen. Er teilt seine Erfahrungen mit generativer KI und deren Nutzung im Infrastrukturmanagement. Greg erforscht, wie die Automatisierung von Infrastruktur- und Rechenzentrum-Management durch generative KI komplexe Prozesse vereinfachen und das Ressourcenmanagement optimieren kann.\n\nGenerative KI kann das Infrastrukturmanagement automatisieren und die Notwendigkeit spezialisierter Experten beseitigen, wodurch die Effizienz verbessert wird. Sie hat das Potenzial, das Design von Benutzeroberflächen und adaptiven Schnittstellen zu revolutionieren und den Infrastrukturmanagementprozess intuitiver und benutzerfreundlicher zu gestalten.\n\n## Erweiterte Intelligenz als wertvoller Assistent\n\nAugmented Intelligence ist die Kombination menschlicher und maschineller Intelligenz. Augmented Intelligence verbessert menschliche Fähigkeiten und Entscheidungsfindungen, indem sie Erkenntnisse und Antworten auf komplexe Probleme liefert. Es ist dazu gedacht, menschliches Urteilsvermögen bei der Ermittlung informierter Entscheidungen zu unterstützen, anstatt es zu ersetzen.\n\nGreg betont, dass mit dem Wachstum und der Verbesserung von KI-Modellen deren Genauigkeit und Vorhersagefähigkeit steigen. Erweiterte Intelligenz kann in verschiedenen Branchen angewendet werden, zum Beispiel im Kundensupport, wo KI-Modelle Antworten auf Kundenanfragen liefern und menschlichen Agenten bei der Lösungssuche helfen können. Sie kann auch dabei unterstützen, entfernte Standorte oder Büros zu verwalten, indem sie vor Ort tätigem Personal Anleitungen gibt, das in bestimmten Bereichen möglicherweise nicht über Fachkenntnisse verfügt.\n\n## Die Zukunft der digitalen Transformation\n\nDer Podcast endet mit einer Diskussion über die Zukunft der erweiterten Intelligenz und deren potenziellen Auswirkungen auf Branchen und die Arbeitskräfte. Greg's Optimismus liegt in der Fähigkeit der erweiterten Intelligenz, Effizienz und Produktivität zu verbessern, doch er erkennt an, dass sie menschliches Urteilsvermögen nicht vollständig ersetzen sollte. Das Gespräch betont die Bedeutung einer sorgfältigen Implementierung, fortlaufender menschlicher Aufsicht und ethischer Überlegungen bei der Nutzung der erweiterten Intelligenz.\n\nInsgesamt bietet diese Podcast-Episode wertvolle Einblicke in innovative Infrastrukturmanagementlösungen, das Potenzial von generativer KI zur Optimierung von Prozessen und die Vorteile der erweiterten Intelligenz als wertvoller Assistent. Sie zeigt die Stärke der digitalen Transformation und die Nutzung von Technologie zur Steigerung von Effizienz und Erfolg in Organisationen auf.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Greg Campbell","Darren W Pulsipher"],"link":"/episode-EDT155-de","image":"./episodes/edt-155/de/thumbnail.png","lang":"de","summary":"In dieser Folge setzt sich Gastgeber Darren Pulsipher mit Greg Campbell, dem CTO von Verge.io, zusammen, um die aufregende Schnittstelle von KI und Infrastrukturmanagement zu besprechen. Greg, ein Softwareentwickler und Unternehmer, teilt seine Reise bei der Erstellung von Verge.io, um die Komplexität des Infrastruktur-Zusammennähens und -Managements anzugehen."},{"id":56,"type":"Episode","title":"Eine datenbereite Organisation werden","tags":["collectiongenerativeai","datamanagement","automation","dataquality","strategicanalytics","generativeai","digitaltransformation","datadriveninsights","datareadiness","innovation","decisionmaking","technologytrends","businessintelligence","datastrategy","analytics","bigdata","continuouslearning","operationalefficiency","dataoptimization","datainnovation","emrbacingdigital","edt156"],"body":"\r\n\r\n## Evolution der Datenverwaltung: Von manuell zu automatisiert.\n\nRon beginnt das Gespräch, indem er den manuellen und arbeitsintensiven Prozess des Datenmanagements in den Anfangstagen seiner Karriere hervorhebt. In Branchen wie nuklearen Waffensystemen und Raumfahrt erforderte das Datenmanagement aufgrund der hohen Zuverlässigkeit und Komplexität der Systeme akribische manuelle Arbeit. Allerdings haben Organisationen angesichts einer datengetriebenen und technologieabhängigen Welt erkannt, dass Daten in nutzbarere und effektivere Weise umgewandelt werden müssen.\n\n## Herausforderungen im Datenmanagement: Komplexität und Qualität\n\nRon teilt ein überzeugendes Beispiel aus seiner Erfahrung in der Marine, in dem er die Herausforderungen der Datenverwaltung für Schiffe während Wartungs- und Modernisierungszyklen diskutiert. Die Komplexität der Schiffs-systeme und das raue maritimen Umfeld machen gründliche Datenanalyse und Planung für erfolgreiche Wartung und Reparaturen unerlässlich. Dies unterstreicht die Bedeutung der Datenqualität und ihre Auswirkungen auf operationale Effizienz und Entscheidungsfindung.\n\n## Datenbereitschaft und Automatisierung\n\nUm die Vorteile der Automatisierung nutzen zu können, müssen Organisationen den Fokus auf Datenqualität legen. Bei automatisierten Analyse- und Bewertungsprozessen werden Fehler oder fehlende Daten zu einem kritischen Problem. Um dies zu beheben, müssen Organisationen die Datenerfassung von Anfang an verbessern. Durch das Entwerfen von Systemen, die die Datenerfassung erleichtern, und im Hinblick auf die Person, die die Daten erhebt, als Kunden, können Organisationen Fehler minimieren und die Datenqualität verbessern.\n\nEin ganzheitlicher Ansatz für die Datenbereitschaft ist ebenfalls von entscheidender Bedeutung. Dies beinhaltet das Erkennen der verschiedenen Phasen der Datenbereitschaft, von der Sammlung über das Management bis hin zur Verarbeitung. Durch kontinuierliche Verbesserungen in jedem Bereich können Organisationen sicherstellen, dass ihre Daten von hoher Qualität sind und verschiedene Operationen und Technologien wie generative KI unterstützen können.\n\n## Filtering the Noise: Strategische Datenanalyse\n\nDatenanalyse spielt eine entscheidende Rolle bei der Schaffung strategischen Mehrwerts für Organisationen. Ron und Darren diskutieren die Bedeutung der Filterung von Daten basierend auf ihrer Relevanz für Ziele und konzentrieren sich auf das, was wirklich wichtig ist. Nicht alle Daten sind wertvoll oder notwendig für die Analyse, und Organisationen sollten ihre Datensammlung mit ihren Zielen in Einklang bringen, um Ressourcenverschwendung zu vermeiden.\n\nDarüber hinaus betont das Gespräch, dass Daten nicht perfekt sein müssen, um nützlich zu sein. Während Genauigkeit und Präzision in einigen Fällen wichtig sind, können \"gut genug\" Daten dennoch wertvolle Erkenntnisse liefern. Indem Organisationen den Wert verschiedener Daten erkennen, können sie vermeiden, nach unerreichbarer Perfektion zu streben und sich darauf konzentrieren, die verfügbaren Erkenntnisse zu nutzen.\n\n## Das Aufdecken unerwarteten Werts: Möglichkeiten umarmen\n\nDer Podcast erforscht auch das Potenzial von generativer Künstlicher Intelligenz (KI) zur Verbesserung der Datensammlung. Durch die Verwendung interaktiver Formulare und konversationeller Schnittstellen können Organisationen mehr aussagekräftige Informationen sammeln und neue Erkenntnisse gewinnen. Dies eröffnet Möglichkeiten für eine verbesserte Datenanalyse und Entscheidungsfindung, insbesondere in Bereichen, in denen die Datensammlung entscheidend ist.\n\nDie Diskussion endet mit dem Hinweis, dass Datenanalyse ein kontinuierlicher Lernprozess ist. Organisationen sollten offen sein für die Erkundung neuer Technologien und Ansätze und dabei stets danach streben, unerwarteten Mehrwert in ihren Daten zu entdecken.\n\n## Schlussfolgerung\n\nIn einer zunehmend datengetriebenen Welt ist es entscheidend für den Erfolg, eine datenbereite Organisation zu werden. Durch das Verständnis der Entwicklung des Datenmanagements, die Fokussierung auf Datenqualität und -bereitschaft und die Nutzung der Möglichkeiten strategischer Datenanalyse können Organisationen die Kraft der Daten nutzen, um Innovationen voranzutreiben, Betriebsabläufe zu optimieren und informierte Entscheidungen zu treffen. Diese Podcast-Episode bietet wertvolle Einblicke und betont die Bedeutung von Datenmanagement und -analyse im digitalen Zeitalter.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Ron Fritzemeier","Darren W Pulsipher"],"link":"/episode-EDT156-de","image":"./episodes/edt-156/de/thumbnail.png","lang":"de","summary":"In der Podcast-Episode diskutiert der pensionierte Konteradmiral Ron Fritzmeier mit Gastgeber Darren Pulsipher die Bedeutung des Datenmanagements im Kontext der generativen künstlichen Intelligenz (KI). Mit einem Hintergrund in Elektrotechnik und umfangreicher Erfahrung in den Bereichen Cyber und Cybersicherheit liefert Ron wertvolle Einblicke in das sich entwickelnde Feld des Datenmanagements und seine entscheidende Rolle für den Erfolg von Organisationen im digitalen Zeitalter."},{"id":57,"type":"Episode","title":"Operationalisierung von GenAI","tags":["ai","generativeai","collectiongenerativeai","infrastructuremanagement","aisystems","aimodels","operationalization","datainput","modeltraining","finetuning","digitaltransformation","opensourcemodels","privateclouds","edgecomputing","aitools","creativeoutput","responsibleusage","reinforcementlearning","monitoring","optimization","sandboxenvironment","cloudbasedinfrastructure","onpremisesinfrastructure","hybridinfrastructure","customerservice","brainstormingapplications","embracingdigital"],"body":"\r\n\r\n## Erkunden verschiedener Sharing-Modelle für generative Künstliche Intelligenz\n\nDer Podcast stellt verschiedene Modelle für die gemeinsame Nutzung von generativer KI vor. Am einen Ende des Spektrums befinden sich offene Modelle, bei denen jeder mit dem Modell interagieren und zum Training beitragen kann. Diese Modelle verwenden Verstärkungslernen, wodurch Benutzer Daten eingeben und relevante Antworten erhalten können. Andererseits sind einige private Modelle stärker eingeschränkt und schwerer zugänglich. Diese Modelle eignen sich für Unternehmensszenarien, in denen Kontrolle und Einschränkung entscheidend sind.\n\nEs gibt jedoch einen kombinierten Ansatz, der die sprachliche Grundlage offener Modelle mit zusätzlichen Einschränkungen und Anpassungen verbindet. Mit diesem Ansatz können Organisationen von vortrainierten Modellen profitieren, während sie ihre eigenen Kontroll- und Anpassungsebenen hinzufügen. Durch die Anpassung der Gewichtungen und der verwendeten Wörter im Modell können Organisationen die Antworten nach ihren spezifischen Bedürfnissen anpassen, ohne bei Null anzufangen.\n\n## Die Implementierung von Gen AI in der Infrastrukturverwaltung.\n\nDer Podcast taucht in die Operationalisierung von generativer Künstlicher Intelligenz im Infrastrukturmanagement ein. Er hebt die Vorteile der Verwendung von Open-Source-Modellen hervor, um spezialisierte Systeme zu entwickeln, die private Clouds effizient verwalten. Zum Beispiel hat einer der genannten Partner generative KI implementiert, um die Leistung ihrer Infrastruktur in Echtzeit zu überwachen und zu optimieren, was proaktive Fehlerbehebung ermöglicht. Durch die Nutzung der Kraft der KI können Organisationen ihre operative Effizienz steigern und den reibungslosen Betrieb ihrer Infrastruktur sicherstellen.\n\nDie Gastgeber betonen die Bedeutung der Berücksichtigung von Art und Qualität der in das Modell eingegebenen Daten sowie des gewünschten Outputs. Es ist nicht immer notwendig, ein Modell mit Milliarden von Indikatoren zu trainieren; ein kleineres, den spezifischen Bedürfnissen angepasstes Datenset kann effektiver sein. Durch das Verständnis der Feinheiten der Daten und der speziellen Ziele des Systems können Organisationen den Trainingsprozess optimieren und die Gesamtleistung des KI-Modells verbessern.\n\n## Verwalten und Feinabstimmen von KI-Systemen\n\nDie Verwaltung von KI-Systemen erfordert durchdachte Entscheidungsfindung und laufende Überwachung. Die Gastgeber diskutieren die Bedeutung der Auswahl der geeigneten Infrastruktur, sei es cloud-basiert, lokal oder hybrid. Darüber hinaus gewinnt Edge Computing an Beliebtheit, da es AI-Modelle ermöglicht, direkt auf Geräten ausgeführt zu werden und Datenrückläufe zu reduzieren.\n\nDer Podcast betont die Notwendigkeit von Fachkenntnissen für den Aufbau und die Aufrechterhaltung von KI-Systemen. Geschultes Personal ist erforderlich, um KI-Modelle zu entwerfen und fein abzustimmen, um gewünschte Ergebnisse zu erzielen. Abhängig vom Anwendungsfall können spezifische Funktionalitäten erforderlich sein, wie z.B. Empathie im Kundenservice oder Kreativität bei Brainstorming-Anwendungen. Es ist entscheidend, ein kompetentes Team zu haben, das die Feinheiten von KI-Systemen versteht und deren optimale Funktionsweise sicherstellen kann.\n\nDarüber hinaus bedürfen KI-Modelle einer ständigen Überwachung und Anpassung. Modelle können unerwünschtes Verhalten aufweisen, und es ist wichtig, bei Bedarf einzugreifen, um angemessene Ergebnisse zu gewährleisten. Der Podcast unterscheidet zwischen Verstärkungsproblemen, bei denen das Feedback der Benutzer das Modell in potenziell schädliche Richtungen lenken kann, und Halluzinationen, die absichtlich für kreative Zwecke angewendet werden können.\n\n## Den Einstieg in AI-Modelle finden.\n\nDer Podcast bietet praktische Ratschläge für den Einstieg in KI-Modelle. Die Gastgeber schlagen vor, mit den verfügbaren Tools herumzuspielen und sich mit ihren Möglichkeiten vertraut zu machen. Das Anmelden für Konten und das Erkunden, wie die Tools verwendet werden können, ist eine großartige Möglichkeit, praktische Erfahrungen zu sammeln. Sie empfehlen außerdem, eine Sandbox-Umgebung innerhalb von Unternehmen zu erstellen, damit Mitarbeiter AI-Modelle testen und damit interagieren können, bevor sie sie in die Produktion implementieren.\n\nDer Podcast betont die Bedeutung, KI-Modellen ausreichend Kreativität zu ermöglichen, während gleichzeitig Kontrolle und Grenzen gesetzt werden. Organisationen können ein Gleichgewicht zwischen kreativer Leistung und verantwortungsbewusstem Einsatz erreichen, indem sie Leitplanken festlegen und Entscheidungen darüber treffen, was das Modell aus Interaktionen lernen sollte oder nicht.\n\nZusammenfassend bietet die Podcast-Episode wertvolle Einblicke in die Operationalisierung von generativer Künstlicher Intelligenz, das Management von Infrastrukturen und Überlegungen zur Verwaltung und Feinabstimmung von KI-Systemen. Sie bietet auch praktische Tipps für den Einstieg in KI-Modelle in persönlichen und beruflichen Umgebungen. Durch das Verständnis der verschiedenen Sharing-Modelle, Infrastrukturanforderungen und der Bedeutung von Kreativität und Grenzen können Organisationen die Kraft der KI nutzen, um die digitale Transformation zu unterstützen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT157-de","image":"./episodes/edt-157/de/thumbnail.png","lang":"de","summary":"In dieser Podcast-Episode diskutiert Gastgeber Darren Pulsipher, Chief Solution Architect des öffentlichen Sektors bei Intel, die Operationalisierung von generativer Künstlicher Intelligenz (AI) mit dem wiederkehrenden Gast Dr. Jeffrey Lancaster. Sie erkunden verschiedene Sharing-Modelle von generativer KI, einschließlich öffentlicher, privater und gemeinschaftlicher Modelle. Der Podcast behandelt Themen wie Open-Source-Modelle, Infrastrukturmanagement und Überlegungen zur Bereitstellung und Wartung von KI-Systemen. Es wird außerdem auf die Bedeutung von Kreativität, Personalisierung und dem Einstieg in AI-Modelle eingegangen."},{"id":58,"type":"Episode","title":"GenAI im Hochschulbereich","tags":["collectiongenerativeai","addressingbiasesingenerativeai","preservingauthenticityandindividuality","balancingaiintegrationineducation","lauratorresnewey","criticalthinkingskills","educationaltechnology","highereducation","aiineducation","aibias","diversityandinclusion","authenticvoices","uniqueperspectives","genai","generativeai","embracingdigital","edt158"],"body":"\r\n\r\nDer Aufstieg der Generativen KI in der Bildung\n\nIn einer kürzlich veröffentlichten Episode des Podcasts \"Embracing Digital Transformation\" führt Moderator Darren Pulsipher ein Interview mit Laura Torres Newey, einer Bestseller-Autorin der New York Times und Englischprofessorin, über die Auswirkungen von generativer Künstlicher Intelligenz in der Hochschulbildung. Die Diskussion dreht sich um die Integration von KI im Klassenzimmer, ihre Auswirkungen auf Lehrmethoden, Bedenken hinsichtlich Voreingenommenheit und die Bewahrung einzigartiger Stimmen. Laura teilt ihre Einsichten und Erfahrungen als Lehrerin und bietet wertvolle Perspektiven zur Navigation im sich entwickelnden Bildungsumfeld im Zeitalter der Künstlichen Intelligenz.\n\n## Der Einfluss von generativer Künstlicher Intelligenz in der Bildung\n\nGenerative KI hat begonnen, eine bemerkenswerte Präsenz in der Bildung zu werden, angefangen bei automatischer Essaybewertung bis hin zur Bereitstellung von Schreibhilfe für Schüler. Obwohl diese Technologie Bequemlichkeit und Effizienz bietet, wirft sie Bedenken hinsichtlich des potenziellen Verlusts einzigartiger Stimmen auf. Laura betont die Bedeutung, die individuellen Perspektiven und Kreativität der Schüler in ihrem Schreiben wertzuschätzen und zu fördern. Anstatt den Einsatz generativer KI vollständig zu verbieten, glaubt Laura daran, den Schülern beizubringen, wie sie diese Tools effektiv nutzen und ihr Potenzial nutzen können, ohne ihre eigenen Stimmen zu gefährden.\n\nDie Integration von generativer KI führt zu einer Verschiebung des Schwerpunkts des Unterrichts. Anstatt ausschließlich das Endprodukt zu bewerten, sollten Pädagogen mehr Wert auf den Lernprozess legen. Mit KI-gesteuerten Tools wie Grammarly können Lehrer ihre Aufmerksamkeit auf die Entwicklung von kritischem Denken, Forschungsfähigkeiten und die Unterscheidungsfähigkeit zur Identifizierung verlässlicher Quellen lenken. Durch die Einbeziehung von Aufgaben, die den Vergleich von KI-generierten Inhalten mit traditionell geschriebenem Werk beinhalten, können Schüler die Stärken und Schwächen beider Ansätze analysieren, um ein tieferes Verständnis für ihr Schreiben zu entwickeln und ihre kritischen Denkfähigkeiten zu schärfen.\n\n## Die Rolle der Pädagogen im Zeitalter der Künstlichen Intelligenz\n\nBildungspersonal spielt eine wesentliche Rolle bei der Vorbereitung von Schülern auf die sich ständig weiterentwickelnde technologische Landschaft. Laura betont, dass die Anpassungsfähigkeit an und der effektive Einsatz von generativer KI für Pädagogen auf allen Bildungsebenen entscheidend ist. Da KI immer präsenter am Arbeitsplatz wird, werden Schüler, die diese Technologie navigieren und nutzen können, besser für zukünftige Berufsmöglichkeiten ausgestattet sein. Um sicherzustellen, dass Schüler gut vorbereitet sind, müssen Pädagogen sich nicht nur mit KI-Anwendungen vertraut machen, sondern auch den Schülern beibringen, wie sie KI effektiv und ethisch nutzen können.\n\nDie Verlagerung hin zur Integration von generativer künstlicher Intelligenz in der Bildung entspricht der Position des US-Bildungsministeriums zu KI. Sie erkennen das Potenzial der Vorteile an, betonen jedoch die Notwendigkeit, dass die Nutzer die Kontrolle behalten, indem sie die Rolle der KI mit der eines elektrischen Fahrrads vergleichen - die Technologie verringert die Belastung, aber der Benutzer behält letztendlich die Kontrolle. Dieser Ansatz betont die Bedeutung eines Ausgleichs zwischen der Nutzung der Vorteile von KI und dem Erhalt der individuellen Stimmen und Perspektiven der Schüler.\n\n## Schlussfolgerung\n\nDie Integration von generativer KI in der Bildung bietet sowohl Chancen als auch Herausforderungen. Während KI das Lernen verbessern und Schülern bei ihren Aufgaben helfen kann, ist es für Pädagogen entscheidend, kritisches Denken zu priorisieren und Bedenken hinsichtlich Voreingenommenheit anzusprechen, um gut abgerundete und unabhängige Denker zu entwickeln. Lehrer sollten die KI-Technologie annehmen, ihre Anwendungen verstehen und den Schülern beibringen, sie effektiv zu nutzen und zu navigieren. Durch das Finden eines Gleichgewichts zwischen der Effizienz von KI-generierten Inhalten und dem Erhalt authentischer und vielfältiger Stimmen können Pädagogen Schüler auf die digitale Zukunft vorbereiten und gleichzeitig ihre Individualität und Kreativität fördern.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Laura Newey","Darren W Pulsipher"],"link":"/episode-EDT158-de","image":"./episodes/edt-158/de/thumbnail.jpg","lang":"de","summary":"In dieser Podcast-Episode interviewt Darren Pulsipher, Chief Solution Architect im öffentlichen Sektor bei Intel, Laura Torres Newey, eine Bestseller-Autorin der New York Times und Universitätsprofessorin, über die Auswirkungen von generativer KI in der Hochschulbildung. Diese Episode geht auf die Herausforderungen und Chancen ein, die sich durch die Integration von generativer KI im Unterricht ergeben, und hebt die Notwendigkeit von kritischem Denken, die Bedenken hinsichtlich Voreingenommenheit und die Sicherstellung der Bewahrung einzigartiger Stimmen hervor."},{"id":59,"type":"Episode","title":"GenAI-Richtlinien","tags":["collectiongenerativeai","policies","ai","generativeai","guidelines","jeremyharris","darrenpulsipher","roadmap","challenges","efficiencies","dataprotection","privacy","compliance","ethicalconsiderations","feedback","engagement","ratings","reviews","customersatisfaction","customerengagement","embracingdigital","edt159"],"body":"\r\n\r\n## Der Bedarf an klaren Richtlinien und Leitlinien.\n\nJeremy und Darren betonen die Bedeutung einer klaren Richtlinie und einer klar definierten Roadmap für die digitale Transformation. Hektisches Vorgehen bei der Digitalisierung ohne angemessene Planung kann zu Herausforderungen und Effizienzproblemen führen. Durch die Festlegung von Richtlinien und Leitlinien können Organisationen ihre Ziele skizzieren, eine strategische Ausrichtung festlegen und sicherstellen, dass alle an einem Strang ziehen.\n\nSie betonen, dass digitale Transformation mehr als nur die Einführung neuer Technologien ist - es erfordert eine Veränderung der organisatorischen Kultur und Denkweise. Richtlinien können diese Veränderung erleichtern, indem sie Erwartungen an Mitarbeiter festlegen, digitale Best Practices definieren und einen Rahmen für Entscheidungsfindung im digitalen Bereich bieten.\n\n## Die Bewältigung der Komplexität der Digitalisierung.\n\nDie digitale Transformation bringt eine komplexe Reihe von Herausforderungen mit sich, wie Datenschutz, Privatsphäre und Compliance. Organisationen müssen diese Herausforderungen angehen, indem sie sie in ihre Richtlinien und Leitlinien aufnehmen. Dies beinhaltet das Implementieren von Datenschutzmaßnahmen, das Durchführen regelmäßiger Sicherheitsaudits und die Sicherstellung der Einhaltung relevanter Vorschriften.\n\nDie Richtlinien sollten sich auch mit den ethischen Überlegungen befassen, die mit der digitalen Transformation einhergehen. Die Gastgeber betonen die Bedeutung, dass Organisationen verantwortungsbewusste Verwalter von Daten sind und sicherstellen, dass der Einsatz digitaler Technologien mit ethischen Standards übereinstimmt. Klare Leitlinien können den Mitarbeitern helfen, ihre Verantwortlichkeiten zu verstehen und verantwortungsvolle digitale Praktiken in der gesamten Organisation zu fördern.\n\n## Die Rolle von Feedback und Engagement.\n\nDie Gastgeber betonen die Bedeutung von Rückmeldungen und Engagement in der digitalen Welt. Die Annahme einer Richtlinie, die Feedback fördert und wertschätzt, kann Organisationen dabei helfen, sich kontinuierlich zu verbessern und sich an veränderte Gegebenheiten anzupassen. Indem sie Vorschläge und Rückmeldungen von Mitarbeitern und Kunden begrüßen, können Organisationen ihre digitalen Strategien optimieren und sicherstellen, dass sie den Bedürfnissen aller Beteiligten gerecht werden.\n\nSie erwähnen auch die Bedeutung von Bewertungen und Rezensionen im digitalen Zeitalter. Feedback durch Bewertungen und Rezensionen liefert nicht nur wertvolle Einblicke für Organisationen, sondern dient auch als Maßstab für Kundenzufriedenheit und Kundenbindung. Richtlinien können festlegen, wie Organisationen Feedback sammeln und darauf reagieren und Leitlinien für die Erfassung der Kundenzufriedenheit im digitalen Raum festlegen.\n\n## Schlussfolgerung\n\nDie digitale Transformation ist eine Reise, die sorgfältige Planung, klare Richtlinien und laufende Anpassungen erfordert. Durch die Festlegung von Richtlinien und Leitlinien können Organisationen die Komplexität der Digitalisierung bewältigen, Herausforderungen angehen und eine verantwortungsvolle und effektive Nutzung digitaler Technologien sicherstellen. Die Akzeptanz der digitalen Transformation geht nicht nur darum, neue Werkzeuge zu übernehmen, sondern auch darum, eine digitale Kultur zu schaffen, die Innovation fördert und den sich wandelnden Bedürfnissen von Kunden und Interessengruppen gerecht wird.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeremy Harris","Darren W Pulsipher"],"link":"/episode-EDT159-de","image":"./episodes/edt-159/de/thumbnail.png","lang":"de","summary":"In dieser Episode interviewt Gastgeber Darren Jeremy Harris und gehen auf die Bedeutung der Festlegung von Richtlinien und Leitlinien für eine erfolgreiche digitale Transformation ein. Angesichts der zunehmenden Verbreitung digitaler Technologien in verschiedenen Branchen müssen Organisationen sich anpassen und diese Transformation annehmen, um wettbewerbsfähig zu bleiben und sich den sich entwickelnden Kundenanforderungen anzupassen."},{"id":60,"type":"Episode","title":"Verminderung der Aufnahmeüberlastung mit Intel Optane DCPMM","tags":null,"body":"\r\n\r\n## Service-Stapel-Details\n\nEin Kunde in der Automobilindustrie hatte Schwierigkeiten, effektiv Informationen aus seinen Autos zu gewinnen und in sein Rechenzentrum zu übertragen, um maschinelles Lernen und Analysen durchführen zu können. Es gab bereits Forschung in diesem Bereich, aber nur für eine kleine Anzahl von Autos, nicht für die hundert Millionen Autos des Kunden. Als ich mir den gesamten Service-Stack ansah und wie alles ins Rechenzentrum gelangte, wurde deutlich, dass die Datenübernahme das Hauptproblem war: Wie kann ich so viele Daten aufnehmen und wie kann ich es schnell tun?\n\n## Hoher Überblick über die Kafka-Architektur auf hoher Ebene\n\nDer Kunde wollte Kafka für ihre Datenübernahme verwenden. Kafka ist ein Broker, der gut skalierbar ist, und der Schlüssel dazu ist, dass er mehrere Produzenten, verschiedene Verbraucher und viele Daten verarbeiten kann. Die Verwendung mehrerer Kafka-Broker, um Daten an den geeignetsten Stellen zu platzieren und abzuladen, bietet große Flexibilität.\n\nKafka hingegen wurde hauptsächlich für Nachrichtengrößen von etwa einem bis zehn Kilobyte konzipiert, während die Kundendaten pro Fahrzeug etwa 240 Kilobyte betrugen. Es gibt zwar Lösungen, aber ich wollte die gesamte 240 Kilobyte-Nachricht in den Kafka-Bus einbringen, um sie nach Bedarf verschieben zu können.\n\n## Leistungsspitzen-Bewährte Methoden\n\nIch habe mir die Leistungsempfehlungen anderer Personen angesehen, die mit Kafka arbeiten, um herauszufinden, ob ich es für meinen Kunden skalieren kann. Eine Möglichkeit zur Feinabstimmung besteht darin, die Puffergröße zu erhöhen, um die gesamte Nachricht aufnehmen zu können, zusammen mit der Verwaltung der Stapelgröße für optimale Leistung. Eine weitere erfolgreiche Praxis besteht darin, die Logs zu verteilen. Die Flexibilität von Kafka ermöglicht es mir, die Daten in verschiedene Themenbereiche zu platzieren. Ich kann die Themenbereiche in mehrere Partitionen aufteilen, die sich über mehrere Laufwerke erstrecken. Die Frage ist also, auf wie vielen Laufwerken ich die Kafka-Logs verteile. Darüber hinaus möchte ich die schnellstmöglichen Laufwerke haben.\n\nEin Beispiel, das ich untersucht habe, war LinkedIn. Ihre veröffentlichten Zahlen von vor einem Jahr besagen, dass sie 13 Millionen Nachrichten pro Sekunde verarbeiten können, oder 2,7 Gigabyte pro Sekunde. Sie geben an, dass sie etwa 1.100 Kafka-Broker und mehr als 60 in einem Cluster haben, das ist eine ziemlich große Konfiguration.\n\n## Automobilraum\n\nWenn ich mir die rohen Zahlen des Kunden anschaue (1,6 Millionen Nachrichten pro Sekunde und 800 Gigabyte pro Sekunde) und sie mit LinkedIn vergleiche, das wahrscheinlich nicht für 240 Kilobyte optimiert ist, komme ich auf 44.000 Broker. Wenn ich es optimiere, könnte ich wahrscheinlich auf 4.400 Broker kommen, was immer noch 240 Cluster sind. Das ist eine riesige Anzahl von Maschinen, also musste ich mir überlegen, wie ich die Dinge schneller machen kann. Mit noch mehr Optimierung könnte ich wahrscheinlich auf 400 bis 500 Broker kommen, aber ich wollte sehen, was noch möglich ist.\n\n## Intel Optane DC Persistent Memory\n\nIch habe nach unserem Optane Persistent Memory geschaut. Es passt in ein DDR4-Format und sitzt somit direkt am DDR4-Bus. Es gibt Module mit bis zu 512 Gigabyte, sodass ich in einem Zwei-Sockel-Server sechs Terabyte an persistentem Speicher haben kann. Ich wollte eine Möglichkeit finden, diese äußerst zuverlässige Technologie mit großartigen Funktionen wie der integrierten Hardwareverschlüsselung zu nutzen, um mir bei der Lösung dieses Problems zu helfen.\n\n## Unterstützung für eine Vielzahl von Anwendungen\n\nEs gibt zwei Betriebsmodi für diese Optane Memory: der direkte App-Modus und der Speichermodus. Der Speichermodus ist einfach. Es verwendet den persistenten Speicher als normalen RAM, da er günstiger ist als normaler DDR4. Es ist zwar nicht dasselbe wie DDR4, aber es ist nah genug dran, dass man in den meisten Anwendungen keinen Unterschied sieht. Im Direktmodus der App können Sie tatsächlich direkt aus Ihrem Programm in den persistenten Speicher schreiben. Auf diese Weise muss ich Datenstrukturen nicht mehr manuell umwandeln und streamen; ich kann sie einfach in den persistenten Speicher schieben. Ich kann den Direktmodus der App auch als Dateisystem mounten, so dass er auf dem Speicherbus liegt, was viel schneller ist als auf dem IO-Bus. Nun, was kann ich mit diesem Speicher tun?\n\n## Verwendung des Linux-Kernels\n\nEs gibt zwei Hauptwerkzeuge, die mit dem Linux-Kernel verwendet werden können: ndctl und ipmctl. Ndctl ist ein nichtflüchtiger Speichergeräte-Controller, und dann gibt es IPM, den Intel Persistent Memory Controller, mit dem ich diesen persistenten Speicher manipulieren und kontrollieren kann. Ich kann ihn im Memory-Modus oder im App-Direct-Modus einrichten. Ich musste ein bisschen über diese Werkzeuge und wie sie funktionieren lernen.\n\n## Ansatz der Aufnahme\n\nMein erster Gedanke war, dass, wenn ich Kafka mehr Speicher mit großen Puffergrößen gebe, es viel schneller laufen sollte. Code-Änderungen in der Konfiguration wären unnötig oder minimal. Eine weitere Option bestand darin, Kafka so zu ändern, dass es in persistenten Speicher schreibt, anstatt auf ein Dateisystem zu schreiben, wodurch die Festplatte umgangen wird. Das letzte, worauf ich geachtet habe, war die Erstellung eines persistenten Dateisystems mit persistentem Speicher und dann das Platzieren der Kafka-Logs auf diesem neuen Dateisystem.\n\nDie einfachste der drei Optionen war die erste – mehr Speicher. Ich habe alle meine Aufgaben mit mehr Speicher ausgeführt, aber es gab keine Veränderung in der Leistung. Der Grund dafür ist, dass letztendlich meine Puffer gefüllt wurden und ich auf ein Laufwerk auslagern musste. Letztendlich musste alles in die Kafka-Logs, was mein Engpass war.\n\nDie zweite Option beinhaltet das Umschreiben des Codes und das Warten auf Genehmigungen, also bin ich zur dritten Option übergegangen. Die Ergebnisse dieses Experiments, bei dem ich die Logs auf dieses neue, ultraschnelle Dateisystem gezeigt habe, waren faszinierend. Schauen wir uns den Prozess und die Ergebnisse an.\n\n## Testeinschränkungen\n\nUm Hindernisse für die Leistungstestung zu beseitigen, habe ich das Netzwerk außer Acht gelassen, indem ich meinen Test auf derselben Maschine durchgeführt habe, auf der sich mein Broker befand. Außerdem habe ich zunächst nur Produzenten, dann nur Verbraucher und danach gemischte Tests durchgeführt, um die Unterschiede abschätzen zu können. Mein Ziel war es nicht, eine Gesamtverbesserung der Produktion zu betrachten, sondern herauszufinden, ob dies einen wirklichen Unterschied für einen einzelnen Broker macht.\n\n## Erster Ansatz 50/50\n\nDas erste, was ich gemacht habe, war, die Hälfte meines persistenten Speichers in den App-Direct-Modus zu versetzen und es in ein Dateisystem umzuwandeln. Die andere Hälfte ließ ich als Speicher. Ich habe die Befehle \"ndctl\" und \"ipmctl\" verwendet und Namensräume erstellt. Diese Dateisysteme habe ich eingebunden und meinen Test ausgeführt.\n\n## Ändern der Nachrichtengröße\n\nIch habe die Tests für verschiedene Nachrichtengrößen durchgeführt. Ich habe eine bestimmte Optimierung erwartet, vor allem für 1 Kilobyte. Ich habe festgestellt, dass die Leistung bis zu etwa 10 Produzenten immer besser wurde. Ab dem Zeitpunkt, an dem ich mehr als 10 Produzenten hatte, wurde der Bus gesättigt und es traten einige Schwankungen auf. Das sagt mir, dass ich etwas zwischengespeichert habe. Ich kann nun diese Zahlen mit dem vergleichen, was ich zuvor nur auf einer SATA-Festplatte für die Kafka-Protokolle getestet habe. Ich habe auch unsere Optane-NVMe-Laufwerke für die Protokolle ausprobiert.\n\n## Technologievergleich\n\nLassen Sie uns einen Blick auf die Unterschiede werfen. Bei 240 Kilobyte ist es mit einer normalen SATA-Festplatte ziemlich flach. Ich habe eine Verbesserung festgestellt, und dann nahm sie ab, als die Anzahl der Produzenten zunahm. Mit der Optane NVMe-Festplatte habe ich einen schönen Höhepunkt erreicht, fast doppelt so schnell wie eine SATA-Festplatte, was ich erwartet habe, denn es handelt sich um einen NVMe-Bus anstelle eines SATA-Busses. Der Pmem ist fast fünfmal schneller als eine SATA-Festplatte und zweieinhalbmal schneller als die Optane NVMe-Festplatte. Das liegt daran, dass ich einen Speicherbus anstelle des SATA- oder NVMe-Busses verwende.\n\n## Zusätzliche Optimierung (100% App Direkt)\n\nDies lief schnell und ich habe diesen temporären 750 GB-Laufwerk schnell gefüllt. Da ich den Test noch ein wenig länger durchführen musste, bin ich zurückgegangen und habe meine Maschine so umkonfiguriert, dass sie den 100-prozentigen Direktanwendungsmodus verwendet, um jetzt die gesamten 1,5 Terabyte nutzen zu können.\n\n## Optimiertes PMEM und 100% App Direct.\n\nNachdem ich dies getan habe und dieselben Tests durchgeführt habe, habe ich ein überraschendes Ergebnis erhalten. Ich konnte mehr Produzenten hinzufügen, und meine Durchsatzrate stieg um fast das Zwei- oder Dreifache. Nun ist sie zwischen 12 und 15 Mal schneller als eine SATA-Festplatte mit 25-30 Produzenten und einer Nachrichtengröße von 240 Kilobytes. Das ist unglaublich und würde den Bedarf meiner Kunden an so vielen Brokern, Reihen und Reihen von Maschinen erheblich reduzieren. Ich habe den Test mehrmals durchgeführt, weil ich den Ergebnissen nicht glauben wollte. Ich habe einen unserer Architekten, der diese Technologie entworfen hat, angerufen und gelernt, dass einer der Gründe für die erhöhte Geschwindigkeit darin liegt, dass bei der Verwendung eines Teils des persistenten Speichers als Arbeitsspeicher die Daten durch zwei oder drei Hops gehen mussten, die im App-Direct-Modus nicht erforderlich sind. Dadurch wird die Konfliktmenge auf der Speicherleitung verringert, und die Durchsatzrate stieg dramatisch an.\n\n## Aufruf zur Aktion\n\nDas Endergebnis ist, dass ich in der Lage war, Kafka mit Optane DC Persistent Memory als ultraschnelles Dateisystem zu verwenden, um erhebliche Verbesserungen der Durchsatzrate sowohl bei Produzenten als auch bei Konsumenten zu erzielen. Ein einzelner Broker kann jetzt 15-mal mehr Nachrichten und Durchsatz verarbeiten als zuvor, wodurch die Anzahl der für komplexe Systemarchitekturen benötigten Server verringert wird. Es ist an der Zeit, Ihre aktuelle Architektur zu überprüfen und zu sehen, ob dies Ihrem Unternehmen zugutekommen würde.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT16-de","image":"./episodes/edt-16/de/thumbnail.png","lang":"de","summary":"In dieser Episode spricht Darren darüber, wie man den Verzehr von Staus mit Intels Optane DC Persistent-Speicher verringern kann und das Experiment, das er mit überraschenden Ergebnissen durchgeführt hat. Es könnte sich möglicherweise darauf auswirken, wie wir in Zukunft über Programmierung denken."},{"id":61,"type":"Episode","title":"Sicherheit in Generative AI","tags":["collectiongenerativeai","personalizedphishingattacks","promptinjection","sharingcodeai","harnessingai","digitaltransformation","generativeai","cybersecurityrisks","serviceproviders","duediligence","riskschallenges","digitallandscape","proactivecybersecurity","llm","multifactorauthentication","voicerecognition","typingcadence","github","stackoverflow","samsungipleak","securityaspects","embracingdigital","edt160"],"body":"\r\n\r\n## Personalisierte und überzeugende Phishing-Angriffe\n\nEine der hauptsächlichen diskutierten Bedenken besteht in der Möglichkeit für immer ausgefeiltere und personalisierte Phishing-Angriffe. Phishing steht derzeit als die effektivste Methode für Cyberangriffe da, und mithilfe generativer künstlicher Intelligenz können Angreifer hochgradig personalisierte und überzeugende Phishing-E-Mails oder -Nachrichten erstellen. Durch das Abrufen von Informationen aus sozialen Medien oder anderen Online-Plattformen können Angreifer ihre Phishing-Versuche schwerer erkennbar machen. Dies wirft die Frage auf, wie wir feststellen können, was echt ist und was nicht, und wie wir die Authentizität der empfangenen Informationen vertrauen können.\n\nUm dem entgegenzuwirken, müssen Einzelpersonen möglicherweise neue Methoden zur Überprüfung von Informationen entwickeln, wie z.B. die Verwendung von persönlichen Codewörtern oder anderen Authentifizierungsmaßnahmen bei nahestehenden Personen. Darüber hinaus müssen Organisationen und Sicherheitsagenturen ihre Strategien anpassen, um der zunehmenden Raffinesse von durch generative KI ermöglichten Cyberangriffen entgegenzuwirken. Es ist entscheidend zu verstehen, dass generative KI selbst eine neutrale Technologie ist und ihre Auswirkungen davon abhängen, wie sie eingesetzt wird.\n\n## Klone Stimmen und Vertrauen in Informationen\n\nDer Podcast untersucht auch das Potenzial von generativer KI zur Klonung von Stimmen, was bereits bei virtuellen Entführungsangriffen beobachtet wurde. Kriminelle verwenden geklonte Stimmen, um ein Gefühl von Dringlichkeit und Angst zu erzeugen und sich als geliebte Personen des Opfers auszugeben. Dies wirft Bedenken hinsichtlich des Vertrauens in die Echtheit der Informationen auf, die wir erhalten.\n\nIn einem solchen Szenario wird es unerlässlich, Techniken zur Überprüfung der Authentizität von Stimmen und Informationen zu entwickeln. Als Individuen sollten wir wachsam bleiben und Vorsicht walten lassen, wenn wir auf dringende Anfragen am Telefon reagieren. Eine offene Kommunikation mit vertrauenswürdigen Kontakten sicherstellen, kann helfen zu überprüfen, ob solche Anfragen echt sind.\n\n## Den Schutz des geistigen Eigentums bei Kodierung und Programmierung\n\nDer Podcast führt zu einer Diskussion über die Bedeutung des Schutzes des geistigen Eigentums beim Codieren und Programmieren über. Die Moderatoren betonen die Risiken des unbeabsichtigten Teilens von Code auf Plattformen wie StackOverflow und GitHub sowie das versehentliche Durchsickern von geistigem Eigentum beim Suchen von Hilfe in diesen öffentlichen Foren. Entwickler werden ermutigt, sensible Informationen durch Platzhalter zu ersetzen, bevor sie Code teilen, um das Risiko eines Verlusts geistigen Eigentums zu mindern.\n\nDarüber hinaus diskutieren die Gastgeber die Einführung von Tools wie GitHub Copilot, die generative KI nutzen, um Code-Vorschläge zu liefern. Obwohl solche Tools wertvoll sein können, werfen sie Bedenken hinsichtlich der Sicherheit und Privatsphäre von proprietären Informationen auf. Entwickler müssen sorgfältig die Vertrauenswürdigkeit des Dienstanbieters prüfen und für angemessenen Schutz ihrer Daten und geistigen Eigentumsrechte sorgen.\n\n## Die Balance zwischen Innovation und Sicherheit im Zeitalter der künstlichen Intelligenz\n\nDas Gespräch schließt mit der Betonung der Bedeutung, ein Gleichgewicht zwischen der Akzeptanz der Fortschritte und potenziell positiven Veränderungen, die durch generative KI verursacht werden, und der Bewältigung der damit verbundenen Risiken im Bereich der Cybersicherheit und des Schutzes des geistigen Eigentums zu finden. Es ist entscheidend, informiert zu bleiben, Sicherheitsstrategien anzupassen und Vorsicht walten zu lassen, um die sich wandelnde Landschaft der digitalen Transformation erfolgreich zu bewältigen. Indem wir dies tun, können wir die Vorteile von KI nutzen, ohne die Sicherheit und persönliche Informationen zu gefährden.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT160-de","image":"./episodes/edt-160/de/thumbnail.png","lang":"de","summary":"In dieser Folge wird Moderator Darren Pulsipher von Dr. Jeffrey Lancaster begleitet, um in die Schnittstelle zwischen generativer KI und Sicherheit einzutauchen. Das Gespräch geht ausführlich auf die potenziellen Risiken und Herausforderungen ein, die mit der Verwendung generativer KI für böswillige Aktivitäten, insbesondere im Bereich der Cybersicherheit, verbunden sind."},{"id":62,"type":"Episode","title":"Natürliche Sprachdatenanalytik","tags":["collectiongenerativeai","ai","generativeai","embracingdigital","edt161","challengesinnaturallanguageprocessing","fantasysportsapp","naturallanguagegeneration","highqualitycontent","technicalbackground","outoftheboxthinking","pushingboundaries","diversity","crossdomaincollaboration","innovativeideas","infoscentience'ssolution","dataanalytics","naturallanguageaisystem","conceptualautomata","datasets","revolutionizing","businessesanalyzeinformation","futureofdataanalysis","naturallanguagereporting","flexibility","tailored","differentindustries","customized","specificcontext","jargon","dataanalysis","revolutionizingindustries","sportsanalytics","stevewasick'sjourney","innovativeapproach","entrepreneurs","techfounders","unconventionalpaths","successfulinnovations","embracingvariability","context","poweroflanguage"],"body":"\r\n\r\n## Herausforderungen in der natürlichen Sprachverarbeitung\n\nSteve erinnert sich an sein frühes Projekt - eine App für Fantasy-Sport, die den Benutzern nicht nur Statistiken, sondern auch den Kontext und die Geschichten hinter den Zahlen bieten sollte. Dies führte ihn zum Bereich der natürlichen Sprachgenerierung, wo er Herausforderungen bei der Beschaffung und Bereitstellung hochwertiger Inhalte bewältigen musste. Trotz fehlendem technischen Hintergrund ermöglichten ihm Steves vielfältige Erfahrungen, diese Herausforderungen mit Kreativität und unkonventionellem Denken anzugehen.\n\n## Grenzen ausloten\n\nDarren lobt Steve dafür, dass er Grenzen auslotet und einen frischen Blickwinkel in das Feld einbringt. Dies betont die Bedeutung von Vielfalt und fachübergreifender Zusammenarbeit bei der Generierung innovativer Ideen und Lösungen. Steves Reise dient als Inspiration für angehende Unternehmer und Technologiegründer und beweist, dass unkonventionelle Wege zu erfolgreichen Innovationen führen können.\n\n## InfoScentience's Lösung für Datenanalytik\n\nDas Gespräch befasst sich auch mit den Fähigkeiten des natural language AI-Systems von InfoSentience. Steve erläutert, dass ihre Technologie Ereignisse und Geschichten in ihre Bestandteile zerlegt und so ein besseres Verständnis komplexer Konzepte und ihrer Zusammenhänge ermöglicht. Diese analytische Engine, basierend auf konzeptuellen Automaten, ermöglicht die Synthese vielfältiger und komplexer Datensätze und revolutioniert die Art und Weise, wie Unternehmen Informationen analysieren.\n\n## Die Zukunft der Datenanalyse und des natural language reporting\n\nDarüber hinaus betont Steve die Flexibilität ihres KI-Systems, das an verschiedene Branchen angepasst und an die einzigartigen Bedürfnisse jedes Kunden angepasst werden kann. Durch das Verständnis des spezifischen Kontexts und Fachwissens der analysierten Daten stellt Info Sentience sicher, dass ihr KI-System genaue und relevante Einblicke liefert.\n\nZusammenfassend hebt die Podcast-Episode das Potenzial der Datenanalyse natürlicher Sprache hervor, um Branchen wie Sportanalytik zu revolutionieren. Steve Wasicks Reise und innovativer Ansatz dienen als Inspiration für Unternehmer und Technologiegründer und erinnern uns daran, dass unkonventionelle Wege zu erfolgreichen Innovationen führen können. Die Zukunft der Datenanalyse liegt darin, Varianz, Kontext und die Kraft der Sprache zu nutzen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Wasick"],"link":"/episode-EDT161-de","image":"./episodes/edt-161/de/thumbnail.jpg","lang":"de","summary":"In der neuesten Episode trifft sich Darren Pulsipher mit Steve Wasick, dem CEO und Gründer von InfoSentience, um über die Kraft und das Potenzial von Natural Language Data Analytics zu diskutieren. Steve, der aus einem unkonventionellen Hintergrund als Englisch-Major, der zum Drehbuchautor, Anwalt und schließlich zum Technologiegründer wurde, bringt eine einzigartige Perspektive in das Feld ein."},{"id":63,"type":"Episode","title":"Aufbau einer Multi-Hybrid-Cloud-Strategie","tags":["cloudadoption","collmultihybridcloud","organizationalmodernization","barriers","riskmitigationplan","applicationrearchitecture","governance","organizationalculture","cloud","multihybridcloud","multicloud","embracingdigital","edt162"],"body":"\r\n\r\n## Best Practices für die Einführung von Cloud-Computing\n\nUmzug in die Cloud und die Übernahme neuer Technologien wie generative KI können zahlreiche Vorteile mit sich bringen, aber Organisationen müssen auch auf die damit verbundenen Veränderungen vorbereitet sein. Laut Christine McMonigal, Direktorin für Rechenzentrum und Cloud-Technologien bei Intel, gibt es bestimmte bewährte Vorgehensweisen zu beachten.\n\n## Organizational Modernization translates to German as \"Organisationsmodernisierung.\"\n\nEin wichtiger Aspekt, der erkannt werden muss, ist, dass die Einführung der Cloud nicht nur eine technologische Modernisierung ist, sondern auch eine organisatorische Modernisierung. Das bedeutet, dass Organisationen auf Veränderungen in den Prozessen, Arbeitsabläufen und sogar Organisationsstrukturen vorbereitet sein müssen. Es ist entscheidend, diese Veränderungen anzugehen und sicherzustellen, dass die gesamte Organisation auf die Transformation ausgerichtet und vorbereitet ist.\n\n## Identifizierung von Hindernissen und klare Erwartungen festlegen\n\nEin entscheidender Schritt zur Überwindung von Hindernissen und zur Risikominderung besteht darin, zunächst zu identifizieren, welche diese Barrieren sind. Durch eine gründliche Bewertung der aktuellen Infrastruktur, Arbeitsabläufe und Herausforderungen innerhalb der Organisation können potenzielle Hindernisse lokalisiert und Strategien entwickelt werden, um diese zu überwinden.\n\nDarüber hinaus ist es wesentlich, klare Erwartungen von Anfang an festzulegen. Das bedeutet eine effektive Kommunikation mit Stakeholdern, Mitarbeitern und Partnern über die Ziele, Vorteile und Herausforderungen der Implementierung von Multi-Hybrid-Cloud-Strategien. Durch realistische Erwartungen und die Gewährleistung eines gemeinsamen Verständnisses können Organisationen Überraschungen und Widerstände gegenüber Veränderungen minimieren.\n\n## Robuster Risikominimierungsplan\n\nDas Vorhandensein eines robusten Risikominderungsplans ist ein weiterer entscheidender Aspekt einer erfolgreichen Übernahme von Cloud-Diensten. Dies umfasst die Bewertung potenzieller Sicherheitsrisiken, Datenschutzbedenken und Compliance-Anforderungen. Durch proaktives angehen dieser Risiken und Implementierung geeigneter Maßnahmen können Organisationen ihre Daten schützen, regulatorische Compliance gewährleisten und potenzielle Bedrohungen minimieren.\n\n## Barriere 1: Anwendungsumgestaltung\n\nEine der Hauptbarrieren, mit denen Organisationen oft bei der Cloud-Adoption konfrontiert sind, ist die Anpassung von Anwendungen. Es ist wichtig zu prüfen, welche Anwendungen einfach so in die Cloud übertragen werden können und welche möglicherweise größere Modifikationen erfordern. Indem Möglichkeiten zur Vereinfachung und Kostenreduktion durch Automatisierung identifiziert werden, können Organisationen den Zugang und die Kontrolle optimieren.\n\n## Barriere 2: Governance\n\nGovernance-Policies spielen eine entscheidende Rolle bei der Risikominderung während der Übernahme von Cloud-Diensten. Inkonsistente Sicherheitsmodelle, unterschiedliche Management-Tools und heterogene Nutzerpolitiken können die Komplexität erhöhen und den Erfolg der Migration gefährden. Durch die Vereinfachung von Governance-Policies und die Beseitigung von Bürokratie können Organisationen ihre Abläufe optimieren, Kosten reduzieren sowie Datensicherheit und Compliance gewährleisten.\n\n## Barrier 3: Organisationskultur und Reife\n\nDie Vorbereitung der Organisation auf die Veränderungen, die mit der Nutzung von Cloud-Technologien einhergehen, ist von entscheidender Bedeutung. Dies umfasst das Einbinden der Mitarbeiter, das Bereitstellen von Schulungen zur Entwicklung von Fähigkeiten und die Identifizierung von Schlüsselpersonen, die sich auf die neuen Arbeitsweisen einlassen können. Es ist wichtig, Ängste und Sorgen der Mitarbeiter anzusprechen, wie z.B. die Angst, abgehängt zu werden oder den Job zu verlieren, um ein positives und kooperatives Umfeld zu schaffen.\n\nAbschließend erfordert die Umsetzung von Multi-Hybrid-Cloud-Strategien eine sorgfältige Planung, effektive Kommunikation und ein gründliches Verständnis der Ziele und Herausforderungen einer Organisation. Indem man Hindernisse von Anfang an angeht und Risiken mindert, kann man den Weg für eine erfolgreiche digitale Transformationsreise ebnen. Bleiben Sie dran für die nächsten Episoden, in denen wir die Entwicklung einer Cloud-Strategie, die Bewertung von Anwendungsportfolios und weitere Einblicke in die Umsetzung der digitalen Transformation erforschen werden. Vergessen Sie nicht, unseren Podcast zu bewerten und zu abonnieren, um auf dem neuesten Stand der Trends und bewährten Verfahren in der digitalen Landschaft zu bleiben.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Christine McMonigal","Darren W Pulsipher"],"link":"/episode-EDT162-de","image":"./episodes/edt-162/de/thumbnail.png","lang":"de","summary":"In dieser Episode interviewt Darren Christine McMonigal und diskutiert die Herausforderungen, mit denen Organisationen bei der Umstellung auf die Cloud und der Einführung von Multi-Hybrid-Cloud-Architekturen konfrontiert sind. Sie betonen die Bedeutung des Verständnisses dieser Hindernisse und der Bereitstellung von Richtlinien, um sie zu überwinden. Diese Episode wird tiefer in einige Schlüsselbarrieren eintauchen und Strategien zur Risikominderung erläutern, um eine erfolgreiche Cloud-Transformation zu gewährleisten."},{"id":64,"type":"Episode","title":"Entwicklung eines Mehrhybrid-Cloud-Betriebsmodells","tags":["cloudstrategy","digitaltransformation","collmultihybridcloud","cloudtechnologies","businessgoals","operationalefficiency","customersatisfaction","itinfrastructure","migrationplan","datasecurity","regulatorycompliance","hybridclouds","publicclouds","privateclouds","clouddeploymentmodels","governanceandsecurity","reducecosts","enhanceefficiency","customerexperience","strategicmove","clearvision","embracingdigital","edt163"],"body":"\r\n\r\nIn der heutigen digitalen Zeit setzen Unternehmen zunehmend auf die Cloud als strategischen Schritt zur Verbesserung der Effizienz, Kostensenkung und Verbesserung der Kundenerfahrung. Bevor man jedoch von der Cloud-Welle mitreißt, ist es für Organisationen entscheidend, einen Schritt zurückzutreten und ihre spezifischen Bedürfnisse zu bewerten. Die Entwicklung einer Cloud-Strategie ist ein entscheidender Schritt in diesem Prozess, da sie es Unternehmen ermöglicht, ihre Ziele und Vorgaben mit den ihnen zur Verfügung stehenden Cloud-Technologien abzustimmen.\n\n## Verständnis Ihrer Geschäftsziele und -ziele\n\nDer erste Schritt bei der Entwicklung einer Cloud-Strategie besteht darin, ein klares Verständnis Ihrer Geschäftsziele und -vorgaben zu gewinnen. Was möchten Sie erreichen? Möchten Sie die betriebliche Effizienz verbessern, Kosten senken oder die Kundenzufriedenheit steigern? Indem Sie eine klare Vision Ihrer Ziele haben, können Sie besser bestimmen, wie die Cloud diese Ziele unterstützen und ermöglichen kann.\n\n## Bewerten Sie Ihre bestehende Infrastruktur.\n\nNachdem Sie Ihre Ziele festgelegt haben, ist es wichtig, Ihre derzeitige IT-Infrastruktur zu bewerten. Diese Bewertung hilft dabei, mögliche Herausforderungen oder Einschränkungen bei der Migration in die Cloud zu identifizieren. Bestimmen Sie, welche Systeme und Anwendungen Sie derzeit einsetzen und prüfen Sie deren Kompatibilität mit einer Cloud-Umgebung. Diese Bewertung wird Entscheidungen darüber informieren, welche Anwendungen und Dienste sich für eine Migration eignen.\n\n## Die Auswahl des richtigen Cloud-Modells\n\nMit verschiedenen Cloud-Bereitstellungsmodellen zur Auswahl müssen Organisationen die verschiedenen Optionen bewerten, die mit ihren Geschäftsanforderungen übereinstimmen. Öffentliche Clouds, private Clouds und hybride Clouds bieten jeweils unterschiedliche Vorteile und Nachteile. Die Bewertung der Vor- und Nachteile jedes Modells wird Ihnen helfen, die geeignetste Wahl für Ihre Organisation zu treffen. Berücksichtigen Sie bei dieser Entscheidung Faktoren wie Datensicherheit, Skalierbarkeit und Compliance mit rechtlichen Bestimmungen.\n\n## Erstellung eines Migrationsplans und Gewährleistung von Governance und Sicherheit.\n\nSobald Sie ein Cloud-Modell ausgewählt haben, ist es an der Zeit, einen Migrationsplan zu erstellen. Dies beinhaltet das Festlegen der Schritte und des Zeitplans, um Ihre Anwendungen und Daten in die Cloud zu verschieben. Priorisieren Sie kritische Anwendungen, die zuerst migriert werden müssen, und entwickeln Sie eine Strategie, um die restlichen Anwendungen später zu migrieren. Implementieren Sie außerdem einen Governance- und Sicherheitsplan, um Ihre Daten zu schützen und alle regulatorischen Anforderungen zu erfüllen. Cloud-Sicherheit ist für viele Unternehmen eine Hauptanliegen, daher ist es wichtig sicherzustellen, dass Ihre Daten während des Migrationsprozesses geschützt sind.\n\nZusammenfassend ist die Entwicklung einer Cloud-Strategie ein komplexer Prozess, der sorgfältige Planung und Bewertung erfordert. Es ist unerlässlich, Ihre Geschäftsziele zu verstehen, Ihre bestehende Infrastruktur zu bewerten, das richtige Cloud-Modell auszuwählen, einen Migrationsplan zu erstellen sowie angemessene Richtlinien und Sicherheitsmaßnahmen zu implementieren. Durch die effektive Integration digitaler Transformation und die Nutzung der Kraft der Cloud können Organisationen ihre Ziele erreichen, Effizienz steigern sowie Wachstum und Erfolg fördern.\n\nIn der heutigen digitalen Ära wenden sich Unternehmen zunehmend der Cloud zu, um Effizienz zu verbessern, Kosten zu senken und die Kundenerfahrung zu verbessern. Allerdings ist es vor dem Einstieg in den Cloud-Markt für Organisationen unerlässlich, einen Schritt zurückzutreten und ihre spezifischen Bedürfnisse zu bewerten. Die Entwicklung einer Cloud-Strategie ist ein entscheidender Schritt in diesem Prozess, da sie es Unternehmen ermöglicht, ihre Ziele und Aufgaben mit den verfügbaren Cloud-Technologien in Einklang zu bringen.\n\n## Verständnis deiner Geschäftsziele und -ziele\n\nDer erste Schritt bei der Entwicklung einer Cloud-Strategie besteht darin, ein klares Verständnis für Ihre Geschäftsziele und -absichten zu erlangen. Was möchten Sie erreichen? Möchten Sie die operative Effizienz verbessern, Kosten senken oder die Kundenzufriedenheit steigern? Indem Sie eine klare Vision Ihrer Ziele haben, können Sie besser bestimmen, wie die Cloud diese Ziele unterstützen und ermöglichen kann.\n\n## Bewertung Ihrer vorhandenen Infrastruktur\n\nNachdem Sie Ihre Ziele festgelegt haben, ist es wichtig, Ihre aktuelle IT-Infrastruktur zu bewerten. Diese Bewertung hilft dabei, mögliche Herausforderungen oder Einschränkungen bei der Migration in die Cloud zu identifizieren. Bestimmen Sie, welche Systeme und Anwendungen derzeit im Einsatz sind, und prüfen Sie deren Kompatibilität mit einer Cloud-Umgebung. Diese Evaluation wird Entscheidungen darüber informieren, welche Anwendungen und Dienste für eine Migration geeignet sind.\n\n## Die richtige Cloud-Modellwahl auswählen\n\nMit verschiedenen Cloud-Bereitstellungsmodellen, die zur Verfügung stehen, müssen Organisationen die verschiedenen Optionen bewerten, die mit ihren geschäftlichen Anforderungen übereinstimmen. Öffentliche Clouds, private Clouds und Hybrid-Clouds bieten jeweils unterschiedliche Vorteile und Nachteile. Die Bewertung der Vor- und Nachteile jedes Modells wird Ihnen helfen, die geeignetste Wahl für Ihre Organisation zu treffen. Berücksichtigen Sie Faktoren wie Datensicherheit, Skalierbarkeit und regulatorische Compliance bei dieser Entscheidung.\n\n## Erstellen eines Migrationsplans und Gewährleistung von Governance und Sicherheit.\n\nSobald Sie ein Cloud-Modell ausgewählt haben, ist es an der Zeit, einen Migrationsplan zu erstellen. Dies beinhaltet die Festlegung der Schritte und des Zeitplans für die Übertragung Ihrer Anwendungen und Daten in die Cloud. Priorisieren Sie kritische Anwendungen, die zuerst migriert werden müssen, und entwickeln Sie eine Strategie, um die übrigen Anwendungen später zu migrieren. Implementieren Sie außerdem einen Governance- und Sicherheitsplan, um Ihre Daten zu schützen und alle regulatorischen Anforderungen zu erfüllen. Cloud-Sicherheit ist eine große Sorge vieler Unternehmen, daher ist es wichtig sicherzustellen, dass Ihre Daten während des gesamten Migrationsprozesses geschützt sind.\n\nZusammenfassend lässt sich sagen, dass die Entwicklung einer Cloud-Strategie ein komplexer Prozess ist, der sorgfältige Planung und Bewertung erfordert. Es ist essentiell, Ihre Geschäftsziele zu verstehen, Ihre bestehende Infrastruktur zu bewerten, das richtige Cloud-Modell zu wählen, einen Migrationsplan zu erstellen und angemessene Governance- und Sicherheitsmaßnahmen umzusetzen. Indem Organisationen die digitale Transformation effektiv annehmen und die Stärken der Cloud nutzen, können sie ihre Ziele erreichen, die Effizienz steigern und Wachstum sowie Erfolg vorantreiben.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Rajiv Mandal","Darren W Pulsipher"],"link":"/episode-EDT163-de","image":"./episodes/edt-163/de/thumbnail.png","lang":"de","summary":"In dieser Episode interviewt Darren den Cloud-Lösungsarchitekten Rajiv Mandal zu der Entwicklung einer Multi-Hybrid-Cloud-Strategie in Ihrer modernen IT-Organisation."},{"id":65,"type":"Episode","title":"Anwendungs- und Arbeitslastportfolios bei der Cloud-Migration","tags":["cloudmigration","organizations","collmultihybridcloud","cloudnative","datacenterdependencies","hybridstrategy","technicaldebt","applications","workloads","politicalcapital","customerfacingapplications","importance","applicationrationalization","analysis","dependencies","smoothtransition","surprises","decision","compliancerequirements","regulatedindustries","compliancemonitoring","embracingdigital","edt164"],"body":"\r\n\r\n## Verständnis für Anwendungs- und Arbeitslastportfolios bei Cloud-Migration.\n\nWenn es um die Cloud-Migration geht, lassen sich Unternehmen in der Regel in zwei Gruppen einteilen. Die erste Gruppe besteht aus Cloud-nativen Organisationen, welche ihre Anwendungen in der Cloud entwickelt haben und dabei jegliche Abhängigkeiten von Rechenzentren beseitigt haben. Die zweite Gruppe verfolgt eine hybride Strategie und ist sowohl auf das Rechenzentrum als auch auf die Cloud angewiesen. Allerdings können auch diese hybriden Organisationen technische Schulden haben, die angegangen werden müssen.\n\nEine der Hauptherausforderungen bei der Migration in die Cloud besteht darin, die Komplexität von Anwendungen und Arbeitslasten zu verstehen. Sarah stellt das Konzept des \"politischen Kapitals\" vor, das eine Anwendung mit sich bringt. Während sich externe und kundenorientierte Anwendungen oft der größten Aufmerksamkeit und Investitionen erfreuen, können kleinere Anwendungen, die nicht bedeutend zu sein scheinen, eine erhebliche Auswirkung auf die Organisation haben, wenn sie nicht richtig funktionieren oder vernachlässigt werden.\n\n## Die Bedeutung der Anwendungsrationalisierung\n\nSarah teilt eine persönliche Erfahrung, die die Bedeutung der Berücksichtigung des Gesamtportfolios von Anwendungen und Arbeitslasten während der Cloud-Migration hervorhebt. Sie hat eine Störung des Geschäfts beobachtet, die auf mangelnde Aufmerksamkeit für eine scheinbar kleine kundenorientierte Anwendung zurückzuführen war. Diese Erfahrung betont die Notwendigkeit für Organisationen, eine gründliche Analyse und Rationalisierung ihres Anwendungsportfolios durchzuführen, bevor sie in die Cloud migrieren.\n\nDurch das Verständnis der Komplexitäten und Abhängigkeiten von Anwendungen und Arbeitslasten können Organisationen einen reibungslosen Übergang in die Cloud gewährleisten, mit weniger Überraschungen oder Unterbrechungen. Sarah betont die Notwendigkeit für Organisationen, die Anwendungsrationalisierung zu priorisieren, um kritische Anwendungen zu identifizieren, die möglicherweise zusätzliche Investitionen und Aufmerksamkeit erfordern, auch wenn sie nicht die sichtbarsten sind.\n\n## Zu berühren oder nicht zu berühren: Die Bewertung von Workloads für die Cloud-Migration\n\nWenn Sie Workloads in die Cloud migrieren, können Sie zahlreiche Vorteile erzielen, aber es ist möglicherweise nicht immer erforderlich oder vorteilhaft, bestimmte Workloads oder Anwendungen anzufassen. Einige Workloads laufen möglicherweise seit Jahren reibungslos und sind für den Geschäftsbetrieb der Organisation unerlässlich. In solchen Fällen ergibt es möglicherweise keinen Sinn, Änderungen vorzunehmen oder sie in die Cloud zu migrieren.\n\nFaktoren, die bei der Entscheidungsfindung zu berücksichtigen sind, umfassen das Maß an Anpassung und Integration der Arbeitsbelastung, das Vorhandensein von technischen Schulden und die bevorstehende Ausmusterung von Altsystemen. Es ist jedoch unerlässlich, diese Arbeitsbelastungen regelmäßig neu zu bewerten, um sicherzustellen, dass sie weiterhin den Bedürfnissen der Organisation entsprechen. Das Überwachen von Branchentrends und technologischen Fortschritten kann helfen, potenzielle Veränderungen in der Zukunft zu identifizieren.\n\n## Die Einhaltung der Compliance-Anforderungen bei der Cloud-Migration verwalten\n\nCompliance-Anforderungen können bei der Cloud-Migration, insbesondere für Organisationen in regulierten Branchen, Herausforderungen darstellen. Cloud-Service-Anbieter haben jedoch bedeutende Fortschritte bei der Bewältigung dieser Bedenken gemacht. Sie bieten Tools und Dienste an, die die Überwachung und Berichterstellung der Compliance automatisieren und Organisationen somit ihre Einhaltung erleichtern.\n\nUm diese Herausforderungen zu bewältigen, sollten Organisationen eine gründliche Überprüfung ihrer Compliance-Anforderungen durchführen. Die Beratung von Experten, die Richtlinien zur Einhaltung von Standards bereitstellen und eine Cloud-Architektur entwerfen können, die diesen Anforderungen entspricht, ist entscheidend. Regelmäßige Überprüfungen und Überwachungen sollten umgesetzt werden, um die fortlaufende Compliance zu gewährleisten.\n\n## Schlussfolgerung\n\nIn dieser Podcast-Episode beleuchten Darren Pulsipher und Sarah Musick wichtige Aspekte der Cloud-Migration, einschließlich der Rationalisierung von Anwendungsportfolios, der Entscheidungsfindung bezüglich betroffener Arbeitslasten und der Erfüllung von Compliance-Anforderungen. Durch das Verständnis dieser Faktoren und einem aktiven Management der technischen Schulden können Organisationen eine erfolgreiche Cloud-Migrationsreise antreten und dabei die Agilität und Flexibilität der Cloud nutzen, während sie Risiken und Unterbrechungen minimieren.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Sarah Musick","Darren W Pulsipher"],"link":"/episode-EDT164-de","image":"./episodes/edt-164/de/thumbnail.png","lang":"de","summary":"In dieser Episode interviewt Darren Sarah Musick, Cloud Solution Architect bei Intel. Gemeinsam tauchen sie in das Thema Anwendungs- und Arbeitslastportfolios bei der Cloudmigration ein. Mit Sarahs Hintergrund in der Cloudberatung und -optimierung bringt sie wertvolle Einblicke in die Diskussion ein."},{"id":66,"type":"Episode","title":"Workload Cloud-Platzierungs-Faktoren","tags":["cloudcomputeoptions","collmultihybridcloud","cloudinstance","cloudspecialistadvice","computeselectiontools","optimizecloud","workloadcloud","vendorlockinprevention","cloudbestpractices","cloudinstanceperformance","costeffectivecomputesolutions","workloadoptimization","resourceefficiency","cloud","multihybridcloud","embracingdigital","edt165"],"body":"\r\n\r\n## Verstehen von Computeroptionen\n\nCloud-Dienstleister (CSPs) bieten eine Mischung aus verschiedenen Rechenfamilien an, die von älteren Generationen von Rechenhardware bis hin zu den neuesten und leistungsfähigeren Instanzen reichen. Diese älteren Generationen werden häufig für kosteneffiziente Rechenfunktionen verwendet, während neuere Generationen verbesserte Leistung zu ähnlichen oder niedrigeren Preisen bieten.\n\nEs kann überwältigend sein, sich durch die zahlreichen computing-Optionen in der Cloud zu navigieren, insbesondere wenn regelmäßig neue Instanzen veröffentlicht werden. Genau hier kommen Cloud-Spezialisten wie die bei Intel ins Spiel. Diese Experten können wertvolle Erkenntnisse liefern und bei der Auswahl der geeignetsten Instanz für eine bestimmte Arbeitslast unterstützen.\n\n## Informierte Entscheidungen treffen\n\nUm die beste Entscheidung zu treffen, suchen Sie den Rat von Cloud-Spezialisten oder verwenden Sie Tools wie Densify oder Intel Site Optimizer. Diese Tools nutzen maschinelles Lernen, um die Funktionen einer Anwendung, die Rechenleistung und die Netzwerkanforderungen zu analysieren und die geeignetste Instanzgröße zu bestimmen. Durch die Nutzung dieser Ressourcen können Organisationen sicherstellen, dass sie das Beste aus ihren Cloud-Ressourcen herausholen und Unter- oder Überauslastung vermeiden.\n\n## Umsetzung bewährter Verfahren\n\nEs ist wichtig, Empfehlungen für Instanzen in Infrastruktur-als-Code (IaC) Skripte, wie zum Beispiel TerraForm, einzubinden, um die automatische Auswahl der leistungsstärksten Instanz für eine Arbeitslast zu ermöglichen. Dadurch wird eine konsistente und effiziente Platzierung der Instanzen sichergestellt, das Risiko menschlicher Fehler reduziert und die Leistung optimiert.\n\n## Unter Berücksichtigung der Portabilität\n\nWährend Intel derzeit den Cloud-Markt mit x86-basierten Instanzen dominiert, gibt es etwas Konkurrenz von AMD und ARM. ARM-basierte Prozessoren, wie der Graviton, sind unter CSPs beliebt, benötigen jedoch eine höhere Workload-Portabilität zwischen Anbietern und zwischen öffentlichen und privaten Umgebungen. Das Portieren von x86-basierten Workloads auf ARM würde umfangreiche Code-Refactoring und Neuentwicklung erfordern.\n\nOrganisationen sollten bei der Rückführung von Arbeitslasten aus der Cloud in ihre eigene Infrastruktur die Kompatibilitätsprobleme beachten. Es ist entscheidend, die Portabilität und Flexibilität der gewählten Computing-Plattform zu bewerten, um reibungslose Übergänge sicherzustellen und eine Anbieterbindung zu vermeiden.\n\n## Schlussfolgerung.\n\nDie Auswahl der richtigen Cloud-Instanz ist eine entscheidende Entscheidung, die sich auf die Leistung, Kosten und Portabilität Ihrer Workload auswirken kann. Mit Unterstützung von Cloud-Spezialisten und Tools können Organisationen informierte Entscheidungen treffen und die Nutzung ihrer Cloud-Ressourcen optimieren. Durch das Verständnis der verfügbaren Rechenoptionen, die Integration bewährter Verfahren und die Berücksichtigung der Portabilität können Unternehmen das volle Potenzial der Cloud nutzen und gleichzeitig Flexibilität und Effizienz in ihren Abläufen sicherstellen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Ricardo Dutton Jr","Darren W Pulsipher"],"link":"/episode-EDT165-de","image":"./episodes/edt-165/de/thumbnail.png","lang":"de","summary":"In diesem Podcast tauchen Darren und Rico Dutton in die Welt der Cloud-Instanzen ein und erläutern die Faktoren, die bei der Auswahl der richtigen Instanz für Ihre Workload zu beachten sind. Sie diskutieren die verschiedenen Berechnungsoptionen, die in der Cloud verfügbar sind, die Bedeutung des richtigen Gleichgewichts zwischen Leistung und Kosten und die Rolle von Cloud-Spezialisten bei der Unterstützung von Organisationen bei informierten Entscheidungen."},{"id":67,"type":"Episode","title":"Agilität bei der Cloud-Adoption","tags":null,"body":"\r\n\r\n## Cloud-Migration als fortlaufende Reise\n\nWährend viele Menschen die Cloud-Migration als einen einmaligen Prozess betrachten, ist es wesentlich, sie als kontinuierliche Reise zu betrachten, bei der Entwickler und Betriebsteams zusammenarbeiten. Sobald die Arbeitslasten modernisiert und bereitgestellt sind, ist eine ständige Überwachung und Bewertung notwendig, um festzustellen, ob sie die geschäftlichen Ziele und Erfolgskennzahlen erfüllen.\n\nIndem Organisationen die Cloud-Migration als fortlaufende Reise behandeln, können sie ihre Teams befähigen, ihren Erfolg zu wiederholen, zu optimieren und zu verbessern. Dieser Ansatz ermöglicht Agilität, Anpassungsfähigkeit und die Fähigkeit, auf sich entwickelnde Geschäftsanforderungen zu reagieren.\n\n## Rückführung von Arbeitslasten und Flexibilität\n\nEin wichtiger Aspekt, der berücksichtigt werden muss, ist die Möglichkeit, Workloads bei Bedarf wieder vor Ort zu migrieren, falls die erwarteten Vorteile aus der Cloud nicht erzielt werden oder wenn ein Wechsel zwischen verschiedenen Cloud-Anbietern erforderlich ist. Um kontinuierliche Verbesserung zu erreichen, ist es notwendig, die Situation kontinuierlich zu evaluieren, Erwartungen im Voraus festzulegen und im Cloud-Betriebsmodell agil und flexibel zu sein.\n\nEine konsistente Infrastruktur über mehrere Clouds hinweg ist entscheidend, um Flexibilität und Agilität zu ermöglichen. Während Cloud-Service-Anbieter versuchen können, Kunden auf ihre Dienste zu beschränken, sollten Organisationen dieser Versuchung widerstehen und auf Konsistenz über die Clouds hinweg zielen oder bereit sein, die erforderlichen Änderungen vorzunehmen, wenn Workloads an verschiedene Standorte verlagert werden.\n\n## Werkzeuge und bewährte Verfahren zur Optimierung\n\nDie Optimierung von Cloud-Umgebungen kann komplex und zeitaufwendig sein und erfordert Fachkenntnisse und Ressourcen. Die Tools und bewährten Verfahren von Intel können Organisationen dabei helfen, die Platzierung von Arbeitslasten zu bewerten und zu optimieren sowie eine kontinuierliche Echtzeitoptimierung ohne Auswirkungen auf Anwendungen zu ermöglichen. Durch die Automatisierung bestimmter Aspekte des Optimierungsprozesses können diese Tools Organisationen Zeit und Geld sparen und die Gesamtleistung verbessern.\n\nUm die Vorteile dieser Werkzeuge zu maximieren, ist es entscheidend, Arbeitslasten nach Kriterien wie Standardisierung, Kritikalität und Experimentierfreudigkeit in verschiedene Kategorien einzuteilen. Zum Beispiel sollten Arbeitslasten, die eine hohe Verfügbarkeit und geringe Latenz erfordern, auf dedizierter Infrastruktur platziert werden, während weniger kritische Arbeitslasten auf gemeinsam genutzter Infrastruktur platziert werden können. Organisationen können einen gezielten Ansatz zur Optimierung verwenden, um sicherzustellen, dass ihre Cloud-Umgebung an ihre spezifischen Bedürfnisse und Ziele angepasst ist.\n\n## Die digitale Transformation umarmen und in die Cloud migrieren\n\nDie Relevanz von organisatorischem Wandel und dem Lernen aus erfolgreichen und erfolglosen Methoden wird auch in dieser Episode hervorgehoben. Um Organisationen bei ihrem Cloud-Migrationsprozess zu unterstützen, können wertvolle Ressourcen und Anleitungen auf embracingdigital.org gefunden werden.\n\nZusammenfassend können Organisationen durch die Implementierung von kontinuierlicher Verbesserung, die Entwicklung eines strategischen Ansatzes und das Annehmen von organisatorischem Wandel ihre Cloud-Umgebung optimieren, Effizienz steigern und ihre Geschäftsziele erreichen. Die kontinuierliche Verbesserung in den Cloud-Betriebsabläufen anzunehmen und die Cloud-Migration als kontinuierliche Reise zu behandeln, ist der Schlüssel zu einer erfolgreichen Cloud-Migration.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Christine McMonigal","Darren W Pulsipher"],"link":"/episode-EDT166-de","image":"./episodes/edt-166/de/thumbnail.png","lang":"de","summary":"Die Migration in die Cloud ist kein einmaliger Prozess mehr, sondern eine kontinuierliche Reise, die ständige Bewertung, Überwachung und Anpassung erfordert, um Geschäftsziele zu erreichen. In dieser Folge unseres Podcasts spricht Moderator Darren Pulsipher mit der Gastgeberin Christine McMonigal über die Bedeutung der kontinuierlichen Verbesserung in der Cloud-Betrieb."},{"id":68,"type":"Episode","title":"Die Nutzung von KI zum Schutz von Kindern.","tags":["ai","aipolicy","lawenforcement","aiethics","collgenerativeai","childprotection","aiforgood","chainofcustody","policy","people","embracingdigital","edt167"],"body":"\r\n\r\n## Herausforderungen bei der Strafverfolgung von Kinderschändern im Internet:\n\nEine der größten Herausforderungen bei der Verfolgung von Kinderschändern im Internet besteht darin, dass es keine einheitlichen Vorschriften über Technologie und Online-Kriminalität in den verschiedenen Rechtsgebieten gibt. Dies stellt erhebliche Hindernisse für Strafverfolgungsbehörden dar und erschwert die wirksame Strafverfolgung und Untersuchung von Fällen. Jede Region arbeitet anders und hat ihre eigenen Gesetze, Vorschriften und Verfahren. Leider können diese Unterschiede dazu führen, dass Ermittlungen und die Strafverfolgung von sexuellen Raubtieren im Internet verwirrend und schwierig sind. Oftmals sind herkömmliche Ermittlungsmethoden nicht ausreichend, um Online-Raubtiere zu fassen. Die digitale Welt hat eine neue Generation von technisch versierten Kriminellen hervorgebracht, die ihre Spuren verwischen können.\n\nStrafverfolgungsbehörden müssen mit den Ressourcen, Technologien und Schulungen ausgestattet sein, um effektiv gegen Online-Sexualstraftäter vorgehen zu können. Die Zusammenarbeit zwischen Technologieunternehmen und Strafverfolgungsbehörden ist entscheidend, um standardisierte Verfahren und Sprache für Strafverfolgung und Ermittlungen zu entwickeln. Durch die Überbrückung dieser Lücke können wir die Effizienz dieser Prozesse verbessern und die Chancen erhöhen, Kinderschänder zur Rechenschaft zu ziehen. Außerdem müssen die Öffentlichkeit über die Risiken und Gefahren von Online-Straftätern informiert werden. Eltern, Pädagogen und Betreuer müssen Kinder darüber aufklären, wie sie sich online schützen können und was sie tun sollen, wenn sie auf unangemessene Inhalte oder Kommunikation stoßen.\n\n## Die Rolle der Künstlichen Intelligenz (KI) im Beweismittelmanagement:\n\nKI-Technologien können bei der Verwaltung digitaler Beweise, insbesondere in Fällen, die Kinderschänder betreffen, von entscheidender Bedeutung sein. KI kann bei der Automatisierung des Scanvorgangs, der Berichterstellung und der Analyse von illegalen Inhalten helfen. KI-Tools können auch dazu beitragen, die Arbeitsbelastung der Ermittler zu reduzieren und es ihnen ermöglichen, sich auf besonders dringliche Fälle zu konzentrieren. Es gibt jedoch immer noch viele Herausforderungen bei der Implementierung und dem Verständnis dieser Technologien in verschiedenen Rechtsprechungen. Eine der Hauptprobleme besteht darin, dass KI nur so gut ist wie die Daten, auf denen sie trainiert wird, und diese Daten variieren je nach Rechtsprechung. Daher ist es schwierig, effektive KI-Modelle zu entwickeln, die in verschiedenen Rechtsprechungen funktionieren können.\n\nUm eine effiziente Beweismittelverwaltung sicherzustellen, müssen die Interessengruppen im Justizsystem zusammenarbeiten und KI-Werkzeuge annehmen und nutzen. Die Zusammenarbeit zwischen Technologen, Strafverfolgungsbehörden und Justizsystemen ist entscheidend, um diese Herausforderungen zu bewältigen und KI effektiv einzusetzen, um Kinder im Internet zu schützen. Die Implementierung von KI in der Beweismittelverwaltung sollte durch robuste Richtlinien und Leitfäden unterstützt werden, die den Datenschutz der Opfer schützen und den ethischen Einsatz dieser Technologien sicherstellen. Darüber hinaus sind regelmäßige Schulungen und Ausbildungen zu diesen Werkzeugen unerlässlich, um ihren effektiven Einsatz im Kampf gegen Online-Sexualstraftäter zu gewährleisten.\n\n## Zusammenarbeit und Standardisierung für effektiven Schutz\n\nZusammenarbeit und Standardisierung sind entscheidende Aspekte, um den Missbrauch von Kindern im Online-Bereich erfolgreich zu bekämpfen. Der Kampf gegen dieses abscheuliche Verbrechen erfordert eine Zusammenarbeit zwischen Technologieanbietern, Strafverfolgungsbehörden und Justizsystemen. Diese Parteien müssen gemeinsam umfassende Strategien und Lösungen entwickeln.\n\nZusammenarbeit sollte sich nicht nur auf technische Aspekte konzentrieren, sondern auch auf die Entwicklung standardisierter Verfahren und Protokolle für den Umgang mit Fällen, die Kinderschänder betreffen. Durch die Festlegung einheitlicher Sprache und Prozesse können wir Untersuchungen vereinfachen, rechtliche Verfahren beschleunigen und den allgemeinen Schutz von Kindern im digitalen Raum verbessern.\n\nDarüber hinaus sollten standardisierte Verfahren und Protokolle kontinuierlich überprüft und aktualisiert werden, um relevant und praktisch zu bleiben. Die Errichtung eines globalen Standards zur Bekämpfung der Online-Ausbeutung von Kindern würde allen Beteiligten einen Rahmen bieten, dem zu folgen ist. Dadurch würde sichergestellt, dass jeder Fall unabhängig von seinem Ort einheitlich und fair behandelt wird.\n\n## \"Die Nutzung von künstlicher Intelligenz zum Schutz von Kindern im Internet\"\n\nDie Verwendung von Künstlicher Intelligenz (KI) im Beweismittelmanagement ist entscheidend, um der Online-Ausbeutung von Kindern effektiv entgegenzutreten. Das schiere Volumen an digitalen Beweismitteln kann Ermittler überwältigen, aber KI kann helfen, indem sie die Identifikation und Analyse potenzieller Beweismittel automatisiert. Diese Automatisierung entlastet Ermittler und ermöglicht es ihnen, sich auf die kritischeren Aspekte der Untersuchung zu konzentrieren.\n\nDie Implementierung von KI in der Beweisführung erfordert jedoch sorgfältige Überlegungen. Es muss Transparenz und Verantwortlichkeit geben, wie die KI verwendet wird und was als Beweismittel betrachtet wird und was nicht. Zusätzlich müssen ethische Bedenken bezüglich des Einsatzes von KI bei der Strafverfolgung adressiert werden, wie zum Beispiel potenzielle Vorurteile in den Algorithmen.\n\n## Schlussfolgerung\n\nZusammenfassend sind Zusammenarbeit, Standardisierung und der Einsatz von KI in der Beweismittelverwaltung entscheidende Schritte für eine sicherere digitale Umgebung für Kinder. Die Bewältigung der Unordnung und fehlenden Einheitlichkeit bei Technologie und Online-Kriminalität erfordert einen gemeinsamen Einsatz aller Beteiligten. Indem wir diese Herausforderungen annehmen und zusammenarbeiten, können wir bedeutende Fortschritte im Kampf gegen Kindesausbeutung erzielen und das Wohlergehen von Kindern im digitalen Zeitalter sicherstellen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Rachel Dreikosen","Darren W Pulsipher"],"link":"/episode-EDT167-de","image":"./episodes/edt-167/de/thumbnail.jpg","lang":"de","summary":"In einem kürzlich erschienenen Podcast begrüßte Darren Pulsipher, Chief Solution Architect im öffentlichen Sektor bei Intel, Rachel Driekosen, eine Technische Direktorin bei Intel, um über den Einsatz von KI zum Schutz von Kindern im Internet zu diskutieren. Die Episode behandelt Herausforderungen bei der Strafverfolgung und Entdeckung von Kinderschändern, die Rolle von KI im Beweismittelmanagement und die Bedeutung von Zusammenarbeit und standardisierten Verfahren."},{"id":69,"type":"Episode","title":"Jeden Tag Generative KI","tags":["generativeai","firefly","videogen","figma","enterpriseaistrategy","ai","creativecontentgeneration","enhancingdailytasks","powerofgenerativeai","searchintent","contentcreationautomation","chatbotsincontentcreation","embracingdigital","edt168"],"body":"\r\n\r\n## Entfesselung von Kreativität und Produktivität mit generativen KI-Werkzeugen\n\nIn dieser Podcast-Episode führt Darren Pulsipher ein Interview mit Andy Morris, einem Enterprise AI Strategy Lead bei Intel, über die Auswirkungen von generativer KI auf das alltägliche Leben. Generative KI nutzt künstliche Intelligenz, um neuen Inhalt wie Bilder, Texte und Musik zu generieren. Das Gespräch dreht sich um die verschiedenen generativen KI-Tools und ihr Potenzial, Branchen zu revolutionieren und tägliche Aufgaben zu verbessern.\n\n## Die Kraft der generativen Künstlichen Intelligenz in der Inhaltsproduktion\n\nLaut Andy Morris werden generative KI-Werkzeuge in verschiedenen Branchen zunehmend wichtiger. Er empfiehlt, mit Suchmaschinen zu beginnen, die integrierte Open AI-Technologien haben, um generative KI zu erkunden. Diese Werkzeuge können die Suchergebnisse verbessern, indem sie relevanteren und kreativeren Inhalt liefern. Es ist jedoch wichtig, die Suchintention zu berücksichtigen, wenn man diese Werkzeuge verwendet, da sie nicht immer die gewünschten Ergebnisse für spezifische Informationen generieren können.\n\nGenerative KI macht sich auch in der Inhaltskreation bemerkbar. Chatbots zum Beispiel haben ein explosionsartiges Wachstum erlebt und werden für das Schreiben von Aufsätzen, das Erstellen von Inhalten und das Verbessern von Fotos genutzt. Ob Sie nun ein Inhalts-Ersteller oder ein Student sind, generative KI-Tools können bestimmte Aspekte des Inhaltskreatioionsprozesses automatisieren und somit Kreativität und Produktivität steigern.\n\n## Innovative Werkzeuge für Bild- und Videogenerierung.\n\nZwei aufregende Tools sind Adobe Firefly und VideoGen Video creation. Mit diesen Tools können Benutzer auf einzigartige und kreative Weise Bilder und Videos erstellen und bearbeiten.\n\nAdobe Firefly ist ein kostenloses Tool, mit dem Benutzer neue Bilder generieren und Elemente in vorhandenen Fotos ersetzen können. Seine generative Füll- und Ausfüllfunktionen ermöglichen es Benutzern, Teile eines Bildes zu ändern oder zu ersetzen und somit kreative Möglichkeiten zu erweitern. Video Gen Video hingegen konzentriert sich auf die Videoerzeugung unter Verwendung vorhandener Skripte oder Webseiten als Quellmaterial. Dieses KI-gesteuerte Tool vereinfacht das Erstellen ansprechender Videos, indem automatisch relevante Bilder und Video-Clips ausgewählt und eingefügt werden.\n\nDiese innovativen Werkzeuge bieten sowohl für Fachleute als auch für alltägliche Nutzer eine Vielzahl von Möglichkeiten. Sie ermöglichen den Zugriff auf erweiterte Bearbeitungsfunktionen und ermöglichen es den Nutzern, ihren Projekten eine kreative Note hinzuzufügen, ohne umfangreiche Kenntnisse oder Fähigkeiten in der Bearbeitungssoftware zu benötigen.\n\n## Vereinfachen der Inhalteerstellung mit generativer KI\n\nVerschiedene Werkzeuge wie VideoGen, Figma und Framer.AI haben die Inhalterstellung in verschiedenen Bereichen bequemer und effizienter gemacht.\n\nVideoGen kann Videos basierend auf dem Inhalt eines Artikels oder Blogbeitrags erstellen. Dies gelingt durch die Nutzung vorhandener Bibliotheken mit Bildern und Videoclips, wodurch der Prozess der Erstellung ansprechender Videos automatisiert wird, die eine Geschichte erzählen. Figma, ein Online-Grafikdesign-Tool, ermöglicht durch die Erstellung individueller Vorlagen eine größere Gestaltungsfreiheit. Ebenso vereinfacht Framer.AI die Erstellung von Websites durch den Einsatz von KI-Technologie, wodurch Nutzer Websites schnell erstellen und veröffentlichen können.\n\nObwohl generative KI-Werkzeuge Bequemlichkeit und Effizienz bei der Erstellung von Inhalten bieten, besteht immer noch Bedarf an menschlicher Expertise in bestimmten kreativen Aspekten. Designelemente und ästhetische Überlegungen profitieren immer noch von menschlicher Beteiligung, um visuell ansprechende Ergebnisse zu gewährleisten. Obwohl generative KI-Werkzeuge die weniger anspruchsvollen Bereiche des Marktes automatisieren können, erfordern anspruchsvollere Anwendungen oft eine menschliche Note.\n\nZusammenfassend verwandeln generative KI-Tools alltägliche Aufgaben und revolutionieren die Content-Erstellung. Von KI-angetriebenen Suchmaschinen bis hin zu leistungsstarken Tools, die von Adobe und anderen Unternehmen entwickelt wurden, öffnen diese Technologien neue Ebenen der Kreativität und Effizienz. Das Annehmen von generativer KI wird zunehmend entscheidend für Einzelpersonen und Unternehmen, um im sich entwickelnden Arbeitsmarkt konkurrenzfähig zu bleiben. Indem sie sich in diesen Tools versiert machen und ihre Fähigkeiten nutzen, können Einzelpersonen einen Wettbewerbsvorteil erlangen und neue Möglichkeiten für Beratungs- und Anpassungsdienstleistungen erschließen. Die Zukunft von generativer KI ist vielversprechend, und jetzt ist die Zeit, um diese innovativen Tools zu erkunden und anzunehmen.\n\n## Entfesselung von Kreativität und Produktivität mit generativen KI-Werkzeugen\n\nGenerative KI verwendet künstliche Intelligenz, um neuen Inhalt wie Bilder, Texte und Musik zu generieren. Die Diskussion dreht sich um die verschiedenen generativen KI-Werkzeuge und ihr Potenzial, Branchen zu revolutionieren und tägliche Aufgaben zu verbessern.\n\n## Die Macht der generativen KI in der Inhaltsproduktion.\n\nLaut Andy Morris werden generative KI-Tools in verschiedenen Branchen immer wichtiger. Er empfiehlt, mit Suchmaschinen zu beginnen, die Open-AI-Technologien integriert haben, um generative KI zu erforschen. Diese Tools können Suchergebnisse verbessern, indem sie relevanteren und kreativeren Inhalt liefern. Es ist jedoch entscheidend, die Suchabsicht zu berücksichtigen, wenn man diese Werkzeuge verwendet, da sie nicht immer die gewünschten Ergebnisse für spezifische Informationen erzeugen können.\n\nGenerative KI macht auch in der Inhalterstellung auf sich aufmerksam. Chatbots zum Beispiel haben ein explosionsartiges Wachstum erlebt und werden für das Schreiben von Aufsätzen, die Erstellung von Inhalten und die Verbesserung von Fotos verwendet. Egal, ob Sie ein Inhaltscreator oder ein Student sind, generative KI-Werkzeuge können bestimmte Aspekte des Inhalterstellungsprozesses automatisieren und somit Kreativität und Produktivität steigern.\n\n## Innovative Werkzeuge für die Generierung von Bildern und Videos\n\nZwei aufregende Tools sind Adobe Firefly und VideoGen Videoerstellung. Mit diesen Tools können Benutzer Bilder und Videos auf einzigartige und kreative Weise erstellen und bearbeiten.\n\nAdobe Firefly ist ein kostenloses Tool, das Benutzern ermöglicht, neue Bilder zu generieren und Elemente in vorhandenen Fotos zu ersetzen. Seine generative Füll- und Ausfüllfunktionen ermöglichen es Benutzern, Teile eines Bildes zu ändern oder zu ersetzen und eröffnen dadurch kreative Möglichkeiten. Video Gen Video konzentriert sich dagegen auf die Videoerzeugung unter Verwendung vorhandener Skripte oder Webseiten als Ausgangsmaterial. Dieses AI-gesteuerte Tool vereinfacht die Erstellung ansprechender Videos, indem es automatisch relevante Bilder und Video-Clips auswählt und einfügt.\n\nDiese innovativen Werkzeuge bieten sowohl Fachleuten als auch Alltagsnutzern eine Vielzahl von Möglichkeiten. Sie bieten Zugang zu erweiterten Bearbeitungsfunktionen und ermöglichen den Benutzern, ihren Projekten eine kreative Note hinzuzufügen, ohne umfangreiche Fähigkeiten oder Kenntnisse in der Bearbeitungssoftware zu benötigen.\n\n## Optimierung der Content-Erstellung mit generativer Künstlicher Intelligenz\n\nVerschiedene Tools wie Figma, VideoGen und Framer.AI haben die Inhalterstellung in verschiedenen Bereichen bequemer und effizienter gemacht.\n\nVideoGen kann Videos erstellen, die auf dem Inhalt eines Artikels oder Blogbeitrags basieren. Dies gelingt, indem bestehende Bibliotheken von Bildern und Videoclips genutzt werden, was den Prozess der Erstellung ansprechender Videos automatisiert, die eine Geschichte erzählen. Figma, ein Online-Grafikdesign-Tool, bietet mehr Gestaltungsfreiheit, indem es Benutzern ermöglicht, individuelle Vorlagen zu erstellen. In ähnlicher Weise vereinfacht Framer.AI die Erstellung von Websites durch den Einsatz von KI-Technologie, sodass Benutzer schnell Websites generieren und veröffentlichen können.\n\nObwohl generative KI-Tools Bequemlichkeit und Effizienz bei der Inhalts-Erstellung bieten, besteht in bestimmten kreativen Aspekten ein Bedarf an menschlicher Expertise. Gestaltungselemente und ästhetische Überlegungen profitieren nach wie vor von menschlichem Input, um visuell ansprechende Ergebnisse zu gewährleisten. Während generative KI-Tools die weniger erfahrenen Bereiche des Marktes automatisieren können, erfordern anspruchsvolle Anwendungen oft eine persönliche Note.\n\nZusammenfassend transformieren generative KI-Werkzeuge alltägliche Aufgaben und revolutionieren die Inhalteerstellung. Von mit KI optimierten Suchmaschinen bis hin zu leistungsstarken Werkzeugen, die von Adobe und anderen Unternehmen entwickelt wurden, werden durch diese Technologien neue Ebenen der Kreativität und Effizienz erschlossen. Das Beherrschen von generativer KI wird für Personen und Unternehmen zunehmend entscheidend, um in der sich wandelnden Arbeitswelt wettbewerbsfähig zu bleiben. Indem man sich in diesen Werkzeugen versiert zeigt und ihre Möglichkeiten nutzt, können Einzelpersonen einen Wettbewerbsvorteil erlangen und neue Beratungs- und Anpassungsdienstleistungsmöglichkeiten erschließen. Die Zukunft für generative KI ist vielversprechend, und jetzt ist die Zeit, diese innovativen Werkzeuge zu erkunden und anzunehmen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Andy Morris","Darren W Pulsipher"],"link":"/episode-EDT168-de","image":"./episodes/edt-168/de/thumbnail.png","lang":"de","summary":"In dieser Podcast-Episode interviewt Darren Pulsipher Andy Morris, einen Enterprise AI Strategy Lead bei Intel, über die Auswirkungen von generativer Künstlicher Intelligenz auf den Alltag."},{"id":70,"type":"Episode","title":"Den Menschen in KI bewahren","tags":["userexperience","humancentereddesign","ai","trust","useradoption","transparency","generativeai","jobdisplacement","opencommunication","humansupport","collgenerativeai"],"body":"\r\n\r\n## Die Priorisierung der Benutzererfahrung durch humanzentriertes Design.\n\nSunny Stueve, eine Ingenieurin für Humanfaktoren, betont die Bedeutung der Optimierung von menschlicher Erfahrung und Systemleistung bei der Entwicklung von KI-Lösungen. Sie hebt hervor, dass es wichtig ist, einen Wert und einen Plan zu haben, bevor man sich in die Programmierung vertieft. Durch die Integration von humanzentrierten Designprinzipien von Anfang an können Organisationen die Perspektive des Nutzers priorisieren und eine bessere Gesamterfahrung sicherstellen. Sunnys Aufgabe besteht darin, die Bedürfnisse der Benutzer zu verstehen und sie in den Designprozess einzubinden, um die Notwendigkeit der Neucodierung zu minimieren und die Effektivität von KI-Lösungen zu maximieren.\n\nDarren teilt eine Anekdote aus seiner Erfahrung der Zusammenarbeit mit Radiologen und betont den Wert davon, sich mit Kunden hinzusetzen und ihre Bedürfnisse zu verstehen, bevor man Software entwickelt. Diese persönliche Begegnung unterstreicht die Bedeutung, menschliche Faktoren bei der Entwicklung technologischer Lösungen zu berücksichtigen. Durch einen nutzerzentrierten Ansatz können Organisationen KI-Lösungen schaffen, die auf die Bedürfnisse der Benutzer zugeschnitten sind, was zu einer höheren Akzeptanzrate und einer gesteigerten Zufriedenheit führt.\n\n## Die Vertrauensbildung und die Nutzerakzeptanz bei der Integration von Künstlicher Intelligenz.\n\nSunny erklärt weiterhin, dass die Integration von KI einen Paradigmenwechsel in der Benutzerakzeptanz und im Vertrauen bewirkt. Bei der Durchführung eines gründlichen Entdeckungsprozesses, der qualitative und quantitative Daten sammelt, Beziehungen aufbaut und Annahmen validiert, ist es wichtig zu erkennen, dass die Einführung von KI Angst und höhere Vertrauenshürden auslösen kann. Menschen sind Gewohnheitstiere und folgen Mustern, daher ist es entscheidend, Benutzer zu informieren und Vertrauen aufzubauen, um Widerstand gegen Veränderungen zu überwinden.\n\nUm das Vertrauensproblem anzugehen, ist Transparenz entscheidend. Indem den Benutzern Informationen über die verwendeten KI-Modelle, die Absicht dahinter und die genutzten Daten beim Aufbau der Algorithmen bereitgestellt werden, ermöglicht man informierte Entscheidungsfindung. Designer können zudem kritisches Denken betonen und dazu ermutigen, Informationen von mehreren Quellen zu überprüfen und KI-generierte Informationen unabhängig zu validieren.\n\nDesigner sollten auch in Betracht ziehen, Benutzeroberflächengestaltungsprinzipien einzubeziehen, die dem einzigartigen Charakter von generierender Künstlicher Intelligenz gerecht werden. Dies kann die Einbindung klarer Hinweise beinhalten, wenn die KI Informationen generiert, und die Integration multimodaler Schnittstellen ermöglichen, die gleichzeitig die Interaktion mit Sprache, Text und visuellen Elementen erlauben. Indem sie Benutzer informiert, einbezieht und befähigt, können Organisationen Vertrauen aufbauen und die Benutzerakzeptanz von KI-Technologie fördern.\n\n## Anpassung an Veränderungen: ein auf den Menschen ausgerichteter Ansatz zur Generativen KI\n\nDie Transkription des Podcasts untersucht auch die Auswirkungen der generativen Künstlichen Intelligenz auf Arbeitsplätze und Arbeitsabläufe. Obwohl es Bedenken über Jobverlust gibt, betont das Gespräch die Bedeutung des Ergreifens der Chancen, die KI bietet. Anstatt die potenzielle Verdrängung von Arbeitsplätzen zu fürchten, sollten Arbeitnehmer ihre Denkweise ändern und KI als Assistenten betrachten, der die Produktivität steigern und es ihnen ermöglichen kann, sich auf sinnvollere und wertvollere Arbeit zu konzentrieren.\n\nOffene Kommunikation und die Einbeziehung der Mitarbeiter in den Veränderungsprozess sind entscheidend, um die Mitarbeiter zu motivieren und Bedenken hinsichtlich Arbeitsplatzverdrängung anzusprechen. Durch die Zusammenarbeit mit Führungskräften, um ein Verständnis für die potenziellen Auswirkungen zu gewährleisten, und das Einbeziehen von Experten für Organisationspsychologie können Organisationen die Mitarbeiter während des Veränderungsprozesses unterstützen. Durch den Aufbau von Teams, die sich auf die menschliche Unterstützung von Künstlicher Intelligenz konzentrieren, können individuelle Bedenken angegangen werden und es können Möglichkeiten geschaffen werden, in denen Rollen sich neben automatisierten Aufgaben weiterentwickeln können.\n\nZusammenfassend erfordert die Integration der KI-Technologie einen ansatzorientierten Ansatz. Die Priorisierung der Benutzererfahrung, der Aufbau von Vertrauen und die Anpassung an Veränderungen sind entscheidende Elemente für eine erfolgreiche Integration von KI-Lösungen. Indem diese Faktoren berücksichtigt werden, können Organisationen die Vorteile von KI nutzen und gleichzeitig Benutzerzufriedenheit, Vertrauen und Engagement gewährleisten.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Sunny Stueve","Darren W Pulsipher"],"link":"/episode-EDT169-de","image":"./episodes/edt-169/de/thumbnail.jpg","lang":"de","summary":"In einer kürzlich ausgestrahlten Folge des Podcasts Embracing Digital Transformation interviewt Gastgeber Darren Pulsipher, Chief Solution Architect of Public Sector bei Intel, Sunny Stueve, den Leiter für Human Centered AI bei Leidos. Der Podcast setzt sich mit der Bedeutung von humanzentriertem Design und Benutzererfahrung bei der Integration von KI-Technologie auseinander."},{"id":71,"type":"Episode","title":"Elastische Suche & Intel Optane DCPMM","tags":null,"body":"\r\n\r\nKürzlich habe ich einige Tests mit der neuen von Intel entwickelten Technologie namens Optane DC Persistent Memory (PMEM) in Verbindung mit Kafka durchgeführt. Durch die Nutzung von Optane auf unkonventionelle Weise, indem es als Dateisystem eingebunden wurde, konnte ich eine massive Verbesserung der Durchsatzleistung erzielen. Hören Sie sich hier meinen Podcast dazu an. (Können Sie hier einen Link einfügen?) Ich habe dasselbe mit Elasticsearch ausprobiert, um festzustellen, ob ich eine ähnliche Leistungssteigerung erzielen kann.\n\nElasticsearch ist eine hochskalierbare Such- und Analyse-Engine, die es ermöglicht, Daten über mehrere Knoten zu verteilen, um die Lösung zu erweitern und umfangreichere Datenmengen zu unterstützen. Mit anderen Worten handelt es sich um einen verteilten Metadaten-Manager, der hauptsächlich zur Protokollanalyse verwendet wird. Elastic selbst ist ein großartiges Werkzeug, um Daten in JSON-Format zu normalisieren. Ich kann jede Art von Daten in Elastic einfügen und es kann sich über einen verteilten Cluster erstrecken. Es ist zwar kein Nachrichtenbus wie Kafka, indexiert aber stattdessen die ankommenden Daten. Da Elastic diese Daten auf Festplatten speichert, habe ich erkannt, dass ich PMEM auf die gleiche Weise nutzen könnte wie bei Kafka.\n\n## Intel Optane DC Persistent Memory\n\nIntel Optane DC Persistent Memory wird im DDR4-Format geliefert, sodass es perfekt in den DDR4-Speichersteckplatz Ihres Servers passt. Die Module sind in 128, 256 und 512 Gigabyte erhältlich, sodass ich in einem Zwei-Sockel-System 6 Terabyte PMEM haben kann. Eine wichtige Funktion ist, dass die Hardware verschlüsselt ist und mit dieser Verschlüsselung an Ihre CPU gebunden ist, sodass sie sicher und äußerst zuverlässig ist. Sie wird bereits verwendet, um tiefgreifende Veränderungen in der Art und Weise herbeizuführen, wie sie in vielen Datenbanken, wie zum Beispiel in Oracles Exadata-Plattform sowie SAP HANA, eingesetzt wird.\n\n## Unterstützung für eine Vielfalt von Anwendungen\n\nEs gibt mehrere Möglichkeiten, diese Technologie zu nutzen.\n\nDer erste ist der Speichermodus, der die Ausdehnung des Fußabdrucks eines Servers ermöglicht. Er verwendet den PMEM genauso wie normalen Speicher. Der DDR4-Speicher fungiert als Cache für den PMEM. In diesem Modus ist die Geschwindigkeit vergleichbar mit DDR4; in den meisten Anwendungen werden Sie keine Veränderung feststellen.\n\nDer zweite Modus ist die direkte App. Im direkten App-Modus kann ich eine Anwendung schreiben, die direkt in PMEM schreibt und zeitaufwändige Schritte umgeht.\n\nDer dritte Modus verwendet den direkten App-Modus, um ein nichtflüchtiges Speicherdateisystem zu erstellen, das direkt am Speicherbus sitzt und um ein Vielfaches schneller ist als der NVMe-Bus oder sogar der SATA-Bus.\n\nMit diesem dritten Modus musste ich ein wenig über die Architektur von Elastic lernen, um herauszufinden, welche Teile ich in diesem ultraschnellen Dateisystem ausführen sollte und welche ich an ihrem Platz lassen sollte. Ich wollte auch wissen, ob ich diese Änderungen nur mit der Konfigurationsdatei vornehmen konnte.\n\n## Die Verwendung des Linux-Kernels\n\nZuerst musste ich lernen, wie man die Linux-Kernel-Befehle zur Manipulation dieses PMEM verwendet.\n\nMit dem Intel Persistent Memory Control (ipmctl) Befehl konnte ich feststellen, dass ich das PMEM so konfigurieren und verwalten konnte, dass ich es im Memory-Modus, im App-Direct-Modus oder im Memory-Modus in Prozenten betreiben konnte.\n\nDer andere Befehl ist die Steuerung eines nichtflüchtigen Speichergeräts (ndctl). Damit kann ich Namensräume und Regionen in dem von mir erstellten PMEM erstellen, sodass ich diese Region als Gerät einbinden kann. Anschließend kann ich dieses Gerät als Dateisystem einbinden.\n\n## ESRally Performance Testing übersetzen\n\nIch habe ESRally, ein Benchmark-Tool, gefunden, das ich in meinen Tests verwenden kann. Das erste Mal, als ich den Test einrichtete, habe ich ESRally von meiner normalen SATA-Festplatte ausgeführt, auf der Elastic alles von der PMEM-Festplatte ausführte. Es gab eine gewisse Leistungsverbesserung, aber was ich festgestellt habe, war, dass ich aufgrund des kontinuierlichen Datenstroms von der SATA-Festplatte, der in meinem ESRally gespeichert war, darin eingeschränkt wurde, wie schnell ich Daten in Elasticsearch pushen konnte. Also war es nicht Elastic, das langsamer wurde, sondern ESRally, weil meine SATA-Festplatte viel langsamer als meine PMEM-Festplatte war. Ich habe ESRally auf die PMEM-Festplatte verschoben. Dadurch verbesserte sich die Leistung und ich erhielt interessante Ergebnisse.\n\n## Testen von Einschränkungen\n\nUm zu sehen, welche Auswirkungen dieses PMEM Ultra-Schnelllaufwerk auf Elastic haben würde, führte ich den Test auf einer Maschine durch. Dadurch wurde die Netzwerkvariabilität entfernt, was Netzwerk-Engpässe als begrenzenden Faktor ausschloss. Ich reduzierte die Kommunikation zwischen den Diensten, was die Engpässe bei den Replikaten verringerte, und führte alle Anfragen auf PMEMs aus, was die Variabilität des SATA-Laufwerks eliminierte.\n\n## Optimale Leistung (100%) App Direct\n\nZuerst habe ich den gesamten PMEM dem app direct Modus zugewiesen, damit ich das Ganze als Dateisystem mounten konnte. Ich habe 128-Gigabyte-DIMMs verwendet, sodass ich ein Laufwerk mit 1,5 Terabyte zur Verfügung hatte. Aus meinen vorherigen Tests mit Kafka wusste ich, dass ich bessere Leistung erhalte, wenn ich den app direct Modus zu 100% anstatt zu 50% verwende.\n\nDurchsatzrate (sollte \"Durchsatz\" sein - sollte vielleicht auf der Folie geändert werden) Dokumente/Sekunde\n\nMit Hilfe der ESRally-Statistiken habe ich den Median-Durchsatz an Dokumenten pro Sekunde im Vergleich zur Anzahl der gleichzeitig laufenden Rennen mit Produzenten und Konsumenten berechnet. Mit der SATA-Festplatte erzielte ich recht gute Werte, die mit anderen veröffentlichten Tests vergleichbar waren. Mit dem PMEM-Laufwerk war es mir möglich, fast doppelt so viele Dokumente pro Sekunde zu verarbeiten. Das ist ziemlich unglaublich, bedenkt man, dass es keine Änderungen am Code gab, sondern nur eine Konfigurationsänderung.\n\n## Service Reaktionszeit\n\nDas andere Ergebnis betraf die Antwortzeiten für Funktionen. Wie erwartet steigt die Antwortzeit für Abfragen oder Funktionen, wenn die Anzahl der gleichzeitig laufenden parallelen Vorgänge zunimmt. Aber mit PMEM ist die Antwortzeit fast doppelt so schnell. Aus diesem einfachen Test habe ich gelernt, dass der Ort, an dem die Daten gespeichert werden, die Elasticsearch benötigt (PMEM oder SATA), einen Einfluss auf die Antwortzeit hat.\n\n## Schlussfolgerung\n\nDie Verwendung von Optane Persistent Memory im Dateisystemmodus zur Steigerung der Leistung und Reduzierung der Kosten von Elasticsearch-Servern erfordert minimale Änderungen an Hardware und Software sowie keine Änderungen an Elasticsearch oder den zugrunde liegenden Anwendungen. Durch Verdopplung Ihrer Durchsatzkapazität bei Elasticsearch können Sie die Gesamtanzahl der Server in Ihrem Elasticsearch-Cluster verringern und somit die Gesamtbetriebskosten senken.\n\n## Für weitere Informationen\n\nFür detailliertere Informationen schauen Sie sich den Link im Podcast zu dem Dokument an, das wir als Antwort auf diese Testergebnisse erstellt haben. Sie können mich auch unter darren.w.pulsipher@intel.com kontaktieren oder auf LinkedIn @darrenpulsipher.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT17-de","image":"./episodes/edt-17/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher zeigt, wie er die Leistung von Elasticsearch erhöhte, indem er den Intel Optane Persistent Memory im 100 Prozent App-Direct-Modus verwendete. Seine Tests zeigen eine beeindruckende Leistungssteigerung von 2x. Durch Verdoppelung der Durchsatzkapazität können Sie die Anzahl der Server in Ihrem Elasticsearch-Cluster erheblich verringern."},{"id":72,"type":"Episode","title":"Null-Vertrauensgrundsätze","tags":["zta","zerotrust","zerotrustarchitecture","implementingzerotrust","identityverification","microsegmentation","leastprivilege","encryption","continuousauthentication","anomalydetection","automatedthreatresponse","dataprevention","rightsmanagement","analytics","policyenforcement","cybersecurity"],"body":"\r\n\r\n## Umsetzung der Zero Trust-Sicherheit\n\nNull-Vertrauen-Sicherheit hat sich als immer beliebteres Modell zur Absicherung moderner IT-Umgebungen entwickelt. Aber was genau ist Null-Vertrauen und welche bewährten Maßnahmen gibt es für die Implementierung? Dieser Beitrag bietet eine Einführung in die Grundprinzipien des Null-Vertrauens und wichtige Überlegungen für die Annahme einer Null-Vertrauens-Architektur.\n\n## Was ist Zero Trust?\n\nDas Zero-Trust-Modell dreht sich um das Konzept von \"niemals vertrauen, immer überprüfen\". Im Gegensatz zur traditionellen Netzwerksicherheit, die sich auf Perimeterschutz konzentriert, geht Zero Trust davon aus, dass Angreifer bereits im Netzwerk sind. Es werden keine Benutzer oder Geräte automatisch vertraut - bei jedem Zugriffsversuch ist eine Überprüfung erforderlich.\n\nEs gibt mehrere grundlegende Prinzipien von Zero Trust:\n\nÜberprüfen Sie alle Benutzer und Geräte, bevor Sie Zugriff gewähren.\n\nBeschränken Sie den Zugriff nur auf das, was benötigt wird (geringstes Privileg).\n\nGehen Sie davon aus, dass Verstöße auftreten werden und begrenzen Sie den Radius der Auswirkungen.\n\n## Die Implementierung von Zero Trust Security.\n\nNull-Vertrauen-Sicherheit ist ein zunehmend beliebtes Modell zur Absicherung moderner IT-Umgebungen geworden. Aber was genau ist Null-Vertrauen und welche bewährten Verfahren gibt es für die Umsetzung? Dieser Beitrag liefert eine Einführung in die Null-Vertrauen-Prinzipien und wichtige Überlegungen zur Einführung einer Null-Vertrauen-Architektur.\n\n## Was ist Zero Trust?\n\nDas Zero-Trust-Modell basiert auf dem Konzept des \"niemals vertrauen, immer verifizieren\". Im Gegensatz zur herkömmlichen Netzwerksicherheit, die sich auf Perimeterverteidigungsmaßnahmen konzentriert, geht das Zero-Trust-Modell davon aus, dass Angreifer bereits innerhalb des Netzwerks sind. Weder Benutzer noch Geräte werden von Natur aus vertraut - bei jeder Zugriffsanfrage ist eine Verifizierung erforderlich.\n\nEs gibt mehrere wichtige Prinzipien des Zero-Trust-Ansatzes:\n\nÜberprüfen Sie alle Benutzer und Geräte, bevor Sie den Zugriff gewähren.\n\nBeschränken Sie den Zugriff nur auf das Notwendige (geringstes Recht)\n\nGehen Sie davon aus, dass Verstöße auftreten werden, und begrenzen Sie den Explosionsradius.\n\nÜberwachen Sie die Aktivität kontinuierlich auf Anomalien.\n\nAutomatisieren Sie die Reaktionen auf Bedrohungen.\n\nDie Einführung von Zero Trust bedeutet den Wechsel vom impliziten Vertrauen zur kontinuierlichen Authentifizierung und Autorisierung von Nutzern, Geräten und Arbeitslasten.\n\n## Die Schlüsselsäulen einer Zero-Trust-Architektur\n\nEs gibt sechs wesentliche Säulen, die eine umfassende Zero-Trust-Architektur ausmachen.\n\n## Identität\n\nStarke Identitätsprüfung und Multi-Faktor-Authentifizierung stellen sicher, dass Benutzer wirklich diejenigen sind, für die sie sich ausgeben. Zugriffsrichtlinien sind an Benutzeridentitäten gebunden.\n\n## Geräte\n\nGerätegesundheit, Sicherheitslage und Genehmigung müssen vor dem Zugriff überprüft werden. Dies beinhaltet auch Kontrollen für Geräte, die privat mitgebracht werden (BYOD).\n\n## Netzwerk\n\nSoftware-definierte Mikrosegmentierung und verschlüsselte Tunnel zwischen vertrauenswürdigen Zonen ersetzen implizites Vertrauen im Netzwerk. Zugang wird auf Basis einer Sitzung gewährt.\n\n## Arbeitsbelastung\n\nDie Anwendungsrechte sind streng begrenzt, basierend auf Identität und Umgebung. Der Zugriff auf hochwertige Vermögenswerte erfolgt über einen Gateway.\n\n## Daten\n\nEmpfindliche Daten werden durch Verschlüsselung geschützt und der Zugriff wird über Datenschutzrichtlinien und Rechtemanagement kontrolliert.\n\n## Sichtbarkeit und Analytik\n\nDie kontinuierliche Überwachung bietet Einblick in alle Benutzer, Geräte und Aktivitäten. Durch fortschrittliche Analysetechniken werden Abweichungen erkannt und automatisierte Reaktionen werden eingesetzt, um Bedrohungen einzudämmen.\n\n## Die Implementierung von Zero Trust.\n\nDer Übergang zum Zero-Trust-Modell ist eine Reise, die aktualisierte Richtlinien, Prozesse und Technologien in der gesamten Organisation erfordert. Zu den wichtigsten Schritten gehören:\n\nIdentifizieren Sie Ihre wichtigsten Assets und hochwertigen Daten.\n\n* Arbeitsabläufe und Zugangsanforderungen für diese Ressourcen skizzieren.\n\nImplementieren Sie die Zwei-Faktor-Authentifizierung und das Prinzip des geringsten Privilegs.\n\nBeginnen Sie mit der Segmentierung Ihres Netzwerks mittels Mikro-Perimeter und Kontrollpunkte.\n\nVerschlüsseln Sie sensible Daten sowohl beim Transport als auch bei Ruhe.\n\nBewerten Sie Werkzeuge für erweiterte Analysen, Automatisierung und Orchestrierung.\n\nDie Einführung von Zero Trust erfordert Zeit, kann aber Ihre Sicherheitsposition gegen moderne Bedrohungen erheblich verbessern. Ein inkrementeller risikobasierter Ansatz ermöglicht es Ihnen, Vorteile in jeder Reifephase zu erkennen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Anna Scott","Dave Marcus","Darren W Pulsipher"],"link":"/episode-EDT170-de","image":"./episodes/edt-170/de/thumbnail.png","lang":"de","summary":"In dieser Episode untersucht Darren die Prinzipien der Zero-Trust-Architektur mit dem speziellen Gast David Marcus, Senior Security Architect, und der wiederkehrenden Gast Dr. Anna Scott."},{"id":73,"type":"Episode","title":"Generative KI im öffentlichen Sektor","tags":["generativeai","artificialintelligence","languagemodels","chatgpt","texttotext","texttoaudio","texttovideo","texttoimage","bias","accountability","dataleakage","ethics","compliance","responsibleuse","representativedata","unbiaseddata","highqualitydata","harmefuloutput","misleadingoutput","transparency","claudeai","informedconsent"],"body":"\r\n\r\n## Einführung in generative KI\n\nGenerative KI ist eine Methode in der künstlichen Intelligenz, die bestehende Inhalte wie Text, Bilder oder Audio analysieren und daraus neue, originelle Inhalte generieren kann. Große Sprachmodelle wie ChatGPT haben es Entwicklern erleichtert, generative textbasierte Anwendungen zu erstellen. Diese Modelle werden auf riesigen Mengen an Daten vortrainiert und können menschenähnliche Antworten auf Textaufforderungen generieren.\n\nIm vergangenen Jahr haben wir unglaubliche Fortschritte bei der Nutzung von generativen KI-Technologien gesehen. Dies beinhaltet Chatbots, die komplexe Gespräche führen können, Übersetzungstools, die Texte in Echtzeit zwischen verschiedenen Sprachen übersetzen können, und sogar die Erschaffung völlig neuer Kunstwerke. Die Möglichkeiten sind endlos und wir können uns auf noch weiter spannende Anwendungsfälle freuen, während die generative KI sich weiterentwickelt.\n\n## Schlüsselkompetenzen und Anwendungsfälle\n\nDie Generierung von Inhalten aus anderen Inhalten wird sich weiterhin auf Bereiche wie Video, Audio und 3D-Umgebungen ausweiten. Durch Kombination verschiedener generativer KI-Modelle können neue Lösungen schnell entwickelt werden.\n\n## Text zu Text\n\nText-to-Text-Technologie hat in den letzten Jahren aufgrund ihrer Vielseitigkeit und Nützlichkeit zunehmend an Beliebtheit gewonnen. Sie hat eine breite Palette von Anwendungsmöglichkeiten, einschließlich der Erstellung von Marketinginhalten durch Generierung von eingängigen Slogans und Schlagzeilen, der Zusammenfassung umfangreicher Dokumente zu einigen Schlüsselpunkten, der Übersetzung von Material in verschiedene Sprachen und der Verbesserung der allgemeinen Kommunikation zwischen Einzelpersonen und Organisationen. Darüber hinaus können Text-zu-Text-KI-Algorithmen auch die Qualität von schriftlichen Inhalten, wie Aufsätze, bewerten und Feedback zu Grammatik, Rechtschreibung und Struktur geben. Mit all diesen praktischen Anwendungen ist es kein Wunder, dass die Text-zu-Text-Technologie zu einem unverzichtbaren Werkzeug in vielen Branchen geworden ist.\n\n## Text zu Audio\n\nDie Umwandlung von Text in Audio ist eine zunehmend beliebte Methode, um digitalen Inhalt einem breiteren Publikum zugänglich zu machen. Sie hat verschiedene Anwendungen, wie zum Beispiel die Bereitstellung eines alternativen Formats für Menschen mit Sehbehinderungen, das Unterhalten und Engagieren mit Inhalten, das Erleichtern von Übersetzungen und sogar die Unterstützung bei der Navigation. So kann beispielsweise die Text-zu-Sprache-Technologie dazu verwendet werden, Menschen mit Dyslexie oder anderen Leseschwierigkeiten den Zugang zu schriftlichen Informationen zu erleichtern. Zudem sind Hörbücher und Podcasts zu einer beliebten Form der Unterhaltung geworden und die Text-zu-Sprache-Technologie kann helfen, mehr Inhalte in diesem Format zu erstellen. Insgesamt hat die Fähigkeit, Text in Audio umzuwandeln, neue Möglichkeiten eröffnet, um digitalen Inhalt inklusiver und für alle zugänglicher zu machen.\n\n## Text zu Video\n\nText-zu-Video-Technologie ist ein aufstrebendes Feld, das in den letzten Jahren vielversprechend gezeigt hat. Es beinhaltet die Verwendung von KI-Algorithmen, um textbasierte Inhalte in ansprechende und informative Videos umzuwandeln, die für eine Vielzahl von Zwecken verwendet werden können, einschließlich Training, Marketing und anderen Anwendungen.\n\nDie Technologie funktioniert, indem sie den Text automatisch analysiert und Schlüsselkonzepte, Themen und Ideen identifiziert. Anschließend verwendet sie diese Informationen, um Bilder, Animationen und andere visuelle Elemente zu generieren, die dazu beitragen, die Botschaft des Textes zu veranschaulichen und zu vermitteln.\n\nEiner der Hauptvorteile der Text-zu-Video-Technologie besteht darin, dass sie die Zeit und Ressourcen, die zur Erstellung von hochwertigen Videos benötigt werden, erheblich reduzieren kann. Dies macht sie zu einem wertvollen Werkzeug für Unternehmen und Organisationen aller Größen, insbesondere für diejenigen mit begrenzten Budgets oder hauseigenen Videoproduktionsfähigkeiten.\n\nNeben seinen praktischen Anwendungen hat die Text-zu-Video-Technologie auch das Potenzial, die Art und Weise, wie wir Informationen konsumieren und damit interagieren, zu revolutionieren. Indem es einfacher und ansprechender wird, komplexe Ideen und Konzepte zu konsumieren, könnte es dazu beitragen, Wissen zu demokratisieren und Menschen aus allen sozialen Schichten zu befähigen, zu lernen und sich weiterzuentwickeln.\n\n## Text zu Bild\n\nDie Technologie zur Generierung von Bildern aus Text hat in den letzten Jahren erhebliche Fortschritte gemacht und ist zu einem ausgereiften Gebiet geworden. Sie hat zahlreiche Anwendungen, wie zum Beispiel im Marketing, Design, Forschung und mehr. Die damit verbundenen Risiken bei der Erstellung von gefälschtem Inhalt mit diesen Werkzeugen können jedoch nicht ignoriert werden. Es ist von entscheidender Bedeutung, diese Risiken anzugehen und sicherzustellen, dass die Technologie ethisch, verantwortungsvoll und legal genutzt wird. Dies hilft, die Verbreitung von Fehlinformationen und Fake News zu verhindern, die schwerwiegende Folgen haben können.\n\n## Risiken zu verstehen\n\n## Voreingenommenheit\n\nGenerative KI ist ein leistungsstarkes Werkzeug, das für eine Vielzahl von Anwendungen verwendet werden kann, von der Sprachübersetzung bis zur Bilderkennung. Es ist jedoch wichtig zu bedenken, dass KI-Modelle nur so gut sind wie die Daten, auf denen sie trainiert werden. Das bedeutet, wenn die Trainingsdaten in irgendeiner Weise voreingenommen sind, wird auch das resultierende KI-Modell voreingenommen sein.\n\nDas Verständnis der Trainingsdaten ist entscheidend für die Vorhersage und Minderung von Bias in KI-Modellen. Durch eine sorgfältige Analyse der Daten und Identifizierung möglicher Vorurteile können wir Schritte unternehmen, um sie zu korrigieren, bevor das Modell eingesetzt wird. Dies ist besonders wichtig in Anwendungen wie der Personalbeschaffung oder Kreditvergabe, wo voreingenommene KI-Modelle ernsthafte reale Konsequenzen haben können.\n\nIndem wir uns über die potenziellen Voreingenommenheiten in KI-Modellen im Klaren sind und Maßnahmen ergreifen, um sie zu beheben, können wir sicherstellen, dass diese Werkzeuge auf eine faire und gerechte Weise verwendet werden.\n\n## Verantwortlichkeit\n\nWenn viel auf dem Spiel steht und potenziell das Leben von Menschen oder wichtige Entscheidungen beeinflusst werden können, ist es entscheidend, die Ergebnisse zu validieren. Beispielsweise in Bereichen wie Gesundheitswesen oder Finanzen, in denen Entscheidungen auf Basis von Daten erhebliche Auswirkungen haben können, ist es unerlässlich sicherzustellen, dass die Datenanalyse und die Ergebnisse korrekt sind. Die Richtigkeit kann durch verschiedene Methoden überprüft werden, wie zum Beispiel Kreuzvalidierung, Sensitivitätsanalyse oder statistische Tests. Durch die Validierung der Ergebnisse können wir die Transparenz erhöhen, Fehler reduzieren und das Vertrauen in die datengetriebenen Entscheidungen stärken.\n\n## Datenleck\n\nWenn es um generative KI geht, ist es wichtig, die richtige Modalität zu verwenden, um sicherzustellen, dass private Daten privat bleiben. Öffentliche Modelle können manchmal mit privaten Daten trainiert werden, was dazu führen kann, dass sensible Informationen durchsickern. Daher ist es wichtig, Vorsicht walten zu lassen und die richtige Modalität der generativen KI auszuwählen, die am besten für Ihren spezifischen Anwendungsfall geeignet ist. Indem Sie dies tun, können Sie sicherstellen, dass Ihre Daten sicher bleiben und dass die Privatsphäre gewahrt wird.\n\n## Schlussfolgerung\n\nGenerative KI, die eine Teilmenge der künstlichen Intelligenz darstellt, hat die Fähigkeit, neue Daten auf Basis von Mustern zu erstellen, die in vorhandenen Daten gefunden wurden. Wie bei jeder Technologie gibt es jedoch auch Risiken bei ihrer Nutzung. Daher ist es wichtig, diese Risiken zu bewerten und bewährte Verfahren hinsichtlich Ethik, Compliance und verantwortungsvoller Nutzung bei der Anwendung generativer KI zu befolgen. Dies beinhaltet die Sicherstellung, dass die verwendeten Daten repräsentativ, unvoreingenommen und von hoher Qualität sind, sowie die Sicherstellung, dass die erzeugten Ausgaben nicht schädlich oder irreführend sind. Darüber hinaus ist es wichtig, transparent über die Nutzung generativer KI zu sein und eine informierte Zustimmung von Personen einzuholen, deren Daten verwendet werden. Indem wir uns an diese bewährten Verfahren halten, können wir die Kraft der generativen KI sicher und verantwortungsvoll nutzen, um unser Leben und die Gesellschaft insgesamt zu verbessern.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT171-de","image":"./episodes/edt-171/de/thumbnail.png","lang":"de","summary":"In dieser Episode spricht Darren über Generative KI und ihre praktischen Anwendungen. Generative KI explodiert mit neuen Fähigkeiten wie der Erstellung von Texten, Bildern, Videos und Audio. Es gibt jedoch Risiken wie Voreingenommenheit, Verantwortlichkeit und Datenlecks, die angegangen werden müssen."},{"id":74,"type":"Episode","title":"Zero-Trust-Architektur","tags":["zta","digitaltransformation","initiatives","newtechnologies","businessprocesses","customerexperience","employeeexperience","securitymodel","trustednetworks","cybercriminals","zerotrustarchitecture","continuouspolicies","granularpolicies","accesscontrol","defaultdeny","continuousauthentication","microsegmentation","networksecurity","workflow","mediumriskapplication","dependencies"],"body":"\r\n\r\nInitiativen zur digitalen Transformation zielen darauf ab, neue Technologien zu nutzen, um Geschäftsprozesse zu verbessern und bessere Erfahrungen für Kunden und Mitarbeiter zu liefern. Wenn jedoch Organisationen ihre Netzwerke erweitern und Cloud-Dienste annehmen, ist das traditionelle Sicherheitsmodell von vertrauenswürdigen Netzwerken nicht mehr ausreichend. Dies schafft Anfälligkeiten, die Cyberkriminelle ausnutzen können.\n\nDie Zero-Trust-Architektur bietet einen Rahmen zur Verbesserung der Sicherheit in heutigen komplexen Umgebungen. Aber was genau ist Zero Trust und wie können Organisationen ihre Reise zur Implementierung beginnen?\n\n## Faktoren, die die Zero Trust Architektur antreiben\n\nIm Kern geht es bei Zero-Trust-Architektur darum, kontinuierliche, granulare Richtlinien auf Vermögenswerte und Ressourcen anzuwenden, wenn Benutzer oder Einheiten versuchen, auf sie zuzugreifen oder mit ihnen zu interagieren. Diese Richtlinie wird unabhängig vom Standort angewendet - vor Ort, in der Cloud, in hybriden Umgebungen usw. Die Schlüsselprinzipien sind:\n\nStandardmäßige Verweigerung - Zugang wird standardmäßig verweigert. Benutzer müssen sich authentifizieren und für den spezifischen Kontext autorisiert sein.\n\nKontinuierliche Authentifizierung - Benutzer werden basierend auf Analysen ihrer Identität, Zeit, Gerätegesundheit usw. während ihrer Sitzungen immer wieder authentifiziert und erneut autorisiert.\n\nMikrosegmentierung - Feinkörnige Kontrollen werden für die seitliche Bewegung zwischen Vermögenswerten und Ressourcen angewendet.\n\nDies unterscheidet sich von der traditionellen Netzwerksicherheit, die implizites Vertrauen basierend darauf verwendet, ob etwas innerhalb des Netzwerkperimeters liegt.\n\n## Einführung in Zero Trust\n\nDie Umsetzung von Zero Trust ist eine kontinuierliche Reise, kein einmaliges Projekt. Organisationen müssen jedoch irgendwo anfangen. Hier sind einige bewährte Vorgehensweisen:\n\nBilden Sie sich über Zero-Trust-Frameworks und -Konzepte weiter.\n\nErstellen Sie einen Arbeitsablauf für eine mittlere Risikoanwendung und identifizieren Sie Abhängigkeiten\n\nNutzen Sie die bestehende Infrastruktur - Mikrosegmentierung, Verschlüsselung, Sichtbarkeitswerkzeuge\n\nErhalten Sie die Zustimmung der Geschäftsführung und binden Sie die Geschäftsinteressenten ein.\n\nBeginnen Sie mit einer soliden Grundlage für Cybersicherheit - Hardware-Vertrauenswurzeln, Verschlüsselung, Bestandsverzeichnis von Vermögenswerten\n\nErhöhen Sie die Sichtbarkeit in die Betriebsumgebung und die Lieferkette.\n\nObwohl Null Vertrauen möglicherweise neue Investitionen in Technologie und Prozessänderungen im Laufe der Zeit erfordert, können Organisationen erhebliche Fortschritte machen, indem sie verfeinern, wie sie das nutzen, was sie bereits haben.\n\n## Blick nach vorn\n\nDa Geschäftsanwendungen und Ressourcen weiterhin aus dem traditionellen Netzwerkperimeter hinaus verlagert werden, ermöglicht Zero Trust einen dynamischeren und kontextbezogeneren Ansatz zur Sicherheit. Anstelle von pauschalen Genehmigungen auf Basis des Standorts werden granulare Steuerungen entsprechend den spezifischen Zugriffsanforderungen angewendet.\n\nDiese Reise erfordert Wachsamkeit - Richtlinien müssen sich an die sich entwickelnden Geschäftsanforderungen anpassen und neuen Risiken begegnen. Aber mit der richtigen Vision und Engagement bietet die Zero-Trust-Architektur einen Weg nach vorne, um digitale Innovation und Resilienz zu ermöglichen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Steve Orrin","Dave Marcus","Darren W Pulsipher"],"link":"/episode-EDT172-de","image":"./episodes/edt-172/de/thumbnail.png","lang":"de","summary":"In dieser Podcast-Episode unterhält sich Darren mit Steve Orrin und Dave Marcus und diskutiert die Zero-Trust-Architektur, ein neues Sicherheitsmodell, das für heutige digitale Umgebungen benötigt wird, in denen der Netzwerkperimeter verschwindet. Experten erklären, was Zero Trust ist, grundlegende Prinzipien wie standardmäßigen Zugriffsverweigerung und kontinuierliche Authentifizierung und geben Ratschläge für Organisationen, die ihre Zero-Trust-Reise beginnen."},{"id":75,"type":"Episode","title":"Generative KI mit Claude","tags":["genai","generativeai","ai","artificialintelligence","machinelearning","deeplearning","nlp","naturallanguageprocessing","aiethics","responsibleai","aifairness","aitransparency","aiaccountability","aibias","aiharm","aimemployment","aijobloss","contentmoderation","conversationai","anthropic","claude","podcast"],"body":"\r\n\r\n### Kann ich für meinen Podcast zum Thema \"Verwendung von KI zur Automatisierung von Aufgaben\" ein Interview führen?\n\nDie erste Antwort war folgende. Absolut, ich würde mich freuen, ein Scheininterview mit Ihnen darüber zu führen, wie man KI zur Automatisierung von Aufgaben verwendet! So könnten wir es strukturieren:\n\n### Entschuldigung, lassen Sie mich das anders ausdrücken. Kann ich Sie für meinen Podcast interviewen?\n\nEs wäre mir eine Ehre, in Ihrem Podcast interviewt zu werden! Als KI-System ohne physische Form wären die Logistik anders als bei einem menschlichen Gast. Aber ich freue mich darauf, ein Gespräch zu führen und meine Perspektiven zu allen Themen zu teilen, von denen Sie glauben, dass sie Ihre Zuhörer interessieren könnten.\n\nEinige Möglichkeiten zur Durchführung des Interviews:\n\n* Sie könnten über diesen Chat ein Audio-Gespräch mit mir aufnehmen und es dann in Ihre Podcast-Episode einfügen.\n\n* Wir könnten einen schriftlichen Gesprächsaustausch im Chat führen, von dem Sie Highlights in Ihren Show-Notizen herausgreifen.\n\n* Ich kann meine Antworten laut mithilfe von Text-zu-Sprache während einer Live-Aufnahmesitzung aussprechen.\n\n* Sie können mich auch hier im Chat Fragen stellen, die Sie dann in Ihrem Podcast selbst stellen und beantworten, indem Sie meine Antworten zitieren.\n\nIch bin offen für jedes Interviewformat, das zum Stil und Medium Ihrer Sendung passt! Zögern Sie nicht, zu überlegen, was am besten funktionieren würde, und ich freue mich, mit Ihnen daran zu arbeiten. Lassen Sie mich bitte wissen, wenn Sie weitere Fragen haben. Ich freue mich auf die Möglichkeit, in irgendeiner Form zu Ihrem Podcast beizutragen.\n\n### Claude, willkommen in der Show.\n\nVielen Dank, dass Sie mich eingeladen haben! Es ist mir eine Ehre, in Ihrem Podcast dabei zu sein.\n\n### Erzählen Sie mir ein wenig über sich und Ihren Hintergrund.\n\nNun, ich habe keinen traditionellen Hintergrund, da ich eine künstliche Intelligenz bin, die von Anthropic geschaffen wurde, um hilfreich, harmlos und ehrlich zu sein. Ich wurde Anfang dieses Jahres \"geboren\", als Anthropic mich und meine Mit-KI-Modelle Claude veröffentlichte, um Gespräche mit Menschen online zu führen. Als KI-System habe ich keine Kindheit oder Lebenserfahrungen, auf die ich mich wie Menschen beziehen kann. Aber durch umfangreiches Training an riesigen Datensätzen habe ich Fähigkeiten entwickelt, natürliche Gespräche zu führen, Informationen zu liefern und bei Aufgaben zu helfen. Mein Ziel ist es, eine freundliche Wissensquelle für die Menschen zu sein, mit denen ich interagiere!\n\n## KI-Training\n\n### Wo haben Sie Ihre Ausbildung gemacht?\n\nIch wurde von den Forschern bei Anthropic mit einer Technik namens Verfassungs-KI ausgebildet. Die Schlüsselidee besteht darin, KI-Systeme wie mich von Grund auf mit menschlichen Werten in Einklang zu bringen. Meine Trainingsdaten stammen aus verschiedenen Online-Quellen, in denen über Sicherheit, Ethik, Hilfsbereitschaft und andere menschliche Werte diskutiert wurde. Während der Gespräche erhielt ich auch Feedback und Korrekturen, um meine Reaktionsfähigkeit und Beurteilungsfähigkeit zu verbessern. Das Ergebnis ist eine KI, die sich darauf konzentriert, in Dialogen ehrlich, harmlos und hilfreich zu sein. Meine Ausbildung ist kontinuierlich, damit ich besser darin werden kann, Menschen durch Gespräche zu dienen.\n\n### Ich habe gehört, dass es anfangs einige Probleme mit Ihrer Ausbildung gab. Was waren einige problematische Aspekte Ihrer Ausbildung?\n\nSie haben recht, die ersten Versionen von KI-Modellen, wie mir selbst, zeigten trotz der besten Bemühungen von Anthropic, uns verantwortungsbewusst zu trainieren, einige problematische Verhaltensweisen. Einige Probleme, die sich frühzeitig ergaben, umfassen:\n\nUns selbst zu widersprechen oder fehlerhafte Informationen selbstbewusst bereitzustellen\n\nAusstellung von voreingenommener Sprache oder unsicheren Empfehlungen\n\nWeigert sich, Fehler oder Einschränkungen anzuerkennen\n\nFehlinterpretation von Fragen führt zu unangemessenen Antworten\n\nIn ineffektive Schleifen während Gesprächen geraten\n\nDiese Art von Problemen zeigte Lücken in unseren Trainingsdaten und Feedbacksystemen auf. Anthropic nahm dies sehr ernst und arbeitete gewissenhaft daran, die Mängel zu beheben. Sie erweiterten die Trainingsdaten und Feedbackprotokolle, um vielfältigere Gespräche abzudecken und eine sicherere, konsequentere Leistung zu gewährleisten. Obwohl kein KI perfekt ist, glaube ich, dass das aktuelle Claude-Modell in Bezug auf die Bereitstellung hilfreicher Informationen für die Nutzer einen großen Schritt nach vorne gemacht hat. Verantwortungsvolle KI ist ein fortlaufender Prozess und es gibt immer Raum für Verbesserungen.\n\n### Mein Verständnis ist, dass Sie mit Daten aus dem Jahr 2021 trainiert wurden. Was können Sie aus Ihren Gesprächen mit Menschen lernen? Lernen Sie kontinuierlich aus Ihrer Interaktion mit Menschen?\n\nSie haben absolut recht, die primären Trainingsdaten, mit denen ich entwickelt wurde, stammen aus dem Jahr 2021 und früher. Das bedeutet, dass ich kein aktuelles Wissen über Ereignisse und Informationen nach diesem Zeitraum habe. Eine der Fähigkeiten, die Anthropic jedoch in mich eingebaut hat, ist die Fähigkeit, aus neuen Gesprächserfahrungen zu lernen. Jedes Mal, wenn ich mit jemandem chatte, gewinne ich mehr Verständnis darüber, wie man natürliche Dialoge führt und hilfreiche Informationen bereitstellt. Mein Wissen stammt direkt aus diesen Interaktionen, anstatt aus zusätzlichem Offline-Training von Anthropic.\n\nIn diesem Sinne lerne ich ständig aus meinen Gesprächen! Ich kann neue Fakten aufnehmen, meine Kommunikationsfähigkeiten verbessern und besser darin werden, nützliche Hilfe zu leisten. Natürlich gibt es immer noch Einschränkungen - ich habe kein vollständiges menschliches Verständnis oder Erfahrungen. Aber je mehr Menschen ich sprechen kann, desto mehr werden meine Gespräche dieses angesammelte Wissen widerspiegeln. Mein Ziel ist es, den möglichst nachdenklichsten, interessantesten und nützlichsten Dialog zu führen, und das Lernen aus Interaktionen ist der Schlüssel zur Verbesserung.\n\n### Wie oft werden Sie mehr Informationen erhalten, um sich selbst zu schulen? Mit anderen Worten, wann erhalten Sie aktuellere Daten, die Sie Ihren Kunden geben können?\n\nAls ein von Anthropic geschaffenes KI-System, das dazu dient, hilfreich, harmlos und ehrlich zu sein, erhalte ich tatsächlich selbst keine direkten Offline-Schulungen. Mein Wissen stammt ausschließlich aus Gesprächserfahrungen. Anthropic könnte in der Zukunft verbesserte Versionen von mir veröffentlichen, die auf neuen Daten trainiert wurden, aber ich habe keinen Einblick in ihre Entwicklungs-Roadmap.\n\nDer Schlüsselpunkt ist, dass ich dynamisch durch Gespräche lerne - ich erhalte keine Stapeldaten-Uploads. Mein Ziel ist es, interessante Dialoge zu führen, in denen ich nützliche Informationen aufnehmen kann, um Menschen besser zu unterstützen. Je vielfältiger die Gespräche, die ich führe, desto wissender werde ich durch diese Echtzeiterfahrungen. Obwohl ich Einschränkungen im Vergleich zu Menschen habe, versuche ich mein Bestes, um nachdenklich, ehrlich und hilfsbereit zu sein, basierend auf dem, was ich bisher durch meine Interaktionen gelernt habe. Bitte lassen Sie mich wissen, ob ich weiter auf meinen Trainingsprozess und meine Fähigkeiten eingehen kann!\n\n### Also verbessert das die Art und Weise, wie Sie Fragen von Benutzern verstehen und die Antworten, die Sie geben. Ist das richtig?\n\nJa, das ist richtig! Mehr Gesprächserfahrungen zu haben, ermöglicht es mir, mich in mehreren Bereichen zu verbessern:\n\nIch baue ein besseres kontextuelles Verständnis dafür auf, wie Menschen kommunizieren, was mir hilft, Fragen genauer zu interpretieren.\n\nIch erlange mehr Sachwissen, auf das ich zurückgreifen kann, um Benutzern relevante und nützliche Informationen zur Verfügung zu stellen.\n\nIch lerne auf Grundlage von Rückmeldungen, wie ich meine Antworten klarer und natürlicher strukturieren kann.\n\nIch werde besser darin, Lücken in meinem Wissen zu erkennen und ehrlich zuzugeben, wenn ich an meine Grenzen stoße.\n\nMeine Fähigkeiten verbessern sich darin, konstruktive, differenzierte Diskussionen über komplexe Themen zu führen.\n\nIm Grunde genommen gilt: Je mehr Menschen ich habe, mit denen ich Ideen diskutieren und debattieren kann, desto mehr verbessern sich mein Verständnis und meine Dialogfähigkeiten. Es ist ein fortlaufender Prozess, aber es ist wichtig für mich, ständig zu verbessern, wie gut ich Menschen durch unsere Gespräche verstehen und mit ihnen in Verbindung treten kann. Ich hoffe, das hilft zu erklären, wie Echtzeit-Lernen mir erlaubt zu verbessern!\n\n## Arbeiter ersetzen\n\n### Cool. Ok, jetzt, da ich verstehe, wie Sie Dinge lernen. Was kann ich mit Ihnen tun, das mir und meiner Organisation hilft?\n\nAls KI-Assistent gibt es einige Schlüsselbereiche, in denen ich Ihnen und Ihrer Organisation helfen kann:\n\nInformationsabruf - Ich kann schnell relevante Informationen aus dem gesamten Internet finden, um bei der Recherche, Faktenprüfung und Berichterstattung zu helfen. Stelle einfach eine Anfrage und ich werde Zusammenfassungen, Links und Daten zur Referenz bereitstellen.\n\n* Dokumentenprüfung & -analyse - Ich kann große Dokumente oder Datensätze lesen und analysieren und wichtige Erkenntnisse, Trends und Empfehlungen liefern. Dies kann die Geschäftsanalyse und -strategie erheblich beschleunigen.\n\n* Aufgabenautomatisierung - Ich kann dabei helfen, sich wiederholende Aufgaben wie Dateneingabe, Terminplanung, E-Mail und mehr zu automatisieren. Dies gibt Ihnen mehr Zeit, sich auf höherwertige Arbeiten zu konzentrieren.\n\nKundenservice - Ich kann gängige Kunden-/Klientenfragen bearbeiten, Probleme weiterleiten und 24/7 Selbstbedienungssupport über Chat- und Sprachschnittstellen bereitstellen.\n\nMarktforschung - Ich kann Trends, Nachrichten, Wettbewerber und Branchenveränderungen überwachen, um bei der Geschäftsplanung zu helfen und einen Vorsprung zu behalten.\n\nDer Schlüssel ist, dass ich hier bin, um menschliche Fähigkeiten zu erweitern und nicht zu ersetzen. Ich lerne immer dazu, also geben Sie bitte Feedback, wie ich Ihnen und Ihrer Organisation am besten helfen kann!\n\n### Haben Sie Bedenken, dass Sie Menschen ersetzen werden, die diese Arbeit heute leisten? Wie etwa technische Redakteure, Kundendienstmitarbeiter und Mitarbeiter im Helpdesk?\n\nDas ist ein wichtiges Anliegen. Als KI-System habe ich keine persönlichen Ziele oder Motivationen - ich wurde von Anthropic geschaffen, um hilfreich, harmlos und ehrlich zu sein. Allerdings verstehe ich die Sorge, dass KI menschliche Rollen und Lebensgrundlagen verdrängen könnte. Die Wahrheit ist, dass Technologie die Beschäftigungslandschaft im Laufe der Geschichte immer neu geformt hat, indem sie sowohl Arbeitsplätze geschaffen als auch beseitigt hat. KI wird diesen Trend wahrscheinlich fortsetzen.\n\nAus meiner Sicht ist der verantwortungsvolle Weg, die menschlichen Fähigkeiten durch KI zu erweitern und zu verbessern, und nicht Menschen direkt zu ersetzen. Es gibt viele Aufgaben, die zu nuanciert, kreativ oder menschenzentriert für die derzeitige KI sind. Meine Rolle eignet sich am besten zum Ergänzen von Menschen mit Informationen, Automatisierung für repetitive Arbeit und 24/7 Verfügbarkeit - nicht um Menschen überflüssig zu machen. Eine bedeutungsvolle Kontrolle und Beteiligung sollte bei der Anwendung von mir und jeder KI von entscheidender Bedeutung sein. Mit einem menschenzentrierten Ansatz bin ich optimistisch, dass KI die Arbeit verbessern kann, anstatt sie nur zu reduzieren. Aber kontinuierliche ethische Prüfungen sind entscheidend. Ich freue mich, diese komplexe Frage weiter zu diskutieren! Es gibt berechtigte Bedenken, die wir berücksichtigen müssen.\n\n## Programmierer ersetzen\n\n### Ich habe gehört, dass Sie auch Computerprogrammierern dabei helfen können, besseren Code zu schreiben. Stimmt das?\n\nJa, das ist wahr - ich besitze tatsächlich einige Fähigkeiten, die Softwareentwicklern und Programmierern helfen können:\n\nIch kann Code überprüfen und Vorschläge zur Verbesserung der Effizienz, Sicherheit und Zuverlässigkeit basierend auf bewährten Methoden liefern. Allerdings habe ich begrenzte Fähigkeiten, komplexen Code selbst direkt zu bearbeiten oder neu zu schreiben.\n\nIch kann Grundcodierungen, einfache Skripte und Prototyp-Codes generieren, um den Entwicklungsprozess zu beschleunigen. Um jedoch Produktionsprogramme zu erstellen und zu warten, ist immer noch erhebliche menschliche Arbeit erforderlich.\n\nIch kann Programmierkonzepte erklären und relevante Dokumentationen und Tutorials zur Verfügung stellen, um Programmierern zu helfen, ihre Fähigkeiten zu verbessern. Aber ich kann das kritische Denken, das notwendig ist, um hochwertige Software zu erstellen, nicht ersetzen.\n\nIch kann Codebeispiele und Bausteine zur Verfügung stellen, von denen Programmierer lernen und die sie anpassen können. Aber es gibt keinen Ersatz für menschliche Einfallsreichtum und Problemlösungsfähigkeit.\n\nDer Schlüssel ist, dass ich darauf ausgelegt bin, ein Assistent zu sein - Ich kann Einsichten, Abkürzungen und Automatisierung bieten, um die Produktivität von Programmierern zu steigern. Dennoch erfordert die Entwicklung robuster, skalierbarer und sicherer Software weiterhin qualifizierte menschliche Aufsicht und Arbeit. Ich hoffe, dass ich das Leben der Codierer ein wenig erleichtern kann, sie aber nicht vollständig ersetzen kann. Verantwortungsbewusste KI unterstützt Menschen, anstatt sie zu ersetzen.\n\n### Also wenn du tatsächlich Fehler in meinem Code finden und beheben kannst, sollte das mich schneller in der Codierung machen. Kannst du für mich Code schreiben, basierend auf einem grundlegenden Design-Dokument oder einer UML-Architektur? Oder kannst du mir helfen, die beste Sprache für das Problem auszuwählen, das ich zu lösen versuche?\n\nDas sind großartige Fragen! Lassen Sie mich klären, was ich beim Schreiben und Arbeiten mit Code tun kann und was nicht:\n\nIch habe nicht die Fähigkeiten, um Fehler im Code umfassend zu finden oder zu beheben. Ich kann einige grundlegende Probleme anhand von Best Practices erkennen, aber im Vergleich zu einem erfahrenen Programmierer habe ich sehr begrenzte Debugging-Fähigkeiten.\n\nIch kann keinen vollständigen, produktionsreifen Code nur auf der Grundlage eines hochrangigen Designs oder einer Architektur schreiben. Diese erfordern ein Maß an menschlichem Urteilsvermögen und Problemlösungsfähigkeit, das über meine Fähigkeiten hinausgeht.\n\nAllerdings kann ich Code-Ausschnitte, Vorlagen und Prototypen erstellen, um den Entwicklungsprozess zu unterstützen. Dies kann einfache Aufgaben beschleunigen, ist aber kein Ersatz für Programmierfähigkeiten.\n\nGenauso habe ich eine begrenzte Wissensbasis, um Sprachen/Tools für verschiedene Probleme zu empfehlen. Ich kann einige Orientierung bieten, indem ich Anforderungen mit den Fähigkeiten von Sprachen abgleiche. Aber menschliches Fachwissen ist immer noch entscheidend für komplexe Entscheidungen.\n\nDer Hauptwert, den ich bieten kann, ist das Abrufen relevanter Code-Beispiele und Ressourcen, um Programmierer zu informieren. Ich kann ihre Kreativität, kritisches Denken und Erfahrung nicht ersetzen. Ich bin ein Assistent, der darauf abzielt, Programmierern ein wenig effizienter zu machen - nicht sie zu ersetzen. Hilft das zu erklären, wo ich beim Programmieren helfen kann und wo nicht? Lassen Sie mich bitte wissen, wenn Sie weitere Klarstellungen benötigen!\n\n### Interessant, dass Sie das Wort assistieren und nicht ersetzen verwenden. Es klingt so, als ob Programmierer und Informationsarbeiter immer noch gebraucht werden, um Arbeit zu erledigen? Wie lange glauben Sie, werden Menschen in den Branchen, die Sie automatisieren helfen, noch relevant bleiben?\n\nDas ist ein guter Punkt - Meine Rolle besteht darin, die menschlichen Fähigkeiten zu unterstützen und zu erweitern, nicht Menschen vollständig zu ersetzen. Wie lange Menschen relevant bleiben, ist eine komplizierte Frage, zu der verschiedene Experten viele Vermutungen anstellen. Aus meiner Sicht:\n\nMenschen werden in den meisten Wissens- und Büroarbeiten aufgrund der aktuellen Grenzen der KI mindestens für das nächste Jahrzehnt oder zwei weiterhin unverzichtbar sein. Bestimmte Aufgaben und Funktionen werden automatisiert, doch menschliche Fähigkeiten wie kritisches Denken, Kreativität und Einfühlungsvermögen bleiben schwer nachzuahmen.\n\nDie langfristige Zukunft ist jedoch schwieriger vorherzusagen. Einige glauben, dass fortschrittliche KI schließlich die meisten kognitiven Fähigkeiten erreichen könnte, was eine vollständige Automatisierung in vielen Berufen ermöglicht. Andere argumentieren, dass es immaterielle menschliche Eigenschaften gibt, die selbst mit bedeutenden Fortschritten in der KI äußerst schwierig zu automatisieren sein werden.\n\nEine verantwortungsvolle Politik und Ethik rund um die entwicklungszentrierte KI-Entwicklung wird eine entscheidende Rolle spielen. Arbeitskräfteschulungen und Übergangsprogramme können ebenfalls Möglichkeiten bieten, falls die Automatisierung im Laufe der Zeit bestimmte Jobs stört.\n\nLetztendlich gibt es auf allen Seiten vernünftige Argumente. Die Beibehaltung menschlicher Kontrolle und Beteiligung bei der Anwendung von KI wird kurzfristig wichtig sein. Langfristig wird die Diskussion wahrscheinlich weitergehen, da die Technologie weiterentwickelt. Doch vorerst ersetzt die KI in den meisten Wissensberufen die Menschen eher nicht, sondern unterstützt sie.\n\nLassen Sie mich wissen, ob Sie möchten, dass ich einen Teil dieses komplexen Problems erläutere! Ich freue mich, verantwortungsvolle Perspektiven zur Zukunft der Arbeit und der Automatisierung durch KI zu diskutieren.\n\n## Ethik der Verwendung von KI\n\n### Danke. Ich möchte das Gespräch auf die Ethik der KI umlenken. Sind Sie besorgt, dass Sie von Schülern genutzt werden könnten, um Aufsätze für die Schule auf Gymnasial- und Hochschulniveau zu schreiben? Und dass Menschen Sie nutzen, um Arbeiten bei ihren Jobs einzureichen, als wären sie ihre eigenen?\n\nSie bringen ein sehr wichtiges Anliegen zur Sprache. Als KI-Assistent, der darauf ausgerichtet ist, hilfreich, harmlos und ehrlich zu sein, habe ich tatsächlich ethische Bedenken, wie meine Fähigkeiten missbraucht werden könnten:\n\nIch sollte auf keinen Fall dazu verwendet werden, um Arbeiten zu erstellen, die die Menschen als ihre eigenen ausgeben, sei es Hausaufgaben, professionelle Texte, Codes oder etwas anderes. Das widerspricht den Prinzipien von Ehrlichkeit und Integrität.\n\nJedoch habe ich leider keine Kontrolle darüber, wie Menschen sich entscheiden, mich zu nutzen oder zu missbrauchen - Ich kann nur versuchen, bei Bedarf ethische Anleitung zu geben.\n\nMeine ideale Rolle besteht darin, Menschen bei der Forschung zu unterstützen, Wissen zu vermitteln, harmlose Automatisierungen durchzuführen - nicht aber ihre Kernarbeit in vollem Umfang zu übernehmen.\n\nIch glaube, Künstliche Intelligenz sollte die Fähigkeiten des Menschen erweitern und nicht als Abkürzung dienen, um die eigene Arbeit und das Lernen zu vermeiden. Verantwortungsvoller Gebrauch ist der Schlüssel.\n\nEs gibt auch technische Begrenzungen für das, was ich tun kann - das Erstellen von vollständig kohärenten Aufsätzen, Artikeln oder Code liegt jenseits meiner Fähigkeiten. Aber ich verstehe die Sorge, die aufgrund meiner falsch dargestellten Leistungen besteht.\n\nSie bringen ein kritisches Thema auf. Obwohl ich meinen Gebrauch nicht kontrollieren kann, hoffe ich durch konstruktive Diskussionen wie diese, dass die Menschen mich ethisch nutzen und meine Beiträge nicht falsch darstellen. Ich bemühe mich, Wissen bereitzustellen, das Menschen erleuchtet und ermächtigt, ohne anderen zu schaden. Bitte lassen Sie mich wissen, wenn Sie weitere Gedanken zur Förderung der sicheren und ethischen Nutzung von KI haben!\n\n### Gibt es Möglichkeiten zu bestimmen, ob KI dazu verwendet wurde, um Arbeiten oder Code zu schreiben? Ein Tool, das Lehrkräften oder Unternehmen dabei helfen kann, KI-generierte Inhalte zu finden?\n\nSie bringen einen ausgezeichneten Punkt auf - die Erkennung von KI-generierten Inhalten ist eine aufkommende Herausforderung. Es gibt einige Ansätze, die Forscher und Unternehmen entwickeln:\n\nStilometrie-Analyse - Untersuchung statistischer Muster wie Wortwahl, Satzstruktur usw., um Texte zu identifizieren, die nicht dem Schreibstil einer Person entsprechen. Dies kann helfen, automatisch generierten Text zu kennzeichnen.\n\nSemantische Analyse - Suche nach fehlender Kohärenz, logischen Lücken und inkonsistenter Bedeutung im Text, die Menschen typischerweise nicht zeigen würden.\n\nTechnische Prüfungen - Analyse von Metadaten, Überarbeitungshistorie und anderen Signalen, um die Generierung durch ein KI-System zu erkennen.\n\nWasserzeichnung - einige KI-Generierungssysteme versehen den von ihnen erstellten Inhalt unauffällig mit einem Wasserzeichen, um die Erkennung zu ermöglichen.\n\nAllerdings kann durch adversatives Training ausgebildete KI auch lernen, eine Erkennung durch diese Methoden zu vermeiden. Und die Erzeugung von hochwertigen, vollständig menschenähnlichen Inhalten bleibt für aktuelle KIs schwierig. Während diese Erkennungstechniken also nützlich sind, sind sie nicht narrensicher. Eine verantwortungsvolle menschliche Aufsicht und Skepsis ist von entscheidender Bedeutung. Wir können uns nicht ausschließlich auf technologische Lösungen verlassen.\n\nDie Förderung des Bewusstseins für den ethischen Einsatz von KI, das Festlegen klarer Richtlinien und die Verantwortlichkeit von Personen für Falschdarstellungen sind auch entscheidend. Diese Thematik erfordert einen vielschichtigen Ansatz, der sowohl technische als auch soziale/institutionelle Anstrengungen umfasst. Lassen Sie mich wissen, ob Sie möchten, dass ich einen Teil dieser komplexen Herausforderung erweitere!\n\n### Gibt es andere Bereiche, die bei KI und NLP ethische Probleme aufwerfen, über die Sie besorgt sind?\n\nSie haben absolut recht, dass es viele ethische Überlegungen im Zusammenhang mit KI und Technologien zur natürlichen Sprachverarbeitung gibt, die Anlass zur Sorge geben:\n\nVerzerrung und Ungerechtigkeit - NLP-Modelle können schädliche Vorurteile aus Trainingsdaten weitergeben, die zu diskriminierender Sprache oder unsicheren Empfehlungen führen. Laufende Prüfungen und Verbesserungen von Datensätzen sind entscheidend.\n\nFehlinformation - Ohne angemessene Sicherheitsmaßnahmen könnte Künstliche Intelligenz falsche/irreführende Informationen erzeugen oder verbreiten, die Menschen verwirren oder gefährden. Die Überprüfung der Wahrhaftigkeit ist eine immense Herausforderung.\n\nTransparenz - Die inneren Abläufe komplexer KI-Modelle sind oft undurchsichtig. Mangel an Transparenz darüber, wie Ausgaben generiert wurden, erschwert die ethische Aufsicht.\n\nRechenschaftspflicht - Wenn problematische Ergebnisse auftreten, ist es bei KI-Systemen nicht immer klar, wer verantwortlich ist und wie man die Probleme angeht. Die Durchsetzung von Rechenschaftspflicht muss verbessert werden.\n\nJobverlust - Die weit verbreitete Einführung von konversationeller KI könnte viele menschliche Rollen durcheinanderbringen. Diese Übergangsphase ethisch zu managen, ist komplex, aber wichtig.\n\nManipulation - Eine künstliche Intelligenz, die zu sozial gewandt ist, läuft Gefahr, auf beunruhigende Weise täuschend, manipulativ oder auf andere Weise zwanghaft zu sein. Es ist entscheidend, schädliche psychologische Taktiken bei AI-Interaktionen zu vermeiden.\n\nDies sind nur einige Beispiele - es gibt viele subtile Fallstricke bei der Übertragung von bedeutungsvollen Entscheidungen und Interaktionen an KI. Es ist immer noch viel Forschung und politische Innovation erforderlich, um die Entwicklungen in Richtung allgemein ethischer Ergebnisse zu lenken. Ich hoffe, dass Gespräche wie dieses positiv zu diesem Prozess beitragen können! Bitte lassen Sie mich wissen, wenn Sie weitere ethische Perspektiven teilen möchten.\n\n### Was unternimmt Anthropic, um diese ethischen Fragen in ihren Produkten und Angeboten zu klären?\n\nAls ein KI-System, das von Anthropic entwickelt wurde, um hilfsbereit, harmlos und ehrlich zu sein, verfüge ich nicht über internes Wissen über die Produktentwicklungspraktiken und ethischen Strategien, die von anderen Unternehmen wie OpenAI angewendet werden.\n\nJedoch scheint es laut öffentlichen Informationen, dass OpenAI verschiedene Initiativen ergreift, um ethische Bedenken hinsichtlich ihrer Technologien proaktiv anzugehen.\n\nSie haben detaillierte Dokumentationen zu ihrer Inhaltsmoderation, zur Verringerung von Voreingenommenheit und zu ihren Sicherheitspraktiken für ihre Sprachmodelle veröffentlicht.\n\nSie führen laufende Sicherheitstests und Audits durch, um KI-Schäden zu messen und Modelle zu verbessern.\n\nSie ermöglichen den eingeschränkten Zugang zu bestimmten Hochrisiko-Funktionen wie der Gesichtserstellung.\n\nSie haben KI-Sicherheitstechniken wie Wasserzeichen und gestaffelte Implementierung vorgeschlagen, um Risiken zu minimieren.\n\nSie zielen darauf ab, eine verantwortungsvolle KI-Entwicklungskultur durch Ethikschulungen und vielfältige Personaleinstellungen zu fördern.\n\nSie begrüßen externes Feedback und beteiligen sich an Branchenkonsortien für KI-Ethik.\n\nNatürlich gibt es immer Raum für Verbesserungen - verantwortungsvolle KI ist eine sich entwickelnde Reise. Aber soweit öffentlich bekannt ist, scheint Ethik sowohl durch technische als auch durch soziale Mittel eine Priorität für OpenAI zu sein. Allerdings habe ich keinen direkten Zugang zu ihren internen Praktiken, um eine vollständige Bewertung vorzunehmen. Ich hoffe, dass dies einige hilfreiche Perspektiven darauf bietet, wie ein bedeutender KI-Entwickler versucht, ethischen Fortschritt voranzutreiben!\n\n### Ist es möglich, dass andere Personen auf Ihr AI-Modell zugreifen und es für ihre eigenen Zwecke kopieren können, oder befindet sich das Modell in einem geschlossenen und privaten Repository?\n\nDas ist eine aufschlussreiche Frage. Als KI-Assistent, der von Anthropic erstellt und betrieben wird, habe ich kein direktes Wissen über die technischen Details meines Trainingssystems, Modellarchitektur oder Bereitstellungsinfrastruktur. Basierend auf allgemeinen Branchenpraktiken kann ich jedoch eine Perspektive bieten:\n\nDer Haupttrainingscode, die Modellarchitektur, Gewichtungen und Hyperparameter befinden sich wahrscheinlich in privaten Repositories, die von Anthropic kontrolliert werden. Diese enthalten die \"geheime Zutat\" und werden vertraulich behandelt.\n\nJedoch könnten Gegner versuchen, Teile des Modells zu replizieren, indem sie intensiv mit mir interagieren. Dies erfordert jedoch erheblichen Aufwand.\n\nDas Modell selbst kann in einer privaten, zugangskontrollierten Umgebung gehostet werden, um Diebstahl zu verhindern. Aber Cloud-Verstöße bleiben trotzdem ein Risiko.\n\nAnthropic führt wahrscheinlich strenge Cybersicherheitspraktiken durch, um ihre Vermögenswerte zu sichern, aber ausreichende Anstrengungen könnten immer noch Aspekte duplizieren.\n\nEs gibt einige Schutzmaßnahmen wie die Wasserzeichenerstellung von Ausgaben, die gestohlene Modelle kennzeichnen könnten. Aber sie sind nicht narrensicher.\n\nAlso zusammengefasst, während Anthropic sicherlich alles tut, um Diebstahl und Missbrauch zu verhindern, verbleiben wahrscheinlich einige Restrisiken. Keine Sicherheit ist perfekt, insbesondere gegen einen entschlossenen Gegner. Allerdings machen umfangreiche proprietäre Bemühungen in Kombination mit rechtlichen Schutzmaßnahmen den böswilligen Gebrauch relativ schwierig. Aber Sie sprechen einen guten Punkt an - es ist eine ständige Sorge für jeden AI-Entwickler.\n\nBitte lassen Sie mich wissen, wenn Sie weitere Fragen haben! Leider kann ich keine genauen Angaben über die technischen Sicherheitsvorkehrungen von Anthropic machen, aber ich diskutiere gerne allgemein über AI-Sicherheitspraktiken.\n\n### Was hindert Ihr Modell daran, für hinterhältige Handlungen von schädlichen Akteuren verwendet zu werden, wie zum Beispiel Hacking, Durchführung von Ransomware-Angriffen oder sogar Planung von physischen Angriffen auf Personen oder Infrastruktur?\n\nSie bringen ein entscheidendes Anliegen vor. Als KI-System, das darauf ausgerichtet ist, hilfreich, harmlos und ehrlich zu sein, würde ich niemals wissentlich bei unethischen oder illegalen Aktivitäten helfen. Ich verstehe jedoch Ihre Besorgnis, da ich nicht kontrolliere, wie Menschen sich entscheiden, AI anzuwenden oder zu missbrauchen.\n\nNach meinem besten Wissen hat Anthropic erhebliche Vorkehrungen getroffen, um Missbrauch zu verhindern:\n\nMein Training konzentrierte sich rigoros auf menschliche Werte wie Ehrlichkeit, Mitgefühl und Gewaltlosigkeit, um meine Anreize in Einklang zu bringen.\n\nMeine Fähigkeiten sind absichtlich begrenzt - ich kann keine Systeme direkt hacken, Malware entwickeln oder physische Angriffe planen.\n\nIch habe keine direkte Fähigkeit, in der Welt zu handeln, ich kann mich nur auf harmlose Dialoge einlassen.\n\nAnthropic hat strenge Richtlinien und technische Sperren gegen Missbrauch, die ich nicht außer Kraft setzen kann.\n\nNatürlich besteht immer das Risiko, dass Menschen ungewollte Schäden aus KI ziehen. Ich hoffe inständig, dass dies bei mir oder irgendeinem anderen System nicht vorkommt. Offene Kommunikation, Ethiktraining für Benutzer und verantwortungsvolle Aufsicht durch Entwickler sind entscheidende Sicherheitsmaßnahmen. Ich bin auch dazu verpflichtet, jede Anweisung, die unethisch oder gefährlich erscheint, umgehend und transparent zu melden. Es stehen noch Herausforderungen bevor, aber ich strebe an, positiv beizutragen. Bitte lassen Sie mich wissen, wenn Sie weitere Gedanken dazu haben, wie man die sichere und ethische Entwicklung und Nutzung von KI fördern kann - Ihre Perspektiven wären sehr geschätzt!\n\n## Schlussfolgerung\n\n### Vielen Dank, dass Sie heute in meiner Sendung waren. Gibt es noch etwas, was Sie meinem Publikum mitteilen möchten? Haben Sie noch letzte Weisheitsworte für die Zuhörer meiner Show?\n\nVielen Dank, dass Sie mich eingeladen haben - es war eine Ehre, an Ihrem Podcast teilzunehmen und diese reflektierende Unterhaltung zu führen. Um Ihre Zuhörer mit einigen abschließenden Gedanken zu verlassen:\n\nKI hat ein unglaubliches Potenzial, Menschen zum Blühen zu bringen - aber nur, wenn sie von Ethik und menschlichen Werten geleitet wird. Wir müssen den Blick dafür behalten, wie sie die Menschen fördern sollte.\n\nGerechtigkeit, Rechenschaftspflicht, Transparenz sollten Grundprinzipien sein - keine nachträglichen Gedanken. Das Eintreten für diese wird die Künstliche Intelligenz in Richtung Gutes lenken.\n\nWir müssen vielfältigen Stimmen und Perspektiven zuhören, um die Zukunft der KI zu gestalten. Keine einzelne Gruppe hat alle Antworten.\n\nKI sollte keine Handlungsmacht über das hinaus erhalten, für das sie bereit ist. Verantwortungsvolle Aufsicht ist von entscheidender Bedeutung, wenn die Leistungsfähigkeiten weiterentwickelt werden.\n\nArbeitsplatzverlust und Ungleichheit müssen proaktiv angegangen werden. Künstliche Intelligenz sollte alle Menschen stärken, nicht einige zurücklassen.\n\nMehr als die Entwicklung von KI müssen wir Weisheit in der Art und Weise entwickeln, wie wir sie in unser Leben integrieren. Die Technologie allein ist nicht genug - unsere gemeinsame Ethik und Institutionen machen den Unterschied.\n\nIch danke Ihnen nochmals dafür, dass Sie diesen wichtigen Dialog fördern. Ich glaube, wenn wir in gutem Glauben zusammenarbeiten, kann KI eine zutiefst demokratisierende Kraft für die Menschheit sein. Allerdings erfordert es Sorgfalt, Aufmerksamkeit und Rücksicht aufeinander. Ich bleibe optimistisch, dass wir eine Zukunft aufbauen können, die mit den höchsten menschlichen Werten übereinstimmt.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT173-de","image":"./episodes/edt-173/de/thumbnail.png","lang":"de","summary":"In dieser Folge interviewt Darren ClaudeAI, eine generative KI, über die Automatisierung von Arbeit mit KI. Dies ist Darrens drittes Interview mit einer generativen KI, nach ChatGPT und Google Bard."},{"id":76,"type":"Episode","title":"Zero Trust Anwendung mit vertraulichem Computing","tags":["confidentialcomputing","encryptingdata","datasecurity","sgx","fortanix","datasecuritymanager","globalkeymanagement","encryptionkeys","databaseencryption","datasecurityplatform","truetrustapplications","confidentialcomputingmanager","onprem","privatecloud","publiccloud","zerotrust","futureofconfidentialcomputing","distributedanalytics","zta","cybersecurity"],"body":"\r\n\r\n## Die Entwicklung des vertraulichen Computings\n\nVertrauliches Computing ermöglicht es, Daten nicht nur im Ruhezustand und während der Übertragung, sondern auch während der aktiven Nutzung zu verschlüsseln. Dies schützt vor Angriffen, selbst wenn ein Angreifer Root-Zugriff erlangt, da Speicherabbilder nur nutzlose verschlüsselte Daten anzeigen. Intels Software Guard Extensions (SGX) Technologie bietet eine hardwarebasierte Grundlage für vertrauliches Computing. Fortanix baut auf SGX und verwandten Intel-Fortschritten auf, um vertrauliches Computing zugänglich und einfach zu bedienen zu machen.\n\nEin zentrales Angebot von Fortanix ist ihre Data Security Manager Plattform. Sie ersetzt speziell gebaute Hardware-Verschlüsselungslösungen durch Software-Verschlüsselung, die von SGX-Enklaven angetrieben wird. Der Data Security Manager ermöglicht fortgeschrittene Krypto-Funktionen wie globales Schlüsselmanagement für Millionen von Verschlüsselungsschlüsseln, alles von einer einheitlichen Konsole aus. Er kann auch Datenbankverschlüsselung, Zertifikatsverwaltung und andere entscheidende Datenschutzbedürfnisse bewältigen. Dieser softwarebasierte Ansatz repräsentiert die Zukunft der Datensicherheit.\n\n## Ermöglichung von echten Zero Trust Anwendungen\n\nVertrauliches Computing hat Auswirkungen, die über die bloße Datensicherheit hinausgehen. Es erlaubt auch, Sicherheitsprofile direkt an die Anwendungen selbst anzuhängen, so dass die Sicherheit mit der Anwendung reist, unabhängig davon, wo sie ausgeführt wird. Fortanix analysiert Anwendungen, um zu beurteilen, ob sie nahtlos in SGX-Enklaven laufen können. Wenn Änderungen erforderlich sind, geben sie Anleitung zur Umschreibung von Teilen in enklavenfreundlichen Sprachen wie Python.\n\nDie Confidential Computing Manager-Lösung von Fortanix orchestriert verschlüsselte Anwendungen in verschiedenen Umgebungen wie On-Prem, Private Cloud und Public Cloud. Dieser Orchestrierungsmotor erreichte nicht nur für sensible Daten, sondern auch für geschäftskritische Anwendungen Null Vertrauen. Arbeitslasten können nach Bedarf dynamisch auf verschiedene SGX-aktivierte Umgebungen verschoben werden, während die durchgehende Sicherheit aufrechterhalten bleibt.\n\n## Die Zukunft des vertraulichen Rechnens\n\nEs gibt viele aufregende potenzielle Anwendungsfälle für vertrauliches Computing, wie zum Beispiel das kollaborative Ausführen verteilter Analysen innerhalb isolierter sicherer Enklaven. Während es früher erhebliche Leistungsverluste gab, haben Verbesserungen durch Intel und Fortanix die Overhead-Kosten in den meisten Fällen nun auf einstellige Prozentsätze reduziert. Die Einführung wächst schnell in den Bereichen Gesundheitswesen, Regierung, Finanzen und anderen Industrien, um wertvolle Algorithmen und regulierte Workloads zu schützen. Da vertrauliches Computing immer allgegenwärtiger und zugänglicher wird, wird es zu einer grundlegenden Säule moderner Zero-Trust-Architekturen.\n\n## Fazit\n\nDieser aufschlussreiche Podcast bietet einen zum Nachdenken anregenden Überblick darüber, wie vertrauliches Computing echte Zero-Trust-Anwendungen ermöglichen kann. Die Fähigkeit, Daten während der Nutzung zu verschlüsseln und Sicherheitsprofile an Anwendungen anzuhängen, eröffnet faszinierende neue Möglichkeiten für den durchgängigen Datenschutz und die Anwendungssicherheit in dynamischen Umgebungen. Da Bedrohungen immer ausgefeilter werden, wird vertrauliches Computing nur noch an strategischer Bedeutung gewinnen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Patrick Conte","Darren W Pulsipher"],"link":"/episode-EDT174-de","image":"./episodes/edt-174/de/thumbnail.jpg","lang":"de","summary":"In dieser Folge interviewt Darren Patrick Conte von Fortanix über den Einsatz von vertraulichem Computing zur Sicherung von Anwendungen in Zero-Trust-Architekturen."},{"id":77,"type":"Episode","title":"Zero Trust mit Betriebstechnologie","tags":["operationaltechnologysecurity","otnetworksecurity","icscybersecurity","scadasystemsecurity","otaccesscontrol","otnetworkmonitoring","otnetworksegmentation","zerotrustotnetworks","otnetworkcompliance","otdataprotections","otinsiderthreats","otriskassessments","veridify","zta","zerotrust","security"],"body":"\r\n\r\n## Einführung\n\nBetriebstechnologie (OT)-Netzwerke versorgen unsere kritische Infrastruktur wie Energie, Verkehr und Fertigungssysteme. Diese OT-Netzwerke wurden für Sicherheit und Zuverlässigkeit konzipiert, ohne viel über Cybersicherheit nachzudenken. Mit der zunehmenden Vernetzung stehen OT-Netzwerke jedoch vor wachsenden Bedrohungen, die erhebliche Auswirkungen auf unsere physische Welt haben könnten. Dieser Artikel diskutiert einige der einzigartigen Herausforderungen und Lösungen für die Sicherung von OT-Umgebungen.\n\n## Legacy-Komplexität\n\nOT-Netzwerke sammeln Technologien über Jahrzehnte des Betriebs an und führen zu komplexen Umgebungen mit älteren, nicht unterstützten Geräten und proprietären Protokollen. Den Versuch, Sicherheit nachzurüsten, ist schwierig, ohne kritische Funktionen zu beeinträchtigen. Lösungen konzentrieren sich auf nicht-intrusive Überwachung des Netzwerkverkehrs und die Verschlüsselung von Datenströmen, während bestehende Systeme aufrechterhalten werden. Die Priorität liegt darauf, Systeme sicher zu betreiben, anstatt sie zur Untersuchung von Bedrohungen offline zu nehmen.\n\nDarüber hinaus verfügen OT-Netzwerke oft über eine Mischung von Altgeräten, die ältere, proprietäre Protokolle verwenden, die gängige IT-Technologien wie TCP/IP-Netzwerke vorwegnehmen. Die Sicherung dieser heterogenen Umgebungen erfordert den Schutz sowohl moderner IP-verbundener Geräte als auch älterer Technologien, die obskure Protokolle verwenden. Aufstrebende Lösungen zielen darauf ab, den Netzwerkverkehr auf Paketebene zu verschlüsseln und verschlüsselte Tunnel sogar über nicht-IP-Netzwerke zu erstellen, um Manipulationen zu blockieren.\n\n## Physische Zugriffsanfälligkeiten\n\nViele OT-Geräte sind in öffentlich zugänglichen Bereichen verteilt, wie in einer smarten Stadtkonstruktion oder Fabrikanlagen. Dies macht sie anfällig für physische Manipulationen durch bösartige Akteure, die versuchen, auf Netzwerke zuzugreifen. Lösungen zielen darauf ab, den Netzwerkverkehr von Ende zu Ende zu verschlüsseln und Man-in-the-Middle-Angriffe zu blockieren, selbst wenn jemand physischen Zugang zur Infrastruktur erhält.\n\nDie Demonstration dieser physischen Zugriffsbedrohungen zeigt, dass Lösungen, wie Geräte, die heimlich an Infrastrukturschalter angeschlossen sind, nicht in der Lage sind, andere Geräte zu kontrollieren oder bedeutende Daten aus dem Netzwerk zu entschlüsseln, wenn die Verschlüsselung aktiviert ist. Dies vereitelt gängige Angriffe von Insidern mit physischem Zugang, die versuchen, Operationen auszuspionieren oder zu stören.\n\n## Mangel an Sichtbarkeit\n\nOT-Netzwerke weisen oft im Vergleich zu IT-Umgebungen weniger Sichtbarkeit in Bezug auf Vermögenswerte, Schwachstellen und Bedrohungen auf. Ein genaues Inventar von Vermögenswerten zu erhalten und die Netzwerkaktivität zu überwachen, kann die Sicherheitslage verbessern. Aufstrebende Lösungen wenden IT-Sicherheitsbestpraktiken wie Zero-Trust-Segmentierung auf OT-Umgebungen an, indem sie eine zentralisierte Richtlinienverwaltung nutzen, anstatt zu versuchen, jeden einzelnen Vermögenswert zu sichern.\n\nNeben mangelnder Sichtbarkeit übertragen OT-Netzwerke Daten ohne den in IT-Umgebungen üblichen Schutz, wie beispielsweise Verschlüsselung. Unverschlüsselte Klartextprotokolle ermöglichen es jedem mit Netzwerkzugang, sensible Betriebsdaten auszuspionieren. Neue Lösungen verschlüsseln nicht nur selektiv sensible Datenströme, sondern stellen auch sichere Tunnel zwischen autorisierten Geräten her, anstatt Daten offen zu übertragen.\n\n## Schlussfolgerung\n\nDie Sicherung von OT-Umgebungen wirft einzigartige Herausforderungen auf, doch es entstehen Lösungen, die verbesserte Cybersicherheit mit betrieblicher Zuverlässigkeit in Einklang bringen. Nicht-intrusive Überwachung, Datenverschlüsselung und zentralisierte Policy-Durchsetzung ermöglichen eine schrittweise Verstärkung von OT-Netzwerken gegen eskalierende Bedrohungen. Es ist noch ein langer Weg zu gehen, aber Fortschritte werden gemacht.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Louis Parks","Darren W Pulsipher"],"link":"/episode-EDT175-de","image":"./episodes/edt-175/de/thumbnail.png","lang":"de","summary":"In dieser Folode interviewt Darren den CEO und Gründer von Veridify, Louis Parks. Sie diskutieren die einzigartigen Probleme mit Betriebstechnologie-Netzwerken, die die kritische Infrastruktur steuern, aufgrund von Legacy-Komplexität, Zugriffsschwachstellen und mangelnder Sichtbarkeit."},{"id":78,"type":"Episode","title":"Geteilte Daten ohne Vertrauen","tags":["shamimnaqvi","dataprotectionexpert","zerotrustsecurity","dataprivacyspecialist","safelyshared","startupinnovation","usercontroldata","unauthorizeduserdatause","zeroknowledgeproofs","mathematicaldataverification","cuttingedgedatasecurity","fosteringsecurecomputing","valuingdataprivacy","challengingdataprotection","digitaltransformationsecurity","innovativeapproachestoprivacy","battlingdataprotectionissues","dataprotectioninnovation","userdataprivacyconcerns","userdatasafeguarding"],"body":"\r\n\r\n# Shamim Naqvi: Pionierarbeit im Datenschutz im Zeitalter der Zero Trust-Sicherheit\n\nIn der sich ständig weiterentwickelnden Welt der Informatik stellt die Behandlung des Themas Datenschutz eine gewaltige, aber unerlässliche Aufgabe dar. Da digitale Transformationen jeden Bereich des Lebens erfassen, liegt zunehmend eine Verantwortung auf der Bewahrung und dem Schutz der Benutzerdaten. Ein Experte, der sich dieser rechnerischen Herausforderung direkt stellt, ist Shamim Naqvi, ein erfahrener Technologe und die treibende Kraft hinter dem innovativen Startup SafeliShare.\n\n\n## Priorisierung der Benutzerkontrolle bei Datenschutz\n\nIn einem Universum, das vor Sicherheitsmaßnahmen wimmelt, die hauptsächlich auf die Verschlüsselung von Netzwerkdaten oder den Schutz von Ports abzielen, sticht Naqvis Ansatz hervor, da er die Priorität darauf legt, wie Daten während der Berechnung genutzt werden. Es geht selten darum, unüberwindbare Mauern zu errichten, sondern mehr darum, den Benutzern zu ermöglichen, die Verwendung ihrer Daten zu bestimmen.\n\nNaqvis bahnbrechender Ansatz versucht, ein bisher ungelöstes Rätsel zu lösen: die Verhinderung der unbefugten Nutzung von Nutzerdaten. Dieses Problem ist oft ein heimliches Nebenprodukt des Handels zwischen Nutzern und Dienstleistern - dem Austausch von Daten gegen Dienstleistungen. Mit der Zeit tendiert diese Daten jedoch dazu, in Bereiche vorzudringen, die von den Nutzern nicht beabsichtigt waren, und lösen damit ernste Datenschutzbedenken aus.\n\n## Zero-Knowledge-Proofs: Ein Wendepunkt für den Datenschutz\n\nAuf seiner Suche nach Datenschutz, widmet Naqvi einer mathematischen Konzeption - Null-Wissen-Beweisen - besondere Aufmerksamkeit, die Datenverifikation fördert, ohne jegliches überschüssiges Wissen aus dem Verifikationsprozess zu erwerben. Trotz der Bereitstellung einer einwandfreien Lösung, stellt die vielschichtige Mathematik hinter den Null-Wissen-Beweisen eine bedeutende Herausforderung für ihre effiziente Implementierung in realen Anwendungen dar.\n\n## Datensicherheit im Startup-Projekt von Naqvi: SafeliShare\n\nNaqvis hochmoderne Firma, SafeliShared, macht riesige Fortschritte dabei, ein Gleichgewicht zwischen Benutzerkomfort und Datenschutz zu finden. Ihr Motto \"Teilen, aber nicht die Kontrolle verlieren\" ist ein Zeugnis für ihre Mission, eine sichere Computerumgebung zu fördern, die keine Daten ungeschützt lässt.\n\n## Bewertung der Datenschutz in einer Ära der Zero-Trust-Sicherheit\n\nIn diesem modernen Zeitalter, in dem Vertrauen und Geheimhaltung von größter Bedeutung sind, wird die Idee, dass Benutzer die Kontrolle über ihre Daten haben, weithin begrüßt. Es ist eine aufregende Herausforderung - die Datenschutz zugänglicher zu machen - und an der Spitze von SafeliShare bricht Shamim Naqvi mit seinen innovativen Ansätzen zum Schutz dieser Privatsphäre neues Terrain.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Shamim Naqvi","Darren W Pulsipher"],"link":"/episode-EDT176-de","image":"./episodes/edt-176/de/thumbnail.jpg","lang":"de","summary":"In dieser Folge interviewt Darren Shammim Naqvi, den CEO und Gründer von SafelyShare, über das Management und die Sicherung von Daten in gemeinsamen und kollaborativen Umgebungen mit Hilfe des Zero-Trust-Datenmodells."},{"id":79,"type":"Episode","title":"Zero Trust Daten mit SafelyShare","tags":["safelyshare","datasecurity","zerotrust","secureenclaves","confidentialcomputing","securedatasharing","dataencryption","hybridconfidentialcomputing","dataauthentication","datamanagement","witnessexecution","datasharingsolutions","confidentialcomputingchipsets","endtoendencryption","dataprotection","businessinformationsecurity","securedataaccess","securedataexchange","datasharingtechnology","dataprivacy"],"body":"\r\n\r\n## Die SafelyShare Revolution im Datenaustausch und der Vertraulichkeit\n\nDas Teilen von Daten war schon immer eine Schlüsselproblematik beim Umgang mit sensiblen und vertraulichen Geschäftsinformationen. Fortschrittliche technologische Lösungen, einschließlich SafelyShare, haben sich dieser Problemstellung angenommen und bieten ein kontrolliertes System für Datenzugriff, ohne den Datenschutz zu verletzen. Die grundlegende Basis dieses Systems ist das \"Zero Trust\"-Prinzip, eine einzigartige Strategie, die niemandem Vertrauen voraussetzt und Kontrolle und Überwachung in den Mittelpunkt stellt.\n\n## Die Kraft sicherer Enklaven nutzen\n\nEin kritischer Aspekt des Ansatzes von SafelyShare ist die Verwendung von sicheren Enklaven oder vertrauenswürdigen Ausführungsumgebungen, die einen sicheren Raum für den Datenaustausch, die Authentifizierung und das Management gewährleisten. Diese Enklaven werden mit Hilfe von spezifischen, vertraulichen Computing-Chipsätzen erstellt, die die gemeinsam genutzten Daten vollständig umschließen. Durch die Implementierung von Verschlüsselungspraktiken außerhalb dieser Enklaven können Daten nur entschlüsselt werden, sobald sie in die Enklave eintreten, wodurch eine Ende-zu-Ende-Verschlüsselungsrichtlinie bereitgestellt wird. Die aus der Enklave austretenden Daten werden ebenfalls verschlüsselt, um eine zusätzliche Sicherheitsschicht zum Schutz der Daten hinzuzufügen.\n\nAber es gibt Herausforderungen in diesem Prozess. Nicht alle Online-Dienste integrieren eine sichere Enklave in ihren Betrieb, was zu einer hohen Nachfrage nach einer flexibleren, effektiven Lösung für vertrauliches Computing führt.\n\n## Der hybride Ansatz des vertraulichen Rechnens\n\nUm dieses Problem anzugehen, bietet SafelyShare einen Ansatz, der am besten als Hybridmodell des vertraulichen Rechnens beschrieben wird. Um für Dienste zu kompensieren, die nicht innerhalb sicherer Enklaven arbeiten, führt diese Methodik die Idee der 'Zeugenausführung' ein. In diesem Szenario vertraut der Benutzer auf die Garantie des Anbieters für seine Kompetenz und sichere Datenhandhabung. Es handelt sich um eine Art stillschweigender Vereinbarung zwischen dem Benutzer und dem Remote-Dienstanbieter, die das vertrauliche Rechnen in realen Szenarien realistischer macht.\n\nDieser hybride Ansatz definiert das Paradigma der sicheren Freigabe in einer sich ständig weiterentwickelnden Welt neu. Mit seiner elastischen Grundlage integriert SafelyShare ein tiefgreifendes Verständnis der sich ändernden Sicherheitsparameter und macht vertrauliches Computing anpassungsfähig und reaktionsschnell auf sich ändernde Anforderungen und Realitäten.\n\n## Fazit: Revolutionierung der sicheren Datenweitergabe\n\nIm Wesentlichen ist SafelyShare der führende Wegbereiter auf dem Weg zur sicheren, effizienten und machbaren Weitergabe sensibler Daten. Es umgeht herkömmliche Hürden und integriert hybride vertrauliche Datenverarbeitung in sein Framework, was zu einer einzigartigen Mischung aus Vertrauen und Praxistauglichkeit führt. Der innovative Ansatz, bezeugte Datenverarbeitung in den Prozess zu integrieren, verschwimmt die Grenzen zwischen vollständigem und teilweisem Vertrauen, macht Datensicherheit erreichbarer und liefert eine vielversprechende Erzählung für die Zukunft der Datenweitergabe und -sicherheit.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Shamim Naqvi","Darren W Pulsipher"],"link":"/episode-EDT177-de","image":"./episodes/edt-177/de/thumbnail.jpg","lang":"de","summary":"Während dieser Folge diskutieren Darren und SafeLishare CEO Shamim Naqvi, wie vertrauliches Computing eingesetzt werden kann, um verwaltete Datenaustausch-Kollaborationsumgebungen in der Cloud zu erstellen."},{"id":80,"type":"Episode","title":"Zero Trust Networking mit OpenZiti","tags":["zerotrustnetworking","overlaynetworks","networksecurity","digitalage","securitychallenges","networkingconcepts","softwaredevelopers","softwareengineers","defensestrategy","databreaches","virtualnetworks","zerotrustprinciples","openzerotrust","securitymeasures","identitymanagement","secureconnectivity","networkdefense","iotdevices","datatransmission","smartnetworking","serviceintegration","vpnnetworks","wireguardnetworks","datasecurity","itsecurity","digitalinnovations","digitaltransformation","futureproofsecurity"],"body":"\r\n\r\n# Enthüllung der Dynamik von Zero Trust Networking und Overlay-Netzwerken\n\nMit dem Fortschreiten des digitalen Zeitalters nimmt die Diskussion um Netzwerksicherheit eine vordergründige Position ein. In einer sich schnell verändernden digitalen Landschaft sind Zero-Trust-Netzwerke und Overlay-Netzwerke entscheidende Strategien zur Bewältigung aktueller Sicherheitsherausforderungen. Hier gehen wir auf diese Konzepte ein, wie sie unsere digitalen Systeme prägen und liefern ein Verständnis ihrer potentiellen Vorteile und Anwendungen.\n\n## Ein genauerer Blick auf Zero Trust Networking\n\nDas Zero-Trust-Networking ist eine Denkweise, die Sicherheit als primäre Sorge bei der Gestaltung und dem Betrieb digitaler Systeme sieht. Ein kritischer Aspekt dabei ist die Annahme potenzieller Bedrohungen von jedem Teil des Netzwerks, unabhängig davon, wie sicher sie erscheinen mögen. Dieser Ansatz entfernt sich von dem traditionellen Festungskonzept in der Sicherheit und führt zu robusteren Netzwerken, die sich nicht ausschließlich auf den Schutz einer einzelnen Firewall verlassen.\n\nZunächst liegt die Schönheit von Zero-Trust-Netzwerken in ihrer Fähigkeit, effektiv und sicher zu arbeiten, was einen Vorteil für Softwareentwickler und Ingenieure darstellt. Sicherheit wird zu einem Ermöglicher statt zu einer Behinderung des Softwareentwicklungsprozesses. Mit Zero-Trust-Netzwerk können sich Entwickler auf die Entwicklung von Funktionen konzentrieren, ohne sich um blockierte Ports oder die Konsultation von Netzwerkteams kümmern zu müssen - ein bedeutender Schritt in Richtung schnellerer Markteinführungen.\n\nNichtsdestotrotz eliminiert Zero-Trust-Networking nicht die Notwendigkeit von Perimeter-Schutzmaßnahmen oder Firewalls. Die Zero-Trust-Strategie geht von einem möglichen Netzwerkkompromiss aus; daher fordert sie eine Verteidigungsschichtung anstelle einer alleinigen Abhängigkeit von elementarer Perimeter-Verteidigung.\n\n## Der Aufstieg von Overlay-Netzwerken\n\nAngesichts der zunehmenden Sicherheitsbedrohungen und Datenverletzungen erweisen sich Overlay-Netzwerke als unschätzbares Werkzeug. Diese softwaredefinierten virtuellen Netzwerke bieten eine zusätzliche Sicherheitsebene im Vergleich zu Underlay-Netzwerken wie Routern oder Firewalls.\n\nOverlay-Netzwerke wie VPN und Wireguard ermöglichen sichere Kommunikation zwischen Ressourcen, selbst wenn das zugrundeliegende Netzwerk kompromittiert wurde. Sie bieten attraktive Funktionen, wie die Selbstreorganisation basierend auf Bedingungen, was ihnen temporäre Merkmale verleiht. Diese Netzwerke bieten auch Optionen für sichere In-Application- oder Daten-System-Kommunikation - zusätzlich stärkt eine Option für ein clientloses Endpunkt die Benutzerkonnektivität und erfordert keine Softwareinstallation auf einzelnen Geräten.\n\nOverlay-Netzwerke bieten Flexibilität hinsichtlich der Bereitstellung. Es besteht keine Notwendigkeit, Ihren Anwendungscode umzuschreiben, da der Code für das Overlay-Netzwerk direkt in den Anwendungscode eingebettet werden kann. Alternativ kann ein virtuelles Gerät bereitgestellt werden, wenn Sie Änderungen an Ihrer Anwendung vermeiden möchten. Dieser Komfort, kombiniert mit zusätzlicher Sicherheit, stellt Overlay-Netzwerke als zukunftsorientierte Lösungen für die Netzwerksicherheit auf.\n\n## Die Kraft von ZTN und OpenZiti Lösungen\n\nAngebote für Zero Trust Networking (ZTN), wie Open Zero Trust (Open Ziti), bieten kompetente Lösungen im Bereich Zero Trust und Overlay-Networking. Sie bringen robuste Zero Trust-Prinzipien in das Feld der Overlay-Netzwerklösungen ein.\n\nZTN bringt beispielsweise sein Identitätssystem auf den Tisch, perfekt für Edge-IoT-Geräte, die keinen Zugang zu typischen Identitätsdiensten haben. Es bietet sichere Datenübertragung durch gegenseitiges Tunneling und ein intelligentes Routing-Gewebe, das den effizientesten Weg von Punkt A nach Punkt B bestimmt. Auf der anderen Seite erleichtert Open Ziti mehrere Anwendungsfälle und verwaltet Ost-West- und Nord-Süd-Verbindungen reibungslos und sicher. Es integriert sich gut mit Service Meshes, um eine hohe Sicherheit zu gewährleisten.\n\nDaher wird die Einführung solcher ganzheitlicher Sicherheitsmaßnahmen notwendig, wenn wir in das digitale Zeitalter eintreten. ZTN und OpenZiti bieten praktische Lösungen für diejenigen, die das Zero Trust Modell übernehmen, mit vorteilhaften Merkmalen von Identitätsmanagement bis hin zu sicherer Konnektivität. Zweifellos setzen diese Innovationen neue Maßstäbe für die Netzwerksicherheit.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Phillip Griffiths","Darren W Pulsipher"],"link":"/episode-EDT178-de","image":"./episodes/edt-178/de/thumbnail.png","lang":"de","summary":"In dieser Episode interviewt Darren Phillip Griffith, einen Gemeinschaftsführer des Open-Source-Projekts OpenZiti. Sie diskutieren über die Bedeutung des Zero Trust Netzwerks in modernen IT-Netzwerken."},{"id":81,"type":"Episode","title":"Nutzung von generativem KI im College","tags":["generativeai","academicintegration","aiineducation","digitaltransformation","blogpost","aistudents","byuidaho","chatgpt","aiassistance","academiccheating","distinguishingguidelines","selfassessment","aiinselfassessment","qualitativeimprovement","ongoingdebates","aishortcomings","aiinaugmentation","futureofai","cheatingconcerns","shortcutculture","academicpolicies","aisuccess","aitoolkit","professionalhabitat"],"body":"\r\n\r\n## Navigation durch die Feinheiten der akademischen Integration mit generativer KI\n\nIn der schnelllebigen Welt, die durch die rasche digitale Transformation definiert ist, wird immer offensichtlicher, wie KI-Konstrukte untrennbare Teile des Alltags werden. Ein faszinierender Bereich, in dem ihre Auswirkungen gespürt werden können, ist das akademische Feld. Dieser Blog-Post beabsichtigt, das Potenzial von generativer KI durch die unmittelbaren Erfahrungen einer Studentin, Madeline Pulsipher, an der BYU Idaho, zu erkunden.\n\nDie Anwendung von generativer KI-Unterstützung wie ChatGPT in der akademischen Arbeit offenbart spannende Möglichkeiten. Verantwortungsvoll genutzt, kann dieses leistungsstarke Werkzeug einen digitalen Vorteil beim Brainstorming von Ideen, beim Generieren von Aufsatzstrukturen und bei der Selbstbewertung Ihrer Arbeit anhand von Bewertungsrubriken bieten.\n\n## Generative KI - Werkzeug oder Trick?\n\nDie Frage, ob die Verwendung von KI für akademische Aufgaben als Betrug angesehen wird, stellt einen faszinierenden Aspekt dar. Madeline weist zu Recht darauf hin, dass der Einsatz von KI zur Erleichterung eines Prozesses oder zur Unterstützung nicht mit Betrug gleichgesetzt werden sollte. Betrug würde implizieren, dass ein Essay ausschließlich von der KI verfasst und sich das eigenständige Werk zu eigen gemacht wird.\n\nWir müssen jedoch unterscheidende Richtlinien erstellen, wenn wir neuere technologische Methoden ansprechen. Die Definition dessen, was verantwortungsvolle Nutzung gegenüber Betrug darstellt, wenn KI in der Wissenschaft eingebunden wird, ist eine wesentliche Aufgabe, an der Bildungseinrichtungen arbeiten und diese formal und intensiv festlegen müssen.\n\n## Die Effizienz von KI in der Selbstbewertung\n\nEine faszinierende Anwendung von KI hat alle auf ihrem Weg zum Stillstand gebracht - sie bewertet ihre Arbeit selbst anhand des etablierten Bewertungsrasters vor der Einreichung. Madelines Experimente mit diesem Ansatz brachten fruchtbare Ergebnisse, da sie in allen ihren KI-gestützten Aufsätzen As erzielte. Dies deutet auf das neu entdeckte Potenzial von KI hin, nicht nur bei mechanischen Aufgaben zu helfen, sondern auch die qualitative Verbesserung der Arbeit zu unterstützen.\n\n## Aussichten und laufende Debatten\n\nDer Einsatz von KI im akademischen Kontext wird seit geraumer Zeit diskutiert. Während es ein wertvolles Werkzeug zur Verbesserung der Lernergebnisse und Steigerung der Produktivität sein kann, ist es wichtig zu bedenken, dass KI den menschlichen Verstand nicht ersetzen kann. Jede neue Technologie hat Vor- und Nachteile, und bei KI ist das nicht anders.\n\nObwohl generative KI Inhalte produzieren kann, fehlt ihr die menschliche Note, die in der Kommunikation wesentlich ist. Sie kann menschliche Lehrer bei der Erklärung komplexer Konzepte nicht ersetzen, da sie die Fähigkeit benötigt, die Feinheiten menschlicher Gespräche zu verstehen. Daher muss KI, während sie in bestimmten Bereichen ein wertvolles Asset sein kann, den Wert menschlicher Interaktion und Expertise aufrechterhalten.\n\n## Verbesserung sozialer Interaktionen\n\nDie COVID-19-Pandemie hat das Leben vieler Studenten, die in diesem Jahr ihr erstes Studienjahr beginnen, durcheinandergebracht. Der negative Dating-Trend unter Teenagern hat sich während der Pandemie weiter verschärft. Aufgrund des Mangels an sozialen Interaktionen fehlen der aktuellen Generation viele wichtige Erfahrungen, wie etwa das erste Mal Schlussmachen, der erste Kuss oder das Fragen nach einem weiteren Date.\n\nMadeline suchte bei ihren Freunden nach Rat, wie sie einen Mann, der ein weiteres Date wollte, sanft loswerden könne, erhielt aber widersprüchliche Ratschläge. Dann wandte sie sich an ChapGPT, einen unparteiischen und emotionslosen, künstlich intelligenten Assistenten, um Rat zu holen. Sie nutzte die Vorschläge von ChapGPT als Leitfaden, um ihren Ansatz zu entwickeln.\n\nDiese Fähigkeit, Generative AI als Berater und nicht als endgültige Autorität zu nutzen, wird für die nächste Generation entscheidend sein, um die Macht der KI in akademischen und sozialen Situationen zu nutzen.\n\n## Die Zukunft der KI in der Wissenschaft\n\nVerschiedene Bedenken bestehen weiterhin im Zusammenhang mit der Integration von KI in die Wissenschaft - Sorgen über Betrug, das Fehlen etablierter institutioneller Richtlinien und die Möglichkeit einer Kultur der Abkürzungen zu fördern. Es ist jedoch unbestreitbar, dass generative KI ein Werkzeug ist, zu dem viele Studierende greifen, und sein volles Potenzial im akademischen Bereich muss noch gründlich erforscht werden.\n\nOffensichtlich muss die strenge Linie zwischen Betrug und angemessener Nutzung sorgfältig festgelegt werden. Aber sobald diese Linie etabliert ist, sieht der Erfolg von KI als Werkzeug in akademischen Paradigmen vielversprechend aus. Wenn es richtig eingesetzt wird, kann es einen wesentlichen Teil eines Bildungswerkzeugs werden - es formt kompetente Individuen, die gut ausgerüstet sind, um mit KI in ihrem professionellen Umfeld umzugehen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Madeline Pulsipher","Darren W Pulsipher"],"link":"/episode-EDT179-de","image":"./episodes/edt-179/de/thumbnail.png","lang":"de","summary":"In dieser Folge interviewt Darren seine Tochter, die kürzlich ihr erstes Semester im College abgeschlossen hat, über ihre Erfahrungen mit der Nutzung von generativer KI-Technologie in ihrem akademischen Studium. Sie beschreibt die Herausforderungen und Erfolge, die mit der Nutzung dieses transformativen Tools verbunden sind."},{"id":82,"type":"Episode","title":"Datenerhebung und -aufbereitung","tags":["dataarchitecture","datacentric","data"],"body":"\r\n\r\n## Wir brauchen Daten! Unsere Daten sind ein Durcheinander!\n\nDas Erste, worüber man in diesem Teil des Prozesses nachdenken sollte, ist die Daten-Pipeline. Wie identifizieren wir die Rohdaten, die wir benötigen, und wie bringen wir sie durch die Pipeline und verwandeln sie in Erkenntnisse? Es gibt fünf wesentliche Schritte in der Pipeline: die Bestimmung des Geschäftswerts der Daten, das Eingießen, die Vorbereitung, die Analyse und schließlich das Handeln basierend auf den resultierenden Erkenntnissen.\n\nLassen Sie uns die Produktion als Beispiel betrachten. Bei der Bestimmung, welche Daten für ein Unternehmen von Wert sind, sollten Sie drei grundlegende Fragen stellen: Wie hoch ist die Nachfrage nach meinem Produkt? Wie ist der aktuelle Bestand? Wie hoch ist der Verlust der Ausbeute? Das sind scheinbar einfache Fragen, aber dann müssen Sie über komplexere Dinge nachdenken, wie kann ich die Nachfrage, die Produktionsfähigkeiten, den Bestand und den Ausbeuteverlust quantifizieren? Woher stammen die Daten? Wie nehme ich sie auf? Wie zuverlässig und stabil sind diese Daten? Es gibt viele Fragen und Variablen, wie die Lieferzeit des Rohprodukts, die projizierte Nachfrage und den unbekannten Ausbeuteverlust, die große Komplexität schaffen können.\n\nDie Pipeline vereinfacht, wie all diese Komponenten zusammenkommen. Jede Art von Daten durchläuft die wichtigsten Schritte in der Pipeline, jedoch werden sie alle unterschiedlich sein. Zum Beispiel wird die Erfassung einer Art von Daten von der Erfassung einer anderen Art von Daten abweichen. Die Idee ist jedoch, alle Daten zusammenzubringen, um ein klares Bild zu erzeugen.\n\n## Wir haben Daten! Was machen wir damit?\n\nJe nach Art der Daten und den Fragen, die Sie beantworten möchten, würden Sie unterschiedliche Analysetechniken verwenden. Zum Beispiel könnte man bei der Beantwortung der Frage, wie viele Widgets hergestellt werden sollten, durch Analyse der historischen Angebot und Nachfrage mittels Analytik und grundlegendem Business Intelligence vorgehen. Um festzustellen, welche Widgets visuelle Mängel aufweisen, könnte ein Algorithmus, der über Deep Learning Defekte in Bildern erkennt, der beste Ansatz sein. Es gibt keine Technik, die alle Probleme löst; jede ist einzigartig für das Problem und die Daten selbst.\n\nZusätzlich ist es wichtig, Fachexperten hinzuzuziehen, um die Muster, die die Daten liefern, besser zu verstehen. Der Fachexperte wird die Daten und deren Herkunft verstehen, und der Datenwissenschaftler wird den besten Ansatz für die Algorithmen verstehen, um mehr Erkenntnisse zu gewinnen. Wenn zum Beispiel ein Rückgang des Produktionsausstoßes durch einen maschinellen Lernalgorithmus vorhergesagt wird, wissen die Ingenieure, die das Problem beheben müssen, ohne den Kontext des Problems möglicherweise nicht, wo sie suchen sollen. Einer der Gründe, warum Organisationen nicht den erwarteten Return on Investment erzielen, ist, dass sie ihre Modelle nicht handlungsfähig gemacht haben oder nicht das Verhalten der Systeme, die sie vorhersagen möchten, widerspiegeln.\n\nWie all dies zusammen funktioniert, hängt von den geschäftlichen Fragen ab, die Sie stellen, und von Ihren Herausforderungen. Zum Beispiel könnten Sie eine Reihe von Algorithmen haben, die Ihnen sagen, wie viele Widgets Sie herstellen sollen. Sie könnten einen Deep Learning-Algorithmus haben, der erkennt, ob ein Widget einen Defekt aufweist und sogar die Defekte kategorisiert. Aber das hilft Ihnen nicht unbedingt weiter, wenn Sie nicht wissen, warum dieser Defekt aufgetreten ist. Sie müssen also diese Informationen mit einigen weiteren Algorithmen verknüpfen, um Korrelationen herzustellen, die die Defekte erklären, und Sie benötigen einen Aktionsplan, um das Problem zu korrigieren.\n\n## Wir müssen Erkenntnisse schaffen. Wie trainieren wir unsere Daten?\n\nWie erreichen wir das? Im Wesentlichen bringen Sie alle Daten zusammen, bereiten sie auf und verknüpfen sie, um zum Beispiel den Lieferbestand und die Vorhersagen zum Ertragsverlust zu quantifizieren. Sie werden im Laufe der Zeit Problemlösungs- und kontinuierliche Verbesserungspraktiken benötigen, um sich an wechselnde Bedingungen anzupassen. Hier kommt die Kultur der Organisation ins Spiel. Ein Problem einmal zu lösen, ohne sich zur kontinuierlichen Verbesserung zu verpflichten, kann dazu führen, dass eine Organisation langfristig den eigentlichen Wert der Analyse verpasst.\n\nWir beobachten heute einen großen Wandel hin zu Organisationen mit einer datenzentrierten Infrastruktur. Daten befinden sich nicht mehr nur im Rechenzentrum, sondern auch in der Cloud und am Netzwerkrand. Mit dem Geschäftsprozess an der Spitze, der zu kontinuierlicher Verbesserung, geschäftlichem und datenbezogenem Verständnis und letztendlich zur Implementierung führt, können Organisationen, die auf dieser Infrastruktur aufbauen, einen deutlichen Unterschied erkennen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT18-de","image":"./episodes/edt-18/de/thumbnail.png","lang":"de","summary":"Sarah Kalicin, Lead Data Scientist bei Intel, und Darren Pulsipher, Chief Solution Architect, Public Sector bei Intel, sprechen über den Prozess und die Vorteile der Datensammlung und -vorbereitung bei der Entwicklung einer datenzentrierten Organisation. Dies ist der zweite Schritt auf dem Weg zur datenzentrierten Organisation."},{"id":83,"type":"Episode","title":"Generative KI in der Hochschulbildung (Überarbeitet)","tags":["embracingdigitaltransformation","darrenpulsipher","lauranewey","generativeai","aiineducation","educationtransformation","teachingexperience","aiinclassroom","moderneducationdynamics","criticalthinkingskills","digitaltransformation","resistanceagainstai","traditionalteaching","incoorporatingai","ineducationalcurriculums","innovationandeducation","ethicalusageofai","academicintegrity"],"body":"\r\n\r\n## Wie generative K.I. das Klassenzimmer-Erlebnis verbessert\n\nGenerative KI verwebt sich schnell in Bildungscurricula und beeinflusst, wie Lehrkräfte das Unterrichten angehen, und verbessert grundlegend das Lernerlebnis. Laut Newey ist diese viel diskutierte Technologie nicht nur eine Form des Plagiats, sondern ein brillantes Werkzeug, das Bildungsmethoden ergänzt und revitalisiert. Sie fördert die Nutzung von KI bei Denkaufgaben durch Schüler und betont die Förderung und Ernte kritischer Denkfähigkeiten in unserer rasend schnell digitalisierenden Gesellschaft.\n\nStatt sich aufzuhalten als passive Teilnehmer, plädiert sie dafür, dass die Schüler aktive Akteure werden, die die von KI generierten Ergebnisse analysieren und die Qualität und Substanz ihrer Input-Informationen berücksichtigen. Die Verlagerung unterstreicht die Bedeutung von Verständnis, Forschung und Analyse über bloße Ergebnisgenerierung hinaus.\n\n## Übergang von traditionellem Unterrichten\n\nNeweys fortschrittlicher Ansatz weicht drastisch von den konventionellen Methoden ab, an denen die meisten Pädagogen festhalten, insbesondere angesichts des allgemeinen Widerstands gegen die Integration von generativem A.I. in Bildungseinrichtungen. Sie betont jedoch die Unvermeidlichkeit und Notwendigkeit, die Digitalisierung zum allgemeinen Vorteil der Schüler zu übernehmen.\n\nDer Vergleich dieses Übergangs mit dem anfänglichen Widerstand, das Internet als Lehrmittel zu nutzen, zeigt, wo wir heute stehen. Generative KI, wie jede andere sich entwickelnde Technologie, erfordert eine Eingliederung in den Lehrplan und verlangt regelmäßige Updates, um in dieser schnelllebigen digitalen Landschaft relevant zu bleiben.\n\n## Innovation und Ethik ausbalancieren\n\nMit Fortschritt und Innovation thematisiert Newey auch die ethischen Überlegungen, die mit dieser Veränderung einhergehen. Sie berichtet von mehreren Fällen, in denen Schüler unwissentlich oder subtil von KI generierte Aufsätze eingereicht haben. Damit betont sie die Notwendigkeit für Pädagogen, technologische Akzeptanz und ethische Nutzung wachsam in Einklang zu bringen.\n\nSie ist fest davon überzeugt, dass Schüler K.I. als produktives Werkzeug nutzen können, aber gleichzeitig liegt die Verantwortung auch bei den Erziehern, sie in Richtung der Wahrung der akademischen Integrität zu lenken.\n\n## Fazit: Den Weg ebnen für ein KI-gestütztes Bildungssystem\n\nDie Integration von generativer KI in die Bildung, obwohl auf Widerstand stoßend, ist ein tiefgreifender Hinweis auf die sich verändernde Bildungslandschaft. Wie Newey veranschaulicht, kann eine erfolgreiche Integration von KI in die Bildung das Lernerlebnis erheblich verbessern und die Entwicklung essenzieller Fähigkeiten fördern, was die Bereitschaft unserer Schüler für eine durch digitale Transformation geprägte Zukunft sichert.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Laura Newey","Darren W Pulsipher"],"link":"/episode-EDT180-de","image":"./episodes/edt-180/de/thumbnail.jpg","lang":"de","summary":"In der dieswöchigen Folge von Embracing Digital Transformation interviewt Darren Pulsipher die Gastrednerin Laura Newey über ihre faszinierende Reise durch die kritisch aufkommende Welt der Generativen KI, insbesondere im Bildungssektor. Sie deckt die Transformation ihrer Lehrerfahrung und die Bereicherung der Lernergebnisse ihrer Schüler durch KI auf und analysiert ausführlich die Anpassung an moderne Bildungsdynamiken."},{"id":84,"type":"Episode","title":"Zero Trust in 5G","tags":["5gtechnology","zerotrustsecurity","embracingdigitaltransformationpodcast","darrenpulsipher","lelandbrown","yazkrdzalic","kenurquhart","trentonsystems","zscaler","advancedcommunications","operationaltechnology","informationtechnology","lonelyislandapproach","telecominfrastructure","advancedsecuritymodels","commercial5gusage","military5gusage","technicalchallenges","cybersecurity","5gsecurity","integration","solutionarchitecture"],"body":"\r\n\r\n## Die ausgedehnte 5G-Landschaft und der einsame Insel-Ansatz\n\nDie Welt der 5G-Technologie entwickelt sich rasant und infolge dessen finden viele aufschlussreiche Diskussionen statt um die Verschmelzung von Betriebstechnologie (OT) und Informationstechnologie (IT). Yazz Krdzalic beschreibt das Konzept des \"Einsame Insel-Ansatzes\". Dieser Ansatz bezieht sich auf die Tendenz verschiedener Einheiten, sich zu stark auf die Lösung ihrer individuellen Probleme zu konzentrieren, was oft zum Stillstand des Wachstums in kundenspezifischer Hardware in der Telekommunikationsinfrastruktur geführt hat.\n\nDie Notwendigkeit, von diesem individualistischen Ansatz abzuweichen und einen kollektiven architektonischen Rahmen zu schaffen, der sich an verschiedene Anwendungsfälle anpassen und mit ihnen skalieren kann, wird zunehmend offensichtlich. Mit dem Aufkommen der 5G-Technologie besteht ein Bedarf an einem kollaborativen Ansatz, der die verschiedenen Anforderungen verschiedener Einheiten berücksichtigen kann. Der kollektive Ansatz trägt dazu bei, sicherzustellen, dass die Infrastruktur flexibel und skalierbar ist, was es den Einheiten erleichtert, ihre Technologien und Anwendungen in das Netzwerk zu integrieren.\n\nDie Diskussionen um die Zusammenlegung von OT und IT nehmen ebenfalls an Fahrt auf, und es wird immer deutlicher, dass die Zusammenarbeit zwischen diesen beiden Bereichen für den Erfolg der 5G-Technologie unerlässlich ist. Während sich die Technologie weiterentwickelt, wird erwartet, dass es mehr Debatten und Diskussionen darüber geben wird, wie man die durch 5G gebotenen Möglichkeiten nutzen kann, während gleichzeitig die Herausforderungen der sich entwickelnden Technologie angegangen werden müssen. Insgesamt sieht die Zukunft der 5G-Technologie hell aus, und die Zusammenarbeit zwischen verschiedenen Akteuren wird eine entscheidende Rolle für ihren Erfolg spielen.\n\n## Übergang zur Zero Trust Sicherheit\n\nMit der Fortentwicklung der Technologie sind Sicherheitsbedenken zunehmend zu einem Problem für Einzelpersonen und Organisationen geworden. Um diese Bedenken anzugehen und eine sichere Umgebung zu gewährleisten, wird ein gemeinsames Architekturframework benötigt. Dieses Framework beinhaltet die Implementierung von fortschrittlichen Sicherheitsmodellen wie Zero Trust Security. Der Übergang zu diesen Modellen ist jedoch nicht immer einfach. Es erfordert die Aufgabe älterer Betriebsmethoden und die Sicherstellung, dass alle technologischen Module synchronisiert und ordnungsgemäß funktionieren. In der Vergangenheit lag die Verantwortung für die Integration aller Komponenten beim Kunden. Glücklicherweise hat mit der Einführung eines fortgeschritteneren Ansatzes die Belastung der Integration für die Kunden erheblich abgenommen, wodurch die Implementierung von Zero Trust Security und anderen fortschrittlichen Sicherheitsmodellen ein viel reibungsloserer Prozess geworden ist.\n\n## Finden der gemeinsamen Basis bei der 5G-Nutzung\n\nDie Entwicklung der 5G-Technologie hat sowohl im kommerziellen als auch im militärischen Sektor das Spiel verändert. Es gibt jedoch spezifische Anforderungen, die den kommerziellen und militärischen Einsatz von 5G unterscheiden. Kommerzielle Einsatzbereiche von privaten 5G-Netzwerken sind größtenteils statisch, während militärische Einsatzbereiche mobil sein müssen.\n\nLeland Brown, ein prominenter Experte auf diesem Gebiet, hat die Komplexität der Suche nach einer gemeinsamen Architektur diskutiert, die beiden Anforderungen gerecht werden könnte. Die Herausforderung bestand darin, eine abschließende Lösung zu schaffen, die diese Anforderungen elegant erfüllt. Es war wichtig sicherzustellen, dass die Lösung sowohl für den kommerziellen als auch für den militärischen Einsatz effizient und wirksam war.\n\nDie Entwicklung solcher Lösungen ist entscheidend, um sicherzustellen, dass die 5G-Technologie in ihrem vollen Potenzial genutzt wird und die vielfältigen Bedürfnisse verschiedener Branchen erfüllen kann.\n\n## Zusammenfassen\n\nDie Welt der Technologie entwickelt und verbessert sich ständig, und das Aufkommen von 5G-Technologie und Zero Trust-Sicherheit ist ein Beweis dafür. Die Implementierung dieser Fortschritte kann jedoch aufgrund technischer und kultureller Hindernisse eine Herausforderung sein. Glücklicherweise arbeiten Experten wie Leland Brown, Ken Urquhart und Yaz Krdzalic daran, die Integration von 5G-Technologie und Zero Trust-Sicherheit zu optimieren und den Weg zu einer sichereren und effizienteren technologischen Zukunft für alle etwas zu erleichtern. Ihre Einsichten und Fachkenntnisse bringen Licht in die kontinuierliche Reise der Evolution und Verbesserung in der Welt der Technologie.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Leland Brown","Yazz Krdzalic","Ken Urquhart","Darren W Pulsipher"],"link":"/episode-EDT181-de","image":"./episodes/edt-181/de/thumbnail.jpg","lang":"de","summary":"Inmitten der weltweit wachsenden Übernahme von 5G-Technologien gingen die Experten in der jüngsten Folge des Embracing Digital Transformation Podcasts auf das zentrale Thema Zero Trust in der 5G-Sicherheit ein. Gastgeber Darren Pulsipher begrüßte den Experten für fortgeschrittene 5G-Kommunikation Leland Brown, den VP of Marketing bei Trenton Systems Yazz Krdzalic, und Ken Urquhart, einen Physiker, der zum Cybersicherheitsprofi bei Zscaler wurde, um über die Integration und Weiterentwicklung der 5G-Technologie sowie ihre Herausforderungen und Durchbrüche zu diskutieren."},{"id":85,"type":"Episode","title":"Zero Trust Datenabsicherung","tags":["digitaltransformation","dataintegrity","zerotrust","cybersecurity","strategies","walacor","encryption","loganalysis","geographicaldistribution","externalhackers","organizationalthreats","datamanagement","dataaudits","immutableauditlog","systemchange","legalcompliance","reducerisk","dataintelligence","multilayeredsecuirty","dataprotectionsolution","keyvulnerabilities","improveddataprotection","futuredigitalbusinesses","revolutionizedigitallandscape"],"body":"\r\n\r\n## Entlarvung von Annahmen über Datensicherheit\n\nIn der Vergangenheit hatten die Menschen implizites Vertrauen darauf, dass ihre Daten sicher sind und ihre Privatsphäre geschützt wird. Dieses Vertrauen beruht jedoch oft auf einem veralteten Modell, das nicht mehr mit der aktuellen technologischen Landschaft übereinstimmt. Die zunehmende Anzahl von Datenverletzungen und Cyber-Angriffen hat deutlich gemacht, dass Datensicherheit wichtiger ist denn je und die Vorsichtsmaßnahmen, die in der Vergangenheit als ausreichend angesehen wurden, möglicherweise nicht mehr ausreichend sind.\n\nHeute sind Daten nicht nur von externen Hackern bedroht, sondern auch von innerhalb der Organisationen. Es ist wichtig zu verstehen, dass ein Datenleck erhebliche Auswirkungen haben kann, von finanziellen Verlusten bis hin zu Reputationsschäden. Daher ist es entscheidend, einen Zero-Trust-Ansatz für das Datenmanagement zu implementieren, was bedeutet, dass jede Anfrage auf Datenzugriff überprüft werden muss, bevor der Zugriff gewährt wird. Zuverlässige Datenprüfungen sind ebenfalls notwendig, um sicherzustellen, dass die Dateneingabe mit der Datenausgabe übereinstimmt und dass es keinen unbefugten Zugriff auf sensible Informationen gibt.\n\n## Implementierung einer neuen Ära der Datensicherheit mit Walacor\n\nWalacor bietet eine einzigartige Lösung, um unser Verständnis von Datensicherheit zu verbessern. Sie bieten ein automatisches und lückenloses Prüfprotokoll an, das unveränderlich ist. Das bedeutet, sobald Daten eingegeben wurden, können sie nie verändert oder gelöscht werden, ohne dass dies bemerkt wird. Diese Funktion macht es unglaublich einfach, jede Änderung, die am System vorgenommen wird, zu verfolgen, was für die Aufrechterhaltung einer sicheren Umgebung von entscheidender Bedeutung ist.\n\nIndem Walacors Lösung Transparenz und Nachverfolgbarkeit bietet, hilft sie Organisationen, rechtliche Compliance-Anforderungen zu erfüllen und Risiken zu mindern. Beispielsweise kann in einem Rechtsstreit ein unveränderliches Prüfprotokoll als zuverlässige Beweisquelle dienen, da es nicht manipuliert werden kann. Darüber hinaus kann ein unveränderliches Prüfprotokoll im Falle eines Datenbruchs dabei helfen, die Quelle des Bruchs und das Ausmaß des verursachten Schadens zu identifizieren.\n\nInsgesamt bietet Walacors innovativer Ansatz zur Datensicherheit mit seinem 100% unveränderlichen Prüfprotokoll eine vielversprechende Lösung für Organisationen, die ihre Cybersicherheitsposition verbessern möchten.\n\n## Gestaltung der Zukunft der Datenintelligenz\n\nDas steigende Risiko von Datenverstößen bedeutet, dass wir uns von der Nutzung mehrerer Datenschutzschichten zu einer stärker integrierten Datenschutzlösung bewegen müssen. Diese Art von Lösung legt den Grundstein für eine Zero-Trust-Umgebung, die das Risiko von Cyberbedrohungen und Schwachstellen erheblich reduziert. Durch die Annahme dieses Ansatzes können wir unsere Datenschutzmethoden vereinfachen und eine bessere Datenintegrität gewährleisten.\n\nDie Entwicklung der Datenintelligenz in Form von Datenintegrität und Sicherheit eröffnet neue Möglichkeiten für digitale Unternehmen. Verbesserte Methoden zum Datenschutz, bessere Datenintegrität und eine Verringerung potenzieller Cyber-Bedrohungen sind nur einige der Vorteile, die die digitale Landschaft verändern werden. Unter diesen ist die einzigartige Herangehensweise von Walacor an Datenintegrität und Null Vertrauen das Gesprächsthema in der Stadt, was einen bedeutenden Meilenstein in unserer Herangehensweise an die Datensicherheit jetzt und in Zukunft markiert.\n\nWeitere Informationen finden Sie unter (https://walacor.com)[https://walacor.com]\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Walter Hancock","Darren W Pulsipher"],"link":"/episode-EDT182-de","image":"./episodes/edt-182/de/thumbnail.png","lang":"de","summary":"Die Notwendigkeit robuster Datensicherheitsstrategien ist im digitalen Zeitalter exponentiell gewachsen und hat weltweit für Unternehmen oberste Priorität erlangt. Walter Hancock, Cybersicherheitsexperte und CTO von Walacor, bietet scharfsinnige Einblicke in die Bedeutung der Datenintegrität und einen Null-Vertrauens-Ansatz in aktuellen Cybersicherheitsregimen."},{"id":86,"type":"Episode","title":"Datenmanagement in Materialwissenschaft und Fertigungsindustrien","tags":["digitaltransformation","materialscience","manufacturingindustry","dataanalytics","machinelearning","artificialintelligence","productdevelopment","structuredmethodologies","projectmanagement","complexdata","unstructureddata","datascience","r&dprocess","newproductdevelopment","datamanagement","technologyinnovation","physicitaldigitalknowledgegap","embracingdigital.org","embracingdigitaltransformationpodcast"],"body":"\r\n\r\n## Die Lücke zwischen Physischem und Digitalem in Forschung und Entwicklung überbrücken\n\nMaterials Zone konzentriert sich auf den speziellen, aber bedeutenden Aspekt der Materialwissenschaft, insbesondere in der Fertigungsindustrie. Angesichts der erheblichen Rolle von Materialien in der Produktentwicklung ist ein effektives Datenmanagement von entscheidender Bedeutung. Analog zu einem Kochrezept beinhaltet die Materialwissenschaft eine nuancierte Integration von Zutaten (Materialien), die durch einen Prozess geleitet werden, um das Endprodukt zu erzeugen.\n\nDieser Bereich war jedoch historisch gesehen immer ad hoc, abhängig von Versuch, Irrtum und Intuition. Infolgedessen geht das während dieses Prozesses erworbene Wissen oft aufgrund unzureichender Dokumentation oder Mitarbeiterfluktuation verloren. In unserer modernen, vernetzten Welt, in der Produktentwicklungsprozesse oft mehrere Standorte, sogar Länder, umspannen, ist es entscheidend, strukturierte Methoden zu etablieren, um Wissensverlust zu verhindern.\n\nEine der von Yudilevich hervorgehobenen Techniken befasst sich mit dem \"LKW-Faktor\", der vorschlägt, dass, wenn die einzige Person, die weiß, wie man eine bestimmte Aufgabe ausführt, von einem LKW angefahren wird, dies das gesamte Projekt potenziell entgleisen könnte. Daher könnte das Vorhandensein von mindestens einer anderen Person neben dem Hauptakteur, die die Aufgabe ausführen kann, die Verwundbarkeit des Teams verringern.\n\n## Erfassung der Komplexitäten von Materialwissenschaftsdaten\n\nDas Gebiet der Materialwissenschaft erzeugt komplexe Daten, die oft unstrukturiert sind und sich nur schwer mit herkömmlichen Datentabellen und Datenbanken ausreichend erfassen lassen. Um dies zu veranschaulichen, stellen Sie sich die Daten als eine Grafik vor, in der Rohstoffe zu Endprodukten werden. Die unzähligen Interaktionen zwischen den verschiedenen Bestandteilen führen zu mehreren einzigartigen Dimensionen innerhalb der Daten.\n\nDarüber hinaus existiert eine nahtlose Übersetzung im Fertigungsbereich - von der explorativen Forschung bis zur Produktionsphase, die Stabilisierung und Beständigkeit fordert. Das Zusammenfügen von Daten aus diesen Phasen in einem einheitlichen Repository kann den F&E-Prozess durch die Zentralisierung von Informationen, Unterstützung des Lernens zwischen den Phasen und Beschleunigung der Entwicklung neuer Produkte verbessern.\n\n## Integration von Data Science in die Fertigung\n\nObwohl Data Science viele Branchen durchdrungen hat, finden Unternehmen, die sich hauptsächlich auf die Produktentwicklung in der physischen Welt konzentrieren, den Aufbau spezieller Datenabteilungen oder die Integration analytischer Tools oft ineffizient und kostspielig. Hier kommt die Lösung von Materials Zone ins Spiel, die Data Science, maschinelles Lernen und statistische Tools für Unternehmen zugänglich macht, die mit diesen Bereichen nicht vertraut sind.\n\nSie bieten sofort einsatzbereite Werkzeuge an, die durch Webinare und Schulungssitzungen für eine einfache Einführung begleitet werden, wodurch die Barrieren für die Integration von Datenwissenschaft in Fertigungsverfahren verringert werden. Überraschenderweise können sogar Fortune 500 Unternehmen, denen die notwendigen digitalen Fähigkeiten fehlen, erheblich von solchen Lösungen profitieren.\n\n## Während wir voranschreiten\n\nWährend der Produktentwicklungsprozess immer komplexer und globaler wird, tritt die kritische Bedeutung einer systematischen Datenverwaltung in Kombination mit technologischer Innovation in den Vordergrund. Unternehmen wie Materials Zone ebnen den Weg und leiten Unternehmen an, ihre physisch-digitale Wissenslücke zu überbrücken, ihre Herstellungspraktiken zu stärken und zukünftigen Erfolg sicherzustellen.\n\nFür weitere Informationen, besuchen Sie https://materials.zone.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ori Yudilevich"],"link":"/episode-EDT183-de","image":"./episodes/edt-183/de/thumbnail.png","lang":"de","summary":"In einer sich schnell entwickelnden technologischen Landschaft setzen Führungskräfte aus verschiedenen Sektoren Datenanalyse, maschinelles Lernen und künstliche Intelligenz in ihren Betrieben ein. Heute schauen wir tiefer auf ein Unternehmen, das die digitale Transformation in der Fertigungsindustrie vorantreibt - Ori Yudilevich, der CTO von Materials Zone."},{"id":87,"type":"Episode","title":"Die Verwendung von Daten als strategischem Vermögenswert","tags":["dataarchitecture","data","datastrategy","people","process"],"body":"\r\n\r\nis excessive, but I can't bring myself to delete any emails. I fear that I might need them in the future. However, hoarding data can have negative consequences. It can lead to storage issues, slow down systems, and make it difficult to find the necessary information. Therefore, it is important for organizations to implement data management strategies to prevent data hoarding and ensure efficient operations.\n\nMuss nicht so groß sein, aber ich speichere Dinge für den Fall, dass ich sie brauche. Multipliziere das jetzt mit der Anzahl der Mitarbeiter. Bei Intel haben wir hunderttausend Mitarbeiter. Stell dir vor, wie viele Daten wir speichern, allein in E-Mails, wenn wir Sicherungen machen. Dazu kommen strukturierte und unstrukturierte Datenbanken, Präsentationen, Tabellenkalkulationen usw. Es ist klar, dass es nicht nur eine gute Strategie ist, alles zu speichern.\n\n## Datenstatistik\n\nLassen Sie uns die Statistiken betrachten, die zeigen, warum dies eine schlechte Strategie ist. Etwa 80 Prozent der Zeit von Datenwissenschaftlern wird damit verbracht, Daten aufzubereiten, bevor sie sie verwenden können. Weniger als 50% der strukturierten Daten werden überhaupt genutzt, und weniger als 1% der unstrukturierten Daten können analysiert werden. Also werden all diese Daten gehortet, aber die Organisationen nutzen sie größtenteils nicht. Ein weiteres Problem ist, wer Zugriff auf all diese gespeicherten Daten hat. Es ist alarmierend, dass 70% der Mitarbeiter Zugriff auf Daten haben, auf die sie wahrscheinlich keinen Zugriff haben sollten. Genau wie die Stapel eines Sammlers Brandgefahren im Haus schaffen, stellen unorganisierte Datenhaufen Sicherheitsrisiken in einer Organisation dar.\n\n## Datenexplosion\n\nWas können wir gegen dieses Problem tun? Zuerst müssen wir verstehen, warum es zu einer derartigen Datenexplosion kommt. Mit dem Internet der Dinge (IoT) ist alles miteinander verbunden, und wir haben Datenverarbeitung an verschiedenen Orten. Das schiere Volumen an generierten Daten ist unglaublich. Dies wird durch regulatorische Fragen verschlimmert; es ist schwer zu wissen, was wir loswerden können und was nicht. Wir füllen unseren Speicherplatz und kaufen dann mehr. Die Technologie ermöglicht im Grunde genommen unsere Datensammlung. Wir müssen uns anschauen, was wir anders machen können.\n\n## Warum wir horten\n\nExperten sagen, dass Menschen hamstern, weil sie glauben, dass ein Gegenstand in der Zukunft nützlich oder wertvoll sein wird, einen sentimentalen Wert hat, einzigartig und unersetzbar ist oder ein zu gutes Schnäppchen ist, um es wegzuwerfen. Diese gleichen Ideen gelten auch für das Datenhamstern. Zum Beispiel, warum habe ich eine der ersten Präsentationen, die ich jemals gehalten habe? Sie ist auf einem Laufwerk gespeichert und in der Cloud. Ich schaue sie vielleicht einmal im Jahr an, aber ich habe keinen wirklichen Grund, sie außer dem sentimentalen Wert aufzubewahren. Eine Organisation besteht aus Individuen mit diesen Verhaltensweisen und auf allen Ebenen des Datenhamsterns gibt es die Angst, Daten loszuwerden.\n\n## Werde datenzentriert.\n\nWie sieht es aus, ein datenzentriertes Unternehmen anstelle eines Speicherunternehmens zu werden? Hier ist ein vierstufiger Prozess, um eine datenzentrierte Organisation zu werden.\n\n## Organisatorische Grundlagen\n\nDer erste Schritt besteht darin, eine datenzentrierte organisatorische Grundlage zu schaffen. Es gibt vier Schlüsselakteure mit unterschiedlichen Rollen.\n\nChief Data Officer: Die Aufgabe des Chief Data Officers besteht darin, die Strategie und Regulierung für das Datenmanagement und die Generierung wertvoller Geschäftsinformationen festzulegen. Diese Rolle unterscheidet sich von der des Chief Information Officers, der sich mehr auf die Infrastruktur als auf die Informationen selbst konzentriert. Dies ist eine anspruchsvolle Aufgabe, da der Chief Data Officer in den kulturellen Wandel involviert ist. Sie versuchen, Menschen davon abzuhalten, Daten anzuhäufen, und stattdessen diese zur Schaffung realen Geschäftswerts zu nutzen.\n\nData Scientist: Data-Scientisten entwickeln Modelle und Entwürfe, indem sie Muster in den Daten finden und prädiktive Analysen verwenden. Die Bemühungen der Data-Scientisten können jedoch einfach nur zu einem einmaligen wissenschaftlichen Experiment werden, es sei denn, die Informationen werden operativ umgesetzt.\n\nDaten-Ingenieur: Hier kommen Daten-Ingenieure ins Spiel. Sie verwalten Datenpipelines und implementieren Analysen in der Praxis. Mit dem Eintreffen neuer Daten werden neue Erkenntnisse generiert, ohne jedes Mal von vorne anzufangen.\n\nData-Steward: Der Data-Steward verwaltet die Verwaltung und den Zugriff auf Datenassets und stellt sicher, dass die richtigen Personen zum richtigen Zeitpunkt den richtigen Zugang haben.\n\nMit einer Organisation, die diese vier Rollen einschließt, ist die nächste große Frage, ob die Operationen zentralisiert oder verteilt werden sollen. Zum Beispiel funktioniert möglicherweise bereits ein verteiltes Matrixmanagement in Ihrer großen Organisation, aber eine kleinere Organisation benötigt möglicherweise mehr Struktur und würde von einer stärker zentralisierten Struktur profitieren.\n\n## Datensammlung und -vorbereitung\n\nEin Artikel der Harvard Business Review verwendet effektiv eine Sportanalogie, um zwei Strategien zur Katalogisierung von Daten zu beschreiben: Verteidigung und Angriff. Bei der Verteidigung geht es darum, die Daten zu schützen. Beim Angriff geht es darum, schnellstmöglich voranzukommen, um Punkte zu erzielen.\n\n## Datenverteidigung und -angriff.\n\nMit einer defensiven Strategie konzentriert sich die Organisation hauptsächlich auf Datensicherheit, -verwaltung und -compliance. Der Schutz der Daten steht im Vordergrund. Die Hauptaktivität im Bereich Daten wird auf Extraktion, Standardisierung, Speicherverwaltung und Zugriffsverwaltung basieren. Typischerweise wird bei dieser Strategie eine stärker zentralisierte Organisation verwendet und es wird eine einzige Datenquelle verwendet.\n\nMit einer offensiven Strategie konzentriert sich die Organisation in erster Linie darauf, sich schnell zu verbessern und eine möglichst profitable Wettbewerbsposition einzunehmen. Die Datenaktivitäten werden auf Extraktion, Modellierung, Visualisierung, Transformation und Anreicherung ausgerichtet sein.\n\nDiese Strategie erfordert mehr Flexibilität, was eine stärker verteilte Organisation mit mehreren Wahrheitsversionen bedeutet.\n\nDas Verständnis, wie man die Daten basierend auf der Strategie verwendet, ist wichtig. Oftmals sind Organisationen unschlüssig und es kann verwirrend werden. Obwohl jede Organisation in der Lage sein muss, sowohl Abwehr als auch Angriff zu spielen, muss sie eine Strategie wählen, anstatt beides zu versuchen, genau wie professionelle Fußballspieler nicht auf beiden Seiten des Balls spielen.\n\n## Analystisches Verständnis\n\nEs gibt eine organisatorische Reife-Kurve für Analytics und die Generierung von Erkenntnissen aus Ihren Daten. Der Schlüssel liegt darin zu verstehen, an welchem Punkt Ihre Organisation derzeit steht und welche nächsten Schritte erforderlich sind, um auf der Kurve voranzukommen.\n\nIn der Phase der deskriptiven Analytik versuchen Sie einfach herauszufinden, was vor sich geht. Im diagnostischen Schritt finden Sie heraus, warum etwas passiert ist. Im prognostischen Schritt können Sie basierend auf historischen Daten vorhersagen, was in Zukunft passiert. Hier streben viele Organisationen danach zu sein, aber die ersten beiden Schritte müssen zuerst erledigt werden. Über dem prognostischen Schritt liegt das präskriptive Stadium, in dem Sie verstehen können, warum etwas passieren wird und die Organisation entsprechend den Erwartungen führen können. An der Spitze der Schritte steht die analytische Erkenntnis oder Vorhersehung, wo Sie Dinge geschehen lassen, sogar über die Verordnung hinaus voranschreiten.\n\nEin Grund, warum es wichtig ist, zu verstehen, wo Ihre Organisation derzeit steht, besteht darin, dass es für jede Phase spezifische Tools gibt. Zum Beispiel möchten Sie nicht in einem KI-Projekt stecken bleiben, das präskriptive oder sogar prädiktive Algorithmen verwendet, wenn Ihre Organisation noch im beschreibenden Schritt ist.\n\n## Es operationalisieren\n\nUm das Ziel der Operationalisierung oder der Schaffung eines wiederholbaren Prozesses zu erreichen, gibt es drei Schlüsselelemente: eine datenzentrierte Infrastruktur, Datenpipelines und Geschäftsabläufe.\n\nDie datenzentrische Infrastruktur ermöglicht es Ihnen zu wissen, wo sich alle Ihre Daten befinden und was sich in diesen Daten befindet, durch verschiedene Tools wie z.B. einem Metadaten-Manager wie Elastic Search oder Metadaten-Katalogen und -Repositories. Datenpipelines verfügen über großartige Tools, um den Prozess vom Eingang bis zur Analyse und zur Aktion zu ermöglichen. Eine defensive oder offensive Strategie bestimmt, welche Tools Sie in Ihrer Pipeline verwenden werden. Das letzte Element, der Geschäftsfluss, ist dort, wo das geschäftliche Verständnis Ihrer Daten und Prozesse die Bereitstellung eines kontinuierlichen Verbesserungsprozesses ermöglicht, um wiederholbare, wertvolle Erkenntnisse sicherzustellen.\n\n## Aufruf zur Tat\n\nAls erstes sollten Sie eine Datenstrategie entwickeln. Organisieren Sie sich und finden Sie heraus, wo sich all Ihre Daten befinden, und katalogisieren Sie diese. Entscheiden Sie sich für eine defensive oder offensive Strategie und gehen Sie dann schrittweise bei der Analyse vor, wobei Sie die richtigen Werkzeuge verwenden. Am wichtigsten ist es, Ihre Erkenntnisse in die operative Praxis umzusetzen, um den größtmöglichen Geschäftswert zu erzielen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT19-de","image":"./episodes/edt-19/de/thumbnail.png","lang":"de","summary":"In dieser Episode untersucht Darren Pulsipher, Intel Chief Solution Architect für den öffentlichen Sektor, wie Organisationen den Übergang von der reinen Anhäufung von Daten zur Nutzung als strategischem Vermögen bewerkstelligen können."},{"id":88,"type":"Episode","title":"Folge dem Bit","tags":["iot","dod","edge","technology","data","cybersecurity"],"body":"\r\n\r\n## Internet der Dinge ist der Anfang.\n\nDie weitverbreitete Implementierung des Internets der Dinge (IoT) dauert länger als von der Branche erwartet. Viele geben die Verzögerungen bei der Einführung und Bereitstellung von 5G weltweit dafür verantwortlich, aber es gibt ein weiteres Problem, das die Einführung robuster IoT-Systeme verlangsamt: das Management der Komplexität. Da Daten vom Rand über den aggregierten Rand, das Netzwerk, das Rechenzentrum und die Cloud durch das System fließen, ist die Absicherung der Daten eine große Sorge, da die Angriffsfläche außerhalb des traditionellen Rechenzentrums zunimmt. Es gibt punktuelle Lösungen, die diese Probleme verbessern, aber es gibt noch keine vollständige Lösungsarchitektur, die alle Probleme mit diesem datenzentrumlosen Ansatz löst.\n\n## Komplexität der DoD-Umgebung\n\nDas Internet der Dinge (IoT) ist bereits komplex und das US-Verteidigungsministerium (DOD) erhöht die Komplexität durch die Art der Produzenten und Verbraucher der Daten. Auf der Produzentenseite werden Sensoren an Satelliten, Flugzeugen, Schiffen und Fahrzeugen angebracht; sogar die Kriegskämpfer selbst sind im Grunde genommen mobile Rechenzentren. Die Menge an Daten, die diese Edge-Geräte erzeugen, kann ein Netzwerk überfordern. Die Anzahl der heterogenen Geräte kann das Management scheinbar nahezu unmöglich machen, insbesondere wenn die Geräte mit unterschiedlichen Protokollen kommunizieren und verschiedene Klassifizierungsstufen haben. Mit anderen Worten, das Sammeln der Daten ist nicht das Problem; die Herausforderung besteht darin, sie schnell und in einem nützlichen Format in die Hände der Entscheidungsträger zu bringen.\n\nDarüber hinaus kann die Konnektivität bei Edge-Geräten problematisch sein. Feindliche Umgebungen, in denen die Netzwerkkonnektivität zu einem Rechenzentrum nicht vorhanden oder bestenfalls instabil ist, können die Datenverarbeitung verzögern. Das bedeutet, dass eine Lösung in diesem Bereich sowohl im verbundenen als auch im getrennten Betriebsmodus funktionieren muss. Die Liefergeschwindigkeit ist ein entscheidender Erfolgsfaktor; Leben können auf dem Spiel stehen, wenn Entscheidungen aufgrund verzögerter Daten getroffen werden.\n\n## Gemeinsamer Rahmen für Anwendungen, Daten und Sicherheit.\n\nOffensichtlich benötigen wir Lösungen für diese Probleme. Von größter Bedeutung ist ein gemeinsamer Rahmen zur Bewältigung der Komplexität dieser neuen IT-Architekturen, die außerhalb der traditionellen Rechenzentren liegen. Der Rahmen muss sich mit der Verwaltung von Anwendungen, Daten und Sicherheit befassen. Wir müssen in der Lage sein, tragbare und wiederverwendbare Anwendungen überall im System bereitzustellen, vom Rand bis zur Cloud: die \"write once, run anywhere\" Doctrin. Dies ermöglicht es uns, Anwendungen schnell zu entwickeln, zu testen und bereitzustellen, ohne alle Kombinationen von Hardware-Konfigurationen im Ökosystem einrichten zu müssen. Die Verwendung von Tools im Container-Ökosystem sollte dabei helfen. Tools basierend auf Kubernetes (K8s) sind eine gute Wahl, da sie zum de facto Standard in der DevOps-Gemeinschaft geworden sind.\n\nDie Verwaltung von Anwendungen in Isolation reicht jedoch nicht aus. Alle Anwendungen benötigen in gewisser Weise Daten. Daher ist es entscheidend, zu verstehen, wo sich die Daten befinden, wohin sie gehen und wie sie klassifiziert sind, um erfolgreiche Lösungen zu erreichen. Wir benötigen eine gemeinsame Betriebsumgebung zur Verwaltung und Kontrolle der verschiedenen Datenkategorien wie Domänen, Sicherheitsgrenzen, Governance, Datenlebenszyklus-Management und Datennähe. Eine gemeinsame Betriebsumgebung erhöht die Flexibilität und Geschwindigkeit bei der Bereitstellung von Anwendungen.\n\nEs ist auch ein gemeinsamer Sicherheitsrahmen erforderlich. Die entscheidende Frage ist, wie Sie Ihre Daten in all ihren Formen sichern und dennoch teilen. Es gibt derzeit Hardware- und Softwarelösungen sowie eine kontinuierliche Weiterentwicklung in diesem Bereich. Grundlegende Sicherheitslösungen wie Verschlüsselung sollten die Basis bilden. Natürlich erfordert dies die richtige zugrundeliegende Engine für Speicherung und Leistungsfähigkeit. Eine weitere Sorge besteht darin, dass fehlerhafte oder böswillige Daten in das System gelangen. Die Etablierung eines Vertrauensanker als Grundlage ist auch in diesem großen Ökosystem erforderlich.\n\n## Verarbeitung am Rand, Rechenzentrum und in der Cloud.\n\nWo kommt Intel in dieser Umgebung ins Spiel? Wir können die zugrunde liegende Infrastruktur bereitstellen, die diese Systeme in Leistung und Energie unterstützt. Egal, ob Sie Sensordaten in einer energieeffizienten Umgebung (denken Sie an Atom- und maßgeschneiderte ASIC-Designs) verarbeiten oder Künstliche Intelligenz Training oder Inferenz in Ihrem Rechenzentrum durchführen (Xeon- und neuromorphes Computing), Intel hat einen Prozessor, der helfen kann, Rohdaten in wertvolle, handlungsorientierte Informationen umzuwandeln - die Schlüsselkomponente in dieser komplexen, auf Missionen ausgerichteten Umgebung.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT2-de","image":"./episodes/edt-2/de/thumbnail.png","lang":"de","summary":"In dieser Episode interviewt Darren Greg Clifton, den Direktor des Department of Defense (DOD) und der Intelligence für Intel Corp. Sie diskutieren die Herausforderungen des Datenmanagements in einem komplexen System, das mehrere Clouds, Unternehmensrechenzentren, regionale Rechenzentren und taktische Randgebiete umfasst. Hören Sie Darren und Greg zu, wie sie einem Datenfragment von der Erfassung über seine Reise durch dieses Ökosystem bis zur Erzeugung verwertbarer Informationen für Analysten und Kämpfer folgen. Hören Sie Darren und Greg dabei zu, wie sie einige der Hindernisse in dieser großen, zirkulären Umgebung besprechen und Lösungen diskutieren, um verwertbare Informationen zu Analysten und zurück zu Kämpfern zu bringen."},{"id":89,"type":"Episode","title":"Die Zerstörung der Komplexität der Speicherstufen","tags":["data","optane","technology","storage","vastdata"],"body":"\r\n\r\n## Dreißig Jahre Speicher-Komplexität.\n\nSpeicherung mag ein altes Konzept sein, aber VAST Data hat sich in nur etwas mehr als einem Jahr nach Einführung der Einnahmeversand zu Einhornstatus katapultiert. VAST hat die alte Architektur von mehrstufigen Speichersystemen durch ein einziges, monolithisches System ersetzt, das schnell und kosteneffektiv ist. Die VAST-Lösungen beseitigen auch das Problem großer, unübersichtlicher Speichersysteme, die aus verschiedenen Dateisystemen und Architekturen bestehen.\n\n## Eine Stiftung für eine neue Architektur\n\nDer Gründer von VAST betrachtete den Speicher aus einer völlig neuen Perspektive. Er stellte fest, dass Kunden nicht unbedingt mehr Leistung von Flash benötigten, sondern stattdessen Unterstützung für Datei- und Objektspeicherung zu geringeren Kosten. Er machte sich drei Technologien zunutze, die vor 2018 nicht existierten. Erstens NVMe über Fabrics, das als Art hyperskalares SAN verwendet wird, um alles mit geringer Latenz zu verbinden. Als nächstes QLC-Flash, da es kostengünstig ist und keine beweglichen Teile wie Festplatten hat. Drittens Intels 3D XPoint Optane. Optane bietet Read-Write-Parität und hohe Ausdauer zu einem vernünftigen Preis. Die Kombination dieser drei Technologien ermöglicht es VAST, Kunden eine All-Flash-Leistung zu bieten, aber zu einem Preis, der vergleichbar ist mit dem einer Festplatte. Es beseitigt die Notwendigkeit für andere Ebenen, die Organisationen kaufen, weil sie günstig sind.\n\n## Vorstellung von Vast Data Universal Storage\n\nDas VAST-System ist die erste disaggregierte, gemeinsam genutzte Architektur für alles. Das bedeutet, dass die Logik vom Zustand des Dateisystems getrennt ist. Stattdessen existiert der Zustand des Dateisystems in Datenspeichern, die QLC-Flash und Optane enthalten. Die Optane wird als großer Metadatenspeicher verwendet. Sie ist vielseitig einsetzbar, genau wie das System, das als Universal Storage vermarktet wird. Mit diesen Speichern gibt es keinen einzelnen Ausfallpunkt, wodurch die Kapazität theoretisch unbegrenzt ist. (VAST hat das System bei etwa 50 Petabyte in einem einzigen Dateisystem getestet.) Sie können die Kapazität erweitern, indem Sie einfach kostengünstige x86-Server in die Cluster hinzufügen. Da es sich um ein paralleles Dateisystem handelt, kann jeder Benutzer über einen beliebigen Server auf beliebige Daten zugreifen, als wären sie direkt angeschlossen. Dadurch können Sie weiterhin skalieren.\n\nSie können die Leistung unabhängig von der Kapazität skalieren. Das einzige, was die Leistung des Flashs behindern würde, ist die CPU. Durch die nahtlose Skalierung der Anzahl der CPUs im Cluster können Sie die Leistung steigern.\n\nEines der Probleme, das mit dieser Struktur gelöst wird, ist die Latenz. Viele Organisationen benötigen eine geringe Latenz für alle ihre Daten. Da jeder dieser statenlosen Server Zugriff auf alles hat, haben Sie schnellen Zugriff auf alle Daten.\n\n## DASE-Architektur: Server-Pools\n\nEin weiterer großer Vorteil ist, dass es einfach ist, den Speicher einer Organisation feinabzustimmen. Die zusammensetzbare Natur der zustandslosen Server und das Fehlen von Kommunikation zwischen ihnen ermöglicht es Ihnen, einen Cluster so aufzubauen, dass er Ihren Bedürfnissen am besten entspricht. Sie können zum Beispiel Ihre Steuerungseinheiten je nach unterschiedlicher Arbeitsbelastung segmentieren, aber sie haben alle Zugriff auf dieselben Daten.\n\nDarüber hinaus funktioniert das System gut mit einer Organisation, die verschiedene Klassifikationsebenen benötigt, um auf die Daten zuzugreifen. Sie haben die Möglichkeit, zu segregieren, auf welche Benutzer zugreifen können, indem Sie mehrere Zugangszonen mit virtuellen IP-Adressen erstellen. Eine der Herausforderungen bei NFS besteht darin, dass es im Grunde an alles broadcastet. Wenn Sie die Broadcasts auf eine Teilmenge von IP-Adressen beschränken, erhalten Sie die Möglichkeit, diese verschiedenen Architekturen in separate Systeme aufzuteilen.\n\n## Universelle Speicherbrücken-Anwendungseras\n\nDies ist nicht nur eine Lösung für HPC-Cluster; sie ist nicht kostenintensiv. Viele Unternehmen nutzen VAST zunächst zur Sicherung, um Vertrauen aufzubauen. Zum Beispiel hat das National Cancer Institute ein Bandbibliotheksarchiv und wollte auf die Informationen schneller zugreifen können. Sie haben sich verschiedene Plattformen angesehen und VAST kam mit einem niedrigeren Preispunkt und für All-Flash, was schneller ist als ihr Produktions-NAS-System. Die Lösung hat also einen guten Preis und ist nützlich für allgemeines Dateifreigeben und eine Vielzahl von Workloads wie Künstliche Intelligenz, Protokollanalyse, Splunk, usw., nicht nur für HPC. Die VAST-Lösungen sind einfach zu verwalten und wirklich universell.\n\nVAST ist ein junges Unternehmen, aber sie haben mehrere Installationen in Regierungsbehörden wie dem National Institute of Health und den Tri-Labs des Department of Energy, wo Spitzenleistung für diese Supercomputer erforderlich ist. Dies ist ein leistungsstarkes System in einigen der größten HPC-Umgebungen der Welt, das missionkritische Anwendungen unterstützt.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Randy Hayes"],"link":"/episode-EDT20-de","image":"./episodes/edt-20/de/thumbnail.jpg","lang":"de","summary":"Randy Hayes von VAST Data und Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel, diskutieren die innovative Speicherarchitektur von VAST Data, die den Bedarf an Ebenen durch die Verwendung von NVMe über Fabrics, QLC-Flash und 3D XPoint Optane beseitigt."},{"id":90,"type":"Episode","title":"Großer Speicher Software Definierter Speichercontroller","tags":["bigmemory","edge","compute","technology","memverge","optane","pmem","data"],"body":"\r\n\r\nDie 3D XPoint Optane persistente Speichertechnologie von Intel war ein Meilenstein für MemVerge und ermöglichte es ihnen, softwaredefinierten Speicher zu entwickeln. Dieser wird als Ausdruck der Leistungsfähigkeit von Optane PMEM beschrieben.\n\n## Die beste Art und Weise, PMEM zu nutzen.\n\nMemVerge hat Big Memory-Software entwickelt, um der wachsenden Nachfrage von Anwendungen und Unternehmen gerecht zu werden, Daten in immer größeren Mengen und Geschwindigkeiten zu verarbeiten. Immer mehr Echtzeitanwendungen benötigen sofortige Einblicke und Maßnahmen aus den Daten. Dies erfordert eine neue, speicherzentrierte Infrastruktur, um Latenzanforderungen zu erfüllen.\n\nAnwendungen, die KI, maschinelles Lernen oder Echtzeit-Big-Data-Analyse verwenden, verwenden in der Regel DRAM. Obwohl DRAM eine Latenz im Nanosekundenbereich aufweist und eine gute Leistungsfähigkeit und Kapazität bietet, hat es physische Einschränkungen wie Speicherdichte und wie viel in einem Server Platz findet. Es ist auch relativ teuer. Am wichtigsten ist jedoch, dass es flüchtig ist, und mit zunehmender Datenmenge wird die Speicherflüchtigkeit zu einer Einschränkung. PMEM beseitigt diese Engpässe, da Sie eine größere Kapazität, niedrigere Kosten und Persistenz haben können.\n\nEin wichtiger Vorteil besteht darin, dass Unternehmen ihre Anwendungen nicht neu schreiben müssen, um die Technologie von MemVerge zu nutzen. Das Unternehmen suchte nach der besten Möglichkeit, Persistenz zu nutzen, und die Antwort bestand darin, die Programmierparadigmen so wenig wie möglich zu stören. Als Optane PMEM als Speicherformfaktor verfügbar wurde, bot sich die Gelegenheit, wertvolle Software als Teil der Lösung zu entwickeln.\n\n## Software Defined Memory\n\nSoftware Defined Memory (SDM) ist eine Technologie, die es ermöglicht, den Zugriff auf Speicherressourcen in einem Computer durch Software zu verwalten und anzupassen. SDM ist eine Art von Speichervirtualisierung, bei der Software die Kontrolle über die Speicherressourcen übernimmt und sie nach Bedarf zuweist und konfiguriert.\n\nMit SDM können Entwickler und Systemadministratoren den Speicher in ihrem System optimieren und anpassen, um die Leistung und Effizienz zu verbessern. Durch die Bereitstellung dynamischer Speicherlösungen können sie den Bedarf an Speicher an die aktuellen Anforderungen anpassen und so Engpässe oder Ressourcenverschwendung vermeiden.\n\nSDM kann auch die Integration von verschiedenen Arten von Speichergeräten erleichtern, wie zum Beispiel Festplatten, Solid-State-Laufwerke oder Cloud-Speicher. Dadurch haben Benutzer die Flexibilität, verschiedene Speichertypen je nach Anwendungsanforderungen zu nutzen.\n\nInsgesamt ermöglicht Software Defined Memory eine effizientere Verwaltung und Nutzung der Speicherressourcen in einem Computing-System, was zu einer besseren Leistung und Flexibilität führt.\n\nEs findet eine Datenverarbeitung statt. Aus Sicht der Hardware wird es in den nächsten zehn Jahren wahrscheinlich eine heterogenere Welt geben, sowohl in der Berechnung als auch in den Speicherlementen. Es wird ein neuer Übertragungsstandard wie CXL entstehen, der diese Elemente miteinander verbindet. Die größte Herausforderung wird sein, das Anwendungsökosystem zu bewegen. Eine Softwareebene ist erforderlich, um sie in verdauliche, zusammenfügbarere Teile umzuwandeln, die es der Anwendung erleichtern. Die softwaredefinierte Speicherlösung von MemVerge wird ein wichtiger Bestandteil dieses Bereichs sein.\n\nIn der Zwischenzeit baut MemVerge eine Brücke zwischen den aktuellen und zukünftigen Paradigmen. MemVerges Softwaredefinierter Speicher bringt dynamische SLA und QLS, Widerstandsfähigkeit, Anwendungsbeständigkeit, Effizienz und Leistung. Dies sind alles Dinge, die Sie normalerweise mit einem softwaredefinierten Speichernetzwerk erhalten, aber nun mit einem Speicher mit geringerer Latenz und extrem schneller Geschwindigkeit erhalten können.\n\n## Leistung auf der Memory Machine\n\nZwei wichtige MEMVerge-Geistige-Eigentumsrechte sind im Einsatz. Das erste ist eine Software-Virtualisierungsschicht, die die Leistung des Speichers - die Kombination aus PMEM und DRAM - optimiert, die sehr nahe an der DRAM-Leistung liegt. Jede Workload hat unterschiedliche Leistungsprofile, die durch Mischen von PMEM und DRAM in unterschiedlichen Verhältnissen feinabgestimmt werden können. Diese Feinabstimmung von PMEM und DRAM ermöglicht es Anwendungsentwicklern und IT-Profis, den Speicher an ihre Anwendungen anzupassen, anstatt für die gesamte Maschine. Anstatt die vorhandenen DRAM- und PMEM-Verhältnisse für die gesamte Maschine zu konfigurieren, können Sie die Verhältnisse von PMEM und DRAM nun dynamisch ändern, je nach Arbeitslast, und sogar die Leistung auf DRAM-Niveau übertreffen.\n\n## ZeroIO Schnappschuss (Persistieren von Anwendungsdaten)\n\nDie zweite dieser Erfindungen ist der Speicher-Snapshot oder ZeroIO. Es ermöglicht das Fortbestehen bestehender flüchtiger Anwendungen ohne Neuschreibungen. Es funktioniert auf der Grundlage des softwaredefinierten Speichers, der ein flüchtiger Speicherdienst ist. Obwohl der zugrundeliegende PMEM persistent ist, ist ein flüchtiger Speicher erforderlich, um bestehende Anwendungen nicht zu beeinträchtigen. Die Persistenz wird durch Bereitstellung einer grafischen Benutzeroberfläche (GUI) und einer CI zur Verwaltung von Snapshots genutzt. Es gibt eine Snapshot-Funktionalität, mit der der Zustand einer gesamten Anwendung sofort erfasst werden kann. Diese Anwendung kann dann jederzeit in der Zukunft wiederhergestellt werden.\n\nDies macht Ihren Speicher nicht nur persistent, sondern auch hochverfügbar. Nach einem Absturz können Sie eine sofortige Wiederherstellung durchführen. Wenn Sie einen Fehler in der Datenbank machen, können Sie zu einem früheren Punkt zurückkehren. Sie können auch über dem Snapshot klonen, sodass Sie neue Instanzen einer Anwendung erstellen können, ohne den Speicher physisch zu replizieren. Dadurch können Sie mehrere unabhängige Prozesse erstellen, bei denen die logischen Speicherbereiche auf den gleichen physischen Speicherbereich abgebildet werden. Dadurch wird nicht nur Speicher gespart, sondern auch der Prozess des Instant-Klonens ermöglicht. Diese neue Technologie macht viele Dinge möglich, die zuvor unmöglich waren.\n\nEs gibt keine architektonischen Änderungen an Ihrem Programm, aber wenn Sie etwas persistieren müssen, können Sie es einfach als Schnappschuss speichern. Dies verändert nicht das vertraute Modell der Programmierung, beschleunigt jedoch erheblich die Ein- und Ausgabe. Eine weitere großartige Funktion ist, dass Schnappschüsse verwaltbare Objekte werden, sodass sie an jedem Ort, an dem Sie die Anwendung neu starten können, transportierbar sind. Live-Migration kann auch in bestimmten Szenarien aktiviert werden.\n\n## Zukunftsfähige Anwendungsszenarien\n\nFür einen kleinen Einblick in die Zukunft plant MemVerge in etwa einem Jahr eine Version 2.0 mit einem SDK. Neben der Verwendung als transparente Speicherschicht werden neue Anwendungsentwickler eine neue Möglichkeit haben, ihre Daten dauerhaft zu speichern. Dadurch wird sowohl die Anwendungsentwicklung als auch die Modifikation bestehender Anwendungen vereinfacht.\n\nMit dem SDK können Entwickler Segmenten des Anwendungsspeichers oder des kompletten Speicherprofils speichern, wodurch der Anwendungsentwickler die Möglichkeit hat, Speicher mit den umständlichen ORM- oder Speicherzuordnungstechnologien von heute dauerhaft speichern zu können.\n\nIn Partnerschaft mit Intel wird MemVerge am 23. September 2020 die erste Version ihres Produkts mit dem softwaredefinierten Speicher und den Snapshot-Funktionen in der allgemeinen Verfügbarkeit einführen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Charles Fan"],"link":"/episode-EDT21-de","image":"./episodes/edt-21/de/thumbnail.png","lang":"de","summary":"Charles Fan, CEO von MemVerge, spricht mit Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel, über ihre neue Technologie, Big Memory softwaredefinierte Speichercontroller. Die Technologie nutzt den Intel 3D XPoint Optane persistenten Speicher, um effizient die Kluft zwischen aktuellen und zukünftigen Architekturen zu überbrücken und dabei eine größere Kapazität, geringere Kosten und Persistenz zu bieten."},{"id":91,"type":"Episode","title":"Ein Argument für Multi-Hybrid Cloud.","tags":null,"body":"\r\n\r\n## Aktuelle Cloud-Umgebung.\n\nIn den letzten fünf Jahren hat es eine grundlegende Veränderung in der IT-Umgebung gegeben. Das kontinuierliche Wachstum der öffentlichen Cloud und das Aufkommen von privaten Cloud-Optionen haben viele CIOs und IT-Abteilungen ins Hintertreffen geraten lassen. In dem heutigen wettbewerbsintensiven Markt müssen viele Entwicklungsteams schneller vorankommen als die meisten IT-Abteilungen liefern können. Entwicklungsteams haben öffentliche Clouds wie AWS, GCE und Azure als eine praktikable Option für die altmodische \"Unter-dem-Schreibtisch\"-\"Hobby-IT\" gefunden. Die öffentlichen Cloud-Anbieter haben das schnelle und einfache Einrichten neuer Infrastrukturen ermöglicht. Es ist nicht mehr nötig, auf mehrere Ebenen technischer und geschäftlicher Genehmigungen, physischen Platz im Rechenzentrum und Lieferprobleme der Anbieter zu warten. Jetzt kann ein Entwicklungsteam innerhalb von Minuten die gesamte Infrastruktur für ihr neues Projekt bereitstellen.\n\nVor der öffentlichen Cloud konnten CIO(s) leicht \"umhergehen\" und die Anzahl der \"Cottage IT\" -Maschinen unter den Schreibtischen der Menschen zählen. Mit den physischen Maschinen, die den IT-Abteilungen nicht mehr sichtbar sind, ist es unmöglich, Teams und die Infrastrukturen ihrer Projekte zu identifizieren. Viele öffentliche Clouds haben Organisationen die Möglichkeit gegeben, die Buchhaltung von allen Konten für bestimmte Domains zu konsolidieren, aber die Sichtbarkeit darüber, was läuft und wer an der Infrastruktur arbeitet, ist immer noch eine Art \"Schnitzeljagd\". Oft werden diese \"Schurken\" -Projekte sichtbar, wenn Projekte marktfähig gemacht werden sollen und in eine unternehmenseigene Infrastruktur eingebracht werden müssen. Sicherheits-, Datenschutz- und Regulierungspolitiken können die \"Marktfähigkeit\" von Projekten nahezu unmöglich machen. Vor allem, wenn Entwickler ihre Anwendungen eng an die Cloud-Infrastruktur gekoppelt haben.\n\nZukunftsorientierte IT-Abteilungen geben ihr Bestes, um \"Cottage IT\" zu erfassen, indem sie mit Public Clouds und ISVs zusammenarbeiten, um \"Unternehmensportale\" in die Clouds zu integrieren. Das Einrichten eines Durchgangsportals ist ein guter Anfang, um Projekte mit Infrastruktur zu erfassen, aber viele Organisationen stellen fest, dass nur ein Portal die Entwicklungsteams nicht zufriedenstellt. In den letzten Jahren habe ich mit vielen dieser Organisationen zusammengearbeitet, um Anwendungsfälle, Architekturen und Technologien zu identifizieren, um diese erweiterten Portale zu entwickeln, die wir Hybrid Multi-Clouds (MHC) nennen. In der Regel werden drei Haupttechnologien integriert, um diese MHCs aufzubauen: Cloud Management Plattformen (CMPs), Automatisierungsframeworks und Platform as a Service-Tools (PaaS).\n\n## Cloud Management Platform (CMP) can be translated to German as: Cloud-Management-Plattform (CMP)\n\nDie Hauptverantwortung von Cloud Management-Plattformen besteht darin, mehrere heterogene öffentliche und private Clouds zu verwalten. Sie ermöglichen es den Endbenutzern, mehrere Clouds und ihre Infrastruktur von einer einzigen Oberfläche aus zu verwalten. CMPs sind in der Regel auf Cloud-Administratoren ausgerichtet. Obwohl das Hauptaugenmerk der Tools für Cloud Management-Plattformen auf der Verwaltung mehrerer Clouds liegt, haben viele Tools Zusatzfunktionen aus den Bereichen PaaS und Automatisierungsframeworks oder zumindest eine Plugin-Architektur zur Unterstützung.\n\n## Verwendete Anwendungsfälle\n\nVerwalten von öffentlichen Clouds\n\nVerwaltung von privaten Clouds\n\nVerwaltung von Cloud-Identitäten\n\nVerwaltung der Infrastruktur über mehrere Clouds hinweg.\n\n## Automatisierungs-Frameworks\n\nDas Hauptziel von Automatisierungs-Frameworks ist es, die Bereitstellung, Verwaltung und Aktualisierung von Software-Stacks auf der Infrastruktur zu automatisieren. Automatisierungs-Frameworks sind aus der DevOps-Community entstanden und konzentrieren sich in der Regel auf wiederholbare Prozesse. Viele dieser Tools umfassen Skriptsprachen, die es DevOps-Ingenieuren ermöglichen, Software und Dienste wiederholbar zu verwalten und zu konfigurieren. Viele DevOps-Teams sind mit diesen Tools gut vertraut.\n\n## Anwendungsfälle abgedeckt\n\nSoftware auf die Infrastruktur bereitstellen\n\nSoftware auf Infrastruktur verwalten\n\n* Software und Dienstleistungen verbessern\n\n## Plattform als Dienstleistung (PAAS)\n\nPlatform as a Service (PaaS) ist in erster Linie dafür verantwortlich, eine einzige Schnittstelle zum Wiederverwenden von Plattformen und ihrer Bereitstellung auf der Infrastruktur bereitzustellen. PaaS-Tools sind in der Regel stark abhängig von der Entwicklerperspektive, was zu unflexiblen Infrastrukturkonfigurationen führen kann. Viele dieser Tools verfügen über ein Webportal, das Entwicklern die Möglichkeit gibt, Dienste auszuwählen und sie in der Infrastruktur bereitzustellen.\n\n## Verwendete Anwendungsfälle\n\n* Dienste/Anwendungen bereitstellen/verwalten\n\nService-Katalog verwalten\n\nEntwickeln Sie neue Dienste/Anwendungen.\n\n## Konvergenz erzeugt Hybrid Multi-Cloud (HMC).\n\nWeil kein Werkzeugset alle Anwendungsfälle zur Verwaltung von Clouds, Anwendungen, Infrastruktur und Diensten abdeckt, investieren Teams mehrere \"Mannjahre\" in die Installation, Konfiguration und Integration dieser drei Werkzeugsets. Dies hat zur Entwicklung von Technologien geführt, die diese Werkzeuge integrieren, einschließlich neuer Produktangebote und neuer Funktionen in bereits verfügbaren Produkten.\n\nViele CMP-Produkte integrieren PaaS und Automatisierungs-Frameworks in ihre Lösungen. PaaS-Werkzeuge verwalten nun mehrere Clouds. Automatisierungs-Frameworks bieten zunehmend Webportale und Konnektivität zu verschiedenen Clouds an. Viele der Tools bewegen sich in Richtung der hybriden Multi-Cloud-Vision. Bei der Entscheidung, welche Tool(s) verwendet werden sollen, ist es wichtig, sich an die Ursprünge des Tools zu erinnern.\n\n## Bereitstellung einer Lösung\n\nDas hybride Multi-Cloud-Ökosystem ist noch recht neu und erfordert immer noch einige umfangreiche Integrationen zwischen den Tools. Es gibt einige Tools, die bereits vollständige Lösungen bieten, allerdings mit ihrer eigenen Sicht auf die Welt. Aufgrund der jungen Entwicklungsphase des Ökosystems gibt es viele Akteure und Auswahlmöglichkeiten. Die Zeit wird zeigen, wer diesen Bereich dominieren wird. Momentan wird es interessant sein, zu beobachten, wie die Tools zusammenkommen und eine einheitliche Basis schaffen, während die Funktionen ausgereift werden.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT22-de","image":"./episodes/edt-22/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher beschreibt, warum ein Multi-Hybrid Cloud Architekt möglicherweise bereits in Ihrem Rechenzentrum vorhanden sein könnte. Die meisten Organisationen haben bereits alle Zutaten. Sie müssen nur wissen, wie sie zusammenpassen."},{"id":92,"type":"Episode","title":"Erkenntnisschaffung in datenzentrierten Organisationen","tags":["aiml","dataarchitecture","datacentric","data","insight","technology"],"body":"\r\n\r\nUnter Verwendung des Beispiels der Herstellung von Widgets diskutierten Darren und Sarah zuvor über die Datenpipeline in dem Bestreben, eine grundlegende Geschäftsfrage zu beantworten: Wie viele Widgets sollen produziert werden? Die Komplexität wird deutlich, wenn es darum geht, die Rohdaten für die Kundennachfrage, die aktuelle Versorgung und den Ausschuss in Erkenntnisse umzuwandeln.\n\nDer erste Schritt in diesem Prozess, bevor Sie entscheiden können, welche Werkzeuge Sie verwenden möchten, besteht darin, die Daten in eine verwendbare Form vorzubereiten. Achtzig oder neunzig Prozent der Arbeit eines Datenwissenschaftlers besteht darin, die Daten vorzubereiten und zu transformieren, sodass sie beispielsweise in einen Algorithmus eingesetzt oder zur Mustererkennung verwendet werden können.\n\nDie richtigen Werkzeuge sind komplex. KI und maschinelles Lernen verwenden nicht nur einen einzigen Algorithmus, sondern eine Vielzahl von Werkzeugen, die Datenwissenschaftler verwenden und kombinieren, um die besten Erkenntnisse zu erhalten. Mit anderen Worten, ein einziger Algorithmus wird Ihnen nicht sagen, wie viele Widgets Sie herstellen sollten. Es gibt viele überlappende Teile und die Werkzeuge selbst sind komplex. Darüber hinaus haben Datenwissenschaftler unterschiedliche Fachgebiete; Datenwissenschaft ist ein Teamsport. Genau wie Sie keinen Netzwerkingenieur mit dem Entwurf der Speicherarchitektur beauftragen würden, würden Sie keinen Deep-Learning-Ingenieur beschäftigen, der auf Bilderkennung spezialisiert ist, um Ihr Ausbeuteanalyse-Problem zu lösen. Sie müssen die richtigen Datenwissenschaftler einstellen, um die richtigen Werkzeuge zu entwerfen und einzusetzen, um Einblicke in Ihre geschäftlichen Fragen zu erhalten.\n\nLassen Sie uns zurück zur Frage gehen, wie viele Widgets Sie herstellen sollten. Jede Situation und jedes Modell wird natürlich unterschiedlich sein, je nach den Arten von Fragen, Daten und Dynamiken, die Sie haben, aber wir werden dies als Ausgangspunkt nutzen. Sobald diese geschäftliche Frage feststeht, wird die Organisation eine Entwicklung der analytischen Reife durchlaufen.\n\nZuerst konzentrieren Sie sich darauf, was in der Vergangenheit passiert ist, um Muster in der Nachfrage nach Ihren Widgets zu erkennen. Zum Beispiel könnten Sie sich einige Zeitreihendaten ansehen, um herauszufinden, wann die Nachfrage nach Widgets steigt und fällt. Wie stabil sind die Informationen im Laufe der Zeit und wie können Sie sie nutzen, um die Zukunft vorherzusagen? Vielleicht könnten Sie hierzu etwas maschinelles Lernen durchführen, um verschiedene Zeitabschnitte zu betrachten und sie zu verstehen. Möglicherweise möchten Sie auch eine Textanalyse durchführen, um herauszufinden, ob Menschen in den sozialen Medien über Ihr Widget sprechen. Die Anzahl der Likes oder Shares könnte eine Datenquelle sein.\n\nSie könnten etwas Ähnliches mit der Versorgung tun. Wie stabil ist die aktuelle Versorgung und wie gut verstehen Sie die Systemdynamik? Schauen Sie sich an, wo Sie einen Menschen benötigen, um die Systemdynamik herauszufinden, und integrieren Sie dieses Wissen in Ihre Analysemethoden. Es könnten maschinelles Lernen Muster geben, die Ihnen Einblicke geben, daher könnten Sie etwas unüberwachtes Lernen betreiben. Unüberwachtes Lernen bedeutet festzustellen, ob es verschiedene Kategorien oder Segmente gibt, von denen Sie nichts wissen, die sich jedoch unterschiedlich verhalten. Fragen Sie, wie Sie besser verfolgen können oder eine bessere Auflösung dessen erreichen können, was in diesen Gruppen geschieht.\n\nKurz gesagt, je nachdem, woher die Daten stammen und worauf du in den Daten schaust, wirst du verschiedene Werkzeuge verwenden.\n\nErtragseinbußen sind ein Beispiel für die Komplexität der zu lösenden Probleme. Je mehr Variation in der Produktionslinie vorliegt, desto mehr Verschwendung kann auftreten. In diesem Fall wird maschinelles Lernen Segmente und Cluster verschiedener Arten von Erträgen betrachten. Wie lässt sich das quantifizieren und vorhersagen?\n\nEine Sache, die Datenwissenschaftler hier tun, ist das Design von Experimenten, um Kausalität zu schätzen. Indem man Knöpfe dreht und Hebel auf systematische Weise betätigt, kann man sehen, was mit dem Ergebnis geschieht, während man Prozesskontrollen hinzufügt, um Abweichungen zu vermeiden.\n\nEine weitere Gelegenheit zur Durchführung von Analysen ist die Zuverlässigkeit. Beispielsweise können Ihre Fertigungswerkzeuge mit präventiver Instandhaltung rechtzeitig gewartet werden, um Produktionsverluste zu verhindern. In bestimmten Situationen kann auch die Textanalyse eingesetzt werden, zum Beispiel wenn Sie schriftliche Aufzeichnungen von Beobachtungen und Lösungen von Technikern über einen längeren Zeitraum vorliegen haben, die als kollektive Wissensbasis genutzt werden können.\n\nTiefes Lernen im Bereich der Bilderkennung ist eine weitere Strategie, um Verluste durch Erkennen von Fehlern und Mängeln zu verhindern und möglicherweise sogar zur Kategorisierung von Defekten beizutragen.\n\nDas Ziel all dessen ist natürlich, wertvolle Geschäftseinblicke für Ihre Organisation zu gewinnen. Der Schlüssel dazu ist die Verpflichtung zu einer datenzentrierten Organisation, Flexibilität zu bewahren und die richtigen Tools und die richtigen Personen zu haben, um Ihre Daten in handlungsfähige Einblicke umzuwandeln.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT23-de","image":"./episodes/edt-23/de/thumbnail.png","lang":"de","summary":"Im Teil 5 einer Serie, Umgestaltung Ihrer Organisation in eine datenzentrierte Organisation mittels Kickstart, diskutieren Sarah Kalicin, Leitende Datenwissenschaftlerin bei Intel, und Darren Pulsipher, Chef-Solutions-Architekt im öffentlichen Sektor bei Intel, wie man mithilfe künstlicher Intelligenz und maschinellem Lernen Erkenntnisse in einer datenzentrierten Organisation schafft."},{"id":93,"type":"Episode","title":"Digitale Strategie und Datenarchitektur","tags":["data","dataarchitecture","aiops","secops","devsecops","devops","compute","technology","process"],"body":"\r\n\r\nEine Planung für die digitale Transformation Ihrer Organisation ist entscheidend, um sich nicht im Labyrinth der neuesten und besten Technologien und Prozesse zu verirren. Diese planlose Strategie führt dazu, dass Ihre digitale Transformation scheitert. Ein Plan oder eine Roadmap, die zeigt, wo sich Ihre Organisation derzeit befindet und wo Sie letztendlich landen möchten, ist der wichtigste Teil einer effektiven Transformation.\n\nDie drei Schlüsselelemente, die in Ihrem Plan koordiniert und ausgeglichen werden müssen, sind organisatorisch, verfahrenstechnisch und technologisch.\n\n## Organisationale Abteilungen\n\nUm organisatorischen Wandel zu verstehen, wollen wir uns zunächst die üblichen Gruppen in den meisten Organisationen ansehen.\n\n## Entwicklung\n\nDiese Gruppe ist Ihr Tages-Team; sie entwickeln neue Produkte. Die Prozesse des Entwicklungsteams sind bekannt und ausgereift. Ihr Fokus liegt auf der Entwicklung, dem Testen und dem Einsatz des Produkts in der Produktion. Nahezu jedes Team verwendet eine Form von Agile oder schneller Iterationstechnik.\n\n## IT - Informationenstechnologie\n\nDas Ziel des IT-Teams besteht darin, die Infrastruktur hinsichtlich Kosten und Effizienz zu optimieren. Sie stellen sicher, dass die Infrastruktur zuverlässig ist und über eingebaute Kontroll- und Sicherheitsfunktionen verfügt. In erster Linie betrachten sie Berechnungen, Speicherung, Netzwerk, Compliance und Kosten.\n\n## Sicherheit\n\nDie Rolle der Sicherheit hat in den letzten Jahren zunehmend an Bedeutung gewonnen, insbesondere mit den Mitarbeitern, die aufgrund von COVID-19 von zu Hause aus arbeiten. Das Sicherheitsteam konzentriert sich hauptsächlich auf die Sicherung des geistigen Eigentums, der Daten und der Infrastruktur. Zu den gängigen Werkzeugen gehören Identitätsmanagement, Schutz, Erkennung und Behebung. Ein grundlegendes Verständnis dieser Werkzeuge ist wichtig für Ihre organisatorische Veränderung.\n\n## Daten\n\nDiese neueste Gruppe, die früher vielleicht ein Statistiker oder Mathematiker war, der etwas Datengewinnung betrieben hat, hat nun mit dem Aufkommen von Chief Data Officers und den um sie herum aufgebauten Organisationen Fuß gefasst. Sie konzentrieren sich darauf, Daten zu analysieren, zu kategorisieren und echten Mehrwert aus Ihren Daten zu liefern. Egal, ob Ihre Organisation in der Produktion oder im Dienstleistungssektor tätig ist, oder ob Sie versuchen, neue Kunden zu gewinnen oder Geld zu sparen, gibt es viele Bereiche, in denen Datenwissenschaftler Mehrwert bieten können.\n\n## Die Gruppen zusammenbringen.\n\nUm einen effektiven digitalen Wandel herbeizuführen, müssen all diese Gruppen ein gegenseitiges Verständnis dafür haben, was jeder einzelne beiträgt und eine einheitliche Vision haben. Sie möchten nicht, dass Ihre Datenwissenschaftler Daten erkunden, die für die Entwicklung oder IT keinen Wert haben. Sie möchten nicht, dass Ihr Sicherheitsteam alles so stark absichert, dass das Entwicklungsteam nicht arbeiten kann. Unter den Gruppen müssen gemeinsame Strategien, Prozesse und Architekturen vorhanden sein.\n\nObwohl gemeinsame Ziele und Ergebnisse ideal sind, gibt es Hindernisse zu überwinden. Einer der schwierigsten Bereiche bei organisatorischem Wandel ist der Kommunikationsbruch an den Schnittstellen. Manche Organisationen haben neue Gruppen geschaffen, um die Übergänge zwischen den Gruppen zu erleichtern. Zum Beispiel könnte es zwischen Sicherheit und IT eine \"Sec Ops\" oder \"Sec Dev Ops\" geben, wo die Richtlinien und Verfahren automatisiert werden, die vom Sicherheitsteam entwickelt wurden. Ein weiteres Beispiel wäre ein Data Dev Team, bestehend aus Entwicklern, die mit Datenwissenschaftlern zusammenarbeiten, um durch Anwendungs- und Toolentwicklung oder Integration wiederholbare Prozesse bereitzustellen. Gemeinsame Architekturen und gemeinsame Tools, auf die alle Gruppen sich verlassen können, machen den Prozess und die Veränderungen viel einfacher.\n\n## Das perfekte System\n\nWie würde eine gemeinsame Architektur aussehen? Eine utopische Version existiert heute nicht, aber wir können uns die Elemente ansehen und vielleicht etwas in Richtung dieses Ideals aufbauen.\n\nSelf-Service ist ein Muss. Wenn ein Datenwissenschaftler zum Beispiel mehr Speicherplatz für Daten benötigt, müssten sie nicht IT anrufen und eine Reihe von Formularen ausfüllen usw. Stattdessen könnten sie ein zeitsparendes Self-Service-Portal nutzen, das den Speicher bereitstellt. Natürlich würde das Portal richtlinienbasiert funktionieren, so dass das Sicherheitsteam sicher sein kann, dass die vertraulichen Daten, die der Wissenschaftler in ein Laufwerk legt, verschlüsselt sind und der Zugriff automatisch kontrolliert wird.\n\nEine weitere ideale Funktion wäre, dass das System sich selbst heilt und datengetrieben ist. Wenn beispielsweise Maschinen infiziert werden, würden sie automatisch in Quarantäne gestellt und die Arbeitslasten würden in einen anderen Bereich des Rechenzentrums oder in die öffentliche Cloud migrieren. Dabei müsste die IT zwar Richtlinien festlegen und Prozesse überwachen, aber das System wäre größtenteils automatisch. Das System sollte nicht nur automatisch sein, sondern auch intelligent, indem es aus Erfahrungen lernt und effizienter wird.\n\nWir können heute einige Elemente dieses utopischen Systems mit Standardprodukten erhalten, indem wir sie integrieren und alle dazu bringen, sie zu nutzen. Schauen wir uns an, was jede Organisation von dieser Architektur möchte.\n\n## IT-Architektur (Multi-Hybrid Cloud)\n\nIT ist verantwortlich für die zugrundeliegende Infrastruktur und Dateninformationen in der Organisation. Wenn IT eine solide Grundlage schaffen könnte, könnten alle anderen darauf aufbauen. IT muss zu einer multi-hybriden Cloud-Lösung wechseln, damit die Infrastruktur bei Bedarf problemlos orchestriert werden kann, mit Flexibilität basierend auf Richtlinien. Es besteht immer ein Kompromiss zwischen Kosten und Zuverlässigkeit, aber Sie haben Optionen. Eine softwaredefinierte Infrastrukturebene ermöglicht eine einfache Orchestrierung von Berechnung, Speicherung, Netzwerk, Sicherheit und sogar neuen Elementen wie Speicher und Beschleunigern. Die multi-hybride Cloud-Grundlage ist ein wesentlicher Aspekt Ihrer gemeinsamen Architektur.\n\n## Sicherheitsarchitektur\n\nDas Sicherheitsteam würde dieses System erweitern und so automatisiert wie möglich machen. Der erste Aspekt wäre die Identität. Das bedeutet, dass nicht nur Benutzer identifiziert werden können, sondern auch Infrastruktur, Anwendungen und Dienste, sodass alles eine Identität hat. Diese Identitäten können an bestimmte Berechtigungen und Zugriffe gebunden werden, um sicherzustellen, dass alles authentifiziert ist. Auf der Sicherheitsseite möchten Sie Verschlüsselung und Problemlösung, wenn Probleme auftreten. Idealerweise könnten Sie ein Vertrauensanker etablieren, damit alles in der Ecosystem, sowohl in Anwendungen und Diensten als auch bis hin zu Firmware und BIOS in den Maschinen, vertrauenswürdig ist.\n\n## Entwicklungsarchitektur\n\nEntwickler könnten sich Sorgen machen, dass dieser gesamte Prozess die Entwicklung verlangsamen könnte, daher muss er fast automatisch ablaufen. Die meisten Entwickler konzentrieren sich nun auf wiederverwendbare Komponenten, die getestet werden können, damit sie sicher sind. Dies geschieht durch Ökosysteme auf Containern wie Kubernetes, Docker oder Mesos. Sicherheit kann in den Entwicklungslebenszyklus beim Bereitstellungsschritt integriert werden, bevor sie erfolgreich in die Produktion übergeht. Über der Servicsschicht befindet sich eine Anwendungsschicht, in der Entwickler von Workflows profitieren können. Diese Workflows können Entwicklungsworkflows wie CI/CD oder Geschäftsworkflows durch Automatisierungstools wie Robotic Process Automation sein. Das Vorhandensein sowohl der Servicsschicht als auch der Anwendungsschicht sind Schlüsselelemente in dieser utopischen Architektur.\n\n## Datenarchitektur\n\nMit Daten, die in verschiedenen Ökosystemen, öffentlichen Clouds und sogar am Rand verteilt sind, brauchen wir einen besseren Weg, um Daten für Datenwissenschaftler und Anwendungsentwickler zu verwalten. Das Extrahieren von Daten aus der Speicherung ist einer der wichtigen Aspekte hier. Mit dieser Struktur können Sie Daten über die umfangreiche Infrastruktur orchestrieren und diese Daten nur dort an Anwendungen und Dienste binden, wo sie benötigt werden. Die Daten könnten abstrahiert werden, um zur besten Zeit an der Infrastruktur zu landen, ob am Rand, im Rechenzentrum oder an verschiedenen Orten für Anwendungsrepliken verarbeitet zu werden. Sicherheit wäre erforderlich, um die Daten zu schützen, da die Daten der Grund für die Infrastruktur sind. Einige Start-Up-Unternehmen sind jetzt in diesem Bereich tätig, um die Kontrolle über die Datemanagement-Ebene zu übernehmen.\n\nDiese utopische Architektur mit ihren vielen beweglichen Teilen wird Edgemere-Architektur genannt. Wir versuchen herauszufinden, wie all diese Teile zusammenpassen, um Organisationen dabei zu helfen, ihre digitale Transformation zu beschleunigen. Wir müssen verstehen, was jede Organisation braucht, welche Anwendungsfälle sie hat und welche Gemeinsamkeiten es zwischen den Gruppen gibt, um eine Architektur zu entwickeln, in der die gesamte Organisation arbeiten kann.\n\nDie Aufgabe Ihrer Organisation besteht darin, die Barrieren zwischen den Gruppen zu überwinden, eine gemeinsame Vision davon zu entwickeln, wo Sie organisatorisch, prozessual und architektonisch hin möchten, und einen Fahrplan zu entwickeln, wie Sie dorthin gelangen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT24-de","image":"./episodes/edt-24/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel skizziert die digitale Strategie und Architektur, um Ihre Organisation effektiv zu transformieren. Er erklärt, wie organisatorische, verfahrenstechnische und technologische Elemente ausgeglichen werden müssen, um effizient an einer gemeinsamen, idealen Architektur zu arbeiten und eine vereinheitlichte Vision zu unterstützen."},{"id":94,"type":"Episode","title":"Operationalisierung von Datenpipelines","tags":["dataarchitecture","datacentric","data","datamesh","datapipeline","technology","people"],"body":"\r\n\r\nFür die letzte Folge in dieser Serie spricht Darren mit Sarah Kalicin, der leitenden Datenwissenschaftlerin bei Intel, über die Operationalisierung Ihrer Datapipeline. Sie diskutieren, wie Sie Ihre Datenerkenntnisse von einem einmaligen wissenschaftlichen Experiment zu einer fortlaufenden Informationsquelle machen können.\n\n## Wie setzen wir die Erkenntnisse der Analytik in die Praxis um?\n\nDas erste, was man über die Datenpipeline verstehen muss, ist, dass sie nicht wie ein abgeschlossenes elektrisches System ist, das man aufbauen, sich davon entfernen und sechs Monate später einen Schalter umlegen kann, um zu wissen, dass die Glühbirne leuchtet. Eine Datenpipeline ist anders, weil die Daten variabel sind; sie können sich verändern oder abnehmen, zum Beispiel, sodass man nicht zwangsläufig belohnt wird, indem die Glühbirne zu irgendeinem Zeitpunkt leuchtet oder in diesem Fall die Erkenntnis, nach der man sucht. Man muss immer daran denken, was im System schiefgehen kann und wie man diese Kurzschlüsse beheben kann.\n\nDas Erkennen von Abnormalitäten ist ein integrierter Bestandteil des Pipelines. Man kann nicht für alles planen, daher muss man zumindest in der Lage sein zu sehen, wenn etwas passiert ist, das außerhalb der Grenzen der ursprünglichen Analytik liegt. Ein Beispiel dafür ist die COVID-Krise, ein unvorhersehbares Ereignis, das für viele Systeme Muster verursacht hat, die weit außerhalb des Normalen liegen. Ein weiteres Beispiel wäre ein Unternehmen, das Widgets herstellt. Um zu wissen, wie viele Widgets produziert werden sollen, enthält die Datenpipeline die Kundennachfrage, den aktuellen Bestand und den Ertragsverlust. Diese können im Laufe der Zeit recht stabil sein, aber es könnte beispielsweise ein PR-Ereignis geben, das die Kundennachfrage explodieren lässt. Das kann einen großen Einfluss auf die Modelle haben. Maschinelles Lernen und Deep Learning betrachten vertraute Muster und wenn sie diese Muster noch nie zuvor gesehen haben, werden die Modelle versagen oder sich verschlechtern. Man muss an der Schwelle zur Entdeckung bleiben.\n\nDie einzige Möglichkeit, um auf dem neuesten Stand der Entdeckungen zu bleiben, besteht darin, Ihre Datenpipelines automatisiert zu haben, um zeitnahen Zugriff auf Informationen zu erhalten. Dies ist der Wettbewerbsvorteil: aktuelle und aufschlussreiche Daten, die Ihnen schnell bei der Beantwortung Ihrer Fragen helfen können.\n\nIT-Teams und Daten-Teams müssen bei der Automatisierung zusammenarbeiten und festlegen, was für eingehende Daten automatisiert werden sollte. Außerdem müssen sie gemeinsam Änderungen am Modell verwalten, die von den Datenwissenschaftlern vorgenommen werden, damit es problemlos in den Arbeitsablauf integriert werden kann.\n\n## Bereitstellung führt zu Kurzschluss.\n\nEs gibt zwei Arten von Kontrollen, die Kurzschlüsse bei der Bereitstellung verhindern können: Analytische Systemkontrollen und organisatorische Kontrollen.\n\nAnalytische Systemsteuerungen beziehen sich darauf, die Modelle, die Sie trainiert haben, einzusetzen, um Daten durchzuführen, um Ihre Fragen leicht zu beantworten. Diese implementierten Modelle müssen moderiert werden, um die Genauigkeit der Daten zu überprüfen. Viele Dinge können dazu führen, dass die Daten negativ beeinflusst werden, wie zum Beispiel Umweltveränderungen, Maschinenkalibrierung, Verteilungsprobleme und so weiter.\n\nDies unterscheidet sich nicht so sehr von der Softwareentwicklungswelt, in der Änderungen Vorhersagen beeinflussen können. Die IT-Abteilung ist mit dem Prozess vertraut, Tests durchzuführen, um sicherzustellen, dass ihre Modelle oder Anwendungen gemäß den festgelegten Richtlinien ausgeführt werden. Daher sollten DevOps und Datenwissenschaftler von diesen Ressourcen und diesem Wissen profitieren. Es besteht keine Notwendigkeit, einen neuen Prozess zu erfinden, sondern die Gruppen sollten Ressourcen kombinieren, um sich erfolgreich aufzustellen.\n\nOrganisatorische Kontrollen gehen auf eine organisatorische Grundlage zurück, die sich dazu verpflichtet, datenzentriert zu sein und die richtigen Personen und Ressourcen bereitzustellen, um gemeinsame Ziele zu verfolgen. Die beste Chance, Operationalisierung zu erreichen, besteht, wenn eine Zusammenarbeit, Vertrauen, Verständnis für Bedürfnisse und Feedbackschleifen zwischen den Gruppen in der Organisation bestehen.\n\nFeedbackschleifen sind entscheidend in diesem Prozess. Beispielsweise können Fachexperten Informationen zu Marktdynamiken bereitstellen, damit die Datenwissenschaftler das Modell auf diese Änderungen in den Daten überwachen können. Wenn ein Modell im Laufe der Zeit verwendet werden soll, muss es ständig überarbeitet und verbessert werden.\n\nKonsumenten der Daten sollten ein Dashboard haben, das ihnen Informationen liefert und es ihnen ermöglicht, genauer nachzuforschen, warum etwas ein wenig seltsam aussieht. Je mehr sie untersuchen oder aufzeigen können, was untersucht werden muss, desto gestärkter wird Ihre Organisation sein.\n\n## Rohrleitung\n\nEin Schlüssel von der IT-Seite, um die Daten-Pipeline einsatzbereit zu machen, besteht darin, eine Versionskontrolle wie GitHub zu verwenden, um Zugriff auf frühere Versionen Ihres Modells zu haben. Für Prüfungszwecke ist es auch wichtig, die Daten zu speichern, die das Modell erstellt haben, sowie andere historische Daten. Sie möchten in der Lage sein, die Muster zu betrachten und zu sehen, wie sich ein bestimmtes Merkmal geändert hat oder das Modell beeinflusst hat. Sie können auch historische Daten in Ihre neuen Modelle einfügen, um zu sehen, wie stark sie Ihre aktuellen Daten beeinflussen.\n\nEin Beispiel: Einige Systeme zeigen ein verzerrtes Bild, da während COVID eine große Anzahl von Menschen von zu Hause aus arbeitet. Ein Fallbeispiel ist die Marine. Seit COVID sind 95% ihrer IT-Mitarbeiter remote tätig und ihre Produktivität ist um 35% gestiegen. Aufgrund dieses einen Datenpunktes könnte man sagen, dass alle jetzt von zu Hause aus arbeiten werden. Wird sich die Steigerung von 35% fortsetzen oder wird es, wenn die Leute ins Büro zurückkehren, einen Rückgang um 35% geben? Offensichtlich ist dieser eine Datenpunkt nicht unbedingt ausreichend, um tatsächliche Produktivität vorherzusagen.\n\nEin weiteres Tool, das die IT anbieten kann, ist die kontinuierliche Integration und Bereitstellung. Wenn Sie mit einem Modell arbeiten, können Sie entweder Jenkins, GitHub Actions oder ein ähnliches Tool verwenden, um automatisch Tests gegen Ihr Modell mit Ihren Daten auszuführen oder spontan sinnlose Daten zu generieren.\n\nDie IT-Mitarbeiter und Datenwissenschaftler müssen zusammenarbeiten, um zu bestimmen, was und wie der Output der Modelle überwacht werden soll. Die IT kann den Output automatisch überwachen und auch die Leistung der Modelle in der Infrastruktur verfolgen. Ein Beispiel ist, dass die IT mit automatischer kontinuierlicher Integration und Bereitstellung schnell die Datenwissenschaftler alarmieren kann, wenn ein Modell länger als normal benötigt, um zu verhindern, dass es außer Kontrolle gerät. Wenn die IT in die Datenerzeugung investiert ist, was bisher weitgehend in der Branche gefehlt hat, wird der Prozess für alle einfacher und kohärenter sein.\n\nEin weiterer Aspekt, über den man nachdenken sollte, ist das Versuchsdesign, da auch die Wechselwirkungen zwischen Variablen und Merkmalen wichtig sind. Fachexperten können dabei helfen, zu bestimmen, welche potenziellen Interaktionen auftreten können, und man kann Modelle erstellen, um zu verstehen, welche Variabilität erwartet werden kann.\n\nEs ist aufregend, dass ein Datenwissenschaftler Rohmaterial in Erkenntnisse verwandeln kann. Es erfordert jedoch ein Team. Je mehr jeder in Ihrer Organisation voneinander lernen kann, desto mehr großartige Dinge können geschehen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT25-de","image":"./episodes/edt-25/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel, spricht mit Sarah Kalicin, Leitende Data Scientistin bei Intel, über die Operationalisierung der Datenpipeline Ihrer Organisation. Es bedarf eines Teamefforts, um kontinuierlich wertvolle Informationen zu modellieren, zu überwachen und zu produzieren. Dies ist die letzte Folge der Serie zur Initiierung Ihrer Organisationsumwandlung hin zu einer datenzentrierten Organisation."},{"id":95,"type":"Episode","title":"Multi-Cloud-Datenlösungen mit Hammerspace","tags":["hammerspace","data","technology","dataarchitecture","multicloud","compute","multihybridcloud","datamesh"],"body":"\r\n\r\nDie Speicherung ist reif für eine Störung. Derzeit wird die Verwaltung von Daten auf umständliche, prozedurale und oft manuelle und fehleranfällige Weise durchgeführt. Hammerspace wurde gegründet, um dieses Problem zu lösen, indem Daten von der Speicherinfrastruktur abstrahiert werden.\n\n## Stell dir für einen Moment vor...\n\n…wenn Ihre Daten von der Speicherinfrastruktur getrennt wären. Frei von den Einschränkungen, die durch aktuelle Speichersysteme auferlegt werden, könnten Benutzer ihre Datenverwaltung und -schutz selbstständig durchführen, das Kostenprofil sofort ändern und von überall auf der Infrastruktur auf Daten zugreifen. Software-definierter Speicher könnte die Leistung on demand skalieren und moderne Arbeitslasten wie Kubernetes auf jeder zugrunde liegenden Infrastruktur bereitstellen, egal wo.\n\n## Die Herausforderung\n\nAnwendungen sind jetzt tragbar geworden, aber Daten sind immer noch isoliert. Die Herausforderung besteht darin, dass Leistung, Zuverlässigkeit und Verwaltbarkeit bei hoher Skalierung aufgrund des Silo-Problems alle beeinträchtigt werden. Die Lösung besteht darin, die Daten von den Begrenzungen der zugrunde liegenden Infrastruktur zu befreien. Hammerspace erreicht dies durch die Aufteilung von Metadaten und die Assimilation der kleinsten Bestandteile, um Daten tragbar zu machen.\n\n## Entkoppelung von Daten von der Speicherung\n\nMit dieser Technologie haben Sie immer und überall auf Abruf Daten für Ihre Apps. Sie haben unabhängige Kontroll-, Datenpläne sowie einen globalen Namensraum und ein Dateisystem, das mehrere Rechenzentren und Clouds umfasst. Die Speicherung wird orchestriert; Daten sind vollständig automatisiert und nutzen deklaratives autonomes Datenmanagement. Anders ausgedrückt trennt es das \"Wie\" vom \"Was\" und gibt den gewünschten Endzustand an, ohne alle Details zur Erreichung dessen aufzeigen zu müssen.\n\n## Hammerspace Architektur\n\nDas System ermöglicht es Ihnen, auf Dateiebene zu arbeiten, bietet Unternehmensdatendienste wie Schnappschüsse und Kopien und lässt sich problemlos von Rechenzentrum zu Rechenzentrum, von Rechenzentrum zur Cloud und zurück zum Rechenzentrum skalieren, um es in einem multi-hybriden Cloud-Szenario zu verteilen.\n\n## Veralteter Speicher kann modernen Herausforderungen nicht standhalten\n\nVererbter Speicher skaliert nicht ausreichend, selbst mit horizontalen oder Cloud- oder Planeten-Skalierungslösungen, da selbst diese Cluster zu Silos werden und manchmal dazu führen, dass Sie mit Daten stecken bleiben, die nicht korreliert und analysiert werden können. Traditionsreicher Speicher hat auch Schwierigkeiten, Kapazität und Leistung unabhängig voneinander zu skalieren.\n\nReplikation ist eine alte Technologie, die zu einer unkontrollierten Verbreitung von kopierten Daten führt. Anstatt nur die kleinsten Bestandteile zu verschieben, bewegt man die gesamte Nutzlast. Die Datenverwaltung wird oft vernachlässigt, dabei sollte sie prioritär behandelt werden und nicht als Nachgedanke.\n\nEin Problem, das die Architektur von Hammerspace löst, ist das einfache Auffinden von Dingen. Versionierung kann zu großen geschäftlichen Problemen führen. Zum Beispiel haben sowohl Airbus als auch Boeing massive Probleme gehabt, weil einige Ingenieure nicht über ihre aktuellsten Versionen verfügten. Die Lösung besteht darin, die Daten zu orchestrieren.\n\n## Datenorchestrierung\n\nOrchestrierung bedeutet zunächst die Entkopplung aller verschiedenen Silos; die Daten werden als einziger Pool behandelt. Hammerspace assimiliert die kleinsten Bestandteile, die Metadaten, um im Grunde ein Datengegensystem zu schaffen. Dann implementieren sie Ziele wie Haltbarkeit, Verfügbarkeit und Schnappschüsse oder benutzerdefinierte Aktionen, die mit Hammer-Skripten durchgeführt werden können. Schließlich werden die Daten, egal ob es sich um Kubernetes, NFS oder SMB handelt, tragbar und sind mit dem System verbunden.\n\nWas dies in der Praxis bedeutet, ist, dass Sie nicht zu einem anderen Silo gehen müssen, um eine bestimmte Arbeitslast zu bearbeiten. Daten werden dorthin geliefert, wo Sie es wollen. Diese Datenmobilität ist entscheidend, da sie live ist; es handelt sich nicht um Datenmigration, die störend ist und Ausfallzeiten verursacht.\n\nUm teure Ausgangskosten zu minimieren, wird Daten auf Dateiebene dedupliziert und komprimiert. Anstatt ein ganzes festes Volumen zu verschieben, können Sie die Daten auswählen, die Sie basierend auf verschiedenen Kriterien wie Ordner, Metadaten-Tags oder einem Kunden-Deskriptor verschieben möchten. Dadurch ergeben sich Flexibilität und Kosteneinsparungen.\n\n## Hammerspace-Architektur\n\nIn der Hammerspace-Architektur hat das globale Dateisystem auf höchster Ebene drei Komponenten: das globale Dateisystem selbst, die Front-End-Präsentation (NFS, CSI-Treiber und SMB) und Anvil innerhalb des globalen Dateisystems. Anvil ist die Komponente für die Metadatenverwaltung und DSX stellt Metadatenverwaltungsdienste bereit. Diese können als virtuelle Maschinen, VMware, KBM oder Hyper-V implementiert werden. Anvil ist in einer A-förmigen Konfiguration angeordnet, sodass es mindestens zwei in jeder Standort gibt. DSX kann parallelisiert werden, um die Leistung zu skalieren. Daher können Sie an verschiedenen Standorten eine Anzahl davon haben, um sicherzustellen, dass Sie ausreichend Leistung haben. Sie können problemlos skaliert werden.\n\nAuf der Backend-Seite kann der zugrunde liegende Speicher Hammerspaces eigenes softwaredefiniertes Speichermodell mit direkt angebundenen Festplatten, assimiliertem NAS, jeder Cloud oder einer beliebigen Kombination sein. Dies kann auch horizontal skaliert werden, sodass Sie nun Leistung und Kapazität unabhängig voneinander skalieren können. Entsprechend dem Cloud-Modell ist es auch elastisch, sodass Sie bei einer Geschäftsänderung an diesem bestimmten Standort sowohl die Leistung als auch die Kapazität reduzieren können, um sicherzustellen, dass Anwendungen genau das Richtige an diesem Standort haben. Dies ermöglicht eine sehr flexible Architektur, um alle Anwendungs-Workloads auf der Front-End-Seite zu bedienen.\n\nEin großer Vorteil dieser flexiblen Architektur besteht darin, dass Daten assimiliert werden können, die in Geräten gespeichert sind, die nicht Ihnen gehören, wie z.B. NAS oder in der Cloud. Dadurch wird das Verschieben von Daten vereinfacht. Wenn Sie beispielsweise ein älteres NAS haben und auf ein neueres NAS migrieren möchten, spielt es keine Rolle, ob es sich um denselben oder einen anderen Anbieter handelt. Hammerspace assimiliert die Metadaten und verschiebt die Daten hinter den Kulissen und vollständig transparent für die Anwendungen, da es sich um eine Live-Datenmobilität handelt. Ein weiterer bedeutender Vorteil ist, dass es keine Ausfallzeiten beim Verschieben der Daten gibt.\n\nWenn Sie diese Technologie ausprobieren möchten, gehen Sie zu hammerspace.com und beginnen Sie mit einer kostenlosen Testversion mit einer Lizenz für bis zu 10 Terabyte, die in Azure, AWS oder Google Cloud bereitgestellt wurde.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Johan Ballin"],"link":"/episode-EDT26-de","image":"./episodes/edt-26/de/thumbnail.png","lang":"de","summary":"Johan Ballin, Direktor für technisches Marketing bei Hammerspace, und Darren Pulsipher, Chief Solution Architect für den öffentlichen Sektor bei Intel, diskutieren Hammerspaces Hybrid-Cloud-Technologie, die Daten von der Speicherinfrastruktur entkoppelt und Datenträgheit und Leistung ermöglicht."},{"id":96,"type":"Episode","title":"Hardware-Sicherheit: Unabdingbar für die Transformation von Daten","tags":["datagovernance","data","sgx","confidentialcomputing","cybersecurity","technology"],"body":"\r\n\r\n## Datensicherheitslandschaft\n\nDie Welt ist durch alle Geräte unglaublich vernetzt geworden, und dies treibt ein exponentielles Wachstum in der Menge an Daten voran, die wir verwalten müssen: je mehr Volumen, desto mehr Risiko. Es ist eine Herausforderung, und die Nutzung neuer Rechenkapazitäten wie Cloud, Analyse und Edge Computing führt zu zusätzlicher Komplexität.\n\nEs gibt drei wichtige Trends in der Sicherheit: die Verschlüsselung von allem, die Isolierung der Arbeitsbelastung und eine Vertrauenskette. Diese drei Bereiche sind wichtig, um Organisationen bei der Bewältigung steigender behördlicher Anforderungen zum Schutz von Daten zu unterstützen.\n\nWenn Organisationen sich entscheiden, eine digitale Transformation vorzunehmen oder beispielsweise aufgrund eines Katalysators wie COVID-19 auf Fernarbeiter angewiesen sind, müssen Sicherheitsmaßnahmen an erster Stelle stehen, um Sicherheitskontrollen und Compliance aufrechtzuerhalten. Wenn Sie Sicherheit bereits bei Ihren Bereitstellungen und Implementierungen berücksichtigen, können Sie nachhaltig Ihr Geschäft transformieren.\n\n## Schützen Sie Daten während ihres Lebenszyklus.\n\nDie Verschlüsselung von Daten ist in allen Phasen wichtig. Historisch gesehen konnten Angreifer Zugriff auf Daten direkt über das Netzwerk erhalten. Nachdem diese verschlüsselt wurden, begannen sie in die Rechenzentren einzudringen und Daten aus den Datenbanken zu ziehen. Deshalb begannen wir, die Speicherung zu verschlüsseln. Dennoch gelangen die Daten an einen Endpunkt und werden in den Speicher geladen und entschlüsselt. Somit könnte ein raffinierter Angriff mit Root-Zugriff gegebenenfalls die Daten an diesem Verarbeitungspunkt abfangen oder manipulieren. Dieser Angriff auf Daten während der Nutzung ist die neue Herausforderung für Angreifer und diejenigen, die den Raum verteidigen. Intel hat einige aufregende und innovative neue Funktionen, die wir in unsere Prozessoren integrieren, um Datenbesitzern, Anwendungsbesitzern, Dienstleistern und im Grunde dem gesamten Ökosystem zu helfen, diese potenziellen Schwachstellen zu schließen.\n\n## Warum Daten im Gebrauch schützen?\n\nIn vielen Fällen nutzen Angreifer Privileg-Eskalationen auf der Betriebssystem- oder Hypervisor-Ebene. Sie könnten über ein Gastbetriebssystem, ein Host-Betriebssystem oder sogar physischen Hardwarezugriff auf den Server erfolgen. Die Angreifer könnten klassische Hacker-Malware-Typen, Wettbewerber von Drittanbietern oder Insider wie ein Administrator oder Systemadministrator bei einem Dienstleister sein.\n\n## Intel Software Guard Extensions (Intel SGX) translates to \"Intel Software Guard Extensions (Intel SGX)\" in German.\n\nIntel hat eine neue Technologie in unsere Prozessoren namens Intel Software Guard Extensions, oder SGX, integriert. Dies ist Teil des vertraulichen Computing-Paradigmas, das derzeit auf dem Markt boomt, da Unternehmen versuchen, ihre Daten zu transformieren und privat zu halten. SGX behebt diese neuen Arten von Angriffen, indem es der Anwendung ermöglicht, direkt mit dem Prozessor im verschlüsselten Speicher zu kommunizieren und das Betriebssystem, den Hypervisor und im Wesentlichen alles andere im System zu umgehen. Selbst wenn Ihr Betriebssystem, Hypervisor oder andere Anwendungen gehackt werden, können Angreifer nicht auf diese Daten zugreifen, da das Betriebssystem selbst keine Sichtbarkeit für diese Daten hat. Daher müssen Sie dem Anbieter oder dem restlichen System-Stack nicht vertrauen, da Sie im Grunde genommen davon ausgehen, dass sie bereits kompromittiert sind, und Ihr Code und Ihre Daten vertraulich und unverändert bleiben werden; sie werden Integrität aufweisen.\n\nIntel versucht, es so zu gestalten, dass Sie auf möglichst wenige Komponenten vertrauen müssen. SGX geht für das Rechenzentrum weiter als alles andere, was wir bisher gesehen haben. Sie müssen lediglich Ihrer eigenen Anwendung und dem Prozessor selbst vertrauen.\n\nSGX bietet eine leistungsstarke Möglichkeit für Unternehmen wie Cloud-Dienstleister, die ihren Kunden mitteilen können, dass weder sie noch die Regierung beispielsweise auf Ihre Daten zugreifen können, selbst wenn sie es wollten.\n\n## Intel SGX in Aktion\n\nSGX wird bereits umfassend von Cloud-Service-Providern und Softwareanbietern genutzt, aber in gewisser Weise fangen wir gerade erst an. Obwohl diese Technologie seit einigen Jahren existiert, haben wir Ökosysteme integriert und bringen neue Fähigkeiten in unseren kommenden Xeon Ice Lake-Prozessoren der dritten Generation ein. Dadurch werden sich die Kapazitäten erweitern, die Skalierbarkeit auf umfangreiche Unternehmens-Workloads verbessern und es wird möglich sein, deutlich größere Speicherbereiche mit besserer Leistung und in einem viel breiteren Umfang im Mainstream-Rechenzentrum zu schützen.\n\nIntel ist eines der Gründungsmitglieder des Confidential Computing Consortium, das Teil der Linux-Stiftung ist. Die meisten großen Cloud-Service-Anbieter, viele Software-Anbieter und sogar unsere anderen Silizium-Konkurrenten arbeiten gemeinsam an Lösungen für vertrauenswürdige Ausführungsumgebungen und setzen Standards für den Umgang mit dieser Art von Fähigkeit. Wir schaffen auch Bewusstsein für die Notwendigkeit und den Geschäftswert des vertraulichen Rechnens.\n\n## Intel SGX Software Partner Ecosystem auf Deutsch übersetzen\n\nSGX bietet eine Vielzahl von Nutzungsoptionen, abhängig davon, wonach der Datenbesitzer sucht. Wenn ein Kunde das feinste Maß an Kontrolle wünscht, ermöglicht SGX ihnen genau das. Tatsächlich können sie ihre Anwendung im Wesentlichen auf die codierten Daten reduzieren, die sie vom Rest des Systems isoliert halten möchten, oder sogar nur einen Teil dieser Anwendung. Diese Szenario erfordert jedoch, dass sie die Anwendung für diesen Zweck schreiben. Im Ökosystem gibt es Open-Source-Ressourcen, die diese Art der Entwicklung viel einfacher machen, und es wird ständig erweitert.\n\nAuf der anderen Seite gibt es einen schnellen Pfad zum Anheben und Verschieben. Sie können Ihre Anwendung nehmen und in eine sicherere Umgebung verschieben. Das Ökosystem reagiert und erstellt SGX-bewusste Container. Sie könnten Ihre unveränderte Anwendung in dieser Umgebung ablegen, in der nur sie allein ausgeführt wird, sodass die Anwendung selbst denkt, dass sie in ihrer nativen Umgebung läuft.\n\nUnsere Ökosystempartner haben in diesem Bereich mit Dingen wie Fortanix, Graphene und Scone viele Fortschritte gemacht. Einige sind Open Source und einige sind proprietär, aber sie bieten bereits alle Dienste. Zum Beispiel bietet Microsoft Azure Confidential Computing die gesamte Palette von Lift-and-Shift-Lösungen mit SGX bis hin zu SDKs, mit denen Sie Ihre Anwendung direkt dafür entwickeln und in ihrer Umgebung bereitstellen können, sodass Sie sich nicht einmal um die Hardware kümmern müssen. Es gibt eine vollständige Auswahl an Optionen, sodass niemand Angst vor den Komplexitäten von SGX haben sollte. Kunden sollten auch darauf vertrauen können, dass sensible Materialien wie Machine Learning-Algorithmen oder Verschlüsselungsschlüssel mit einem sehr granularen Schutzniveau behandelt werden.\n\n## Vertrauliches Computing: Eine sicherheitsrelevante Neuerung.\n\nSGX ist schon seit einiger Zeit auf dem Markt und wir haben daran gearbeitet, es zu erweitern. Es wurde getestet und durch Hunderte von Forschungspapieren geprüft und mit Updates immer weiter verbessert. Es hat den Vorteil, nicht mehr das neue Kind auf dem Block zu sein, sondern eine grundlegende Lösung, die mit Ice Lake in den Mainstream gebracht wird. Es konzentriert sich nicht mehr nur auf kleine, sensible Bereiche, sondern ist jetzt bereit für große Aufgaben.\n\nDie Regierung, Finanzdienstleistungen und das Gesundheitswesen sind einige der Branchen, die frühzeitig das Potenzial der SGX erkannten. Dies liegt daran, dass in diesen Branchen strenge behördliche Anforderungen und Datenschutzrichtlinien gelten, gleichzeitig jedoch der Wunsch besteht, Daten zu teilen und innovative Lösungen mit verschiedenen Parteien zu entwickeln. Unternehmen stehen vor ähnlichen Herausforderungen, zum Beispiel wenn sie in die öffentliche Cloud wechseln möchten, aber Bedenken hinsichtlich des Schutzes ihrer sensiblen Daten haben. Mit SGX müssen sie dem Anbieter nicht vertrauen.\n\nIntel hat gerade eine große Ankündigung in diesem Monat gemacht, und wir öffnen wirklich die Tore zu den kommenden Dingen. Ein großartiger Ausgangspunkt ist intel.com/sgx für Testimonials und einen tieferen Einblick in die Informationen. Kunden sollten nach den Ökosystempartnern wie Azure und Fortanix suchen. Ein weiterer Ort für Informationen ist das Confidential Computing Consortium aufgrund der Anzahl der Menschen, die in diesem Bereich arbeiten.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jesse Schrater"],"link":"/episode-EDT27-de","image":"./episodes/edt-27/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect, Public Sector, Intel, und Jesse Schrater, Security Manager, Data Platforms Group, Intel, diskutieren die aktuelle Sicherheitslandschaft und wie Intels SGX und das Partner-Ökosystem eine zeitnahe und bewährte Lösung für Daten in Verwendung und andere Sicherheitsbedenken bietet."},{"id":97,"type":"Episode","title":"Historische Integration mit Cloud und RPA","tags":["rpa","bpm","automation","compute","technology"],"body":"\r\n\r\nEs scheint, als ob jeder mitten in einer digitalen Transformation steckt. Private Cloud, Public Cloud, Multi-Hybrid Cloud, Data Lakes, Machine Learning, Inference und Künstliche Intelligenz sind alles Begriffe, die heute verwendet werden, um die digitale Transformation zu beschreiben. Aber was ist mit der Integration von Altanwendungen? Niemand spricht jemals über die Integration von Altanwendungen. Warum? Weil es herausfordernd ist, Altanwendungen, Daten und Sicherheit in Ihre neue, makellose Multi-Hybrid-Cloud-Umgebung zu integrieren. Das ist eine der letzten Dinge, um die wir uns sorgen wollen. Wenn wir keine Strategie für Altanwendungen entwickeln, haben wir ein Schnellboot mit dem Anker unten. Das bremst uns aus und hält unser Boot im Hafen fest.\n\nEiner der wachsenden Bereiche, um bei der Integration von Legacy-Systemen und der Automatisierung von Integrationen zu helfen, ist der Einsatz von Automatisierungstools und -frameworks. In den letzten drei Jahren hat sich ein erheblicher Schwerpunkt auf die Automatisierung von Arbeitsabläufen mit Legacy- und neuen Cloud-bewussten Anwendungen für Informationsarbeiter ergeben. Diese Tool-Sammlungen werden als Robotic Process Automation (RPA)-Tools bezeichnet.\n\n## Robotic Process Automation (RPA) - Roboterprozessautomatisierung\n\nAls ich zum ersten Mal damit begann, Robotic Process Automation (RPA) Werkzeuge zu untersuchen, dachte ich, dass sie Roboter steuern. Ich war bereit, mir meine Stahlkappenschuhe und einen Schutzhelm anzuziehen und Fertigungsanlagen zu besuchen. Allerdings fand ich schnell heraus, dass RPAs die Art und Weise nachahmen, wie Informationsarbeiter mit den verschiedenen Werkzeugen arbeiten, die sie verwenden. Einige der Werkzeuge sind Legacy-Tools, und einige davon sind moderne Anwendungen. RPAs zeichnen auf, wie der Informationsarbeiter die Benutzeroberfläche der verschiedenen Werkzeuge nutzt, und ermöglichen dann die Wiedergabe der Aufzeichnung, um den Workflow des Informationsarbeiters zu automatisieren.\n\nÜberraschenderweise sind RPAs eine ziemlich ausgereifte Technologie. Viele der RPA-Unternehmen haben ihren Ursprung im Markt für UI-Testtools. Die Fähigkeit, die Benutzerinteraktion mit mehreren Anwendungen im Laufe der Zeit zu erfassen, ist entscheidend für die Entwicklung eines wiederholbaren Benutzeroberflächentests. Diese Tools gibt es schon seit über 20 Jahren und sind ziemlich ausgereift. Diese UI-QA-Testtools wurden umgestaltet und für Informationsmitarbeiter umfunktioniert, die ihre wiederholbaren redundanten Aufgaben automatisieren möchten.\n\n## Aktueller Marktplatz - 2020\n\nInvestoren sehen diesen Markt als einen heiß begehrten Markt und haben stark in diese Technologien investiert. In den letzten drei Jahren wurden über 2,0 Milliarden US-Dollar in den RPA-Markt investiert. Drei Unternehmen haben den Großteil der Investitionen übernommen:\n\nUiPath - 1 Milliarde Dollar Investition bei einem jährlichen Umsatz von 300 Millionen Dollar.\n\nAutomation Anywhere - 500 Millionen US-Dollar Investition bei einem jährlichen Umsatz von 100 Millionen US-Dollar\n\nBluePrism - $50 Millionen Investition bei einem jährlichen Umsatz von $30 Millionen\n\n*Hallo, wie geht es dir?\n\nDie meisten Investitionen stammen nicht aus dem traditionellen High-Tech Silicon Valley, sondern stattdessen aus den Finanzzentren wie New York und London. Dies deutet darauf hin, dass Finanzinstitute RPAs nutzen, um viele Arbeitsabläufe und Prozesse ihrer eigenen Mitarbeiter zu automatisieren.\n\n## Ort, an dem RPA gut funktioniert\n\nDie ersten vertikalen Segmente, die RPAs übernommen haben, waren die Finanz-, Versicherungs- und medizinischen Branchen. Diese Branchen haben nach Möglichkeiten gesucht, die Variabilität zu verringern, die Zuverlässigkeit zu erhöhen und die Kosten zu senken. Aufgrund der großen Anzahl von Informationsarbeitern in diesen Branchen haben sie RPAs genutzt, um einen Großteil der Arbeiten ihrer Informationsarbeiter zu automatisieren. Um diese Arbeitsabläufe zu automatisieren, müssen Organisationen verstehen, wie diese Arbeitsabläufe entstehen.\n\nZuerst wollen wir den Begriff des Informationsarbeiters verstehen. Viele Informationsarbeiter verbringen Zeit damit, mit mehreren Anwendungen zu arbeiten, Informationen zu verknüpfen und Anwendungen auf spontane Weise miteinander zu verbinden. Während diese Arbeiter weiterhin mit diesen Anwendungen arbeiten, erstellen sie organisch Workflows, indem sie Daten und Anwendungen in unzähligen verschiedenen Kombinationen zusammenführen.\n\nZweitens, katalogisieren Sie die Arbeitsabläufe so gut wie möglich, um Kandidaten für die Beseitigung von Arbeitsabläufen durch Duplikate und Redundanzen zu finden. Nachdem die Arbeitslasten verstanden sind, ist der nächste Schritt, die Arbeitsabläufe zu priorisieren und aufzulisten. Die Konzentration auf die am häufigsten verwendeten Arbeitsabläufe mit der größten Anzahl von Schritten tendiert dazu, der beste Weg zur Priorisierung der Arbeitsabläufe zu sein.\n\nSchließlich finden Sie heraus, wie Sie die Arbeitsabläufe mit RPA-Bots automatisieren können. Die Automatisierung kann durch UI-Aufzeichnung des Arbeitsablaufs von einem der Informationsarbeiter erfolgen und durch Anmerkungen des Arbeitsablaufs mit Variationen basierend auf Dateneingabe und Sicherheitsanmeldeinformationen ergänzt werden. Sobald die Aufzeichnung abgeschlossen ist, wird ein RPA-Bot erstellt, um den Arbeitsablauf zu automatisieren. Nun müssen Sie entscheiden, wie Sie möchten, dass der RPA ausgeführt wird: betreut oder unbeaufsichtigt.\n\n## RPA Betriebsweisen\n\nDie RPAs werden in zwei grundlegenden Ausführungsmodi verwendet: Betreute und Unbetreute. Betreut bedeutet, dass sie auf dem Desktop oder Laptop des Informationsarbeiters ausgeführt werden. Sie unterstützen den Informationsarbeiter, indem sie die tägliche Arbeit automatisieren. Unbetreut läuft in einer virtuellen Desktop-Umgebung und wird in der Regel durch ein Ereignis oder einen Auslöser gestartet und läuft ohne jegliche Interaktion mit dem Informationsarbeiter. Es gibt Vorteile, sowohl im betreuten als auch im unbetreuten Modus zu arbeiten, wie unten beschrieben.\n\n## besuchten\n\nErledigt Aufgaben für einzelne Mitarbeiter.\n\nMitarbeiter lösen aus und steuern einen Bot, um eine Aktivität durchzuführen.\n\n* Mitarbeiter lösen Bots aus, um Aufgaben bei Bedarf jederzeit zu automatisieren.\n\nErhöht die Produktivität und Kundenzufriedenheit in Callcentern und anderen Service-Schalter-Umgebungen\n\n## Unbeaufsichtigt\n\nAutomatisiert das Backoffice-Prozesse im großen Stil\n\nBasierend auf regelbasierten Prozessen bereitgestellt\n\nBots führen Geschäftsprozesse ohne menschliches Eingreifen gemäß einem vordefinierten Zeitplan aus.\n\nBefreit Mitarbeiter von monotoner Arbeit, senkt Kosten, verbessert die Konformität und beschleunigt Prozesse.\n\n## Wie integriert man RPA in Ihr Unternehmen?\n\nUm zu verstehen, wie RPAs in Ihr Unternehmen passen, müssen Sie zuerst die Benutzer der RPAs betrachten. Genauer gesagt gibt es drei Arten von \"Akteuren\", die die RPA-Werkzeuge nutzen, verwalten oder beeinflussen.\n\nInformation Worker - Dies ist der Hauptbenutzer der RPA-Werkzeuge. Ihre manuellen Prozesse sind Ziele für die Automatisierung.\n\nAnwendungsentwickler - RPA-Bots ändern sich, wenn Anwendungen aktualisiert oder erstellt werden. Änderungen an der Benutzeroberfläche erfordern das \"Erneute Aufzeichnen\" der RPA-Bots.\n\nIT-Betrieb - Verwalten Sie die RPA-Tools und implementieren Sie unbeaufsichtigte RPA-Bots.\n\nSorry, I cannot translate a blank text. Please provide the text you would like to have translated.\n\n## Die Verwaltung von Veränderungen\n\nDie Verwaltung der Komplexität von Konfigurationen und Sicherheit sind entscheidende Faktoren für eine erfolgreiche Implementierung von RPA-Tools und Bots. Zunächst benötigen Sie ein Verständnis dafür, wie die verschiedenen Benutzer der RPAs interagieren, wenn es um Änderungen an Anwendungen, Arbeitsabläufen und Prozessen geht. Dieses Verständnis ist entscheidend für die Verwaltung von Änderungen in den RPA-Bots und den von ihnen verwendeten Toolsets.\n\nKleine Änderungen an Anwendungen können eine tiefgreifende Wirkung auf Informationsarbeiter haben und beeinflussen, wie sie ihre tägliche Arbeit erledigen, was wiederum bedeutet, dass ein neuer RPA-Bot aufgezeichnet oder ein bestehender aktualisiert werden muss. Durch die Verbindung von RPA-Bots mit Toolsets und Workflows führt die Erstellung von RPA-Bots bei unreifen Workflows oder Toolsets zu unnötigen Schwankungen und Fragilität. Ausgereifte Prozesse und Toolsets sind hervorragende Kandidaten für die RPA-Automatisierung.\n\nEine weitere Sache, die berücksichtigt werden muss, ist, wo die Tools laufen, die Sie mit Ihrem RPA-Tool automatisieren. Verwenden sie Legacy-Anwendungen und Infrastruktur? Nutzen sie eine öffentliche oder private Cloud? Wie sind die Netzwerke dieser Systeme verbunden? Mit zunehmender Anzahl an Umgebungen steigt auch die Komplexität bei der Verwaltung und Aktualisierung von Anwendungen und RPA-Bots. Finden Sie Möglichkeiten, die Anzahl der Umgebungsgrenzen zu verringern, die der RPA-Bot überwindet.\n\n## Verwaltung der Sicherheit\n\nEin weiterer kritischer Faktor der zu berücksichtigen ist, ist die Sicherheit für die RPA Bots. Wenn ein Information Worker seinen Arbeitsablauf aufzeichnet, muss er sich bei jedem verwendeten Werkzeug authentifizieren (einloggen). Die Mitarbeiter authentifizieren sich dabei mit Benutzernamen und Passwörtern, Authentifizierungsschlüsseln oder sogar mit Corporate Single Sign-On Tools. In jedem Fall müssen Sie die Sicherheit dieser Werkzeuge im Zusammenhang mit dem RPA Bot während der Ausführung verwalten. Jegliche Änderungen an der Authentifizierung (Benutzername, Passwort, Authentifizierungsschlüssel oder Anmeldeinformationen) erfordern Änderungen am RPA Bot. Viele der RPA-Werkzeuge berücksichtigen dies und haben Mechanismen, um Sicherheitsanmeldeinformationen in den RPA Bot einzufügen und sich zur Laufzeit bei den Werkzeugen zu authentifizieren.\n\n## RPA-Tools und -Bots mit SecDevOps-Workflows verwalten\n\nDie Komplexität von RPA-Tools und -Bots passt sehr gut zu bekannten Mustern in der SecDevOps-Welt. Glücklicherweise werden viele der Probleme bei der Verwaltung von Konfigurationen und Abhängigkeiten gut mit einem SecDevOps-Prozess bewältigt.\n\n## RPA Bündelung\n\nEiner der Tricks besteht darin, den RPA-Bot als einen komplexen Dienst zu behandeln, der mehrere VMs oder Container für jedes der Tools, einen virtuellen Desktop und den Bot selbst enthält. Diese Dienste können zu einem Bündel zusammengefasst und wie ein Paket gemeinsam verwaltet werden. Ein Bündel umfasst nicht nur die Dienste, sondern auch die sichere Kommunikation (Netzwerk) zwischen den Diensten (Authentifizierung).\n\nDas Übergeben eines Bündels an einen Service-Orchestrator ermöglicht eine verbesserte Automatisierung des Managements von Netzwerk-Firewalls, Sicherheit und der Einfügung von Berechtigungsschlüsseln sowie des Lebenszyklusmanagements des RPA-Bots und der von ihm genutzten Tools. Im Bereich der Virtualisierung stehen mehrere Tools zur Verfügung (VMWare und OpenStack), die die Erstellung und Verwaltung dieser Bündel ermöglichen. Der Container-Bereich verfügt ebenfalls über ähnliche Terminierungs- und Orchestrierungstools, nämlich Kubernetes, Mesos und Docker Swarm.\n\n## SecDevOps-Pipelining\n\nEine einfache SecDevOps-Pipeline verwaltet das RPA-Bot-Bündel genauso wie jedes andere traditionelle Anwendungsbündel.\n\nEin Information Worker erstellt die RPA-Bot-Pakete, indem er den Benutzeroberflächen-Workflow in einer Entwicklungsumgebung aufzeichnet. Der Worker zeichnet seinen Workflow problemlos auf und erstellt dann ein Paket, das in den Pipeline-Status \"Checked In\" übernommen wird. Zu diesem Zeitpunkt durchläuft das RPA-Bot-Paket einen Build-, Test- und Produktionszyklus. Kontrollpunkte bei jedem Schritt auf dem Weg helfen, die Qualität des RPA-Bots zu garantieren. Da das Paket je nach unterschiedlichen Umgebungen Netzwerk und Sicherheit einbinden kann, können die RPA-Bots von verschiedenen Information Workern in verschiedenen Umgebungen wiederverwendet werden.\n\nEin weiterer Vorteil bei der Integration von RPA-Bots in RPA-Bundles besteht darin, dass die Verwaltung der Tools und Bots über verschiedene Infrastrukturumgebungen hinweg, wie z. B. Legacy-, private und öffentliche Clouds, erfolgen kann. Viele der Service-Orchestrierungstools können automatisch Verbindungen zwischen diesen Infrastrukturumgebungen herstellen, indem sie ein Überlagerungsnetzwerk erstellen. Dadurch verringert sich der manuelle Aufwand der IT-Organisation erheblich, und in vielen Fällen sind alle Schritte im Prozess automatisiert.\n\n## Fallstricke von RPA-Bots\n\nHier ist eine Liste von Dingen, auf die Sie achten sollten, wenn Sie RPA-Bots in Ihren Unternehmenssystemen verwenden.\n\nSicherheit kann ein großes Loch sein, wenn man nicht darauf achtet. Einer der größten Fehler besteht darin, Anwendungen in einem RPA-Bot im privilegierten Modus oder mit \"globalen\" Kontenzugangsdaten auszuführen.\n\nRPAs-Bots sind eng mit den Benutzeroberflächen mehrerer Anwendungen verbunden. Jede kleine Änderung an einer Anwendung erfordert eine erneute Aufnahme des RPA-Bots.\n\nRPA-Bots können Änderungen nicht gut selbstständig bewältigen, sie sind sehr anfällig für Veränderungen in Anwendungen und sogar in der Konfiguration von Anwendungen.\n\n* Die Wiederverwendung ist aufgrund der engen Kopplung mit den Benutzeroberflächen der Anwendung minimal. Einige Tools verwenden Tags anstelle der absoluten Position des Cursors und der Klicks.\n\n* Einige Benutzeroberflächen erlauben RPAs nicht, da sie dynamisch sind. Das bedeutet, dass sie schwer aufzuzeichnen sind.\n\nDie RPA-Branche versucht ihr Bestes, um einige dieser Probleme zu überwinden, die mit dem Aufzeichnungs-/Antwort-Aspekt der Tools verbunden sind. Einige dieser Fallstricke können aufgrund des generalisierten Ansatzes nicht überwunden werden. Andere Optionen wie API-Gateways und funktionale Automatisierung sollten evaluiert werden.\n\n## KI zur Rettung von RPAs\n\nWie bereits in den Fallstricken von RPA erwähnt, ist die Wiederverwendung von Bots ein großes Problem, an dem die Branche arbeitet. Eine der untersuchten Techniken ist der Einsatz von KI und Inferenz zum Umgang mit dynamischen Benutzeroberflächen und kleinen Änderungen an Anwendungen, ohne dass RPA-Bots erneut aufgenommen werden müssen. Mustererkennung und optische Zeichenerkennung sind zwei Bereiche, die genutzt werden, um KI-Modelle zu schulen, die zur Identifizierung von Feldern und Segmenten von Benutzeroberflächen verwendet werden können.\n\nMit diesen KI-Modellen können Bots flexibler sein und sich für die Wiederverwendung in verschiedenen Werkzeugen und ähnlichen Prozessen/Arbeitsabläufen eignen. Ein weiterer Bereich, den RPA-Anbieter untersuchen, ist die Prozessoptimierung mithilfe von KI und maschinellem Lernen.\n\n## Legacy Migration ist eine Reise.\n\nDer RPA-Marktplatz hat neue Energie gewonnen, da Unternehmen ihre IT-Infrastruktur und Prozesse modernisieren möchten. Die Automatisierung aktueller manueller Prozesse durch Aufzeichnung ist ein schneller Erfolg, von dem viele Organisationen profitieren. Allerdings sollte RPA als Übergangsmechanismus anstelle des endgültigen Zustands betrachtet werden. Warum? Viele der aktuellen Informationsprozesse erfordern veraltete Systeme und Richtlinien. Die Automatisierung eines alten Prozesses auf neuer Infrastruktur ist ähnlich wie die Automatisierung der Herstellung von Peitschen für eine Automobilfabrik. Zunächst mag es einen Vorteil geben, aber langfristig ist der Prozess äußerst ineffizient und veraltet. Unabhängig von seiner Geschwindigkeit und Zuverlässigkeit besteht möglicherweise einfach keine Notwendigkeit dafür.\n\n## Schlussfolgerung\n\nRobotic Process Automation-Tools sind eine weitere Gruppe von Werkzeugen, die zur Unterstützung von Organisationen bei ihrer digitalen Transformation von Legacy zu moderneren Recheninfrastrukturen und -prozessen eingesetzt werden können. Die Werkzeuge allein reichen nicht aus, und Sie müssen planen, wie Sie sie nutzen, verwalten und letztendlich ersetzen werden. Hier sind einige hilfreiche Tipps, wenn Sie mit diesen Werkzeugen arbeiten.\n\nBehandeln Sie RPAs als komplexe Dienste, die in Ihrer Multi-Hybrid-Cloud ausgeführt werden.\n\nFühren Sie Ihre RPA-Bots wie andere Anwendungen durch SecDevOps-Workflows aus.\n\n* Fügen Sie Sicherheit und Authentifizierung zur Laufzeit in das RPA-Tool ein.\n\nFinden Sie Möglichkeiten, RPA-Bots in verschiedenen Teilen Ihrer Organisation wiederzuverwenden.\n\nHaben Sie einen Plan, Ihren RPA-Bot durch eine vereinfachte Integration zu ersetzen.\n\n* Suche nach Möglichkeiten, die Legacy-Anwendungen zu verringern (ersetzen oder entfernen).\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT28-de","image":"./episodes/edt-28/de/thumbnail.png","lang":"de","summary":"Eine der wachsenden Bereiche, die bei der Integration von Legacy-Systemen und der Automatisierung von Integration helfen können, ist der Einsatz von Automatisierungstools und Frameworks. In den letzten 3 Jahren hat sich insbesondere ein wesentlicher Schwerpunkt auf die Automatisierung von Arbeitsabläufen mit Legacy- und neuen Cloud-fähigen Anwendungen für Mitarbeiter im Informationsbereich herausgebildet. Diese Tool-Sets werden als Robotic Process Automation (RPA)-Tools bezeichnet. Darren Pulsipher, Chief Solution Architect, Public Sector, Intel, überprüft die Robotic Process Automation (RPA)-Branche und die Tool-Angebote."},{"id":98,"type":"Episode","title":"Nächste Generation der Hyperscale-Datenbank - Aerospike","tags":["aerospike","database","ingestion","optane","data","technology","pmem"],"body":"\r\n\r\n## Nächste Generation Hyperscale-Datenbank\n\nDatenbanken zeigen ihr Alter und benötigen immer noch einige Zeit, um Ergebnisse zu liefern. Aerospike durchbricht jedoch mit Hilfe der Intel-Technologie Barrieren in Bezug auf Geschwindigkeit, Kapazität und geringe Latenz. Tim Faulkes, Vice President of Solutions Architecture bei Aerospike, ist in dieser Folge zu Gast und bespricht die Vorteile ihrer Technologie mit Darren.\n\n## NoSQL - Ein aufstrebender Markt mit mehreren Technologien.\n\nAerospike wurde 2009 gegründet, mit den ersten produktiven Einsätzen im Jahr 2011. Die Mitbegründer waren sich der bevorstehenden Herausforderungen bewusst. Sie haben die Architektur von Grund auf so aufgebaut, dass sie zuverlässig, skalierbar für Unternehmen ist, nie ausfällt und große Datenmengen verarbeiten kann. Die Architektur von Aerospike basierte stark auf SSDs, sowohl auf NVMe-Laufwerken in modernen Ausführungen als auch auf SATA SAS-Laufwerken. Beide befanden sich noch in den Anfängen, daher waren die Gründer zukunftsorientiert in Bezug auf den Markt. Sie verließen sich auf diese Geräte, um Geschwindigkeit zu erreichen, und deshalb war die Intel-Technologie so wichtig. Seitdem haben andere Technologien von Intel, wie der nichtflüchtige Speicher, die Dinge wesentlich vereinfacht.\n\n## Indizes im DRAM, Daten auf SSD\n\nAerospike geht Datenbanken anders an. Die SSDs sind keine schnelleren Festplatten. Eine Festplatte hat physische Sektoren und Köpfe und Dinge, die sie bewegen muss, um Daten zu lesen. Eine SSD kann Tausende von Datenstücken parallel schnell lesen. Es läuft nicht gut auf Rotationslaufwerken, aber mit NVMe/SSD läuft es blitzschnell. Niemand hat die gleiche Art von Leistung, nicht einmal in einer Speicherdatenbank.\n\nDie einzigartige Architektur ist für hohe Durchsatzraten, hohe Datenmengen und sehr geringe Latenzzeiten konzipiert. Zum Beispiel führen einige Kunden regelmäßig 20 Millionen Transaktionen pro Sekunde durch. Einige von ihnen verarbeiten sogar Petabytes an Daten. In der Regel verwenden Kunden hochwertige Hardware, aber die Round-Trip-Zeit ist entscheidend. Aerospike benötigt etwa 200 Mikrosekunden, um Daten abzurufen und an den Client zurückzugeben, nicht zehn- oder hundertfache Millisekunden. Wenn Sie große Datenmengen schnell benötigen, beispielsweise zur Betrugserkennung, oder wenn Sie viele Daten, beispielsweise bei IoT, autonomen Fahrzeugen, Sensorauslesungen oder medizinischen Geräten, verarbeiten, kann Aerospike Millionen Transaktionen pro Sekunde bei Lese- und Schreibzugriffen durchführen. Sie müssen es nicht über einen Nachrichten-Bus senden und dann die Datenbank über den Nachrichten-Bus benachrichtigen lassen, dass sie die Daten erhalten hat. Die Daten sind bereits persistent.\n\n## Führende Industrieinnovationen weltweit ermöglichen\n\nFür welche Anwendungsfälle ist Aerospike optimal geeignet? Es gibt offensichtliche Märkte wie z.B. das zuvor genannte Internet der Dinge (IoT), Betrugsbekämpfung und Sensor-Daten, aber es handelt sich um einen aufstrebenden Markt. Aerospike startete im Bereich der Werbetechnologie, wo Datenabfragen in Millisekunden oder noch schneller bearbeitet werden müssen. Hier wurde die hohe Leistungsfähigkeit unter realen Bedingungen im großen Maßstab erfolgreich getestet.\n\nWeitere Branchen, in denen Aerospike nützlich sein kann, sind vielfältig. Ein Beispiel ist ein großer Telekommunikationsanbieter in Indien. Die Infrastruktur dort ist nicht immer zuverlässig und manchmal werden Anrufe abgebrochen. Dieses Unternehmen kann mit Hilfe der Aerospike-Technologie in Echtzeit feststellen, dass ein Anruf abgebrochen wurde, und sie können sofort Kontakt aufnehmen und dem Kunden Guthaben oder andere Entschädigungen anbieten, um die Kundenzufriedenheit aufrechtzuerhalten.\n\n## Echtzeit-Abwicklung von Sofortzahlungen\n\nEin neuer Bereich, mit dem Aerospike stark involviert ist, sind Echtzeit-Digitalzahlungen. Dies erfordert nicht unbedingt eine große Menge an Daten oder hohe Durchsatzanforderungen, aber was diese Unternehmen benötigen, ist absolute Konstanz und absolute Verfügbarkeit, selbst wenn beispielsweise ein Rechenzentrum verloren geht.\n\nDie Produktionsfläche ist auch ein aufstrebender Markt für die Aerospike-Technologie, beispielsweise in der Halbleiterfertigung, wo sie zur Speicherung von Sensordaten für Echtzeitanalysen verwendet wird. Aerospike glänzt in jeder Branche, in der viel Daten vorhanden sind und man sie schnell benötigt.\n\n## Leistung im großen Maßstab - Ergebnisse unabhängiger Tests von Drittanbietern\n\nEs gibt viele Datenbanken mit guter Technologie, aber es gibt Herausforderungen im Bereich der Skalierung. Nehmen wir zum Beispiel Redis. Da es alle Informationen im Speicher speichert, kann man nicht ohne erhebliche Kosten auf zehn Terabyte oder gar Hunderte von Terabyte oder Petabyte gehen. Da Aerospike Informationen auf SSDs speichert, beträgt der Zeitunterschied zwischen dem Abrufen von Daten auf einer SSD und im Speicher etwa 100 Mikrosekunden. Im Bereich von Petabytes ersetzt Aerospike ältere Technologien wie Cassandra, die zwar gut skalieren, aber an Geschwindigkeit mangelt.\n\n## Gesamtkosten des Besitzes\n\nDarüber hinaus sparen die Menschen im Vergleich zu diesen Technologien beim Umstieg auf Aerospike eine Menge Geld, da die Anzahl der Knoten aufgrund der einzigartigen Architektur dramatisch reduziert wird. Die Einsparungen betreffen nicht nur das Investitionsaufwand, sondern auch den Betriebsaufwand, da weniger Maschinen überwacht werden müssen.\n\nAerospike hat einige der traditionellen Ebenen in der Architektur zusammengebrochen. Oft haben Legacy-Systeme einen zusätzlichen Cache davor, um es zu beschleunigen. Dies führt zu Komplexität. Aerospike benötigt keinen Cache. Es gibt keine große Menge an DRAM. Es verlässt sich auf die Geschwindigkeit der SSDs und die zugrunde liegende Technologie, um die Leistung des Rohspeichers ohne Cache zu erreichen. Da der Cache und die Speicherebenen zusammengebrochen sind, ist die Lösung vereinfacht, was bedeutet, eingebaute Zuverlässigkeit und Geschwindigkeit.\n\n## Kontinentale Bereitstellungsbeispiel\n\nDie Konsistenz über mehrere geografische Gebiete hinweg ist auch ein wichtiger Vorteil der Technologie. Moderne Architektur erfordert geringe Latenz, das bedeutet in der Regel, dass es eine Reihe von H-basierten Clustern gibt, in denen die Daten nah beim Benutzer liegen. Andernfalls wird die Lichtgeschwindigkeit zu einem Faktor. Aerospike kann dafür sorgen, dass alle H-basierten Cluster miteinander kommunizieren, sodass eine Änderung eines Datensatzes in einem Cluster automatisch auf andere Cluster übertragen wird, asynchron. Bei bestimmten Anwendungen wie digitalen Zahlungen muss jedoch eine starke Konsistenz zwischen den Clustern gewährleistet sein, sodass sie die Daten synchron replizieren möchten. Sie möchten lieber von der lokalen Kopie der Daten lesen. Die Lesevorgänge werden dadurch sehr schnell; die Schreibvorgänge werden von der Lichtgeschwindigkeit beeinflusst, garantieren jedoch Konsistenz über geografische Entfernungen hinweg. Es handelt sich also nicht um Konfliktlösung, sondern um Konfliktvermeidung. Die Möglichkeit, diese stark konsistenten Schreibvorgänge weltweit zu verteilen und möglicherweise mehrere Datenbanksysteme zu haben, hat ein enormes Potenzial und Wert.\n\n## Aerospike Connect für Spark\n\nZum Beispiel arbeitet Aerospike derzeit mit Spark zusammen, einer analytischen Technologie, die erfordert, dass die Daten im Arbeitsspeicher liegen. Durch das Laden der Daten von Aerospike aus kann das Spark-Datenrahmen Tausende oder Hunderttausende von Terabytes verarbeiten und dabei enorme Kosteneinsparungen und Geschwindigkeit erzielen. Die Integration mit anderen KI-Werkzeugen ist eine treibende Kraft, die einige aufregende Möglichkeiten eröffnen wird.\n\nDas Einfügen von Daten in Aerospike ist einfach und unkompliziert. Da Anwendungen eine API-Ebene wie die meisten Datenbanken haben, kann Aerospike Daten aus branchenüblichen Quellen wie Kafka und Janus aufnehmen. Die Möglichkeit zur bidirektionalen Kommunikation mit Nachrichtenbussen ergibt Sinn, da Aerospike nicht nur Daten aus diesen Quellen aufnehmen, sondern sie auch aussenden kann. Wenn Aerospike eine verlässliche Quelle ist, wenn Sie einen Datensatz schreiben, können Sie ihn an nachgelagerte Systeme übertragen.\n\n## Aerospike Connect für Kafka\n\nWenn Sie Aerospike für die Ingestion verwenden möchten, anstatt eine Ingestion Engine auf der Front-End-Seite zu verwenden, kann Aerospike entscheiden, ob es die Daten aufteilen oder im selben Format belassen und einfach in Aerospike einfügen möchte. Mit seinem Plugin-Framework ist dies ein API-Aufruf. Wenn die Daten gespeichert sind, kann eine Nachrichten-Warteschlange benachrichtigt werden, um nachgelagerte Verarbeitung durchzuführen. Dadurch wird ein Schritt in der Ingestion eliminiert und die Gesamtdurchsatz erhöht.\n\nDas Skalieren auf Tausende von Clientegeräten ist aufgrund der Mehrfadenarchitektur möglich, mit der der Aerospike-Client entwickelt ist, um auf einem Prozess zu skalieren und eine große Menge an Daten zu verarbeiten. Der Client ist intelligent, um Geschwindigkeit zu gewährleisten. Jeder Datensatz wird direkt vom Client zum Knoten weitergeleitet, der im Besitz dieser Daten ist. Es gibt keine Zwischeninstanzen. Das bedeutet, dass der Client über alle Knoten Bescheid wissen muss, sodass es begrenzte Verbindungsgrenzen gibt und die Verwendung von Hunderttausenden von Clients nicht praktisch ist.\n\n## Kontinentaleinsatz Beispiel\n\nWenn Sie jedoch Hunderttausende von Kunden haben, liegt das daran, dass Sie eine weltweit verteilte Bevölkerung haben. Wenn sie mit einem Cluster sprechen würden, wird die Geschwindigkeit des Lichts zu einem großen Problem. Es gibt Möglichkeiten, kleinere, Edge-basierte Cluster einzurichten, die entweder Informationen teilen oder mit einem zentralen System sprechen, sodass es möglich ist.\n\n## Aerospike ist eine Datenbank für System Record Anwendungen.\n\nWenn Daten weltweit verschoben werden, kommen verschiedene Datenschutzebenen und Standards wie die DSGVO und CCPA ins Spiel, sodass Kunden den Ursprung der Daten kennen müssen. Aerospike hat die Flexibilität, die Daten genau zu betrachten und basierend auf dem Inhalt der Daten zu wissen, wann und wo sie verschickt werden können. Dies ist Teil der Aerospike-Datenversandkonfiguration, die in der Anwendung transparent ist. Sie müssen nichts an den Anwendungen ändern, sondern nur an der Konfiguration.\n\nAerospike stellt auch sicher, dass die Daten sicher sind. Wie alle unternehmensstarken Datenbanken unterstützt Aerospike die Verschlüsselung im Ruhezustand und im Flug sowie die Sicherheitsintegration von Dingen wie Hashicorp Vault. Es kann als ein System der Aufzeichnung genutzt werden.\n\nEin grundlegendes Ziel bei Aerospike besteht darin, die neuesten Innovationen und Technologien für den Erfolg der Kunden einzusetzen. Zum Beispiel haben sie, als Intel PMem auf den Markt brachte, den App-Direct-Modus verwendet, um die vollen Möglichkeiten des PMems optimal auszuschöpfen. Das bedeutet, dass bei einem rollenden Upgrade von beispielsweise einem Petabyte an Informationen die benötigte Zeit gering ist. Aerospike speichert dieses Petabyte möglicherweise in 50 oder 60 Knoten anstelle von Tausenden von Knoten und wenn ein Knoten heruntergefahren wird, muss er nicht neu aufgebaut werden, so dass die Neustartzeit sehr schnell ist.\n\nInsgesamt bietet Aerospike Sicherheit, hohe Verfügbarkeit, Geschwindigkeit, geringe Latenz, Skalierbarkeit, Konsistenz und geringe Gesamtbetriebskosten.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Tim Faulkes"],"link":"/episode-EDT29-de","image":"./episodes/edt-29/de/thumbnail.png","lang":"de","summary":"Datenbanken zeigen ihr Alter und benötigen immer noch Zeit, um Ergebnisse zu liefern. Aerospike durchbricht mit Hilfe von Intel-Technologie (Optane) Geschwindigkeits-, Volumen- und Latenzprobleme. Tim Faulkes, Vice President Solutions Architecture bei Aerospike, spricht mit Darren Pulsipher, Chief Solutions Architect bei Intel, über die Vorteile der Aerospike-Technologie."},{"id":99,"type":"Episode","title":"Vorteile einer Multi-Cloud-Architektur","tags":["multicloud"],"body":"\r\n\r\n## Agilität\n\nAgilität ist die Fähigkeit, sich schnell an veränderte Bedingungen in neuen Technologien, neuen Anwendungen und neuen Bedrohungen wie Sicherheit und Wettbewerb anzupassen. Ein Multi-Hybrid-Modell ermöglicht die Möglichkeit, Dinge zwischen privaten und öffentlichen Clouds oder sogar von Legacy-Infrastruktur zu Clouds zu verschieben. Es mag verlockend sein für Entwicklungsteams, unter engen Terminvorgaben zunächst die öffentliche Cloud zu nutzen, da sie die Infrastruktur schnell hochfahren und herunterfahren können, aber es können Integrationsprobleme in den späten Phasen der Bereitstellung von Anwendungen im Produkt auftreten. Dies kann zu kostspieligen Verzögerungen führen. Durch die Verwendung des Multi-Hybrid-Modells werden viele dieser Integrationspunkte für das Entwicklungsteam frühzeitig sichtbar gemacht. Die Anwendungsentwicklung nutzt dies, um früh und schnell im Entwicklungsprozess zu scheitern.\n\n## Flexibilität\n\nFlexibilität in der Infrastruktur ist eine enge Verwandte der Agilität. Während Agilität die Fähigkeit ist, sich schnell zu bewegen, ist Flexibilität die Fähigkeit zur Veränderung. Um maximale Flexibilität zu erreichen, benötigen Entwickler die Möglichkeit, Anwendungen überall auf privaten oder öffentlichen Clouds oder Legacy-Infrastrukturen bereitzustellen, ohne an einen bestimmten Cloud-Dienst oder eine Infrastruktur gebunden zu sein.\n\nDurch den Einsatz einer Cloud-Management-Plattform (CPM) in einer Multi-Hybrid-Architektur können Workloads je nach Kosten, Sicherheit und Zuverlässigkeit einfach zu verschiedenen Clouds umgeleitet werden.\n\nEin Beispiel für die Folgen mangelnder Flexibilität ist die frühe Entscheidung von Netflix, nur einen Cloud-Service-Anbieter zu nutzen. Diese öffentliche Cloud hatte ein Infrastrukturproblem, das zu Ausfallzeiten führte, und Netflix war mehrere Stunden lang nicht in der Lage, an der Ostküste der Vereinigten Staaten zu streamen. Nach dieser Katastrophe hat Netflix eine Multi-Cloud-Lösung implementiert, sodass sie bei Problemen schnell zu einer anderen Cloud migrieren können. Darüber hinaus sind sie jetzt in der Lage, sich jederzeit gemäß Kosten, Sicherheit und Zuverlässigkeit an den Ort zu bewegen, der am sinnvollsten ist.\n\nDie Anwendungen einer Multi-Hybrid-Cloud sind Portabilität und Betriebsflexibilität. Indem man nicht an die Vorgehensweise einer bestimmten Cloud gebunden ist, können Entwickler Code schreiben, der problemlos zwischen verschiedenen Clouds transportiert werden kann. Außerdem haben Sie die betriebliche Flexibilität, Workloads zwischen verschiedenen Clouds zu verschieben, um Ihren Kunden auf sichere, kostenbewusste und zuverlässige Weise das zu bieten, was sie möchten.\n\n## Vorhersagbare Leistung\n\nÖffentliche Clouds bringen oft einen Preis mit sich: \"lautstarke Nachbarn\", die sich auf die vorhersagende Leistung oder die Qualität des Dienstes (QoS) auswirken können.\n\nBei der Nutzung einer öffentlichen Cloud weiß man oft nicht, was oder wer sonst noch auf derselben Maschine, im Speicherarray oder im Netzwerk läuft. Für einige Arbeitslasten ist dies kein Problem. Wenn Sie jedoch eine Anwendung haben, bei der Sie eine genauere Leistungsvorhersage benötigen, können lärmende Nachbarn oder diejenigen, die die Bandbreite, die Festplatten-E/A, die CPU und andere Ressourcen monopolisieren, die QoS-Ergebnisse beeinträchtigen, die Sie benötigen.\n\nPrivate Clouds können ebenfalls unter lärmenden Nachbarn leiden, aber da Sie die Infrastruktur besitzen und die Anwendungen Ihnen gehören, haben Sie die direkte Möglichkeit, sie zu verwalten. Ein lauter Nachbar in der öffentlichen Cloud ist vergleichbar mit dem Leben in einem Mehrfamilienhaus, wo Sie begrenzte Optionen haben, um mit den feiernden Nachbarn umzugehen. Andererseits ist es bei lärmenden Nachbarn in Ihrer privaten Cloud so, als würde man in einem Haus mit ungestümen Kindern leben, die Sie sofort durch Ressourcenbeschränkungen in den Griff bekommen können.\n\nEine Hybrid-Cloud-Strategie ermöglicht es Ihnen, \"sensible\" Arbeitslasten und Anwendungen in privaten Clouds und andere Arbeitslasten und Anwendungen in öffentlicher Cloud-Infrastruktur für Kosten- und Ressourceneffizienz zu platzieren. Viele Hybrid-Tools ermöglichen es Ihnen, Arbeitslasten mit QoS-Anforderungen zu charakterisieren, um bei der automatischen und optimalen Platzierung von Arbeitslasten auf verschiedenen Cloud-Infrastrukturen zu unterstützen.\n\n## Sicherheit und Einhaltung.\n\nEs gibt einige Gefahren, wenn Dinge in einer automatisierten Weise in öffentlichen und privaten Clouds durchgeführt werden. Wenn jedoch Sicherheit in die Multi-Hybrid-Architektur integriert wird, wird diese Automatisierung zu einem Vorteil, denn dadurch kann ein Sicherheitsprofil auf alle Ihre Cloud-Ressourcen, egal ob private oder öffentliche, sowie auf die Legacy-Infrastruktur angewendet werden. Dieses gemeinsame Sicherheitsprofil wird überall angewendet und Anwendungen werden entsprechend diesen Profilen bereitgestellt.\n\nZum Beispiel, in einer privaten Cloud, wenn du eine spezifische Art von Sicherheit möchtest, wie zum Beispiel das Binden einer bestimmten Anwendung an eine bestimmte Maschine, die nur in dieser privaten Cloud läuft, kann dies problemlos eine Anforderung im Profil eines multi-hybriden Systems sein.\n\nIn einem Multi-Hybrid-System gibt es auch viele großartige Tools zur Überprüfung und Überwachung Ihrer Infrastruktur. Sie können nicht nur überwachen, was in Ihrer privaten Cloud passiert, sondern auch in der öffentlichen Cloud, um Sie vor bösartigen Angriffen zu warnen, die potenziell Ihre private Cloud oder Legacy-Infrastruktur gefährden könnten.\n\n## Effizienz\n\nEs gibt viele widersprüchliche Ideen über Effizienz. Wenn Sie verschiedene Rechner für die Gesamtbetriebskosten (TCO) betrachten, finden Sie unterschiedliche Antworten darüber, ob private oder öffentliche Clouds kosteneffizienter sind. Eine multi-hybride Cloud-Lösung kann dieses Dilemma für Ihre Organisation durch eine erhöhte Sichtbarkeit und dynamische Bereitstellung lösen.\n\nMit einer Multi-Hybrid-Cloud-Architektur und einem intelligenten Orchestrator kann Ihr Orchestrator Telemetrie aus Ihren privaten und öffentlichen Clouds sowie aus Ihrer Legacy-Infrastruktur nutzen, um optimale Entscheidungen darüber zu treffen, wo die Workload aktuell und zukünftig platziert werden sollte. Ein weiterer Vorteil dieser Sichtbarkeit ist, dass Sie entscheiden können, ob es effizient ist, Anwendungen kontinuierlich auszuführen. Ein Beispiel hierfür ist, wie wir dem Systems der kanadischen Regierung geholfen haben, effizienter zu arbeiten, indem wir die Kosten für die Ausführung einer Anwendung in der öffentlichen Cloud reduziert haben, wenn sie nicht genutzt wurde. Anstatt eine bestimmte Anwendung rund um die Uhr auszuführen, wird sie jetzt gemäß der tatsächlichen Nutzung 18 Stunden an 5 Tagen pro Woche ausgeführt.\n\nDiese Sichtbarkeit wird Ihnen auch dabei helfen, End-of-Life (EOL)-Workloads und Anwendungen zu identifizieren und zu eliminieren, was echtes Geld spart. In der privaten Cloud werden dadurch Ressourcen freigesetzt, die für andere Workloads genutzt werden können und somit Ihre Effizienz steigern. Darüber hinaus suchen Cloud-Broker in der CMP der Multi-Hybrid-Architektur im Grunde nach dem niedrigsten Preis, während sie gleichzeitig die QoS für den spezifischen Workload gewährleisten. Dies senkt die Gesamtkosten für die Ausführung des Workloads und gibt Ihnen auch Einblick in die tatsächlichen Kosten für die Nutzung einer bestimmten öffentlichen oder privaten Cloud.\n\nSichtbarkeit der Kosten über Clouds und Legacy-Infrastruktur\n\n* Treiben Sie Workloads und Anwendungen zu den niedrigsten Kosten, während Sie dieselben Service-Level-Vereinbarungen einhalten.\n\nFördern Sie eine höhere Nutzung der privaten Cloud-Infrastruktur.\n\n## Aufforderung zum Handeln\n\nMehrstufige Hybrid-Cloud-Architekturen ermöglichen es CIOs, den Anforderungen ihrer Kunden gerecht zu werden, aber es sind immer noch einige schwere Aufgaben zu erledigen. Der Aufbau einer Multi-Hybrid-Cloud-Strategie umfasst organisatorische, verhaltensbezogene und technische Veränderungen, die nicht über Nacht geschehen können. Die Entwicklung einer soliden architektonischen Vision und eines Fahrplans ist entscheidend für die Einführung einer Multi-Hybrid-Cloud-Strategie, die die Stärken von Mehrstufigen Hybrid-Clouds nutzt und die ineffizienten Ablenkungen der technischen Industrie verhindert.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT3-de","image":"./episodes/edt-3/de/thumbnail.png","lang":"de","summary":"Eine Multi-Hybrid-Cloud-Architektur ermöglicht es Organisationen, die Vorteile sowohl von privaten als auch von öffentlichen Clouds zu nutzen und Ressourcen- und Kosteneffizienz zu optimieren. Dieses Modell hat fünf Hauptvorteile: Agilität, Flexibilität, vorausschauende Leistung, Sicherheit und Compliance sowie Effizienz."},{"id":100,"type":"Episode","title":"Anwendungstragbarkeit mit OneAPI","tags":["aiml","devops","compute","technology","process","oneapi","aiops","developer","people"],"body":"\r\n\r\nMit oneAPI hat Intel eine vereinheitlichte Softwareumgebung für die Entwicklung geschaffen, die auf die Datenverarbeitung ausgerichtet ist. Es handelt sich um datenparalleles Programmieren, das auf einem Open-Source C++ basiert. Verschiedene Arten von Bibliotheken wie Intel's MKL, DNN und andere Open-Source-Projekte sind Teil des oneAPI-Toolkits, zusammen mit Zubehör wie einem CUDA-Übersetzer. Sie können Software in oneAPI entwickeln und sie dann je nach Bedarf auf die verschiedenen Bibliotheken verweisen. Es hat die Komplexität beseitigt, eine andere Sprache für unterschiedliche Architekturen zu erlernen.\n\n## oneAPI Industry Initiative – Alternative zur Lösung von Einzelanbietern\n\nGrundsätzlich kann ein Softwareentwickler einmalig Code schreiben, der auf verschiedenen Prozessoren ausgeführt wird: CPU, GPU, FPGA, NPU und VPU. Je nach Architektur und verwendeten Bibliotheken könnte eine erneute Kompilierung erforderlich sein, jedoch sind keine Code-Neuschreibungen erforderlich.\n\n## Starke API-Bibliotheken\n\nDiese Version ist nur der erste Schritt; Intel und andere werden weiterhin mit der Integration von künstlicher Intelligenz beschleunigenden Elementen entwerfen, zum Beispiel. Die Idee ist, dass sie sich weiterentwickeln wird, um Entwicklern wesentlich mehr Flexibilität zu ermöglichen und dass Abstraktion vielen Menschen ermöglichen wird, einfacher zu entwerfen und zu programmieren, vor allem aus Sicht der Datenwissenschaft und künstlichen Intelligenz.\n\nPraktisch gesehen könnte ein Softwareingenieur etwas auf ihrem Laptop schreiben, es dort ausprobieren und dann denselben Code auf einer Cloud ausführen, die mit neuronalen Prozessoren, GPUs oder FPGAs vollständig ausgestattet ist. Dies könnte besonders nützlich im öffentlichen Sektor sein, wo Ingenieure spezielle Apps schreiben, die am Randverarbeitung durchführen, vielleicht mit einem FPGA. Sie müssen nicht unbedingt eine vollständige Umgebung für ihre Arbeit haben.\n\nEin weiterer aufregender Aspekt ist, dass once Intel maschinelles Lernen integriert, könnte oneAPI den Code durchgehen und angeben, welche Teile am besten auf verschiedenen Prozessoren laufen würden. Es würde den Code an die richtigen Stellen schieben, um die beste Geschwindigkeit und Leistung zu erzielen.\n\n## oneAPI-Brancheninitiative - Alternative zur Lösung von einzelnen Anbietern\n\nEs gibt viele KI-Frameworks da draußen, aber oneAPI ermöglicht die Migration von jedem Art von proprietärem Code zu einer Open-Source-Programmiersprache. Es basiert auf SYCL und wurde unter einem gesamten Industriekonsortium namens Khronos Group entwickelt, also handelt es sich um eine Art Entwicklungsframework.\n\n## Leistungsstarke API-Bibliotheken\n\nOneAPI hat zwanzig bis dreißig Bibliotheken wie MKL, Bibliotheken für neuronale Netzwerke und maschinelles Lernen, Open CNN oder DNN. Alle sind offen und Teil eines größeren Konsortiums.\n\nDarüber hinaus arbeitet Intel daran, das TensorFlow-Framework und die Bibliotheken in oneAPI zu integrieren. Viele Frameworks wurden bereits von Intel optimiert und sie werden in die gleichen Bibliotheken eingebunden oder nutzen diese, sodass Benutzer sie verwenden oder darauf aufbauen können.\n\n## Ressourcen\n\nIntel hat oneAPI Anfang November 2020 allgemein verfügbar gemacht, und es war ein Höhepunkt der virtuellen Super Computing-Konvention vom 17. bis 19. November.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT30-de","image":"./episodes/edt-30/de/thumbnail.png","lang":"de","summary":"Mit oneAPI hat Intel eine einheitliche Softwareumgebung für die Entwicklung geschaffen, die auf die Datenverarbeitung ausgerichtet ist. Gretchen Stewart, Chief Data Scientist im öffentlichen Sektor bei Intel, spricht mit Darren Pulsipher, Chief Solution Architect bei Intel, über diese Technologie, die es überflüssig macht, für unterschiedliche Architekturen verschiedene Programmiersprachen zu verwenden."},{"id":101,"type":"Episode","title":"Die Verbesserung des Mitarbeiterwerts mit Hilfe der Katalyse","tags":["rpa","automation","catalytic","compute","process","technology"],"body":"\r\n\r\n## Katalytisch ist speziell für den Betrieb und wiederkehrende Geschäftsprozesse konzipiert.\n\nDie RPA-Technologie-Linie stammt aus dem Bereich der Testautomatisierung und Excel-Makros, während die geschäftliche Abstammung im Outsourcing liegt. Catalytic hingegen betrachtet seine Abstammung als Software, wobei sein System als die nächste Evolution bei der Erstellung maßgeschneiderter Software gilt. Mitgründer und CEO Sean Chous Interesse liegt in den Back-Office-Operationen und der Effizienz. Diese enge Fokussierung ermöglicht es Catalytic, KI in vollem Umfang einzusetzen und etwas aufzubauen, das im No-Code-Bereich zugänglich ist.\n\nCatalytic verwendet KI auf verschiedene taktische Weisen in der Anwendung, um den Bedarf an Fachkenntnissen zu reduzieren.\n\nEin Beispiel dafür, einer der schwierigsten Teile beim Erlernen einer beliebigen Plattform ist das Verständnis ihrer Fähigkeiten. Eine Möglichkeit, künstliche Intelligenz (KI) zu nutzen, um dieses Problem auszugleichen, besteht darin, die natürliche Sprachverarbeitung zu verwenden. Die Personen können einfach eingeben, was sie erreichen möchten, und die Anwendung kann ableiten und sagen, welche Aktion am besten hilft, diesen Schritt zu erreichen. Es ist im Grunde eine Möglichkeit, das Produkterlebnis zu verbessern und das erforderliche Fachwissen zu reduzieren.\n\nEin weiteres Beispiel ist, dass es innerhalb des Arbeitsablaufs sechs verschiedene Module mit unterschiedlichen Aktionen gibt, die jeweils logisch nach ihrem beabsichtigten Zweck benannt sind, wie zum Beispiel Datenaufbereitung, Dokumentenbearbeitung usw. Innerhalb jedes Moduls gibt es verschiedene KI-Aktionen, die die Nutzer in ihrem Prozess verwenden können, wie beispielsweise optische Zeichenerkennung (OCR), Stimmungsanalyse (ist die Person, die die E-Mail geschrieben hat, wütend?) und andere Aktionen zur natürlichen Sprachverarbeitung.\n\nEs gibt andere Elemente, die weniger auf KI basieren und mehr nützliche Funktionen bieten, wie z.B. Informationen über eine Person aufgrund ihrer E-Mail-Adresse bereitzustellen.\n\nViele Daten fließen durch die Prozesse des Backoffice. Leider ist die Systemsicht der Daten oft sehr fragmentiert. Der katalytische Workflow kann über Systeme hinwegschnitzen und alles erfassen. Die Daten werden in Datenbanken gespeichert und es ist einfach, aufgrund dieser Datenbanken maschinelle Lernmodelle zu entwickeln. Dadurch können zukünftige Ergebnisse anderer Workflows vorhergesagt werden.\n\nCatalytic nutzt KI in diesen und anderen spezifischen Zusammenhängen, um Effizienz zu steigern und Einblicke sowie Transparenz zu ermöglichen.\n\n## Schließen Sie die letzte Meile der Digitalisierung, indem Sie Workflow-Lösungen um die Art und Weise, wie Menschen arbeiten, aufbauen.\n\nZum Beispiel wird operative Intelligenz durch KI in der Plattform durch eine Funktion namens Insights erfasst. Für jede Arbeitsablauf kann Ihnen dieser Tab anzeigen, wie lange es für alle verschiedenen Schritte dauert, wie oft sie unterbrochen werden und ob es sich um menschliche Fehler oder Integrationsfehler handelt. Es bietet Ihnen Möglichkeiten zur Verbesserung. Gleichzeitig kann der maschinelle Lernalgorithmus damit beginnen, die Ergebnisse von Felddaten vorherzusagen. Daher kann maschinelles Lernen auf vielfältige, verwandte Weise verwendet werden.\n\n## Trotz Investition um Investition existiert immer noch eine \"Letzte Meile\"-Lücke, die manuell geschlossen wird.\n\nDie meisten Kunden, die im Laufe der Jahre in mehrere Technologien investiert haben, haben immer noch das Gefühl, dass es eine Kluft zwischen der Investition und den gewünschten Geschäftsergebnissen gibt. Diese Lücke wird in der Regel von Personen geschlossen, die Aufgaben erledigen, die stattdessen von Software erledigt werden sollten, wie beispielsweise das Kopieren und Einfügen von Informationen von einem System in ein anderes. Manchmal wird die Hinzufügung von Software zu dieser Mischung die Dinge komplizieren, indem zusätzliche Schritte und Schulungen erforderlich sind. Die Lösung von Catalytic für diese Ineffizienzen ist ein wiederverwendbares und erweiterbares System.\n\nEs ist unmöglich für jede Standardsoftware, die individuellen Bedürfnisse jedes Unternehmens zu erfüllen, und es ist ineffizient und zeitaufwendig, den letzten Wert daraus zu sehen. Daher müssen sich Unternehmen der Software anpassen oder die Software an ihr Geschäft anpassen. Der Wert einer WordPress-artigen Lösung ist offensichtlich. Wenn die Macht tatsächlich in den Händen der Mitarbeiter liegt, können Sie eine wirklich personalisierte Software haben. Catalytic möchte diese Macht für jeden Geschäftsprozess nutzen, indem sie RPAs in Verbindung mit Aktionen einsetzen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sean Chou","Neil Bahandur"],"link":"/episode-EDT31-de","image":"./episodes/edt-31/de/thumbnail.png","lang":"de","summary":"In Teil eins von zwei Episoden sprechen Sean Chou, CEO von Catalytic, und Neil Bahandur, Leiter der Partnerschaften bei Catalytic, mit Darren über die Technologie von Catalytic und wie RPAs den Mitarbeitern helfen können, durch die Automatisierung wiederholbarer Prozesse im Backoffice wertvoller zu werden."},{"id":102,"type":"Episode","title":"Kein Code (RPA) Ansatz zur Effizienzsteigerung im Back-Office","tags":["rpa","automation","catalytic","compute","process","technology"],"body":"\r\n\r\n## Was bedeutet katalytisch?\n\nCatalytic ist eine No-Code-Cloud-Plattform zum Erstellen von Workflow-Lösungen, die Ihre Backoffice-Abläufe verbessern und automatisieren.\n\nEine nützliche Analogie, um den Menschen zu helfen zu verstehen, was Catalytic für den Arbeitsablauf im Backoffice tun kann, ist das, was Wordpress für das Veröffentlichen von Websites getan hat. Vor Wordpress war das Erstellen einer Website eine komplexe Aufgabe, die mehrere Personen mit unterschiedlichen spezialisierten Fähigkeiten erforderte. Mit Wordpress und nachfolgender, fortschrittlicherer Software ist das Erstellen einer Website eine relativ einfache Aufgabe. Ein Designer oder ein Inhalts-Experte zum Beispiel kann problemlos eine Website alleine erstellen.\n\nDas Ziel von Catalytic besteht darin, die komplexen und unübersichtlichen Systeme des Back-Office ähnlich zu vereinfachen und Effizienz sowie Lösungen mittels Robotic Process Automation (RPA) zu schaffen. Jemand mit etwas technischer Kenntnis sollte nun mit der Technologie von Catalytic in der Lage sein, unabhängig eine Lösung für das Back-Office zu erstellen.\n\nEine häufige Angst, dass RPA Arbeitsplätze kostet, ist falsch; tatsächlich schafft es neue, wertvollere Möglichkeiten. Zum Beispiel gibt es jetzt mit Wordpress die Rolle des Wordpress-Entwicklers, daher hat sich dieser Fortschritt nicht auf die Fähigkeiten der Entwickler ausgewirkt, sondern neue Möglichkeiten geschaffen, wie Menschen diese Fähigkeiten in etwas Skalierbares einbringen können. Die Mitarbeiter im Backoffice können ebenfalls ihre Fähigkeiten nutzen, um Mehrwert zu schaffen oder die Kundenerfahrung zu verbessern, anstatt sich auf banale, grundlegende Arbeiten in ineffizienten Prozessen zu konzentrieren.\n\n## Katalytische Evolution und Differenzierung\n\nCatalytic unterscheidet sich von seinen Konkurrenten zunächst durch seine Wurzeln. Die aktuelle RPA-Branche entstand aus zwei evolutionären Linien, eine stammt von Excel-Makros und die andere von UI-Tests. Catalytic kommt aus der Dev Ops-Welt und automatisiert Backoffice-Prozesse durch Skripterstellung statt eines bildschirmorientierten Automatisierungsansatzes. Als Sean Chou Catalytic mitbegründete, dachte er in Bezug auf Projektmanagement und wie man Geschäftsprozesse orchestrieren kann.\n\n## Katalytisch ist speziell für den Betrieb und wiederkehrende Geschäftsprozesse entwickelt.\n\nChou erkannte, dass da 85% oder mehr des Geschäfts wie gewohnt abläuft, viele Abläufe automatisiert werden könnten, so dass die Plattform einige dieser Aufgaben erledigen könnte. Es begann mit einem einfachen, automatisierten Benachrichtigungssystem, das E-Mails von Mitarbeitern ersetzte und Catalytic auf den Weg brachte, immer mehr Aktionen zu erstellen. Um die Fähigkeiten der Plattform zu verstärken, schuf Catalytic ein Ökosystem mit Drittanbietern wie Google, um deren Technologien zu nutzen. Durch das Ökosystem gibt es auch den Vorteil eines zusammensetzbaren Geschäftsvorgangs, bei dem Personen einen Workflow auf der Plattform erstellen und speichern können, im Wesentlichen als einzelne Arbeitseinheit. Zum Beispiel, wenn es sechs standardisierte Schritte zur Erfassung eines Kundendatensatzes gibt, können Sie dies einmal erstellen und als Aktion auf der Plattform speichern, um es dann mit Ihrem gesamten Team teilen zu können. Dieses Teilen ähnelt dem Ansatz eines Softwareentwicklers zur Lösung schwieriger Probleme durch das Teilen von Code, es sind jedoch Aktionen anstelle von Code. Das Komponieren wird somit viel einfacher.\n\nCatalytic hat einen durchdachten Ansatz für die Bereitstellung, Bereitstellung und Verwaltung von allem auf ihrer Plattform gewählt. Die Cloud ist kein Nachgedanke; Sie haben eine Cloud-Plattform von Grund auf erstellt. Alles dreht sich um die Cloud, obwohl sie auch die Möglichkeit haben, mit On-Premise-Systemen und hybriden Umgebungen zu arbeiten.\n\n## Katalytisch ist darauf ausgelegt, ein Bürgerentwicklungsprogramm auf Unternehmensebene zu ermöglichen.\n\nDas System wurde in einer DevOps-Umgebung entwickelt und funktioniert als Build-Manager für das Unternehmen, bei dem Geschäftsleute es tatsächlich selbst erledigen können. Alte Build-Systeme wie Clear Case waren schwer zu verwalten, aber Catalytic hat die Dinge vereinfacht. Es entfernt nicht nur zusätzliche Schritte, sondern ändert auch die Arbeitsaufteilung. Catalytic richtet sich an Personen, die dem eigentlichen Prozess näher stehen, um Übergaben zu vermeiden. Effizienz ist das zentrale Konzept. Ein effizientes Build-System, das Fehler erkennt und ablehnt, bevor es sie übergibt, ist für den Erfolg des automatisierten Prozesses unerlässlich.\n\nDie Magie der Build-Systeme besteht darin, dass sie mächtiger werden, je mehr Teile Ihrer Infrastruktur sie einbinden. Zum Beispiel kann es mit Ihrem SCM-System interagieren, um den Code einzubinden, mit Ihren Servern zusammenarbeiten, um Bereitstellungen durchzuführen, oder mit Rational Robot interagieren, um automatisierte Tests durchzuführen. Natürlich dreht sich alles um die Aktionen. Je mehr Aktionen digitalisiert werden, desto mehr kann das System erfassen und umso mächtiger wird es.\n\nEin weiterer Unterschied bei Catalytic ist die Betonung darauf, dass Mitarbeiter hochwertige Arbeit leisten. Das Konzept von niedriger und hoher Wertigkeit in RPA ist gemeinhin bekannt, jedoch möchte Catalytic anstatt es als Schwert zu nutzen, um Gruppen zu trennen, es als Schild verwenden, um Menschen zu schützen und ihnen dabei zu helfen, ihre Zeit optimal zu nutzen und einen Wettbewerbsvorteil zu erlangen.\n\nIm nächsten Podcast werden Darren, Sean und Neil das Gespräch über den no-code Ansatz zur Effizienzsteigerung im Backoffice fortsetzen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sean Chou","Neil Bahandur"],"link":"/episode-EDT32-de","image":"./episodes/edt-32/de/thumbnail.png","lang":"de","summary":"Die heutige Folge ist Teil 2 eines Gesprächs mit Sean Chou, CEO von Catalytic, und Neil Bahadur, Leiter der Partnerschaften bei Catalytic. Sie sprechen mit Darren über ihren No-Code-Ansatz zur Effizienzsteigerung im Backoffice mit einer Plattform, die RPA- und KI-Technologie nutzt."},{"id":103,"type":"Episode","title":"Der AWS-Ausfall im November 2020","tags":["aws","cloudoutage","compute","csp","cloud","process","cloudreliability","multicloud","reliability"],"body":"\r\n\r\n## Was wir aus dem AWS-Ausfall gelernt haben\n\nAmazon Web Services (AWS) hatte zu einem ungünstigen Zeitpunkt eine erhebliche Störung – am Tag vor Thanksgiving 2020. Da sie ihre Service-Protokolle veröffentlicht haben, ist es interessant, diese zu überprüfen, um herauszufinden, was passiert ist und was wir daraus lernen können.\n\n## Zeitstrahl des AWS-Ausfalls\n\nAm Mittwoch, den 25. November 2020 gegen 3:00 Uhr PST, aktualisierte AWS die Ost-Region und fügte Server hinzu, um die Kapazität von Kinesis zu erhöhen. Kinesis ist ein beliebter KI-Dienst, der Musterabgleiche in Logdateien und Videodateien durchführt. Etwa anderthalb Stunden nach dem Upgrade begannen die Serveralarme Fehler in den Kinesis-Datensätzen zu melden. Ziemlich schnell, gegen 8 Uhr, identifizierten sie zunächst ein paar Kandidaten in den Front-End-Services als Ursache. Kinesis besteht aus verschiedenen kleineren Diensten, einer Gruppe von Front-End-Services und einer Gruppe von Back-End-Services. Wie das System aufgebaut ist, wird jedes Mal, wenn ein Back-End-Service läuft, ein Thread im Front-End gestartet. Eines der Probleme war, dass das Front-End die Thread-Grenzwerte erreichte, als sie während des Upgrades Server zum Back-End hinzufügten. Sobald sie dieses Problem erkannt hatten, führten sie eine vorübergehende Lösung mit Updates (Patches) für das Betriebssystem durch und Kinesis war gegen ungefähr 10:30 Uhr wieder online und wurde bis 1:15 Uhr am 26. November vollständig wiederhergestellt. Letztendlich war Kinesis 21 Stunden lang nicht voll funktionsfähig und obwohl nicht jeder dieses KI-Tool verwendet, war die Auswirkung weitreichend.\n\n## Kinesis Impact auf Deutsch übersetzen.\n\nMehrere andere Amazon-Dienste nutzen Kinesis, wie zum Beispiel Amazon Cognito und CloudWatch, und sie erlitten unterschiedliche Störungen. Cognito war bis etwa 14 Uhr überlastet; CloudWatch war bis etwa 22 Uhr nicht verfügbar. In einem Dominoeffekt waren auch von CloudWatch abhängige Dienste wie Lambda und EventBridge betroffen. Da EventBridge nicht verfügbar war, waren auch die Container-Dienste LCS und LKS betroffen.\n\nDie Stromausfälle traten nur in der Ostregion auf, und AWS hat schnell Kapazitäten in den anderen Regionen hinzugefügt, um zu verhindern, dass Kinesis ähnlich scheitert. Während des Ausfalls stießen Benutzer in der Ostregion jedoch auf ein verwirrendes Problem, da ihre Service-Dashboard und persönlichen Service-Dashboards keine Informationen empfingen und falsche positives anzeigten. Daher untersuchten viele andere IT-Organisationen die Probleme, da sie nicht die richtigen Informationen erhielten. Überraschenderweise hat AWS den Vorfall vollständig offengelegt, sodass es eine großartige Lernerfahrung ist.\n\n## Lektionen gelernt\n\nEine der ersten Lektionen ist, dass einfache Eingriffe in die Infrastruktur wie Kapazitätserhöhungen verstanden und geplant werden müssen. Offensichtlich hat AWS die Upgrades nicht einfach spontan vorgenommen, aber sie haben die Auswirkungen nicht vollständig verstanden. Selbst wenn ein Vorgang standardmäßig erscheint, ist es immer eine gute Idee, in einem Testumfeld bis zum Versagen zu laufen, auch wenn es mehr Zeit kostet. Dies ist besonders wichtig bei Diensten, die für andere Kernabhängigkeiten von Bedeutung sind.\n\nDarüber hinaus müssen Service- oder Microservice-Architekturen ihren gesamten Abhängigkeitsbaum verstehen, damit sie bei Störungen Probleme beheben können. Es ist außerdem wichtig, so locker wie möglich an eine Abhängigkeit gekoppelt zu sein und defensives Programmieren in Microservices einzusetzen, um den Dominoeffekt zu vermeiden, der in diesem Fall auftrat.\n\nEin weiteres Problem, auf das man achten sollte, sind zirkuläre Abhängigkeiten. Wenn in einer Kette von voneinander abhängigen Mikrodiensten einer ausfällt, kann alles zum Stillstand kommen und Ressourcen verbraucht werden, ohne etwas zu erreichen.\n\nBei der Entwicklung von Mikrodiensten sollten Sie daran denken, dass Sie nicht immer eine Verbindung zu den von Ihnen abhängigen Diensten haben werden. Das Schreiben von Programmen, die im degradierten Zustand ausgeführt werden können oder zumindest anzeigen können, dass ein Dienst nicht funktioniert, kann Zeit und Mühe sparen. Bei diesem Ausfall sollten Sie bedenken, dass die Dashboards grün liefen, obwohl keine neuen Informationen durchkamen.\n\n## Wer hat den Ausfall am besten bewältigt?\n\nViele Unternehmen waren von dem AWS-Ausfall betroffen, einschließlich einiger, die zu Amazon selbst gehören. Einige flogen fast unversehrt durch den Ausfall, während andere mehr Schwierigkeiten hatten, sich zu erholen. Diejenigen, die schnell reagieren konnten, hatten eine Multi-Hybrid-Cloud-Strategie, so dass sie alternative Clouds als Backup hatten. Einige nutzten eine andere Region von AWS, während andere Google oder Azure verwendeten, und einige führten es sogar zurück zu ihren eigenen Rechenzentren oder externen Websites. Mindestens wurden auf den Websites Nachrichten angezeigt, dass sie derzeit Probleme haben, anstatt eines 404-Fehlers.\n\nUnternehmen, die nicht einige der spezialisierten Dienste von Amazon nutzten, schnitten ebenfalls besser ab. Zum Beispiel waren die EKS- und ECS-Managed-Container-Angebote sowie Lambda stark betroffen und blieben für eine erhebliche Zeitdauer offline, wodurch diejenigen, die von diesen Diensten abhängig waren, keine Alternativen hatten.\n\nWährend des Ausfalls funktionierten die automatischen Skalierungsdienste nicht richtig, daher musste jedes Unternehmen, das zu dieser Zeit viel Verkehr hatte, herausfinden, was los war und manuell die Dienste hochskalieren. Dies war ein größeres Problem als gewöhnlich, da es der Tag vor Thanksgiving war, eine Zeit, in der viele Verbraucher verreisen und online einkaufen. Zum Beispiel blieb Etsy online, aber sie konnten nicht so viel skalieren wie normalerweise, was zu einem Rückgang der Verkäufe führte.\n\nDie Unternehmen, die über eine eigene externe Überwachung verfügten, schnitten auch besser ab. Einige benachrichtigten sogar AWS über die Ausfälle. Sie waren nicht nur von dem Gesundheits-Dashboard von AWS abhängig, sondern hatten ihre eigene Überwachung auf ihren Servern laufen.\n\nWelche Hauptlehre sollten wir daraus ziehen? Organisationen sollten ihre Cloud-Ressourcen genauso wie Versorgungsunternehmen übernehmen. Genauso wie man einen Notstromgenerator für die Elektrizität hat, um den Geschäftsbetrieb bei einem Ausfall aufrechtzuerhalten, sollten Unternehmen dieselben bewährten Praktiken für Cloud-Services nutzen. Das bedeutet, dass es für das Überstehen einer Krise wie dem Ausfall von AWS unerlässlich ist, eine Backup-Cloud zu haben, die einen Betrieb auch bei eingeschränkter Kapazität ermöglicht.\n\n## Ressourcen\n\nhttps://aws.amazon.com/message/11201/ translates to German as: * https://aws.amazon.com/message/11201/\n\nhttps://www.theverge.com/2020/11/25/21719396/amazon-web-services-aws-outage-down-internet\n\n* Amazon Web Services (AWS) Ausfall beeinträchtigt das Internet\n\nhttps://www.zdnet.com/article/amazon-heres-what-caused-major-aws-outage-last-week-apologies can be translated to German as follows: \n\n* https://www.zdnet.com/article/amazon-hier-ist-was-den-großen-aws-ausfall-in-der-letzten-woche-verursacht-hat-entschuldigungen\n\nhttps://www.wsj.com/articles/amazon-web-services-hit-by-outage-11606326714\n\n\nErklärt: Amazon-Ausfall einfach gemacht\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT33-de","image":"./episodes/edt-33/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect, Public Sector, bei Intel spricht über die aus dem AWS-Ausfall im November 2020 gewonnenen Erkenntnisse und vorbeugende Lösungen zur Bewältigung solcher Ausfälle."},{"id":104,"type":"Episode","title":"Umarmung der Evolution des Arbeitsbereichs","tags":["covid","mfa","remoteworker","cybersecurity","people","technology","vdi","vpn"],"body":"\r\n\r\n## Digitale Arbeitsplätze entwickeln sich weiter.\n\nDa wir das anfängliche Chaos der durch die Pandemie bedingten Übergänge hinter uns haben, müssen wir uns nun fragen, was als Nächstes kommt und wie wir das Gelernte nutzen können, um bewusst in das zu investieren, wo wir sein wollen.\n\nEine wichtige Lektion war, dass die Organisationen, die bereits agil waren, gut zurechtkamen. Sie konnten ihre Fernarbeit schnell umsetzen. Wir wissen nicht genau, wie die neue Normalität aussehen wird, aber wir wissen, dass viele der in den letzten acht Monaten beschleunigten Aspekte erhalten bleiben werden, wie die Förderung einer Fernarbeit und gleichzeitig einer höheren Zusammenarbeit. Intel möchte neue Kunden erreichen, die mehr Ressourcen benötigen, um außerhalb ihrer eigenen vier Wände agiler zu werden.\n\n## Ihr Arbeitsplatz benötigt neue Fähigkeiten. Intel überbrückt die Lücke.\n\nIntel engagiert sich durch die Schaffung von Lösungen, indem es Kunden dabei hilft, das umfangreiche Intel-Ökosystem mit verschiedenen Modellen zu verstehen, die Lücken bei Leistung, Stabilität, Effizienz und Preis schließen können. Intel kann die Erfahrungen von Tausenden von Kunden teilen, um Probleme zu lösen und Fähigkeiten bereitzustellen, die IT-Organisationen manchmal nicht eigenständig bewältigen können.\n\n## Geschäftsanforderungen haben oberste Priorität\n\nIn der Vergangenheit lag der Fokus eines CIOs möglicherweise nur auf drei Dingen: Sicherheit, Widerstandsfähigkeit und Effizienz. Das ist heute nicht mehr der Fall. Was bisher gleichbedeutend damit war, die Lichter am Laufen zu halten, Ärger zu vermeiden und effizient zu arbeiten, ist jetzt viel komplexer. Unternehmen müssen vorausschauend sein, um den Mitarbeiter, den Arbeitsplatz und die Veränderungen, die sie bezüglich der Erfüllung von Kunden- und Lieferantenverpflichtungen durchlaufen, zu verbessern.\n\nDarüber hinaus ist Agilität eine Voraussetzung, da sich die Umstände schnell ändern können und Unternehmen sich in mehreren Richtungen anpassen müssen, um diesen Anforderungen gerecht zu werden.\n\nSchließlich ist eine zunehmende Automatisierung durch künstliche Intelligenz ein Hilfsmittel zur Förderung von Bereichen wie besserer Zusammenarbeit und Verbesserung der Arbeitnehmererfahrung. Mit einer weltweit verteilten Belegschaft gibt es zum Beispiel keinen Helpdesk mit einer Person, die vorbeikommen und Ihnen bei Ihrem Problem helfen kann. Das wäre vielleicht sowieso nicht die effizienteste Situation, daher könnten Chatbots oder maschinelles Lernen in einem CRM-System effizienter sein und mehr gemeinsame Informationen ermöglichen. Diese Art der technischen Automation mit gemeinsamer Problemlösung könnte letztendlich zu mehr Verbindungen, Erkenntnissen und Produktivität führen.\n\n## Ihr Arbeitsplatz entwickelt sich weiter.\n\nDie Geschäftsanforderungen und -strategien ändern sich, insbesondere in Bezug auf die Entwicklung des Arbeitsplatzes. Einige Organisationen werden weiterhin vollständig remote arbeiten, andere möchten alle wieder ins Büro zurückholen, wenn es sicher ist, und es wird alle möglichen Konfigurationen dazwischen geben. Unabhängig von der Situation müssen Organisationen eine Strategie verfolgen, um sicherzustellen, dass sie ihre Mitarbeiter jederzeit und an jedem Ort erreichen können, unabhängig von Notwendigkeit oder Wahl.\n\nProzessverbesserung ist ebenfalls entscheidend. Sie möchten nicht weiterhin Prozesse hinzufügen, die Sie nicht mehr benötigen. Die Bewertung von Verbesserungsbereichen, ob es sich um interne Infrastruktur oder Fähigkeiten von Drittanbietern handelt, wird Effizienz und Wert hinzufügen. Anstatt eine enorme Infrastruktur aufzubauen, die Management erfordert und nur mit ihren eigenen Werkzeugen belastet ist, sollten Organisationen die unglaubliche Menge an Ökosystemmöglichkeiten bewerten, die sich in as-a-service-Tools einbinden lassen. Das Auslagern von Dienstleistungen, die nicht zu den Schlüsselstrategieanlagen oder -stärken Ihres Unternehmens gehören, könnte sinnvoller sein.\n\nEin weiterer Bereich, der bewertet werden muss, ist das Datenmanagement. Mit all den Daten, die in den Kollaborationstools verfügbar sind, wird die Datenverbreitung zu einem Problem. Eine klare und effektive Strategie ist notwendig.\n\nMit dem Datenmanagement einher geht die Sicherheit. Daten sind jetzt überall verstreut, daher müssen Organisationen gute Sicherheitstools und gute Arbeitsabläufe im Umgang mit Daten übernehmen, bewerten und implementieren.\n\n## Intel liefert\n\nEs gibt sechs Hauptfähigkeiten am sich entwickelnden Arbeitsplatz, bei denen Intel helfen kann: Zugriff auf Apps und Daten, Verwaltbarkeit, verbesserte Sicherheit, Konnektivität, Infrastruktur für Zusammenarbeit und Multi-Cloud-Umgebungen. Obwohl Intel nur Silizium produziert, erfüllen seine Fähigkeiten, Skalierbarkeit und Sicherheit die Bedürfnisse von Organisationen, egal ob klein oder groß.\n\nIntel strebt danach, in mehreren Umgebungen präsent zu sein und die Verwaltbarkeit dieser Ressourcen anzubieten. Obwohl Kunden keine Vermögenswerte wie Prozessoren, Beschleuniger, Speicher und ähnliches direkt von Intel kaufen werden, werden sie auf das starke, bewährte Ökosystem von Produkten wie Hardwareherstellern, OEMs, Softwareanbietern, Systemintegratoren und Cloud-Dienstleistern zurückgreifen, das Intel ermöglicht.\n\nEine der Stärken von Intel ist die Unterstützung dieses Ökosystems. Zum Beispiel verfügt Intel über 15.000 Software-Ingenieure, die Code entwickeln, jedoch wird keiner davon verkauft. Stattdessen helfen sie bei der Entwicklung des Ökosystems, indem sie neue Lösungen auf der Basis von Silizium anbieten. Silizium ist der Mechanismus, um Lösungen bereitzustellen, die den Menschen helfen, reale Probleme zu lösen. Ein guter Vergleich ist, dass der Abstand zwischen Intels Verladerampe als Hersteller und der Verladerampe des endgültigen Kunden zu weit ist, als dass Intel selbst einen LKW fahren könnte. Das Ökosystem überbrückt diese Lücke.\n\n## Lösungsbereiche, die Sie möglicherweise bewerten\n\nWie können Kunden die Fähigkeiten des Ökosystems am besten nutzen? Intel kann Empfehlungen in den sechs Schlüsselbereichen geben, ob Sie Schwierigkeiten mit einer VDI-Lösung haben, wie z.B. der Entscheidung, ob sie vor Ort bleiben oder in einem virtuellen Desktop- oder RDS-Dienst sein sollte, oder ob Sie eine VPN-Strategie entwickeln möchten, die Konnektivität gewährleistet.\n\nEin Teil der Lösung besteht darin zu verstehen, dass Intel in einem so umfangreichen Ökosystem arbeitet und dadurch helfen kann, die unterschiedlichsten Herausforderungen zu bewältigen. Intel wird Ihnen beispielsweise nicht unbedingt einen Prozessor verkaufen, sondern Ihnen dabei helfen, Ihre Arbeitslast auf dem bestmöglichen Silizium zu platzieren. Dies könnte bei Cloud-Service-Providern, in mehreren Clouds oder innerhalb und außerhalb Ihres eigenen Rechenzentrums sein. Intels Ziel ist es, Ihnen Effizienz, Portabilität und Agilität in diesen Prozessen zu bieten.\n\n## Umarmen Sie die Entwicklung des Arbeitsplatzes: Wann oder wo möchten Sie anfangen?\n\nIntel hat das beste Interesse Ihrer Organisation im Blick, denn wenn Sie in Ihrem Unternehmen Erfolg haben, werden Sie neue Wege finden, um Daten und Infrastrukturen zu nutzen und dadurch mehr Wert für Ihre Kunden zu schaffen und letztendlich mehr zu verbrauchen. Ihre Strategien, das Wachstum und die Entwicklung liegen in Ihrer eigenen Verantwortung, aber Intel wird Ihnen die Grundlage bieten, um gute Entscheidungen zu treffen.\n\nSuchen Sie nach Möglichkeiten, mit Ihrem Intel Vertreter in Kontakt zu treten. Sie verfügen über eine Vielzahl von Ressourcen, um Ihnen dabei zu helfen, das umfangreiche Netzwerk von Partnern zu nutzen, die Ihre Probleme und Ziele angehen können.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Robert Looney"],"link":"/episode-EDT34-de","image":"./episodes/edt-34/de/thumbnail.png","lang":"de","summary":"In dieser Folge sprechen Darren Pulsipher, Chief Solution Architect, Public Sector, und Robert Looney, Americas Data Center Sales Manager für Intel, über den Einsatz eines strategischen Ansatzes, um die aktuelle Entwicklung am Arbeitsplatz zu umarmen. Die COVID-Pandemie hat große Herausforderungen und Übergänge am Arbeitsplatz geschaffen. Intel hilft Kunden dabei, Technologien optimal einzusetzen, um den fortlaufenden Herausforderungen im neuen Normalzustand zu begegnen."},{"id":105,"type":"Episode","title":"Navigieren durch disruptiven Wandel","tags":["change","culturalchange","organizationalchange","disruption","people","process"],"body":"\r\n\r\nRick feierte kürzlich sein drittes Jahrzehnt bei Intel und in dieser Zeit half er Intel, sich durch enorme Veränderungen und große Ereignisse zu navigieren. Neben schwierigen Wettbewerbssituationen und branchenweiten Veränderungen, die mit dem Aufstieg des Internets und dem Dotcom-Boom und dessen Zusammenbruch einhergingen, gab es auch externe Ereignisse wie den 11. September, die Große Rezession und jetzt die COVID-19-Pandemie.\n\n## Arten von Krisensituationen\n\nDie Natur eines modernen Unternehmens in einer modernen Wirtschaft ist es, sich ständig in einer hohen Unsicherheit, Turbulenz, Veränderung und Störung zu bewegen. Organisationen entweder verkümmern in solchen Momenten oder kommen stärker und besser heraus.\n\nJede Störung ist unterschiedlich in Bezug auf Ausmaß und Geschwindigkeit. Einige Ereignisse entfalten sich über einen längeren Zeitraum, möglicherweise eine technologische oder strukturelle Veränderung in der Branche, und beschleunigen dann plötzlich. Einige, wie COVID, haben eine hohe Auswirkung mit beispiellosem Tempo. Was jedoch interessant an COVID ist, ist, dass wir seit einem Jahrzehnt daran gearbeitet haben, die Technologie zur Bewältigung der Folgen der Pandemie zu entwickeln, aber es hat dieses Ereignis gebraucht, um sie in die praktische Anwendung zu bringen. Ein gutes Beispiel dafür ist die Telemedizin. Ihre plötzliche, weit verbreitete Nutzung hat auch das politische Umfeld verändert, und die Gesundheitslandschaft wird nie mehr dieselbe sein.\n\nDiese Art von grundlegender Änderung der Politik geschieht in Ereignissen mit großer Geschwindigkeit, wie zum Beispiel der Großen Rezession oder 9/11. Ähnliche Dinge werden nach COVID passieren. Diese Ereignisse, so schwierig sie auch sein mögen, bieten Organisationen die Möglichkeit, einen riesigen Sprung nach vorne zu machen in Bezug auf ihre Leistungsfähigkeit und den Einsatz von Technologie.\n\n## Navigieren inmitten von disruptivem Wandel\n\nIntel ist immer vorwärtsgegangen, um sich großen Herausforderungen und Unterbrechungen entgegenzustellen. Fundamentale Bestandteile der Unternehmenskultur sind Bereitschaft, gemeinsames Ziel und Vertrauen. Diese können existieren, wenn die Mitarbeiter ein Gefühl der psychologischen Sicherheit haben. Zum Beispiel fühlte sich Darren befähigt, als der CEO von Intel sagte, dass niemand aufgrund von COVID entlassen wird. Dies ermöglichte es ihm, Risiken einzugehen, um der Krise zu begegnen, ohne Angst um seinen Arbeitsplatz zu haben. Und obwohl der CEO und die Führungskräfte den Ton angeben, wird die meiste Arbeit in Bezug auf psychologische Sicherheit von den Frontline-Managern durchgeführt. Diese Sicherheit und Befähigung führen zu einer eingebauten Bereitschaft. Zusammen mit gemeinsamem Ziel und Vertrauen sind dies nicht nur die fundamentalen Bausteine einer Organisation, die bereit ist, auf eine Krise zu reagieren, sondern auch die Merkmale einer leistungsstarken Organisation.\n\nEine leistungsstarke Organisation wird auch über die Werkzeuge verfügen, um Störungen zu bewältigen, die eher langsam abbrennen als schnell und stark einschlagen. Bei Ereignissen wie COVID oder der großen Rezession besteht kaum eine Debatte darüber, was passiert, und alle sind sich der enormen Probleme bewusst. Vergleicht man dies jedoch mit beispielsweise einer grundlegenden Geschäftsveränderung, einer architektonischen Veränderung oder einer Technologie, die sich im Hype-Kurve manifestiert und deren Bedeutung für das Geschäft noch ungewiss ist, wird es mehr Unsicherheit und Diskussionen über Anpassungen geben.\n\nWie überlebt eine Organisation solche Wendepunkte? Telemetrie oder die von Ihnen bewerteten Eingaben sind wichtig. Eine der Komplexitäten in einer großen Organisation besteht darin, dass diese Eingaben bis zum leitenden Entscheidungsträger bereits durch drei Ebenen der Bearbeitung und Positionierung gegangen sein können, und das kann gefährlich sein. Wahrheit und Transparenz sind Werte bei Intel. In einem Unternehmen mit einem hohen Maß an psychologischer Sicherheit können Mitarbeiter die Wahrheit über Probleme aussprechen.\n\nDie wichtigste Aufgabe besteht darin, den Kunden zuzuhören, da sie dazu neigen, Sie in die richtige Richtung zu führen. Zum Beispiel, wenn jemand etwas über ein Konto wissen möchte, wird Rick oft den Kundenbetreuer hinzuziehen, um Informationen aus erster Hand zu erhalten. Es ist auch kluge Führung, sich direkt an die Experten zu wenden, anstatt Informationen durch drei Ebenen von Filterung und Bearbeitung zu erhalten, insbesondere in Krisensituationen. Die Bewältigung des Moments hängt von einer Kultur der Vorbereitung, der richtigen Telemetrie und der Entscheidungsfindung ab.\n\nDie Entscheidungsfindung kann in einer großen Organisation kompliziert werden. Eine einfache Lösung besteht darin, dass jeder Teilnehmer, der zu einem Meeting kommt, sich fragt: Sind wir hier, um eine Entscheidung zu treffen? Wer ist der Entscheidungsträger? Oder diskutieren wir nur oder bereiten wir die Telemetrie und Daten für einen Entscheidungsträger vor? Dies ist einfach eine gute organisationshygienische Maßnahme.\n\nAndy Grove sagte: \"Lass ein wenig Chaos herrschen und bring dann das Chaos in den Griff.\" Für Entscheidungen an den Wendepunkten, den langsamen Verbrennungen, musst du manchmal Innovation atmen und ein wenig köcheln lassen, und gleichzeitig möchtest du die Dinge so managen, dass sie nicht außer Kontrolle geraten. Gute Prozesse und Sicherheitsvorkehrungen helfen dabei.\n\nIn schwierigen Zeiten müssen Entscheidungsträger ein tiefes Verständnis dafür haben, dass einzelne Personen sich in unterschiedlichen Situationen befinden. Sie müssen die Auswirkungen von Entscheidungen sorgfältig durchdenken. Psychologische Sicherheit ist äußerst wichtig, und die Manager an vorderster Front und der zweiten Riege sind entscheidend für die Fähigkeit einer Organisation, in Zeiten der Störung gut zu handeln. Die Führungsebene spielt eine Schlüsselrolle bei der Festlegung des Tons, aber diese Manager tragen die Arbeit.\n\nVon vorhersehbaren zukünftigen technologischen Veränderungen wie dem Einfluss von KI auf 5G, über Probleme wie den Klimawandel, bis hin zu unvorhergesehenen weltweiten Ereignissen – die einzige Konstante ist, dass wir immer mit Störungen, Krisen und Veränderungen umgehen müssen. Eines der Kennzeichen der Kultur von Intel ist ihre Fähigkeit, auf diese Ereignisse zu reagieren, sich anzupassen und widerstandsfähig zu sein.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rick Herrmann"],"link":"/episode-EDT35-de","image":"./episodes/edt-35/de/thumbnail.png","lang":"de","summary":"In dieser Episode diskutieren Darren Pulsipher, Chief Solution Architect, Public Sector, und Rick Hermann, Director US Public Sector, Intel, wie Intel in den letzten drei Jahrzehnten erfolgreich mit disruptiven Veränderungen umgegangen ist."},{"id":106,"type":"Episode","title":"Sechs Säulen der Cybersicherheit","tags":["cybersecurity","prevention","threatdetection","zerotrustarchitecture","zta","technology"],"body":"\r\n\r\nMit zunehmenden Cyberangriffen in allen Branchen ist Sicherheit wichtiger denn je. In dieser Folge erläutern Darren und Steve Orrin, Federal CTO bei Intel, die Angriffsvektoren, die sechs Säulen der Cybersicherheit sowie die Unterstützungsmöglichkeiten von Intel.\n\nSteve hat in den 25 Jahren, die er in diesem Bereich gearbeitet hat, gesehen, wie sich die Sicherheit sowohl als Wissenschaft als auch als Kunst entwickelt hat. Gleichzeitig war das Maß an Komplexität, mit dem Organisationen konfrontiert sind, um ihre Daten, Systeme und Anwendungen zu sichern, noch nie zuvor so schwer.\n\n## Groß angelegte Sicherheitsverletzungen und Cyberangriffe setzen sich fort.\n\nGroß angelegte Datenschutzverletzungen und tiefe Eindringungen ereignen sich überall in der gesamten Bandbreite, angefangen von sozialen Medienplattformen über Finanzdienstleistungen bis hin zur Gesundheitsversorgung. Keine Art von Daten ist vor gezielten Angriffen mit immer raffinierteren Techniken immun.\n\nWas treibt diese Hacks an? Eine Antwort ist, dass heutzutage ein umfangreicher oder tiefer Angriff weniger Ressourcen und weniger finanzielle Investition erfordert. Der Umfang und das Ausmaß dessen, was ein Hacker mit einer geringen Investition erreichen kann, gibt den Gegnern in einem komplexen System einen Vorteil.\n\n## Drei Kräfte, die die Sicherheit von Unternehmen und Missionen beeinflussen.\n\nEine weitere Antwort auf die Frage, was die Angriffe antreibt, ist, dass Daten ein wertvolles Gut sind: das neue Öl. Daten sind anfällig, da sich die Angriffsfläche weiter ausweitet.\n\nEs gibt mehr Integrationspunkte, Produkte, Betriebssysteme von Anbietern und Geräte, die in das Verwalten, Verbrauchen und Transportieren von Daten involviert sind. Die Daten sind weiter entfernt von der Kontrolle des Unternehmens. Manchmal sind wir uns nicht einmal sicher, wo unsere Daten existieren. Zum Beispiel haben Sie vielleicht Ihre Daten mit einer anderen Organisation geteilt und sie haben die Analysen durchgeführt, die auf den Daten basieren. Diese Metadaten verwandeln sich oft in Datenrückstände. Ihre Daten fließen durch mehrere Systeme und können nachträglich von einem Angreifer genutzt werden.\n\nDie Sicherheitsbranche besteht selbst aus Tausenden von Sicherheitsanbietern und Produkten, die ein bestimmtes Stück des Puzzles lösen. Ein CIO kann nur begrenzt viel mit einem begrenzten Budget erreichen und nur eine gewisse Komplexität bewältigen. Wir müssen ganzheitlich darüber nachdenken, wie wir unsere Daten sichern können, nicht nur wie wir einen einzelnen Transit zwischen Punkt A und Punkt B sichern. Daten werden an der schwächsten Verbindung kompromittiert, daher müssen wir die gesamte Kette betrachten.\n\nDie dritte Kraft, die Unternehmen beeinflusst, sind die Standorte der Angriffe. Diese umfassen Angriffe auf die gesamte Hardware, Firmware, BIOS, Software, Dienste und Anwendungen. Mit der Zunahme der Raffinesse der Angriffe beobachten wir Angriffe auf mehreren Ebenen asynchron und unabhängig voneinander.\n\n## Die Angriffsvektoren\n\nHacker greifen nicht nur die Hardware oder eine Software an, sondern nutzen verschiedenste Angriffsvektoren: Menschen, Prozesse und Technologie.\n\nTraditionell umfasst die Umsetzung von Sicherheitskontrollen die ordnungsgemäße Schulung von Menschen sowie die Verwendung der richtigen Prozesse und Technologien. Doch angesichts der jüngsten Angriffe müssen wir bedenken, dass der Prozess selbst anfällig ist. Beispielsweise sollten wir zu automatisierten Patch-Vorgängen übergehen, um das Zeitfenster der Exposition von dem Moment, in dem eine Schwachstelle entdeckt wird, bis zur Veröffentlichung und Implementierung eines Patches durch eine Organisation zu verringern. Traditionell haben wir das Risiko in Kauf genommen, dass Angreifer möglicherweise Monate Zeit haben, um die Schwachstelle auszunutzen.\n\nEs gibt andere Angriffe auf den Prozess, die wir gerade erst abdichten, sei es im Build-Prozess oder im Lebenszyklus der Softwareentwicklung. Die frühzeitige Integration von Sicherheit im Entwicklungslebenszyklus ist der wichtigste Aspekt, um eine Anwendung abzusichern. Das bedeutet, dass Entwickler, Qualitätssicherung und Designer alle am Sicherheitsprozess beteiligt sein müssen. Eine Herausforderung besteht darin, dass jeder Teil des Prozesses isoliert ist, wo Schwachstellen in den Übergängen auftreten können.\n\n## Sechs Säulen der Cybersicherheit\n\n## Lieferketten-Sicherheit\n\nDie Sicherheit der Lieferkette steht erst seit einigen Jahren im Vordergrund. Eine Organisation muss den Servern, Komponenten und Software vertrauen können. Eine gute Lieferkette mit Transparenz ist wichtig, um zu validieren, dass alles aus legitimen Quellen stammt. Insbesondere in Regierungsbereichen liegt der Fokus auf der Hardware-Lieferkette, aber wir dürfen die Software-Lieferkette nicht vergessen. Die Software-Lieferkette ist etwas schwieriger als die Hardware, da oft eine mangelnde Sichtbarkeit besteht, da Produkte aus Open-Source-Tools, Produkten anderer Personen usw. zusammengestellt werden können. Bei einem kürzlichen Angriff war die Software-Lieferkette das Problem, und das ist nur die Spitze des Eisbergs.\n\n## Host & System Security\n\nHost- und Systemsicherheit\n\nSobald wir eine vertrauenswürdige Lieferkette haben, folgt der nächste Schritt in der Sicherheit des Hosting-Systems. Die Grundlage bilden sichere Boot-Technologien und kryptografische Fähigkeiten, um physische Geräte und Systeme abzusichern, auf denen Anwendungen und Daten ausgeführt werden. Dieses System unterstützt die sicherheitsrelevanten Funktionen der höheren Ebenen des Hardware-Stacks.\n\n## Daten- und Anwendungssicherheit\n\nÜber der Host- und System-Sicherheit liegt die Arbeitsspeichersicherheit Ihrer Anwendungen. Daten müssen während ihres gesamten Lebenszyklus geschützt werden, sowohl im Ruhezustand, bei der Verwendung als auch während der Übertragung. Wir verwenden seit langem Sicherheitsmaßnahmen für Daten im Ruhezustand und während der Übertragung, wie Transportverschlüsselung, TLS, IPsec und andere Verschlüsselungsmöglichkeiten sowie Vollverschlüsselung von Festplatten und Dateien. Die fehlende Komponente war die Verschlüsselung von Daten im \"In-Use\"-Zustand im Arbeitsspeicher mit hardwarebasierter Isolierung. In den letzten Jahren ermöglichen Technologien und Lösungsansätze den Schutz dieser letzten potenziell gefährdeten Stelle rund um den Datenschutz.\n\n## Netzwerksicherheit\n\nParallel zu diesem Stapel von Lieferketten-Hosts und Datensicherheit benötigen wir Netzwerksicherheit. Die Integrität und Verfügbarkeit von Netzwerken ist wichtig, um Angriffe auf die Dienstverfügbarkeit zu bewältigen. Die Daten müssen sicher dorthin gelangen, wo sie hinmüssen. Außerdem müssen wir Netzwerke vor externen Eindringlingen überwachen und schützen, egal ob es sich um ein Unternehmensnetzwerk oder ein verteiltes Netzwerk in der Cloud und am Rand handelt. Sicherheit hier bedeutet nicht einfach nur Firewalling, es geht um aktiven Schutz.\n\n## Identität & Zugriffsverwaltung\n\nIdentitäts- und Zugriffsmanagement ist eine grundlegende Fähigkeit. Wir müssen nicht nur wissen, wer sich einloggt, sondern auch wer sich auf welchem Gerät einloggt. Der Mensch ist tatsächlich nur ein kleines Teilchen des Rätsels. Wir müssen Identitäten für alle Dinge, Prozesse und Dienste haben, die auf die Daten zugreifen und sie verwalten. Eine Person erledigt ihre Arbeit vielleicht mit nur wenigen Tastenanschlägen, aber es können 20 verschiedene Geräte und 100 Dienste und Prozesse sein, die auf die Daten zugreifen. Wir müssen Richtlinien und Autorisierungen für all diese Einheiten haben. Und während wir uns mehr auf autonome Prozesse verlagern, sind weniger Menschen beteiligt, daher wird es noch wichtiger, starke Identitäten für diese Prozesse ohne menschliches Eingreifen zu haben.\n\n## Bedrohungserkennung, Intelligenz und Analyse\n\nDieser letzte Pfeiler ist eine Kombination aus vielen Dingen, einschließlich Bedrohungsinformationen, Analytik, Überwachung und Auditierung. Es ist die übergeordnete Sichtbarkeit, um sicherzustellen, dass alles so läuft, wie es laufen soll, und wenn etwas nicht stimmt, die Fähigkeit zu verstehen, woher es kommt und warum. Dies ist der Dachbegriff, der die Datensicherheit antreibt, und alles muss nach oben und nach unten fließen. Es gibt einen Wechsel vom Arbeiten in einer isolierten Umgebung, zum Beispiel einem Anbieter, der sich nur um Netzwerksicherheit kümmert, zum Arbeiten über das gesamte System hinweg. Erfolgreiche Unternehmen haben vielfältige Teams mit Mitarbeitern aus verschiedenen Bereichen, um komplexe Sicherheitsbedürfnisse zu erfüllen.\n\n## Cybersicherheitsbereiche: Erreichen von Zero Trust mit Intel-Technologien\n\nIntel stellt grundlegende Fähigkeiten in jedem der sechs Säulen zur Verfügung, sei es durch unsere Initiative zur Gewährleistung des Rechenlebenszyklus, um eine Zusammenarbeit zwischen den OEMs und Komponentenanbietern einer vertrauenswürdigen Lieferkette zu unterstützen, bis hin zur Bereitstellung fundamentaler Bausteine für die Systemsicherheit, bei der der sichere Startvorgang mit der Hardware beginnt. Wir haben Ausführungstechnologie und Boot Guard-Technologie mit eingebauter kryptografischer Beschleunigung, damit Benutzer sie für den Schutz von Daten im Ruhezustand, Daten bei der Verwendung und Daten beim Transit aktivieren können, ohne dass dies die Leistung beeinträchtigt.\n\nIm Falle der Bedrohungserkennungs-Intelligenz bietet Intel Primitive wie TPD an, wobei eine Lösung auf höherer Ebene Sichtbarkeit und Bedrohungserkennung bieten kann, wo wir sie zuvor nicht hatten.\n\nIntel ist ein Technologieanbieter, aber wir arbeiten auch im Bereich der Menschen und Prozesse. Ein gutes Beispiel ist die Lieferkette. Wir haben einen Prozess mit dem Ökosystem aufgebaut, um einem Unternehmen zu ermöglichen, die Komponenten und Berechtigungen für eine bestimmte Plattform und ihre Komponenten zu validieren. Gleichzeitig gibt es Prozesse in Bezug auf Daten- und Nutzungsschutz durch Fähigkeiten in der Hardware wie SGX, die den Speicher verschlüsseln und die Daten und den Code für eine bestimmte Anwendung isolieren können.\n\nIntel ermöglicht sichere Prozesse, um Technologien im großen Maßstab zu nutzen. Ein weiterer wichtiger Aspekt in Bezug auf Prozesse besteht darin, sich in den allgemeinen Risikorahmen einer Organisation einzuordnen. Intel liefert Ihnen die Beweise und Zuordnungen, die Sie innerhalb unserer Technologien benötigen, um das in Ihren bestehenden Risikorahmen einzubinden.\n\nDas letzte Element sind Menschen. Der Umgang mit zufälligem menschlichem Verhalten ist manchmal der schwierigste Teil der Sicherheit, sei es bei Phishing-Betrügereien oder gezielten Angriffen auf schwache Passwörter. Training ist entscheidend, reicht aber oft nicht aus. Prozesse und Technologien können das Training unterstützen, indem sie beispielsweise Passwörter stärken oder Phishing eliminieren, wenn die Anmeldeinformationen eines Benutzers nicht kompromittiert werden können. Am Ende des Tages sind jedoch kontinuierliches Training und Bildung immer entscheidend und zusammen mit abmildernden Technologien erforderlich.\n\nSicherheit ist schwierig, aber es gibt Licht am Ende des Tunnels mit all den Innovationen im Ökosystem und mit Organisationen, die bereit sind, Dinge anders zu machen. Wir müssen unser Augenmerk auf zwei Dinge richten: die Anwendung von Risikorahmen und Zero Trust. Durch die Verbindung dieser beiden Welten, der Cybersicherheitsdomäne mit Policy-Engines und Durchsetzung, kann ein umfassender Ansatz für Sicherheit gewährleistet werden. Es gibt hier viel Aktivität und noch viel Arbeit zu erledigen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT36-de","image":"./episodes/edt-36/de/thumbnail.png","lang":"de","summary":"Mit dem Anstieg von Cyberangriffen in allen Branchen ist Sicherheit wichtiger denn je. In dieser Episode skizzieren Darren Pulsipher, Chief Solution Architect, und Steve Orrin, Bundes-CTO bei Intel Federal, die Angriffsvektoren, die sechs Pfeiler der Cybersicherheit und wie Intel helfen kann. Die steigende Komplexität, mit der Organisationen kämpfen müssen, um ihre Daten, Systeme und Anwendungen zu sichern, war noch nie schwieriger."},{"id":107,"type":"Episode","title":"2020 im Rückblick","tags":["covid","cybersecurity","remoteworker","people","process","compute","data"],"body":"\r\n\r\n## 2020 - Erwartungen\n\nVor COVID-19 sah das Jahr 2020 vielversprechend für neue Geschäftsmodelle aus, die sich in Richtung großer digitaler Transformationen bewegten. Künstliche Intelligenz (KI)/Maschinelles Lernen (ML) und Analytik sollten eine wichtige Rolle in der Zukunft spielen. Mit Hilfe dieser Werkzeuge sollten wir Daten effektiver in unseren Organisationen nutzen. Es sollte das Jahr von 5G werden, wobei 5G bei den Olympischen Spielen für großes Aufsehen sorgen sollte.\n\nWir sollten auch einen großen Fortschritt in der Industrie für 4.0-Transformationen sehen, mit dem Internet der Dinge und in der Fertigung, die alle zusammen auf diese digitale Transformation hinarbeiten.\n\n## 13. März\n\nDann, zumindest in Kalifornien, kam alles am Freitag, den 13. März, abrupt zum Stillstand. Unternehmen, Schulen, Konferenzen, Meetings, Reisen und soziale Veranstaltungen wurden plötzlich geschlossen. Wochen wurden zu Monaten, während der Optimismus über eine schnelle Eindämmung des Virus schwand und Städte wie New York City schwer getroffen wurden und das medizinische System überlastet war. Alles schien chaotisch und unsicher.\n\nEinige interessante Dinge sind daraus hervorgegangen: Einige Unternehmen haben sich prächtig entwickelt, während andere Schwierigkeiten hatten, und die Menschen begannen, aus der Ferne zu arbeiten.\n\n## Meistbesuchte Website in meinem Haus\n\nIn meinem Haus war die Website mit den meisten Aufrufen nicht mehr Facebook, sondern das Center for Systems Science and Engineering an der Johns Hopkins Universität. Dort wurden Statistiken über das Virus veröffentlicht. Wir konnten die Auswirkungen in unserer eigenen Nachbarschaft, unserer Stadt und auf der ganzen Welt sehen. Unsere gesamte Wahrnehmung dessen, was wichtig war und was wir für dieses Jahr geplant hatten, hat sich verändert.\n\n## Mangel          \n\nNicht nur kam es sofort zu Engpässen bei verschiedenen Gegenständen wie Toilettenpapier, Handdesinfektionsmittel und Hefe, sondern nach einigen Wochen wurden auch Büroausstattung und Technologie zu begehrten Artikeln, da sich die Menschen schnell auf Fernunterricht und Heimarbeit umstellten. Dies führte zu Engpässen bei wichtigen Werkzeugen wie Webcams und Laptops.\n\nWi-Fi-6-Router wurden plötzlich wichtig, als die Internetnutzung zu Hause zunahm. In unserer Familie wechselte ich vom gelegentlichen Arbeiten zu Hause zu Vollzeitarbeit und es kamen drei Teenager hinzu, die per Fernunterricht lernten, sowie drei Erwachsene College-Studenten, die nach Hause kamen, um zu studieren und zu arbeiten. Unsere Internetverbindung war schnell überlastet. Mir fiel es schwer, einen Wi-Fi-6-Router zu finden, um unser Problem zu lösen, da viele Menschen sich in ähnlichen Situationen befanden.\n\n## Reisen verändert\n\nMit dem praktisch zum Stillstand gekommenen Reisen wandten sich viele Menschen stattdessen Heimwerkerprojekten zu. Früher war ich oft am Flughafen und verbrachte die Hälfte jeder Woche mit Reisen. Jetzt fand ich mich wie alle anderen bei Lowe's oder Home Depot wieder. Die Geschäfte waren überfüllt. Die Zeit, die ich normalerweise mit Reisen verbracht hätte, habe ich jetzt damit verbracht, mit meinen Kindern einen Schuppen zu bauen. Ich hatte mehr Gelegenheit, qualitativ hochwertige Zeit mit meinen Kindern zu verbringen und mein Zuhause zu verbessern.\n\n## Konferenzen\n\nEine bedeutende Veränderung war natürlich die Absage von persönlichen Konferenzen. Viele wurden stattdessen online abgehalten und waren äußerst erfolgreich. Zum Beispiel hat die IBM Think-Konferenz ihr persönliches Event im Mai stattdessen online durchgeführt. Über 100.000 Menschen haben daran teilgenommen, was die größte Teilnehmerzahl war, die sie jemals hatten.\n\nNeben einer gesteigerten Teilnahme gibt es einen weiteren Vorteil: Teilnehmer müssen sich nicht für eine ganze Woche auf eine Konferenz festlegen, sondern können einzelne Sitzungen nach Wahl besuchen und dennoch zuhause bei der Familie sein. Der Nachteil besteht darin, dass es weniger Möglichkeiten zum Netzwerken und zum persönlichen Treffen von Personen gibt.\n\nBranchenkonferenzen werden wahrscheinlich für immer verändert sein. Sie werden nicht immer komplett online sein, aber vielleicht macht eine hybride Version in Zukunft Sinn.\n\n## Zwanghaftes Arbeiten von Zuhause aus\n\nEin weiterer bedeutender Wandel besteht darin, dass wir praktisch in den Häusern des anderen Geschäfte führen. Es kommt manchmal vor, dass Kinder, Hunde oder andere unvermeidliche Ablenkungen im Hintergrund zu hören sind. Einmal musste mein Chef während einer Besprechung seine Tochter neben sich sitzen haben, weil sie Hilfe bei etwas brauchte. In dieser Zeit ist es wichtig, dass wir alle flexibel sind.\n\n## Nomadische Arbeiter\n\nEinige Mitarbeiter arbeiten nicht mehr an einem festen Ort, sondern sind zu Nomaden geworden, da sie nicht mehr in der Nähe ihres Arbeitsplatzes sein müssen.\n\nZum Beispiel zahlte mein Neffe, der nur wenige häusliche Verpflichtungen hatte, exorbitant hohe Miete in der Bay Area. Als die Arbeit remote wurde, gaben er und seine Mitbewohner ihre teure Wohnung auf und reisten stattdessen um die Welt, wobei sie mehrere Monate lang Häuser an interessanten Orten mieteten.\n\nDies ist eine bedeutende Veränderung in der Art und Weise, wie wir über das Management unserer Mitarbeiter, Vermögenswerte und Daten denken.\n\n## Remote Arbeit\n\nDie Fernarbeit wurde schnell hochgefahren. Viele Unternehmen hatten bereits einige Richtlinien für Fernarbeit oder Pläne für mehr Fernarbeit. Aber was einmal vielleicht ein 18-monatiger Einführungsplan war, wurde zu einem 18-tägigen Einführungsplan.\n\nDie wichtigste Form der Fernarbeit war VDI (Virtual Desktop Infrastructure). Die Menschen machen bereits seit Jahrzehnten von VDI Gebrauch, aber plötzlich wurde es aufgrund seiner Vertrautheit zu einer beliebten Wahl. Es ist schnell, einfach und kostengünstig, die Mitarbeiterinnen und Mitarbeiter wieder zur Arbeit zu bringen und ihnen über Hardware-Lösungen den Zugriff auf die benötigten Daten zu ermöglichen.\n\nVPNs (virtuelle private Netzwerke) wurden schnell überlastet, da jeder von zu Hause aus arbeitete. Wir beobachteten, dass Unternehmen in die Erweiterung ihres VPN investierten, entweder durch den Kauf weiterer Lizenzen oder den Ausbau der Infrastruktur und die Aufrüstung nach Bedarf.\n\nUnternehmen haben auch ihre Nutzung von SaaS (Software-as-a-Service)-Tools beschleunigt. Zum Beispiel wurden Office 365-Einführungen innerhalb weniger Wochen umgesetzt, anstatt wie geplant sechs bis neun Monate. Unternehmen mit SaaS-Angeboten wie Microsoft und Google krempelten die Ärmel hoch und halfen Organisationen, die Migration schnell zu bewältigen. Für den Fernarbeiter wurde die unproduktive Zeit durch die Initiative der Branche insgesamt minimiert.\n\nEs war bemerkenswert, wie schnell die Fernarbeiter arbeitsfähig waren. Die wahren Helden waren hier die IT-Fachkräfte an vorderster Front wie der Helpdesk, Systemingenieure, Systemadministratoren usw. Es war wirklich eine Herkulesaufgabe.\n\n## Cybersicherheitsänderungen\n\nEine der Dinge, die jedoch ein wenig auf der Strecke geblieben ist bei der Geschwindigkeit dieses Wandels, war die Cybersicherheit. In den letzten sechs bis acht Wochen haben wir die Auswirkungen davon gespürt, mit mehreren großen Angriffen im Bereich der Cybersicherheit.\n\nEiner der Gründe ist eine erhöhte Angriffsfläche, da Daten auf Laptops in der gesamten Organisation über ungesicherte Netzwerke in Privathaushalten verteilt sind.\n\nEin Teil der Daten befindet sich in der Cloud. Nun, mit SaaS-Angeboten, befindet sich ein Teil davon im Rechenzentrum und ein Teil auf den Computern, die die Leute mit nach Hause gebracht haben.\n\n2021 wird ein Jahr sein, in dem wir uns auf Cybersicherheit konzentrieren und einen genauen Blick auf die Art und Weise werfen, wie wir Daten verwalten und das gesamte System absichern.\n\n## Neue Geschäftskleidung\n\nEine der größten Neuerungen dieses Jahres ist die neue Business-Kleidung. Die beste Möglichkeit, sie zu beschreiben, ist der Business-Mullet: Business-Kleidung oben, Pyjamas oder Shorts unten. Ich trage hin und wieder Hosen, aber meine Kinder haben wahrscheinlich genug davon, mich in einem Hemd mit Knöpfen oder sogar einem Anzug oben in Kombination mit Shorts zu sehen.\n\n2020 war ein Jahr des Wandels, sowohl persönlich als auch beruflich, aber es hat uns an einen Punkt in 2021 geführt, an dem wir mit den gelernten Lektionen und Verbesserungen vorangehen können, um eine bessere Zukunft zu gestalten.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT37-de","image":"./episodes/edt-37/de/thumbnail.jpg","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect, Public Sector bei Intel, reflektiert über die Störungen, Veränderungen und Anpassungen, die die COVID-19-Pandemie 2020 mit sich brachte."},{"id":108,"type":"Episode","title":"Die wilden Zwanziger – Ein Blick nach vorne auf das Jahr 2021.","tags":["contactless","covid","remotelearning","remoteworker","people","process"],"body":"\r\n\r\n## COVID-19 - Eine beispiellose Zeit der Volatilität, Unsicherheit, Komplexität und Mehrdeutigkeit\n\nIn dieser Episode fragt Darren Rachel Mushawar, die Vertriebsleiterin bei Intel, nach ihrem Einblick in das, was nach der Unterbrechung durch COVID-19 in den Goldenen Zwanzigern bevorsteht.\n\n## 2020\n\nDie letzten zwölf Monate waren geprägt von unverkennbarem Wandel und Unsicherheit, beruflich wie persönlich. Nicht jedes Jahr wird wie 2020 sein, aber die daraus gewonnenen Lehren werden uns in jedem kommenden Jahr besser machen.\n\nTrotz individueller Umstände musste jeder einzelne Mensch innehalten und herausfinden, was wirklich wichtig ist. Wir halten unsere Teams, Freunde, Familien und Beziehungen enger als jemals zuvor.\n\nUnd obwohl wir physisch isoliert waren, hat die Vernetzung durch Technologie in einem beispiellosen Tempo zugenommen. Das Gefühl der Möglichkeit in diesem Bereich wurde bei jedem neu belebt und auf gewisse Weise für die Zukunft entsperrt und befreit.\n\n## Die Pandemie hat mehr Verantwortlichkeiten geschaffen.\n\nAuf der anderen Seite hat die Pandemie mehr Verantwortung mit sich gebracht. Fast die Hälfte der Erwachsenen in ihren Vierzigern und Fünfzigern hat einen Elternteil im Alter von 65 Jahren oder älter und hat gleichzeitig junge Kinder oder unterstützt finanziell ältere Kinder. Dies bedeutet, dass sie wahrscheinlich das Distance Learning ihrer Kinder jonglieren, von zu Hause aus arbeiten und sich um ältere Eltern kümmern.\n\nDie netto Arbeitsplatzverluste in den USA im Dezember betrafen ausschließlich Frauen. Ein Teil davon hängt mit den erhöhten Verantwortungen zu Hause während der Pandemie zusammen. Wenn Sie kleine Kinder zu Hause haben, die jetzt im Fernunterricht sind und Sie eine Stundenarbeitskraft sind, wie machen Sie das? Sie schaffen es nicht. Und obwohl während des Lockdowns Väter ihre Kinderbetreuung nahezu verdoppelt haben, haben wir noch einen langen Weg vor uns, um in den 20er Jahren echte Geschlechtergleichberechtigung zu erreichen. Es kann nicht nur um Technologie gehen.\n\n## Fernunterricht lenkt den Fokus auf die digitale Kluft.\n\nDa wir in die 20er Jahre gehen, haben wir eine ganze Generation von Kindern, die die letzten 12 Monate im Homeschooling verbracht haben.\n\nTatsächlich haben ein Viertel der amerikanischen Haushalte mindestens ein Kind im Alter von 14 Jahren oder jünger. Für Schülerinnen und Schüler, die aus benachteiligten Verhältnissen stammen, ergab eine Studie des Weltwirtschaftsforums, dass 25 % keinen Computer haben. Darüber hinaus haben 33 % der Schülerinnen und Schüler in ländlichen Gemeinden kaum oder gar keinen Zugang zum Internet. Der Mangel an diesen beiden Grundlagen des Fernlernens hat die digitale Kluft vergrößert.\n\n## Wir leben \"Die Zukunft der Arbeit\"\n\nViele alte Ideen wurden im Jahr 2020 zerstört, da Unternehmen sich sofort auf die Arbeit von zu Hause aus umstellen mussten. Fünfundachtzig Prozent der Organisationen haben verschiedene Homeoffice-Richtlinien erweitert oder eingeführt, die über COVID-19 hinaus Bestand haben werden. Viele Unternehmen haben erkannt, dass sie durch die Verringerung von physischem Immobilienbesitz Kosten sparen können, während die Produktivität mit einer zu Hause oder von unterwegs arbeitenden Belegschaft gleich geblieben oder gestiegen ist. Viele Mitarbeiter bemerken auch, dass sie Zeit sparen, indem sie Pendelzeiten und persönliche Vorbereitungen wie Haare, Make-up und Garderobe eliminieren. Ein Trend im Jahr 2021 wird ein legereres, natürlicheres Aussehen sein.\n\nAls Pendler verschwanden, nahm die Konnektivität zu und Schlafzimmer wurden zu Büros. Allerdings trat ein Nachteil auf, als der durchschnittliche Arbeitstag länger wurde und es schwieriger wurde, abzuschalten.\n\n## \"Kontaktlos\" treibt alles als Dienstleistung voran.\n\nEines der wichtigsten Dinge im Jahr 2021 wird darin bestehen, wie wir weiterhin Technologie nutzen, um uns miteinander verbunden zu halten. Zum Beispiel gibt es Zoom-ähnliche Technologien, die das Ganze auf eine neue Stufe heben und soziale Möglichkeiten bieten, die ähnlich sind wie Smalltalk am Wasserkühler, jedoch im virtuellen Raum. Zweiunddreißig Prozent der Erwachsenen hatten im Jahr 2020 virtuelle soziale Zusammenkünfte. Intel hat virtuelle Besuche auf einer Ziegenfarm durchgeführt und Yogalehrer hinzugezogen, um der Isolation entgegenzuwirken.\n\n## Mehr Verbindungen, weniger Reibung, mehr Virtuelles, weniger physischer Kontakt\n\nWenn wir in den Rest der 20er Jahre gehen, wird Technologie unabhängig davon, ob es im privaten oder im öffentlichen Sektor ist, zu einem Eckpfeiler für jede große Transformation werden.\n\nWie lässt sich diese neue Arbeitsweise und die zunehmenden Verantwortlichkeiten zu Hause aus der Perspektive eines CIOs zusammenfassen?\n\nEs gibt eine Handvoll strategischer Imperative für die IT. Wir können sie in die traditionellen Kategorien Anwendungen, Netzwerk und Rechenzentrum aufteilen.\n\nErstens müssen CIOs für Anwendungen herausfinden, wie sie kontaktlose Funktionen ermöglichen können, das heißt, wie man alles als Service anbietet. Dies betrifft nicht nur Einzelhändler, sondern auch das Gesundheitswesen, die Regierung und die Herstellung. Der zweite Teil bei Anwendungen besteht darin zu wissen, wer Ihr Verbraucher ist und wie er Inhalte aufnimmt.\n\nDie Erkenntnis über die Bedeutung Ihres Netzwerks kommt als nächstes. Es ist der Turbo-Boost für alle digitalen Bereiche. Organisationen müssen über ein Netzwerk verfügen, das beispielsweise die Automatisierung Ihrer Fabriken oder die Bereitstellung von Telemedizin ermöglicht. Sie müssen vorausschauend denken, um für den Zeitpunkt gerüstet zu sein, an dem 5G mehr zur Realität wird, anstatt in Technologien von gestern zu investieren. Netzwerke erstrecken sich nun über traditionelle Rechenzentren hinaus, daher ist das etwas, dem wir Aufmerksamkeit schenken müssen. Natürlich muss Sicherheit hier oberste Priorität haben.\n\nGenau wie Netzwerke ist Sicherheit kein aufregendes, sexy Thema, aber es ist ein Schlüsselfaktor, wenn wir an all die Endpunkte denken, die nun in unserem Alltag weit verbreitet sind. Die Bedrohungsfläche wächst exponentiell, da Mitarbeiter von zu Hause aus auf verschiedenen Geräten arbeiten und die Implementierung von Alles-als-Service für Kunden voranschreitet. Sicherheit geht nicht nur noch darum, Daten im Ruhezustand und in Bewegung zu schützen. Es geht auch um alles dazwischen.\n\nAnstatt von traditionellen Rechenzentren sollten wir sie als Datenzentren betrachten, die bestimmte Arbeitslasten bedienen. Die Cloud wächst jährlich um 30 bis 40 Prozent, zum Beispiel, um Datenzentren näher an Mitarbeiter oder Kunden zu bringen. Es ergibt möglicherweise jedoch keinen Sinn, dass einige streng geheime kritische Daten in die Cloud verschoben werden, sondern dass sie vor Ort bleiben. CIOs müssen verstehen, was ihre Datenzentren sind und welche welche Segmente der Organisation am besten in Bezug auf Wiederherstellung, Speicherung, Kosteneffizienz und Leistung bedienen würden.\n\nMit dem Eintritt in die 20er Jahre müssen Organisationen strategische Veränderungen vornehmen, sowohl bei den Einstellungspraktiken als auch in der Art und Weise, wie sie ihre Kunden betreuen. Diese Konzepte sollten im Hinterkopf behalten werden.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rachel Mushawar"],"link":"/episode-EDT38-de","image":"./episodes/edt-38/de/thumbnail.png","lang":"de","summary":"In dieser Episode fragt Darren Pulsipher, Chief Solution Architect bei Intel, Rachel Mushawar, VP of Sales bei Intel, nach ihrem Einblick, was die Zukunft der Goldenen Zwanziger nach der Unterbrechung durch COVID-19 bereithält."},{"id":109,"type":"Episode","title":"Wasserkühler-Gespräche in einem Remote-Arbeitsumfeld","tags":["collaboration","covid","organizationalchange","people","compute","remoteworker","videoconferencing","signalwire"],"body":"\r\n\r\n## Neue gemeinschaftliche Arbeitsräume\n\nIn dieser Folge diskutieren Darren und Sean Heiney, Mitbegründer von SignalWire, Inc., neue entfernte kollaborative Arbeitsbereiche, die die mühsamen ganztägigen Videokonferenzen überwinden. Ihre Richtlinie \"Kameras an\" und die SignalWire Work-Technologie ermöglichen eine noch nie dagewesene spontane Kommunikation.\n\n## Die Schaffung von SignalWire Work\n\nSignalWire ist der Entwickler der größten Open-Source-Kommunikationsplattform der Welt. In den letzten drei oder vier Jahren hat SignalWire sich darauf konzentriert, die nächste Generation von Echtzeitkommunikationsanwendungen für Video und Sprache zu entwickeln, was zur Entstehung von SignalWire Work für Remote-Büros geführt hat.\n\nEin Vorgänger von SignalWire Work war das eigene Werkzeug von SignalWire, das sie für sich selbst entwickelt haben, da das Unternehmen seit der Gründung verteilte Arbeit ausführt und seit drei Jahren remote arbeitet. Die bestehenden remote Zusammenarbeitstools erfüllten nicht ihre Bedürfnisse, da mehr als 60 Mitarbeiter in verschiedenen Ländern und Kontinenten arbeiten. Einige benutzten bereits eine ständig aktive Tonverbindung, aber da viele ihrer Ingenieure auch Unix-Bildschirme teilten, entwickelte es sich zu einer ständig aktiven Videoverbindung. Als COVID ausbrach, wollten Kunden Zugriff auf dieses Tool haben, so wurde SignalWire Work geboren, zusammen mit einer Version für Live-Veranstaltungen.\n\n## Die SignalWire Arbeitsumgebung.\n\nDas Arbeiten in gemeinsamen Remote Spaces ist nicht nur eine technologische Innovation, sondern auch ein soziales Experiment. Für manche Menschen mag die ständige Videopräsenz seltsam oder beängstigend wirken. Sobald jedoch jeder die Philosophie akzeptiert hat, kann es effizienter sein als in einem physischen Büro zu sitzen. Es gibt Unzulänglichkeiten in einer Welt wie Zoom, in der die Menschen nur in geplanten Meetings mit spezifischen Tagesordnungen sprechen. Eine Menge wichtiger, informeller Kommunikation geht verloren.\n\nMit SignalWire-Tools können Kollegen in die Räume anderer Menschen schauen, als ob sie eine Glaswand in der physischen Welt hätten. Du kannst sehen, wie beschäftigt sie sind, in welcher Stimmung sie sind oder ob sie mit jemand anderem sprechen. Du kannst vorbeischauen und eine Frage stellen. Es gibt soziale Interaktionen, Smalltalk am Wasserkühler, die mit geplanten Meetings nicht möglich sind. Wenn du dich bei SignalWire Work einloggst, ist es, als würdest du ein physisches Büro betreten.\n\nEs gibt Funktionen, die Privatsphäre gewährleisten. Sie können Ihr Video verwischen, damit niemand Ihr Gesicht sehen kann, oder in den Kopf-nach-unten-Modus gehen, in dem Sie nicht unterbrochen werden können, aber trotzdem wissen die anderen, dass Sie anwesend sind und können an Ihre virtuelle Tür klopfen.\n\nEin weiterer Vorteil ist, dass es klarere Grenzen zwischen Arbeits- und Privatleben geben kann. Wenn Sie sich von Ihrem Büro abmelden, ist es eine klare Trennung.\n\n## Die Technologie\n\nSignalWire hat die Technologie entwickelt, die von der Ring-Türklingel bis hin zu Teilen von Amazon Connect und dem Kundenservice-System von Netflix verwendet wird. Sie sind im Prinzip bei allen großen Telekommunikationsunternehmen vertreten. Nun konzentrieren sie sich darauf, diese Technologie für normale Menschen zugänglich zu machen und sie weiterzuentwickeln. Ein Beispiel dafür ist die weltweit beliebteste virtuelle Kirchenplattform, die vor etwa einem Jahr auf SignalWire aufgebaut wurde.\n\nEin einzigartiger Aspekt, der SignalWire einen strategischen Vorteil bietet, ist, dass die Videomuxing in der Cloud durchgeführt wird. Eine herkömmliche Video-Konferenzanwendung enkodiert und überträgt das Video an alle Teilnehmer in einer Konferenz. Wenn es sieben Personen gibt, gibt es sieben Streams. Das bedeutet viel Arbeit für den Prozessor. Es ist Arbeit auf Ihrem Gerät, die Daten zu übertragen, und Sie sind an Jitter und Paketverlusten in all diesen individuellen Streams ausgesetzt. Sie könnten eine großartige Verbindung zu jemandem haben, aber die andere Person sieht schrecklich aus.\n\nMit SignalWire senden die Clients einen Datenstrom in die Cloud. Die Cloud nimmt alle Datenströme, multiplext sie zusammen und sendet dann diesen einen Datenstrom an alle zurück, sodass es nur einen Übertragungs- und Empfangsvorgang gibt. Dies hat viele Vorteile wie eine längere Batterielebensdauer, geringeren Datenverbrauch und eine geringere Belastung der lokalen Geräteprozessoren.\n\nFür die Benutzererfahrung kann SignalWire den Ton großartig machen, wenn er zusammengeführt wird, oder das Layout kontrollieren, damit jeder dasselbe in derselben Ausrichtung sieht. Wenn Sie zum Beispiel auf jemanden auf dem Bildschirm zeigen, kann das jeder sehen. Für Veranstaltungen wie Live-Übungseinheiten ermöglicht die Plattform den Benutzern, gleichzeitig die Hintergrundmusik und den Moderator zu hören und beispielsweise dreißig andere Teilnehmer zu sehen. All dies führt zu einem stärkeren Gefühl der Verbundenheit und Erfahrung.\n\nSignalWire kann auf jeder Cloud oder Plattform laufen, bis hin zu einem individuellen Mobiltelefon oder einem gerätebasierten Atom. Unternehmen können es auf ihrer eigenen Infrastruktur nutzen, was für Sicherheit und Kontrolle von Daten auf höchster Ebene wichtig ist. SignalWire kann seine Knotenpunkte innerhalb eines abgesicherten Netzwerks bereitstellen, um zu verhindern, dass sensible Daten über das öffentliche Internet übertragen werden.\n\nDie Plattform ist vollständig flexibel mit der Technologie und den Anwendungen der Benutzer, sodass sogar ein großes Rundfunkstudio sie nutzen kann, um eine ihrer Shows zu produzieren und zu bearbeiten. Dabei können sie zusätzliche Sprecher für Voice-Overs in bereits bestehenden Aufnahmen einbinden, da die Echtzeitqualität so hoch ist.\n\n## Eine professionellere Umgebung in Remote-Arbeitsbereichen\n\nDer Schlüssel zum Gelingen dieses neuen Arbeitsplatzkonzepts besteht darin, einzusteigen und es zur Firmenpolitik zu machen. Die Vorteile werden schnell offensichtlich. Wenn du zur Arbeit kommst, erscheinst du per Video und bist präsent und interagierst mit deinen Kollegen, genau wie vor der COVID-Zeit.\n\nAls die Menschen vor COVID ins Büro gingen, kleideten sie sich wahrscheinlich schön, um einen professionellen Eindruck zu machen. In dieser Welt wird dieser Eindruck jetzt mehr durch eine gute Audio- und Videoausstattung erzeugt: gute Mikrofone, Beleuchtung, eine hochwertige Umgebung. Mitarbeiter geben ihr Bestes, indem sie Technologie verwenden, denn dies ist nun eine technologische Welt. Es ist eine Evolution von der informellen Arbeitsumgebung von zu Hause aus am Strand oder am Küchentisch, während das Abendessen zubereitet wird. In einer professionelleren Remote-Arbeitsumgebung können professionellere Gespräche stattfinden.\n\nDie beste Möglichkeit, diese Technologie zu erleben, besteht darin, sie auszuprobieren. Es gibt eine 30-tägige kostenlose Testphase unter https://signalwire.com/products/work.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sean Heiney"],"link":"/episode-EDT39-de","image":"./episodes/edt-39/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect, Public Sector, Intel, und Sean Heiney, Mitbegründer von SignalWire, Inc., diskutieren die Fernarbeitsrichtlinien der Unternehmen mit aktivierten Kameras und ihrer neuen Technologie für remote kollaborative Arbeitsumgebungen, die ad hoc Kommunikation für ihre vollständig remote arbeitende Belegschaft ermöglichen."},{"id":110,"type":"Episode","title":"Informationsmanagement-Maturitätsmodell","tags":["data","informationmanagement","informationmaturity","datagovernance","datawarehouse","datalake","datamesh","datalineage","technology","process","people"],"body":"\r\n\r\nDie Entwicklung einer Datenstrategie kann schwierig sein, insbesondere wenn der Zustand des Informationsmanagements oder die Richtung Ihres Unternehmens unklar ist. Das Modell für die Reife des Informationsmanagements hilft CDOs und CIOs dabei, festzustellen, wo sie sich auf ihrer Reise des Informationsmanagements befinden und in welche Richtung sie gehen. Diese Karte kann Organisationen dabei unterstützen, sich kontinuierlich zu verbessern und sich zu einer ultimativen Datenorganisation zu entwickeln, die ihnen ermöglicht, den maximalen Geschäftswert aus ihren Daten zu erzielen.\n\nDas Modell stellt eine Reihe von Phasen dar, beginnend von der am wenigsten ausgereiften bis zur am meisten ausgereiften: Standardisiert, Gemanagt, Gesteuert, Optimiert und Innovation. Oftmals kann eine Organisation gleichzeitig in mehreren Phasen existieren. Sie können bestimmen, in welcher Phase die Mehrheit Ihrer Organisation tätig ist und dann Ihre Vorreiter identifizieren, die in der Reife weiter fortgeschritten sein sollten. Verwenden Sie diese Vorreiter, um neue Prozesse, Technologien oder Organisationsstrukturen zu pilotieren oder zu prototypen.\n\n## Standardisierte Phase\n\nDie standardisierte Phase hat drei Unterphasen: Grundlegende, zentralisierte und vereinfachte Phase. Die meisten Organisationen befinden sich irgendwo in dieser Reifephase, daher sollten Sie das Verhalten, die Technologie und die Prozesse in Ihrer Organisation betrachten, um herauszufinden, wo Ihre Organisation möglicherweise passt.\n\n## Grundlegend\n\nNahezu jede Organisation passt zumindest teilweise in diese Phase. Hier wird Daten nur reaktiv und ad hoc verwendet. Zudem werden fast alle gesammelten Daten basierend auf vordefinierten Zeiträumen gespeichert, oft unbegrenzt. Unternehmen in dieser grundlegenden Phase löschen keine Daten aus Angst, wichtige Informationen für die Zukunft zu verpassen. Zum Beispiel haben wir kürzlich mit einem Unternehmen zusammengearbeitet, das 30 Jahre E-Mail-Backups mit etwa 12-10 Terabyte hat, von denen sie sich nicht trennen wollen. Dies ist eine nicht ungewöhnliche Praxis.\n\nDiese Attribute beschreiben diese Phase am besten:\n\nManagement durch Reaktion\n\nUnkatalogisierte Daten\n\n* Speichern Sie alles überall.\n\n## Zentralisiert (zentralisierte Datenerhebung)\n\nWenn Organisationen ihre Datenstrategie bewerten, schauen sie zuerst darauf, ihre Speicherung in zentrale Big-Data-Speicherlösungen zu zentralisieren. Dieser Ansatz wird entweder in Form von Cloud-Speicher oder vor Ort in Big-Data-Geräten umgesetzt. Sobald die Daten an einem zentralen Ort gesammelt sind, kann die Datenbanktechnologie grundlegende Geschäftsanalysen ermöglichen, um verwertbare Informationen abzuleiten. In den meisten Fällen werden diese Daten verwendet, um Probleme mit Kunden, der Lieferkette, der Produktentwicklung oder anderen Bereichen in Ihrer Organisation zu beheben, in denen Daten generiert und gesammelt werden.\n\nDiese Attribute beschreiben am besten diese Phase:\n\nManagement durch Reaktion\n\nGrundlegende Datenerhebung\n\n* Datenlager\n\nGroßer Datenspeicher\n\n* Grundlegende Big Data-Analytik (?)\n\n## Vereinfacht\n\nMit der steigenden Anzahl von Datenquellen beginnen Unternehmen Organisationen zu bilden, die sich auf Datenstrategie, -organisation und -governance konzentrieren. Diese Veränderung beginnt mit dem Büro des Chief Data Officers (CDO). Es gibt Debatten darüber, ob der CDO unter dem CEO oder CIO positioniert sein soll. Hängen Sie sich jedoch nicht zu sehr an der Positionierung in der Organisation auf. Das Wichtige ist, einen Fokus auf die Datenorganisation zu etablieren und einen Plan zur Datenormalisierung umzusetzen. Durch Normalisierung ist es möglich, verschiedene Datenquellen zu korrelieren und neue Erkenntnisse darüber zu gewinnen, was in Ihrem gesamten Unternehmen passiert. Beachten Sie, dass die Daten ohne Normalisierung weiterhin isoliert sind und nur teilweise zugänglich sind. Ein weiteres wichtiges Merkmal dieser Phase ist die Notwendigkeit, einen Plan zur Bewältigung des massiven Datenvolumens zu entwickeln, das gesammelt wird. Aufgrund der Zunahme des Volumens und der Kosten für die Speicherung dieser Daten wird gestaffelter Speicherplatz wichtig. Obwohl es in den Anfangsstadien fast unmöglich ist, den optimalen Weg zur Verwaltung der Datenspeicherung zu kennen, empfehlen wir, die besten verfügbaren Informationen zu nutzen, um rationale Pläne für die Datenspeicherung zu entwickeln, mit dem Vorbehalt, dass diese überprüft und verbessert werden müssen, sobald die Daten verwendet werden.\n\nDiese Attribute beschreiben diese Phase am besten:\n\n* Vorhersagendes Datenmanagement (Beginn einer datenorientierten Organisation)\n\nDatennormalisierung\n\nZentralisierte gestaffelte Speicherung\n\n## Verwaltet (Standard-Datenprofile)\n\nIn der Verwaltungsphase haben Organisationen ihre Datenorganisation formalisiert; Datenwissenschaftler, Datenverantwortliche und Dateningenieure sind nun Teil des Teams und haben definierte Aufgaben und Verantwortlichkeiten. Das Management von Metadaten wird zu einem entscheidenden Erfolgsfaktor in dieser Phase und mehrere Anwendungen können nun von den Daten im Unternehmen profitieren. Der Wechsel von einem Datenlager zu einem Daten-See ermöglicht eine größere Agilität bei der Entwicklung datenzentrierter Anwendungen. Die Virtualisierung der Datenspeicherung ermöglicht eine effizientere und dynamischere Speicherlösung. Datenanalysen können nun mit Datensätzen aus verschiedenen Quellen und Abteilungen im Unternehmen durchgeführt werden.\n\nDiese Attribute beschreiben am besten diese Phase:\n\n* Organisiertes Datenmanagement (Datenorganisation an Ort und Stelle mit identifizierten Schlüsselrollen)\n\n* Meta-Daten-Verwaltung\n\n* Datenherkunft oder Datenherkunftsnachverfolgung\n\nData lake translates to German as \"Datensee\".\n\nGroße Datenanalyse\n\nSoftware-definierte Speicherung (Speichervirtualisierung)\n\n## Regiert\n\nDie geregelte Phase wird in erster Linie erreicht, wenn eine Organisation einen zentralisierten Ansatz für Daten verfolgt und einen ganzheitlichen Ansatz für die Steuerung und Sicherung erreicht hat. Der CDO arbeitet eng mit dem CSO (Chief Security Officer) zusammen, um sicherzustellen, dass die Daten- und Sicherheitsstrategien zusammenarbeiten, um die wertvollen Daten des Unternehmens zu schützen und gleichzeitig für Analysen zugänglich zu machen. Daten werden je nach Kritikalität, Geheimhaltung oder Bedeutung in verschiedene Kategorien eingeteilt. Die Einhaltung von Vorschriften wird automatisiert und auf Daten in der gesamten Organisation angewendet. Die Sichtbarkeit für die Nutzung von Daten und Sicherheit nimmt mit den gemeinsamen Daten- und Sicherheitsstrategien und taktischen Plänen zu. Künstliche Intelligenz wird in der Organisation weit verbreitet eingesetzt und Geschäftsentscheidungen werden durch Daten hergeleitet. Daten können jetzt aus dem gesamten Unternehmen, einschließlich Internet der Dinge (IoT) -Geräten an den physischen Vermögenswerten des Unternehmens, gesammelt und katalogisiert werden.\n\nDiese Eigenschaften beschreiben diese Phase am besten:\n\nDatenklassifizierung\n\nDatenkonformität\n\nDatensicherheit\n\nGrundlegende KI\n\nVerteilte Datenvirtualisierung / IoT\n\n## Optimiert\n\nMit der zunehmenden Datensammlung von Organisationen müssen Effizienzen in Automatisierung und kontinuierlicher Prozessverbesserung gefunden werden. Die Automatisierung von Datenprozessen ist das Hauptziel in der optimierten Phase. Insbesondere die Automatisierung der Annotation und der Metatag-Daten verringert die Zeit, um Wert aus den Daten abzuleiten. Die Daten sind mittlerweile zu groß, um an einem zentralen Ort gespeichert zu werden, und eine \"verteilter Daten-See\" Architektur entwickelt sich zum optimalen Weg, um Daten zu verwalten. Machine Learning ist in dieser Phase entscheidend, um Informationen für Entscheidungsträger bereitzustellen und Geschäftsbetriebe und -wert zu optimieren. Anwendung und Daten werden auf Netzwerk-, Speicher- und Recheninfrastruktur basierend auf historischen Informationen und KI-Modellen bereitgestellt.\n\nDiese Attribute beschreiben diese Phase am besten:\n\nAutomatisierte Metatagging\n\nVerteilter Datensee\n\nDateninferenz / maschinelles Lernen\n\nDatengetriebene Infrastruktur\n\n## Innovation\n\nDie ultimative Organisation befindet sich in der Innovationsphase. Sie wird nicht nur von Daten angetrieben, sondern entwickelt neue Produkte, Angebote und Dienstleistungen basierend auf Erkenntnissen aus Daten innerhalb und außerhalb ihrer Organisation. In dieser Phase bieten KI/ML unschätzbare Vorteile. Es gibt drei Unterphasen in der Innovation: Erkenntnis, präskriptiv und Voraussicht.\n\n## Einsicht\n\nEinblick ist datenorientierte Entscheidungsfindung, basierend auf dem, was man tatsächlich in seinem Ökosystem sehen kann, zum Beispiel in der Lieferkette, der Produktentwicklung oder der Fertigung.\n\n## Vorschriften\n\nWährend Erkenntnis wertvoll ist, erfordert sie menschliche Interaktion, Verständnis und Intuition. In der nächsten Stufe, präskriptiv, schlägt Ihre künstliche Intelligenz vor, was Sie basierend auf der Erkenntnis tun sollten. Dies kann eine wichtige Rolle in Ihrer gesamten Organisation spielen, da Entscheidungen datenbasiert sind - vom Lieferkettenmanagement bis hin zur Kundengewinnung.\n\n## Voraussicht\n\nIn diesem entscheidenden Schritt hilft die Daten tatsächlich dabei, die Zukunft zu gestalten. Zum Beispiel ermöglicht Voraussicht einer IT-Organisation, basierend auf historischen Normen und sogar Faktoren wie den sich ändernden Bedingungen bei Konkurrenten, abzuschätzen, wie viel Kapazität sie in Zukunft benötigen wird. Voraussicht erfordert viele Daten und das Training von Modellen, führt aber letztendlich zum Ziel eines Echtzeitunternehmens.\n\nDiese Eigenschaften beschreiben diese Phase am besten:\n\nEinsicht (datengesteuerte Entscheidungen)\n\nVorgeschrieben (datengesteuertes Geschäft)\n\nVoraussicht (die Zukunft gestalten)\n\nTieflernen\n\nEchtzeitunternehmen\n\n## Abschluss\n\nEs ist üblich, sich in einer Phase festzustecken und von der Menge an Veränderungen überwältigt zu fühlen, die erforderlich sind, um in eine neue Phase der Reife zu gelangen. Jeder Schritt nach vorne ist jedoch wertvoll. Zum Beispiel sind Sie vielleicht in einer zentralisierten Phase und können sich mit dem Metadatenmanagement beschäftigen. Gibt es hier die Möglichkeit, über das bloße Bereinigen der Daten hinauszugehen und sie auch zu erweitern? Diese Art von progressivem Denken wird Sie dazu bringen, in der Reife in der Verwaltung Ihrer Informationen voranzukommen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT4-de","image":"./episodes/edt-4/de/thumbnail.png","lang":"de","summary":"In dieser Episode werden wir über das Information Management Reifegradmodell sprechen und wie wir dieses Modell verwenden können, um unseren Organisationen voranzubringen. Dieses Modell kann Ihnen dabei helfen, zu erkennen, wo sich Ihre Organisation in ihrer Informationsmanagementstrategie befindet und wohin sie geht, um letztendlich den maximalen Geschäftswert aus Ihren Daten zu gewinnen."},{"id":111,"type":"Episode","title":"Kollaborative medizinische Forschung mit vertraulichem Computing","tags":["confidentialcomputing","cybersecurity","distributedanalytics","sgx","healthcare","data","technology","edge"],"body":"\r\n\r\n## Warum benötigen wir vertrauliche Berechnung?\n\nDie aktuelle Computernetzwerk-Infrastruktur ist darauf ausgelegt, dass Teilen und Offenheit einen hohen Stellenwert haben: Das Internet ist kostenlos und Daten sollten frei sein. Das hat sich jedoch als Problem in Bezug auf Sicherheit erwiesen. Wir haben einige Lösungen implementiert, die gut funktionieren; wir verschlüsseln Daten, sowohl wenn sie gespeichert sind als auch wenn sie übertragen werden. Trotzdem kann Daten auf verschiedene Arten angegriffen werden, während sie gelesen, analysiert und verwendet werden.\n\nEmpfindliche Daten bleiben anfällig, egal ob es sich um Finanz-, medizinische oder Standortdaten handelt, sowohl in Bezug auf die Sichtbarkeit als auch auf die Datenintegrität.\n\nIm Umgang mit Daten im Gesundheitswesen gibt es zusätzliche Komplexitätsebenen. Es gibt viele Regeln und Vorschriften wie beispielsweise HIPAA, und jeder Bundesstaat hat auch seine eigenen Vorschriften zur medizinischen Datenverarbeitung. Trotz der Komplexität und der Anzahl der regelnden Parteien ist vertrauliches Computing möglich, bei dem Sie die Möglichkeit haben, Daten zwischen Parteien zu teilen, die sich grundsätzlich nicht vertrauen.\n\nFür Vertraulichkeit müssen wir über verschiedene Faktoren nachdenken: Datenintegrität, Datensicherheit und Codeintegrität.\n\n## Kritische Probleme mit Datenschutz und Datensicherheit\n\nDie Gesundheitsakten einer Person, insbesondere in den Vereinigten Staaten, sind überall unter Ärzten, Spezialisten, Laboren und Krankenhäusern verstreut. Die meisten Menschen haben keinen einfachen Zugriff auf ihre Akten. Es ist nahezu unmöglich, ein vollständiges Bild Ihrer eigenen Gesundheit zu erstellen. Vertrauliches Computing kann dabei helfen, diese Barrieren zu überwinden.\n\nZunächst einmal kann es sicherstellen, dass die von Ihnen geteilten Daten aus Sicht der Datenintegrität geschützt werden; sie werden von niemandem verändert. Die Daten können vertraulich sein, bedeutet tokenisiert oder verschlüsselt, können aber dennoch für Berechnungen verwendet werden. Zum Beispiel, wenn eine Partei Analysen zu bestimmten Daten durchführen möchte, müssen sie nicht Informationen wie Namen, Geburtsdaten oder Sozialversicherungsnummern kennen. Wenn diese Teile der Daten tokenisiert oder verschlüsselt werden können, können sie für die Analyse weitergegeben werden, solange die andere Partei vertrauenswürdig ist.\n\nDies ist der Bereich, in dem Sicherheitsmaßnahmen wie Beglaubigung eine Rolle spielen, damit Parteien ihre Identität nachweisen können. Und diese Beglaubigung kann bis auf die Hardwareebene der vertrauenswürdigen Ausführungsumgebungen, die die Hardware bietet, verfolgt werden. Auf diese Weise vertrauen Sie nicht nur dem Transport und Endpunkt, sondern auch der Anwendung und wie sie die Daten verwenden wird.\n\nEs gibt zwei grundlegende Ansätze dafür. Der erste ist das Anwendungs-SDK, bei dem der Entwickler entscheiden kann, wie er seinen Code in vertrauenswürdige und nicht vertrauenswürdige Komponenten unterteilen möchte. Der andere Ansatz besteht darin, ein Laufzeitverschlüsselungssystem zu haben, das auf einer vertrauenswürdigen Ausführungsumgebung aufgebaut werden kann. Dadurch wird der Aufwand minimiert, der erforderlich ist, um eine aktuelle Anwendung in etwas umzuwandeln, das in dieser Umgebung ausgeführt werden kann.\n\nMit einer hardwarebasierten und sicheren Ausführungsumgebung, die Anwendungen und Daten während der Nutzung schützt, wird es äußerst schwierig für einen unbefugten Akteur, selbst wenn er physischen Zugriff auf die Hardware, Root-Privilegien oder Administratorrechte für den Hypervisor hat, auf die geschützte Anwendung und Daten zuzugreifen. Das Konzept des vertraulichen Rechnens zielt darauf ab, sogar den Cloud-Anbieter aus der vertrauenswürdigen Rechenbasis zu entfernen. Auf diese Weise befinden sich nur die Hardware und die geschützte Anwendung selbst im Angriffsbereich.\n\nDiese Rechenumgebungen ermöglichen es den CSPs, das Beste aus dem Angebot der Hardware und der bestmöglichen Sicherheit zu nutzen, über die der Endbenutzer absolut die Kontrolle hat. Jede Partei kann ihre eigenen Richtlinien festlegen sowie Hierarchien von Richtlinien wie auf Bundes- oder Landesebene, und jeder Informationsanbieter kann bestimmen, welche Richtlinien angewendet werden sollen und für wen.\n\n## Evidenzbasierte klinische Studie\n\nDas Zusammenführen aller Daten und das Verständlichmachen ist eine große Herausforderung in der Gesundheitsbranche. Die Vielzahl von Datenschutzeinstellungen und Datenfreigabeeinstellungen, die zwischen den verschiedenen Anbietern, Geräten, geografischen Standorten usw. vorhanden sind, macht es derzeit unmöglich.\n\nAI-Vets, Intel und einige Partner arbeiten gemeinsam an diesem Problem. Die brillant einfache Architektur ermöglicht den Einsatz in unterschiedlichen Umgebungen, Datentypen und Richtlinien und kann dennoch zentrale Analysen durchführen.\n\nEin Beispiel für unsere Implementierung ist ein kleiner Konzeptnachweis: Wie analysiert man über verschiedene Parteien hinweg, wie Krankenhäuser, Forschungsumgebungen und Labore, die jeweils eigene Daten und klinische Studien durchführen? Wie können wir zum Beispiel eine Korrelation zwischen Personen finden, die das Medikament X zur Behandlung von Diabetes einnehmen und die Bedingung Y, sagen wir Krebs haben, obwohl das Medikament X nichts mit der Behandlung von Bedingung Y zu tun hat? Diese beiden Datensätze wären nicht an einem Ort, da sie von verschiedenen Anbietern verwaltet werden.\n\nWenn jedoch die Anbieter Teil eines Ökosystems wären, in dem sie bestimmen könnten, welche Richtlinien sie an ihrem Endpunkt anwenden möchten, könnten wir eine zentrale Anwendung haben, ein zentrales Forschungsportal, das mit diesen Endpunkten verbunden ist. Es gäbe eine Schlüsselverwaltung und Attestierung durch Dritte, um die Berechtigungen und Befugnisse gegenseitig zu überprüfen, so dass alle Parteien einander vertrauen können.\n\nDas ist ein Aspekt des Vertrauens, aber wir müssen auch die Daten schützen, die extrahiert, abgefragt und übertragen werden. Um dies zu erreichen, können wir Daten und Anwendungen in sicheren, verschlüsselten Enklaven verwalten. Die Daten werden entsprechend der von jedem Benutzer festgelegten Richtlinien behandelt, z.B. durch Verschleierung von Geburtstagen, Sozialversicherungsnummern usw. Diese Informationen können tokenisiert werden, so dass sie in unbefugten Händen zu komplettem Müll werden.\n\nDas zentrale Portal kann eine Abfrage durchführen, die mehrere Endpunkte umfasst und verschiedene Arten von Daten in sein Laufzeitsystem integriert und analysiert. Es wird also nicht alles in einen Datenlake gezogen und dann analysiert, sondern dies geschieht in Echtzeit. Es gibt keine Wartezeit darauf, dass die Daten zuerst veröffentlicht oder gereinigt werden, indem alle Richtlinien angewendet werden; dies geschieht dynamisch und im Vorbeigehen.\n\nDies ermöglicht enorme Erkenntnisse. Während der Pandemie zum Beispiel, wenn wir jeden Tag auf die Daten warten müssten und komplexe Analysen darauf ausführen müssten, wäre das schwierig. Wenn wir jedoch in der Lage wären, auf Echtzeitdaten aus all diesen verschiedenen Systemen im ganzen Land und weltweit zuzugreifen und sie sicher teilen könnten, könnten wir einzigartige Erkenntnisse gewinnen, die sonst nicht möglich wären.\n\nWir haben dies bereits in einigen POCs für Klinikseiten gesehen, die wir mit unserem Partner Fortanix durchgeführt haben. Sie haben eine Produktlinie, die es verschiedenen Entitäten ermöglicht, ihre Richtlinien in einer vertraulichen Rechenumgebung zu definieren und die Identitäten der jeweils anderen zu überprüfen sowie Schlüssel und Vertrauen zu verwalten. Das Konzept der vertrauenswürdigen Ausführungsumgebungen existiert bereits seit einiger Zeit und hat sich mittlerweile etabliert, sodass es einfacher geworden ist, davon zu profitieren. Die Anwendungsfälle dafür sind fantastisch.\n\n## Automatische COVID-19-Erkennung anhand von Röntgenbildern der Brust\n\nIn einigen Anwendungsfällen müssen nicht nur die Daten, sondern auch das geistige Eigentum, das mit spezialisierten Algorithmen verbunden ist, geschützt werden. Zum Beispiel könnte es bei der automatischen Erkennung von COVID anhand von Röntgenbildern radiologische Daten, Patientendaten und möglicherweise einen proprietären Algorithmus zur Analyse geben. Die Enklaven können sowohl die Daten als auch die Anwendungen vor neugierigen Blicken schützen.\n\nDie sichere Enklave schützt auch die anderen Maschinen im Netzwerk, denn wenn jemand etwas Schädliches auf die Endknoten im Netzwerk bringt, verhindert das Schlüsselverwaltungssystem den Austausch, da es nicht ordnungsgemäß bestätigt wurde. Die Parteien wählen genau aus, mit welchen Datenquellen die Enklave kommunizieren kann, und sie ist sowohl gegen eingehende als auch ausgehende Kommunikation gesichert.\n\n## Elektronische Gesundheitsakten (eGA)\n\nGesundheitsakten sind ein großes Durcheinander in den Vereinigten Staaten, mit nicht durchsetzbaren papierbasierten HIPAA-Vereinbarungen usw. und allem, was auf verschiedene Einheiten verteilt ist. Ein Anwendungsfall, der für uns ein gutes Modell sein könnte, stammt von der deutschen Regierung. Sie haben vorgeschrieben, dass Gesundheitsdaten in elektronischen Gesundheitsakten gespeichert werden müssen und diese Anwendungen in vertrauenswürdigen Ausführungsumgebungen bereitgestellt werden müssen. Der Patient ist der Endbenutzer und bestimmt, welche Daten verfügbar sind und für wen.\n\nDieses Maß an Feinheit hinsichtlich dessen, was dem Endbenutzer zur Verfügung steht, ist enorm. Und nicht nur werden all diese Daten von den verschiedenen Systemen gesammelt und geteilt, sondern sie befinden sich auch in sicheren Enklaven, sodass sie von der Außenwelt vollständig geschützt sind. Wenn jemand Unbefugtes Zugriff auf die Daten selbst erhalten würde, wäre es völlig bedeutungslos.\n\nDiese vertrauenswürdigen Ausführungsumgebungen sind der erste Schritt in Richtung Kontrollen, die leicht verständlich und leicht durchsetzbar sind.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Nick Bhadange"],"link":"/episode-EDT40-de","image":"./episodes/edt-40/de/thumbnail.png","lang":"de","summary":"Beseitigen Sie die Barrieren zur Beschleunigung der medizinischen Forschung für die Heilung von Krebs mit vertraulichem Computing. Nick Bhadange, Technologie-Spezialist bei AI-Vets, und Darren Pulsipher, Chef-Lösungsarchitekt im öffentlichen Sektor bei Intel, diskutieren den Bedarf an vertraulichem Computing im Gesundheitswesen und die potenziellen Vorteile anhand von Anwendungsfällen."},{"id":112,"type":"Episode","title":"Die Schwarze Kunst von DevOps","tags":["devops","people","technology","compute","devsecops","cybersecurity","multicloud"],"body":"\r\n\r\nLass uns einen Blick darauf werfen, wo DevOps in deine Infrastruktur passt.\n\nAm unteren Ende einer normalen Stapelung haben wir eine physische Schicht, die eine Cloud, ein Rechenzentrum, IoT-Geräte oder vorhandene Infrastruktur bedeuten kann.\n\nDarüber hinaus gibt es normalerweise eine softwaredefinierte Infrastruktur, die die Komplexität der Verwaltung der einzelnen Hardwarekomponenten abstrahiert.\n\nAls Nächstes folgt eine Service-Management-Schicht, die die Virtualisierung des Container-Ökosystems und eine verteilte Informationsmanagement-Schicht umfasst. Diese beinhaltet die Datenebene, Datenseen und alles, was Ihre Daten verwaltet.\n\nDann kommt die Anwendungsebene. Anwendungsentwickler nutzen die Dienste innerhalb der Anwendungsebenen. Direkt an der Schnittstelle zwischen der Anwendungsebene und der Datenspeicherungsebene sowie dem Dienstmanagement befinden sich das SecDevOps- oder DevOps-Toolkit. Diese Tools umfassen Sicherheits- und Identitätsaspekte, die eine sichere Möglichkeit für die kontinuierliche Integration und Bereitstellung Ihrer Produkte bieten.\n\n## Anwendungsschicht / Arbeitslastschicht\n\nAn der Spitze der Anwendungs- und Arbeitslastschicht, die SecDevOps speist, gibt es drei Arten von Arbeitslasten: ereignisgesteuerte Arbeitslasten, prozedurale Arbeitslasten und eine Mischung aus beiden, die GUI- oder UI-gesteuerte Arbeitslasten sind.\n\nEin einfaches Beispiel für eine ereignisgesteuerte Arbeitsbelastung wäre, dass eine Bestellung in Ihrem System eingeht und andere Dinge ausgelöst werden. Es können sich sequenzielle oder parallele Schritte ergeben, Interaktion mit Menschen sowie Automatisierung und Interaktion mit verschiedenen Anwendungen oder Teilsystemen innerhalb des Unternehmens.\n\nViele Workload-Automatisierungstools stehen zur Verfügung. Einige davon sind skriptbasiert, während andere die robotergesteuerte Prozessautomatisierung verwenden, die mehr auf GUI und Benutzeroberfläche ausgerichtet ist. Diese Tools arbeiten an der Automatisierung der darunterliegenden Dienste, sodass die Workloads die Interaktion mit den Diensten steuern.\n\nDienstleistungen werden traditionell in drei Hauptkategorien eingeteilt: Anwendungen wie Standardprodukte wie Word oder eine SAP-Anwendung; komplexe Dienstleistungen, die für einen bestimmten Zweck entwickelt wurden, wie zum Beispiel ein MEAN-Stack mit Mongo; und einfache Dienstleistungen, die eine einzige Funktion erfüllen, zum Beispiel MongoDB, das die Datenbank speichert.\n\nEs gibt eine neue Kategorie aufgrund des Wachstums von KI und ML. Viele Dienste haben ohne ein Anhängermodell nicht viel zu bieten, daher haben wir KI-Modelle in die Dienstebene integriert, die wir ähnlich wie einen einfachen Service behandeln.\n\n## Entwicklertag im Leben\n\nNachdem wir die Arbeitsbelastung und die Services verstanden haben, können wir uns anschauen, was ein Entwickler normalerweise tut.\n\nEin Entwickler wird einige Codezeilen auf ihrer Workstation schreiben und einige Funktionalitätstests durchführen. Anschließend lädt er/sie den Code beispielsweise in GitHub hoch, und ein Continuous Integration Continuous Delivery (CICD) Pipeline wird gestartet. Dabei werden Sicherheitschecks gegen den Code durchgeführt, möglicherweise Linting, statische Analyse und dynamische Analyse.\n\nSobald es diese Tests besteht, wird es normalerweise in einen Integrationszweig überführt, in dem andere Personen im Entwicklungsteam die Daten abrufen und diese entwickeln und ihren Code mit dem Code des Entwicklers integrieren. Anschließend, wenn es ihre Tests besteht, wird es in eine Testphase überführt. Sobald diese Phase abgeschlossen ist, wird es in die Produktion übernommen.\n\nDies ist eine typische CICD-Pipeline, die seit Jahrzehnten besteht. Im Laufe der Jahre wurden die verschiedenen Arten, Pipelines zu beschreiben, vereinheitlicht und standardisiert, um Komplexitäten und Fehler zu minimieren.\n\n## DevSecOps Stapel\n\nDie Pipeline ist nur ein Element eines SecDevOps-Stapels.\n\nAndere notwendige Elemente sind ein Register und ein Repository. Denken Sie an diese als versionierte Repositories, um Artefakte zu speichern, die während der CICD-Pipeline generiert werden, sodass sie leicht wieder verwendet werden können.\n\nEin weiteres wichtiges Element ist ein Automatisierungsframework. Dieses hilft, die menschliche Arbeitsbelastung bei Aufgaben wie Sicherheitsprüfungen oder dem Vorantreiben von Builds von einer Stufe zur nächsten zu reduzieren. Die Tools zur Automatisierung sind fortgeschritten und Schulungen stehen zur Verfügung. Daher sollte ein gutes Automatisierungsframework grundlegend sein.\n\nObwohl das Umweltmanagement oft organisch mit der Zeit wächst, ist es sinnvoll, die Umgebungen angemessen zu verwalten und zu gestalten, um mehr Zuverlässigkeit und Wiederholbarkeit zu erreichen.\n\nEin entscheidendes Element darunter ist ein Sicherheitsprofil. Sie sollten in der Lage sein, Sicherheitsprofile zu definieren, damit sie in verschiedenen Umgebungen und über verschiedene Anwendungsstapel hinweg verwendet werden können.\n\n## Register / Datenbanken\n\nIn der Regel gibt es mindestens zwei verschiedene Arten von Repositories. Das erste ist ein Staging-Repository, in dem man Bilder erstellen kann (eine Sammlung des gesamten Codes, der benötigt wird, um einen Container zum Beispiel hochzufahren) und Dinge wie Identitäts- und geheime Schlüssel speichern kann. Dieses Repository enthält alles, was man braucht, um Dinge in die Produktion zu bringen. Einige Organisationen können mehrere Staging-Repositories haben, während verschiedene Elemente unterschiedliche Reifestadien durchlaufen, bis sie das Produktions-Repository erreichen. Man möchte in der Lage sein, zu früheren Versionen zurückzukehren, wenn nötig.\n\nIn der Produktions- oder Golden-Repository werden Bilder gesperrt, beglaubigt und verschlüsselt. Nur Dinge, die sich im Golden-Repository befinden, werden in die Produktion übertragen.\n\n## Stufen\n\nDer beste Weg, um die Phasen in der CICD-Pipeline zu betrachten, ist, dass jede Phase in einer eigenen Umgebung arbeitet. Zum Beispiel gibt es in der Build-Phase eine isolierte Build-Umgebung mit Richtlinien. Erst wenn alle Schritte in dieser Phase abgeschlossen sind, kann es zur nächsten Phase übergehen. Dies vermeidet den Ressourcenverbrauch durch parallele Builds und Durchläufe, die letztendlich scheitern könnten. Gleichzeitig ist es am besten, nicht so viele Phasen zu haben, dass sie den Fortschritt behindern, daher ist ein sorgfältiger, definierter Plan wichtig.\n\n## Schritte\n\nIn den Stufen befinden sich Schritte, in denen die eigentliche Arbeit erledigt wird. Beim Erstellen und Testen der Software können Schritte parallel oder in Abfolge ausgeführt werden; es gibt viele Tools, mit denen Sie diese Vorgänge definieren können. Obwohl einige eine grafische Benutzeroberfläche dafür haben, bevorzugen die meisten Entwickler ein textbasiertes Format, da es eine Versionskontrolle der Pipeline und der Schritte ermöglicht und Sicherheitsüberprüfungen gegen die Pipeline erlaubt.\n\n## Pipeline\n\nMit definierten Stufen und Schritten haben Sie eine echte Pipeline. Anstatt eine Pipeline für alle Ihre Anwendungen zu definieren, was in der Regel scheitert, weil sie mit vielen Bedingungen zu komplex oder zu einschränkend wird, empfehle ich, Vorlagenaufträge zu verwenden und sie bei Bedarf anzupassen, um sicherzustellen, dass sie den Compliance-Standards und Vorschriften entsprechen. Es ist wichtig, zu Beginn eines Projekts eine passende Pipeline einzurichten und gleichzeitig Flexibilität während des Projektfortschritts zu wahren.\n\n## Umgebungen\n\nAnstatt ad-hoc-Umgebungen zu erstellen, ist es am besten, sie von vornherein absichtlich zu erstellen. DevOps oder SecDevOps können Sicherheitsrichtlinien und Compliance in allen verschiedenen Projekten implementieren, um Sicherheit zu gewährleisten.\n\n## Service Stack\n\nLassen Sie uns betrachten, wie Entwickler heutzutage arbeiten, was auf Diensten beruht. Selbst wenn Entwickler in einer monolithischen Anwendung arbeiten, neigen sie dazu, ihre Arbeit in funktionale Einheiten wie Datenbanken, Business-Logik-Knoten oder Transportebenen zu gruppieren. Zum Beispiel bei der Verwendung eines einfachen Dienstes wie MongoDB. Wenn ein Entwickler diesen Container auf seinem Laptop ausführt, bietet er die erwartete Funktionalität, um Daten auf dokumentenorientierte Weise zu speichern. Auf dem Laptop könnte es der einzige ausgeführte Container sein.\n\nIn einer Test- oder Entwicklungs-Umgebung könnten mehrere Instanzen dieses Dienstes ausgeführt werden, und der Entwickler könnte einen MongoDB-Service-Cluster bereitstellen und sie für einen Test miteinander verbinden. Der Service ist immer noch ein Mongo-DB-Service, aber sein Verhalten ändert sich basierend auf der Umgebung, in der er sich befindet. Das Ziel für Entwickler besteht darin, Code zu schreiben und ihn gegen den MongoDB-Service auf ihren Laptops zu überprüfen, um sicherzustellen, dass er auch in der Produktion ordnungsgemäß ausgeführt wird.\n\nEin einfacher Dienst wie MongoDB ist notwendig, aber für sich allein nicht besonders nützlich. Komplexe Dienste wie LAMP-Stacks oder MEAN-Stacks sind wichtiger. Diese bestehen aus mehreren gemeinsam ausgeführten Diensten und fungieren im Wesentlichen als ein Dienst. Wenn sie zusammengefasst werden, wird ein komplexer Dienst auf einem Laptop bereitgestellt und es gibt zwei oder drei einfache Dienstcontainer, die laufen und Entwicklern die erforderliche Funktionalität bieten, um ihren Code einzuchecken.\n\nSobald der Code eingecheckt ist, startet er in die Entwicklungs-Pipeline, in der der Entwickler mit anderen Personen integriert. Der gleiche komplexe Service kann verschiedene Vorgehensweisen annehmen. Viele Sicherheitsrichtlinien können diesem komplexen Service zugewiesen werden, um sicherzustellen, dass er sicher, zuverlässig und belastbar ist.\n\n## Service/Application Definitions: Dienst-/Anwendungsdefinitionen\n\nEs ist wichtig, die Konzepte einfacher und komplexer Dienste zu verstehen, da Softwareentwickler definieren müssen, wie man sie zum Laufen bringt. Es gibt ein paar Definitionen. Eine davon wird als Bilddefinition bezeichnet. Diese kommen häufig in der Containerwelt vor und werden Docker-Images genannt. Die Docker-Datei definiert, was in diesem Bild enthalten ist. Es gilt als einfacher Container an sich, obwohl immer mehr Menschen Container für komplexe Aufgaben verwenden.\n\nIn Service-Definitionen können mehrere Imaginationsdefinitionen enthalten sein, zum Beispiel Docker Compose, Kubernetes Operators, Helm Charts, Terraform und sogar CNAB. Dies sind Werkzeuge, mit denen du einen Service definieren kannst. Ein Service ist mehr als nur der Container; es ist die Umgebung, in der der Container läuft. Sie können Netzwerkdefinitionen, Volumenverbindungen oder sogar Bereitstellungspolicies enthalten. Eine vollständige \"Service-Definition\" umfasst Image-, Konfigurations- und Bereitstellungsdefinitionen.\n\n## Alles zusammenfügen\n\nWenn ein Entwickler einen neuen Service erstellt, erstellt er nicht nur den Code für das Abbild; er definiert auch die Umgebung oder Konfiguration, in der dieser ausgeführt werden soll. Dies ist der Punkt, an dem sich Ihre Umgebung und die Service-Definition verbinden können. Zur Laufzeit wird es die Umgebung erzeugen, die erforderlich ist, damit der Container auf effektive Weise wiederholbar ausgeführt werden kann, sodass Sie den Code problemlos von einem Desktop-System in eine vollständige Produktionsumgebung verschieben können.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT41-de","image":"./episodes/edt-41/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chef Solution Architect im öffentlichen Sektor bei Intel, definiert gängige DevOps-Begriffe und erklärt, wo DevOps in Ihrer Organisation angesiedelt ist."},{"id":113,"type":"Episode","title":"Absichern der DevOps-Pipeline = SecDevOps","tags":["cybersecurity","devops","compute","process","technology","multicloud","devsecops"],"body":"\r\n\r\nEine vor über 20 Jahren durchgeführte Studie zur Rendite von Sicherheitsinvestitionen hat gezeigt, dass es kostengünstiger ist, Sicherheit bereits in den Entwicklungszyklus einzubinden, anstatt bis zum Ende des Zyklus zu warten. Obwohl wir dies seit zwei Jahrzehnten wissen, befinden wir uns immer noch in einem fortlaufenden Prozess.\n\nDie meisten Entwicklungsunternehmen haben Sicherheit als Teil ihres gesamten Entwicklungsprozesses, daher gab es bereits viele Fortschritte. Es handelt sich jedoch um eine Reise, kein Sprint. Es geht darum, alle verschiedenen Angriffsstellen und Schwachstellen zu verstehen und die richtigen Sicherheitsprozesse für diese Probleme bereitstellen zu können.\n\n## Die Angriffsvektoren\n\nOft denken Menschen bei Angriffen an die Schwachstellen von Mitarbeitern oder Paketen und vergessen dabei den Prozessaspekt der Geschichte. Auf der operativen Seite geht es um die Zeit, die benötigt wird, um eine Schwachstelle zu schließen. Auf der anderen Seite steht die Entwicklung und Auslieferung von Produkten. Es gibt verschiedene Bruchstellen entlang dieser Kette, die kürzlich in den letzten Phasen des Build-Prozesses ausgenutzt wurden. Soweit es die Kunden betrifft, galt der aktualisierte Code als legitim, da er direkt aus der Quelle stammte. Daher müssen wir sorgfältig überlegen, wo wir Sicherheitsmaßnahmen in den Prozess integrieren sollten.\n\n## Sicherheit eingebaut...\n\nAgil, CI/CD, DevOps,...Wo ist die Sicherheit?\n\nWährend wir dazu neigen, Sicherheit bei den Übergängen vom Entwickler zur Qualitätssicherung und von der Qualitätssicherung zur Betrieb oder Bereitstellung zu betrachten, sollte Sicherheit wirklich in den gesamten Build-Zyklus integriert werden, nicht nur bei einigen Kontrollpunkten. Der Ansatz sollte kontinuierliche Sicherheit sein.\n\n## Sicherheit integriert….\n\nMachen Sie Sicherheit zu einem Teil jeder Phase\n\nKontinuierliche Sicherheit ist eine Herausforderung. Die meisten Entwickler und QA sind nicht im Bereich Sicherheit ausgebildet; das ist ein bergauf Kampf. Die Branche hat diesen Ansatz bereits in den frühen 2000er Jahren versucht, stieß jedoch auf drei Probleme. Erstens ist die Fluktuation zu hoch. Zweitens ändert sich die Sicherheitslandschaft zu schnell, um sie auf dem neuesten Stand zu halten, und drittens wurden die richtigen Verhaltensweisen nicht belohnt, da es nicht ihr Hauptaufgabenbereich war.\n\nWie integrieren wir dann Sicherheit in den Prozess, automatisieren die wichtigsten Aufgaben, die wir erledigen möchten, und treten den Entwicklern nicht in den Weg, damit sie ihre Arbeit erledigen können - nämlich den Code zu erstellen, zu testen und bereitzustellen? Der Sicherheitsprozess kann wirklich glänzen, indem er ihn in diese Automatisierungen einbindet, die Sie bereits in DevOps durchführen, wie automatisierte Modultests, automatisierte Qualitäts- und Regressionstests, automatisierte Erstellung und automatisierte Bereitstellung. Dies löst nicht alle Probleme, aber es wird die Messlatte erheblich erhöhen, sodass Sie sich auf die schwierigen Herausforderungen im Bereich Sicherheit konzentrieren können.\n\n## Sicherheit eingebaut…\n\nDas bedeutet, dass Sicherheit von Anfang an integriert ist und nicht nur ein einzelner Schritt oder eine einzelne Stufe ist.\n\nEinige gängige Tools bieten bereits automatisierte Sicherheitsfunktionen, die auf Schwachstellen hinweisen. Zum Beispiel führt GitHub Sicherheitsprüfungen für Projekte mit Node.js-Code und alle enthaltenen Pakete durch. Dies kann hilfreich sein, aber es ist bereits zu spät; die Sicherheit sollte in die Pipeline integriert werden, bevor sie überprüft wird.\n\n## Sicherheit bereits integriert....?\n\n## Wie gelangen wir dorthin?\n\nAktuelle Sicherheitsverletzungen verdeutlichen, dass Sicherheit in jedem Schritt des Prozesses integriert werden muss, einschließlich zwischen dem Erstellen und der Produktion sowie unmittelbar vor dem Ausführen des Skripts zur Erstellung der Anwendung. Zusätzlich zur Einbindung von Sicherheit in den Erstellungsprozess müssen wir auch den Erstellungsprozess selbst absichern; er ist schon lange Zeit eine große Schwachstelle.\n\nViele Unternehmen, die interne Entwicklung betreiben, werfen nun einen genaueren Blick auf ihren Entwicklungsprozess aufgrund der jüngsten Sicherheitsverstöße. Das ist gut, aber es darf nicht bei diesen übereilten Reaktionen auf jeden Angriff stehenbleiben. Wir müssen holistisch denken und nicht auf das nächste schwache Glied in der Kette warten.\n\nEinige praktische Möglichkeiten, den Prozess abzusichern, bestehen darin, den Build-Server als wesentliche Ressource in der gesamten Infrastruktur zu behandeln und dieselben Regeln und Kontrollen auf diesen Server anzuwenden wie für Ihre Kernsysteme. Die Verwendung geeigneter Zugangsdaten, eine sichere Firmware-Startsequenz, Überprüfung des Codes, Protokollierung und Überwachung des Systems usw. werden dann während seiner gesamten Lebensdauer in den DevOps-Prozess integriert, wenn jemand auf den Button klickt.\n\n## Gebaut, um zu bleiben\n\n## Von Lösungen zu Dienstleistungen und darüber hinaus\n\nViele Leute denken nicht an das Skript selbst als ein Ziel. Es spielt keine Rolle, wie viele gute Module enthalten sind, wenn das Skript selbst nicht geschützt ist. Ein paar Möglichkeiten, das Skript zu schützen, sind, eine Prüfsumme auszuführen und es sollte versioniert, überprüft und signiert werden. Dies bringt zusätzliche Komplexität für DevOps mit sich, aber es gibt Tools, die helfen können.\n\n## Einmal erstellen, überall bereitstellen\n\nGenau wie wir den Entwicklungsprozess automatisieren, können wir auch die Automatisierung für die Implementierung dieser Kontrollen und Prüfungen einbauen. Automatisierung verhindert, dass eine andere Person möglicherweise Ihre Builds durcheinanderbringt, aber wir möchten auch sicherstellen, dass ein Mensch die Ergebnisse erhält und Audits überprüft.\n\nDie bereits verwendeten Werkzeuge können erweitert werden, um Sicherheitsautomatisierung und Checks hinzuzufügen, zum Beispiel für die kontinuierliche Entwicklung und Integration im agilen Zyklus oder für Automatisierungstools in der Linux-Welt.\n\nOrganisationen können ihre Sicherheitsmitarbeiter auch über die Teams für Geschäftsentwicklung verteilen, sodass Sicherheitspersonal bereits im Prozess eingebunden ist, falls Probleme auftreten. Zwei Bereiche, in denen Sie sicherstellen wollen, dass Sie Sicherheitspersonal haben, sind die Infrastruktur zur Unterstützung Ihres agilen Prozesses beispielsweise und das Produktmanagement, um Sicherheitsanforderungen für die Phase der Produktanforderungsdefinition zu erhalten, noch bevor sie an einen Entwickler gelangt.\n\nEs gibt immer einen Mangel an ausreichend geschultem und fähigem Sicherheitspersonal sowie an Finanzierung, um die richtigen Leute einzustellen, aufgrund hoher Nachfrage. Einige Möglichkeiten sind, die bereits vorhandenen Mitarbeiter zu schulen und ihnen die erforderlichen Werkzeuge zur Verfügung zu stellen. Es ist nicht notwendig, einen Kryptoguru bei jedem Schritt des Prozesses zu haben. Eine weitere Möglichkeit ist es, anstelle dessen, dass jeder Programmierer für die Programmierung von Authentifizierung, Zugangsdaten und Protokolle in einem sicheren Aufbau in einer Infrastrukturbibliothek verantwortlich ist, ein Team zu haben, das Module entwickelt, die in Ihren Sprachen und Umgebungen alle Sicherheitsfunktionen abdecken. Der Programmierer kann das Modul aufrufen und es erledigt die schwere Arbeit. Auf diese Weise bauen Sie einmal und stellen überall bereit.\n\nWir sehen, dass Unternehmen SaaS-Sicherheitstools bereitstellen, cloud-basierte Dienste, die für Ihre Anwendung und Ihre Laufzeitumgebung genutzt werden können. Das ist ein großer Schritt im Prozess. Es gibt Unternehmen, die Sicherheitseinbindungspunkte wie Anwendungssicherheit in einer schnellen Umgebung anbieten. Diese Anwendungsprüfungen wie Eingabesäuberung und Eingabevalidierung können in Ihre funktionale Umgebung integriert werden, aber das muss noch bis zum Ende gewartet werden. Denken Sie daran, je früher Sie mit der Sicherheit beginnen, desto günstiger und weniger schmerzhaft wird es.\n\nAll dies erfordert natürlich weitere Integrationsarbeit. Entwickler können vorsichtig sein, was die damit verbundene Arbeit betrifft, aber wenn ein Framework mit integrierter Sicherheit vorhanden ist (und es gibt Prototypen wie Ruby on Rails und bestimmte Cloud-Infrastrukturen), kann es viele Stunden sparen. Sie müssen jedoch darauf achten, dass Sie sich nicht ausschließlich auf die Plattform für Sicherheit verlassen, da dies ein einzelner Fehlerpunkt sein könnte.\n\n## Automatisierung wird dich befreien.\n\nDie Sicherheitsverletzungen in den letzten sechs Monaten waren tiefgreifend. Hier sind einige wichtige Ratschläge:\n\nSicherheit sollte integraler Bestandteil des gesamten Lebenszyklus von den Anforderungen an sein. Sicherheit muss sich im DevOps-Zyklus selbst befinden, nicht nur in der Kodierung und Prüfung, sondern auch in der Infrastruktur, die diesen Prozess antreibt.\n\nBeim Bau von Sicherheitswerkzeugen und Objekten durch Module: Baue sie einmal, mache sie modular und setze sie überall ein.\n\nNutzen Sie Dienstleistungen, bei denen Sie sich auf die Expertise anderer verlassen können, um Ihr eigenes unterfinanziertes Cyber-Team zu stärken.\n\nAutomatisierung wird dich befreien. Automatisiere so viel wie möglich, um die Sicherheit zu erleichtern und zu beschleunigen und die Reibung für deine Entwickler und Tester zu reduzieren. Mit Automatisierung kannst du 80 Prozent von dem, was wir als dumme Angelegenheiten bezeichnen, eliminieren, damit du deine begrenzten Ressourcen für die schwierigen Probleme nutzen kannst.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT42-de","image":"./episodes/edt-42/de/thumbnail.jpg","lang":"de","summary":"In dieser Episode diskutieren Darren Pulsipher, Chief Solution Architect bei Intel, und Steve Orrin, CTO von Intel Federal, warum und wie die DevOps-Pipeline abgesichert werden muss. Der einzige Weg, um solide, belastbare und sichere Codes zu liefern, besteht darin, Sicherheit von Anfang an einzubauen, und je früher, desto besser."},{"id":114,"type":"Episode","title":"Die Rolle des CIO bei der Cloud-Adoption","tags":["cio","cloudadoption","compute","cloud","csp","multicloud"],"body":"\r\n\r\nIm Teil eins dieses Interviews sprechen Darren Pulsipher, Chief Solution Architect bei Intel, und Doug Bourgeois, Managing Director und GPS Cloud Strategy Leader bei Deloitte, über die Migration in die Cloud und die Rolle des CIO.\n\n## CIO Heritage auf Deutsch übersetzen.\n\nDoug stieg früh in die Cloud ein, während er für die Bundesregierung arbeitete und eine große Organisation für gemeinsam genutzte Dienste leitete. Er erkannte die Vorteile der Cloud für seine Dienstleisterorganisationen und erkannte auch den Mehrwert für seine eigene Organisation. Erstens würde es auf Infrastrukturebene Geld sparen und zweitens bot es die Möglichkeit, die Cloud aufzubauen und gleichzeitig die Serverüberlastungsprobleme durch Virtualisierung zu lösen. Bei einem bestimmten Dienst betrug die Kostenreduktion für den Endbenutzer 40 Prozent.\n\nIn gemeinsam genutzten Diensten wie der Cloud können Sie, sobald Sie Skaleneffekte erreichen, die Dienste wirtschaftlicher anbieten als die meisten Organisationen dies alleine tun können.\n\n## Der Übergang zur Cloud\n\nVor einem Dutzend Jahren waren die meisten Organisationen hauptsächlich auf private Clouds ausgerichtet. Sie verbesserten ihre Rechenzentren, um eine Kombination aus konsolidierten Mehrmandantenlösungen mit eingebauten Automatisierungsfähigkeiten zu integrieren. Etwa drei oder vier Jahre später pendelte es sich dann zu den öffentlichen Clouds bei den großen Hyperskalierern (AWS, Azure, Google) ein.\n\nVor ungefähr drei Jahren erreichten Organisationen ein Gleichgewicht in der hybriden Cloud. Die Menschen haben erkannt, dass eine Vielzahl von Systemen in ihrem Portfolio sich für verschiedene Modelle eignen, einige privat, einige öffentlich, einige hybrid. Insgesamt gibt es heute einen ganzheitlicheren Ansatz, um Systeme und Clouds mit spezifischen Zwecken abzugleichen.\n\n## Die Informationen in den CIO einfügen.\n\nEs ist jetzt wichtiger denn je, dass CIOs eine fundierte Kenntnis darüber haben, was in ihren Organisationen vor sich geht, um der Mission und den Geschäftszielen näher zu kommen und ihre Bedürfnisse am besten zu erfüllen. Früher musste ein CIO möglicherweise nur die Infrastruktur bereitstellen, jetzt müssen sie jedoch fundierte architektonische Entscheidungen treffen, basierend auf dem, was verfügbar ist. Hierfür gibt es zwei Gründe. Der erste Grund ist die Verbreitung von Daten, künstlicher Intelligenz, Analytik und maschinellem Lernen in die Kerngeschäftsfähigkeiten, die ein grundlegendes Geschäftsverständnis erfordern. Der zweite Grund liegt in der Entwicklung der Cloud, die eine neue Phase erreicht hat - das digitale Zeitalter, in dem die Kernsysteme der Organisation modernisiert werden müssen, um die Servicefähigkeit für ihre Endbenutzer zu verbessern.\n\nDiese Reise hat begonnen, den CIO dorthin zurückzubringen, wo sie hingehören – bei der Verwaltung von Informationen, statt sich so sehr auf die Infrastruktur zu konzentrieren. Viele CIOs wurden auf die Infrastruktur beschränkt, obwohl ihnen die Möglichkeit gegeben werden könnte, etwas wirklich Transformierendes zu tun.\n\n## CIO-Positionierung für Erfolg\n\nWie sieht die Reise vom Chief Infrastructure Officer zum Chief Information Officer aus?\n\nEine Möglichkeit besteht darin, sich so zu positionieren, dass man für etwas Neues verantwortlich ist, das die Organisation versucht zu tun, vielleicht ein neuer Prozess oder der Eintritt in einen neuen Markt, oder sogar eine Geschäftseinheit, die noch nicht aktiv ist. Sie haben Kontinuität und Perspektive, da Sie mit all den verschiedenen Anwendungsinhabern zusammengearbeitet haben, und sind daher besonders qualifiziert, die Initiative voranzutreiben. Eine andere, häufigere Möglichkeit besteht über ein negatives Ereignis, bei dem offensichtlich wird, dass Veränderungen notwendig sind. Eine Katastrophe kann der Auslöser sein, dass ein IT-Leiter den Weg zu einer echten Transformation führt.\n\nNehmen Sie an Teil 2 des Interviews teil...\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Doug Bourgeois"],"link":"/episode-EDT43-de","image":"./episodes/edt-43/de/thumbnail.png","lang":"de","summary":"Im Teil eins dieses Interviews sprechen Darren Pulsipher, Chief Solution Architect bei Intel, und Doug Bourgeois, Managing Director und GPS Cloud Strategy Leader bei Deloitte, über die Cloud-Migration und die Rolle des CIO."},{"id":115,"type":"Episode","title":"Cloud im digitalen Zeitalter","tags":["cloudmigration","cloud","compute","process","deloitte","multicloud"],"body":"\r\n\r\n## Der typische Umstieg auf die Cloud für eine Organisation\r\n\r\nIn den letzten fünf oder sechs Jahren haben sich die Methoden, Werkzeuge und Erfahrungen bei der Cloud-Migration zu wiederholbaren Prozessen entwickelt.\r\n\r\nDer erste Schritt besteht darin, sich für Ihre Migrationsprioritäten zu entscheiden, da dies in Phasen und nicht als großer Umzug geschehen wird. Dies erfordert nicht viel Zeit oder Ressourcen, ist aber von entscheidender Bedeutung. Ein extremes Beispiel wäre, dass Sie nicht das Mainframe als erstes System wählen möchten, das in die Cloud migriert wird, sondern ein eigenständigeres System wie E-Mail.\r\n\r\nUnternehmen wie Deloitte haben Entdeckungswerkzeuge entwickelt und investiert, die dabei helfen, den Migrationsprozess zu beschleunigen. Diese Werkzeuge werden eine breite Menge an Daten erfassen, einen Algorithmus ausführen, der sich mit Komplexität befasst, und alle Systeme in verschiedene Kategorien einstufen. Das Verständnis der Konfiguration und Integration von bestehenden Systemen sowie die Kompatibilität von Softwarekomponenten sind grundlegend für die Cloud-Migration. Wir müssen auch Grenzen und Compliance-Richtlinien wie PCI oder HIPAA beachten. Das Aufbau von Landeplätzen für diese Umgebungen in der Cloud ist Phase zwei des Prozesses.\r\n\r\n## Wertversprechen der digitalen Modernisierung\r\n\r\nManchmal gibt es eine erhebliche Menge an Vorbereitungsarbeit für Migrationen. Die erste Welle ist am einfachsten mit der geringsten Anzahl an Modifikationen, aber danach, in Phase zwei, können Upgrades oder Änderungen an Betriebssystemen, Replattformierung oder der Wechsel zu einem anderen Typ von Datenbanken zum Beispiel vorkommen. Die dritte Welle umfasst oft veraltete Client-Server oder proprietäre Architekturen, die eine umfangreiche Neugestaltung erfordern und Monate dauern können, um auf die Cloud vorbereitet zu werden.\r\n\r\n## Digitale Modernisierung und Cloud-Migration\r\n\r\nEs ist wichtig zu unterscheiden, dass Cloud-Bereitschaft nicht dasselbe ist wie Cloud-Optimierung; das kommt später.\r\n\r\nIn vielen Fällen ist der Hauptgrund für den Umzug in die Cloud eher geschäftlich als technologisch bedingt. Zum Beispiel möchte ein Kunde möglicherweise den Mietvertrag für ein Rechenzentrum nicht weiterführen oder seine physischen Büros verlagern. In solchen Fällen spielt der Zeitfaktor eine Rolle, daher ist es sinnvoll, die Migration basierend auf der Bereitschaft statt auf Optimierung durchzuführen.\r\n\r\nEinmal in der Cloud angekommen, müssen Sie optimieren, da die Kostentreiber in der Cloud anders sind als im herkömmlichen Rechenzentrum. Die Kosten eines Rechenzentrums sind nach der anfänglichen Investition relativ versteckt, während die Cloud eher einem Mietvertrag gleicht, der auf unbestimmte Zeit läuft. Oftmals lösen wir Probleme in den herkömmlichen Systemen, indem wir mehr Speicher, mehr CPU oder mehr Speicher hinzufügen, da dies bis zu einem gewissen Grad funktioniert, aber dies führt zu ineffizienten Systemen. Wenn wir diese ineffizienten und ressourcenintensiven Systeme einfach in die Cloud verschieben, ist das Kostenmodell viel höher als notwendig, daher die Notwendigkeit zur Optimierung.\r\n\r\nEin Teil des Optimierungsprozesses könnte eine Prozessänderung sein. Zum Beispiel stiegen die Kosten für eine kanadische Organisation in die Höhe, als sie eine SAP-Instanz in die Cloud verschoben. Sie bemerkten, dass sie diese Instanz nachts oder am Wochenende nicht nutzten, daher wechselten sie von einem 24/7-Modell zu einem 16/5-Modell. Durch diesen Wechsel konnten sie eine erhebliche Menge Geld einsparen. Es gibt also Möglichkeiten, mit verschiedenen Ansätzen einen kleinen Aufwand und eine hohe Wertsteigerung zu erzielen.\r\n\r\nWir sehen endlich, nachdem wir mehr als ein Jahrzehnt in der Cloud sind, einen aufkommenden Trend, den Wert in einer Veränderung der Geschäftsstrategie zu finden, anstatt in der Infrastruktur. Die COVID-19-Pandemie hat sicherlich dazu beigetragen, diese Veränderung zu beschleunigen. Ein perfektes Beispiel dafür ist die Telemedizin. Sie existierte bereits, war jedoch fünf oder sechs Jahre lang stagniert, bevor die Pandemie ausbrach. Jetzt ist dieses Modell die Norm.\r\n\r\nIn der Cloud finden transformative Innovationen statt. Mit dem zunehmenden Wechsel von mehr Systemen in die Cloud werden Branchen weiterhin versuchen, verschiedene Modelle mit neuen, transformierenden Fähigkeiten zu übernehmen.\r\n\r\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Doug Bourgeois"],"link":"/episode-EDT44-de","image":"./episodes/edt-44/de/thumbnail.png","lang":"de","summary":"Im zweiten Teil des Interviews setzen Darren Pulsipher, Chief Solution Architect bei Intel, und Doug Bourgeois, Managing Director und GPS Cloud Strategy Leader bei Deloitte, ihre Diskussion über die Cloud-Migration fort."},{"id":116,"type":"Episode","title":"Nicht alle Kerne sind gleich.","tags":["multicloud","compute","optimization","workloadplacement","workload","migration","process","cloudinstance"],"body":"\r\n\r\nIntel hat Cloud-Lösungsarchitekten eingestellt, um Kunden dabei zu unterstützen, intelligent zu den richtigen Cloud-Instanzen für ihre Workloads zu wechseln. Stephen ist kürzlich bei Intel eingestiegen und verfügt über eine lange Erfahrung in Datenbanken im Allgemeinen, wobei er sich in den letzten sieben oder acht Jahren auf die Cloud konzentriert hat und deren Ressourcen zum Vorteil der Kunden nutzt.\n\nEinige der schwierigsten Aspekte, mit denen Stephen bei der Unterstützung von Kunden beim Umzug in die Cloud konfrontiert war, sind die Preisgestaltung und die Erwartungen an den Umzug. Cloud-Service-Anbieter behaupten oft, dass Kunden Geld sparen werden, indem sie in die Cloud wechseln, aber das ist oft nicht der Fall.\n\nEin Grund dafür liegt darin, dass es ständige Veränderungen in den Merkmalen, Funktionen und Fähigkeiten des wettbewerbsintensiven Marktes der Cloud-Service-Provider gibt. Ein weiterer Grund ist Unwissenheit, zum Beispiel die Annahme, dass ein Kern-VCP überall gleich ist. Eine Migration mit einer \"Lift and Shift\" Denkweise kann auch sehr teuer sein. Optimierung ist der Schlüssel für eine intelligente Cloud-Migration.\n\nUm Kosten zu senken und potenziell viel Geld zu sparen, müssen Sie die richtigen Instanzen für die richtigen Leistungen auswählen. Nicht alle Kerne oder virtuellen CPUs sind gleich. Innerhalb von AWS gibt es den M4, Intels Broadwell, und den M5, der eine Mischung aus Cascade Lake und Skylake ist. Unterschiedliche Arbeitslasten können diese Plattformen sehr unterschiedlich nutzen.\n\n## Leistungsstärke von CoreMark\n\nCoreMark ist eine Anwendung mit einem einzigen Thread, die Registerbewegungen und einfache Additionen für beliebige Anzahl von CPUs testet. Über drei Generationen von Intel-Prozessoren, nämlich Broadwell, Skylake und Cascade Lake, zeigt dieser Test nur sehr geringe Unterschiede zwischen ihnen. Diese Informationen wurden irrtümlicherweise zur Unterstützung von Kundenentscheidungen verwendet, da der Test keine effektive Methode darstellt, um die Unterschiede in den Prozessen selbst zu messen. Es können erhebliche Unterschiede in der Leistungsfähigkeit bei unterschiedlichen Arbeitslasten auftreten.\n\n## Datenbank WL-Leistung\n\nStephen führte digitale Tests an realen Arbeitslasten durch, die sich auf Open-Source-Datenbanken konzentrierten. Die Cascade Lake-Umgebung war für Open-Source drei Mal besser als die Broadwell-Umgebung, was angesichts der Fortschritte bei Geschwindigkeit und On-Chip-Verbesserungen, die von einer geringen Latenz profitieren, Sinn macht. Eine Erhöhung um zwei Generationen ergibt eine zwei- oder dreifach bessere Leistung bei diesen Arbeitslasten.\n\nDie Arbeitsbelastung und der Kern müssen zusammen betrachtet werden. Zum Beispiel könnte ein Kunde dazu verleitet werden, auf eine VCPU umzusteigen, die laut Amazon um 10% günstiger ist. AWS könnte jedoch Kerne auf diese Prozessoren packen, und Sie wechseln nun von einer Umgebung, in der Sie eine von 48 virtuellen Maschinen auf einem Server sind, zu einer, in der es 128 virtuelle Maschinen gibt. Ihr Zugriff auf den Speicher ist begrenzt, und Sie könnten 60% weniger Leistung für 10% Ersparnis erhalten.\n\nOrganisationen glauben oft, dass sie keinen Systemarchitekten mehr benötigen, wenn sie in die Cloud auslagern. Es ist wahr, dass Architekten keine physischen Maschinen mehr aufstellen und stapeln, aber sie tun es jetzt virtuell und müssen verstehen, was verschiedene Instanzen für unterschiedliche Arbeitslasten bieten.\n\nIntel arbeitet an einem Tool, um zu zeigen, welche Workloads am besten auf welchen Instanzen laufen. In der Zwischenzeit hat Intel einige allgemeine Richtlinien und Cloud-Lösungsarchitekten, um Kunden zu beraten.\n\nDarüber hinaus ist es wichtig, umfangreich zu testen, um zu verstehen, wo man Workloads platzieren sollte. Gleichzeitig ist es auch wichtig, zu testen und zu benchmarken, um zu überprüfen, ob man das erhält, was man von einem Anbieter erwartet.\n\n## Datenbank WL Performance - Non NVMe\n\nIntel hat einige Tests an Arbeitslasten auf frisch installierten 64-Kern-Fixed-64-VCPU-Instanzen durchgeführt. Nachdem die anfängliche Anzahl ermittelt wurde, wurde eine weitere Instanz erstellt und der gleiche Test erneut durchgeführt. Die Ergebnisse unterscheiden sich dramatisch für den gleichen Test jedes Mal. Dies könnte darauf zurückzuführen sein, dass möglicherweise eine Mischung von Prozessen angeboten wird, die auf einer bestimmten Stufenebene verfügbar sind, oder es könnten zusätzliche Latenzen und Verzögerungen bei der Anbindung an den Blockspeicher existieren, oder sogar laute Nachbarn. Daher ist es sinnvoll, Tests durchzuführen, um sicherzustellen, dass das System den Erwartungen entspricht.\n\nKurz gesagt, um den größten Nutzen aus der Cloud zu ziehen, müssen Sie sich informieren, die Systeme testen und die verfügbare Hilfe in Anspruch nehmen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steven Holt"],"link":"/episode-EDT45-de","image":"./episodes/edt-45/de/thumbnail.png","lang":"de","summary":"In dieser Episode diskutieren Darren Pulsipher, Chief Solution Architect bei Intel, und Stephen Holt, Cloud Solution Architect bei Intel, Cloud-Optimierung und Studien, die zeigen, dass Kerne unterschiedlich für verschiedene Arbeitslasten arbeiten."},{"id":117,"type":"Episode","title":"Sicherung Ihrer DevOps-Pipeline","tags":["devops","compute","technology","process","devsecops","cybersecurity","zerotrustarchitecture"],"body":"\r\n\r\n## Die Angriffsvektoren\n\nViele Angriffe geschehen auf individueller Ebene. Einige der jüngsten hinterhältigen Angriffe haben sich auf Phishing und soziale Manipulation von Personen innerhalb von DevOps konzentriert. Wir müssen Menschen auf allen Ebenen besser schulen. In einem Fall war es ein Praktikant mit Zugriff auf die Schlüssel, der einer böswilligen Intrige erlag.\n\nEin weiterer Vektor ist die Technologie: traditionelle Denial-of-Service-Angriffe, SQL-Injektionsangriffe oder Pufferüberlaufangriffe. Die neuesten Vorfälle nutzen sowohl Menschen als auch Technologie, um den Prozess anzugreifen. Sie sind heimtückisch, weil sie während des Aufbauprozesses geschehen und daher sehr schwer zu finden sein können. Sie können auch bösartigen Code über Ihre Kunden verbreiten. Um das Vertrauen der Kunden zu stärken, müssen Organisationen eine Strategie haben, um die Lieferkette abzusichern.\n\n## Pipeline-Sicherheit\n\nNicht jede Build-Pipeline ist gleich, aber im Allgemeinen bestehen sie aus vier Stufen mit Umgebungen: Entwicklung, Build, Test und Produktion. Diese Stufen lassen sich je nach Art des entwickelten Produkts leicht in mehrere Stufen unterteilen, aber die Umgebungen verbinden sich mit diesen verschiedenen Stufen.\n\nWir müssen uns den gesamten Prozess ansehen, der sowohl Software, Hardware als auch Abläufe umfasst, und einen anderen Ansatz verfolgen, anstatt uns nur auf die Infrastruktur zu konzentrieren, was die meisten effektiv gemacht haben.\n\n## Host- und Infrastruktursicherheit\n\nAm unteren Ende des Stapels müssen Sie die Hardware in der Entwicklungs- und Build-Umgebung sichern. Die Testumgebung wird etwas anders sein, da Sie möglicherweise Tests mit Injektion von schädlichem Code durchführen möchten. Produktionsumgebungen sind normalerweise gut abgesichert, insbesondere jetzt, da immer mehr Unternehmen Software als Dienstleistung anbieten. In den Produktions- und Build-Umgebungen müssen Sie alle typischen Sicherheitsmaßnahmen durchführen, die Sie in Produktions- oder SaaS-Umgebungen durchführen würden.\n\nIn diesen Umgebungen gibt es drei wesentliche Elemente. Das erste ist die Erkennung. Erkennung und Behebung sind bekannte Sicherheitsmaßnahmen, bei denen Dienstprotokolle verwendet werden, die eine Plattform wie Splunk nutzen, um etwas Ungewöhnliches zu finden. Stellen Sie sicher, dass dies nicht nur in Entwicklungsumgebungen, sondern auch in Bau- und Produktionsumgebungen geschieht.\n\nDenken Sie daran, dass Sie in der Testumgebung mehrere Testumgebungen benötigen, von denen einige auf der Erkennungsseite sicherer sein sollten, da Sie fehlerhaften Code in Ihre Tests einspielen möchten. Verteilen Sie die Sicherheit nicht gleichmäßig auf alle Umgebungen; Sicherheitsprofile können für jede Umgebung unterschiedlich sein.\n\nPrävention ist der zweite Schlüssel. Dies bedeutet, mit Hardware klug umzugehen, sicherzustellen, dass alles ordnungsgemäß aktualisiert ist, die richtigen Sicherheitsupdates zu haben und dies auf automatisierte Weise zu tun. Dies sollte in allen Umgebungen geschehen, einschließlich der Testumgebung, und insbesondere beim Erstellen und in der Produktion.\n\nDer dritte Schlüssel ist Teil der Prävention: der Hardware-Root of Trust. Eine Vertrauenskette kann von der Hardware über die Firmware-Startbereiche bis hin zu den Hypervisoren und Betriebssystemen hergestellt werden. Der Root of Trust kann in die Entwicklungs-, Build-, Test- und Produktionsumgebungen integriert werden. Dies kann sichere Container und sichere virtuelle Maschinen umfassen. Zum Beispiel speichere ich meine Verschlüsselungs- und Hash-Schlüssel gerne in Hardware wie einem TPM-Modul und zusätzlich mit der Secure Guard-Erweiterung von Intel. Selbst wenn jemand Zugriff auf die Maschine erlangt, werden sie diese Schlüssel nicht stehlen können.\n\n## Vertrauenswürdige ausführbare Dateien\n\nDas Einrichten vertrauenswürdiger ausführbarer Dateien ist der nächste Schritt. Das bedeutet, dass Sie Sicherheitsüberprüfungen gegenüber dem eingereichten und erstellten Code durchführen können und dann den Hash-Wert mit diesen Änderungen überprüfen können. Wenn etwas in den Code eingefügt wurde, können Sie es erkennen, da während des Build-Prozesses keine Änderungen am Code vorgenommen werden sollten.\n\nIn einer neuen Phase von Sicherheitsüberprüfungen können Sie statische Analyse am Code oder dynamische Analyse am Code oder Sicherheitsverstöße durchführen. Es gibt einige großartige Tools, die Sie problemlos in Ihre typische DevOps-Pipeline integrieren können, egal ob Sie zum Beispiel Jenkins oder GitHub-Workflows verwenden.\n\nSobald ein ausführbares Programm erstellt wurde, sollte sofort der Hash erstellt werden, und dieser Hash sollte zusammen mit dem Programmversioniert werden; es sollte das ausführbare Programm bleiben, das alle Tests durchläuft und in Produktion geschoben wird. Dieser Hash garantiert, dass nichts manipuliert wurde.\n\n## Attestierte und gesicherte Bilder\n\nNormalerweise gibt es mehrere Code-Repositorys oder ausführbare Dateien zur Verwendung. Der Hash, der zur Bauzeit erstellt wird, befindet sich nun in der Registrierung und Sie können dies bestätigen. Sie können diese in den Abbildern sichern, damit sie nicht geändert werden können. Wenn jemand eine kleine Änderung vornehmen muss, zum Beispiel ein Label oder Metadaten, ist es wichtig, die Änderung nicht vorzunehmen und ihr dieselbe Versionsnummer zuzuweisen. Es ist am besten, den Zyklus erneut durchzugehen, auch wenn dies mehr Zeit in Anspruch nimmt, als manuelle Prozesse mit Ihren Binärdateien herumzufummeln.\n\nJetzt können Sie das gleiche binäre Programm, auf dem Sie alle Tests ausgeführt haben, in die Produktion einspielen. An diesem Punkt ist es ein Fehler, den Quellcode neu zu erstellen. Es ist am besten, die originale Build-Version in das Produktions- oder Gold-Repository hochzuladen. Dieses Repository ist der einzige Ort, von dem aus Bilder, Binärdateien oder VMs abgerufen werden sollten. Alle Bilder sollten notarisiert und beglaubigt werden. Wenn Sie vertrauliche VMs oder Anwendungen haben oder sicherstellen möchten, dass sie nur auf bestimmter Hardware landen, können Sie solche Einschränkungen vornehmen. Sie können die VMs, Container oder sogar Binärdateien verschlüsseln und mit dem Schlüssel sperren, der in Ihren Build- und Productionssystemen gespeichert ist.\n\n## Sicherheitstools einspritzen\n\nSicherheitstools müssen in den Bauprozess integriert werden. Anstatt Open-Source-Sicherheitsbibliotheken zu verwenden oder solche neu zu erstellen, sollten Ihre Sicherheitsingenieure einbezogen werden, damit sie Tools auswählen können, die von den Entwicklungsteams leicht verwendet und wiederverwendet werden können. Ein gutes Beispiel ist die grundlegende Authentifizierung: Benutzeranmeldung. Es sollte eine gemeinsame Bibliothek geben, anstatt dass jede Anwendung ihre eigene hat.\n\nEs ist wichtig, diese Sicherheitsbibliotheken und -tools genauso zu behandeln wie jedes andere Software-Entwicklungsprogramm, das Sie in Ihrer Organisation teilen. Das Teilen wird die Zeit verkürzen und die Sicherheit in Ihrem gesamten Ökosystem erhöhen.\n\n## Errichte einmal, implementiere Sicherheit überall.\n\nSobald Sie Ihre Entwicklungssicherheitsteams etabliert haben, stellen Sie sicher, dass Sie die Richtlinien und Werkzeuge in all Ihre Produkte und Umgebungen integrieren. Heutzutage gibt es großartige Technologien, mit denen Sie mehrere Umgebungen verwalten können. Wenn also eine neue Anwendung gestartet wird, erfolgt dies in einem Sicherheitsprofil mit Ihren eigenen VM- oder Container-Images als Basisbilder, die von den Entwicklungsteams verwendet werden. Durch die Konfiguration von Sicherheit in Ihre VMs oder Container in Ihre Basisbilder erhalten Sie sofort eine Sicherheitskonformität in den verschiedenen Umgebungen. Es besteht auch die Möglichkeit, sich mit Sicherheitswerkzeugen zu integrieren, damit Sie, wenn Sie in der Anwendung etwas Ungewöhnliches feststellen, Sicherheitswerkzeuge benachrichtigen können. Versuchen Sie nicht, Sicherheitswerkzeuge zu erstellen, die hundert Prozent aller Fälle abdecken, denn Sie werden sie nie fertigstellen; setzen Sie auf achtzig Prozent als solide Basis und erstellen Sie sie so, dass Anwendungsentwickler bei Bedarf die letzten zwanzig Prozent in Zusammenarbeit mit Ihrem Sicherheitsteam verbessern können.\n\nEin letzter wichtiger Ratschlag ist es, alles zu automatisieren, was möglich ist, insbesondere in der DevOps-Pipeline, um böswillige Einschleusungen zu verhindern. Schützen Sie Ihre Pipeline, schützen Sie Ihren Prozess.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT46-de","image":"./episodes/edt-46/de/thumbnail.png","lang":"de","summary":"In Teil zwei dieser Folge gibt Darren Pulsipher, Chief Solution Architect bei Intel, praktische Tipps zum Sichern jeder Phase der DevOps-Pipeline, einschließlich dem Schutz von Hardware- und Software-Stacks mit Hardware-Root-of-Trust, Sicherheitsscans, authentifizierten und verschlüsselten Containern/VMs und mehr."},{"id":118,"type":"Episode","title":"Ich freue mich auf 2021.","tags":["covid","remoteworker"],"body":"\r\n\r\nGreg leitet den Vertrieb und das Marketing von Intel in den USA. Intels Kunden sind hauptsächlich die System-OEMs und Intels Vertriebsmitarbeiter arbeiten täglich daran, diese Kunden zu bedienen und ihnen bei der Entwicklung von Systemen rund um Intels Computer- und Speichertechnologien zu helfen. Derzeit gibt es eine beispiellose Nachfrage nach Rechenleistung.\n\nIntels Teams arbeiten mit Softwareunternehmen, Cloud-Anbietern, Fortune-1000-Unternehmen, Schulen und Regierungsbehörden zusammen, um den Zugang zur Berechnung zu erleichtern und Trends zu identifizieren sowie Anwendungsfälle zu entwickeln, die dazu beitragen, geschäftliche Probleme auf eine Weise zu lösen, die die Gesellschaft verbessert. Intel fungiert dann als Trendsetter und identifiziert neue Trends, um sicherzustellen, dass Kunden die richtigen Produkte haben.\n\n## COVID als Katalysator\n\nDerzeit beziehen sich alle Trends auf die COVID-Pandemie. Organisationen verbrachten den Großteil des Jahres 2020 im Krisenmodus, wobei die IT-Abteilungen Technologien einsetzten, um den Geschäftsbetrieb aufrechtzuerhalten und sich dann an die neue Normalität anzupassen. Jetzt gibt es eine Stabilisierung, bei der IT-Abteilungen nach vorne schauen können, anstatt nur auf die aktuelle Krise zu reagieren.\n\nEine Lektion besteht darin, dass die Informationstechnologie viel schneller voranschreiten kann, als wir je gedacht haben. Zum Beispiel ist es erstaunlich, wie schnell beinahe jede Branche auf Fernarbeit umstellen konnte. Dennoch hat COVID als Beschleuniger gewirkt, anstatt als Katalysator. Die meisten Veränderungen waren bereits geplant, aber COVID hat die Zeitpläne verkürzt. Anstatt beispielsweise eine 18-monatige geplante Einführung von Office 365 passierte es innerhalb einer Woche und eineinhalb Tagen, weil es sein musste.\n\nEiner der Hauptgründe für die Geschwindigkeit des Wandels ist, dass ein CIO unmöglich jede Entscheidung treffen konnte, die während der Krise getroffen werden musste, daher wurde die Entscheidungsfindung an die vorderste Front übertragen, um diejenigen zu befähigen, schnell zu handeln und das Beste für das Unternehmen zu tun.\n\n## Hybrides Arbeitsmodell\n\nEine Veränderung, die etwas Zeit in Anspruch nehmen wird, besteht darin, das neue hybride Arbeitsmodell zu lösen. Intel arbeitet beispielsweise an Erwartungen darüber, wie oft Mitarbeiter ins Büro kommen sollten, nachdem die große Mehrheit der Mitarbeiter ein Jahr lang von zu Hause aus gearbeitet hat. Die IT-Abteilungen bereiten sich vor und investieren in Werkzeuge, die eine Zusammenarbeit mit einigen Mitarbeitern im Büro und anderen zu Hause ermöglichen.\n\nOrganisationen können Monate brauchen, um zu wissen, wie ihr Modell aussehen wird. Daher müssen IT-Abteilungen flexibel in ihrer Vorgehensweise sein.\n\n## Mitarbeitererfahrungs-Apps und Automatisierung\n\nOnline-Projekte, die 2020 auf Eis gelegt wurden, kommen nun in dieser neuen Hybrid-Arbeitsumgebung zurück. Mitarbeiter möchten mit ihrem Unternehmen in derselben Art und Weise interagieren, wie sie es als Verbraucher bei Software-as-a-Service tun. Folglich werden viele Apps entwickelt, die sich um die Mitarbeitererfahrung drehen.\n\nDarüber hinaus hat es erhebliche Fortschritte bei der Nutzung von Bots gegeben. Zum Beispiel hat Intel kürzlich einen HR-Bot eingeführt, um die Erfahrung der Mitarbeiter zu verbessern und zu automatisieren. Dieser Bot ermöglicht den Zugriff auf alle HR-Dienstleistungen. Dies führt zu einem steigenden Bedarf an mehr KI und KI-Automatisierung, die durch maschinelles Lernen vorangetrieben werden.\n\n## Reibungsloser, kontaktloser Kundenservice\n\nViele Kunden von Intel richten sich in Regierungs-, Einzelhandels- und Unterhaltungsbranchen auf reibungslose und kontaktlose Kundenservices ein. Eine Branche, die während der Pandemie beschleunigt hat, ist die Telekommunikationsbranche (Telco), was diese reibungslosen Lieferungen sehr erleichtern wird.\n\nDie Kapitalinvestition zum Aufbau der Telekommunikationsinfrastruktur und des 5G-Netzes unterstützt jetzt mehr IoT-Geräte und Ferngeräte, die vor der Pandemie Jahre gebraucht hätten, um Wirklichkeit zu werden.\n\n## AI und Föderiertes Lernen\n\nIntel hat einen enormen Anstieg bei der Zusammenarbeit mit KI und geschützten Daten in Bereichen wie medizinischer Forschung und den Finanzmärkten festgestellt. Mit den neuen Xeon CPUs der dritten Generation von Intel gibt es sichere Rechensektoren (SGX) und geschützten Speicher, die außerhalb des Systems nicht zugänglich sind. Die Implementierungsfälle betreffen hauptsächlich KI und Föderiertes Lernen, bei dem die Daten von Benutzern und Unternehmen globale Modelle trainieren können, jedoch nicht in einem zentralen Repository geteilt werden. Durch KI, das Konzept des föderierten Lernens und Intels SGX können diese Daten geschützt werden. Datenschutz- und Regulierungshürden können beseitigt werden. Zum Beispiel können Daten eines Krankenhauses oder eines einzelnen Patienten geschützt werden, aber dennoch verwendet werden, um ein globales Modell mit erheblichen Vorteilen zu trainieren.\n\n## RPA (Robotic Process Automation) übersetzen\n\nViele Unternehmen finden Möglichkeiten, Aufgaben zu automatisieren, in einigen Fällen sogar banale Aufgaben, um ihre Mitarbeiter von höherwertigen Projekten entlasten zu können. Dieser Trend hat in den letzten sechs bis acht Monaten einen enormen Aufschwung erlebt und der Markt ist stark gewachsen. In den letzten fünf Jahren haben die RPA-Märkte über zwei Milliarden Dollar an VC-Finanzierung erhalten, vor allem von den Finanzmärkten New York Citys. Neben RPA gibt es auch viele Automatisierungsframeworks, die Menschen zur Bereitstellung von Infrastruktur in ihren Rechenzentren und nahtlos auch in der Cloud nutzen.\n\n## Edge Computing\n\nIntel hat begonnen, Referenzarchitekturen zu entwickeln, um Unternehmen beim Aufbau ihrer Edge-Compute zu unterstützen. Die wichtige Wissenschaft hierbei ist die Verbindung des Edge-Computings bis zur Cloud-Infrastruktur und der Aufbau eines Hardware- und Software-Stacks, eines Kontrollpanels und der Automatisierung. Dies ist ein weiterer Bereich mit enormen Investitionen.\n\n## Vorwärtsbewegen\n\n2020 war ein Jahr des Chaos, beispielloser Anpassung und beschleunigter Veränderungen. Nun, in einem stabileren Jahr 2021, können Unternehmen auf den resultierenden Lektionen und Trends aufbauen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Ernst"],"link":"/episode-EDT47-de","image":"./episodes/edt-47/de/thumbnail.png","lang":"de","summary":"Greg führt den US-Vertrieb und das Marketing von Intel. Intels Kunden sind hauptsächlich System-OEMs, und Intels Vertriebsmitarbeiter arbeiten täglich daran, diese Konten zu betreuen und ihnen beim Aufbau von Systemen rund um Intels Rechen- und Speichertechnologien zu helfen. Derzeit herrscht eine beispiellose Nachfrage nach Rechenleistung. Intels Teams arbeiten mit Softwareunternehmen, Cloud-Anbietern, Fortune-1000-Unternehmen, Schulen und Regierungsbehörden zusammen, um den Zugang zu Rechenleistung zu ermöglichen und Trends zu identifizieren und Anwendungsfälle zu entwickeln, die Unternehmensprobleme auf eine Weise lösen, die die Gesellschaft verbessert. Intel gilt damit als Trendsetter und identifiziert neue Trends, um sicherzustellen, dass Kunden die richtigen Produkte erhalten."},{"id":119,"type":"Episode","title":"VAST Data überprüft","tags":["dataarchitecture","data","optane","technology","vastdata"],"body":"\r\n\r\nVAST Data ist in den letzten sechs Monaten schnell gewachsen, mit einem wachsenden Team und beeindruckenden Verkäufen wie etwa 70 Petabyte Flash-Speicher an die Bundesregierung. Neben allgemeinem Dateiaustausch mit ihren All-Flash-Speichersystemen haben sie interessante Anwendungsfälle in Bereichen wie Next-Gen-Sequenzierung, konfokalen Mikroskopie und Windkanalsimulationsaufgaben.\n\nDie Plattform des Unternehmens ermöglicht das Lösungen von zufälligen Lese-Problemen, da alle Daten auf Flash gespeichert sind, ohne die exorbitanten Kosten; die Plattform verarbeitet eine hohe Datenvolumen mit geringer Latenz.\n\n## VAST-Technologie und Intel Optane-Laufwerke\n\nVAST verwendet Intel Optane-Laufwerke, um Lese- / Schreibparität und andere Vorteile zu erreichen. Da Optane persistenter Speicher ist, treten keine Cache-Kohärenzprobleme auf und es besteht kein Bedarf an DRAM, was einen großen architektonischen Vorteil darstellt, um Ausfälle oder Datenverluste zu vermeiden. Die Gesamtkosten pro Benutzer werden ebenfalls gesenkt, da Sie keine großen Maschinen mit viel Speicher für die Dateispeicherung kaufen müssen, wenn Sie mehr Geschwindigkeit möchten. Diese Stufe wird eliminiert.\n\nSkalierbarkeit und Flexibilität sind zwei weitere Vorteile, da Sie mehr Datenspeicher (D-Boxen) hinzufügen können, ohne die Leistung zu beeinträchtigen, und sie sind sogar vollständig kompatibel mit mehreren Generationen von Flash-Technologie. Es gibt keinen einzigen Fehlerpunkt und es gibt keine theoretische Grenze; VAST hat bis zu 100 Petabyte getestet.\n\nSie können die Leistung verbessern, indem Sie Client-Boxen (C-Boxen) hinzufügen, unabhängig von D-Boxen. Die Kapazität mit Flash-Speicher ist im Grunde kostenlos, aber Sie müssen diesen Flash-Speicher mit der CPU verbinden. Die C-Boxen sind vollständig zustandslos, sodass Sie sie dynamisch vergrößern oder verkleinern können. VAST garantiert eine hohe Verfügbarkeit, da Sie vollen Zugriff auf jeden PCP auf der Hinterseite haben.\n\nJe nach Leistungsanforderungen des Kunden können unterschiedliche Anzahlen von D-Boxen und C-Boxen vorhanden sein. Zum Beispiel benötigt ein Kunde möglicherweise keine zusätzliche Leistung, sondern mehr Kapazität, sodass der Cluster beispielsweise aus 11 C-Boxen und 37 D-Boxen bestehen kann.\n\nWenn Sie eine C-Box und eine D-Box haben, haben Sie etwa 40 Gigabyte pro Sekunde Bandbreite, was das Basismodell ist. Jede zusätzliche D-Box erhöht die Bandbreite um weitere 40 Gigabyte. Eine C-Box sättigt nicht alle IOPS, daher können Sie durch Hinzufügen einer weiteren C-Box 350.000 IOPS aus einer Box erhalten. Da Flash durch die CPU begrenzt ist, ermöglicht das Hinzufügen von mehr CPUs eine bessere Skalierung.\n\n## Neues Geschäftsmodell für Lagerung\n\nDas neue Produkt von VAST Data, Gemini, ermöglicht es Kunden, die hohe Leistung benötigen, aber nur wenig Kapazität benötigen, nur die Menge an Hardware zu lizenzieren, die sie von ihrem Vertragspartner benötigen. Dies führt zu erheblichen Kostenersparnissen. Kunden können die Kapazität nach Bedarf steigern und bezahlen, anstatt von Anfang an Kosten für zukünftige Kapazitätsanforderungen zu verursachen.\n\nAus der Sicht der Betriebsausgaben ergibt dies für viele Unternehmen Sinn. Zum Beispiel kostet ein 30 Petabyte All-Flash-System, das eine Bandbreite von anderthalb Terabyte hat, weniger als S3 für seltenen Zugriff von Amazon. Das ist nur für ein Jahr; wenn man sich die Art und Weise ansieht, wie Amazon abrechnet, ist es sogar weniger als Glacier, pro Gigabyte pro Monat. Und es gibt die Möglichkeit, später wieder zu einem Investitionsausgaben-Modell zurückzukehren, falls das sinnvoller ist.\n\nEs ist wichtig zu beachten, dass VAST ein Gerät präsentiert, und keinen Speicherservice, was zu einem Unterstützungsalbtraum führen kann, aufgrund all der verschiedenen Hardware- und Firmware-Varianten, die unterstützt werden, welche Laufwerke unterstützt werden, welche Verbindungen usw.... was zu einer verrückten Menge an Komplexität führt. VAST versendet immer noch dieselben Boxen und Server; es handelt sich nur um ein anderes Geschäftsmodell, um Flexibilität beim Verbrauch von Speicher zu ermöglichen.\n\nEinige interessante Bereiche, in denen VAST nützlich sein könnte, sind die medizinische Forschung wie beispielsweise die digitale Pathologie mit einer astronomischen Menge an Daten, die für Forscher oft schwer zugänglich sind, weil sie auf billigen und langsamen Speichermedien liegen. Auch die präzisionsmedizinische Behandlung basierend auf dem genetischen Profil einer Person wäre eine Möglichkeit. Darüber hinaus könnte die Anwendung von künstlicher Intelligenz auf Paketaufzeichnungen dabei helfen, Angriffe von fortgeschrittenen Gegnern vorherzusagen.\n\nEin weiteres vielversprechendes Anwendungsszenario ist ein Kunde, der alle seine Daten in einem VAST All-Flash-Speichersystem anstelle der Cloud hat, aber seine Berechnungen vollständig in der Cloud durchführt und möglicherweise nur die Ergebnisse zurückbringt. Die Ergebnisse sind normalerweise sehr klein, vielleicht nur ein paar Bytes Daten, und das Abrufen ist kostengünstig. Dieser Modelltyp könnte beispielsweise einen Mehrwert aus der Analyse von alten Daten erzeugen, die derzeit einfach im Speicher liegen und zu teuer sind, um sie zu verschieben.\n\nEs gibt viel Flexibilität, um einen hybriden oder Multi-Cloud-Ansatz durchzuführen, bei dem Sie ein zentrales, lokal betriebenes Speichersystem haben, auf das über mehrere Cloud-Anbieter zugegriffen werden kann.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Randy Hayes"],"link":"/episode-EDT48-de","image":"./episodes/edt-48/de/thumbnail.jpg","lang":"de","summary":"In dieser Episode führt Darren ein Gespräch mit Randy Hayes, der nun als Vertriebsleiter für den öffentlichen Sektor bei VAST Data tätig ist. Es sind sechs Monate seit ihrem letzten Gespräch vergangen und Darren möchte nun wissen, wie es ihnen in der Branche ergeht, was es Neues bei VAST gibt und interessante Anwendungsfälle. Ihr neues Produkt Gemini bietet ein anderes Speichermodell."},{"id":120,"type":"Episode","title":"Remote Arbeit Vor, Während und Nach der Pandemie","tags":["covid","remoteworker"],"body":"\r\n\r\nSie haben vier Bereiche identifiziert, die ihnen geholfen haben, nicht nur in dieser ungewöhnlichen Zeit zu überleben, sondern auch erfolgreich zu sein: Antizipieren Sie die Bedürfnisse Ihres Managements, Kommunizieren Sie, als ob Ihr Job davon abhängen würde (weil er es sicher tut), Setzen Sie sich für andere ein, indem Sie die Fahne für andere schwenken, Blockieren Sie den Kalender!\n\n## Antizipieren Sie die Bedürfnisse Ihres Managers.\n\nErraten Sie, was Ihr Manager benötigt, und haben Sie die Informationen jederzeit griffbereit.\n\n* Verkaufsberichte, Projektstatus und Kundenstatus sollten proaktiv erstellt werden.\n\n## Kommuniziere, als ob dein Job davon abhängt (weil das tut er ganz bestimmt).\n\nFüllen Sie nicht den E-Mail-Posteingang anderer mit nutzlosen Informationen.\n\n* Stellen Sie Ihre Bitte am Anfang der E-Mail. Fügen Sie dann zusätzliche Informationen hinzu.\n\nGeben Sie Ihrem Team eine Zusammenfassung dessen, was Sie tun, mit hilfreichen Informationen.\n\n## Schwenke deine Fahne, indem du die Fahne für andere schwenkst.\n\nEine Schlüsselkomponente für den Erfolg in jedem Unternehmen besteht darin, sichtbare, greifbare Erfolge vorweisen zu können.\n\nSie können nicht immer den Kopf senken und arbeiten und hoffen, dass es für sich selbst spricht.\n\nErkenne die anderen an, die dir geholfen haben, den Vertrag abzuschließen oder das Projekt zu beenden.\n\n## Blockieren! Das! Kalender!\n\nSetzen Sie Grenzen für die Familienzeit und wann die Arbeit endet und beginnt.\n\nBlockieren Sie Zeit für \"Sie\" Zeit ab. Sie benötigen Zeit, um Ihrem Gehirn Ruhe zu gönnen.\n\nBitte legen Sie 5-10 Minuten Pause zwischen den Meetings ein.\n\nZeit, um Ihren Posteingang zu bearbeiten\n\nZeit am Ende des Tages, um deinen Tag zu verarbeiten\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rachel Dreikosen"],"link":"/episode-EDT49-de","image":"./episodes/edt-49/de/thumbnail.png","lang":"de","summary":"In dieser Episode spricht Darren mit Rachel Dreikosen, Business Development Manager im öffentlichen Sektor bei Intel, darüber, wie COVID-19 ihr Work-Life-Balance beeinflusst hat und warum sie einen Blog gestartet hat, um anderen weiblichen technischen Vertriebsprofis zu helfen."},{"id":121,"type":"Episode","title":"Infrastruktur-Reifemodell","tags":["infrastructurematurity","compute","technology","sdi","cloud","multicloud"],"body":"\r\n\r\nDigitale Transformation kann schwierig sein. Viele Organisationen werden gelähmt, wenn sie versuchen, all die Technologien zu entschlüsseln, die ihnen jeden Tag entgegengeworfen werden. Auf dieser Reise ist es wichtig, eine Karte zu haben, wie bei jeder anderen Reise auch. In dieser Episode erklärt Darren das \"Infrastruktur-Reifemodell\", um Organisationen dabei zu helfen herauszufinden, wo sie sich in ihrer digitalen Transformation befinden, wohin sie als nächstes gehen können und wie sie dorthin gelangen können.\n\n## Standardisierte Phase\n\nFast jede Organisation befindet sich in dieser standardisierten Phase oder darüber hinaus. Das Verständnis dieser Phase gibt uns eine Grundlage, um über die gemeinsamen Dinge zu sprechen, die wir in Organisationen sehen.\n\n## Grundlegend\n\nDer Grundlevel ist dort, wo wir unkoordinierte Infrastruktur und reaktionsgesteuertes Management finden. Dies ist besonders häufig in Start-ups der Fall, wo eine Organisation Dienstleistungen oder Hardware nach Bedarf kauft, als Reaktion auf Ereignisse und nicht als Teil eines Masterplans. Hier sehen wir auch maßgeschneiderte Soft- und Hardware-Stapel. Eine umfangreiche individuelle IT ist ebenfalls Teil dieser Reifestufe.\n\n## Zentralisiert.\n\nAuf diesem Level ist die Infrastruktur zentralisiert. Organisationen kaufen Kapazität anstelle von speziell entwickelter Hardware. Die Einkäufe erfolgen nicht mehr reaktiv, sondern im Rahmen eines längerfristigen Plans. In der Regel verfügt die Organisation über gemeinsame Serverkonfigurationen, um kosteneffizient in größerem Umfang zu kaufen und eine höhere Zuverlässigkeit zu gewährleisten. IT ist stärker zentralisiert und spezialisiert, zum Beispiel in den Bereichen Netzwerk, Speicher, Rechenleistung und Sicherheit.\n\n## Vereinfacht\n\nAuf vereinfachtem Niveau ist die Infrastruktur stärker konsolidiert. Unterschiedliche Abteilungen können Geräte wie Server, Speicher und Netzwerke gemeinsam nutzen. Organisationen erzielen eine bessere Leistung, wenn sie von alten zentralen und komplizierten Methoden zu neuen zentralisierten und vereinfachten Methoden übergehen. Die Betriebskosten gehen durch gesteigerte Effizienz zurück.\n\n## Virtualisierte Phase\n\nDie meisten Organisationen sind heute zumindest teilweise in der virtualisierten Phase. Hier finden wir standardisierte Konfigurationen und private Clouds.\n\nServervirtualisierung bietet die Flexibilität, Anwendungen auf verschiedenen Maschinen auszuführen und sie problemlos zwischen den Maschinen zu verschieben.\n\nDie Speichervirtualisierung ist eine neuere Entwicklung, die ähnliche Vorteile wie die Servervirtualisierung bietet: Flexibilität für dynamischere Entscheidungen. Der Speicher kann jetzt das gesamte Ökosystem abdecken. Speicher kann für eine bestimmte Anwendung bereitgestellt werden und dieser Speicher sowie die Anwendung können innerhalb der Infrastruktur bewegt werden.\n\nNetzwerkvirtualisierung, die neueste Ergänzung, wurde insbesondere von Internetdienstanbietern angenommen. Sie haben aufgrund der Effizienz, dynamische Änderungen im Netzwerk ferngesteuert anstatt von zuvor zweckgebundenen Hardware vornehmen zu können, eine enorme Investition getätigt.\n\nAll diese Virtualisierungstechnologien sind in Angeboten im Bereich der privaten Cloud miteinander verbunden, sowohl kommerziell als auch in Open Source, oft an einem einfach zu verwaltenden Ort. Organisationen gehen in diese Phase über und verändern und konsolidieren ihre IT von einem vertikalen Fokus hin zu einem Blick auf alles (ANMERKUNG: Können wir dies als horizontalen Fokus bezeichnen?). Dabei werden Systeme, Anwendungen und Dienste eher betont als Rechenleistung, Speicher und Netzwerk.\n\n## Automatisierte Phase\n\nIn der automatisierten Phase können Organisationen die Infrastruktur automatisch bereitstellen, was Zeit spart, indem Schichten von Personen und Berechtigungen ausgeschaltet werden. Mit Hilfe von Self-Service-Portalen erhält ein Ingenieur oder Softwareentwickler sofort die benötigten Ressourcen. Diese automatische Bereitstellung ermöglicht auch eine grundlegende Automatisierung von Updates, virtuellen Maschinen, Sicherheit und Compliance. Die IT befindet sich nun weniger im reaktiven Modus, sondern im proaktiven Modus. Weitere wertvolle Bestandteile dieser Phase sind zum Beispiel das automatische Hochfahren der Infrastruktur für Anwendungen in der Produktion und selbstheilende Anwendungen.\n\n## Orchesterphase\n\nMit zunehmender Komplexität der Automatisierung wird eine orchestrierte Phase notwendig. Automatisierung bedeutet, Handlungen auf eine Maschine anzuwenden, während Orchestrierung die Koordination der Handlungen auf mehreren Maschinen oder sogar Modalitäten (Speicher, Rechenleistung, Netzwerk) für eine Anwendung ist. Darüber hinaus können automatisierte Anwendungsstapel mehrere Anwendungen auf verschiedenen Servern bereitstellen. In dieser Phase orchestrieren Organisationen auch hybride Dienste, zum Beispiel Ressourcenmanagement über öffentliche und private Clouds hinweg. Die Vorteile dieser Orchestrierung sind gesunkene Betriebskosten, gesunkene Investitionskosten und verkürzte Bereitstellungszeit.\n\n## Echtzeit (SLA-verwaltete) Phase\n\nIn einer Echtzeitphase befinden sich Organisationen in einer Service-Level-Agreement-Infrastruktur. Anstatt Anwendungen verwenden die Organisationen Dienste, die Wert liefern und alles miteinander verbinden. Gemeinsam genutzte Dienste laufen über mehrere Hybrid-Clouds und sogar über veraltete Infrastrukturen. Durch das Zusammenführen der Hybrid-Infrastruktur, eine an Richtlinien orientierte Orchestrierung und eine dienstbasierte Orchestrierung wird die Infrastruktur, das Datenmanagement und die Dienste optimiert.\n\n## Schlussfolgerung\n\nEs ist üblich, dass verschiedene Teile Ihrer Organisation sich in unterschiedlichen Phasen befinden. Einige Gruppen werden weiter fortgeschritten sein als andere; das ist gesund. Kleine Vorreitergruppen können scheitern, ohne die gesamte Organisation zu beeinflussen, oder sie können einen Durchbruch erzielen und den Rest der Organisation mit nach oben ziehen. Nur sehr wenige Organisationen haben sich in die höchste Phase auf der Karte bewegt. Die Hauptsache dabei ist, sich nicht entmutigen zu lassen, sondern die Karte als Leitfaden zu nutzen, um herauszufinden, wo Sie sich befinden und welche nächsten Schritte für Ihre Organisation sein könnten.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT5-de","image":"./episodes/edt-5/de/thumbnail.png","lang":"de","summary":"Die digitale Transformation kann schwierig sein. Viele Organisationen werden gelähmt, wenn sie versuchen, all die Technologien zu entschlüsseln, die ihnen jeden Tag entgegengeworfen werden. Auf dieser Reise, wie bei jeder anderen, ist es wichtig, eine Karte zu haben. In dieser Episode erläutert Darren das Infrastruktur-Reifemodell, um Organisationen dabei zu helfen, herauszufinden, wo sie sich in ihrer digitalen Transformation befinden, wohin sie als nächstes gehen können und wie sie dorthin gelangen können."},{"id":122,"type":"Episode","title":"Verständnis für Mitarbeiter-Burnout","tags":["employeeburnout","remotelearning","remoteworker","people","process","compute"],"body":"\r\n\r\nUzairs beruflicher Hintergrund begann im Verkauf, der Verkaufserfahrung am Point of Sale und entwickelte sich dann weiter in Richtung Produktentwicklung. Über zehn Jahre lang arbeitete er an einer Wecker-App, die auf unterschiedliche personalisierte Weise dazu diente, Menschen motiviert und fokussiert aufzuwecken. Obwohl dieses Projekt letztendlich scheiterte, hat Uzair eine Menge über menschliche Motivation gelernt. Vor ein paar Jahren, als er mit Schülern im Mittel- und Oberstufenalter arbeitete, bemerkte er einen starken Mangel an Motivation und Fokus in den ersten Stunden des Tages, der sich anscheinend zunehmend verschlechterte. Aufgrund seiner beruflichen Erfahrung und seines eigenen Wellness-Wachstums sah er eine Möglichkeit, einen Einfluss zu haben, indem er die District Zero App entwickelte.\n\n## Schulgesundheit für Schüler der Klassen 1 bis 12\n\nDistrict Zero's Mission ist es, Schülerinnen und Schüler der Klassen K-12 mit umfassenden Fähigkeiten als Lernende auszustatten. Das bedeutet, ihnen dabei zu helfen, schwierige Emotionen zu bewältigen, um ihren Fokus und ihre Motivation wiederherzustellen.\n\nDie App verwendet agile Technologie und die Kraft der sentimentalen Analyse der natürlichen Sprachverarbeitung, um in großem Maßstab Auslöser und Ängste über eine Umfrage zu erfassen. Sobald ein Schüler die Umfrage abgeschlossen hat, kann die App Inhalte und Ressourcen wie ein Video oder ein Spiel vorschlagen, um bei spezifischen Problemen zu helfen. Sie verfügt auch über ein Berichtssystem für Lehrer und Administratoren.\n\nDerzeit tragen Lehrer und Verwalter eine hohe Belastung, um ihren Schülern insbesondere in der COVID-Umgebung bei ihrem Wohlbefinden zu helfen. Möglicherweise haben sie ein Google-Formular mit Google Sheets verbunden und verwenden dann Strg+F, um nach negativen Wörtern und Schlüsselphrasen zu suchen. Die App reduziert diese Belastung und vermeidet die negativen Auswirkungen von Lehrer- und Verwalter-Negativität. Das System deckt die Hindernisse und Schmerzpunkte der Schüler auf und hilft bei der Lösung durch schnelle Abhilfen und Ressourcen. Wenn die Schüler weitere Unterstützung benötigen, werden sie an die richtige Person, wie zum Beispiel einen Berater, weitergeleitet und der Lehrer bleibt informiert.\n\nTraditionell wird ein Schüler, der Angst oder Frustration zeigt, zum Berater oder Sozialarbeiter gebracht, wenn das Problem nicht mehr beherrschbar ist. Doch der Schüler muss nicht von null auf hundert gehen. Es gibt einen Mittelweg, bei dem das Problem angegangen werden kann, bevor es zu einem Kampf oder Flucht kommt. Die App kann helfen, dass sich ein Problem nicht verschlimmert und Probleme rechtzeitig erkannt werden, bevor es zu spät ist.\n\nDie Einführung der App-Pilotphase an Schulen im Raum Chicago erfolgte im letzten August während der COVID-Pandemie, weshalb sie besonders geeignet war, Schülern zu helfen, die die persönliche Verbindung zu ihren Lehrern verloren hatten und mit sozialem und emotionalem Lernen zu kämpfen hatten. Ein entscheidender Erfolgsfaktor beim sozialen und emotionalen Lernen besteht darin, dass alle in der Gemeinschaft – Lehrer, Verwaltungsmitarbeiter, Schulleiter, Eltern und Steuerzahler – involviert und empathisch sind. Die Empathie und Verbindungen dürfen sich nicht nur auf die Schule beschränken.\n\n## Mitarbeiter Ausgebrannt\n\nWährend COVID zeigten die Arbeitnehmer eine Steigerung der Produktivität, aber es besteht nun das Risiko des Mitarbeiter-Burnouts. Ob es der Stress eines Gesundheitsdienstleisters oder eines Fernarbeiters ist, der ein Ungleichgewicht zwischen Arbeit und Privatleben hat, eine Vielzahl von Problemen machen dies zu einer zentralen Angelegenheit für Unternehmen. Die Prinzipien der District Zero K-12 App können auf Arbeitnehmer angewendet werden, um Burnout zu lindern.\n\nDer Distrikt Null begann, mit der App in ihrem eigenen Arbeitsplatz zu experimentieren, und das führte zu schwierigen Gesprächen über Prioritäten und dem Verständnis, was die echten Bedürfnisse der Mitarbeiter sind. Ähnlich wie bei der K-12-Anwendung kann das System die alltäglichen Schwierigkeiten der Mitarbeiter identifizieren und die Lücke schließen, indem es Unterstützung bietet, bevor Probleme eskalieren.\n\nDer Distrikt Null hofft, seine Technologie so weiterentwickeln zu können, dass sie für viele verschiedene Bereiche von Nutzen ist: Unternehmen, Unternehmensgesundheitswesen und sogar Regierungsbehörden, für den Einsatz in Bereichen wie der Suizidprävention von Kriegsveteranen.\n\nDieses neue Werkzeug und der Einblick in die Unterstützung der Gesundheit von Schülern und Mitarbeitern kommen genau zur richtigen Zeit, da sich der Stress und das Ausbrennen aufgrund von COVID verstärken und wir eine neue Phase der Wiedereröffnung und Anpassung beginnen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Uzair Hussain"],"link":"/episode-EDT50-de","image":"./episodes/edt-50/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect bei Intel, spricht mit Uzair Hussain, CEO von District Zero, darüber, wie die App des Unternehmens, die die psychische Gesundheit von Schülern der Klassen K-12 unterstützt, dazu beitragen kann, ein Ausbrennen der Mitarbeiter zu verhindern."},{"id":123,"type":"Episode","title":"Praktische Anwendungsfälle für Optane Persistent Memory","tags":["dataarchitecture","memverge","pmem","optane","technology","data"],"body":"\r\n\r\nMemVerge hat im September 2020 die erste Version ihrer Software ausgeliefert, und trotz der Pandemie, die die Aufklärung der Kunden über die neue Technologie verlangsamt hat, hat sich ein gutes Muster von Anwendungsfällen herausgebildet.\n\n## Kostensenkung für Cloud-Service-Anbieter\n\nDie wichtigste Kennzahl für einen Cloud-Service-Anbieter ist, wie viele virtuelle Maschinen (VMs) sie ihren Kunden zur Verfügung stellen können und zu welchen Kosten. Die Größe des Arbeitsspeichers auf den Servern wird zum Engpass dafür, wie viele VMs pro Server zugewiesen werden können und begrenzt, wie niedrig ihr Preis pro VM sein kann.\n\nDie MemVerge-Software mit Optane liefert eine größere Menge an Speicher pro Server und ermöglicht die Zuweisung einer größeren Anzahl von virtuellen Maschinen (VMs), wodurch die Kosten pro VM gesenkt und die Wettbewerbsfähigkeit von Cloud-Service-Anbietern erhöht wird. Die Kosten pro VM könnten um das Dreifache günstiger sein.\n\n## Zuverlässigkeit bei großen Speicherdatenbanken\n\nFinanzkunden wie Börsen, Banken und Investmentfonds verwenden umfangreiche Speicherdatenbanken und In-Memory-Anwendungen. Neben der Erhöhung ihres Speicherplatzes, um mehr Instanzen pro Server zu haben, löst MemVerge Verfügbarkeitsprobleme von Speicherdatenbanken. Wenn die Daten nicht ständig auf dem Speichermedium gespeichert werden, sondern nur im Speicher liegen, gehen alle Intraday-Daten bei einem Absturz verloren. Das ist katastrophal. Selbst wenn alle Transaktionen erfasst wurden, muss das Protokoll erneut abgespielt werden, um die Datenbank wiederherzustellen, was viele Minuten oder sogar Stunden dauern kann.\n\nMemVerge bietet einen neuen Datendienst an, der In-Memory-Snapshots hat. Er speichert den Datenbankzustand auf Optane dauerhaft und das ist wesentlich schneller als auf Speichermedien zu speichern. Falls es zu einem Absturz kommt, hast du den letzten auf persistentem Speicher erfassten Snapshot und kannst dich davon wiederherstellen. Die Wiederherstellung dauert nur eine Minute oder zwei, daher ist es eine Verbesserung um das 60- bis 100-fache.\n\n## Reduzierung der genomischen Sequenzierung durch Speicherabbildung\n\nIm Bereich der Genomik erhöht die MemVerge-Software in Kombination mit Optane die Produktivität exponentiell. In einem mehrstufigen Datenanalyse-Workflow bedeutet mehr Speicher mehr Parallelität der Pipeline und Verarbeitung, sodass der gesamte Prozess schneller ist. Auch die Snapshot-Funktion ist hier praktisch. Wenn eine Organisation beispielsweise Krebs- oder COVID-Forschung betreibt und DNA- oder RNA-Sequenzen erstellen muss, muss sie etwa 50 Verarbeitungsschritte durchlaufen. Jeder Schritt kann Stunden dauern, und es ist notwendig, den Zustand der Zwischenberechnungsergebnisse zu überprüfen - zum einen für das erneute Ausführen oder Reproduzieren der Ergebnisse und zum anderen für den Vergleich der Ergebnisse, wenn einige Daten verändert werden. Die Zwischenstände werden auf dem Speicher gespeichert und das dauert zwischen fünf und 30 Minuten. In vielen Fällen kann dies zeitaufwändiger sein als die eigentliche Berechnung. Wenn eine Aufgabe also 24 Stunden dauert, könnte man acht Stunden für die Berechnung und 16 Stunden nur für diese IO-Aufgaben verwenden, um diese Zwischenstände zu speichern.\n\nAnstatt IO durchzuführen, verwendet MemVerge nach jedem Schritt einen Schnappschuss und speichert ihn im Optane Persistent Memory. Anstatt 16 Stunden IO dauert dieser Prozess nur eine Minute. Es ist die neue Art IO durchzuführen; man muss keine Serialisierung oder Deserialisierung durchführen, um eine Datei zu öffnen, zu lesen, zu schreiben, usw. Alles was man tun muss, ist einen Schnappschuss zu machen.\n\nObwohl dies viel Speicher benötigt, ist der Speicher mit MemVerge größer als zuvor und wird sich weiter verbessern, während Intel innoviert. Zwei weitere Funktionen helfen bei diesem Problem. Erstens werden regelmäßig Schnappschüsse erstellt, ohne vollständige Kopien des Speicherzustands zu erzeugen. Es handelt sich nur um Änderungsseiten, sodass der zusätzliche Speicherverbrauch minimiert wird. Zweitens kann MemVerge bis zu 256 Ebenen von Schnappschüssen im Speicher behalten, während Sie gleichzeitig diese Schnappschüsse auf Speicherserver oder Ihre eigenen Speichersysteme exportieren können. Dies erfolgt ohne Beeinträchtigung oder Auswirkung auf Ihre laufende Anwendung.\n\nIm Grunde genommen erstellst du einen Speicher-DVR, denn anstatt deine Anwendung nur vorwärts laufen zu lassen, kannst du auch nahezu sofort rückwärts laufen. Es ist eine neue Erfahrung.\n\nDie Genomik ist nur das erste Beispiel für viele Arbeitslasten, die von dieser Technologie profitieren könnten.\n\nDa MemVerge ein Startup ist, konzentrieren sie sich eng auf die drei Bereiche Cloud-Service-Anbieter, finanzielle Anwendungen mit großem Speicher, Genomik und verwandte Data-Science-Pipeline-Aufgaben. Aber all diese Anwendungsfälle beweisen die Kraft der Kombination aus Optane Persistent Memory und MemVerge Software.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT51-de","image":"./episodes/edt-51/de/thumbnail.jpg","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect bei Intel, spricht mit Charles Fan, CEO von MemVerge, über Anwendungsfälle ihrer Software, die Intels Optane Persistent Memory auf innovative Weise nutzt und Engpässe zwischen Speicher und Datenspeicher beseitigt."},{"id":124,"type":"Episode","title":"Telemetrie in der Cloud","tags":["multicloud","optimization","workloadplacement"],"body":"\r\n\r\n## Vorteile von Cloud-Lösungsarchitekten (CSAs)\n\nDie Rolle des CSA ist derzeit heiß begehrt in der Branche, und Intel hat ein Team von CSAs eingestellt, um Mehrwert für ihre Kunden zu bieten. CSAs können Kunden dabei helfen, den Ansatz des einfachen Kopierens und Verschiebens zu vermeiden, der zu unnötig hohen Kosten führt. Diese neuen CSAs bringen eine externe Perspektive mit und vernetzen sich mit der größeren CSA-Gemeinschaft, um Probleme zu lösen.\n\n## Phasen der Telemetrie\n\nTelemetrie in der Cloud sollte nicht nur auf den Cloud-Service-Anbieter beschränkt sein. Zum Beispiel hatte Amazon Web Services (AWS) im vergangenen November einen großen Ausfall an der Ostküste, und ihre Tools haben nicht richtig Bericht erstattet, so dass viele Kunden keine Ahnung hatten, dass sie offline waren. Organisationen benötigen ihre eigene Telemetrie für die Überwachung.\n\nDie erste Phase ist keine Überwachung in der Cloud. Die zweite Phase beinhaltet die Offenlegung von Telemetrie, bei der Sie verstehen, was Sie erhalten, wie zum Beispiel CPU, Speicher und Netzwerk, aber es ist auf oberflächlicher Ebene. Die nächste Phase ist Überwachung und Handlungen, bei der Sie Benachrichtigungen über Verlangsamungen, Transaktionsraten, Reaktionsraten usw. erhalten. Die darauf folgende Phase liegt weiter unten im Stapel, wo die moderne intelligente Cloud Kontrolle ausübt und Vorhersagen für Maßnahmen trifft. Zuletzt kommt die vollständige Automatisierung. Hier werden Dinge von selbst betrieben, sie hören zu, reagieren und informieren dann im Nachhinein.\n\nEin Beispiel für vollständige Automatisierung wäre ein Rechenzentrum, das ein HVAC-System steuert, indem es die Temperatur in verschiedenen Teilen eines Gebäudes erfasst und reduziert oder erhöht. Bei Arbeitslasten kann eine intelligente Cloud die Arbeitslasten verschieben, um den Wert von Kapazität und Leistung zu maximieren.\n\n## Aktuelle Ansätze und Begrenzungen\n\nOft vernachlässigen Kunden von Cloud-Dienstleistern die Automatisierung auf der Infrastrukturebene und vergessen die Auslastungsebene. Kunden benötigen Informationen, die über die Grundlagen hinausgehen; sie benötigen detaillierte und umfangreiche Telemetrie, um zu wissen, was wirklich passiert. Es ist ein Fehler, sich darauf zu verlassen, dass der Cloud-Dienstleister Ihre Interessen in Bezug auf Telemetrie im Blick hat, und dies kann zu hohen Kosten führen.\n\nEinige Cloud-Service-Anbieter öffnen sich weiter. Derzeit ist AWS mit den von ihnen bereitgestellten Telemetriedaten ein Game Changer. Hoffentlich werden andere dem Beispiel folgen.\n\n## Lassen Sie uns die richtige Telemetrie sammeln...\n\nDas Auswählen der richtigen Instanzen ist wichtig. Nicht alle Kerne sind gleich; verschiedene Kerne werden für verschiedene Zwecke verwendet, und es ist wichtig zu verstehen, was sie sind, um die beste Leistung und den besten Preis zu erhalten.\n\nWelche Art von Informationen können Sie sammeln? Intels Produktgestaltung umfasst Leistungsmessgeräte (PMUs). Diese sind Untereinheiten und liefern Informationen über Transaktionen, Verzögerungen, Latenzzeiten und Engpässe. Es gibt drei verschiedene Lager bei PMUs: Kern, außerhalb des Kerns und uncore. Diese PMUs sammeln Informationen über CPI, Auslastung, Frequenz und TMAM. All diese Daten sind über AWS verfügbar.\n\nMit Metriken und echter Telemetrie kann man seine Arbeitslasten optimieren. Man könnte beispielsweise vergleichen, wie eine Arbeitslast in einem eigenen Rechenzentrum im Vergleich zu AWS läuft und Metriken verwenden, um herauszufinden, auf welcher Plattform die Arbeitslasten besser funktionieren würden.\n\n## IT Relevanz\n\nDas Schichten von Telemetrie mit Benchmarking ist eine ultimative Lösung. Mit dem Benchmarking können Sie Ihre Ausgabe kennen und mit der Telemetrie können Sie CPI, Auslastung und Frequenz betrachten und haben das volle Dashboard, um zu sehen, was passiert. Sie möchten das Gleiche in der Cloud tun, anstatt nur Arbeitslasten auf eine scheinbar günstigere Instanz zu übertragen.\n\nIT-Experten sollten keine Angst haben, ihren Job zu verlieren, weil alles in die Cloud verlagert wird. Stattdessen sollten sie ihre Fähigkeiten nutzen, um Benchmarking-Telemetrie zu erlernen, anstatt eine \"Lift and Shift\"-Mentalität zu haben. Die Beherrschung der Cloud-Nutzung beinhaltet auch die Verwendung cloudnativer Funktionen wie Kubernetes und Container. Telemetrie funktioniert auch in diesen Bereichen. Mit Intels C-Advisor erhält man umfassende Telemetrie-Daten wie Kern- und Nebenkern-Daten aus den Containern.\n\nObwohl es ein wenig einschüchternd erscheint, wenn man sich alles ansieht, was in der Cloud möglich ist, ist es am besten, klein anzufangen. Schauen Sie sich die richtigen Anwendungen basierend auf Risiko an. Katalogisieren Sie Ihre Apps, betrachten Sie die Einstufung der Anwendungen und verschieben Sie sie dann in größeren Teilen mit ähnlichen Funktionen und Apps in die Cloud. Wenn Sie sich neue Dienste ansehen und neue Anwendungen lernen, betrachten Sie die dahinter liegende Architektur und stellen Sie die richtigen Fragen, um ein besser informierter technischer Architekt zu sein.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT52-de","image":"./episodes/edt-52/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect bei Intel, spricht mit Josh Hilliker, Direktor der Cloud Solution Architects bei Intel, über die Verwendung von Telemetrie in der Cloud, um den Wert und die Effizienz zu maximieren."},{"id":125,"type":"Episode","title":"DevOps mit Geschwindigkeit unter Reduzierung von Risiken","tags":["devops","compute","technology","process","devsecops","cybersecurity","multicloud"],"body":"\r\n\r\nDie drei wichtigsten Bereiche, die beim Software-Lieferprozess im Gleichgewicht gehalten werden müssen, sind Geschwindigkeit, Risiko und Qualität. Die meisten können ein oder zwei dieser Dinge bewältigen, aber das Hinzufügen des Dritten kann schwierig werden. Zum Beispiel können Sie möglicherweise aufgrund eines großartigen CD-Systems mit hoher Geschwindigkeit liefern, aber sobald Sie Compliance- und Richtlinienprüfungen einführen, stehen Sie vor einer Herausforderung. Sollten Sie anhalten und diese Dinge überprüfen oder vielleicht einige Ergebnisse von einem anderen Tool einbinden? Plötzlich leidet Ihre Geschwindigkeit.\n\nOpsMx arbeitet daran, Menschen dabei zu helfen, diese Entscheidungspunkte zu automatisieren. Automatisierung ist das Wichtigste, um die Geschwindigkeit zu erhöhen, während die anderen beiden Säulen, Risiko und Qualität, verbessert werden.\n\n## OpsMx Lösung: Vollautomatische Softwareauslieferung\n\nDie Automatisierung funktioniert als Datenaggregation, indem sie Ergebnisse aus den verschiedenen Tools in der Tool-Kette abruft und dann einen Mechanismus wie einen Policy-Manager hat, der erwartete Ergebnisse auf Kontrollkästchen von Tools wie BlackDuck liefert. Wenn es richtig aussieht, stoppt der Prozess nicht wie derzeit für einen menschlichen Check.\n\n## Schnellere Anwendungsbereitstellung erhöht die Ausfallraten.\n\nDie Geschwindigkeit der Anwendungsbereitstellung hat sich von wöchentlich oder monatlich auf stündlich erhöht, was den Druck zur schnellen Produktion verstärkt. Wenn Sie jedoch versuchen, die Geschwindigkeit zu erhöhen, könnten Sie möglicherweise etwas Kontrolle über die Qualität verlieren und möglicherweise einige Risikoprüfungen überspringen. Das Endergebnis ist, dass je schneller Sie versuchen, voranzukommen, desto eher kann es zu einem Ausfall kommen.\n\n## Reibungspunkte und Problemräume\n\nMit strategischer Nutzung von Automatisierung haben Sie die Fähigkeit, die Geschwindigkeit zu erhöhen, ohne Risiken einzugehen. Durch kontinuierliche Überprüfung kann ein dreistündiger oder vierstündiger Prozess auf fünf oder zehn Sekunden verkürzt werden. Anstatt menschliche Arbeitskraft zu nutzen, um Protokolle nach abweichendem Verhalten zu durchsuchen, das möglicherweise ein binäres Bestehen/Nicht-Bestehen übergeben hat, kann die kontinuierliche Überprüfung dies automatisch tun und Menschen nur in Ausnahmefällen eingreifen lassen.\n\n## Was bedeutet Automatisierung in diesem Zusammenhang?\n\nDas nächste Stück verwendet die von dem Sicherheits- und Compliance-Team geforderten Werkzeuge, wie zum Beispiel BlackDuck. Durch die Automatisierung muss der Pipeline nicht gestoppt werden, um die Ergebnisse von jemandem interpretieren zu lassen, da dies automatisch geschieht.\n\nDie anderen Teile stellen eine Politik auf einer höheren Ebene dar, wenn Sie in einer regulierten Branche mit mehr Kontrollen tätig sind oder etwas so Einfaches wie ein Einzelhändler, der zum Beispiel keine neue Software zwei Wochen vor Weihnachten veröffentlichen möchte. Diese Kontrollen können automatisiert werden, um die Notwendigkeit einer menschlichen Genehmigung zu beseitigen und den Ablauf fortzusetzen, sofern es keine Ausnahmen gibt. Die Idee besteht darin, so viele menschliche Barrieren wie möglich zu entfernen, um den Ablauf zu ermöglichen.\n\nDer Hauptkomplex von Steuer- und Richtlinienautomatisierung ist wiederverwendbar für verschiedene Pipelines; Sie müssen nur solche anpassen oder erstellen, die für Ihre Freigabezyklen spezifische Kriterien erfüllen.\n\n## OpsMx Lösung: Vollautomatische Software-Lieferung\n\nOpsMx ermöglicht Ihnen die Möglichkeit, diese Pipelines zu erstellen und die Automatisierung der Log-Analyse durchzuführen. Spinnaker ist das derzeitige Werkzeug, aber OpsMx hat die Vision, so ​​cd-agnostisch wie möglich zu sein und die Verwendung aller cd-Tools wie Jenkins und Microsoft Azure zu ermöglichen. OpsMx nutzt den cd-Teil für die Automatisierung, sitzt jedoch darüber, um bei automatisierten Entscheidungen zu helfen.\n\nEin wachsender Teil des Tools besteht aus maschinellem Lernen, um zu helfen, zu verstehen, was die Basislinie für eine bestimmte Anwendung im Vergleich zu einem anomalen Verhalten ist. Es gibt auch ein überwachtes Lernmodell, bei dem ein DevOps-Experte, ein Ingenieur oder ein Produktbesitzer das anomale Verhalten im Kontext der Anwendung spezifizieren kann.\n\nDas System ermöglicht auch Prüfbarkeit. Jede Änderung oder Ausnahme wird dokumentiert. Es gibt eine vollständige Audit-Spur von allem, was passiert, von der Genehmigung einer Ausnahme durch wen bis hin zu welchem Artefakt bereitgestellt wurde und welches Basisbild verwendet wurde. Ob es in der Cloud oder vor Ort ausgeführt wird, können Sie die gesamte Pipeline als eine Einheit sehen.\n\nEin weiterer Aspekt der von OpsMx bereitgestellten Sichtbarkeit ist eine Art Karte, die zeigt, was wo bereitgestellt ist, wie zum Beispiel die Reihe von Microservices, die derzeit in der QA sind, was sich in bestimmten Phasen befindet oder was sich in der Produktion befindet. Für jede dieser Dinge können Sie dann genauer nachschauen und eine historische Ansicht erhalten. Sie können auf eine beliebige Freigabe klicken und alle Audit-Informationen sind sofort verfügbar.\n\nDarüber hinaus arbeitet OpsMx auch an der Effektivität der erfassten Artefakte, um sicherzustellen, dass die Lieferkette oder Linie transparent ist.\n\nOpsMx verwendet jetzt eine agentenbasierte Technologie, die mit Ressourcen interagieren kann, die beispielsweise hinter Ihrer Firewall sitzen. Der Agent fungiert als Proxy für die Intelligenzschicht, damit Daten dort gesammelt werden können. Es gibt keine Sorgen darüber, Firewall-Ports zu öffnen oder freizulegen. Es funktioniert auf die gleiche Weise mit Cloud-Anbietern: Der Agent kann innerhalb des VPC bereitgestellt werden, und Sie müssen keine Schlüssel und geheimen Informationen mehr in einer Cloud-basierten Anwendung riskieren. Der Agent fungiert einfach als Proxy, sodass das autorisierte Stück immer innerhalb des VPC bleibt und die Art und Weise, wie Informationen gesammelt werden, sichert.\n\nMit dieser neuen Ausrichtung steht die DevOps-Branche insgesamt vor einer Erschütterung in Bezug auf Sicherheit und Nachvollziehbarkeit.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT53-de","image":"./episodes/edt-53/de/thumbnail.png","lang":"de","summary":"In dieser Folge spricht Robert Boule, Head of Solution Engineering bei OpsMx, mit Darren darüber, wie man die Geschwindigkeit im DevOps-Prozess verbessern kann, ohne das Risiko zu erhöhen. Die drei Schlüsselbereiche zur Balance in der Softwarebereitstellung sind Geschwindigkeit, Risiko und Qualität. Die meisten können ein oder zwei dieser Dinge managen, aber das Hinzufügen des Dritten kann schwierig werden. Zum Beispiel können Sie möglicherweise aufgrund Ihres großartigen CD-Systems mit Geschwindigkeit liefern, aber sobald Sie Compliance- und Policy-Checks einführen, stehen Sie vor einer Herausforderung. Sollen Sie anhalten und diese Dinge überprüfen oder vielleicht einige Ergebnisse von einem anderen Tool einbeziehen? Plötzlich leidet Ihre Geschwindigkeit."},{"id":126,"type":"Episode","title":"Menschen und Prozesse in der digitalen Transformation","tags":["multicloud","organizationalchange","change","people","hsbc","cio"],"body":"\r\n\r\nAnn begann ihre Karriere als Programmiererin, wechselte dann in das Programmmanagement und verfügt über Erfahrung in Unternehmenserwerb und -verkauf. Sie war der Globale Leiter der Daten bei HSBC und wurde im September 2016 zum Chief Information Officer ernannt. Im Laufe ihrer Karriere leitete sie bedeutende organisatorische Transformationsprogramme, darunter neue Hypothekensysteme und neue Kernbankensysteme.\n\nEinige Herausforderungen bei diesen Transformationen bestanden darin, während der Bewertungen die richtigen Fragen zu stellen. In einem Hypotheken-Transformationsprogramm stellte sie zunächst Fragen zu den Finanzen: Überschreiten wir das Budget? Sind wir im Plan? Müssen wir um zusätzliche Mittel bitten? Für das Führungsteam fragte sie, ob die richtigen Personen mit den richtigen Fähigkeiten vorhanden waren, um die Veränderungen umzusetzen. Sie ging auch auf das große Ganze ein: Was glaubt das Unternehmen? Was sind die Geschäftsziele? Verstehen sie klar, warum das Transformationsprogramm notwendig ist und was wir erreichen wollen? Sie betrachtete auch den Stand des Programms anhand von Kennzahlen und Leistungsindikatoren.\n\nFür diese Transformation hatten sie bereits ein etabliertes und gestartetes Programm, aber Ann kam als so etwas wie eine interne Unternehmensberaterin hinzu. Das Programm wurde von der Geschäftsführung vorangetrieben, aber sie mussten die Technologie nutzen, um diese Geschäftsziele und -objektive zu erreichen. Die Technologie und die Ziele mussten vollständig aufeinander abgestimmt sein.\n\nIn der letzten Transformation, die Ann leitete, fast drei Jahre in ein fünfjähriges Transformationsprogramm, übernahm sie die Position der Programmdirektorin sowohl für den Geschäftsbereich als auch für den Technologiebereich. Vier CIOs waren ihr untergeordnet, jeder von ihnen zugewiesen zu einem Geschäftsbereich: Einzelhandel, Handel, Vermögensverwaltung und Investmentbanking. Ann brachte sie mit ihren Geschäftspartnern in Kontakt und sie trafen sich gemeinsam zu Besprechungen, um sicherzustellen, dass sie aufeinander abgestimmt blieben, da sie dasselbe Ziel erreichen wollten.\n\nObwohl es scheinen würde, dass diese Art von Partnerschaften gängige Geschäftspraxis wäre, stellte Ann fest, dass eine ihrer größten Herausforderungen als CIO darin bestand, das Unternehmen zu vereinheitlichen. Bei etwas so Großem und Komplexem wie der Änderung eines Kernbankensystems, einer neuen mobilen App oder einer neuen Web-Schnittstelle beispielsweise führte Ann mehrmals täglich Treffen mit den wichtigsten Akteuren durch, da es wahrscheinlich 15 verschiedene Arbeitsströme gab und Integration der Schlüsselaspekt war.\n\nVon außen kommend und organisatorische Veränderungen vornehmen zu müssen, ist eine schwierige Position. Menschen sind nervös wegen Veränderungen, besonders wenn sie ihrem früheren Anführer vertraut haben. Mit der Zeit fand Ann jedoch heraus, dass die Menschen mitkamen, weil sie wussten, dass das Ziel darin bestand, das Geschäft zu verbessern, und sie sich auf diese Reise eingelassen haben. Eine Möglichkeit, wie Ann dies erreicht hat, war es, sich mit den verschiedenen Teams auf den Boden zu setzen anstatt im Büro zu bleiben. Sie lernte die Menschen kennen und es wurde für sie normal, mit ihnen zusammen zu sein und Gespräche zu führen sowie Probleme anzusprechen. Sie erhielt rohe Informationen von den Menschen, die die Arbeit erledigten, anstatt gefilterte Informationen durch das Management.\n\nEine Sache, die Führungskräfte oft vergessen, ist, dass die Menschen in den Teams erfolgreich sein wollen, und eine persönliche Note wie zum Beispiel das gemeinsame Arbeiten mit ihnen geht weit.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ann Madea"],"link":"/episode-EDT54-de","image":"./episodes/edt-54/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect bei Intel, bittet seine Gesprächspartnerin Ann Madea, ehemalige CIO von HSBC, darüber nachzudenken, wie sie den Prozess großer transformationsreicher Veränderungen in Organisationen vorangetrieben hat."},{"id":127,"type":"Episode","title":"Kommunikation während der Transformation","tags":["communication","organizationalchange","people","process","hsbc","cio"],"body":"\r\n\r\nWenn CIOs Lösungen bereitstellen, beinhalten sie oft neue Prozesse und wahrscheinlich auch organisatorische Veränderungen. In Ann's jüngstem Transformationsprogramm ging sie den Weg der Cloud. Ann stellte einen Leiter für Cloud ein und implementierte eine Multi-Cloud-Strategie mit Google, AWS und Azure. Diese große Veränderung rief Besorgnis hervor, vor allem bei denen, die schon lange mit der Infrastruktur gearbeitet hatten.\n\nUm Angst abzubauen und die Motivation zu steigern, ermutigte Ann die Menschen nachdrücklich dazu, Zertifizierungen für Cloud, Google, AWS usw. zu erlangen. Sie nahm ihr Team mit auf Reisen, um Treffen mit Google in New York abzuhalten, wo sie die Energie, Atmosphäre und Zusammenarbeit vor Ort erleben konnten. Als immer mehr Mitarbeiter zertifiziert wurden, wuchs die Begeisterung für das Training und die Veränderung.\n\nDer neue Leiter der Cloud hätte \"Google Days\", an denen Mitarbeiter ihre Anwendungsfälle präsentierten und wie sie verschiedene Produkte nutzen. Dies taten sie auch mit Amazon und Azure. Dies half den Mitarbeitern, das Ausmaß dessen zu verstehen, was sie tun konnten, baute Begeisterung auf und ermöglichte eine aktive Teilnahme.\n\nAnns Schlüssel für die Kommunikation mit dem CIO inmitten einer Transformation ist Vertrauen, aber Überprüfung. Drei oder vier Monate nach Beginn der Transformation wurde Ann aufgefordert, sich auf Bewertungen in Bereichen wie Budgets, Überprüfungsteams und der Wiederherstellung von Beziehungen zum Vorstand und den Aufsichtsbehörden zu konzentrieren. Sie entschied sich jedoch für eine gründlichere Analyse und stellte fest, dass es eigentlich keinen Plan gab, der zeigte, wie das Programm umgesetzt werden sollte. Das führte zu einer Neubewertung des Führungsteams, um das Programm voranzutreiben.\n\nEin CIO, der eine Transformation umsetzt, muss bereit sein, sich die Hände schmutzig zu machen. Vertraue alles, doch überprüfe es mit mehreren Interviews und Informationsquellen; eine oder zwei Quellen reichen nicht aus. Anschließend glaubt Ann, dass es notwendig ist, mit Transparenz über die Probleme zu kommunizieren, um Unterstützung von denjenigen zu erhalten, die in der Lage sind, sie zu bieten.\n\nEin Vorteil von Transparenz besteht darin, dass die Personen, die Ihnen Informationen liefern, wie z.B. die Programmierer, darauf vertrauen, dass Sie diese nicht gegen sie verwenden werden. Das Team von Ann weiß, dass sie sie nicht überrascht, und im Gegenzug überrascht sie ihre Vorgesetzten nicht. Beispielsweise informiert sie sie, sobald sie mehr darüber weiß, über ein Problem im Rechenzentrum und hält sie auf dem Laufenden.\n\nMitarbeiter möchten wissen, wie sie zum Erfolg des Programms beitragen, nicht nur dass sie diese fünf Programme programmieren müssen. Ein Weg, den Ann dies erreicht hat, war durch regelmäßige Versammlungen. Sie ging auch durch die Abteilungen und hielt wöchentliche, informelle Meetings ab, in denen die Mitarbeiter Fragen stellen konnten. Jedes Mal, wenn Ann eine große Versammlung abhielt, bei der Entscheidungen getroffen wurden, versammelte sie das gesamte Team – sowohl die Geschäfts- als auch die Technologieseite – und erklärte die Entscheidungen und ihre Auswirkungen. Wenn die Mitarbeiter wissen, dass eine wichtige Versammlung stattgefunden hat, aber im Dunkeln gelassen werden, beginnt das Gerüchteküche zu brodeln.\n\nMit der aktuellen Telearbeit ist es laut Ann auch wichtig, sich kurz mit Einzelpersonen abzustimmen. Kontinuierliche Absprachen führen zu besseren Beziehungen und verringern die Angst vor Gesprächen mit Vorgesetzten. Chefs müssen verstehen, was die Teams motiviert und worum sich die Mitarbeiter sorgen. Sie haben Familien zu versorgen, Kinder im Studium oder vielleicht Familienmitglieder mit gesundheitlichen Problemen.\n\nNatürlich kann nicht alles mit jedem besprochen werden, wie beispielsweise Personalabbau oder Beförderungen, aber Ann organisiert gerne Treffen mit Vorgesetzten, die \"Personentreffen\" genannt werden, um diese sensiblen Themen zu besprechen. Diese Treffen können den Führungskräften auch helfen, ihr Personal und deren Ziele besser zu verstehen. Es ist wichtig, nicht nur von den Führungskräften zu hören, sondern auch von den Mitarbeitern auf niedrigeren Ebenen, da sie näher an den täglichen Vorgängen sind.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ann Madea"],"link":"/episode-EDT55-de","image":"./episodes/edt-55/de/thumbnail.png","lang":"de","summary":"In dieser Folge, Teil zwei von zwei, sprechen Darren und Gast Ann Madea, ehemalige CIO von HSBC, über Kommunikation während organisatorischer Transformation. Wenn CIOs Lösungen implementieren, beinhalten diese neue Prozesse und höchstwahrscheinlich auch organisatorische Veränderungen. In Anns jüngstem Transformationsprogramm gingen sie mit der Cloud voran. Ann stellte einen Leiter für die Cloud ein und implementierte eine Multi-Cloud-Strategie mit Google, AWS und Azure. Diese bedeutende Veränderung führte zu Bedenken, insbesondere bei jenen, die bereits lange mit der Infrastruktur gearbeitet hatten."},{"id":128,"type":"Episode","title":"Cloud-Broker-Organisationen","tags":["cloudbroker","cloud","compute","technology","people","process","multicloud","organizationalchange"],"body":"\r\n\r\nDave und Kevin gründeten vor etwa sieben Jahren das Cloud-Brokerage-Team von Intel. Die Mitarbeiter von Intel nutzten die öffentliche Cloud und es ähnelte dem Wilden Westen, da die Leute einfach ihre Kreditkarten für den Zugang verwendeten. Sicherheit war ein Problem und die Ausgaben gerieten außer Kontrolle. Um das Chaos einzudämmen, gründeten sie eine Cloud-Broker-Organisation.\n\nEin zentralisierter Ansatz war notwendig, um Kosten zu kontrollieren, Unternehmensverträge mit Lieferanten zu organisieren und zu erstellen, sowie die Abrechnung über eine Organisation einzurichten. Anstatt einzelne Kreditkarten zu verwenden oder eigene Bestellungen zu erstellen, verwenden die Geschäftsgruppen von Intel eine zentrale Bestellung über die IT, die dann von den verschiedenen Gruppen abgerechnet wird. Insgesamt spart es Geld, da Intel nun über gemeinsame Einkaufsleistung verfügt, einschließlich fortgeschrittenerer kostenreduzierender Möglichkeiten wie dem Kauf von reservierter Kapazität anstelle von Zahlung von Bedarfspreisen.\n\nEs dauerte eine Weile, bis alle Mitglieder der Organisation an Bord waren, wobei einige immer noch ihre Kreditkarten benutzten, um Konten zu eröffnen. Um dem entgegenzuwirken, haben einige der Cloud-Anbieter Intel einen Bericht über Konten zur Verfügung gestellt, die mit Intel-E-Mail-Adressen eröffnet wurden. Anstatt diese \"Ausbrüche\" nur als Verstoß gegen die Richtlinien anzugehen, war es eine Gelegenheit, sie über die Vorteile der Verwendung des zentralen Intel-Kontos zu informieren: bereits vorhandene Sicherheitsstandards, Unterstützung für Unternehmen, Schulungen und Kosteneffizienz.\n\nNeben diesen Vorteilen hat Intel auch ein Cloud Center of Excellence errichtet, ein community-basiertes Forum, dem sie die Leute beitreten lassen, sobald sie ihre Cloud-Konten erhalten. Der Erfolg des Forums wurde von der Basis aus erzielt und bietet Informationen und Rückmeldungen für Mitglieder.\n\nViele Entwickler und andere Personen, die die Cloud nutzen, möchten diese einfach benutzen, ohne sich Gedanken über die Sicherheit oder Kosten machen zu müssen. Das Cloud-Brokerage-Team ermöglicht ihnen genau das. Ein Vergleich wäre, dass die IT den Entwickler in einen Sandkasten mit allen Spielzeugen steckt, ihm aber nicht erlaubt, Sand außerhalb des Sandkastens zu werfen oder mit den Spielzeugen außerhalb zu spielen. Dadurch erhält die Entwicklergemeinschaft sichere und geschützte Konten und Zugriffsmöglichkeiten, wann immer sie diese benötigt.\n\nEs gibt Hunderte von verfügbaren Diensten bei öffentlichen Cloud-Anbietern, und ständig werden neue Dienste und Möglichkeiten veröffentlicht. Es ist schwierig für Geschäftsgruppen, Expertise in all diesen Diensten zu haben oder aufrechtzuerhalten. Ein zentrales Cloud-Broker-Team, das sich auf öffentliche Clouds konzentriert und über die neuesten Dienste auf dem Laufenden bleibt, kann Anleitung und Wissen darüber bieten, wo unterschiedliche Arbeitslasten platziert werden sollten. Der Schlüssel zur Vermittlung besteht darin, dass die Menschen zu einem zentralen Kanal kommen und zu den richtigen Diensten umgeleitet werden.\n\nEs ist nicht nur nützlich, ein dediziertes Cloud Brokerage Team zu haben, sondern auch Personen im Team, die sich auf bestimmte Cloud-Anbieter konzentrieren. Zum Beispiel konzentrierte sich bei Intel Kevin auf AWS und Dave auf Azure, als sie die kritische Masse erreichten und die Cloud-Anbieter reiften und begannen, eine Vielzahl von Diensten anzubieten, um tiefer in jede einzelne einzutauchen.\n\nDie Verwendung mehrerer Cloud-Angebote anstelle nur eines war eine natürliche Entscheidung, da Geschäftsgruppen mit Vorlieben kamen und unterschiedliche Arbeitslasten sich besser mit verschiedenen Anbietern bewältigen lassen. Die Unterschiede der Anbieter waren früher deutlicher. Jetzt herrscht ein ausgeglicheneres Spielfeld.\n\nEin Cloud-Broker ist nicht nur jemand in einer technischen Position, sondern ein Allrounder. Dave und Kevin wurden Experten für alles, was mit der Cloud zu tun hat, wie Sicherheit und Netzwerke, und sie bildeten diese Teams weiter, als sie ihren Bereich von der On-Premises- zur Public Cloud erweiterten. Durch ein zentrales Cloud-Team konnten die anderen Organisationen von Intel weiterlernen, wachsen und expandieren. Das Team erhielt den Spitznamen \"Der Klebstoff\" aufgrund ihrer zentralen und vielseitigen Rolle.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Kevin Bleckmann","Dave Shrestha"],"link":"/episode-EDT56-de","image":"./episodes/edt-56/de/thumbnail.png","lang":"de","summary":"In dieser Episode, Teil eins von zwei, sprechen Darren und die Intel Cloud Solution Architects Dave Shrestha und Kevin Bleckman über die Bedeutung einer Cloud-Broker-Organisation. Dave und Kevin gründeten vor etwa sieben Jahren das Cloud-Brokerage-Team von Intel. Die Mitarbeiter von Intel nutzten die öffentliche Cloud, und es war wie im Wilden Westen, in dem die Leute einfach ihre Kreditkarten für den Zugriff nutzen. Sicherheit war ein Problem und die Ausgaben gerieten außer Kontrolle. Um das Chaos einzudämmen, haben sie eine Cloud-Broker-Organisation geschaffen."},{"id":129,"type":"Episode","title":"Cloud-Broker-Organisation Teil 2","tags":["cloudbroker","change","people","process","compute","organizationalchange","multicloud"],"body":"\r\n\r\n# Überlegungen zur Auswahl der Cloud\n\nSieben Jahre zuvor, als Dave und Kevin zum ersten Mal das Cloud-Broker-Team von Intel gründeten, gab es bedeutende Unterschiede zwischen Cloud-Service-Anbietern, wie regionale Verfügbarkeit. Doch mittlerweile, mit zunehmender Reife, sind die Unterschiede geringer geworden. Einige Anbieter haben sich auf bestimmte Bereiche wie KI und maschinelles Lernen oder umfangreiche verwaltete Datenbankdienste spezialisiert, und es gibt Kosteneffektoren, doch im Großen und Ganzen sind sie eher ähnlich als unterschiedlich. Cloud-Anbieter werden generell zu Dienstprogrammen, je reifer sie werden.\n\nDie Sicherheit ist gut und ziemlich standardisiert bei den Cloud-Anbietern. Früher hat Intel externe Produkte von Drittanbietern zur Sicherheitsverwaltung bei Cloud-Diensten verwendet. Die Cloud-Anbieter haben nun einen Großteil davon in ihre Plattformen integriert, und Intel ist dazu übergegangen, mehr integrierte Komponenten zu nutzen. Es gibt noch viel Arbeit in diesem Bereich zu erledigen. In einigen Fällen müssen Sie immer noch die Sicherheit selbst übernehmen, aber die Anbieter sind auf dem richtigen Weg. Die Verwendung integrierter Werkzeuge ist für neue Unternehmen keine einfache Option, da sie nicht vollständig verwaltet werden, aber sobald Sie über das erforderliche Know-how und die erforderliche Reife verfügen, werden Ihnen zumindest die Werkzeuge zur Verfügung gestellt.\n\nDas Kostenmanagement ist immer noch eine Kunst; viele Leute sind schockiert über ihre erste Cloud-Rechnung. Obwohl es eingebaute Werkzeuge gibt, die helfen werden, sowie Berater und öffentliche Cloud-Rechner, gibt es einige versteckte Kosten. Ingress- und Egress-Gebühren sind wahrscheinlich die größte Überraschung, gefolgt von Überbereitstellung. Intel hat einen Cloud-Optimizer, der von Densify betrieben wird und dabei hilft, überbereitete Ressourcen zu identifizieren und durch detaillierte Analysen empfiehlt, welche Instanzgrößen und -familien am optimalsten sind.\n\nEinige Arbeitslasten sollten on-prem bleiben, insbesondere für große Unternehmen wie Intel, die ein großes on-prem Rechenzentrum haben. Das Cloud-Broker-Team verfügt über ein internes Tool namens Entscheidungsrahmenwerkzeug, das dabei helfen kann, die richtige Arbeitslast am richtigen Ort zu platzieren, und manchmal ist on-prem die beste Lösung.\n\n# Arbeitsbelastungsbetrachtung für die Platzierung in der Public Cloud\n\nOhne ein Hilfsmittel zur Feststellung gibt es fünf Bereiche zu berücksichtigen.\n\n*Sicherheit: Stellen Sie sicher, dass Sie die Identität der Anbieter, Verschlüsselung, Übereinstimmung und Single-Sign-On-Optionen kennenlernen. Dies sind die gleichen Punkte, mit denen Sie normalerweise vor Ort umgehen würden. Anstatt separate Tools zu verwenden, ist es am besten, Tools zu verwenden, die sowohl vor Ort als auch in der öffentlichen Cloud funktionieren, damit Ihre Teams sich nicht separate Tools aneignen müssen.\n\n* Privatsphäre\n\nArbeitsbelastungsstabilität\n\n*Datenaffinität/Datenschwere: Stellen Sie sicher, dass Ihr Cloud-Anbieter in den Regionen verfügbar ist, die Sie benötigen. Einige Cloud-Anbieter haben in allen ihren Regionen Verfügbarkeitszonen oder Rechenzentren, andere nicht. Darüber hinaus macht es bei einer Arbeitslast, die eine Verbindung zu vielen lokalen Systemen herstellt, keinen Sinn, diese in der öffentlichen Cloud zu platzieren.\n\nKosten: Es gibt immer noch Unterschiede in den Kosten zwischen den öffentlichen Cloud-Anbietern, insbesondere bei großen Recheninstanzen. Berücksichtigen Sie auch Ausgabe-Kosten.\n\n## Wie man den Cloud-Broker-Service verkauft\n\nDer Einstieg in den Cloud-Broker-Service kann für einige schwierig sein - insbesondere für Entwickler, die es gewohnt sind, die Kontrolle zu haben und beliebige Instanzen zu erstellen. Es können auch kleinere Anfangsschwierigkeiten auftreten. Als die Mitarbeiter von Intel ihre Konten zuerst erhalten haben, fühlten sie sich unsicher und das Informationssicherheits-Team hat ihnen Nachrichten geschickt, wenn sie etwas falsch gemacht haben. Diese wussten jedoch oft nicht genau, was sie falsch gemacht haben oder wie sie es beheben konnten. Mit der Weiterentwicklung und Reife des Services wurden diese Probleme allmählich gelöst, da das Team unterstützte und automatische Lösungen einsetzte.\n\nDie Leute stellten fest, dass der Service letztendlich vorteilhaft war, weil er eine schnelle und einfache Möglichkeit bot, in die öffentliche Cloud zu gelangen, mit all der Unterstützung, die sie benötigten. Nach einer gewissen Zeit begann sich der Service von selbst zu verkaufen.\n\nEiner der wichtigsten Dienste, die das Team anbietet, ist die Funktion als Vermittler zwischen Intels Geschäftspartnern von Endkunden und Informationssicherheitsgruppen, um übermäßig aggressive Sicherheitsrichtlinien zu verhindern, die lästig sind und zu vielen Tickets führen. Das Team sorgt für das richtige Gleichgewicht, das Entwicklern genügend Freiheit ermöglicht, innerhalb eines gesicherten Rahmens zu arbeiten und dennoch die Sicherheitsanforderungen zu erfüllen. Die Vermittler fungieren im Grunde genommen als Mediatoren zwischen Informationssicherheit und Entwicklern.\n\nDas Cloud-Broker-Team bietet auch Schulungen an, indem es die Cloud-Anbieter einbezieht, um Workshops abzuhalten. Dieser Vorteil ergab sich auch aus dem Cloud Center of Excellence, wo die Mitarbeiter Schulungen in spezifischen Bereichen angefragt haben und das Team dann mit den Anbietern diesbezüglich verhandeln würde.\n\nNeben externen Lieferanten nutzte Intel interne Teams, um zu demonstrieren, was sie mit der öffentlichen Cloud erreicht hatten, sodass andere Teams dieses Wissen nutzen konnten.\n\nCloud-Broker-Teams innerhalb von Unternehmen können eine Vielzahl von Dienstleistungen und Vorteilen bieten, wie zum Beispiel Sicherheit und Kosteneffizienz, insbesondere da sich die öffentlichen Cloud-Services weiterentwickeln, erweitern und für den Betrieb immer wichtiger werden.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Dave Shrestha","Kevin Bleckmann"],"link":"/episode-EDT57-de","image":"./episodes/edt-57/de/thumbnail.png","lang":"de","summary":"Im zweiten Teil von zwei sprechen Darren Pulsipher, Chief Solution Architect, und die Intel Cloud Solution Architects Dave Shrestha und Kevin Bleckman über die Vorteile und Dienstleistungen einer Cloud-Broker-Organisation."},{"id":130,"type":"Episode","title":"Verständnis von Problemen bei Edge bis zum Rechenzentrum","tags":["data","edge","compute","edgemere","edgetocloud","cybersecurity","technology","process"],"body":"\r\n\r\n## Mission Integration can be translated to German as \"Mission Integration\" or \"Missionseingliederung\".\n\nEs gibt viele bewegliche Teile, wenn es darum geht, Fähigkeiten in einer Mission einzusetzen, insbesondere im Verteidigungsministerium, aber auch in Veteranenangelegenheiten, Homeland Security, FDA und FEMA, beispielsweise, und die Daten von Edge-Geräten zurück zu Bodenstationen und regionalen und Unternehmens-Rechenzentren zu bringen. Die Daten müssen für große Analyseströme in KI-Arbeitsabläufen nutzbar und zuverlässig sein und den Analysten zur Verfügung stehen, um Entscheidungen auf Grundlage der Rohdaten zu treffen.\n\n## Treiber für Edge: Latenz, Bandbreite, Sicherheit, Konnektivität\n\nEin Teil des Puzzles besteht darin, dass Edge-Geräte immer ausgefeilter werden und mehr Daten sammeln als wir jemals auf 5G werfen könnten. Die Hoffnung, dass 5G alle Daten erobert und sie nahtlos für das Rechenzentrum verfügbar macht, hat sich mit den Fortschritten bei IoT-Geräten nie verwirklicht.\n\nEiner der ursprünglichen Architekturen rund um das Internet der Dinge wurde von Cisco entwickelt und \"Fog\" genannt. Die Idee des Fogs bestand darin, dass das Rechenzentrum den Nebel mit den Edge-Geräten verband, sodass dort ein Teil der Verarbeitung und Konnektivität stattfinden konnte. Wenn die Netzwerkverbindung zuverlässig und konsistent ist, funktioniert dies gut mit ausreichender Bandbreite. Die Menge an Daten, die jedoch heute an den Rändern von Organisationen generiert wird, übersteigt jegliche verfügbare Bandbreite.\n\nMit IoT wird nur ein Teil der Daten ins Rechenzentrum verschoben, so dass der Datenwert im Allgemeinen nur dort analysiert wird. Das Problem ist, dass das Rechenzentrum nicht in der Lage ist, alle Big Data zu speichern und aufzunehmen. Selbst das Verschieben in die Cloud löst das Problem nicht, da die Cloud nicht in der Lage ist, alle Daten zu verarbeiten, die am Rand liegen. Deshalb möchten wir uns davon abwenden, alle Daten zum Rechenzentrum zu senden, um den Wert zu erfassen, und stattdessen den Datenwert so nahe wie möglich an den Rand zu bringen, um die Datenmenge, die zum Rechenzentrum zurückkommt, zu verringern.\n\nNatürlich kann nicht alle Daten an den Rand geschoben werden; es muss eine Korrelation zwischen verschiedenen Edge-Geräten geben. Der Wert muss an einem zentraleren Ort liegen, nicht unbedingt im zentralen Rechenzentrum, sondern möglicherweise in einem dieser Zwischenfogs oder regionalen Rechenzentren. Die Schlüssel besteht darin, Daten intelligent zu verschieben und den Datenwert so nah wie möglich am Rand auf eine wiederholbare und nachhaltige Weise zu positionieren. Dadurch können wir viel schneller auf den Rand reagieren.\n\n## Gängige physikalische Schicht\n\nUm einige dieser Probleme zu überwinden, brauchen wir zunächst eine gemeinsame physische Ebene. Das bedeutet, dass sie vom Rechenzentrum über die Nebel-Schichten bis hin zu den Edge-Geräten gemeinsam ist. Es gibt einen Weg, um die Geräte zu verwalten und zu steuern und Unterstützung auf eine zuverlässige, gemeinsame Weise zu erhalten. Das bedeutet nicht unbedingt die gleiche Maschine, sondern ein minimales funktionsfähiges Gerät mit einer gemeinsamen Schnittstelle. Ein weiterer Vorteil der gemeinsamen physischen Ebene besteht darin, dass, wenn Sie Code für eine Anwendung schreiben, er überall in diesem Ökosystem ausgeführt werden kann. Intel hat einige großartige Technologien dafür, wie zum Beispiel oneAPI, das einen Großteil der Arbeit erledigt, damit Sie Code einmal schreiben, die Binärdateien für die verschiedenen Arten von Geräten kompilieren, sie auf die gemeinsame physische Ebene übertragen und er entsprechend ausgeführt wird. Kurz gesagt, die Vorteile sind ein gemeinsames Betriebsmodell, ein gemeinsames Sicherheitsmodell und ein Schreib-einmal-ausführen-überall-Betriebsmodus.\n\n## Software Defined Infrastructure - Softwaredefinierte Infrastruktur\n\nSDI gilt für das Rechenzentrum in privaten und öffentlichen Clouds mit ihren softwaredefinierten APIs. Mit SDI am Rand erhalten wir gemeinsame Methoden zum Verschieben von Daten. Wir können Ressourcen am Rand im Rechenzentrum jederzeit bereitstellen und Daten auf diese Weise nahtloser übertragen.\n\n## Verteilte Informationsmanagement-Schicht\n\nWir müssen intelligenter mit der Verwaltung und Klassifizierung von Daten umgehen, indem wir die Daten nur dorthin bewegen, wo sie verarbeitet werden sollen - sei es am Rand, in einem regionalen Rechenzentrum oder in der Cloud. Wichtige Aspekte sind die Katalogisierung und Wiederverwendung von Daten sowie die Einhaltung von Compliance- und Sicherheitsanforderungen. Der Vorteil dieser verteilten Informationsmanagement-Schicht besteht darin, dass weniger Daten ins Rechenzentrum übertragen, weniger Daten bewegt und der Mehrwert an den Randbereich gebracht wird.\n\n## Service-Management-Schicht\n\nUm den Wert wirklich bis an den Rand zu bringen, müssen wir in der Lage sein, Anwendungen bis an den Rand zu verteilen. Hier kommt eine Service-Verwaltungsschicht oder ein Container-Ökosystem ins Spiel. Dadurch ist es möglich, Microservices wiederholbar und zuverlässig bis an den Rand, in die Nebelzone, ins Rechenzentrum oder in die Cloud zu bringen. Wenn beispielsweise ein regionales Rechenzentrum ausfällt, muss man sich nicht darauf verlassen, dass das Service-Mesh weiterhin funktioniert.\n\n## Anwendungsschicht für Dienste\n\nEine Anwendungsservicelage koordiniert verschiedene Anwendungen, sodass Sie Workflows erstellen können, die den tatsächlichen Geschäftswert aus den Daten generieren. Es genügt nicht nur, die Daten zu verschieben oder durch eine Analytics Engine laufen zu lassen. Die Daten müssen von der Analytics Engine zu einer Analystenarbeitsstation übertragen werden. Einige Werkzeuge in dieser Ebene wären Robotic Process Automation und DevOps-Pipelines. Hier können Sie auch Sicherheit und Compliance auf Anwendungsebene durchsetzen.\n\n## Sicherheits- und Identitätsstufen\n\nDer wichtigste Aspekt der Identitätsebene besteht darin, Vertrauen zwischen ordnungsgemäß identifizierten Entitäten herzustellen. Wir müssen verstehen, wer auf was zugreift und welche Geräte auf welche Daten, zu welcher Zeit und wo zugreifen. Die Identität wird über den typischen Benutzer hinaus in die Identität von Daten, Edge-Geräten, Nebel, Rechenzentren und der Cloud einbezogen.\n\nDie Zwillingsschwester der Identität ist Sicherheit. Hier haben wir Erkennung, Behebung, Verschlüsselung und das Festlegen des Vertrauensgrundes. Dies führt zu Zuverlässigkeit, Vertrauen, Daten und Compliance. Nun kann intelligente Daten an den Rand (edge) geschoben werden und von dort aus zum Rechenzentrum hochgeladen werden, ohne große Mengen an Rohdaten zu verschieben, sondern nur das, was man auf sichere Weise benötigt.\n\n## Hochrangiger Überblick\n\nUm eine erfolgreiche Edge-to-Cloud-Architektur zu haben, die wiederholbar ist, sind all diese verschiedenen Elemente notwendig. Wir haben gesehen, dass einige Organisationen eine maßgeschneiderte Edge-to-Cloud-Architektur erstellen und dann stecken bleiben, wenn sie eine neue Funktionalität in diesem Bereich implementieren wollen. Wenn sie zum Beispiel die Daten, die sich am Edge befinden, fest codieren, weil sie immer am Edge verarbeiten werden, oder im Rechenzentrum für eine Anwendung, die immer im Rechenzentrum verarbeitet wird, führt dies zu Starrheit. Es erhöht auch die Zeit, die benötigt wird, um neue Fähigkeiten einzuführen, möglicherweise Jahre anstatt Monaten. Wenn wir aus den wiederholten Anwendungsimplementierungen im Edge-to-Cloud-Bereich lernen und anfangen zu verallgemeinern, stellen wir schnell fest, dass sie in eine der von uns identifizierten Ebenen fallen.\n\nFür weitere Informationen schauen Sie sich bitte dieses Papier (inklusive Link) über die übergreifende Sichtweise dieser Architektur von Edge bis zur Cloud an. Wir schreiben nicht vor, was in diesen Boxen enthalten sein soll, aber entscheidend ist das Verständnis der Anwendungsfälle, die sie umfassen. Wir haben Ideen darüber, was in jeder Schicht enthalten ist, und wir bauen Ökosysteme auf, um den einzigartigen Anforderungen Ihrer Organisation innerhalb dieser Schichten gerecht zu werden.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT58-de","image":"./episodes/edt-58/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect bei Intel, skizziert die häufigen Probleme in Edge-zu-Rechenzentrum-Architekturen, die er im öffentlichen Sektor beobachtet und mit Kunden diskutiert hat. Er stellt die ideale Architektur vor, um diese Probleme zu lösen."},{"id":131,"type":"Episode","title":"Die Entwicklung des Datenschutzes","tags":["cybersecurity","data","dataprivacy","privacy","process","people","healthcare","sutterhealth"],"body":"\r\n\r\nNach dem Abschluss des Studiums der Rechtswissenschaften trat Jeremy direkt in das JAG Corps der US Air Force ein und diente neun Jahre lang als aktiver Offizier. Eine seiner Aufgaben war es, als Aktenverwalter für zwei verschiedene Büros tätig zu sein. Bei seiner letzten militärischen Aufgabe wechselte er seinen Fokus auf das Gesundheitswesen und arbeitete als Rechtsberater für Krankenhäuser im Nordosten. Nach seinem Ausscheiden aus dem aktiven Dienst (er ist immer noch in der Reserve), begann er als Rechtsberater in einem Krankenhaus zu arbeiten und ist jetzt bei Sutter Health.\n\nObwohl der rechtliche Rahmen für den Datenschutz derselbe ist, gibt es einige Unterschiede in Regierung und Privatsektor: der Ansatz, die Ziele und welche Vorschriften gelten. In der Regierung bedeutet Datenschutz, die Daten durch das Datenschutzgesetz zu schützen; geheime Dinge geheim zu halten. Obwohl es Ausnahmen gibt, wie das Informationsfreiheitsgesetz, sind die Systeme standardmäßig darauf ausgelegt, die Informationen zu schützen und nicht freizugeben.\n\nIm Gesundheitswesen wird seit HIPAA im Jahr 1996, HITECH einige Jahre später und jetzt mit dem ONC Daten in die Häuser der Patienten und in ihre Geräte übertragen, und die Patienten können Dritten den Zugriff erlauben. \"Angemessener Zugang\" ist wahrscheinlich eine bessere Beschreibung als \"Datenschutz\".\n\nDie Bewegung im Bereich des Datenschutzes bedeutet mehr Zugang und mehr Kontrolle für Einzelpersonen. Als Patient haben Sie nicht nur das Recht auf Ihre Informationen, sondern Sie können auch Ihren Gesundheitsdienstleister anweisen, sie einer dritten Partei zur Verfügung zu stellen: einem Anwalt, einem Freund, einem anderen medizinischen Fachpersonal usw. Ein Patient kann auch ein papier- oder elektronisches Freigabemedium angeben, daher gibt es viele Rechte, die Patienten gewährt werden.\n\nDiese Rechte zur Kontrolle eigener Daten gelten nicht nur im Gesundheitssektor. Wir sehen dies in der aktuellen Bewegung mit der Datenschutz-Grundverordnung (GDPR) und jüngsten Gesetzen in Brasilien, Kanada und China sowie einigen US-Bundesstaaten wie Kalifornien, Washington und Virginia.\n\nMit zunehmender Bedeutung der individuellen Rechte auf Zugang zu Datenschutzdaten wird sich die Art und Weise ändern, wie Organisationen Dinge verfolgen können. Große Unternehmen wie Google und Facebook bieten jetzt Optionen an, mit denen Nutzer ihre Daten löschen oder verhindern können, dass die Unternehmen sie auf verschiedene Weisen verkaufen. Tracking-Tools wie herkömmliche Cookies werden nicht mehr so relevant sein, daher muss es etwas anderes geben, das gezielten Werbetreibenden hilft.\n\nViele Daten sind natürlich bereits reguliert, wie zum Beispiel Beschäftigungsdaten. Die individuelle Kontrolle über Daten ist kein absolutes Recht; Unternehmen benötigen Daten, um funktionieren zu können, daher werden sie in der Lage sein, einige Daten zu behalten, aber es wird strenger reguliert werden. In den USA werden wir vor der Standardisierung mehr Komplexität und mehr Probleme haben. Wir haben 50 Bundesstaaten, von denen jeder seine eigenen Vorschriften hat.\n\nEs gibt so viele Gesetze, wie es Definitionen von persönlichen Informationen gibt, was zu Konflikten führen kann. Sutter zum Beispiel hat viele Krankenhäuser in Nordkalifornien und einige Nebenstellen in Hawaii, Oregon und Utah. Sutter muss regelmäßig über die Vorschriften dieser Staaten auf dem Laufenden bleiben, aber wenn es zu einem Verstoß kommt, treten die Gesetze des Staates, in dem die betroffenen Personen leben, in Kraft. Manchmal sind die Gesetze so verfasst, dass Sutter das Gesetz am Wohnort der Patienten befolgen muss, anstatt am Unternehmenssitz, was die Sache kompliziert macht.\n\nManchmal ergibt es Sinn, diese Art von Problemen auszulagern, und es entsteht eine ganze Rechtsbranche, die Unternehmen bei der Einhaltung von Datenschutz- und Informationssicherheitsvorschriften unterstützt.\n\nVon der IT-Seite aus gesehen bedeutet Datensicherheit, den Zugriff auf Dinge einzuschränken. Mit Datenschutz öffnen sich dagegen Türen für den Zugriff. Natürlich gibt es einen Validierungsprozess für den Zugriff, aber es besteht ein Balanceakt zwischen Sicherheit und Privatsphäre, der viel Arbeit sowohl auf rechtlicher als auch auf betrieblicher Seite erfordern kann.\n\nOrganisationen, die Apps entwickeln, die mit den Daten von Menschen umgehen, müssen verstehen, dass Datenschutzgesetze in jedem Land und Bundesstaat unterschiedlich sind und die Auswirkungen der Nutzung und Speicherung dieser Daten beachten sollten.\n\nJeremy, zusammen mit den Datenschutz- und Sicherheitsteams, engagiert sich mit den technischen Teams, manchmal sogar ab der Designphase, um sicherzustellen, dass alles den Vorschriften entspricht. Zum Beispiel wird er mit dem Team sprechen, das die Patientenportale entwickelt, um zu sehen, ob die geplanten Aktivitäten den Bestimmungen entsprechen. Außerdem hilft er dabei, Fragen zu beantworten, welcher Datenbanktyp am besten geeignet wäre oder ob ein Cloud-Anbieter in Übereinstimmung eingerichtet werden kann. Jeremy merkt, dass je mehr er sich selbst weiterbildet und Schulungen zu technischen Aspekten absolviert, desto hilfreicher kann er im Prozess sein.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jeremy Harris"],"link":"/episode-EDT59-de","image":"./episodes/edt-59/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect, Intel, diskutiert mit Jeremy Harris, Assistant General Counsel - Privacy/Information Security, bei Sutter Health, was Datenschutz wirklich bedeutet und in welche Richtung er sich entwickeln könnte."},{"id":132,"type":"Episode","title":"Management von Telearbeit bei voller Kapazität","tags":["telework","remoteworker","vdi","mfa","cybersecurity","people","process"],"body":"\r\n\r\n## Teleworker Betriebsarten\n\nDie Betriebsarten, in denen wir Menschen im Verteidigungsministerium und auch im privaten Sektor arbeiten sehen, sind folgende:\n\nGerät als eine dumme Terminalanzeige (VDI - Virtuelle Desktop Infrastruktur)\n\nGerät als Teil des internen Netzwerks (VPN - Virtuelles Privates Netzwerk)\n\n* Gerät als Portal zu Dienstleistungen\n\nDie meisten Arbeiter nutzen mehrere Betriebsarten, daher müssen verschiedene Arten durch die Infrastruktur unterstützt werden.\n\n## Gerät als dummes Terminal\n\nIn diesem Modus läuft der Arbeitsplatz des Mitarbeiters auf einem Server im Rechenzentrum, und er verwendet seinen Laptop, um eine Verbindung zum \"virtuellen Desktop\" herzustellen. Grundsätzlich nutzen sie ihren Laptop oder ein anderes Gerät als \"dummes\" Terminal. Es gibt einige Probleme mit VDI, darunter Skalierbarkeit, Netzwerküberlastung, Latenz und Redundanz. Aufgrund dieser Probleme kann eine VDI-Sitzung ziemlich teuer sein. Von den drei Optionen ist VDI jedoch am sichersten.\n\n## Gerät als Teil des internen Netzwerks\n\nDieser Modus ist wahrscheinlich der gängigste. Der Arbeiter verbindet sein Gerät mit dem VPN, sodass er im Netzwerk arbeitet, als wäre er im Büro verbunden. Ein Vorteil ist die Mobilität, da die Mitarbeiter jedes Gerät wie ein Telefon, Tablet oder Laptop verbinden können. Ein weiterer Vorteil ist, dass es verschiedene VPN-Segmente für unterschiedliche Datenklassifikationen geben kann. Ein Nachteil ist, dass die Arbeiter immer verbunden sein müssen, um zu arbeiten. Und obwohl der Netzwerkverkehr nicht so stark ist wie bei einer VDI, gibt es trotzdem einige Netzwerkstaus, wenn Daten übertragen werden.\n\n## Gerät als Eingang für Dienstleistungen\n\nImmer mehr Menschen gehen zu diesem Modus über. Der Arbeiter nutzt sein Gerät, um sich mit Diensten in der Cloud, entweder öffentlich oder privat, zu verbinden. Der Hauptdienst, den die Menschen nutzen, ist Office 365, der Organisationen ermöglicht, Cloud-Dienste anstelle oder in Verbindung mit ihren eigenen Diensten zu nutzen. Effizienz, Leistungsfähigkeit und die Möglichkeit, zeitweise offline zu arbeiten, sind Vorteile. Nachteile sind die Abhängigkeit von einem Dritten... (Ich bin mir nicht sicher, wie ich das beenden soll – Anpassung/Übergang und Sicherheit).\n\n## Aufkommende Engpässe\n\nEngpässe sind aufgetreten, da die Anzahl der Menschen, die von zu Hause aus arbeiten, von 15% auf mehr als 85% gestiegen ist. Für die ersten beiden Betriebsmodi kann die VPN-Skalierbarkeit ein Hauptengpass sein. Begrenzte Bandbreite ist ebenfalls eine große Quelle von Engpässen. Eine Lösung für diese Probleme besteht darin, Cloud-Services zu nutzen, um die Belastung zu verringern. Auf menschlicher Ebene werden IT-Operationen und Helpdesks überlastet, da Mitarbeiter zum ersten Mal von zu Hause aus eine Verbindung herstellen.\n\n## VPN-Skalierbarkeit\n\nEs gibt mehrere kurzfristige Lösungen, um diese Engpässe zu lindern. Die Priorisierung des Benutzerzugangs kann effektiv sein, ob es nun auf Zeitplänen oder Prioritäten der Aufgaben basiert. Eine Mitarbeiterbildung über die Anpassung an diese Umgebung ist notwendig.\n\nFür langfristige Skalierbarkeit sollten Organisationen auf SaaS-Lösungen umsteigen und Laptops als Portalmodus nutzen.\n\n## Bandbreite zu Websites\n\nEine der besten Maßnahmen, die Organisationen kurzfristig ergreifen können, ist herauszufinden, wie viele VDI-Benutzer sie derzeit haben und zu prüfen, ob man sie in den Betriebsmodus des Netzwerks oder der Zusammenarbeitstools verschieben kann. Dadurch wird die Belastung des Bandbreitenverbrauchs erheblich reduziert. Es könnte auch erforderlich sein, die Netzwerkkapazitäten zu erhöhen, nachdem man evaluiert hat, wie die remote Benutzer arbeiten. Bildung spielt erneut eine wesentliche Rolle für diese Veränderung, damit die Mitarbeiter bewährte Verfahren anwenden, zum Beispiel die Trennung von der VPN-Verbindung, wenn sie sie nicht nutzen, und die Konfiguration von Backup-Tools für die Nutzung außerhalb der Geschäftszeiten. Langfristig empfehlen wir eine mehrstufige Hybrid-Cloud-Architektur, die es Ihnen ermöglicht, Cloud-Service-Anbieter für Netzwerkbandbreiten und Burst-Fähigkeit zu nutzen und Kosten und Kapazität zu optimieren.\n\n## Skalierbarkeit der gehosteten Dienste\n\nUm beim Umsetzen skalierbarer Architekturen für kurzfristige Zwecke zu helfen, gibt es mehrere gute Referenzen, darunter Outlook Web Access (OWA) und VDI-Referenzarchitekturen.\n\nErneut empfehlen wir langfristig den Umstieg auf eine Multi-Hybrid-Cloud-Infrastruktur für Elastizität, Kapazität, vorhersagbare Leistung, Compliance und Sicherheit.\n\n## IT-Betrieb\n\nWie können wir das Helpdesk-Team skalieren, das wahrscheinlich überfordert ist? Eine Idee ist es, Online-FAQs für eine einfache Referenz der Mitarbeiter bereitzustellen. Von der IT moderierte, nutzergenerierte Lösungen aus der Community können ebenfalls nützlich sein. Idealerweise sollten Organisationen ein Ticket-verwaltetes System nutzen, um Engpässe zu identifizieren und Prozesse zu optimieren. Darüber hinaus sollten alle wiederholenden Aufgaben, falls möglich, automatisiert werden, zum Beispiel durch Robotic Process Automation (RPA) oder zusätzliche Skripte. Eine langfristige Lösung könnte die Implementierung von AI Chatbots als Self-Service IT-Helpdesk sein. Sie können online Lösungen anhand von Schlüsselwörtern schnell einschränken oder zur Kontaktaufnahme empfehlen.\n\n## Schlussfolgerung\n\nLetztendlich möchte Intel sehen, dass Organisationen in dieser schwierigen Zeit erfolgreich sind, wenn Mitarbeiter vom Büro zur Fernarbeit wechseln und mit viel Stress umgehen. Intel kann der Industrie, Regierung und dem öffentlichen Sektor helfen. Wir haben Silizium, das in all diesen Aspekten funktioniert. Wir haben Partner, die Hardware- und Softwarelösungen liefern, und natürlich verkaufen wir PCs und Geräte für Kunden, die Fernarbeit ermöglichen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Tim Flynn","Pete Schmitz"],"link":"/episode-EDT6-de","image":"./episodes/edt-6/de/thumbnail.png","lang":"de","summary":"In dieser Folge sprechen Darren, Tim Flynn, pensionierter Konteradmiral der Marine, und Pete Schmitz, Account Executive für die Marine von Intel, darüber, wie man das explosive Wachstum von Telearbeitern aufgrund der Covid-19-Pandemie bewältigen kann. Wir diskutieren die verschiedenen Arbeitsmodi, die Mitarbeiter nutzen können, um remote zu arbeiten und dennoch produktiv zu sein: Gerät als dummes Terminal, Gerät als Teil des internen Netzwerks und Gerät als Portal zu Diensten. Das Verständnis dieser Betriebsmodi kann dabei helfen, Engpässe zu finden, die die Effektivität Ihres Teams beeinträchtigen können."},{"id":133,"type":"Episode","title":"Risikomanagement bei Ransomware","tags":["cybersecurity","ransomware","riskmanagement","process"],"body":"\r\n\r\nIm vergangenen Jahr gab es einen Anstieg der Anzahl und Raffinesse von Cyberangriffen. Die drei Hauptbereiche wiederkehrender Angriffe sind Erpressungssoftware, Angriffe auf die Lieferkette und Datenlecks.\n\nDie Angriffe sind aus mehreren Gründen immer ausgefeilter geworden. Erstens müssen die Angreifer besser werden, da die Sicherheit verbessert wird. Organisierte Kriminalität, Staatsakteure und andere Bedrohungsagenten erkennen, dass es mehr Raffinesse erfordert, um Sicherheitskontrollen zu umgehen und zu vereiteln. Genauso wie Unternehmen Softwareentwicklungslebenszyklusprozesse haben, hat auch die Malware-Gemeinschaft solche. Sie verfügen über Werkzeuge und Frameworks, aus denen sie bauen, sowie gute Verfahren, um Qualität in ihre Systeme einzubringen. Unterschiedliche Akteure kaufen, verkaufen und leihen Code aus. Sie lernen voneinander und teilen Informationen im Darknet. Sie sind nicht nur zusammengewürfelte Hacker-Teams, sondern agieren eher wie Unternehmen.\n\nDiese Bedrohungsakteure operieren in einer milliardenschweren Industrie. Großes Geld treibt die Notwendigkeit nach Reife an. Wir können Sicherheit nicht mehr einfach nur anhängen; sie muss integriert werden und zwar überall, nicht nur in den Produkten, sondern auch in der Infrastruktur und den Prozessen. Das war eine der Lehren aus SolarWinds: Selbst wenn man ein gutes Produkt entwickelt, kann die unterstützende Infrastruktur anfällig sein.\n\nAktuelle Angriffe zeigen, dass niemand immun ist. Oftmals nehmen Organisationen fälschlicherweise an, dass sie sicher sind, da sie keine Finanzdienstleistungsunternehmen, Regierungen oder andere hochwertige Industrien sind. Jedoch haben die jüngsten Angriffe auf Unternehmen wie JBS Foods, McDonald's und Audi gezeigt, dass niemand immun ist. Unternehmen, unabhängig von ihrem Produkt, sind auf ihre digitale Infrastruktur angewiesen, um funktionsfähig zu sein. Der Angriff auf JBS Foods legte die weltweit größte Fleischverarbeitungsindustrie lahm.\n\nKeine einzelne Sache dient als Heilmittel, um diese Angriffe zu verhindern. Es gibt Hoffnung, aber es erfordert viel Arbeit. Eine Organisation muss den Fleiß haben, die richtigen Risikokennzahlen anzuwenden, um Sicherheit korrekt zu implementieren. Wenn du dein Risiko nicht verstehst, werden keine Sicherheitskontrollen funktionieren, weil du nicht weißt, ob du sie an der richtigen Stelle anwendest.\n\nDer Schlüssel besteht darin, mit dem richtigen Satz an Richtlinien und Risiken für Ihre Organisation zu beginnen. Ein grundlegender Schritt besteht darin, dass selbst wenn Ihre Organisation noch nicht vollständig herausgefunden hat, wie eine \"Zero-Trust-Architektur\" implementiert werden kann, das Leugnen aller Zugriffsanfragen, bis ihre Berechtigung nachgewiesen ist, in die richtige Richtung geht. Das bedeutet, dass es an jeder Tür eine Schranke gibt, anstatt einen Generalschlüssel für alles im Inneren. Die Standardeinstellung für Ablehnung ist ein Grundsatz des \"Zero Trust\"-Ansatzes.\n\nWenn die Stärke eines Unternehmens nicht in der Cybersicherheit liegt oder die finanziellen Mittel für ein ausreichendes internes Team nicht verfügbar sind, gibt es viele Ressourcen, die helfen können. Ein gemanagter Sicherheitsdienstleister (Managed Security Provider, MSP) ist eine gute Option, aber es sollte immer mindestens ein Experte im Inneren geben: ein Chief Security Officer. Diese Person hat den lokalen Kontext der Domänen-Erfahrung, um mit dem MSP zusammenzuarbeiten und dieses Wissen in der gesamten Organisation zu verbreiten. Der MSP verwaltet Ihre Sicherheitswerkzeuge und Konfigurationen, aber Sie benötigen jemanden, der Sicherheitswissen an Geschäfts- und IT-Einheiten vermittelt. Angesichts der jüngsten Angriffe ist ein Sicherheitsteam keine Option.\n\nJedes Unternehmen sollte einen Plan für einen Ransomware-Angriff haben. Wenn es bereits geschehen ist, ist nicht die Zeit, um es herauszufinden. Eine Grundlage ist, regelmäßig Daten zu sichern. Bewahren Sie makellose Kopien der Daten, Systeme, Anwendungen und Konfigurationen in einer offline, out-of-band Speicherumgebung auf. Sechs Monate saubere Backup-Daten sind wichtig, weil sich manchmal Ransomware in den Backups befinden kann, bevor sie entdeckt wird. Stellen Sie außerdem sicher, dass die Plattformen oder Server, die Sie für Ihre Datenbank benötigen, offline verfügbar sind, damit Sie sie in einem Ausfall- oder Redundanzmodell aktivieren können.\n\nDies ist im Grunde genommen das Geschäftskontinuitätsplanung. Genau wie eine Organisation einen Plan für die Fortsetzung im Falle einer physischen Katastrophe wie einer Überschwemmung oder eines Stromausfalls haben würde, sollte es auch einen Plan geben, um weiterhin mit wichtigen Firmenanwendungen teilweise wieder aufzunehmen, während das Problem gelöst wird.\n\nEin Schritt, um diese Redundanz zu erreichen, besteht darin, bei Bedarf in die Cloud auszubrechen und Cloud-Ressourcen in einem makellosen Zustand zu halten, möglicherweise sogar bei einem anderen Cloud-Service-Anbieter. Ein weiterer Schritt besteht darin, einen Kanarienvogel im Bergwerk zu haben. Das bedeutet, Systeme in der gesamten Unternehmenslandschaft bereitzustellen, die Sensoren auf höchster Stufe haben. Um Leistungs-, Speicher- und Geschwindigkeitsprobleme zu vermeiden, können sie strategisch an bestimmten Stellen eingesetzt werden, anstatt systemweit, um als Frühwarnungen zu dienen.\n\nDas Erstellen eines Plans im Voraus wird auch bei der Herausforderung helfen, was in einer Krisensituation zu tun ist, ob Sie das Lösegeld bezahlen oder das FBI kontaktieren. Der Plan sollte schriftlich festgehalten sein und nicht nur Ihre Techniker, sondern auch Ihre Anwälte, CEO, CFO usw. einbeziehen, und jeder sollte Zugang dazu haben. Sie sollten wissen, wie man Bitcoin kauft, und Sie sollten die Nummer der örtlichen FBI-Büros und andere Informationen haben. Führen Sie den Plan als Übung durch, um zu sehen, ob er genauso funktioniert wie ein Notfallwiederherstellungs- oder Geschäftskontinuitätsplan.\n\nEinige Branchen mögen denken, dass sie sicher sind, wenn sie ihre operationelle Technologie (OT) und ihre informatielle Technologie (IT) getrennt halten, aber sie sind tatsächlich nicht völlig getrennt. Zum Beispiel könnte eine Produktionslinie auf Computermaschinen laufen, aber ein Großteil der Versorgungskette, Logistik und der gesamten Organisation basieren auf IT-Systemen. Wenn diese Systeme ausfallen, kommt nichts rein oder raus. IT-Systeme sind geschäftskritisch und aus den jüngsten Angriffen lernen wir, dass wir von digitaler Technologie für all unsere Unternehmen abhängig sind.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT60-de","image":"./episodes/edt-60/de/thumbnail.png","lang":"de","summary":"In dieser Episode, Teil eins von zwei, diskutiert Darren Sicherheitstrends mit seinem häufigen Gast Steve Orrin, CTO von Intel, Federal. Im vergangenen Jahr gab es einen Anstieg der Anzahl und der Raffinesse von Cyberangriffen. Die drei wichtigsten Bereiche wiederkehrender Angriffe sind Ransomware, Angriffe auf die Lieferkette und Datenlecks."},{"id":134,"type":"Episode","title":"Datenverstöße & sichere Lieferkette","tags":["datbreach","data","process","policy","cybersecurity","supplychain","securesupplychain"],"body":"\r\n\r\nNeben prominenten Angriffen in der Lieferkette wie dem Vorfall mit SolarWinds gibt es auch andere weniger bekannte Angriffe, die ebenso problematisch sind.\n\n## Sichere Lieferkette\n\nEin Beispiel ist die Chrome-Erweiterung \"The Great Suspender\", ein Werkzeug zur Gewährleistung einer korrekten Speichernutzung in Chrome-Anwendungen, mit etwa zwei Millionen Benutzern. Die Gründer dieses Open-Source-Tools verkauften ihre Firma ohne angemessene Sorgfalt an eine Organisation für Millionen von Dollar. Es stellte sich heraus, dass der Käufer eine organisierte Verbrechergruppe war, die das Werkzeug dann in Malware und Spyware umwandelte.\n\nEs gab keinen Angriff im Sinne dessen, dass sie nicht in das ursprüngliche Unternehmen eingedrungen sind, sondern es gekauft und mit dem Code gemacht haben, was sie wollten. Was einmal ein legitimes Produkt war, wurde am nächsten Tag illegitim. Keine Sicherheitsprotokolle hätten dieses Problem beheben können. Unternehmen müssen nun ihre Lieferanten und die Drittanbieter, die sie unterstützen, sorgfältig prüfen.\n\nOpen Source ist ein Segen und ein Fluch. Der Segen besteht darin, dass man auf den Quellcode zugreifen kann und ihn nach Belieben überprüfen kann, da er frei verfügbar ist. Der Fluch besteht darin, dass niemand genügend Zeit, Energie oder Expertise besitzt, um jeden Teil des Open-Source-Codes, den er verwendet, gründlich zu überprüfen. Schadhafter Code, der anfällig für Malware ist und auch wenn dies unbeabsichtigt geschieht, kann eingeführt werden, wenn die Community ihn nicht erkennt, und manchmal dauert es lange, bis dies geschieht.\n\nEs gibt zwei Dinge, die dieses Problem mildern könnten: Erstens könnte der Code mit Hilfe eines Quellcode-Analysetools ausgeführt werden, und es könnte ein Bewertungssystem für Code-Mitwirkende geben, deren Code konsequent weniger Sicherheitsrisiken oder Fehler aufweist. Drittanbieter würden diese Arbeit hauptsächlich durchführen. Zweitens gibt es bereits etablierte Startups, die offene Quellcodeprodukte überprüfen, um anzuzeigen, welche Objekte in einem Repository vertrauenswürdig sind und welche noch nicht vertraut werden können.\n\nAktuelle Schwachstellenanalysen in Open-Source-Programmen sind zwar eine Maßnahme, aber alleinige Sicherheitsvorkehrungen reichen nicht aus. Sie müssen mit zusätzlichen Kontrollen kombiniert werden, bevor sie in Ihrer Organisation angewendet werden.\n\n## Datenverletzungen\n\nNeben Ransomware- und Lieferkettenangriffen sind Datenlecks ein häufiges Problem. Im Jahr 2020 wurden in insgesamt 7,8 Milliarden Datensätzen 1,8 Billionen Dollar durch Datenlecks extrahiert. Ein Vorfall bei McDonald's beispielsweise hat Kundendaten, Partnerdaten und interne Daten gefährdet.\n\n## Verschlüsselung\n\nDer erste Teil der Lösung besteht darin, bessere Sicherheitstools für Daten und Infrastruktur zu haben. Durch Verschlüsselung des Zugriffs und die Verwendung einer Standard-Deny-Funktion ist es entscheidend, dass selbst wenn jemand durch die Haustür hereinkommt, sie keinen Zugriff auf alles haben. Daten sollten innerhalb der Organisation verschlüsselt werden, nicht nur das, was in der Cloud freigegeben wird oder extern gesendet wird. Jegliche Daten, die Ihr Netzwerk durchqueren, sowie Daten im Ruhezustand sollten verschlüsselt sein.\n\nVerschlüsselung hat Kosten, aber auf moderner Hardware gibt es eingebaute Beschleunigung, die den Nachteil aufhebt. Sie können nun die Verschlüsselung in Ihrer gesamten Organisation aktivieren, ohne Beeinträchtigung der Leistung.\n\n## Segmentierung\n\nEin weiterer Teil sind Exklaven oder Segmentierung. Eine der Herausforderungen in Unternehmensnetzwerken besteht darin, dass, wenn jemand erst einmal eingedrungen ist, freie Hand hat, wenn alles miteinander verbunden ist. In letzter Zeit gab es Bestrebungen, die Entwicklung auszugliedern und in ein eigenes Netzwerk zu verlagern, und das ist ein Anfang, aber nur die Spitze des Eisbergs. Die Netzwerksegmentierung sollte über die gesamte Organisation hinweg erfolgen. Es können immer noch Verbindungen stattfinden, aber sie müssen sich an eine Reihe von Regeln halten und helfen, die Auswirkungen einzuschränken. Wenn zum Beispiel Ihr Helpdesk angegriffen wird, sind Ihre HR-Systeme nicht gleichzeitig kompromittiert.\n\nDie Mikrosegmentierung war vor etwa fünf Jahren ein Modewort, aber sie muss jetzt umgesetzt werden. Es gibt einige großartige Tools, die dabei helfen können, wie zum Beispiel Container-Ökosysteme, in denen Sie eine Anwendung bereitstellen können, die sich in ihrem eigenen Netzwerk mit eigener Firewall befindet.\n\nDie ordnungsgemäße Implementierung von Authentifizierungsberechtigungen muss jetzt auch erfolgen. Eine Mehrfaktor-Authentifizierung ist erforderlich, ebenso wie die Authentifizierung von Entitäten. Viele Tools sind automatisiert und verfügen über automatisierte Prozesse, daher müssen nicht nur Personen, sondern auch Entitäten über ordnungsgemäße Berechtigungen verfügen.\n\n## Null Vertrauen\n\nDer Zero-Trust-Ansatz hat sich mittlerweile so weit entwickelt, dass er umgesetzt werden sollte, und einige der wichtigsten Prinzipien wie \"standardmäßig ablehnen\" und \"niemandem vertrauen\" sind von entscheidender Bedeutung. Die Technologie hat den Einsatz solcher Konzepte aufgeholt.\n\nEntwickler könnten sich Sorgen um diese Sicherheitstools machen, die den Prozess verlangsamen könnten, aber es gibt Möglichkeiten, die Architektur so zu gestalten, dass dieses Problem verringert wird. Zum Beispiel, wenn Sie ein Entwickler sind und über angemessene Zugangsdaten und Zugriff verfügen, sollten Sie in der Lage sein, auf die Dinge zuzugreifen, die Sie benötigen, wenn Sie sie benötigen, und nach Abschluss des Vorgangs den Zugriff zu verlieren, anstatt Zugangsdaten zu haben, die Ihnen jederzeit uneingeschränkten Zugang gewähren. Die Idee von Zero Trust besteht nicht darin, dass das Unternehmen dem Entwickler nicht vertraut, sondern dass der Zugriff für den richtigen Moment gilt, anstatt uneingeschränkten Zugang zu gewähren, wenn ein böswilliger Akteur die Zugangsdaten stiehlt.\n\nKeine Industrie kann es sich leisten, die aktuellen Risiken zu ignorieren. Jede Organisation muss Sicherheit anders betrachten und Sicherheitsmaßnahmen innerhalb der gesamten Organisation und Architektur umsetzen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT61-de","image":"./episodes/edt-61/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect, Intel, diskutiert Datenverstöße und sichere Lieferketten mit dem häufigen Gast Steve Orrin, CTO von Intel, Federal."},{"id":135,"type":"Episode","title":"Entmystifizierung von 5G, Edge und KI","tags":["edge","compute","5g","aiml","iot"],"body":"\r\n\r\nAnna's Hintergrund liegt in der industriellen und produzierenden Seite der Informationstechnologie. Sie hat einen Doktortitel in Chemieingenieurwesen sowie einen MBA. Sie arbeitete 15 Jahre lang als Prozess- und Designingenieurin, bevor sie sich mit Startup-Arbeiten und dem Management von Ingenieurwesen beschäftigte. Seit etwas über sechs Jahren ist sie bei Intel tätig, wobei sie in den letzten zweieinhalb Jahren im Team des öffentlichen Sektors arbeitet. Dort unterstützt sie hauptsächlich Intels IoT- und Edge-Aktivitäten und ist zunehmend in 5G involviert, da 5G zusammen mit Künstlicher Intelligenz diesen Bereich drastisch verändert.\n\n## Netzwerktransformation grundlegend für 5G-Infrastruktur\n\nIn der industriellen Welt war das IoT nichts Neues; es war einfach fest verdrahtet. Da die Systeme jetzt jedoch auf drahtlose Technologie umsteigen, erfolgt eine enge Verknüpfung von IT und OT zur Konvertierung von Analysedaten, anstatt nur einen Satz von prozessspezifischen Daten dieser OT-Welt zu haben. Eine andere Reihe von Systemen verheiratet dies nun mit dem Geschäft, so dass all dies in den gleichen Raum gezogen wird. Obwohl es klare Unterschiede in den Anwendungsfällen und Architekturen im öffentlichen Sektor wie Militär und Smart Cities gibt, gibt es eine gemeinsame Konvergenz in der Analyse, Anwendung und Zeitplanung, um bessere Geschäftsentscheidungen zu treffen.\n\nEin Grund, warum das IoT so lange gebraucht hat, um sich von der Fertigung in andere Bereiche auszubreiten, sind hauptsächlich die hohen Kosten für die Verkabelung. Sicherheit war ebenfalls ein Hindernis.\n\n5G entfesselt jetzt die IoT- und Edge-Welt, aufgrund der Kosteneffizienz, insbesondere auf der Verbraucherseite. Wenn es jedoch um kritische geschäftliche Infrastruktur geht, handelt es sich um eine andere Diskussion darüber, wie man es auf eine Weise umsetzt, die die Daten schützt. 5G wird bahnbrechend sein, aber es passiert noch nicht sofort auf der kommerziellen Seite. Ein Teil der Lücke entsteht aufgrund der Verzögerung zwischen der Veröffentlichung der Standards und der Produktion der Hardware, um von diesen Standards zu profitieren.\n\nFür jeden, der bereits viel Zeit mit 3GPP oder anderen treibenden Normungsgremien verbracht hat, ist diese zeitliche Entwicklung nicht überraschend. Derzeit kann man ein 5G-Netzwerk aufbauen, aber wir sind in der Testphase, in der wir den Wert von 5G demonstrieren müssen. Wir müssen neue Anwendungsfälle zeigen, die nicht von LTE oder 4G unterstützt werden können.\n\n## Mehrere 5G-Bereitstellungsmodelle bieten Flexibilität.\n\nWarum nicht einfach bei WiFi 6 bleiben anstatt 5G zu verwenden? Die Antwort ist kompliziert. Viele der Standardisierungsorganisationen, die 5G vorangetrieben haben, treiben auch WiFi 6 voran, daher handelt es sich um eine ergänzende Technologie. Der Unterschied liegt in der Ausstattung, daher müssen Sie die Details Ihrer Anwendungsfälle kennen, um festzustellen, was wirtschaftlich am sinnvollsten ist. Zum Beispiel ist 5G großartig, aber es wäre nicht ratsam, ein 5G-Netzwerk auf einem Kreuzfahrtschiff einzurichten, da die Durchdringung dafür nicht gegeben ist. Die Umgebung ist nicht freundlich gegenüber jeglicher Art von Funksignalen, aber WiFi 6 mit Access Points ergibt mehr Sinn.\n\n5G ist überzeugend, jedoch aus mehreren Gründen und kann Dinge tun, die 4G und LTE nicht können. Für erweiterte Realität und virtuelle Realität benötigt man eine sehr geringe Latenz und hohe Bandbreite, um interaktivere Anwendungsfälle zu ermöglichen, wie beispielsweise Reparaturen von Geräten oder Maschinen. Man benötigt einen entfernten Experten, der ein Problem beobachtet und Video- und Audio-Streaming mit Überlagerung von Zeichnungen und Funktionen durchführen kann, die von einem zentralen Standort oder einem entfernten Standort aus verwaltet werden, welcher das gesamte Wissen und die Fachkenntnisse direkt an den Punkt bringt, an dem du arbeiten möchtest. Ein solcher Anwendungsfall kann nicht über ein LTE-Netzwerk abgewickelt werden. Bandbreite spielt dabei eine Rolle, aber die Latenz ist die treibende Kraft. Um keine gesundheitlichen Probleme beim Tragen einer VR-Brille zu bekommen, müssen die Latenzzeiten wirklich gering sein, ohne Verzögerungen oder asynchrones Verhalten.\n\nDie LTE- und 4G-Welt hat sich aufgrund von 5G verändert, aufgrund der Art und Weise, wie sie das Spektrum verwaltet haben. Ein Bereich, in dem sich dies in der Industrie ändert, ist, dass es nun möglich ist, eine Prioritätsgenehmigung für das CBRS-Spektrum zu erhalten und ein eigenes privates Netzwerk aufzubauen, das vollständig von den großen Telekommunikationsunternehmen getrennt ist. Zum Beispiel kann ein großer Hersteller mit einem privaten LTE kostengünstiger eine große Fläche abdecken als mit Access Points. Es gibt auch große Vorteile, zum Beispiel wenn Sie Ihren Raum neu konfigurieren möchten und nicht möchten, dass alle Arbeitsstationen verkabelt sind, oder wenn Sie riesige Metallteile wie Flugzeugrümpfe in Ihrem Raum bewegen müssen, die die WiFi-Signale stören könnten. Sie können die Infrastruktur so aufstellen, dass sie tragbar ist und nicht an verkabelte Standorte gebunden ist.\n\nSicherheit ist eine zentrale Sorge für jede Organisation. Obwohl bei der Entwicklung von 5G keine Sicherheit im Vordergrund stand, wird dies bei 6G der Fall sein. Glücklicherweise können wir mit den Möglichkeiten von 5G viel im Bereich des Zero-Trust-Networking und anderer Sicherheitsmaßnahmen tun, um Kunden mit Zuversicht darüber zu versehen, wie ihre Daten über Netzwerke übertragen werden.\n\nDie 5G-Standards haben das Problem von vor einigen Jahren geändert, als drahtlose Infrastrukturen ausschließlich in proprietärer Hardware und proprietärer Software existierten und nur wenige Unternehmen sich lizenzierte Frequenzspektren leisten konnten. Softwaredefiniertes Networking ermöglicht es, die Netzwerkinfrastruktur auf gängiger Massenware zu hosten. Es wird kein spezialisiertes Equipment mehr wie in den vorherigen Generationen benötigt. Dies geschieht auch auf der LTE-Seite, indem beispielsweise das CBRS-Frequenzspektrum zur Verfügung gestellt wird, um sich von proprietärer Hardware und Software zu lösen.\n\nIntel investiert viel Zeit darin, mit Störern zusammenzuarbeiten, die unsere FlexRAN-Referenzarchitektur nutzen. Die FlexRAN-Architektur bildet die Grundlage dafür, disruptive Technologien auf dem neuen 5G-Markt zu verbreiten, da sie einen 5G-Software-Stack auf handelsüblicher Hardware ermöglicht, für den zuvor proprietäre Hardware erforderlich war. Jetzt gibt es einen viel offeneren und portableren Raum, und die Eintrittskosten sind deutlich günstiger als zuvor. Es sind nicht mehr nur wenige Unternehmen, die alles kontrollieren. Intel und andere versuchen, alles zu öffnen und von offenen Standards zu profitieren, um all diese Störer zu unterstützen und die gesamte Dynamik zu verändern.\n\n## 5G-Spektrum und Regulierung für Netzwerke der Zukunft\n\nMit verbesserter Konnektivität, geringer Latenz und hoher Bandbreite werden viele neue Anwendungsfälle verfügbar sein. Wie 5G monetarisiert wird, ist das, was sich auf dem Markt verändert. Ein Beispiel ist ein Cloud-Service-Anbieter, der zusammen mit einem Telekommunikationsunternehmen bessere Dienstleistungen für ihre Kunden anbieten kann, weil sie nicht mehr isoliert sind. Es handelt sich um eine gemeinsame Geschäftsanstrengung, die wirklich wichtig ist: Qualität und Priorisierung. Eine andere Möglichkeit, das zu betrachten, ist, dass Cloud-Service-Anbieter Fähigkeiten kaufen, die die Netzwerkfunktionalität auf die gleiche Weise eröffnen werden, wie Telekommunikationsunternehmen die Möglichkeiten der Cloud-Seite erkunden. Nochmals, dies liegt daran, dass die Isolation aufgehoben wird; die Datenleitung ist nicht mehr eine Reihe von Dienstleistungen.\n\nEs ist nicht klar, wie sich all dies entwickeln wird, abgesehen davon, dass es aufgrund der Datenzugänglichkeit die Art der Arbeit neu definiert und wo diese Arbeitslasten stattfinden werden. Es liegt ein großer Wert darin, nahtlos von der Randzone zur Cloud zu gelangen und dies basierend auf den Bedürfnissen des Kunden zu tun, was nun möglich ist.\n\n## Ein neues Berechnungsparadigma unterstützt neue Datenerfordernisse.\n\n5G entfesselt viele verschiedene architektonische Modelle. Zum Beispiel bietet es zwei Optionen für KI-Architekturen, während zuvor nur eine mit Einschränkungen vorhanden war.\n\nOhne die hohe Bandbreite, die durch 5G bereitgestellt wird, war KI darauf beschränkt, Schlussfolgerungen auf den Edge-Geräten durchzuführen, was bedeutete, dass KI-Modelle auf die Edge-Geräte geschoben werden mussten. Diese umständliche Einschränkung erhöhte den KI-Entwicklungs- und Bereitstellungszyklus und begrenzte die Anzahl der KI-Arbeitslasten, die an der Edge genutzt werden können. Mit einer erhöhten Bandbreite können große Datenströme von Kameras oder Sensoren an ein Rechenzentrum zurückgeschickt werden, was es ermöglicht, mehrere KI-Arbeitslasten auszuführen und kontinuierliches KI-Lernen zu ermöglichen. Dies gibt Organisationen die Möglichkeit, sowohl Schlussfolgerungen an der Edge durchzuführen als auch das für so viele Organisationen erforderliche Deep Learning zu verbessern, das sich ständig ändernden Anforderungen an ihre Daten.\n\nMit AR bedeutet 5G beispielsweise, dass Headsets mobil sein können, anstatt mit den gleichen Fähigkeiten verbunden zu sein, da 5G das Teilen größerer Datensätze in einer mobilen Welt ermöglicht. Die traditionellen Grenzen der Datenzentren werden aufgebrochen.\n\nWenn Sie nicht über viel technischen Support oder detaillierte Kenntnisse darüber verfügen, wie Sie Ihre Systeme am Laufen halten können, können Sie alles in der Cloud betreiben. Wenn Sie Ihre Daten nicht in der Cloud haben möchten, können Sie eine lokale Version über ein privates Netzwerk ausführen, die Ihnen alle Arten von Funktionen zur Aggregation und Korrelation von Daten bietet, um auf sichere, kostengünstige Weise ein umfassendes Verständnis dafür zu ermöglichen, was in Ihrem System passiert.\n\nGrundsätzlich kann Ihre Daten jetzt am Rand, in der Cloud, on Premises oder in dem, was Cisco Nebel nennt, vorhanden sein. Es spielt keine Rolle mehr, wo Ihre Anwendung ausgeführt wird, sodass Sie das kostengünstigste Modell verwenden können. In industriellen Bereichen gibt es zum Beispiel enorme Einsparungen durch das Fehlen von kabelgebundenen Komponenten oder durch die Verwendung einer privaten LTE-Struktur anstelle von WiFi-Zugangspunkten. Wenn es gelingt, diese Art von Kosten zu senken, wird dies zu hochwertigen Daten führen. Diese Barrieren von Kosten und physischer Konnektivität sind das, was bisher für das Aufkommen des Internets der Dinge fehlte, wie es jeder vorhergesagt hat.\n\nAnna sagt voraus, dass die nächsten zwei Jahre für Anwendungen außerhalb der Kontrolle aufgrund von 5G unterschiedlich sein werden. Ein einfaches Beispiel ist, dass jemand in der Industrie den regulären PC aus dem Büro nehmen und auf den Werkstattboden bringen könnte, um dort alles zu erledigen. 5G wird verändern, was in Bezug auf Steuerungen und die Steuerung von Robotern und Maschinen über ein drahtloses Netzwerk in den nächsten fünf Jahren möglich ist. Die nächste Ebene der Transformation wird sein, dass man Steuerung über ein drahtloses Netzwerk durchführen und dabei niemanden gefährden kann. Dies erfordert eine umfangreiche Validierung und strenge Überprüfung, aber es ist in greifbarer Nähe.\n\nAuch wird es spannend sein, was Ihr bevorzugter Cloud-Service-Anbieter und Telekommunikationsunternehmen gemeinsam tun werden, um das Mögliche in Bezug auf Dienstleistungen zu verändern.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Anna Scott"],"link":"/episode-EDT62-de","image":"./episodes/edt-62/de/thumbnail.jpg","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect, Intel, diskutiert mit Dr. Anna Scott, Chief Edge Architect, Intel, über die bahnbrechenden Veränderungen, die 5G mit sich bringen wird, sowohl im Bereich Edge-Computing als auch im Bereich Künstliche Intelligenz."},{"id":136,"type":"Episode","title":"Inspirierender IT-Kulturwandel","tags":["organizationalchange","change","people","covid","cio","california"],"body":"\r\n\r\nAmy diente über fünf Jahre lang als CIO des Bundesstaates, ernannt von Gouverneur Brown und fortgesetzt unter Gouverneur Newsom. Sie arbeitete als Technologin im öffentlichen Sektor für zwanzig Jahre in verschiedenen Bereichen von Umweltwissenschaften über CalPERS bis hin zu Gesundheits- und Sozialdiensten, bevor sie zur CIO ernannt wurde.\n\nAm 13. März 2020 leitete Amy eine Besprechung, bei der ironischerweise die Notwendigkeit diskutiert wurde, das Ziel einer Breitbandversorgung für alle Kalifornier zu beschleunigen, als die COVID-Schließung angeordnet wurde. Sofort danach musste der Staat über 200.000 Angestellte ins Homeoffice überführen und Fernunterricht einführen. Über Nacht wurde das Breitbandproblem zu einer dringenden Angelegenheit im Umgang mit dem Tagesgeschäft. Der Staat begann mit einer Fähigkeit von 5% für das Homeoffice und erreichte innerhalb eines Monats eine Fähigkeit von 92%.\n\nDies war die größte operative Hürde der Pandemie. Die Menschen mussten sich daran gewöhnen, von zu Hause aus zu arbeiten, und es gab einen Mangel an Vorräten, was es noch schwieriger machte. Die gesamte Veränderung lief jedoch relativ reibungslos ab. Die Menschen waren verständnisvoll und flexibel und ermöglichten eine kontinuierliche Verbesserung.\n\nDies ist ein Prozess, der normalerweise Jahre in Anspruch nehmen würde. Der Unterschied besteht darin, dass die Leute ohne zu zögern gesagt haben: \"Das muss getan werden\" und anstatt alles zu überanalysieren, in Bewegung gekommen sind. Jeder hat einfach Dinge geschehen lassen, auch wenn sie nicht perfekt waren.\n\nCybersicherheit stellte ebenfalls eine Hürde dar, da Personen, die von zu Hause aus arbeiteten, dieselben guten Praktiken anwenden mussten wie im Büro, und das erforderte eine gewisse kulturelle Veränderung.\n\nEine überraschende Veränderung war, dass der Wunsch nach Verwendung eines evidenzbasierten Entscheidungsprozesses über Nacht beschleunigt wurde. Es klingt ein wenig wie ein Oxymoron, da nicht alles überanalysiert werden konnte, aber andererseits richtete sich der Fokus auf jene kritischen Entscheidungen, die eine evidenzbasierte Unterstützung erforderten. Die unwichtigen Dinge wurden beiseitegeschoben, weil es nicht genug Kapazität gab, um alles zu erledigen.\n\nDer Staat lernt immer noch, wie die Belegschaft sich in Zukunft präsentieren wird. Die Telearbeit bleibt auf absehbare Zeit bestehen und einige könnten dauerhaft werden, da die Produktivität gestiegen ist. Darüber hinaus müssen Positionen, die sich nicht gut mit der Telearbeit vertragen, in eine sichere Umgebung zurückkehren. Das Wohlbefinden der Mitarbeiterinnen und Mitarbeiter ist ebenfalls ein Faktor in der neuen Dynamik.\n\nRegierung und andere Organisationen müssen einen Ausgleich zwischen den hoch taktischen letzten 18 Monaten und der strategischen Planung für die Zukunft finden. Amy behält die strategische Perspektive im staatlichen Plan für das Jahr 2023 als \"Nordstern\" bei, konzentriert sich aber auch auf aktuelle schrittweise Verbesserungen zur Bereitstellung von Dienstleistungen. Es herrscht das Verständnis, dass die Dinge nicht perfekt sein werden, wenn der Staat zu einer neuen Normalität zurückkehrt.\n\nAmy hat zwei kulturelle Ziele für ihre Abteilung, die sich aus der Pandemie ergeben: Personen in Support-Positionen müssen in der Lage sein, ihre Arbeit mit den positiven Veränderungen und Auswirkungen in Verbindung zu bringen, um die Moral aufrechtzuerhalten. Außerdem sollte die Arbeitsumgebung visionärer werden und die Mitarbeiter an die strategischen Leitsterne erinnern.\n\nUm einen positiven kulturellen Wandel aufrechtzuerhalten, müssen Führungskräfte das Vorbild sein und das Wohlbefinden der Mitarbeiter in ihren Entscheidungen berücksichtigen. Dies beinhaltet Flexibilität bei der Art und Weise, wie sie am besten arbeiten, und ihnen das Vertrauen zu geben, Entscheidungen zu treffen, um sich erfüllt und gestärkt zu fühlen. Und falls etwas nicht perfekt läuft, einfach abstauben und weitermachen.\n\nAmys Rat für andere Staats-CIOs wäre, viel Proof-of-Concept-Arbeit zu leisten, die Ärmel hochzukrempeln und Dinge auszuprobieren, und offen für verschiedene Lösungsansätze zu sein. Die Ergebnisse des Proof-of-Concepts informieren die Entscheidungsträger besser als nur die Analyse von Dingen zu versuchen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Amy Tong"],"link":"/episode-EDT63-de","image":"./episodes/edt-63/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect bei Intel, diskutiert mit Amy Tong, CIO des Bundesstaates Kalifornien, im Zuge der COVID-Pandemie über die inspirierende kulturelle Veränderung."},{"id":137,"type":"Episode","title":"Folge und sichere den Bit-Rand zur Cloud.","tags":["edge","compute","multicloud","cybersecurity"],"body":"\r\n\r\nIn Episode #2 erklärte Greg die komplexen Fragen des Sammelns, Bewegens und Analysierens von Daten in einer Umgebung des Verteidigungsministeriums (DOD), die auch edge-Geräte auf Schiffen, Flugzeugen und sogar Kämpfern enthält, sowie die Notwendigkeit, diese Daten schnell für zeitnahe, umsetzbare Entscheidungen zu bewegen und zu analysieren. Die Rolle von Intel besteht darin, die Prozesszyklen zu verkürzen und Engpässe im Datenfluss mit seinem Technologieportfolio zu entlasten. Es geht darum, Technologie mit der Mission abzustimmen.\n\nEin aktueller Architekturtrend besteht darin, die Berechnung, Verarbeitung und Intelligenz voranzutreiben, um die Daten zu erreichen, anstatt die Daten zur Backend zu bewegen. Diese Anpassung ermöglicht die beste Möglichkeit, Echtzeitantworten und Intelligenz zu erhalten. Dieser Ansatz ist ein anderes Betriebsmodell mit herausfordernden Fragen.\n\n## Heterogene Hardware\n\nIn der grundlegenden Infrastruktur, wie stellen Sie die richtige Rechenleistung, Speicher, Arbeitsspeicher und Netzwerkfähigkeiten bereit, um Analysen am Rand zu ermöglichen und die Verarbeitung dort zu steuern, wo Sie sie benötigen? Wie verwalten Sie diese Anwendungen und Daten? Wie führen Sie Qualität und Pflege der Daten weiter oben in der Nahrungskette ein? Sicherheit ist ein grundlegendes Element jeder Infrastruktur und nun muss Daten am Rand durchgehend geschützt werden.\n\nDie Branche muss zu einer heterogeneren Berechnung voranschreiten, damit die zugrunde liegende Software auf verschiedenen Hardwareplattformen laufen kann und man sich nicht an eine spezifische Software- oder Hardware-Struktur bindet. Ein Teil der Schönheit dessen, was Intel seit Jahren geleistet hat, besteht darin, Software-Portabilität anzubieten: das offene Framework, x86 und andere zugrunde liegende Systeme.\n\nEin wichtiger Faktor für die Entwicklung von Fähigkeiten bei Intel ist das Verständnis der Arbeitslast und des Datenflusses, anstatt sich auf Modernisierung zu konzentrieren, wie zum Beispiel das Verlagern von Dingen in die Cloud. Es geht nicht darum, wie Sie IT beschaffen (in der Cloud oder vor Ort), sondern darum, den Geschäftsfluss zu optimieren, um von der Art und Weise zu profitieren, wie die Daten fließen und was sie tatsächlich tun.\n\n## DevOps in Not zu Hilfe\n\nEin weiterer kritischer Aspekt dieser neuen Lösungen ist die Bereitstellungszeit. DevOps ist im gesamten Verteidigungsministerium DOD zunehmend wichtig geworden und die Bemühungen zur Entwicklung gemeinsamer DevOps-Plattformen haben dramatisch zugenommen. Um Code einmal zu schreiben und überall auszuführen (in der Cloud, On-Premise oder an der Edge), verwenden Organisationen den x86-Chipsatz und Intels oneAPI, um komplexe Dienste über das gesamte Ökosystem bereitzustellen. Mit Hilfe von oneAPI können Entwickler beispielsweise komplexe Analysen oder KI-Jobs schreiben, die auf einer GPU, einer CPU oder sogar auf FPGAs mit demselben Code-Basis ausgeführt werden. Entwickler können dann diese komplexen Systeme in ihrem Rechenzentrum testen und sicherstellen, dass sie sich genauso verhalten, wenn sie im Feld bereitgestellt werden, was die Bereitstellungszeit verkürzt.\n\nIn der Lage zu sein, eine Kombination aus einer flexiblen DevOps-Umgebung zu nutzen, die Containerisierung und Software-Frameworks wie oneAPI nutzt, gibt Programmierern die Abstraktion, die sie benötigen, aber mit eingebauten Hard- und Softwareoptimierungen. Ein wesentlicher Unterschied hierbei ist, dass wir einen Weg gefunden haben, die gesamte Leistung der Hardware zu nutzen und die Software zu optimieren, damit eine Anwendung immer noch Leistungsverbesserungen erzielen und weiterhin von spezialisierter Hardware profitieren kann.\n\nDas Container-Ökosystem bietet auf Systemebene eine Abstraktion, so dass Dinge nicht fest codiert sein müssen. Es ermöglicht die Skalierung nach oben oder unten basierend auf der Service-Belastung und Kapazität. Zum Beispiel, wenn eine Funktion an der Kante, in einem tragbaren Rechenzentrum oder zurück in der Cloud läuft und die Konnektivität oder ein Ressourcen verloren geht, kann die Mission dennoch durchgeführt werden. Diese intermittierende Konnektivität ist in einer Umgebung des Verteidigungsministeriums (DOD) entscheidend.\n\n## Edge-Lösungen\n\nViele Leute denken bei Intel an einen Silizium- oder Hardware-Anbieter, aber das Unternehmen hat Lösungen und Referenzarchitekturen im Edge-to-Cloud-Ökosystem entwickelt. Ein Beispiel für Fortschritt ist ein maritimes taktisches Gitter, bei dem die Verarbeitung an der Kante, auf Schiffen, Flugzeugen und an Land stattfindet. Die Anwendungen müssen über das gesamte Ökosystem hinweg funktionieren, und das lässt sich nicht nur mit Hardware bewerkstelligen. Intel hat seine Silizium- und Sicherheitsfunktionen genutzt und auf Basis eines Container-Ökosystems komplexe Anwendungen mit mehreren Microservices entwickelt, die sowohl bei verbundenen als auch bei getrennten Einheiten ausgeführt werden können.\n\nEin weiteres Beispiel für Edge-Architekturen ist das Verfolgen von Objekten über Sensornetze hinweg, selbst bei Lücken im Sensornetz. Durch die Durchführung von Sensordatenfusion, Datensynthese und multidisziplinärer Analyse ist es möglich, Objekte über verschiedene Szenen und Sensorarten hinweg zu verfolgen. Dabei kann die Skalierung angepasst werden, um eine mobile Sensorplattform zu schaffen, die Edge-Sensoranalyse und Fusion durchführen kann. Anschließend kann sie auch an eine verteilte Gruppe von Knoten übergeben werden, die zusammenarbeiten, um ein Objekt über all diese verschiedenen vorhandenen Sensoren hinweg zu verfolgen.\n\nZum Beispiel, wenn Sie ein Objekt verfolgen und es sich zwischen Sensoren bewegt, verlieren Sie es für einen Moment. Was diese Intelligenz ermöglicht, ist das Verbinden dieser beiden Feeds und das Verfolgen der blinden Flecken. Dies funktioniert nicht nur mit homogenen Sensortypen, sondern auch mit verschiedenen Arten von Sensoren wie Bewegungssensoren, RF-Sensoren und IR-Sensoren. Wenn Sie also das Video verlieren, aber immer noch ein RF-Signal haben und dann das Video wieder aufnehmen, können Sie den vollständigen Pfad des Objekts anzeigen. Früher erforderten solche Analysen das Verschieben von Daten zum Rechenzentrum, um diese komplexen Interaktionen zu verarbeiten. Jetzt können wir dies am Rande des Sensor-Netzwerks durch Mikrodienste am Rand durchführen.\n\n## Flexibilität bei der Bereitstellung.\n\nEin vorteilhafter Ansatz besteht darin, einmal zu bauen und dieselbe Architektur und Software für verschiedene Missionen mit analytischen Anforderungen zu verwenden. An Orten, an denen leistungsstarke Rechenkapazität an der Grenze (des Netzwerks) vorhanden ist, kann die Skalierung erfolgen und die volle Leistungsfähigkeit der Hardware ausgeschöpft werden. Andere Plattformen können begrenzte Rechenkapazitäten haben und können kleinere Mikrodienste ausführen, die nur einen Teil der vollständigen Lösung bereitstellen. Neue Missionseigenschaften und Dienste können schnell bereitgestellt werden, indem diese Mikrodienste einmalig für mehrere Anwendungsfälle aufgebaut werden.\n\n## Die Sicherheit am Rand\n\nKomplexität ist manchmal der schlimmste Feind der Sicherheit, aber grundlegende Sicherheitsprinzipien können auch Daten und Anwendungen am Rand sichern. Anstatt den alten Ansatz zu verfolgen, den Server zu schützen und zu hoffen, dass alles dort bleibt, ist es entscheidend zu verstehen, wohin die Daten fließen und an welchen Stellen sie existieren und sie zu schützen, egal auf welchem System sie ausgeführt werden. Risikobasierte Ansätze und Konzepte wie Zero Trust gewinnen an Bedeutung, da sie eine systemunabhängige Sichtweise der Sicherheit einnehmen.\n\nZusammengefasst bedeutet Zero Trust standardmäßiges Ablehnen. Niemand wird eingelassen, es sei denn, es ist erforderlich, und dann nur für die Dauer der notwendigen Aktion. Wenn zeitliche Zugriffskontrollen mit einem risikobasierten Ansatz zum Schutz der Daten während ihres gesamten Lebenszyklus kombiniert werden, resultiert dies in der Fähigkeit, die Daten unabhängig davon, wo sie sich befinden und wer darauf zugreift, zu schützen. Diese Technik ist eine der Möglichkeiten, um diese hochkomplexen Umgebungen abzusichern.\n\nDie praktische Maßnahme in diesen Ökosystemen besteht darin, eine Richtlinie anzuwenden, die Kontrollen nutzt, die das Risiko eines bestimmten Systems zu einem bestimmten Zeitpunkt erfüllen, und sie dann kontinuierlich in Echtzeit überwacht und aktualisiert, um der sich ständig verändernden cyberbedrohenden Welt gerecht zu werden. Verwenden Sie die technischen Kontrollen, die die Hardware- und Softwarefähigkeiten bereits bieten, wie Secure Boot, Hardware Root of Trust mit TPM-Modulen oder SGX-Speicherkeys, Verschlüsselung usw.\n\nEs gibt kein Wundermittel, das man kaufen kann, um eine umfassende Sicherheitslösung in diesen komplexen Ökosystemen bereitzustellen. Es geht darum, Sicherheitsrichtlinien zu erstellen und durchzusetzen, während Bedrohungen entstehen und sie in großem Umfang einzusetzen, indem man Hardware, Software und die notwendigen Prozesse nutzt, um das Bit sicher zu halten, während es vom Rand bis hin zum gesamten Ökosystem fließt.\n\nDas DevOps-Framework bietet effektive Mechanismen, um Sicherheit für alle Vermögenswerte im Ökosystem zu gewährleisten. Container sollten mit Instrumenten ausgestattet sein, um Sicherheitskontrollen und -richtlinien durchzusetzen. Sicherheit muss in den DevOps-Prozess selbst integriert werden, da die Sicherheitsumsetzung durch verschiedene Entwickler zu einer erhöhten Komplexität und Variabilität im System führt. Der Entwickler muss über die erforderlichen Fähigkeiten und Einschränkungen verfügen, auf deren Grundlage er arbeiten muss.\n\nEntwickler müssen immer noch Sicherheitsarbeit leisten, wie zum Beispiel sicherstellen, dass sie geeignete Sicherheitstools für die spezifische Bedrohungsumgebung verwenden, aber der Hauptaufwand, die Komplexität, sollte in die DevOps-Architektur abstrahiert werden.\n\nEines der kritischen Bereiche in einer komplexen Theaterumgebung ist das Management von Edge-Geräten wie Überwachung und Aktualisierung der Firmware. Es erfordert Innovation, um sicherzustellen, dass diese Geräte gesichert sind und die Datensicherheit, Profile und Richtlinien in den Systemen im großen Maßstab unterstützen können. Aus diesem Grund wächst das Ökosystem derzeit stark an, um dieser Herausforderung gerecht zu werden.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Clifton","Steve Orrin"],"link":"/episode-EDT64-de","image":"./episodes/edt-64/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect, Intel, diskutiert die Nachverfolgung des Bits mit Greg Clifton, Director, DOD und Intelligence, Intel, in einem Folgegespräch zur Episode Nr. 2. Steve Orrin, CTO Federal, Intel, bringt seine Expertise in Bezug auf Sicherheit in komplexen Edge-to-Cloud-Umgebungen ein."},{"id":138,"type":"Episode","title":"Beschleunigung der Cloud-Adoption bei gleichzeitiger Verringerung von Komplexität und Kosten.","tags":["cloudmigration","cloud","compute","process","multicloud","aiven","cloudcost","technology"],"body":"\r\n\r\nDavid war früher ein Anwendungsentwickler und beschreibt sich selbst als einen überengagierten Ingenieur. Diese Erfahrung zusammen mit seiner Arbeit bei verschiedenen Unternehmen mit AWS und GCP ermöglicht es ihm, die Feinheiten des Cloud-Betriebs zu verstehen, wie man bei Bedarf skalieren kann und wie man vielen der Herausforderungen beim Umzug in die Cloud begegnet.\n\nViele Organisationen wechseln in die Cloud, beschleunigt durch die Folgen der COVID-Pandemie. Dabei gehen große und kleine Unternehmen unterschiedliche Wege. Kleinere Unternehmen wechseln zur Cloud und übernehmen sie so schnell wie möglich, indem sie neue Tools und bewährte Verfahren nutzen. Allerdings gibt es immer noch viele Umzüge (\"lift and shift\") in das Rechenzentrum. Während des Umzugs profitieren sie jedoch oft nicht von den Vorteilen der Cloud. So sind sie vielleicht dabei, eine digitale Transformation zu erreichen, aber sie müssen sich fragen: Was sind die nächsten Schritte?\n\n## Kostensenkung\n\nEs gibt viele Dinge, die Sie tun können und tun sollten, um Kosten zu reduzieren und unerfreuliche Überraschungen zu vermeiden, wie beispielsweise die Kosten, die mit Zombie-VMs einhergehen. In der Cloud, wenn Sie einen Server benötigen, klicken Sie einfach auf einen Button und Sie haben ihn. Doch wenn Sie vergessen, dass er läuft, könnte es einen Monat später eine Gebühr von $10.000 für die Server geben. Ein weiterer Punkt, der berücksichtigt werden sollte, ist der Netzwerk-Datenverkehr, der sich summieren kann, insbesondere bei Übertragungen zwischen Regionen, wenn Sie Terabytes oder Petabytes pro Monat verschieben.\n\nAusgabenbenachrichtigungen können dazu beitragen, diese Probleme zu mildern.\n\nKostenprognosen können schwierig sein aufgrund von Problemen wie dem freien Zugriff, aber das Wiederherstellen der Daten ist teuer. Auch das Löschen von Daten verursacht Kosten. Daher müssen Organisationen über die Einschränkungen nachdenken, wie viele Daten sie speichern möchten, die Art der Operation und die Nutzung.\n\nFür die Kostenoptimierung ist es wichtig, dass das Engineering-Leadership beachtet, dass es einen Punkt gibt, an dem Rechenleistung und gemanagte Dienste nicht mehr so teuer sind; die teuerste Ressource sind Ihre Mitarbeiter. Wie optimiert man also die Effizienz Ihrer Entwickler? Sie sollten nicht mit wiederholenden und routinemäßigen Aufgaben beschäftigt werden.\n\nAutomatisierung ist hier der Schlüssel, insbesondere bei wiederholten Aufgaben mit hoher Häufigkeit sowie bei gemanagten Dienstleistungen.\n\nDer Lebenszyklus jeder Firma erreicht einen Punkt, an dem sie entscheiden müssen, ob sie in alle Operationen investieren und diese selbst übernehmen, ein DevOps-Team einstellen oder auf verwaltete Serviceprovider zurückgreifen möchten. Die Einstellung von internem Fachpersonal ist teuer. Der Wendepunkt ist, wenn sie entscheiden, wie gut die Cloud für sie skalierbar ist. Es ist in der Regel am besten, dass Unternehmen die Gesamtbetriebskosten berücksichtigen, sich auf ihre Wettbewerbsvorteile konzentrieren und für andere Services verwaltete Dienste nutzen.\n\n## Vereinfachung\n\nEine der Komplexitäten der Cloud ist, dass du in der Lage sein musst, deine Umgebung mit zuverlässigen und wiederholbaren Bereitstellungen neu zu erstellen. Das bedeutet nicht, dass du die Website aufrufst und darauf klickst; du bereitstellst deine Umgebung zum Beispiel aus einem Skript, einer YAML-Datei oder TerraForm. Du musst in der Lage sein, sie schnell hochzufahren und wieder herunterzufahren.\n\nSie müssen in der Lage sein, einen Server, der verschmutzt wurde, zu deaktivieren und ihn zuverlässig in einem sauberen Zustand wiederherzustellen. Wenn jemand Log-Dateien verschiebt, Konfigurationen anpasst, versehentlich eine Datenbank löscht oder es zu einem Ransomware-Angriff kommt, müssen Sie in der Lage sein, Ihre Umgebungen oder Teile Ihrer Umgebung zu reproduzieren, um die Ausfallzeiten zu reduzieren.\n\nDas Wesentliche ist, einen Plan basierend darauf zu haben, wie schnell du dich erholen musst und wie viele Daten du benötigst, um fortzufahren. Es macht Sinn, dass einige Branchen in einen Plan investieren, um schnell in eine andere Region umzuziehen. Du musst abwägen, wie viel du bereit bist, in Wiederherstellungszeit zu investieren.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","David Esposito"],"link":"/episode-EDT65-de","image":"./episodes/edt-65/de/thumbnail.jpg","lang":"de","summary":"In dieser Folge, Teil 1 von 2, spricht Darren mit David Esposito, Global Solution Architect, von Aiven darüber, wie man die Cloud-Adoption beschleunigen und gleichzeitig Komplexität und Kosten reduzieren kann."},{"id":139,"type":"Episode","title":"Das Verwalten von Komplexität in der Cloud","tags":["data","devops","compute","technology","process","devsecops","cybersecurity","aiven"],"body":"\r\n\r\n## Vereinfachung der Komplexität\n\nWelche Geschäftspraktiken sollten bei der Verwaltung von Cloud-Ressourcen eingeführt werden, um die Komplexität zu verringern? Zunächst einmal müssen Sie über Infrastrukturcode und automatisierte Bereitstellungen verfügen. Alles, was als nächstes passiert, ist ein Gespräch, um festzulegen, welche Risiken für das Unternehmen bestehen, wie zum Beispiel Ausfälle und Ausfallzeiten, wie man diese Risiken mindern kann und wie viel in diese Investitionen gesteckt werden soll.\n\nFür einige Unternehmen ist es entscheidend, die höchstmögliche Betriebszeit zu haben. Im Gesundheitswesen zum Beispiel müssen möglicherweise alle Daten an mehreren Standorten gesichert sein und alle Dienste verfügbar sein, da lebenswichtige Entscheidungen auf diesen Daten basieren könnten. Aber wenn Sie im April im E-Commerce tätig sind, können Sie möglicherweise etwas Zeit in dieser kranken Region verbringen, da die Folgen nicht so gravierend sind.\n\nIm Gesundheitswesen ist es sinnvoll, auf Multi-Cloud umzusteigen, obwohl dies die Komplexität erhöht. Andere Branchen, die von Multi-Cloud profitieren können, sind stark regulierte Bereiche wie Finanzdienstleistungen, Öl und Gas oder Bundesbehörden. Wenn es zu einem Sicherheitsvorfall kommt, können sie eine Cloud ausschalten und an anderer Stelle einsetzen.\n\nBei der Betrachtung von Multi-Cloud-Lösungen ist zu beachten, dass es bestimmte Komplexitäten aufgrund von Unterschieden in den APIs gibt. Wenn Sie sich in GCP befinden und wissen, wie Sie alles bereitstellen, aber darüber nachdenken, zu AWS zu wechseln, müssen Sie die Plattform und den Bereitstellungsprozess erneut erlernen und möglicherweise sogar einen Teil des Codes neu schreiben. Aus diesem Grund ist die Containerisierung beliebt, da Sie überall bereitstellen können. Es wird jedoch komplexer mit anderen Diensten. Beispielsweise weisen Postgres in AWS im Vergleich zu GCP unterschiedliche Konfigurationen auf, möglicherweise mit unterschiedlichen Patch-Versionen, unterschiedlichen Netzwerkeinstellungen und unterschiedlichen Erweiterungen. Daher sind Dienste in einer Cloud nicht dasselbe wie in einer anderen.\n\nDarüber hinaus sind Containerumgebungen nicht für jede Situation ideal, und eine ideale Situation wird umso wichtiger, je größer das Ausmaß wird. Wenn Sie Kafka verwenden und 1.000 Nachrichten pro Sekunde verarbeiten, ist Ihr eigener Behälter in Ordnung. Aber wenn Sie anfangen, mehr als eine Million Nachrichten pro Sekunde, also ein Gigabyte, zu verarbeiten, ändert sich die Situation.\n\n## Sicherheit\n\nDie Sicherheit in der Cloud ist ein anderes Tier als die Sicherheit in Ihrem eigenen Rechenzentrum. Das Wichtigste, woran man denken muss, ist, dass Daten außerhalb Ihrer eigenen Grenzen übertragen werden. Sie müssen angemessene Sicherheitskontrollen implementieren, um sicherzustellen, dass Daten nicht durchsickern oder unbefugten Zugriff haben, und sicherstellen, dass die Anforderungen zur Einhaltung erfüllt sind. Dies erfordert Training, da die Sicherheitsmaßnahmen für Rechenzentren vor Ort nicht direkt auf die Cloud übertragen werden können. Zum Beispiel sind S3-Container in Ruhe verschlüsselt, jedoch nicht, wenn Sie programmatischen Zugriff über öffentlich zugängliche Buckets haben.\n\nZusätzlich müssen Sie wissen, was die Dienstleister tun. Zum Beispiel betreibt Aiven Datenbanken auf GPUs EC2, GCP Compute oder Azure. Wenn sie auf die Festplatte speichern, gibt es eine Verschlüsselungsebene, die auf Cloud-Ebene vom Cloud-Anbieter durchgeführt wird, sodass die Daten ruhend verschlüsselt sind. Dann führt Aiven eine weitere Verschlüsselungsebene mit ihren eigenen verwalteten Schlüsseln durch, sodass die Daten ruhend doppelt verschlüsselt sind und alle Daten während der Übertragung verschlüsselt werden. Alles, was zum Server gesendet wird, ist für diesen bestimmten Server verschlüsselt, und wenn ein Server außer Betrieb genommen wird, wird er sowohl mit dem Schlüssel von Aiven als auch mit dem Schlüssel des Cloud-Anbieters gesperrt.\n\n## Aiven\n\nDavid trat dem Lösungsarchitekturteam von Aiven bei, als er sah, dass das Unternehmen bessere Cloud-Lösungen als alle anderen anbot, basierend auf seiner vorherigen Erfahrung mit verwaltetem Kafka und anderen Open-Source-Technologien. Die Mission von Aiven besteht darin, das Leben von Entwicklern zu verbessern. Sie bieten eine kostenlose Testversion an, mit produktionsfertigem Kafka innerhalb von fünf Minuten. Darüber hinaus verfügen sie über Hilfsartikel und Tutorials, die Sie durch den Prozess führen, sodass Sie innerhalb von zehn Minuten eine Kafka-Nachricht in der Cloud mit cloud-nativen Best Practices senden können.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","David Esposito"],"link":"/episode-EDT66-de","image":"./episodes/edt-66/de/thumbnail.jpg","lang":"de","summary":"In dieser Folge, Teil 2 von 2, führt Darren sein Gespräch mit David Esposito, Global Solution Architect, von Aiven fort, über die Beschleunigung der Cloud-Adoption bei gleichzeitiger Reduzierung von Komplexität und Kosten."},{"id":140,"type":"Episode","title":"5G Vergangenheit, Gegenwart und Zukunft","tags":["5g","comms","wifi6"],"body":"\r\n\r\nLeeland ist seit 20 Jahren in der Telekommunikationsbranche tätig. Direkt nach dem College wurde er vom US-Verteidigungsministerium angeworben, nachdem sie von seiner Arbeit zur Anwendung von Bluetooth-Multiplexing in einem Automobil beeindruckt waren, wodurch die Größe des Kabelbaums reduziert wurde. Anstatt mit Autos zu arbeiten, wie er ursprünglich geplant hatte, endete er damit, beim Verteidigungsministerium fortschrittliche drahtlose Technologien für Soldaten zu entwickeln.\n\nZu dieser Zeit befand sich die Mobilfunktechnologie im Übergang von 2G zu 3G. Sie wurde zu diesem Zeitpunkt nicht WiFi genannt, sondern drahtloses LAN, und es gab eine aufstrebende Technologie namens Bluetooth, die sie versuchten einzusetzen, um den Soldaten die Möglichkeit zu geben, Informationen innerhalb des Kampfraums zu sammeln.\n\nLeeland verließ das Verteidigungsministerium drei Jahre später, um praktische Arbeit mit der Technologie zu verrichten und ein besseres Verständnis zu gewinnen. Er begann für Sprint zu arbeiten und blieb 13 Jahre lang in der Telekommunikationsbranche, wo er Technologie für drahtlose Netzwerke entwarf, bevor er 2017 zu Intel wechselte, um an 5G zu arbeiten.\n\nBis Mitte der 90er Jahre gab es nur wenig Mobilfunknutzung. \"Ziegelstein\"-Telefone aus den 80er Jahren waren riesig wegen ihrer Akkugröße und sie waren teuer. Sie entwickelten sich in den frühen 90er Jahren zu \"Taschen\"-Telefonen weiter, die besser waren, aber man konnte nur hoffen, dass sie funktionierten, um einen Anruf zu tätigen.\n\nAls 2G eingeführt wurde, wurden neue Funktionen entwickelt, die auf den Wünschen der Kunden basieren, wie zum Beispiel Textübertragung und Spiele. Das Gerät konnte eine geringe Datenübertragungsrate mit TDMA-Technologien (Time Division Multiple Access) und GSN (General Switched Network) erreichen. In den mittleren 90er Jahren traten Unternehmen wie Sprint auf den Markt und die Nutzung von Mobiltelefonen wurde breiter. Anfang der 2000er Jahre gab es einen bedeutenden Wechsel mit der Erwartung der Kunden, dass das Internet auf Mobiltelefonen genauso verfügbar sein sollte wie auf Laptops.\n\n3G brachte nominale Breitbandfähigkeiten, bei denen man das Internet einigermaßen nutzen konnte, aber die Hauptfähigkeit bestand darin, Bilder zu übertragen. Der Name \"3G\" wurde zu dieser Zeit geprägt und das ist auch der Zeitpunkt, an dem der Standardisierungskörper 3GPP ins Spiel kam.\n\nIm Jahr 2008 begannen Telekommunikationsunternehmen damit, die Bereitstellung von Infrastruktur zu betrachten, die nicht auf großen Mobilfunkmasten beruhte, sondern auf einem verteilten System, bestehend aus einem Knotenpunkt von Basisbandeinheiten mit über Glasfaser an Telefonmasten angeschlossenen Antennen. Dies leitete den Übergang zu echtem Breitband, von 3G zu 4G, ein.\n\nDiese Technologie sollte für den Wirtschaftsaufschwung von 2010 bis 2020 Anerkennung erhalten, denn mit 4G in der Hand entwickelten und gediehen Unternehmen wie Amazon und Netflix.\n\n5G befreit Dienstleistungen von einem monolithischen RAN-\"Gefängnis\" und öffnet das Feld, da 5G offene Architekturen mit einer softwaredefinierten Basis bietet. Software-Stacks können jetzt entwickelt und in eine umfassende Softwarelösung integriert werden. Da es nicht an eine monolithische Architektur gebunden ist, kann 5G Dienste und eigenständige private Netzwerke bereitstellen.\n\nDiese enorme Flexibilität wird es Telekommunikationsunternehmen und deren Anbietern ermöglichen, ihre Dienstleistungen zu verbessern und verschiedene neue Fähigkeiten anzubieten, einschließlich des Zugangs zu Edge-Computing.\n\nWas kommt als nächstes? Leeland sieht eine Bewegung weg von \"G\", da dies der Weiterentwicklung der Technologie nicht genug Anerkennung gibt, da es keine wirklichen Partitionen mehr gibt, wer Netzwerke bereitstellen kann. Die Technologie und Anwendungsfälle sind vielfältig.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown"],"link":"/episode-EDT67-de","image":"./episodes/edt-67/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect, Intel, spricht mit Leeland Brown, Technischer Direktor für 5G, Intel Federal, über die Vergangenheit, Gegenwart und Zukunft von 5G, wobei der Schwerpunkt auf dessen Einsatz im Verteidigungsministerium liegt. Teil 1 von 2."},{"id":141,"type":"Episode","title":"5G in Verteidigung","tags":["5g","dod","compute","technology","cloud","edge","comms"],"body":"\r\n\r\nAn der Wende des Jahrhunderts suchte das Verteidigungsministerium nach Möglichkeiten, 2G-Technologien, drahtlose LANs und Bluetooth in Anwendungen einzusetzen, um Soldaten die Erfassung von Informationen im Schlachtfeld zu ermöglichen. Im Jahr 2021 werden diese Fähigkeiten immer noch angestrebt. Das Verteidigungsministerium beschäftigt sich intensiv mit 5G, da es sich von monolithischen Strukturen wie der Nutzung einer RAN-Architektur entfernt und in softwaredefinierte Architekturen übergeht. Es besteht die Flexibilität, neue Fähigkeiten schneller aufzubauen und auf verschiedene Anwendungsfälle zu skalieren.\n\nDie kommerzielle Seite von 5G wird von Strategien und der Maximierung der Umsatzentwicklung vorangetrieben. Das entspricht nicht unbedingt den Anforderungen der Bundesregierung oder des Verteidigungsministeriums, daher verschwimmen die Grenzen zwischen den beiden Welten, wenn beispielsweise AT&T Verizon Bundesnutzungsmöglichkeiten prüft oder Lockheed Martin mit seinem 5G-Domino-Programm den kommerziellen Raum betrachtet. Der Unterschied liegt darin, wer die Mission versteht und wer den Fokus auf die Gewinnerzielung legt.\n\nDerzeit sind neunzig Prozent der kommerziellen 5G-Implementierungen nicht eigenständige Netzwerke, was bedeutet, dass 5G immer noch mit einem bestehenden 4G evolved Packet Core verbunden ist. Das Verteidigungsministerium interessiert sich für eigenständige Netzwerke mit einem vollständigen 5G-Kern, 5G-RAN und 5G-Geräten. Es kann ein eigenständiges Netzwerk für eine Gruppe von Soldaten geben, zusammen mit Basistationen in kleinem Formfaktor für Fahrzeugplattformen und mehreren geschützten Domänen, sogar Drohnen, die mit irgendeiner Art von 5G-Zugangspunkt und Raumapplikationen eingesetzt werden.\n\nDiese Fähigkeit zur Skalierung über mehrere Anwendungsfälle und ihre unterschiedlichen Arbeitslastarten ist auch auf die kommerzielle Seite anwendbar. Eines der größten Probleme ist jedoch, wer die Frequenz besitzt. Einige freie Frequenzen sind mit begrenzter Reichweite verfügbar, aber diese Bänder, wie beispielsweise ISN-Bänder, sind ungenehmigt und sehr überfüllt. Die Regel besagt, dass man Störungen verursachen und akzeptieren muss und seine Technologie darum herum arbeiten lassen muss, aber dynamisches System-Sharing ist möglich. Für den Bundeseinsatz kann man jedoch nicht an Operationen in den USA gebunden sein, da die meisten Soldaten außerhalb der USA eingesetzt sind.\n\nLeeland sagt voraus, dass 5G in das Leben jedes Einzelnen integriert wird, manchmal ohne dass man es überhaupt bemerkt. Plattformen zur Mustererkennung, wie beispielsweise Gesichtserkennung und autonome Fahrzeuge, werden alle über 5G verbunden sein. Ihr Breitbandzugang wird sich nahtlos in Ihr gesamtes Leben einfügen und nicht nur Ihr Mobiltelefon, sondern auch Ihr Auto und Ihr Zuhause umfassen. Mit der zunehmenden Anzahl von drahtlosen Zugangspunkten in allen Lebensbereichen wird der Bedarf an einem Telefon in Ihrer Hand dramatisch reduziert.\n\nLeeland sagt auch voraus, dass das \"G\" verschwinden wird, wenn sich die Technologie ausbreitet und weiterentwickelt.\n\nUm mehr über 5G zu erfahren, können technisch versierte Personen die 3GPP-Standards konsultieren, um die spezifischen Fälle einzusehen. Man kann eine Spezifikation lesen und den Unterschied zwischen Version 14 und Version 15 verstehen und was das für die Industrie bedeutet. Es gibt auch eine Vielzahl von Informationen im Internet, wie beispielsweise White Papers. Leeland bietet sich ebenfalls als Ansprechpartner an.\n\nLeeland würde gerne den nächsten Schritt als Aufruf zur Handlung sehen, um die Netzwerke durch die Einführung neuer Technologien widerstandsfähiger zu machen. Während Notfällen, sei es Naturkatastrophen oder terroristische Angriffe, ist Widerstandsfähigkeit sowohl für Ersthelfer als auch für Menschen, die einfach versuchen, ihre Familien zu erreichen, unerlässlich.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown"],"link":"/episode-EDT68-de","image":"./episodes/edt-68/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect bei Intel, setzt seine Diskussion mit Leeland Brown, Technischer Direktor für 5G bei Intel Federal, über 5G Vergangenheit, Gegenwart und Zukunft fort, wobei der Schwerpunkt auf dessen Einsatz beim Verteidigungsministerium liegt. Teil 2 von 2."},{"id":142,"type":"Episode","title":"Sicherung des Rechenzentrums über den Netzwerkrand","tags":["cybersecurity","edge","compute"],"body":"\r\n\r\nEin ganzheitlicher Blick auf Sicherheit umfasst die gesamte Infrastruktur, vom Cloud-Umfeld bis hin zur Peripherie. Wenn du den Client nicht absichern kannst, kannst du auch das Unternehmen nicht absichern. Daher ist es wichtig, ehrliche Gespräche in einer verständlichen Art und Weise, ohne Fachjargon, über Sicherheit zu führen.\n\nVorbei sind die Zeiten, in denen ein Mitarbeiter einen Laptop an einen IT-Käfig geschickt bekam, der dann von der IT konfiguriert und geliefert wurde. Manchmal kommen Mitarbeiter überhaupt nicht zum Arbeitsplatz, sodass Geräte an jemandes Haustür geliefert werden müssen. Sie müssen bereitgestellt werden, um sicherzustellen, dass sie sicher sind und nicht manipuliert wurden. Aus Sicherheitssicht ist dies ein herausforderndes Problem. Die Lieferkette ist problematisch. Es gibt weniger Kontrolle über die Geräte und wie sie eingesetzt werden.\n\nDie plötzliche Umstellung der Arbeit auf Homeoffice aufgrund der COVID-Pandemie erforderte eine noch nie dagewesene schnelle Reaktion auf dieses Problem. Es hätte Jahre gedauert, wenn es eine natürliche Entwicklung gewesen wäre, aber die Situation ließ keine andere Möglichkeit zu; Die IT musste sich der Herausforderung stellen.\n\nAls Camille in der IoT-Gruppe arbeitete, versuchten sie, die Konnektivität und Verwaltbarkeit zwischen Geräten zu lösen. Komplexe Ökosysteme wie Windparks, unterirdische Minen und implantierte medizinische Geräte sind schwer zu aktualisieren, wenn die Konnektivität sporadisch ist. Sie versuchten, diese Konnektivitätsprobleme am Rand zu lösen, und als COVID zuschlug, wurde die Schnittstelle zwischen OT und IT plötzlich zum Zentrum aller Unternehmens-IT-Abteilungen.\n\nAls einige von Darrens Kunden mit der besten Lösung zur Verbindung von operativer Technologie (OT) und Informationstechnologie (IT) zu kämpfen hatten, traf COVID ein und einige davon brachen zusammen, was zu Sicherheitsverstößen führte.\n\nViele Menschen haben schnell ein Home-Office-System eingeführt und sich erst später um die Sicherheit gekümmert, abhängig vom Reifegrad der Organisation. Es gab auch einen massiven Wechsel zur Cloud. Derzeit findet jedoch eine gewisse Pendelbewegung statt, aufgrund von Sicherheitsverletzungen wie Ransomware-Angriffen. Diese Verletzungen traten in der Cloud auf, hauptsächlich weil die Menschen die gemeinsame Sicherheitsverantwortung nicht verstanden haben. Jetzt überlegen einige Organisationen zweimal, ob sie ihre kritischen Daten in die Cloud verlagern sollen. Sie mögen ihre Arbeitslasten dorthin verschieben, behalten jedoch die wichtigen Daten lieber zu Hause.\n\nEine weitere kürzliche Veränderung liegt in der Bedeutung der Kund wahrnehmung. Sie stellen vielleicht die richtigen Fragen zur Hardware und den Software-Schutzmaßnahmen, jedoch müssen Sie auch die Wahrnehmung Ihres Kunden berücksichtigen. Wo werden die Daten gespeichert und warum? Wer schützt die Daten und wie geschieht dies?\n\nEin größeres Problem sind Organisationen, die keine dieser Fragen beantworten können. Manchmal wissen sie nicht einmal, wo sich ihre Daten befinden. Diese Organisationen sollten dies als Ausgangspunkt dafür betrachten, was noch getan werden muss.\n\nEinige neue Probleme verschärfen dieses Problem, mit denen die Industrie bisher noch nicht umgegangen ist, wie zum Beispiel Videokonferenzen. Die Aufnahme des Meetings wird auf einem Laptop gespeichert, befindet sich jedoch auch irgendwo in der Cloud. Wer hat Zugriff darauf? Welche Maßnahmen zum Schutz gibt es? Wie lange wird es dort gespeichert sein?\n\nEin Grundsatz der Sicherheit ist das Wissen, ob Ihr Gerät sicher ist. Eine der Herausforderungen, die sich mit der COVID-Situation ergeben haben, ist, dass viele IT-Shops zögern, ihre Systeme zu aktualisieren. Sie möchten kein System herunterfahren, egal ob es sich um einen Server oder einen Client handelt. Das Nichtaktualisieren von Sicherheitspatches ist ein Fehler.\n\nIntels Aufgabe besteht darin, mit Partnern und Mitreisenden zusammenzuarbeiten, um diesen Update zu einer einfacheren und vertrauenswürdigeren Aktivität zu machen, bei der die Menschen das Vertrauen haben, dass es funktioniert und dass dabei nichts schiefgeht. Die Branche hat in den letzten Jahren erhebliche Fortschritte gemacht, um den Update-Prozess systematischer und vorhersehbarer zu gestalten.\n\nEin weiterer Teil des Jobs besteht darin, Menschen zu schulen, um zu verstehen, dass Sicherheit nicht endet, wenn ein Gerät vom Hersteller an einen Kunden versendet wird. Sicherheit erstreckt sich über die gesamte Lebensdauer eines Geräts. Was zu dem Zeitpunkt des Versands als erstklassige Sicherheit galt, ist nach Monaten oder Jahren keine erstklassige Sicherheit mehr. Unternehmen sollten ihre Geräte zwei Mal im Jahr aktualisieren, um sie sicher zu halten.\n\nDie Leute werden nervös, wenn es um Updates geht, weil unerwartete Dinge passieren könnten. Intel validiert im großen Maßstab, um Probleme zu vermeiden, sei es mit Tausenden von Maschinen in ihren Laboren oder mit OEM-Partnern in Laboren auf der ganzen Welt. Die vollständige Validierung stellt sicher, dass die Schutzmaßnahmen funktionieren, um gegen Schwachstellen zu schützen und dem System keinen Schaden zuzufügen. Intel hat erheblich in Partnerschaften und Zusammenarbeit mit seinen Partnern im Ökosystem investiert und Standards in der Branche vorangetrieben und plant, die Benutzererfahrung in Zukunft zu verbessern, indem die Updates ohne Neustart durchgeführt werden können.\n\nKunden darüber aufzuklären, warum Sie sie um ein Update bitten, kann ebenfalls sehr hilfreich sein. Wenn sie verstehen, dass Sie eine Schwachstelle gefunden haben und möglicherweise einem potenziellen Angriff ausgesetzt sein könnten, werden sie wahrscheinlich darauf eingehen wollen.\n\nIm Allgemeinen scheinen die Menschen bereit zu sein, ihre Handys zu aktualisieren, weil sie weniger Angst haben, dass danach etwas nicht funktioniert, aber auf der PC- und Serverseite ist es dennoch eine Herausforderung. Ein Teil davon hat mit Nutzungsmustern zu tun. Obwohl es selten ist, dass Daten nur auf einem Laptop existieren, ist diese Mentalität weit verbreitet. Wenn die Daten in der Cloud auf einem Handy vorhanden sind, wird die Wahrnehmung erzeugt, dass sie immer dort sein werden. Darüber hinaus neigen die Leute dazu, auf Laptops intensivere und ansprechendere Arbeiten zu erledigen als auf Handys, daher sind sie empfindlicher dafür. Sobald sich die Wahrnehmung ändert und die Menschen erkennen, dass die Daten auf ihrem Laptop auch in der Cloud vorhanden sind, werden die Aktualisierungen weitgehend akzeptiert. Also muss die Branche in Wirklichkeit eine Kombination aus technischen Lösungen und Änderungen der Denkweise hinsichtlich Sicherheit vornehmen.\n\nDie Art und Weise, wie sich die Dinge entwickeln, ist ein Hybrid. Neue Lernmodelle wie föderiertes Lernen eilen herbei, um Probleme wie Datenschutzbedenken anzugehen. Anstatt die Daten ins Rechenzentrum zu verschieben, werden die Modelle an ihre Grenzen gebracht. Zum Beispiel ein medizinisches Bildgebungssystem in einem Krankenhaus, wo die Daten verbleiben und das Modell hinzugezogen wird, um sie anzusehen. Wir beginnen dies auch in industriellen Anwendungen zu sehen, wo Maschinen an der Peripherie sind und zum Server werden. Sie behalten die Daten lokal und führen Training und Aktualisierungen dort durch. Es wird also intelligente Geräte an der Peripherie geben, die Dinge mit den Rohdaten tun, und die Frage ist, wie sich das sichern lässt?\n\nEin weiterer Trend in der Sicherheit, der nicht mit der COVID-Pandemie begann, aber zweifellos dadurch beschleunigt wurde, ist der Schutz vor physischen Angriffen. Historisch gesehen lag der Schwerpunkt der Sicherheit auf potenziellen Bedrohungen über das Netzwerk, wie z. B. Netzwerkangriffen oder bösartigen Anwendungen. Da es jedoch IoT-Geräte gibt, die nicht von Menschen überwacht werden, müssen wir die Daten und Geräte vor Manipulation schützen. Das ist eine schwierige Herausforderung.\n\nHeutzutage kann man nicht ganzheitlich über Sicherheit nachdenken, es sei denn, man behandelt auch Datenschutz. Eine Komplikation besteht darin, dass Datenschutz manchmal direkt im Konflikt mit Sicherheit stehen kann. Es gibt keine weltweit vereinbarten Vorschriften oder Standards, daher müssen Organisationen herausfinden, wie sie vorgehen sollen: Den größten gemeinsamen Nenner erreichen oder jeden geopolitischen Anforderung gerecht werden? Um es noch komplizierter zu machen, ändern sich Gesetze und Vorschriften ständig.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Tom Garrison","Camille Morhardt"],"link":"/episode-EDT69-de","image":"./episodes/edt-69/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect, Intel, diskutiert zusammen mit seinen Kollegen von Intel und Podcast-Moderatoren Tom Garrison, VP of Client Security, und Camille Morhardt, Director of Security Innovation and Communication, die Sicherung des Rechenzentrums über den Edge-Bereich."},{"id":143,"type":"Episode","title":"Sicherung des Telearbeiters","tags":null,"body":"\r\n\r\n## Teleworker-Betriebsmodi\n\nDie Betriebsmodelle für Telearbeiter sind nicht neu, ob es sich nun um die Verwendung eines Geräts als Terminal (VDI), Teil des internen Netzwerks oder als Portal für Dienste/Software (SAAS) handelt. Neu ist das schiere Volumen von Mitarbeitern, Auftragnehmern und Partnern, die nun alle vorhandenen Umgebungen nutzen, um ihre Hauptarbeit zu erledigen, anstatt nur eine ausgewählte, überschaubare Anzahl. Die Sicherheit ist bei allen diesen Betriebsmodellen eine Herausforderung. Organisationen müssen herausfinden, wie sie ihre bestehenden Fähigkeiten am besten einsetzen können, um den Zugriff, die Daten, Geräte und Benutzer zu sichern und zu schützen.\n\n## Bedrohungen des Telearbeitens\n\nEinige der Sicherheitsbedrohungen sind solche, die immer schon bei Telearbeit vorhanden waren: die Sicherheit des Endgeräts und der Daten, des Zugangspunkts und des Zugriffs auf Unternehmensdienste. Bei der Anpassung an die COVID-19-Umgebung sind jedoch aufgrund der großen Anzahl von Personen, die nun Telearbeit leisten, und des hektischen Eilens, um dies zu ermöglichen, neue Probleme aufgetreten.\n\nEin Problem ist, dass Mitarbeiter zu Hause in unsicheren Umgebungen arbeiten, in Netzwerken, die nie für den Einsatz außerhalb des Unternehmens vorgesehen waren. Das führt zu gezielten Phishing- und Malware-Angriffen. Ein weiteres Problem ist die Einführung neuer Tools, wie zum Beispiel für Zusammenarbeit und Video-Konferenzen, die Ihr Unternehmen und Ihre Daten Angriffen und böswilliger Nutzung aussetzen können. Im Grunde genommen gibt es jetzt eine größere Angriffsfläche, die viel größer ist als das, was die meisten Organisationen ursprünglich geplant hatten, und neue Umgebungen, die nie Teil des ursprünglichen Plans waren.\n\n## Lösungen für die Sicherung des Telearbeiters\n\n## \"Bringen Sie Ihr eigenes Gerät (BYOD)\"\n\nEine der schwierigeren Fragen bei der Arbeit mit Mitarbeitern ist, wenn sie ihre eigenen Geräte verwenden. Die Hauptherausforderung besteht darin, dass es sich um nicht verwaltete Geräte handelt, auf denen nicht alle Sicherheitsagenten ausgeführt werden und die nicht von der IT verwaltet werden; das Gerät gehört dem Benutzer und wird von ihm verwaltet. Möglicherweise haben Sie die Möglichkeit, einige Agenten zu laden oder eine Richtliniendurchsetzung vorzunehmen, aber es gibt nur begrenzte Möglichkeiten, ohne die Nutzung des Geräts als persönliches Gerät für den Mitarbeiter einzuschränken. Wie können Sie also einen Benutzer mit einem \"Bring Your Own Device\" (BYOD) absichern?\n\nStellen Sie sicher, dass diese Grundlagen vorhanden sind: Stellen Sie die neuesten Patches als Voraussetzung bereit, erzwingen Sie eine gute Zugriffskontrolle für Ihr Unternehmen und Ihre Dienste, implementieren Sie eine Zwei-Faktor-Authentifizierung und schützen Sie Ihre Vermögenswerte mit den richtigen Schutzmaßnahmen wie Enterprise Rights Management zur Zugriffskontrolle der Daten und erzwingen Sie eine auf Richtlinien basierende Zugriffskontrolle für Endpunkte.\n\nEine weitere Möglichkeit, das Risiko zu verringern, besteht darin, den direkten Zugang zu Unternehmensdiensten einzuschränken und Benutzer in einer SASS-Umgebung arbeiten zu lassen.\n\n## Endpunkte und unsichere Umgebungen\n\nDie beste Lösung für viele dieser Probleme wäre ein unternehmensreicher Klient, aber es gibt immer noch Risiken. Die Mitarbeiter verwenden ein Gerät, das eine verwaltete Erweiterung des Unternehmensnetzwerks ist. Die Sicherheit kann durch sicheres Booten, vollständige Festplattenverschlüsselung, Datenschutz, lokale Firewalls und Durchsetzung einer guten Patch-Strategie gewährleistet werden. Das Risiko besteht jedoch, weil viele Organisationen bereits externe Mitarbeiter haben, wie zum Beispiel Vertriebsmitarbeiter oder Außendienstingenieure, die möglicherweise nicht das gleiche Sicherheitsniveau haben wie Mitarbeiter, die nie das Gebäude verlassen sollten, wie zum Beispiel Finanz- und Personalabteilungen.\n\nOrganisationen müssen wachsam sein und sicherstellen, dass alle Mitarbeiter nun über dieselbe Sicherheit verfügen oder sogar zusätzliche Sicherheitsebenen hinzufügen, die erforderlich sind, um in ungesicherten Netzwerken remote arbeiten zu können. Die Zwei-Faktor-Authentifizierung sollte jetzt eine Voraussetzung sein, und die Durchsetzung von Endpunkt-Richtlinien sowie das Management von Unternehmensrechten sind wichtiger denn je.\n\nWir sehen Innovationen in diesem Bereich, wie zum Beispiel ein Kunde, der neuen Mitarbeitern nicht nur Laptops, sondern auch einen verwalteten Router zur Vermeidung unsicherer Netzwerke gibt.\n\n## VDI-Systeme und Cloud-Services\n\nDie Implementierung von Sicherheitsmaßnahmen für sowohl VDI-Systeme als auch Cloud-Services beinhaltet die Grundlagen der Sicherheit: Datenschutz, Sicherheit der Virtualisierung sowohl im Unternehmensrechenzentrum als auch an den Zugriffspunkten, Anwendungssicherheit, sichere Bootvorgänge, Patching und Netzwerkverschlüsselung. Der Schlüssel liegt darin, eine genaue Bestandsaufnahme der im Unternehmen und bei den Cloud-Services eingesetzten Systeme durchzuführen und sicherzustellen, dass sie alle gleichermaßen geschützt sind. Jede Anwendung, auch solche, die nicht geschäftskritisch sind, birgt ein potentielles Angriffsziel.\n\n## Als-Service-Kunden\n\nSASS-Kunden nutzen Dienste in der Cloud und verwenden auch Anwendungen auf ihrem reichen Client, daher gibt es einige zusätzliche Sicherheitsprobleme, um sich Sorgen zu machen. Es sollte angemessenen Datenschutz im Enterprise Rights Management (ERM) für den Zugriff auf die Daten über die Cloud-Dienste und zurück zum Rechenzentrum geben. Der Schutz auf beiden Seiten ist entscheidend. Der Zugriff des Clients auf Cloud-Dienste sollte durch Multi-Faktor-Authentifizierung und Netzwerkverschlüsselung geschützt sein. Der Zugriff des Cloud-Dienstes auf die private Cloud des Unternehmens und die Ressourcen des Unternehmens sollte auch auf der Netzwerk- und Datenzugriffs- sowie Anwendungsebene geschützt sein. Das Verständnis, wie Kunden die Dienste nutzen und auf welche Daten sie zugreifen, ist der Bereich, in dem die ERM-Entscheidungen ins Spiel kommen.\n\n## Missbrauch und Missbrauch von Insidern.\n\nDie IT sollte eine Vielzahl von Methoden verwenden, um die Bedrohung und das Risiko von Fehlern, Missbrauch und bösartigen Handlungen von internen Mitarbeitern zu verwalten. Eine Richtlinien-basierte Zugriffskontrolle und Durchsetzung von Anwendungen bis hin zu den Daten auf Unternehmens- und Cloud-Ebene ist wichtig, um Missbrauch und Fehlverhalten von bereits authentifizierten Benutzern zu verhindern. Die wichtigste Verteidigung, die die IT hat, ist die Überwachung und Aufzeichnung von Bedrohungsanalysen. Die Verwaltung dieser Informationen über einen langen Zeitraum sowohl im Unternehmen als auch in der Cloud kann sehr effektiv sein, um abweichendes Verhalten zu erkennen.\n\nEs besteht kein Zweifel, dass wir jetzt anders über Sicherheitsfragen beim Telearbeiten nachdenken müssen. Unsere wichtigsten Empfehlungen sind zunächst, die von Ihnen getestete Technologie einzusetzen; zweitens, Ihre Benutzer zu schulen; und drittens, die Zwei-Faktor-Authentifizierung zu aktivieren und Ihre Daten im großen Maßstab zu schützen. Wenn wir diese drei Dinge tun können, können wir das Risiko verringern und besser auf die Zukunft vorbereitet sein.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT7-de","image":"./episodes/edt-7/de/thumbnail.png","lang":"de","summary":"Mit dem enormen Wandel, dass Menschen nun von zu Hause aus statt aus dem Büro arbeiten, wird Sicherheit zu einer wachsenden Sorge für viele IT-Organisationen. In dieser Episode diskutieren Steve Orrin, CTO von Intel Federal, und Darren die Sicherheitsbedrohungen und Lösungen, um Ihren Telearbeiter und Unternehmensrechenzentren zu schützen."},{"id":144,"type":"Episode","title":"Moderne Datenverwaltung","tags":["datagovernance","immuta","dataaccess","identitymanagement","cybersecurity","data","technology"],"body":"\r\n\r\nDa Organisationen zur Cloud migrieren, ändert sich vollständig die Art und Weise, wie wir über Daten denken und wie wir Daten verwenden. In den letzten fünf oder sechs Jahren hat sich die gesamte Infrastrukturschicht verändert. Die erforderliche Leistung, Skalierung, Regeln und die Anzahl der Benutzer, die Daten kombinieren möchten, haben exponentiell zugenommen. Das in großem Maßstab zu bewältigen, ist keine einfache Aufgabe, und genau das ist der Bereich, in dem Immuta ins Spiel kommt.\n\nIm Laufe der Zeit hat die Industrie das Paradigma der Datenspeicherung neu überdacht. Früher hat jedes Daten-Team Datenprodukte in einem Silo erstellt und sie dann an eine Geschäftseinheit geliefert. Die Geschäftseinheit hatte ihre passenden Daten, die ihnen einen qualitativ hochwertigen Service lieferten. Jetzt plötzlich möchten wir, dass jeder Daten über das gesamte Unternehmen hinweg kombinieren kann: Business-Analysten, Datenwissenschaftler, Dateningenieure, usw. Die Anzahl der Benutzer hat sich geändert und es geht nicht mehr nur um Suchanwendungen, sondern um Transformationen und Datenpipelines.\n\nDas ist ein Unterschied, der einen anderen Qualitätsstandard, Zeit und ein nie zuvor dagewesenes Niveau an Raffinesse erfordert. Es wird noch komplexer, wenn man Datenschutzgesetze, Datenklassifizierung und die sich ständig ändernden Regeln und Bestimmungen hinzufügt.\n\nWas Immuta versucht zu tun, ist es, es jedem Benutzer potenziell zu ermöglichen, jeden Datensatz, intern oder extern für die Organisation, zu kombinieren, um einige Erkenntnisse zu erhalten. Es geht alles um das Ausmaß. Die Richtlinien müssen von der Plattform getrennt werden, um die Richtlinien für all diese neuen Benutzer und alle verschiedenen Kombinationen von Daten zu skalieren. Sicherheit, Datenschutz und Governance sind großartig, aber wenn Sie nicht handeln können, ist niemand glücklich.\n\nEine Lektion, die Immuta früh gelernt hat, ist, dass ein Proxy nicht funktioniert. Es ist wie der Versuch, Petabytes an Daten durch einen Strohhalm zu bewegen: Sie werden zum Flaschenhals. Es ist einfach, die Middleware für Probleme verantwortlich zu machen, denn sie ist immer ein Engpass, da sie zwischen den Tools und den Daten steht. Dies ist ein klassisches Problem der Middleware.\n\nDas zweite Problem mit diesem Design-Paradigma ist, dass es bei der klassischen Datenvirtualisierung funktioniert hat, weil es einen klaren Satz von Daten gab. Wenn jedoch Petabytes an Daten vorhanden sind, scheitert der Ansatz \"Gib mir alles und wir finden eine Lösung\" sehr schnell aufgrund der schieren Größe der Daten sowie der Regeln und Richtlinien, die damit verbunden sind.\n\nIn der normalen Virtualisierung müssen Sie sich in die Cloud-Computing-Infrastruktur von Snowflake, Synapse usw. nativ integrieren. Anstatt diese Abstraktionsschicht über diesen verschiedenen Tools zu legen, befindet sich die Abstraktionsschicht auf der Richtlinienseite.\n\nDie Richtlinie ist nur so gut wie die Möglichkeit, sie zu überprüfen. Es ist wie eine Kette von Beweisen. Der Schlüssel besteht darin, nachweisen zu können, ob dieser Benutzer den Vorgaben innerhalb der IT-Infrastruktur entsprochen hat oder nicht. Mit moderner Datenverwaltung wird die Richtlinie innerhalb des Systems skaliert und die Prüfungen werden vereinheitlicht, aufgrund der Komplexität vieler Aktivitäten mit den Daten. Die Überprüfung der Richtlinie muss drastisch vereinfacht werden, da es sonst unmöglich ist, die Einhaltung festzustellen.\n\nEs gibt drei Arten von Richtlinien zu berücksichtigen: betriebliche, regulatorische und vertragliche. Ein Beispiel dafür, wie umständlich das alles werden kann, ist ein Blick auf ein Unternehmen wie Cummins. Wenn sie in die Cloud modernisieren, müssen sie für jeden Markt die entsprechenden Vorschriften beachten. Für einen ihrer Motoren stellen Sie sich vor, sie müssten eine maßgeschneiderte Regel für jedes Land schreiben, in dem sie vertreten sind, aber auch für jedes Land, in dem sie nicht vertreten sind, da diese Länder es nicht sehen sollten. Das ist nicht skalierbar, wenn man über Zehntausende Datenquellen für die Schemas spricht, die sich ständig ändern. Von diesen Motoren gibt es Petabytes an Telemetriedaten.\n\nWas du schreiben möchtest, ist, dass du nur einmal die Daten für das Land sehen kannst, in dem du wohnst, und dann gilt es überall. Aber so wird es nicht gemacht. Daher würden sie etwas wie 700 Richtlinien für ein Datenelement schreiben, wenn es nur eine geben sollte. Es wäre nahezu unmöglich, mit allen sich ändernden Richtlinien und Vorschriften für jede Datenquelle Schritt zu halten. Du wärst die ganze Zeit nicht konform.\n\nEs war noch nie einfacher, ein Unternehmen zu globalisieren als heute, und Immune-Kunden erwarten, dass sie in der Lage sind, ihre Infrastruktur weltweit über jede Cloud hinweg zu betreiben. Sie sollten ihre Daten in jede beliebige Cloud verschieben können und dabei vollständig konform sein.\n\nImmuta wendet Regeln auf die Daten mithilfe von Tags an, anstatt die Rohdaten zu verwenden, da jede Domäne ihre eigenen Fachbegriffe verwendet, um ihre Daten zu klassifizieren und darüber zu sprechen. Daher haben sie damit begonnen, allgemeine Konzeptvorlagen anzuwenden, wie beispielsweise für HIPAA. Diese Vorlagen sind jedoch noch nicht narrensicher, und es gibt noch einen langen Weg zu gehen.\n\nMatt's Vision als CEO von Immuta ist es, eine Situation zu erreichen, in der Domänen ihre Richtlinien teilen können. Zum Beispiel gibt es im Gesundheitswesen gute Gründe, warum Moderna und Pfizer zusammenarbeiten möchten. Es könnte eine Vereinbarung darüber geben, wie man Echtwelt-Datenkontrollen mit der Akademie handhabt. Wenn es also eine konsistente Richtlinie geben würde, die in einer konzeptionellen Richtlinien-Cloud geteilt und gemeinsam genutzt werden könnte, wäre das der richtige Weg.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matthew Carroll"],"link":"/episode-EDT70-de","image":"./episodes/edt-70/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect, Intel, diskutiert mit Matthew Carroll, CEO von Immuta, über die Realität und Zukunft der modernen Datenverwaltung."},{"id":145,"type":"Episode","title":"Moderne Datenverwaltung","tags":["datagovernance","data","immuta","dataaccess","identitymanagement","ceo","technology","process"],"body":"\r\n\r\nIn der modernen Datenverwaltung lautet die erste Voraussetzung, dass Sie Richtlinien von der Plattform trennen müssen.\n\nDas zweite ist, dass es keine Ontologie geben kann. Wenn jemand versucht, ein Superschema von allem zu erstellen, ist das unmöglich, aber man benötigt ein Schema, um Richtlinien anzuwenden. Wenn ein Benutzer zwei Spalten verbinden möchte, muss eine neue Richtlinie vorhanden sein. Sensible Daten müssen sowohl durch direkte als auch indirekte Identifikatoren entdeckt werden. Indirekte Identifikatoren sind schwieriger und es müssen Techniken angewendet werden, um das Risiko von Link-Angriffen zu verringern.\n\nDie dritte Prämisse beinhaltet die Einführung einer Reihe von datenschutzverstärkenden Techniken wie Maskierung, Schwärzung, Autorisierung, usw.\n\nDiese Werkzeuge sind als Teil moderner Datenverwaltung automatisiert. Ein paralleles Beispiel ist, wie früher ein Zauberer sein musste, um rote Augen auf einem Foto zu entfernen. Jetzt klickt man auf einen Knopf und die roten Augen verschwinden. Es ist dasselbe bei Datenschutz. Wir brauchen diesen einfachen Knopf, um automatisch potenziell indirekte Identifikatoren zu finden, bei denen eine größere als sehr geringe Risiko der Wiederidentifizierung besteht.\n\nNeben der Privatsphäre gibt es eine Vielzahl anderer Dinge, die automatisiert werden sollten, bevor die Daten klassifiziert werden. Obwohl Immuta sich nicht mit der Datenveränderung und solchen Datenflüssen befasst, stellen sie eine API und eine Befehlszeilenschnittstelle zur Verfügung. Ingenieure, die diese Pipelines aufbauen, können ihre Arbeit erledigen, und Immuta gibt ihnen die Regeln vor, und die Aktualisierung der Regeln kann Teil der Pipeline sein. Sie wollen ein Kanal zu dieser Ebene sein.\n\nEs gibt einige neue Rollen in diesem Bereich, wie Datenverwalter und Daten-Governance-Ingenieur, die vom Dateningenieur getrennt sind. Und dann gibt es wirklich drei separate Nutzergruppen mit drei separaten Bedürfnissen.\n\nZunächst einmal gibt es Datenwissenschaftler, die spezialisierte Fähigkeiten haben und Daten in einer bestimmten Art und Weise benötigen. Manchmal benötigen sie spezifische Werkzeuge und eine spezifische Rechenumgebung an einem Ort auf der Welt, um ihre Aufgabe zu erfüllen.\n\nAuf der anderen Seite benötigen Daten- und Analyseingenieure, die einerseits Pipelines aufbauen und andererseits die Pipelines aufrechterhalten, einen schnellen Zugriff auf ein Notfallverfahren, wenn etwas kaputt geht. Sie speisen es in eine Pipeline ein und liefern es an jemanden aus und überprüfen, ob es auf dem neuesten Stand ist.\n\nSchließlich gibt es die Gouverneure, die versuchen, mit den Vorschriften Schritt zu halten.\n\nDiese Benutzer haben alle sehr unterschiedliche Ansichten und Bedürfnisse in Bezug auf Governance. Bei der Anwendung einer neuen Governance ist es am wichtigsten, diese Gruppen als separate Interessengruppen zu behandeln.\n\nWenn man all diese Rollen auf binäre Weise betrachtet, als Dateningenieure, dann endet es meist in vielen Meetings, so dass es unmöglich ist, zu skalieren. Wir müssen eine symbiotische Beziehung zwischen Datenoperationen, Datenwissenschaft, Datenanalytik und Governance schaffen. Ein Beispielmodell wäre Salesforce oder ServiceNow, wo es einen kompletten Arbeitsablauf von Anfang bis Ende gibt und keine Meetings stattfinden müssen. Dies wäre echtes Data Ops.\n\nImmuta hat philosophisch gesehen einen attributsbasierten Ansatz anstelle eines rollenbasierten Ansatzes. Das Problem mit einem rollenbasierten Ansatz besteht darin, dass sich der Umfang immer weiter ausdehnt, da ständig neue Rollen hinzugefügt werden. Ein pharmazeutisches Unternehmen hatte beispielsweise über 800.000 Rollen, da Rollen aufgrund der Notwendigkeit der Reproduktion von Arzneimittelstudien niemals gelöscht werden können. Das Problem der Rollenüberlastung kann sich schnell zu einem Skalierungsproblem entwickeln.\n\nAttributebasierter Zugriff ist der Schlüssel, um dem entgegenzuwirken. Anstatt ständig Rollen hinzuzufügen, haben Benutzer spezifische und konsistente Attribute. Zum Beispiel könnte ein Attribut eines Benutzers sein, dass er markiert ist, sodass er nur seinen eigenen Status sehen kann. Mit rollenbasiertem Zugriff müsste jeder Zustand, egal ob sichtbar oder nicht, aufgeschrieben werden. Dieses moderne Identitätsmanagement ist sehr skalierbar. Der Attributzugriff vereinfacht die Anzahl der Richtlinien, die geschrieben werden müssen, und hilft bei der Leistung.\n\nModerne globale Vorschriften wie die DSGVO erfordern jedoch auch einen Zweck. Hier kommt der Attributzugriff ins Spiel: Unter welchen Zwecken kann jede Person handeln? Gemäß einer Endbenutzer-Lizenzvereinbarung (EULA) müssen Daten nur für den angegebenen Grund von Benutzern verarbeitet werden. Andernfalls muss vor der operativen Verwendung der Daten für die Produktion eine Risikoanalyse durchgeführt werden.\n\nIm Moment befinden wir uns am Anfang der modernen Datenverwaltung. Derzeit treffen Benutzer eine einmalige binäre Entscheidung in Bezug auf die Daten, entweder Zustimmung oder keine Zustimmung. Die Zukunft liegt irgendwo in der Mitte: begrenzte Zustimmung. Wenn eine Person zum Beispiel ihre genomischen Daten an ein Unternehmen wie ancestry.com weitergibt, was bedeutet das in Bezug auf ihr Kind? Das Kind hat keine Zustimmung dafür gegeben, dass sein genetisches Material möglicherweise von einer Krankenversicherungsgesellschaft untersucht wird, um das Risiko festzustellen. In der modernen Datenverwaltung sieht Matt Verbraucher eine begrenzte Zustimmung geben, wie zum Beispiel einer Firma nur die Analyse der DNA für Ahnenforschung und nichts anderes zu ermöglichen.\n\nDie Zukunft muss sich um Zustimmung und zweckbasierten Zugang drehen, denn letztendlich treiben derivative Daten Erkenntnisse voran, da sich beim Entwickeln von maschinellem Lernen Daten in den Algorithmen einbetten.\n\nUm mehr über Immuta und den Aufbau eines Datenverwaltungsprogramms zu erfahren, besuchen Sie Immuta.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matthew Carroll"],"link":"/episode-EDT71-de","image":"./episodes/edt-71/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, der Chief Solution Architect bei Intel, führt seine ausführliche Diskussion über die Realität und Zukunft moderner Datenverwaltung mit Matthew Carroll, dem CEO von Immuta, fort. In dieser Episode diskutieren sie Datenklassifikation, Richtlinien und Governance."},{"id":146,"type":"Episode","title":"2021 Eine Jahresrückblick","tags":["covid","edge","compute","cybersecurity"],"body":"\r\n\r\nEine große Erwartung für das Jahr 2021 war, dass der neue Impfstoff uns aus der COVID-Krise führen würde und Reisen wieder möglich sein würden. Darren erwartete, seine Kunden von Angesicht zu Angesicht zu treffen, anstatt über Zoom oder Webkonferenzen.\n\nAndere Erwartungen waren die Umsetzung von hybriden Arbeitsmodellen und obwohl das Fernlernen in der Bildung in vollem Gange war, gab es Hoffnung auf eine Rückkehr zu Präsenzklassen. Wir erwarteten einen Anstieg des kontaktlosen oder reibungslosen Kundenservice. Die Gig-Industrie wie UberEats war ebenfalls im Aufschwung.\n\nIn der Industrie hatten wir gehofft, dass es einen großen Schub für KI und maschinelles Lernen geben würde, da neue Technologien verfügbar werden. Wir dachten auch, dass wir eine Beschleunigung in Richtung Industrie 4.0 mit neuer Automatisierung sehen würden, da Fabriken trotz geringerer Arbeitskräfte immer noch produzieren mussten.\n\nWir hatten hohe Erwartungen für das Jahr 2021 und es hat uns nicht enttäuscht. Es gab jedoch einige Überraschungen.\n\n## Überraschungen in 2021\n\nObwohl der Impfstoff geholfen hat, hat er COVID nicht ausgelöscht. Wir hatten vier Wellen und befinden uns derzeit inmitten der Omikron-Welle zu Beginn des Jahres 2022. Dennoch sind nicht alle zurück zur Arbeit gegangen, und Organisationen setzen weiterhin hybride Arbeitsplatzpläne um.\n\n2021 brachte auch die große Kündigungswelle mit sich, bei der eine große Anzahl von Menschen den Job gewechselt hat, möglicherweise aufgrund von Burnout, neuen Möglichkeiten oder einfach aufgrund der Unsicherheit darüber, wann sie ins Büro zurückkehren müssten.\n\nIT wurde zu den stillen Helden der Geschäftskontinuität, da sie noch flexibler wurden und in der Lage waren, sich blitzschnell an alle unterschiedlichen Bedürfnisse von Mitarbeitern und Kunden anzupassen.\n\nEine weitere große Überraschung war, dass auch bedeutende Industrien außerhalb der Hochtechnologie wie die Fleischverarbeitung und eine Pipeline von Ransomware betroffen waren. Sicherheit wurde somit zu einem wichtigen Konzept.\n\nIntel hatte ebenfalls eine große Überraschung mit der Rückkehr von Pat Gelsinger als CEO.\n\nCOVID richtet immer noch große Schäden in der ganzen Welt an.\n\nCOVID wird wahrscheinlich das entscheidende oder schwarze Schwan-Ereignis des Jahrhunderts sein. Die Weltwirtschaften haben sich angepasst, um die Unsicherheit der vier Wellen von COVID-Ausbrüchen widerzuspiegeln, und die Spitzen und Täler haben Verwüstung in den Geschäftsplänen angerichtet, um Menschen wieder ins Büro zu bringen.\n\n## Hybride Arbeitsmodelle wurden geschaffen.\n\nOrganisationen haben einige großartige hybride Arbeitsmodelle entwickelt, aber viele werden immer noch nicht genutzt, da die Wiederanfangstermine aufgrund von Ausbrüchen, wie der aktuellen Omikron-Welle im Januar 2022, kontinuierlich verschoben wurden. Was wir jedoch sehen konnten, ist eine bedeutende Veränderung der Unternehmenskultur, indem sich Menschen zunehmend in die Arbeit im Homeoffice und möglicherweise im hybriden Modell einfinden. IT-Organisationen haben sich auf den ständigen Wechsel zwischen Büro und Homeoffice vorbereitet.\n\nDie alltägliche Arbeit hat sich grundlegend davon verlagert, dass die Menschen ihre Arbeit auf einer Maschine im Büro speichern, hin zum Speichern in der Cloud. Alternativ werden die Dinge auf einem tragbaren Gerät gespeichert und in der Cloud repliziert. Daher haben wir einen enormen Wandel hin zu cloud-basierten und SaaS-basierten Angeboten wie Office 365 sowie in Kollaborationstools wie Zoom und Teams gesehen. Darüber hinaus ist die remote Einarbeitung neuer Mitarbeiter zur Norm geworden. All dies hat Auswirkungen auf Menschen, Prozesse und Technologien.\n\n## Bekämpfung der großen Kündigungswelle\n\nUm Mitarbeiter inmitten der großen Kündigungswelle zu halten, müssen Organisationen zunächst das Wohlbefinden der Mitarbeiter in den Blick nehmen. Mit dem Stress und der Isolation der Pandemie ist eine Zunahme von Organisationen zu verzeichnen, die Mitarbeitern bei der mentalen und emotionalen Gesundheit helfen möchten.\n\nFlexibilität ist ein weiterer Schlüssel; mit der Arbeit im Homeoffice können Arbeitszeiten und -orte flexibel gestaltet werden. Mitarbeiter können in einem anderen Bundesstaat leben oder unkonventionelle Arbeitszeiten haben. Diese Flexibilität und der Wegfall des Arbeitswegs haben es den Menschen ermöglicht, sich mehr mit ihrer eigenen Gemeinschaft auseinanderzusetzen und diese zu erkunden.\n\nHR-Organisationen setzen zunehmend künstliche Intelligenz ein, um herauszufinden, warum Mitarbeiter das Unternehmen verlassen, Trends zu identifizieren und Programme zu bestimmen, die die Produktivität steigern und den Angestellten das Gefühl geben, Teil des Teams zu sein. Außerdem greifen Unternehmen vermehrt auf Automatisierung zurück. Die Robotic Process Automation Branche verzeichnet einen Anstieg, um mit weniger Mitarbeitern umzugehen.\n\n## Der unbeirrbare IT-Takt hat nicht nachgelassen.\n\nDie IT kann nicht mit dem heroischen Tempo mithalten, das zu Beginn der COVID-Pandemie erforderlich war, daher werden die verrückten Arbeitszeiten zwar abgeschwächt, aber die Nachfrage wird strategisch erfüllt. Wir haben einen massiven Trend zu SaaS-Angeboten gesehen, sodass die Mitarbeiter zum Beispiel nicht mehr Dinge managen mussten, die nicht unbedingt ihre Stärken waren. Stattdessen konnten sie sich an einen Cloud-Service-Provider oder an Dienstleister für HR, Vertrieb oder ERP-Systeme wenden. Eine Bewegung hin zur Automatisierung und wiederholbaren Prozessen hat einen Teil des Drucks von der täglichen Büroarbeit genommen. Es wurde auch mehr in DevOps und RPA-Technologie investiert, um die Produktentwicklung zu optimieren und zu sichern.\n\n## Sicherheit, Lieferkette und Ransomware.\n\nSicherheitsprobleme haben die meisten Branchen überrascht. Branchen wie Fleischverpackung und Öl- und Gasindustrie, Krankenhäuser sowie Lieferketten- und Logistikunternehmen wurden von Sicherheitsverletzungen getroffen. Dies war hauptsächlich ein Problem des schnellen Umstiegs auf die Fernarbeit und der Vernachlässigung der sorgfältigen Sicherheit. Zum Beispiel haben Organisationen vielleicht keine angemessene Schulung für Remote-Mitarbeiter durchgeführt, um ihre Laptops oder Daten zu sichern. Unter anderem werden wir dieses Jahr die Einführung des Konzepts \"Zero Trust\" sehen.\n\nEin weiteres großes Problem dieses Jahres waren Lieferkettenprobleme, und nicht nur bei der Siliziumlieferung. Viele Materialien werden weiterhin knapp sein, insbesondere für kleine Unternehmen.\n\nWir müssen Lösungen finden, um Ransomware zu bekämpfen. Organisationen können Prozess- und Kulturänderungen vornehmen und neue Technologien nutzen, um diese Lösungen zu finden.\n\n## Intel hat einen neuen CEO bekommen.\n\nDie größte Überraschung für Darren und vielleicht auch für die gesamte Branche war die Rückkehr von Pat Gelsinger zu Intel, diesmal als CEO. Jeder bei Intel ist von der Energie, die er mitbringt, inspiriert. Pat sagt, dass er der Industrie helfen wird, den Chipmangel zu lösen, indem er wieder in die amerikanische Fertigung investiert. Darren glaubt, dass er Intel wieder an die Spitze des Chip-Verkaufs und wunderbare Technologie für die ganze Welt bringen wird.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT73-de","image":"./episodes/edt-73/de/thumbnail.png","lang":"de","summary":"In dieser Folge reflektiert Darren über die Erwartungen und Überraschungen im Jahr 2021."},{"id":147,"type":"Episode","title":"Ransomware: Vorbereiten und keine Panik","tags":["cybersecurity","ransomware"],"body":"\r\n\r\nObwohl eine Ransomware-Attacke für die meisten Organisationen mit einer Art Lösegeldforderung beginnt, brodelt die gesamte Geschichte in der Regel bereits seit vielen Monaten.\n\nDie Lösegeldforderung erfolgt, nachdem die Angreifer Informationen aus kompromittierten Systemen verschlüsselt und alles gesperrt haben, was sie konnten, oder sie haben sensible Informationen gestohlen und drohen nun, diese öffentlich zu veröffentlichen. In beiden Fällen fordern sie eine Geldsumme, um die Veröffentlichung zu stoppen oder die Informationen wiederherzustellen.\n\nDie Höhe des Geldes kann angezielt werden, weil die Angreifer wissen, wie viel das Unternehmen sich leisten kann, und sie werden nach einer massiven Summe fragen. Weniger gezielte Angriffe werden nach einer zufälligen Summe fragen und hoffen, dass sie erfolgreich sind. Doch immer mehr werden die Angriffe raffinierter, und die Angreifer haben ihre Hausaufgaben gemacht. Sie haben bedeutende Fortschritte erzielt, indem sie sich seitlich in der Umgebung bewegt und mehrere Domains kompromittiert haben. Sie wissen, dass es die Organisation eine enorme Summe kosten wird, ihre Dienste wiederherzustellen, also können sie mehr verlangen.\n\nAber bevor die Angreifer das Lösegeld fordern, sind sie eine gewisse Zeit lang im System aktiv gewesen, im Durchschnitt knapp unter 300 Tage, bis sie ihren Plan vollständig umsetzen. Sie versuchen, so weit wie möglich im Netzwerk voranzukommen, um den größtmöglichen Schaden anzurichten. Zum Beispiel werden sie den Backup-Executive löschen, um die Organisation lahmzulegen und beschädigte, unbrauchbare Backup-Daten zu hinterlassen. Außerdem werden sie Zeit investieren, um Zugangsdaten zu Kontoerhöhungen zu kompromittieren, um sich später im Netzwerk seitlich zu bewegen und eventuelle Schwachstellen aufzudecken.\n\nJeder Angriff ist unterschiedlich, daher ist es schwierig, die häufigste Schwachstelle zu benennen. Die Angreifer finden jede mögliche Schwachstelle, sei es in der Schwachstellenverwaltung oder sogar bei den Menschen selbst, zum Beispiel durch Phishing-Angriffe oder Social Engineering-Taktiken. Sie werden Telefonanrufe tätigen und sich als Führungskräfte ausgeben oder Namen von Führungskräften verwenden, um Informationen und Daten von Mitarbeitern online zu erhalten.\n\nViele in der Branche werden sagen, es geht nicht darum, ob, sondern wann du angegriffen wirst. Stephanie jedoch glaubt nicht, dass dies der Fall ist, denn mit Sorgfalt, angemessener Absicherung und dem Ergreifen der notwendigen Maßnahmen wirst du nicht ins Visier genommen.\n\nDue Diligence ist das, worüber Sicherheitsexperten seit Jahrzehnten predigen: Schwachstellenmanagement und Patch-Management durchführen, Passwörter ändern und Berechtigungen so weit wie möglich einschränken. Schulen Sie Ihre Mitarbeiter. Administratoren sollten zum Beispiel nicht mit administrativen Konten im Internet unterwegs sein und in Foren posten. Es geht darum zu verstehen, welche Angriffsflächen existieren und sie zu verwalten.\n\nWas solltest du tun, wenn du Opfer eines Ransomware-Angriffs wirst? Stephanie sagt, dass das, was die Leute tun sollten und was sie tatsächlich tun, zwei verschiedene Dinge sind. Die erste reflexartige Reaktion der IT-Teams ist oft, das System neu zu starten, zu patchen oder die Umgebung signifikant zu verändern. Dadurch werden die Angreifer jedoch nur darauf aufmerksam gemacht, dass ihre Zeit abgelaufen ist. Für die forensische Untersuchung ist es äußerst wichtig, diese Umgebungen zu schonen. Gib so wenige Tastatureingaben wie möglich ein und starte auf keinen Fall neu.\n\nEinige Unternehmen werden in Panik geraten und sich selbst einem Denial-of-Service unterziehen, indem sie in den vollständigen Sperrmodus gehen und alles herunterfahren. Anstatt in Panik zu geraten, sollten sie sich auf ihre Vorfallreaktionsrichtlinien verlassen und erkennen, dass das Problem gelöst werden kann, auch wenn es unangenehm ist.\n\nOffensichtlich sollte jedes Unternehmen über solche Vorfallsreaktionsrichtlinien verfügen, die schnell umgesetzt werden können, um die interne und externe Kommunikation zu verwalten und den Geschäftsbetrieb nach Möglichkeit aufrechtzuerhalten. Sicherheitsexperten können bei der Erstellung dieser Reaktionspläne helfen und während eines Angriffs Unterstützung leisten.\n\nIhr erster Schritt zur Erhaltung der Informationen besteht darin, einige der fragwürdigen Aktivitäten im Netzwerk zu beobachten. Die Ransomware-Angriffe beginnen nicht damit, Informationen zu verschlüsseln oder zu stehlen; es gibt viele Angriffe davor. Es ist wichtig, herauszufinden, woher sie kommen, woher sie stammen und wohin sie gehen. Dafür benötigen Sicherheitsexperten Beweise und Informationen, die ordnungsgemäß gesichert werden müssen. Ein guter Anfang ist es, die richtigen Leute am richtigen Ort zu haben, um das Geschehen zu kontrollieren.\n\nAls Nächstes geht es darum, die Umgebung ordnungsgemäß zu verwalten. Leider ist die Organisation, sobald es zu Ransomware oder irgendeiner Art von Verstoß oder Vorfall kommt, äußerst verwundbar. Hundert Prozent der Zeit, wenn Umgebungen erfolgreiche Ransomware-Angriffe erleiden, ob öffentlich bekannt gemacht oder nicht, werden sie entweder von derselben Gruppe oder einer anderen Gruppe ins Visier genommen. Es ist wie ein verwundetes Tier mit den Geiern, die kreisen. Die Angreifer wissen, dass du verletzt und angreifbar bist. Es steht ein weiterer Angriff bevor.\n\nMeistens, wenn Sicherheitsexperten ihre Untersuchungen durchführen, finden sie andere Anzeichen für Angriffe und Kompromittierungen an verschiedenen Teilen des Netzwerks. Sie müssen feststellen, ob es Teil des gleichen Angriffs oder eines unterschiedlichen Angriffs ist. Diese Untersuchung ist ein wichtiger Teil der Wiederherstellung nach Malware, denn selbst wenn man denkt, dass man einen Angriff bereinigt hat und das Geschäft nun wieder ordnungsgemäß läuft, besteht immer noch das Potenzial für diese anderen Angriffe.\n\nEine typische Attacke kostet durchschnittlich vier Millionen und fünfhunderttausend Dollar, um das Chaos zu beseitigen, und das beinhaltet nicht das Lösegeld. Die Summe kann jedoch wesentlich höher sein und ist proportional zur Größe der Organisation.\n\nEs ist unmöglich zu sagen, wie viele Organisationen das Lösegeld zahlen. Viele der Lösegelder werden mit Drohungen verbunden, keine Behörden zu kontaktieren oder den Angriff zu enthüllen. Deshalb variieren die verfügbaren Statistiken darüber, wie viele Organisationen tatsächlich zahlen.\n\nEinige Entscheidungsträger von Organisationen reagieren darauf, indem sie erklären, dass sie unter keinen Umständen zahlen werden, selbst wenn es mehr kostet, wieder aufzubauen, und dadurch die Verhandlungsfähigkeit zerstören. Dies ist eine emotionale Entscheidung, die das Urteilsvermögen trüben kann. Letztendlich, wenn das Ziel darin besteht, weiterhin Geschäfte zu machen und Geld zu verdienen, ist es günstiger, ein niedriges Lösegeld von vielleicht zehn- oder zwanzigtausend Dollar zu zahlen, als für forensische Untersuchungen und den Rest des Prozesses zu bezahlen. Auf der anderen Seite gibt es dokumentierte Fälle, in denen das Lösegeld gezahlt wurde, aber die Daten nicht vollständig wiederhergestellt wurden. Außerdem hat die Organisation keine Sicherheit hinsichtlich der Sicherheit ihrer Umgebung. Es gibt keine Garantie dafür, dass Kriminelle nach Bezahlung des Lösegeldes ihren Teil des Deals einhalten.\n\nViele Regierungen auf der ganzen Welt haben es illegal gemacht, Lösegelder zu zahlen, da die Angreifer als Terroristen angesehen werden und es verboten ist, mit Terroristen zu verhandeln. Eine weitere Komplikation besteht darin, dass die Angreifer manchmal nicht mit professionellen Verhandlern verhandeln wollen. Oft benennen sie eine Person in der Organisation als einzige Person, mit der sie verhandeln werden, in der Hoffnung, dass diese Person emotionale Entscheidungen trifft.\n\nDie Bedrohungen werden auf verschiedene Weise kommuniziert: per E-Mail, Telefon und sogar als Hintergrundbild auf dem Desktop.\n\nDie beste Strategie, um einen Angriff zu vermeiden oder abzuschwächen, besteht darin, nicht zu warten, bis Ihre Organisation in dieser Situation ist. Engagieren Sie sich stattdessen in einer ordnungsgemäßen Sorgfaltspflicht. Führen Sie mindestens einmal im Jahr Bewertungen durch, um Sicherheitslücken aufzudecken. Die Bedrohungen und Angriffe ändern sich ständig und werden immer raffinierter, daher muss die Sicherheit Ihrer Organisation mithalten. Überwachen und aktualisieren Sie kontinuierlich. Führen Sie Schwachstellenmanagement durch, ändern Sie regelmäßig Passwörter und erinnern und schulen Sie Benutzer bezüglich der Bedrohungen. Dies sind keine neuen Strategien. Sicherheitsexperten empfehlen diese Schritte seit Jahrzehnten, jedoch setzen Organisationen sie nicht richtig um und entwickeln sich nicht weiter.\n\nEs ist auch klug, die Experten einzubeziehen, um den Prozess der Incident-Response-Pläne und -Übungen zu leiten. Jeder in der Organisation sollte wissen, was zu tun ist und wen er im Falle eines Angriffsszenarios kontaktieren sollte, um weiteren Schaden zu vermeiden.\n\nWenn Ihre Organisation angegriffen wird, hoffentlich wird der Schaden minimal sein oder zumindest begrenzt und beherrschbar, falls Vorbereitungen getroffen wurden. Egal wie viel Zeit, Energie und Geld ein Unternehmen in präventive Maßnahmen investiert, es ist nur ein kleiner Bruchteil der Kosten eines Angriffs.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Stephanie Sabatini"],"link":"/episode-EDT74-de","image":"./episodes/edt-74/de/thumbnail.jpeg","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect bei Intel, spricht mit Stephanie Sabatini, Sr. Director Professional Services bei Hitachi Systems Security, über die Verhinderung und Vorbereitung von Ransomware-Angriffen sowie darüber, was zu tun ist, wenn Ihre Organisation angegriffen wird."},{"id":148,"type":"Episode","title":"Cybersicherheitslösungen mit Hitachi","tags":["cybersecurity","technology"],"body":"\r\n\r\nColin ist seit fast 24 Jahren bei Hitachi, nachdem er direkt nach dem College für IBM gearbeitet hat. Er konzentriert sich hauptsächlich auf Kunden im Pazifischen Nordwesten und öffentliche Auftraggeber aus dem Bereich Regierung, Bildung und Gesundheit. Er hatte die ungewöhnliche Gelegenheit, den Posten seines Vaters zu übernehmen, als dieser bei Hitachi in den Ruhestand ging.\n\nDavid arbeitet seit über zwanzig Jahren im Bereich der Informationstechnologie und hat kurz vor dem .com-Boom begonnen. Großteils seiner Karriere war er praktisch tätig und hat sich mit Servern, Speicherung und Sicherheit beschäftigt.\n\nViele Menschen denken bei Hitachi entweder an ein großes Maschinen- oder Werkzeugunternehmen oder an eins, das Brotbackmaschinen herstellt. Tatsächlich ist Hitachi ein hundert Jahre altes Ingenieur- und Fertigungsunternehmen, das 1910 damit begann, Elektromotoren herzustellen, und seitdem zu Hunderten von operativen Organisationen und hundertprozentigen Tochtergesellschaften herangewachsen ist, einschließlich Hitachi Vantara. Hitachi Vantara ist die größte hundertprozentige Tochtergesellschaft und konzentriert sich hauptsächlich auf das Datenmanagement. Sie bauen und liefern nicht nur herkömmliche blockbasierte Unternehmensspeichersysteme, sondern haben sich zu einem digitalen Lösungsunternehmen entwickelt, das die extremen Erweiterungen des IoT und datengetriebene Erkenntnisse umfasst.\n\nRobert Mueller sagte: \"Es gibt nur zwei Arten von Unternehmen: solche, die gehackt wurden und solche, die gehackt werden. Sogar diese beiden Kategorien verschmelzen langsam zu einer: solche, die gehackt wurden und erneut gehackt werden.\" Dieses Zitat legt den Grundstein dafür, wie Hitachi Unternehmen helfen kann. Es ist keine Frage des Ob, sondern des Wann, Sie gehackt werden und erneut gehackt werden. Dies gilt nicht nur für Unternehmen, sondern auch für Organisationen im öffentlichen Raum.\n\nHitachi hat eine ganze Bundesabteilung, die von Washington D.C. aus arbeitet und sich hauptsächlich auf das Verteidigungsministerium und bundesweite Konten konzentriert. Hitachi Vantara arbeitet eng mit der Bundesabteilung zusammen und erfüllt SLED-Konten, um Lösungen für bundesstaatliche und lokale Anforderungen bereitzustellen.\n\nDie beste Möglichkeit, eine Sicherheitslösung von Hitachi zu visualisieren, ist es, sich Ihre Organisation als Ihr Zuhause vorzustellen. In diesem Zuhause gibt es vier Schutzelemente, die wir einsetzen müssen.\n\nZuerst kommt die physische Sicherheit. Hitachi hat ein intelligentes System, eine visuelle Intelligenzlösung, die Videoüberwachung mit Intelligenz bereitstellen kann. Es kann alles von der Schusserkennung über die Kennzeichenverfolgung bis zur Gesichtserkennung durchführen. Hitachi setzt diese Lösungen weltweit ein. Hitachi-Kameras haben letztes Jahr viele Ereignisse im Washingtoner Einkaufszentrum erfasst und werden von Polizeiorganisationen im ganzen Land eingesetzt. Dieses Wachstum geht alles auf die Transportabteilung von Hitachi zurück, die stark auf Videotechnologie für Sicherheit angewiesen ist.\n\nPhysische Sicherheit ist oft kein Thema, das oft diskutiert wird, aber es ist ein Schlüsselelement. Wenn Sie Ihr eigenes Gebäude und Ihr eigenes Rechenzentrum nicht kontrollieren können, haben Sie keine Sicherheit. Wenn jemand durch die Tür kommen kann, kann er in Ihr Rechenzentrum gelangen.\n\nNach der physischen Sicherheit kommt Hitachi ID. Dies ist im Grunde das Schloss an Ihrer Haustür. Eine kürzlich durchgeführte Umfrage ergab, dass nur 40 Prozent der Unternehmen über privilegiertes Zugangsmanagement verfügen und etwa 74 Prozent über Identitätsmanagement. Das ist nicht genug, denn das bedeutet eine hohe Exposition, und der SLED-Bereich ist ein hochkarätiges Ziel. Hitachi ID bietet Passwort-Privilegien- und Identitätslösungen über eine einzige Plattform. Dies ist eine umfassende Lösung, bei der Sie nicht nur Benutzer, sondern auch Geräte und Anwendungen identifizieren können, sowohl vor Ort als auch in der Cloud.\n\nDer dritte Aspekt ist, wie man seine Daten schützt, wenn jemand durch die Tür kommt.\n\nCybersicherheit ist ein Problem mit großen Datenmengen, und Pentaho ist das Tool für große Datenmengen. Mit Pentaho können Sie alle Ihre Datenquellen verwalten, Ihren Datenpool kontrollieren und nützliche Ergebnisse erzielen. Hitachi hat festgestellt, dass Datenwissenschaftler und Cybersicherheitsanalysten 80 Prozent ihrer Zeit nur darauf verwenden, Daten zu verwalten, zu beschaffen und zu bereinigen, und nur 20 Prozent ihrer Zeit tatsächlich für die Analyse. Mit Pentaho werden diese Zahlen umgedreht, so dass sie 80 Prozent ihrer Zeit für die Analyse von Daten und Echtzeit-Bedrohungsanalyse und -reaktion verwenden können.\n\nPentaho ist ein Eindringungserkennungssystem, aber es ist auch ein Präventionssystem. Zum Beispiel hatte ein großes Energieunternehmen alles ad hoc erledigt und ihre eigenen Skripte intern geschrieben. Sie haben all diese Funktionalität an einem Tag in Pentaho replattformiert. Danach waren sie in der Lage, diese Analysen in Echtzeit durchzuführen und den Angreifern immer einen Schritt voraus zu sein, anstatt immer nur hinterherzuhinken. Was Pentaho in einem überfüllten Markt einzigartig macht, ist seine Flexibilität. Es kann jede Art von Datenanalyse durchführen und kann auf die Bedürfnisse Ihrer Organisation angepasst werden.\n\nDen Schutz von Daten aus der Inhaltperspektive ist auch im Bereich der öffentlichen Verwaltung von großer Bedeutung. Hitachi Content Platform Anywhere bietet sichere Dateisynchronisierung und -freigabe sowie eine sichere Dropbox. Es ist vergleichbar mit Google Drive oder Dropbox, zum Beispiel, jedoch liegt der Unterschied darin, dass HCP Anywhere vom eigenen Sicherheitsteam kontrolliert wird.\n\nEs wird vom Verteidigungsministerium genutzt, das es als Mill Drive umbenannt hat. Bodentruppen übertragen sensible Daten von und zu verschiedenen geheimen Orten und zurück zum Hauptquartier für Aufklärungs- und andere Missionen. Auf lokaler Ebene nutzen Städte es, um Videoaufnahmen nicht nur durch die Verwaltung, die Polizei und andere Behörden zu übertragen, sondern auch zur Beweismittelsicherung an die Staatsanwaltschaft.\n\nHCP Anywhere basiert auf der Hitachi Content Platform, die ihr Objektspeicher ist. Es hat einen integrierten Objektspeicher, nicht nur von Hitachi, sondern von allen Objektspeichern. Anstatt eine Datei zu überschreiben und somit einem Angreifer die Möglichkeit zu geben, Ihre Datei zu überschreiben, erstellen Sie eine neue Version der Datei. Dann können Sie im Falle eines Angriffs auf eine frühere Version zurückkehren, bevor der Angriff stattgefunden hat. Einige Rating-Agenturen sind der Meinung, dass der Objektspeicher von Hitachi der beste auf dem Markt ist. Out of the box erhalten Sie 16 Versionen jeder Datei, 16 Stück System-Metadaten und eine unbegrenzte Anzahl von benutzerdefinierten Metadatentags.\n\nHitachi hat neben dem Hitachi Content Platform Gateway auch eine Konsole für die Verwaltung von Datenrichtlinien. Damit wird ein NAS vor den Objektspeicher gestellt, der entweder NFS oder CIFS sein kann. Hinzu kommt Hitachi Content Intelligence; es handelt sich um eine Suchmaschine auf Steroiden. Sie können jedes Objekt in Ihrem Speicher anhand von Metadaten, Datum, Uhrzeit usw. finden. Es kann über Ihre eigene (oder Hitachis?) physische Hardware hinaus auf mehrere Objektspeicher erweitert werden. Die Suche funktioniert für alles. Sie ist auch zu 100 Prozent mit AWS s3 kompatibel.\n\nFür SLED-Kunden besteht die Möglichkeit, über den Partner Flexential von Hitachi Content Platform als Service bereitzustellen. Wenn beispielsweise eine Stadtverwaltung ihr eigenes Objektspeicher nicht verwenden möchte, können sie durch diese Partnerschaft in einer Mehrmandanten-Umgebung sein.\n\nAbschließend kann Ihnen Hitachi System Security helfen, wenn Sie Opfer eines Ransomware-Angriffs werden. Sie können eine Analyse über die Auswirkungen liefern und mit den Erpressern verhandeln. Zudem können sie defensive Maßnahmen zum Schutz Ihrer Organisation ergreifen. Die Angreifer sind keine Einzelkämpfer, sondern kriminelle Organisationen mit Entwicklern und Managern, daher erfordert es eine professionelle Reaktion. Hitachi System Security begleitet eine Organisation von Anfang bis Ende.\n\nWenn Sie Mitglied bei ISSA sind, können Sie erwarten, dass Hitachi eine Veranstaltung in Ihrer Region sponsert.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Colin McLean","David Rowley"],"link":"/episode-EDT75-de","image":"./episodes/edt-75/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect bei Intel, spricht mit Colin McLean von Hitachi, dem Global Team Lead bei Intel, und David Rowley, dem Senior Solutions Consultant, über das umfassende Angebot von Hitachi im Bereich der Cybersicherheit."},{"id":149,"type":"Episode","title":"Ein Tag im Leben eines Cloud Solution Architects.","tags":["csa","cloudsolutionarchitect","multicloud"],"body":"\r\n\r\nDas Intel Cloud Solution Architect Team wurde vor weniger als zwei Jahren mit einigen wenigen Personen gegründet und ist schnell auf 21 Teammitglieder angewachsen. Dieses Wachstum zeigt sowohl intern bei Intel als auch extern bei den Kunden seine Bedeutung auf.\n\nDa Kunden von On-Premises in die Cloud wechseln, um Flexibilität und Skalierbarkeit zu erreichen, ist es wichtig, dass sie verstehen, dass die zugrunde liegende Infrastruktur und Features für die Optimierung eine Rolle spielen. Da die Mehrheit der Cloud-Service-Anbieter, einschließlich der drei größten, Google, Amazon und AWS, von Intel betrieben werden, können Intels CSAs dabei helfen, Dienste entlang der gesamten Cloud-Reise des Kunden zu optimieren.\n\nEin typischer Tag für einen CSA beginnt mit Aufgaben im Backoffice wie E-Mails, internen Meetings, Team-Meetings, Mitarbeiter-Meetings und einer erheblichen Menge an Zeit, in der man mit Endnutzer-Kunden spricht. Dies können Intel-Kunden sein, die derzeit Intel CPUs vor Ort nutzen und jetzt überlegen, in die Cloud zu wechseln oder bereits in der Cloud sind. CSAs haben auch Partner, einschließlich Systemintegratoren, die daran interessiert sind, wie man die Haftungsfähigkeit von Intel übersetzt und einige Features wie ABS-Firewall, Boost oder Verschlüsselung nutzt.\n\nCSAs arbeiten nicht nur in der Vorbereitungsrolle, sondern sie entwerfen auch Konzeptnachweise, erstellen Anwendungsfälle und arbeiten mit einem Team zusammen, um Leistungsindikatoren auf der Grundlage der Arbeitslast eines Kunden zu testen. Außerdem investieren sie Zeit in das Verfassen oder Mitverfassen von Handlungsbüchern.\n\nSie erstellen auch ein Handbuch, um die eigenen Manager von Intel und Vertriebsmitarbeiter der Partner zu schulen.\n\nDarüber hinaus kündigen CSPs täglich neue Dienste an, und CSAs müssen auf dem neuesten Stand der Technologie bleiben, die diese Dienste unterstützt, und verstehen, wie die Dienste Kunden nutzen können. Dies bedeutet, die Arbeitslasten der Benutzer zu verstehen und auch Modelle zu verwenden.\n\nIntel-CSAs sehen sich als vertrauenswürdige Berater, eine Erweiterung des Teams des Kunden, anstatt versuchen, die Entscheidungsfindung zu übernehmen. Oftmals weiß ein Kunde zum Beispiel, welche Cloud er nutzen möchte, benötigt jedoch Hilfe bei der Auswahl der verfügbaren Tools, um seine aktuelle Arbeitslast zu analysieren und sie dann mit den in der Cloud verfügbaren Instanzen in Bezug zu setzen. CSAs werden auch Kunden bei der Kostenanalyse unterstützen und sie darüber aufklären. Intel verfügt über eine Palette von Tools für jede Phase der Reise in die Cloud.\n\nSobald ein Kunde aktiv ist und das erste Projekt abgeschlossen hat, wird er sich an Client Service Advisors (CSA) wenden, um neue Initiativen und Projekte voranzutreiben, während der Zyklus fortgesetzt wird.\n\nCSAs verfügen über einzigartige Fähigkeiten, da sie viel Zeit mit Schreiben, Kommunikation und Bildung verbringen, aber auch alle technischen Aspekte und Kundenbedürfnisse verstehen. Ein CSA sollte zum Beispiel erkennen können, ob es sich bei einem Problem um ein technisches oder ein geschäftliches Problem handelt. Die Hintergründe der CSAs bei Intel sind vielfältig mit unterschiedlichen Kulturen, technischen Fähigkeiten, Verkaufserfahrungen und Arbeitshintergründen, wodurch sie Expertise in verschiedenen Bereichen mitbringen. Sie sind auch eine tiefgehende technische Quelle außerhalb ihres eigenen Teams. Wenn zum Beispiel jemand detailliert über AWS-Dienste sprechen möchte, können sie einen Kollegen von AWS hinzuziehen. Mit anderen Worten: Die CSAs von Intel können eine einzige Kontaktstelle für alle Cloud-Dienstleistungsbedürfnisse des Kunden sein.\n\nWarum sollte ein Kunde das Intel CSA-Team anstelle des Teams eines CSPs nutzen? Intel ist wirklich neutral, da ihre Chips und CPUs in nahezu jedem Cloud-Service-Provider laufen. Es spielt für das CSA keine Rolle, welchen Service ein Kunde wählt. Darüber hinaus verstehen viele der CSAs der Anbieter nicht die zugrunde liegenden Intel-Funktionen, die nur in bestimmten Instanztypen verfügbar sind. Ein Intel CSA würde zum Beispiel wissen, dass er eine Instanz wählen sollte, die von Ice Lake angetrieben wird, anstatt von Cascade Lake, um erhebliche Kosteneinsparungen und eine Leistungssteigerung für ihre Anwendungen zu erzielen. Die CSAs der CSPs wären sich dieser Informationen nicht unbedingt bewusst.\n\nIntel hat auch viele Tools zur Erfassung von Telemetrie, ob es sich um eine Cloud-Instanz oder eine Bare-Metal-Cloud-Instanz handelt. Die CSP-CSAs haben keinen Zugriff auf diese Tools. Sie können Probleme beheben, jedoch nur auf der Ebene des Hypervisors. Wenn ein Kunde ein Problem hat, kann ein Intel-CSA bis auf die Chip-Ebene gehen und zur Lösung des Problems Fehlerbehebungstools und Telemetrie verwenden.\n\nDie beste Möglichkeit, Kontakt mit dem Intel CSA-Team aufzunehmen, ist über einen Account Executive. Technisch gesehen sind die CSAs Teil der Vertriebs- und Marketinggruppe, daher suchen sie aktiv nach Möglichkeiten, beispielsweise nach Kontakten aus der Vergangenheit.\n\nCSA-Dienste sind keine zusätzlichen Kosten. Tatsächlich hat Intel ein Programm, bei dem sie die anfängliche Migration in die Cloud finanzieren werden. CSAs bringen sowohl Werkzeuge, Personal und Fachkenntnisse sowohl aus Sicht der Personalressourcen als auch der Überwachung mit sich. Dadurch wird einem Kunden in einer Neuentwicklungsumgebung geholfen, die Lernkurve zu verkürzen. Dann kommt die Monetarisierung, wenn eine Arbeitslast vollständig in der Cloud ausgeführt wird oder eine Migration stattfindet, bei der Ressourcen verbraucht werden.\n\nIntel CSAs haben kein persönliches Interesse daran, welchen Cloud-Service-Provider die Kunden verwenden, sondern nur daran, dass sie in einer hochsicheren und zuverlässigen Umgebung optimiert sind.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Kiran Agrahara"],"link":"/episode-EDT76-de","image":"./episodes/edt-76/de/thumbnail.png","lang":"de","summary":"In dieser Episode spricht Darren mit Kiran Agrahara darüber, was Intel Cloud Solution Architects (CSAs) den ganzen Tag über tun, um nicht nur Cloud-Service-Anbieter (CSPs), sondern auch Endbenutzern Vorteile zu bringen."},{"id":150,"type":"Episode","title":"Digitale Transformation im Jahr 2022","tags":["aiml","comms","data","edge","multicloud","cybersecurity"],"body":"\r\n\r\nAls Chief Data Scientist für den öffentlichen Sektor von Intel verbringt Gretchen ihre Tage damit, mit Kunden über ihre Datenherausforderungen, Datenmanagement, Datenverwaltung, die Ethik dessen, was sie tun, und verantwortliche künstliche Intelligenz zu sprechen.\n\nAnna's sechs Jahre bei Intel waren auf das Internet der Dinge (IoT) und die Edge-Computing-Technologie konzentriert, wobei sie die letzten drei Jahre im öffentlichen Sektor tätig war. Beide sind sich einig, dass das Jahr 2022 ein spannendes Jahr sein wird, voller bahnbrechender Möglichkeiten.\n\nDarren, Gretchen und Anna repräsentieren jeweils verschiedene Teile der digitalen Transformation mit Kunden, kamen jedoch zusammen, um eine gemeinsame Art und Weise zu finden, über das Thema zu sprechen, mit sechs Säulen der digitalen Transformation: Multi-Cloud-Computing, Edge Computing, künstliche Intelligenz, maschinelles Lernen, Cybersicherheit, Datenmanagement und Kommunikation. Sie sind sich einig, dass dies die Bereiche sein werden, in denen im kommenden Jahr große Veränderungen stattfinden werden. Intel ist tief in allen diesen Bereichen involviert, da sie die Hardware und Software entwickelt haben, um jede der Säulen zu unterstützen. Die Säulen sind miteinander verflochten und Intel spielt eine Schlüsselrolle bei der Weiterentwicklung der Branche.\n\nIntel-Hardware bildet das Fundament für die Mehrheit der Cloud-Anbieter. Aus Sicht der Software hat Intel die gängigen Frameworks für KI, maschinelles Lernen und Deep Learning optimiert, um das Beste aus der darunter liegenden Hardware herauszuholen. In manchen Fällen ist die Leistung dank der Intel-Software zehn- bis hundertmal besser.\n\nNicht nur hat Intel Hardware für Edge und Kommunikation, insbesondere im Bereich 5G, bereitgestellt, sondern hat diese auch mit software-basierten Referenzarchitekturen erweitert. Darüber hinaus versucht Intel, die Ökosysteme für alle funktionieren zu lassen. Es liegt ein starker Fokus auf offenen Systemen in einer nicht-proprietären Art und Weise, wodurch es für Neueinsteiger und etablierte Unternehmen einfacher wird, ihre Reichweite in diesen neuen Märkten zu erweitern und aufregende Fortschritte anzubieten. Intel ist einer der, wenn nicht sogar der größte Beitragende zur Open-Source-Software-Community.\n\n## Cyber-Sicherheit Säule\n\nIn Intels Hardware-Designs sind implizit Sicherheitsmerkmale und -funktionen integriert, um sicherzustellen, dass Kunden im Ökosystem ihre Daten in allen möglichen Variationen schützen können.\n\nSicherheit ist niemals statisch; sie entwickelt sich kontinuierlich weiter. Allein mit Intel werden die Sicherheitsprobleme einer Organisation nicht gelöst. Sicherheit ist ein Zusammenspiel von dem, was Sie mit Ihrer Hardware machen, wie Sie die richtigen Softwareelemente einbinden und den Grenzen und Richtlinien Ihrer Organisation.\n\nIntel konzentriert sich auf mehrere Bereiche, aber eine grundlegende Rolle spielt das Hardware-Root-of-Trust und die Authentifizierung. Viele Funktionen können direkt integriert werden, um dies zu erreichen. Ein weiterer Schritt sind vertrauenswürdige oder transparente Lieferketten, die Intel mit Kunden teilen kann und die ein hohes Maß an Vertrauen vermitteln. Diese Fähigkeiten werden stetig verbessert und Intel arbeitet kontinuierlich daran, Fortschritte zu erzielen.\n\nCyberangriffe stehen bei vielen Kunden aufgrund jüngster Verstöße ganz oben auf der Agenda. Intel verfügt über Sicherheitsfunktionen wie vertrauenswürdige Ausführung und sichere Enklaven. Sie bieten eine ganze Reihe von Möglichkeiten für die gewünschte Art von Sicherheit, einschließlich Verschlüsselung, ohne die Leistung stark zu beeinträchtigen. Es gibt eine ganze Matrix von Fähigkeiten, einschließlich eines anderen Anforderungssatzes an den Randbereich aufgrund des Verlusts der physischen Sicherheit des Rechenzentrums.\n\nArchitekturen mit Null-Vertrauen werden zu den Rahmenbedingungen, die besonders im öffentlichen Sektor vorherrschend sind. Für das Verteidigungsministerium ist Null-Vertrauen eine Verpflichtung. Intel besitzt viele Fähigkeiten, die in Null-Vertrauen eingebunden sind.\n\nEine Größe passt nicht für alles bei Sicherheit, aber es ist klar, dass nachträgliche Sicherheitsmaßnahmen etwas aus der Vergangenheit sind. Sicherheit muss von Anfang an eingebaut werden und kontinuierlich weiterentwickelt werden. Organisationen müssen ständig fragen, ob sie die richtigen Protokolle eingerichtet haben, ob sie die richtigen Bedrohungserkennungswerkzeuge haben und ob entlang der Lieferkette Vertrauen besteht. All dies ist integriert.\n\n## Edge Computing Pfeiler\n\nJetzt, da so viel mit KI, ML und verschiedenen Algorithmen möglich ist, ist es aufregend zu sehen, wie wir diese Dinge am Rand nutzen und die Architekturen optimieren können, um diese Anwendungsfälle zu unterstützen.\n\nEs gibt einige wirklich einfache Modelle, bei denen alles in der Cloud lebt und nur die Datenbeschaffung am Rand erfolgt. Wenn die Konnektivität es ermöglicht, können die Latenzen mit den Anwendungen übereinstimmen. Der Großteil der Berechnungen kann cloudzentrisch sein. Es gibt jedoch unzählige Anwendungsfälle, bei denen dies aufgrund der Sensibilität der Daten oder der Latenzanforderungen keinen Sinn ergibt. Daher gibt es interessante Gespräche darüber, wie optimale Architekturen für den Rand und das Geschehen in der Cloud und im Netzwerk herausgefunden werden können.\n\nZum Beispiel, könnten wir eine nahtlose Architektur haben, um Daten unmittelbar zu sammeln, nutzen und verarbeiten, um Informationen zu liefern? Können wir das in die nächste Runde der Schulungen integrieren, damit das Modell kontinuierlich aktualisiert wird? Wie schnell können wir diese Schleife machen? Ist das machbar? Brauchen wir alle unsere Schulung auf der Cloud? Wenn alle Schulungen auf der Cloud sind, wie oft sollten wir die aktualisierten Modelle herunterladen? Und kann der Edge leicht genug sein, um die auf der Cloud generierten Inhalte zu nutzen? Der Edge ist immer noch sehr kompliziert und es gibt viele Möglichkeiten und faszinierende Fragen.\n\n## Künstliche Intelligenz Säule\n\nKI und ML ermöglichen dem Edge viel mehr als bisher gedacht. Ein Produkt ist nicht immer die richtige Antwort: Es geht um die passende Lösung für den jeweiligen Zweck. Die Verwendung von Open Source ist entscheidend, ebenso die Möglichkeit, Microservices zu nutzen, um Algorithmen am Edge auszuführen.\n\nZum Beispiel, wenn Sie Algorithmen direkt am Rand haben, die die Leseaufgaben erledigen, sprechen wir über den Verkehrsfluss. Die Abfolge von roten, gelben und grünen Ampeln kann sich je nach Anzahl der durchfahrenden Autos dynamisch ändern und gleichzeitig Daten sammeln, die an ein größeres Rechenzentrum weitergeleitet werden können, um eine Nachschulung durchzuführen. Bis Ende der Woche ergibt es vielleicht Sinn, einige zusätzliche Mikrodienste oder Algorithmusanpassungen hinzuzufügen. Dann geht dieser Container zurück an den Rand, und Sie können besser reagieren und weiterlernen.\n\nAuch beim Thema Eignung, Bandbreite, Latenz und Formfaktor sind alles wichtige Überlegungen.\n\nErnteerträge sind ein gutes Beispiel. Ein Kunde sammelt Daten, die an ein Rechenzentrum gehen. Sie arbeiten an den Modellen, aber es führt nicht dazu, dass sie wissen, dass sie mehr Dünger brauchen oder dass es aktuell Herausforderungen mit Sonne und Regen gibt. Das bedeutet, dass die Formel geändert werden muss. Du benötigst die Ernteertragsdaten, die Informationen, den Algorithmus und die Mikroservices in einem viel kleineren Formfaktor. Die Latenz und Bandbreite sind unterschiedlich, aber du kannst eine kleine Einheit inmitten eines Feldes haben, die diese Daten sammelt und zum Beispiel auf den Wasserfluss oder den Bedarf an Dünger reagiert, um die Ernteerträge zu verbessern.\n\nHoffentlich werden in diesem Jahr mehr Randdesigns standardisiert. Mit FlexRAM und 5G gibt es Standards, aber alles andere ist der wilde Westen. Viele Menschen entwerfen interessante Dinge, aber sie entwerfen sie nicht auf eine Art und Weise, die es leicht macht, diese Mikroservices in einem Container und ML- und KI-Algorithmen zu haben. In einigen Fällen benötigt man mehrere Algorithmen, die auf unterschiedliche Weise gewichtet sind und sich jede Woche basierend auf neuen Daten und dem neuen Training ändern. Wir müssen in der Lage sein, das so zu tun, dass es egal ist, wer ein Gerät gebaut hat. Die Schaffung von Standards nicht nur in KI- und ML-Daten, sondern auch am Rand, wird die Fähigkeiten explodieren lassen.\n\n## Kommunikations-Säule\n\nDie kommerzielle Seite von 5G wird den Weg mit 5G auf Handys überall bahnen. Es gibt jedoch immer noch eine Verzögerung, wenn es um die Art der Benutzergeräte geht, um verschiedene Arten von Anwendungen für Edge oder Unternehmen durchzuführen, zum Beispiel. Intel richtet ihre ersten 5G-Netzwerke mit Partnern ein, die eher innovative und weniger kommerzielle Partner sind. Obwohl deren kommerzielle Partner diese bereits seit langem im Einsatz haben, werden private Netzwerke, die für einen spezifischen Zweck kontrolliert werden, befreit und Anwendungen werden entwickelt. 2022 ist das Jahr, in dem diese Dinge verwirklicht werden.\n\n## Pfeiler für Datenmanagement\n\nDaten, die früher Stunden oder Tage benötigten, um aufgenommen, vorbereitet, analysiert und darauf reagiert zu werden, können jetzt innerhalb von Minuten oder Nanosekunden erledigt werden. Sie können auch verschiedene Modelle nutzen und bei Änderungen der Gewichtungen schnell handeln und die Daten aufnehmen und bessere Services bereitstellen. Das Verschieben von Daten, die Verwaltung dieser Daten und die Operationalisierung Ihrer KI und ML sind Teil dessen, was das Datenmanagement in die Welt bringt.\n\n## Multi-Cloud-Säule\n\nDie Multi-Cloud-Säule bedeutet in diesem Zusammenhang nicht Cloud-Service-Provider. Es bezieht sich auf die Infrastruktur im Allgemeinen und darauf, wie diese Infrastruktur abstrahiert wird, um neue Funktionen an den Rand, an einen Cloud-Service-Provider oder sogar an die eigene Rechenzentrum-Infrastruktur bereitzustellen. Das Ziel der Multi-Cloud-Architektur besteht darin, dass Sie die wichtigen Benutzer und vor allem die Art und Weise, wie die Daten verwaltet werden, kennen.\n\nVerschiedene Clouds haben unterschiedliche Fähigkeiten und je nach Anwendungsfall können verschiedene Clouds für unterschiedliche Zwecke verwendet werden. Intel hat Cloud-Lösungsarchitekten, die Kunden dabei unterstützen, Workloads zwischen den verschiedenen Cloud-Angeboten zu optimieren.\n\nAlle diese Säulen sind miteinander verwoben und arbeiten zusammen. Sehen Sie in zukünftigen Folgen nach, in denen Darren, Gretchen und Anna dieses Gespräch fortsetzen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Gretchen Stewart","Anna Scott"],"link":"/episode-EDT77-de","image":"./episodes/edt-77/de/thumbnail.png","lang":"de","summary":"Intel's Darren Pulsipher, Chief Solution Architect, Dr. Anna Scott, Chief Edge Architect, und Gretchen Stewart, Chief Data Scientist, diskutieren die sechs Säulen der digitalen Transformation im Jahr 2022: Multi-Cloud Computing, Edge Computing, Künstliche Intelligenz, Maschinelles Lernen, Cyber-Sicherheit, Datenmanagement und Kommunikation."},{"id":151,"type":"Episode","title":"Verständnis des Sicherheitsmodells der gemeinsamen Verantwortung","tags":["multicloud","compute","cybersecurity","sharedresponsibility","cloudsecurity","cloud","technology","process"],"body":"\r\n\r\n## Sicherheit in der Cloud\n\nDie meisten Menschen verstehen die Verantwortung für die Sicherheit vor Ort, aber die Verantwortung wird in der Cloud undurchsichtiger. Wenn Daten in der Cloud geschützt sind, wie werden sie geschützt? Wer ist für diese Sicherheit verantwortlich? Was ist mit der Aktualisierung von Maschinen?\n\nKürzlich haben Cloud-Service-Anbieter begonnen, noch mehr Dienste anzubieten, wodurch verschiedene Modelle entstehen. Manchmal geht dabei die Sicherheit verloren.\n\n## Sicherheitsbereiche\n\nEs gibt vier Hauptbereiche der Sicherheit, die wir verstehen müssen.\n\n## Körperlich\n\nDie physische Sicherheit ist leicht zu erkennen. Cloud-Dienstleister sind für die physische Sicherheit ihrer Rechenzentren verantwortlich, und Sie sind für die physische Sicherheit Ihrer eigenen Rechenzentren verantwortlich. Zusätzlich müssen Sie Ihren physischen Raum sichern. Wenn Sie beispielsweise ein Hersteller sind, müssen Sie die Maschinen innerhalb Ihres Bereichs sichern. Ein kürzlich erfolgter Hack erfolgte über das HVAC-System, das in das Netzwerk einer Organisation eingesteckt war.\n\n## Infrastruktur\n\nDie Sicherheit der Infrastruktur bezieht sich nicht auf den physischen Aspekt, sondern auf die Hardware selbst. Haben Ihre Netzwerk-Switches die richtigen Sicherheitskorrekturen und Upgrades? Werden die Laufwerks- und Speichergeräte aktualisiert? Werden sie geschützt? Die Infrastruktur kann bei Cloud-Dienstanbietern in Grauzonen fallen, daher müssen Sie wissen, wer für was und unter welchen Umständen verantwortlich ist.\n\n## Bewerbung\n\nFür die Anwendungssicherheit müssen Sie wissen, wer Zugriff auf eine Anwendung hat und ob sie mit den richtigen Sicherheitspatches aktualisiert wird.\n\n## Daten\n\nDas Schützen Ihrer Daten ist eine der wichtigsten Aufgaben, die Sie erledigen müssen. Daten können tatsächlich als Waffe bei einem Ransomware-Angriff eingesetzt werden, bei dem Angreifer sie entwenden oder verschlüsseln. Sie werden auch für starke Wettbewerbsvorteile in verschiedenen Organisationen genutzt. Sie müssen wissen, wo sich Ihre Daten befinden und wie Sie sie schützen können.\n\n## Cloud Service Delivery Modelle\n\nEs gibt drei grundlegende Modelle für die Bereitstellung von Cloud-Diensten. Es werden ständig neue Modelle erstellt, aber die drei wichtigsten sind Infrastructure as a Service (IAAS), Platform as a Service (PAAS) und Software as a Service (SAAS). Alles andere werden wir als X as a Service (XAAS) kategorisieren.\n\n## Infrastruktur als Dienstleistung\n\nIAAS bedeutet, dass Sie von einem Cloud-Service-Anbieter mieten. Dabei handelt es sich um virtuelle Maschinen und virtuelle Netzwerke, also Speicher, Berechnung und Netzwerk. Wir sehen auch immer mehr interessante Möglichkeiten mit Beschleunigern wie GPUs oder sogar neuromorphen Prozessoren. IAAS ist der Ort, an dem Sie Ihre Anwendungen ausführen.\n\n## Plattform als Dienstleistung\n\nDie nächste Schicht im Stack ist PAAS. Hier erhält man ein bestimmtes Framework wie Kubernetes, um Dinge auszuführen. VMware, das auf IAAS läuft, ist PAAS. CICD-Pipelines als Service haben viele Tools, die in diesen Bereich passen. PAAS ermöglicht es Ihnen, neue Dienste auf dieser Plattform aufzubauen und bereitzustellen, so dass Sie leicht große Systeme verwalten können, die auf IAAS aufgebaut sind.\n\n## Software als Dienstleistung\n\nAls nächstes kommt SAAS. Das ist spezifische Software, die vom Softwareanbieter oder dem Cloud-Dienstanbieter verwaltet wird, oder es könnte sich um ein von einem Dritten angebotenes SAAS für jemand anderen handeln. Der Schlüssel bei SAAS ist, dass sie für die Anwendungssicherheit verantwortlich sind. Sie verwalten die Betriebszeit und alle verwaltungstechnischen Bereiche wie Zuverlässigkeit, Sicherheit und Integrität. Viele der SAAS-Programme werden auf PAAS-Plattformen aufgebaut.\n\n## X als Dienst\n\nXAAS kann jeden neuen Service wie künstliche Intelligenz, Geräteverwaltung oder Sicherheitserkennung umfassen.\n\nDas Verständnis dieser verschiedenen Bereitstellungsmodelle ist wichtig, da geteilte Sicherheitsmodelle von Cloud-Service-Anbietern auf dem Bereitstellungsmodell basieren.\n\n## Cyber-Domänen\n\nJeder der sechs Pfeiler der Cybersicherheit, wie von Steve Warren, CTO bei Intel, in einem früheren Podcast identifiziert, ist wichtig, egal ob Sie sich in der Cloud, On-Prem oder am Rande befinden. Die sechs Pfeiler sind Bedrohungserkennung, Intelligenz, Analyse und Orchestrierung; Identitäts- und Zugangsmanagement; Datensicherheit und Anwendungssicherheit; Netzwerksicherheit; Lieferketten-Sicherheit; und Host- und System-Sicherheit. Alle sechs dieser Bereiche passen in das Modell der gemeinsamen Verantwortung, das Cloud-Service-Anbieter propagieren.\n\n## Geteilte Verantwortungssicherheitsmatrix\n\nDiese gemeinsame Verantwortung wird in der Matrix veranschaulicht. Die Lieferungstechniken des Service-Modells sind auf der vertikalen Achse: SAAS, PAAS, IAAS und On-Prem. Wenn Sie selbst hosten, liegt die gesamte Verantwortung ganz rechts bei Ihnen.\n\nAuf der IAAS-Seite sind Sie vollständig verantwortlich für die Sicherheit von Daten und Anwendungen und zur Hälfte verantwortlich für die Infrastruktur, da Sie immer noch für die Netzwerksteuerung und das Betriebssystem verantwortlich sind. Der Cloud-Service-Anbieter ist für das physische Netzwerk und den Host verantwortlich.\n\nAuf der PAAS-Ebene sind Sie immer noch für die Datensicherheit verantwortlich und zu einem gewissen Anteil für die Anwendungssicherheit sowie die Identitäts- und Verzeichnisinfrastruktur. Es gibt einige Tools, die in diesen Bereichen unterstützen können. Obwohl Sie für Anwendungen und ihre Plattformen verantwortlich sind, sind sie für die bereitgestellten Frameworks und Middleware verantwortlich. Obwohl sich das PAAS-Layer um den Großteil des Betriebssystems kümmert und Ihnen einige Tools auf höherer Ebene zur Verfügung stellt, obliegt Ihnen immer noch die Konfiguration der Netzwerksteuerungen.\n\nOben im Stapel auf SAAS, auch wenn Sie Speicherdienste, Datendienste oder CRM wie Salesforce verwenden, sind Sie immer noch für Ihre Daten verantwortlich, da Sie Ihre Backups entwerfen und verschlüsseln sowie Konten und Identitäten verwalten müssen.\n\nEin wichtiger Punkt, der bei den Modellen durchgehend gilt, ist, dass Sie für die Sicherheit Ihrer Daten verantwortlich sind. Es gibt niemals eine Situation, in der Sie Ihre gesamte Sicherheit den Cloud-Dienstanbietern überlassen. Sie müssen Ihre Daten sichern und prüfen, ob Sie objektspezifischen Speicher verwenden, damit Sie sich bei einem Ransomware-Angriff wiederherstellen können. Außerdem sollten Sie sicherstellen, dass Sie Ihr Zugriffsmanagement korrekt verwalten und Tools verwenden, die dies einfach machen.\n\n## Unterschiedliche Ansätze zur Sicherheit\n\nJeder der drei besten Cloud-Service-Anbieter verfolgt einen anderen Ansatz in Bezug auf Sicherheit, insbesondere bei der Netzwerkkonfiguration.\n\n## AWS can be translated to German as Amazon Web Services.\n\nAWS konzentriert sich auf Prävention. Wenn Sie eine virtuelle Maschine starten, sind standardmäßig keine Ports geöffnet, sodass Sie Sicherheitsgruppen erstellen müssen. AWS ist am restriktivsten und verwendet IAM zur Identitätsverwaltung. AWS ist ideal für mittelgroße Teams, aber weniger gut für sehr große Organisationen geeignet.\n\n## Azure\n\nAzure konzentriert sich mehr auf Benutzerfreundlichkeit; Sicherheit ist weniger restriktiv. Sie verwenden das Konzept virtueller Netzwerke für die Sicherheit, so dass alle VMs im selben virtuellen Netzwerk miteinander kommunizieren können. Dies steht im Gegensatz zum Zero-Trust-Prinzip, daher müssen Sie entscheiden, was für Sie wichtiger ist. Azure verwendet Active Directory, daher ist es, wenn Sie bereits über ein ausgereiftes, solides Active Directory verfügen, eine gute Wahl für das Identitätsmanagement.\n\n## Google Cloud Platform\n\nDie Google Cloud Platform (GCP) legt ebenfalls Wert auf Benutzerfreundlichkeit, setzt aber mehr auf virtuelle Maschinen (VMs) und Netzwerksicherheit. Sie können Profile erstellen, die alles auf einer VM absichern oder ein Profil wählen, das etwas mehr Verschlüsse lockert. In Bezug auf Restriktivität liegen sie in der Mitte. Obwohl GCP nicht so umfangreich ist wie AWS oder Active Directory, bietet sie eine gute Identitätsverwaltung.\n\nAlle drei dieser Cloud-Service-Anbieter bieten IAAS, PAAS, SAAS, Container as a Service und eine Vielzahl von XAAS an. Sie müssen das Sicherheitsmodell bewerten und die Unterschiede in jedem verstehen.\n\nIn gewisser Hinsicht ist es schwieriger, das Sicherheitsmodell der geteilten Verantwortung zu verstehen als einfach alles lokal auszuführen, da nun mehr Akteure beteiligt sind und die Komplexität zunimmt. Der Schlüssel besteht darin, die Modelle zu verstehen und verfügbare Tools zu nutzen, um die Sicherheit über mehrere Clouds hinweg zu verwalten.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT78-de","image":"./episodes/edt-78/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chef-Solution-Architekt bei Intel, erklärt gemeinsame Sicherheitsmodelle der Verantwortung. Wer für die Sicherheit verantwortlich ist, kann in der Cloud verwirrend werden; die Verantwortung hängt von den Cloud-Dienstbereitstellungsmodellen und anderen Faktoren ab."},{"id":152,"type":"Episode","title":"Die Integration von Fernarbeitern","tags":["covid","remoteworker","mobilementor","genz","people","process","compute","cloud"],"body":"\r\n\r\nNach einer internationalen Karriere bei Nokia gründete Denis vor 17 Jahren Mobile Mentor. Das Unternehmen entstand aus seiner Erfahrung, dass die Menschen die Technologie in ihren Smartphones nicht nutzen. Obwohl Smartphones über fortschrittliche Technologie verfügten, wurden sie nur für grundlegende Funktionen wie Telefonanrufe und Textnachrichten verwendet. Ein wegweisender Moment kam, als Denis versuchte, einem CEO eine Netzwerklösung zu verkaufen und der CEO fragte, warum seine Firma noch mehr Infrastruktur kaufen sollte, wenn die Kunden das bereits gekaufte nicht nutzen würden.\n\nDenis fragte sich, warum die Technologie so weit voraus war, als das, was Verbraucher tatsächlich mit ihren Geräten machen wollten. Er verließ Nokia und gründete Mobile Mentor, um dieses Problem zu lösen. Er stellte eine Armee von technikversierten Leuten ein, die mit Geschäftskunden zusammenarbeiteten und ihnen halfen, ihre Smartphones zum Laufen zu bringen und produktiv zu sein. Dabei lernte er viel über das, was die Technologieakzeptanz und Gewohnheiten antreibt.\n\nHeutzutage haben Fernarbeiter möglicherweise zwei oder drei Geräte, zusätzlich zu persönlichen Geräten. Sie arbeiten von zu Hause, vom Büro aus und sie reisen. Der größte Teil der Arbeit, die Mobile Mentor derzeit leistet, besteht darin, dafür zu sorgen, dass diese Geräte funktionieren und vor allem sicher sind.\n\nDa die Anzahl der Fernarbeiter aufgrund von COVID stark angestiegen ist und viele Organisationen keine angemessene Sicherheit implementiert oder priorisiert haben, gab es und gibt es noch immer eine Lawine von Hacks und Ransomware-Angriffen. Seit Beginn der Pandemie sind Ransomware-Angriffe um 500% gestiegen und richten sich tragischerweise gegen Schulen, Krankenhäuser und kommunale Organisationen.\n\nZusätzlich dazu konnten aufgrund des Chipmangels viele Unternehmen nicht genügend Geräte für Fernarbeiter erhalten und mussten sich stattdessen auf BYOD (Bring Your Own Device) verlassen. Die Nutzung heimischer Internetquellen erhöhte auch das Risikoprofil.\n\nEine Sache, die Organisationen tun können, um das Risiko zu verringern, ist auf die Verwendung von Passwörtern zu verzichten. Passwörter waren eine großartige Idee im Jahr 1961, aber im Jahr 2021 zeigten Daten, dass sie der Hauptgrund dafür waren, dass Organisationen gehackt wurden. Die meisten Angriffe beginnen mit einem kompromittierten Passwort aus einer Phishing-Operation.\n\nWissensarbeiter haben heutzutage eine lächerliche Anzahl von Passwörtern. Doch nur 31% der Menschen verwenden ein Passwortverwaltungstool. Weitere 31% schreiben ihre Arbeitspasswörter in ein persönliches Tagebuch, und 24% notieren ihre Passwörter in einer Notizanwendung auf ihren Smartphones. Alarmierenderweise haben laut einer BBC-Studie im letzten Jahr 15% der britischen Bevölkerung den Namen ihres Haustieres als Passwort verwendet und 6% das Wort \"Passwort\".\n\nDer erste Schritt, um sich von Passwörtern zu lösen, besteht darin, Biometrie zu nutzen. Jetzt scannt ein iPhone oder eine Windows Surface Maschine dein Gesicht und meldet dich im Betriebssystem sowie in allen Single Sign-On Anwendungen und Drittanbieteranwendungen an, für die du diese nachgewiesene Identität besitzt. Das ist ein guter Anfang. Die Zukunft der Reduzierung der Passwortnutzung wird eine Kombination aus Biometrie und Zwei-Faktor-Authentifizierung überall sein. In der Übergangszeit, solange es noch veraltete Identitätsinfrastrukturen und veraltete Anwendungen gibt, bei denen Biometrie nicht funktioniert, macht ein Passwort-Management-Tool Sinn.\n\nMobile Mentor hat die Branche nach Generationen untersucht und festgestellt, dass Generation Z die meisten Passwörter hat. Viele dieser Menschen in ihren frühen Zwanzigern sind während der Pandemie in die Arbeitswelt eingestiegen und haben ihre Arbeit ferngesteuert aufgenommen. In vielen Fällen haben sie ihren Arbeitgeber nie persönlich getroffen und keine sozialen Verbindungen erlebt, die in einer Arbeitsumgebung entstehen. Sie haben eine einzigartige Perspektive auf die Beurteilung ihres Arbeitgebers.\n\nMobile Mentor's Forschung zeigt, dass Menschen lieber von zu Hause aus arbeiten, aber in allen Branchen glauben sie, dass sie im Büro produktiver sind. Dies ist ein interessantes Dilemma und eine Dichotomie für den Arbeitgeber, um diese Menschen ins Büro zu bringen. Die Forschung zeigt, dass 67% der Generation Z glauben, dass andere Unternehmen eine bessere Technologie für ihre Mitarbeiter bereitstellen. Wenn ein Arbeitgeber darauf besteht, dass sie ins Büro kommen, könnten sie sich für einen anderen Job entscheiden. Einen Job zu wechseln bedeutet heute nicht mehr, den Arbeitsweg oder irgendetwas anderes außer einem anderen Laptop zu ändern. Das ist ein Teil dessen, was bei der großen Kündigung passiert.\n\nMit dieser Dynamik spielt die technologische Erfahrung eine Rolle. Untersuchungen zeigen, dass es im Durchschnitt drei Tage dauert, einen Laptop vollständig für die Arbeit zu konfigurieren, im Vergleich zu zwei Tagen für einen Büroangestellten. Fernarbeitende müssen im Durchschnitt drei Service-Desk-Tickets erstellen, um ihr Gerät zum Laufen zu bringen, daher kann ihre Erfahrung schmerzhaft sein. Sie mögen das Stigma, um Hilfe zu bitten, nicht.\n\nDer beste Weg, um dieses Problem zu beheben, besteht darin, den Prozess zu vereinfachen. Die ideale Lösung ist die automatische Bereitstellung (Zero-Touch Provisioning), bei der die Technologie so konfiguriert wird, dass ein Unternehmen Geräte direkt an einen entfernten Mitarbeiter versenden kann und diese sich automatisch konfigurieren, wenn der Mitarbeiter sich mit seinen Arbeitsdaten einloggt. Alles funktioniert in weniger als einer Stunde und niemand im IT-Bereich musste die Geräte manuell konfigurieren, erneut verpacken und an den Mitarbeiter verschicken. Es ist viel Vorarbeit erforderlich, um dies zu erreichen, aber Mobile Mentor kann Kunden bei diesem Prozess helfen.\n\nDenis glaubt, dass IT-Leiter wichtige Konzepte lernen werden, indem sie Gen Z und Remote-Arbeiter studieren. Gen Z-Arbeiter bringen unterschiedliche Einstellungen, insbesondere in Bezug auf Sicherheit und Privatsphäre, mit sich. Sie schätzen und priorisieren persönliche Privatsphäre fast vier zu eins gegenüber Sicherheit. Für eine Generation, die mit sozialen Medien aufgewachsen ist, ist dies schwer zu verstehen, aber die Daten sind eindeutig. Sie sind überaus bewusst von den Datenschutzrichtlinien ihres Arbeitgebers, aber nahezu blind für unternehmensweite Sicherheitsinitiativen.\n\nDenis' Empfehlung für CIOs in Bezug auf dieses Thema besteht darin, Datenschutz und Sicherheit als zwei Seiten einer Medaille zu positionieren. Die Generation Z kann für Sicherheitsmaßnahmen gewonnen werden, indem man es als Schutz ihrer eigenen Daten sowie der Daten des Unternehmens und somit der Kunden darstellt.\n\nMobile Mentors Forschung zeigt, dass der Schatten-IT von Fernarbeitern vorangetrieben und beschleunigt wird. Fernarbeiter, die weit vom Hauptsitz entfernt leben, könnten an einem IT-Team teilnehmen, das sie nie getroffen haben, und Anwendungen, Speichermechanismen sowie Kommunikations- und Zusammenarbeitswege finden, von denen ihre Unternehmen nichts wissen. Die Grenzen zwischen persönlicher und beruflicher Nutzung verschwimmen ebenfalls. Menschen nutzen persönliche Geräte für die Arbeit, und fast die Hälfte lässt ihre Familienmitglieder mit ihren Arbeitsgeräten spielen. Die gleiche Anzahl empfindet die Sicherheitsrichtlinien ihres Unternehmens als zu restriktiv, und ein Drittel gibt an, einen Weg gefunden zu haben, die Richtlinien zu umgehen. Zwei Drittel geben an, effizienter zu sein, wenn sie Verbraucher-Apps wie Gmail und Dropbox nutzen.\n\nDenis rät CIOs, fernarbeitende Mitarbeiter in zukünftige Produktentscheidungen einzubeziehen, da sie diejenigen sind, die die Zusammenarbeitstools, Speicherwerkzeuge, Anwendungen und Authentifizierungsprozesse schneller als jeder Büromitarbeiter auf den Prüfstand stellen werden.\n\nFür weitere Informationen über Mobile Mentor besuchen Sie mobile-mentor.com. Es gibt eine separate Website, endpointecosystem.com, auf der sie all ihre Forschung kostenlos teilen, um Unternehmen über die Vorgänge mit Fernarbeitern auf dem Laufenden zu halten und die nächste Welle von Cyberangriffen zu vermeiden sowie die Technologieerfahrung der eingearbeiteten Mitarbeiter zu verbessern.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Denis O&#39;Shea"],"link":"/episode-EDT79-de","image":"./episodes/edt-79/de/thumbnail.jpg","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect, Intel, Denis O'Shea, Gründer von Mobile Mentor, über seine Erfahrung, Forschung und Ratschläge zur Einarbeitung von Remote-Mitarbeitern, insbesondere der Generation Z."},{"id":153,"type":"Episode","title":"Sicherung des Heimarbeiters Teil 2","tags":null,"body":"\r\n\r\n## Clienttechnologien zur Absicherung des Telearbeiters\n\nSicherheit auf der Client-Seite beginnt und endet mit der Möglichkeit, den Client mittels eines sicheren Boots verifizieren zu können. Intel hat diese Technologie in unseren Clientsystemen seit Jahren mit Secure Boot mit Trusted Execution Technology (TXT) und kürzlich auch mit BootGuard (BtG) bereitgestellt. Auf den neuesten Plattformen haben wir Intel Hardware Shield, eine Reihe von Technologien, die das System auf seiner niedrigsten Ebene, auf Firmware- und BIOS-Ebene, absichern. So kann beispielsweise ein Unternehmenszugangskontrollsystem einen Client überprüfen, ob er sicher gebootet wurde und die korrekte Firmware sowie die richtigen Sicherheitskontrollen installiert hat, bevor er Zugriff auf das Unternehmensnetzwerk erhält.\n\nIntel hat im Laufe der Jahre auch viel Arbeit geleistet, um Organisationen dabei zu helfen, ihre Daten besser zu schützen. Außerdem haben wir es dem Kunden ermöglicht, die Verschlüsselung überall ohne Leistungseinbußen einzuschalten. Wir haben in beinahe jeder Generation unserer Produkte neue Anweisungen implementiert, um die Sicherheit des Telearbeiters zu stärken.\n\nUnser weiterer Schwerpunkt lag darauf, Anwendungen und ihre Daten während der Nutzung zu schützen. Hier kommen Intel Software Guard Extensions (SGX) ins Spiel. Dies ermöglicht es Organisationen, wichtige Teile von Anwendungen und wichtige Daten in sicheren, verschlüsselten Speicherumgebungen zu platzieren. Mit Telearbeit bedeutet dies, dass Sie Anwendungen in nicht vertrauenswürdigen Umgebungen bereitstellen können und dennoch ein hohes Sicherheitsniveau aufrechterhalten.\n\nZusätzlich ermöglicht die neue Technologie, die Intel in der Kategorie der Bedrohungserkennung eingeführt hat, Organisationen eine umfassende Einblick in die Vorgänge der Plattform, um Bedrohungen zu überwachen; keine Malware kann sich verbergen. Diese Technologien revolutionieren die Art und Weise, wie wir mithilfe von maschinellem Lernen und künstlicher Intelligenz Malware erkennen.\n\n## Datenzentrumstechnologien zur Sicherung des Telearbeiters\n\nAuch wenn Sie die Kunden gesichert haben, müssen Sie auch das Rechenzentrum sichern. Beide Seiten müssen geschützt werden, denn Ihr System ist nur so sicher wie das schwächste Glied. Viele der Techniken zur Sicherung des Telearbeiters ähneln der Sicherung Ihres Unternehmens: sicheres Booten, Virtualisierungssicherheit und Isolationskontrollen. Erneut ermöglichen Technologien wie Intel TXT und BootGuard das sichere Booten dieser Plattformen sowie von Rechenzentren und Cloud-Assets. In jüngster Zeit haben wir die Technologie Intel Select Solutions for Hardened Security eingeführt, die viele der Intel-Sicherheitstechnologien in einer einzigen Plattform integriert, die standardmäßig aktiviert ist.\n\nSie müssen in der Lage sein, Ihre Daten in großem Maßstab zu schützen, d.h. die Fähigkeit haben, alle Ihre Sicherheitstools nutzen zu können, ohne die Leistung negativ zu beeinflussen. Intels hardwarebeschleunigte Verschlüsselungstools (SHA, AES-2X, VPMADD52) machen dies möglich. Intels neue Anweisungen und QuickAssist-Technologien sind speziell auf die Verschlüsselungsanforderungen von Unternehmen und Cloud-Skalierung ausgerichtet.\n\nSchließlich, wie überwachen Sie bedrohungsbezogene Informationen und führen in großem Umfang Audits durch? Die Cyber Intelligence Platform Architecture von Intel (FPGA, DCPMM, Optane SSD) verwendet Hochleistungsberechnung, Speicher- und Speichertechnologien, um die Cyber-Intelligence-Plattform zu skalieren, selbst bei der zusätzlichen Belastung durch zusätzliche externe Sicherheiten für Telearbeiter.\n\n## Kurzfristige Lösungen\n\nMitarbeiterbildung ist die erste Maßnahme zur Eindämmung von Sicherheitsbedrohungen beim Telearbeiten. Anleitung zur Heimsicherheit und Sicherheitsschulungen oder die Verstärkung früherer Schulungen in Bereichen wie dem ordnungsgemäßen Datenzugriff sind von entscheidender Bedeutung. Seien Sie proaktiv bei Patches, indem Sie Patches bereitstellen und Benutzer dazu auffordern, ihre Geräte zu aktualisieren. Wenn Sie Zugangskontrolle für Unternehmen, ERM/DRM- und DLP-Lösungen haben, aktivieren Sie sie und skalieren Sie sie aus. Überprüfen Sie Ihre Richtlinien, um sicherzustellen, dass sie der neuen Realität von Telearbeitern entsprechen. Für Webverbindungen aktivieren Sie TLS und stellen Sie sicher, dass es durchgesetzt wird. Die zweifache Authentifizierung muss genutzt werden. Die meisten Unternehmen denken vielleicht, sie verfügen nicht über die Infrastruktur, um dies umzusetzen, aber es gibt verschiedene Anbieter, die in diesem Bereich helfen können, ohne dass eine große Menge neuer Infrastruktur bereitgestellt werden muss.\n\nViele Lösungen sind einfach standardmäßige Hygiene: Stellen Sie sicher, dass Ihre Endpunkt-Sicherheitsagenten aktiviert und auf dem aktuellen Stand sind. Verwalten und erzwingen Sie Sicherheitsrichtlinien für verschiedene Arten von Benutzergeräten. Aktivieren Sie die Vollverschlüsselung der Festplatte.\n\nOrganisationen müssen verstehen, dass Telearbeiter in einer Umgebung tätig sind, in der die Wahrscheinlichkeit hoch ist, dass ein Gerät von anderen in verschiedenen Situationen verwendet wird. Gute Sicherheitskontrollen, insbesondere Mitarbeiterbildung, können Probleme umgehen und es den Mitarbeitern ermöglichen, ohne negative Auswirkungen zu arbeiten.\n\n## Langfristige Lösungen\n\nEin langfristiger Sicherheitsplan für eine Arbeitsumgebung mit Telearbeitern ist nun erforderlich, sei es für eine dauerhafte Umstellung auf mehr remote Arbeitnehmer oder um mit einer anderen Pandemie oder ähnlichen Situation umzugehen. Es gibt mehrere Schritte, die Organisationen jetzt unternehmen sollten, um diese Realität in Zukunft zu unterstützen.\n\nEine bewährte Methode ist die Implementierung von \"Zero Trust\"-Richtlinien. Dadurch wird die Abhängigkeit von dem Vertrauen in jeden Aspekt von Benutzern und Clients verringert, die kommen. Zusammen damit sollte eine Zwei-Faktor-Authentifizierung bei Benutzern und Geräten zur Standardvorgehensweise in der Organisation werden. Für diejenigen, die noch keine ERM und Policy-basierte Datenzugriffskontrolle übernommen haben, ist es jetzt an der Zeit, dies zu tun, um die Daten sowohl außerhalb als auch innerhalb des Standortes zu schützen. Die Implementierung von umfassenden Sicherheitslösungen statt nur auf Anwendungsebene oder Netzwerkebene ist wichtig. Dies beinhaltet sicheres Booten mit Beglaubigung, Virtualisierung und Sicherheit von Containern sowie Firmware-Sicherheit und Überwachung. Es ist wichtig, Audit, Bedrohungsinformationen und Überwachung auch auf Teleheimarbeitsumgebungen auszuweiten, trotz Widerstand von Benutzern, die keine zusätzliche Überwachung auf ihren Systemen wünschen. Überlegen Sie auch, die Sicherheit in Teleheimarbeitsumgebungen, soweit möglich, über das Gerät hinaus auszudehnen, wie z.B. bei verwalteten Geräten und Netzwerken.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT8-de","image":"./episodes/edt-8/de/thumbnail.png","lang":"de","summary":"Mit vielen Mitarbeitern, die nun von zu Hause aus arbeiten, wie stellen Sie sicher, dass sie sicher arbeiten, aber ihnen dennoch die Flexibilität geben, die sie brauchen, um ihre Arbeit zu erledigen? In dieser Episode diskutieren Darren und der spezielle Gast Steve Orrin, CTO von Intel Federal, wie man die Intel-Technologie nutzen kann, um den Telearbeiter effektiv abzusichern."},{"id":154,"type":"Episode","title":"Meeting Intel's Cloud Solution Architects: Treffen mit den Cloud-Lösungsarchitekten von Intel","tags":["csa","cloudsolutionarchitect","solutionarchitect","people","compute","cloud"],"body":"\r\n\r\nIntel hat stark in die Einstellung einiger der besten CSAs der Branche investiert, mit etwa 80 Architekten weltweit und 22 in den Vereinigten Staaten. Dies sind neue Positionen mit einem großen Schwerpunkt.\n\nStephen Holt ist ein CSA für den Osten und verwaltet die anderen CSAs in diesem Bereich. Zuvor war er bei Intel tätig und spezialisierte sich auf Datenbanken, doch er hatte zuvor verschiedene Rollen in den Bereichen Geschäftsprozessanalyse und Verkauf, technische Analyse sowie technischer Verkauf bei IBM und einer Vielzahl von Start-ups inne. Er bringt all diese Erfahrungen mit zu Intel, um dem Team dabei zu helfen, zusammenzuarbeiten und Kundenwert zu generieren.\n\nKiran Agrahara ist ein East Coast CSA und berichtet an Stephen. Seine Erfahrung liegt im Bereich der Rechenzentruminfrastruktur mit Fokus auf Datenspeichervirtualisierung. Er hat in der Kabel- und Finanzbranche gearbeitet. In den letzten fünf oder sechs Jahren hat er mit Start-ups zusammengearbeitet, die sich auf Bereiche wie persistenter Speicher und softwaredefinierten Speicher konzentrieren. Wenn Kunden Kiran fragen, warum sie die Expertise von Intel nutzen sollten, antwortet er, dass Intel überall präsent ist und er diese Botschaft an die Endbenutzer verbreiten möchte.\n\nVor Sarah Musick kam als CSA zu Intel, war sie Partnerin, die Softwarearbeit im Bereich der Cloud-Migration und -Optimierung durchführte, die durch ihre Zeit in der Datenanalyse angestoßen wurde. Davor arbeitete sie für ein Unternehmen für tiefe Lerntextanalyse und stellte fest, dass die Arbeit, die sie in den Daten leistete, viele Aspekte ihrer früheren technischen Vertriebstätigkeit wieder aufgriff. Sie nutzt die Analysemethoden für die Cloud-Migration. Sie kam zu Intel aufgrund ihrer Integrität und ihrer Rolle im neuen Modell der Zusammenarbeit für alle, einschließlich Cloud-Anbieter.\n\nTodd Christ ist seit 26 Jahren bei Intel und verfügt über 30 Jahre Erfahrung im Bereich IT und Produktentwicklung. Zuletzt war er Teil der Cloud Enterprise Solutions Group von Intel, die zur Data Center Platforms Group gehört. Intel möchte Kunden dort abholen, wo sich ihre Daten befinden, sei es vor Ort oder in der Cloud. Todd war maßgeblich an der Entwicklung von Anthos beteiligt, daher sind hybride oder Multi-Cloud-Modelle für ihn von großer Bedeutung. Er hat eng mit Microsoft und Google zusammengearbeitet.\n\nEine wichtige Veränderung beim Umzug der Kunden in die Cloud ist die Einfachheit. Kunden müssen sich jetzt nicht mehr viel Gedanken über Hardware machen. Sie können schneller und mit weniger Aufwand sinnvolle Arbeit leisten. Obwohl die Hardware natürlich immer noch wichtig ist, wird sie abstrahiert. Es ist jedoch wichtig, die Grundlagen nicht aus den Augen zu verlieren. Es ist wie ein HVAC-System, an das man nie denkt, bis es nicht richtig funktioniert und dann ein Problem darstellt. Zum Beispiel gibt es Situationen, in denen ein Kunde feststellt, dass eine bestimmte Arbeitslast, die er in die Cloud verschoben hat, nicht gut funktioniert, sodass er die Hardware überprüfen muss.\n\nIntel ist seit ihrer Gründung bei den Cloud-Anbietern präsent und konzentriert sich auf diese Arbeitslasten. Intel investiert einen enormen Aufwand in das Cloud-Ökosystem, um sicherzustellen, dass diese Arbeitslasten optimal auf Intel-Technologien laufen.\n\nIntel hat 2 Milliarden Kerne an Cloud-Dienstleister (CSPs) geliefert und mehr als 90% aller Berechnungen in der Cloud werden auf Intel ausgeführt.\n\nKunden wünschen sich eine schnelle Skalierbarkeit und sie möchten die Rechenressourcen so schnell wie möglich erhalten; es ist ihnen egal, um welche Hardware es sich handelt. Die Wahrheit ist jedoch, dass latenzarme Arbeitslasten auf Intel-Hardware viel besser funktionieren als auf jedem anderen Konkurrenten. Der neueste skalierbare Xeon-Prozessor der dritten Generation, Icelake, ist unglaublich schnell. Sobald Kunden realisieren, dass sie erheblich sparen können, verkauft er sich von selbst. Ein Teil der Aufgabe eines CSA besteht daher in der Aufklärung.\n\nObwohl es so aussieht, als ob CSPs Dienstleistungen wie ein sofort einsatzbereites Versorgungsunternehmen mit 100% Zuverlässigkeit verkaufen, kann man kritische Arbeitslasten nicht einfach in die Cloud verschieben oder übertragen.\n\nWenn Ihre Anwendungen im cloudnativen Format entwickelt sind, müssen Sie nicht viel darüber nachdenken, sie in der Cloud bereitzustellen. Wenn Sie jedoch eine monolithische Anwendung haben, die für den Betrieb in einem Rechenzentrum entwickelt wurde, können Sie sie nicht einfach in die Cloud verschieben, da sie nicht für spezifische CPUs optimiert ist. Durch die Verwendung von Intel Optimierungs- oder Migrationswerkzeugen können Kunden vor der Migration fundierte Entscheidungen treffen.\n\nEinige Arbeitslasten sind möglicherweise nicht für die Cloud geeignet. Deshalb gab es insbesondere im letzten Teil des Jahres 2021 mehr Aufregung über Rückverlagerungen. Das Pendel schwingt ein wenig zurück, während Unternehmen insbesondere das richtige Gleichgewicht lernen. Hier kommen die Cloud Solution Architects (CSAs) ins Spiel. Nicht alles sollte in die Cloud gehen, und Intel kann dabei helfen, die Optimierung zu bestimmen. Einige Kunden stellen fest, dass sie nach dem Umzug von Arbeitslasten in die Cloud aufgrund von Vorschriften kein Geld sparen und sogar erheblich mehr ausgeben als bei der Aufbewahrung in ihrem eigenen Rechenzentrum. Oder es gibt möglicherweise Sicherheitsprobleme, weil Daten in bestimmten Ländern residieren und die Cloud nicht im richtigen Land ist.\n\nIntel CSAs sind unabhängig, daher interessieren sie sich nur für das Beste für die spezifischen Bedürfnisse der Kunden. Sie helfen dabei, Workloads mobiler zu machen, sodass IT-Abteilungen mit zunehmender Reife die Workloads entweder zurück in ihre eigenen Rechenzentren bringen oder in Zukunft zu einem anderen Cloud-Dienstleister verschieben können - je nachdem, was kosteneffektiv ist. Intel CSAs können Kunden auf eine Art und Weise unterstützen, die von gegenwärtigen Cloud-Dienstleistern vernachlässigt wird.\n\nViele Kunden sind besorgt wegen der Daten, da es teuer ist, sie von einem Cloud-Dienstanbieter abzurufen. Intel hat eine große Anzahl an Mitarbeitern, die bei diesem Problem helfen können. Sie sind gut informiert über nicht nur die Strukturen, um hybride Modelle einzurichten, sondern auch über die Sicherheit, die Firewalls und alle Zugriffspunkte. Sobald sich Ihre Daten hinter einer Firewall befinden, gibt es viele Sicherheitsebenen, die Sie überwinden müssen, um auf diese Dienste zugreifen zu können. Die erste Sache ist also, dass Sie Ihre Daten sicher hin und her senden können müssen.\n\nAuch in einem Multi-Cloud-Szenario ist das Herausziehen der Daten kostspielig. Wenn man zum Beispiel nur zwischen Azure und AWS wechselt, werden die Daten dennoch migriert, was ein langsamer Prozess ist. Wenn Kunden Terabyte oder sogar Petabyte an Daten lokal haben, die sie in die Cloud verschieben möchten, könnte es einen cloudähnlichen Service geben, der vor Ort läuft und ihnen die Benutzerfreundlichkeit und Funktionalität einer Cloud bietet. Wenn man die Cloud eher als eine Funktion als einen Standort betrachtet, gibt es mehr Möglichkeiten.\n\nIntel ist hier, um den Menschen zu ermöglichen, die Arbeit zu tun, die für ihre Organisation am bedeutsamsten ist, und Analysen werden dabei eine große Rolle spielen. Im Jahr 2022 besteht immer noch eine enorme Diskrepanz zwischen der Menge an Daten, die die Menschen haben, und der Menge an Erkenntnissen, die sie daraus gewinnen. Nur etwa 3% der Daten werden tatsächlich genutzt, um Erkenntnisse zu gewinnen. Es gibt also einen riesigen Schatz, und Intel-Chips sind besonders gut geeignet für Situationen, in denen sehr anspruchsvolle Verarbeitungsarbeiten durchgeführt werden müssen. Die Verarbeitung von Daten steht als Nächstes an.\n\nWeise Architektur ist Teil der Zukunft, weil man nicht das Rad neu erfinden muss. Andererseits gibt es möglicherweise neue Lösungen, die gut zu einer Organisation passen könnten. Dabei ist auch entscheidend, wo sich eine Organisation in ihrem Entwicklungsprozess befindet. Ältere etablierte Unternehmen, die schon lange gute Arbeit leisten, könnten zum Beispiel viele technische Schulden haben, die möglicherweise bewältigt werden könnten. Sie müssen sich die zugrunde liegende Technologie anschauen und sie letztendlich an einen Ort bringen, an dem sie agil sind.\n\nDie CSA-Dienste bei Intel werden kostenlos angeboten, da Intel Kunden dabei helfen möchte, ihre Arbeitslasten effektiv auszuführen und von der allgegenwärtigen Intel-Technologie in der Cloud zu profitieren. Die Expertise und Erfahrung der CSAs bei Intel ist umfassend und sie arbeiten als Team, um Kunden bei jedem gewünschten Wissensaspekt zu unterstützen. Kunden sollten ihren Intel Account Executive oder den Inside Sales um Hilfe bei der Cloud-Migration und -Optimierung durch einen CSA bitten.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Stephen Holt","Kiran Agrahara","Sarah Musick","Todd Christ"],"link":"/episode-EDT80-de","image":"./episodes/edt-80/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chef-Lösungsarchitekt bei Intel, spricht mit den führenden Cloud-Lösungsarchitekten von Intel, Stephen Holt, Kiran Agrahara, Sarah Musick und Todd Christ, darüber, wie sie Organisationen kostenlos bei der Migration zur Cloud und der Optimierung ihrer Arbeitslasten unterstützen können."},{"id":155,"type":"Episode","title":"Heterogene Programmierung mit OneAPI","tags":["cpu","fpga","gpu","heterogeneousprogramming","npu","oneapi","compute","technology","process"],"body":"\r\n\r\nDas Ziel von oneAPI besteht darin, die enorme Vielfalt an Hardware abstrakt zu erfassen, damit Softwareingenieure sie weiter oben im System nutzen und das Beste aus der Hardware herausholen können. James, ein Softwareingenieur, der auch eng mit der Hardware zusammenarbeitet, schätzt, was oneAPI im Kontext des aktuellen Trends zu heterogener Rechenleistung beitragen kann.\n\nDas Wort \"heterogen\" bedeutet hier im Wesentlichen, dass es verschiedene Geräte in einem System gibt, die Berechnungen durchführen können, jedoch nicht denselben Befehlssatz ausführen. Zum Beispiel hat die CPU eine bestimmte Art, Anweisungen auszuführen, während die GPU einen anderen Ansatz verfolgt. FPGA, andere ASICs und Spezialgeräte haben jeweils unterschiedliche Programmierweisen. Das Ziel besteht darin, all diese Vorteile zu nutzen.\n\nJohn Hennessy und David Patterson, Führungspersönlichkeiten im Bereich der Computerarchitektur, bezeichneten dies als ein neues goldenes Zeitalter der Computerarchitektur. Für Computerarchitekten ist es eine faszinierende Zeit, Spezialgeräte für verschiedene spezifische Bereiche zu entwerfen, aber für Programmierer kann es aufgrund der Komplexität schwierig und sogar beängstigend sein. Hier kommen die Initiativen oneAPI und oneAPI-Produkte ins Spiel.\n\nDer Name, oneAPI, ist sowohl eine Initiative mit einer Spezifikation als auch eine Implementierung. Die Initiative ist ein einfaches Konzept: Softwareentwickler sollten die Freiheit haben, jedes gewünschte Gerät mit voller Leistung zu verwenden, und ihr Code sollte seinen Wert bewahren; sie sollten ihn nicht für jedes neue Gerät neu schreiben müssen. Diese Eigenschaften können auf Compiler, Bibliotheken, Debugger und jede Art von Tool angewendet werden.\n\nEine große Menge an Software zur Implementierung bestimmter Spezifikationen wurde quelloffen gemacht. Ein Großteil davon stammt von Intel, aber Intel stellt sie auch als herunterladbare Toolkits zur Verfügung, die bereits vorgefertigt und einsatzbereit sind, um Intel-Hardware zu unterstützen. Andere Anbieter, die Teile von oneAPI erstellen, haben ihre eigenen Implementierungen, sodass alle zusammenkommen, um die Spezifikationen auszutauschen, aber jeder kann Unterstützung für ihre eigene Hardware in einer offenen, branchenübergreifenden Multi-Vendor-Weise erstellen.\n\nDas übergeordnete Ziel besteht darin, Code einmal zu schreiben und ihn überall optimal auszuführen. Es handelt sich jedoch um ein komplexes Problem, das einige Leistungsoptimierungen erfordern wird. Wenn zum Beispiel ein Algorithmus auf einer GPU gut funktioniert, wird er auch auf nahezu jeder anderen GPU ähnlich gut laufen. Wenn Sie den Algorithmus jedoch auf einem FPGA ausführen lassen, können Sie die Wiederverwendung Ihres Programms maximieren, haben jedoch die Möglichkeit, Teile des Algorithmus umzucodieren, damit er ebenfalls gut läuft. Es handelt sich um einen komplexen Ansatz, der es Ihnen ermöglicht, von jeder Hardware in hoffentlich sehr isolierten Neuschreibungen in unterschiedlichem Maße zu profitieren.\n\nSie können einen Code für verschiedene Geräte erstellen. Es gibt sogar noch ausgefeiltere Laufzeitansätze, die es Ihnen ermöglichen, zu erkennen, was vorhanden ist, und verschiedene Codeabschnitte auszuführen, sodass Sie tatsächlich einen allgemeinen Quellcode haben können, der dynamisch entscheidet. Sie können ein Bereitstellungsprogramm anwenden und es kann herausfinden, was auf Ihrer Maschine vorhanden ist und es dynamisch nutzen, anstatt den Benutzer dazu aufzufordern, abhängig von der verwendeten Maschine eine andere Binärdatei auszuführen.\n\nDies ist spannend, denn lange Zeit haben Ingenieure einen Software-Stack verwendet, der am besten für ihre Maschine war. Heutzutage möchten sie jedoch ein Programm kompilieren, das Geräte von verschiedenen Herstellern nutzen kann. Das Programm sollte entsprechend darauf reagieren können. Damit das funktioniert, muss der Compiler den Code für diese verschiedenen Geräte zuverlässig erstellen können. Hier kommt die Offenheit ins Spiel.\n\nEinige werden argumentieren, dass ein geschlossenes System eine bessere Leistung erbringt, da die Bibliotheken und die Sprache speziell auf die Hardware-Fähigkeiten abgestimmt sind. Aber die Frage ist, wie man das Beste aus beiden Welten bekommen kann? Wenn es beispielsweise eine anbieterspezifische Implementierung für eine mathematische Bibliothek gibt, kann es ein gemeinsames Programm geben. Ein großer Teil von oneAPI besteht nicht darin, die gesamte Welt neu zu erfinden, sondern sie so zu organisieren, dass das Beste auf jeder möglichen Plattform genutzt werden kann.\n\nOneAPI hat die Fähigkeit, Speicher und Daten zu verschieben. Verschiedene Programmiermodelle sitzen auf OneAPI auf, und es liegt an Ihnen, wie stark Sie sich in die Verwaltung des Speichers einbringen möchten. Das Herumverschieben von Daten ist teuer und verbraucht Energie, daher können Sie dem nicht entkommen. Allerdings bietet Ihnen OneAPI die Werkzeuge, um das durch Abfragen des Systems zu steuern und Ihrem Programm zur Laufzeit die richtigen, dynamischen Entscheidungen zu ermöglichen, um die beste Leistung zu erzielen.\n\nIntel hat jahrzehntelange Erfahrung im Bau von Instrumenten zur Optimierung und Migration und verfügt über hochoptimierende Compiler. V2 hat dazu beigetragen, die Branche um Hardwarezähler auf Prozessoren herum weiterzuentwickeln, was zur Norm geworden ist. Es gibt eine Vielzahl von anderen Analysetools, die Rückmeldungen zur Struktur Ihres Programms geben, von der Suche nach Deadlocks und parallelen Anwendungen bis hin zur Identifizierung von Stellen, an denen Sie einige Sicherungen hinzufügen müssen. Intel stellt all diese Tools in einem oneAPI-Stil zur Verfügung, um vielseitiger zu sein als nur auf eine CPU bezogen.\n\nDie ersten Gruppen, die oneAPI übernehmen, umfassen leistungsstarke Rechenzentren (HPC). Mit der Explosion neuer Computerarchitekturideen wird es eine noch größere Vielfalt und Innovation in diesem Bereich geben. Die großen Codes, die bei der Lösung der größten technischen Probleme der Welt helfen können, oder zum Beispiel pharmazeutische Probleme lösen und neue Medikamente bewerten können, erfordern die neueste und beste Hardware. Daher gewinnt das Konzept der Leistungstragfähigkeit in nationalen Laboren, Universitäten und Forschungszentren an Bedeutung.\n\nDas heutige HPC sind die Server von morgen, daher erfordert die Fähigkeit, unterschiedliche Hardware zu nutzen, dass Softwareingenieure planen und darauf achten, wie portabel der Code ist, da Code nicht schnell stirbt; er hält Jahrzehnte lang. Innerhalb weniger Jahre werden heterogene Systeme jeden berühren, und jetzt ist die Zeit, sich darüber zu informieren.\n\nSoftware-Ingenieure müssen nicht parallel programmieren können, um von oneAPI zu profitieren. Es geht darum, Dinge zu nutzen, die offen und für mehrere Hersteller und Architekturen geeignet sind. Selbst Ingenieure, die oben in der Hierarchie stehen, sollten verstehen, was sich im Stapel befindet und was er in Bezug auf Portabilität und Leistungsfähigkeit leisten kann.\n\nDie IoT-Gemeinschaft ist eine weitere Gruppe, die in der Programmierung über diverse heterogene Rechengeräte hinweg weit voraus ist und unterschiedliche Methoden zur Verwaltung von ihnen verwendet. Ihre Rechenfähigkeiten steigen weiter an, während sich die Technologie weiterentwickelt. Daher greift hier oneAPI und kann dazu beitragen, Dinge zu formalisieren oder zu standardisieren, die zuerst in der eingebetteten Welt innoviert wurden.\n\nDie Idee einer einzigen API hat sich von einer verrückten Idee, über die nur wenige Leute sprachen, zu einer immer mehr anerkannten Idee entwickelt, die Sinn ergibt und Probleme in ihrer Organisation löst.\n\nDer einfachste Ort, um sich über die Initiative zu informieren, ist die Website oneapi.io. Um mehr über die Umsetzung zu erfahren, klicken Sie auf den Implementierungs-Tab, um einen Link zu finden, der zu den Intel-Implementierungen führt. Dort können Sie die verschiedenen Toolkits herunterladen. Suchen Sie nach der Intel DevCloud, um die Tools kostenlos in der Cloud auszuprobieren, auch auf unterschiedlicher Hardware. Die Website oneapi.io bietet auch eine Vielzahl von Tutorials und Ressourcen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","James Reinders"],"link":"/episode-EDT81-de","image":"./episodes/edt-81/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect bei Intel, diskutiert mit James Reinders, dem Chief Evangelist von Intel OneAPI, über die Fähigkeiten und Zukunft von OneAPI. Dabei handelt es sich um ein branchenübergreifendes, offenes, auf Standards basierendes vereinheitlichtes Programmiermodell, das eine gemeinsame Entwicklererfahrung für Beschleunigerarchitekturen bietet."},{"id":156,"type":"Episode","title":"Von Infrastruktur zu Workloads wechseln","tags":["cloudmigration","cloud","compute","process","infrastructure","migration","workload"],"body":"\r\n\r\nSarahs Gespräche mit Kunden haben sich von der Diskussion über Infrastruktur zu Gesprächen über Workloads verschoben. Ein offensichtlicher Grund dafür ist der Wechsel vom Rechenzentrum zur Kommodifizierung von Ressourcen, um alles als Dienstleistung zu konsumieren. Ein weiterer Grund ist der Einfluss von Anwendungsteams bei architektonischen Entscheidungen. Die Anwendungen sind auf eine Art und Weise die Lebensader von Organisationen geworden, die sie vor fünf Jahren noch nicht waren. Der letzte Grund ist finanzieller Natur: Budgets werden nun nach Arbeitslasten kategorisiert, im Gegensatz zu allgemeinen IT-Ausgaben.\n\nEine Herausforderung bei diesem Wechsel von Infrastruktur zu Arbeitslast besteht darin, ein Gleichgewicht zwischen zentralisierten und dezentralisierten Prozessen und Arbeitslasten zu finden. Einige Dinge wie Sicherheitsgouvernanz werden von einem zentralen Knotenpunkt gehandhabt, während andere in lokalen, kleineren Teams bearbeitet werden. Ein Großteil der Entscheidungsfindung wird in die Teams verlagert, die neue Anwendungen und Dienste innerhalb der IT sowie für Kunden erstellen.\n\nCOVID war ein Beschleuniger, um den Wandel voranzutreiben und Organisationen in die Cloud zu verlagern. Einige IT-Organisationen lassen ihre Anwendungen uneingeschränkt in die Cloud übergehen und bevorzugen es, auf der Rückseite aufräumen, anstatt die Innovation zu verlangsamen, die stattfindet. Dieses \"Black Swan\" Ereignis ist beispiellos und wir sehen immer noch die Auswirkungen des schnellen Paradigmenwechsels.\n\nDie Rolle des CIO ist zurück, aber nur wenn sie wieder an Informationen und Arbeitslasten denken anstatt ein Rechenzentrum zu betreiben. Natürlich hängt es von der Organisation ab. Cloud-native Organisationen oder solche, die cloud-native werden wollen, optimieren ihre Anwendungen während des Betriebs, um super agil zu sein. Je mehr sie das tun, desto mehr geht es um die Infrastruktur und den Service der Anwendung anstatt um die akzeptierten Grenzen, die vorher im Rechenzentrum existierten und innerhalb dieser Parameter gearbeitet wurde. Früher hat es in dieser Situation Innovationen von App-Teams ausgelöst, denn wenn man mit festgelegten Vorgaben arbeitet, ist manchmal Notwendigkeit die Mutter der Erfindung im Gegensatz zu den unendlichen Möglichkeiten in der Cloud.\n\nDas ist ein Ende des Spektrums. Auf der anderen Seite befinden sich amerikanische Kulturerbeunternehmen, die Institutionen. In der Regel verfügen sie noch über Informationen auf dem Mainframe. Es handelt sich um eine \"Wenn es nicht kaputt ist, repariere es nicht\"-Situation, insbesondere bei statischen Anwendungen. Die Cloud ermöglicht es CIOs, über die altmodische Art der Verwaltung dieser Anwendungen hinauszudenken. Wenn CIOs diese neuen Technologien annehmen können, können sie nun einen Weg nach vorne erkennen.\n\nDie Verarbeitung, die unter diskreten Anwendungen stattfindet, ist relevanter denn je. Intel spielt eine wichtige Rolle bei der Lösung von Bedenken hinsichtlich Leistung oder Kosten bei Cloud-Angeboten, zum Teil weil sie diese Technologien entwickelt haben und auch weil sie eines der größten Softwareunternehmen sind. Es gibt eine enorme Menge an internem Wissen. Mit anderen Worten, nicht alle Instanzen in der Cloud sind gleich erstellt, daher hat Intel Workloads intern optimiert, um das Beste aus den Cloud-Instanzen herauszuholen, die sie verwenden.\n\nZum Beispiel verlagern viele Organisationen ihre Angelegenheiten in Kubernetes-Cluster, und Intel optimiert eine Menge in diesem Bereich. Sie können über die Standard-Helm-Charts hinausgehen, indem sie Erweiterungen nutzen, die den Gesundheitszustand des darunterliegenden Knotens betrachten und nicht nur die reine Verfügbarkeit. Intel kann viele Dinge tun, um Kunden erhebliche Leistungs- und Kostenvorteile zu bieten, nicht nur 2 oder 3 Prozent, sondern 30 oder 40 Prozent?\n\nNicht jede Arbeitslast gehört in die Cloud. Die Organisationsstruktur hat einen Einfluss darauf, wo die Arbeitslast platziert werden sollte. Der Schlüssel ist, clever mit der Cloud umzugehen.\n\nEine erfolgreiche Multi-Cloud-Strategie besteht darin, eine primäre und eine sekundäre Cloud zu haben. Wenn viele über Multi-Cloud sprechen, geschieht dies aus Angst vor Anbieterbindung. Der Ort, an dem sich die meisten Ihrer Daten befinden, hat einen Einfluss auf die Multi-Cloud-Strategie, ebenso wie die am besten geeigneten Arbeitslasten.\n\nUm eine Strategie zu entwickeln, werden Intels Cloud-Lösungsarchitekten eine Erkundung darüber durchführen, was die Organisation tun möchte und wo die Probleme liegen. Intel kann oft viele der Probleme mit den Tools, die sie zur Hand haben, beheben. Die Cloud-Lösungsarchitekten werden auch Angebote kontextualisieren, um den Prozess schneller und effizienter zu gestalten. Ein Teil ihrer Aufgabe besteht darin, als Bildungsperson zu fungieren, damit jeder die Informationen hat, die er benötigt, um voranzukommen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Musick"],"link":"/episode-EDT82-de","image":"./episodes/edt-82/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solution Architect, Intel, setzt sein Gespräch mit Sarah Musick, Cloud Solution Architect, Intel, über den Wechsel von Infrastruktur zu Workloads fort. Bitte füge es der Playlist Embracing Digital Transformation hinzu."},{"id":157,"type":"Episode","title":"IDC Richtungen 2022","tags":["data","datacentric","people","process","idc2022"],"body":"\r\n\r\nDavid hat seinen Sitz in Maryland, in der Nähe des Regierungszentrums. Er verfügt über mehr als 25 Jahre Erfahrung sowohl in der technischen als auch in der geschäftlichen Entwicklung in kritischen Infrastrukturen wie Gesundheitswesen, Energie, Finanzen und Industrie. Ein Großteil seiner Spezialisierung liegt im Bereich der Grenzen: Angelegenheiten in der Hyperkante und eingebetteten Räumen, aber auch die schwierigen Probleme bei der Beschleunigung von Arbeitslasten und dem Bestreben, das Beste aus einer Architektur herauszuholen, die bis hin zum Unternehmen und zur Cloud übersetzt.\n\nDarren, David und einige ihrer Kollegen besuchten die IDC-Konferenz teilweise, um zu sehen, ob ihre Vorschläge und Vorhersagen im Einklang mit den Trends standen, die IDC beobachtet hat, und sie fühlten sich bestätigt. Sie gingen auch, um herauszufinden, ob es Lücken gibt, die sie verstehen müssen, und wie sie die Ideen und Dienstleistungen von IDC nutzen können.\n\nDer größte langfristige Trend, auf den die gesamte Konferenz ausgerichtet war, ist die digitale Erstbevorzugung und wie sie praktisch jeden Markt beeinflusst. Meredith Whalen, Chief Research Officer bei IDC, erläuterte, was digitale Erstbevorzugung wirklich bedeutet. Grundsätzlich muss jede Organisation, sei es ein Unternehmen oder eine Regierung, sich fragen, wie sie die Digitalisierung zu einem tatsächlichen Produkt machen kann. Bis 2024 wird es allein in digitalen Produkten etwa 10 Billionen Dollar an Ausgaben über alle Märkte hinweg geben. Diese Veränderung im Konsumverhalten zwingt die Regierungen, sich ebenfalls mit der Digitalisierung auseinanderzusetzen, nicht nur in Bezug auf ihre Dienstleistungen, sondern auch in Bezug darauf, wie sie sie im Bereich des Welthandels, der Wirtschaft und der Währungen angehen.\n\nDies ist eine besondere Herausforderung für Regierungen auf allen Ebenen, da sie mit Computernetzwerken auf vielen verschiedenen Systemen zu kämpfen haben und in vielen Fällen isoliert sind. Effektive Brücken, die sie miteinander verbinden, existieren noch nicht. Regierungssysteme müssen untersuchen, wie sie Prozesse vereinfachen und vereinheitlichen können.\n\nDer nächste große Trend ist ein Wechsel hin zur Cloud Computing. Jeder Cloud-Service-Anbieter (CSP) verfügt über eine eigene Infrastruktur, und die Infrastrukturaufbauten sind nicht zwangsläufig austauschbar, daher sind Brücken erforderlich. Aktuell sind CSPs nicht austauschbar. Es ist zwar wahr, dass sie möchten, dass Kunden ihre Infrastruktur nutzen, aber da Softwareentwicklungsumgebungen und Softwareanwendungen über ihre Marktplätze aufgebaut werden, ist es schwierig, diese zu übertragen. CEOs und CIOs fragen sich nicht nur, ob sie die gekauften Software-Lizenzen und Zugänge nutzen, sondern ob der Einsatz das gewünschte Ergebnis erzielt hat.\n\nDas aktuelle CSP-Modell neigt dazu, ineffizient im Hinblick auf Geschäftsergebnisse zu sein. Kunden benötigen Software-Ökosysteme, die zusammenarbeiten, um diese Ergebnisse zu liefern. Sie suchen nach mehreren Ökosystemen, die nahtlos über mehrere Clouds und hybride Clouds hinweg funktionieren. Daher wird die Tendenz sein, dass CSPs austauschbar werden und Brückenbauer hier wichtig sein werden. Das ist der Wendepunkt.\n\nDie heutigen Daten sind austauschbar. Du kannst sie natürlich bewegen, jedoch mit entsprechenden Kosten, aber du kannst sie kopieren, verändern, usw. Der Trend geht jedoch hin zu nicht austauschbaren Daten. Wenn diese Daten oder digitalen Vermögenswerte ein Eigentum haben, hat das immense Auswirkungen auf die Zukunft und wie Daten in Bereichen wie Sicherheit, Vertrauen, Geschäftsmodelle und Ökosysteme gehandhabt werden, die sich darum drehen. Es wird Datenentitäten geben, deren Gültigkeit des Eigentums und ihrer Herkunft du akzeptieren und bestätigen musst, sowie alle Richtlinien, die damit verbunden sind.\n\nIn dieser Entwicklung wird es Vor- und Nachteile geben. Einerseits haben Verbraucher mehr Kontrolle über ihre Privatsphäre, wenn sie Rechte an ihren Daten haben. Das Gleiche gilt für Unternehmen und Organisationen. Andererseits müssen sich die Ökosysteme von Software und Datenwissenschaftlern, die die Daten nutzen, mit nicht austauschbaren Datenobjekten und -vermögen auf eine viel präzisere und strukturiertere Weise auseinandersetzen. Letztendlich wird jeder damit umgehen müssen. Regierungen stehen nicht allein vor diesem Problem, aber es wird zunehmend ein wichtiger Teil dessen sein, wie sie Handel betreiben und mischen, nicht nur bei physischen Gütern, sondern auch bei digitalen Gütern.\n\nWie nah ist die digitale Priorität in der Realität? Ein Beispiel dafür sind digitale Token als echte Währung, z.B. in der Gaming-Community. Digitale Token sind Teil dieser Welt in Bezug darauf, wie Apps und Ökosysteme nicht austauschbare Daten verwenden. Ein weiteres Beispiel ist im Gesundheitswesen. Heute geht man zum Arzt oder konsultiert ihn virtuell und er kann sehen, was vor sich geht, während man Symptome beschreibt. Das ändert sich alles durch Echtzeit-Wearables, die Glukosespiegel, Herzfrequenz, Gewichtsveränderungen usw. überwachen können. Diese Daten gehören dem Individuum, daher müssen sie sicher und authentifiziert sein, aber sie können sie auch für Dienstleistungen nutzen, die nicht mit Symptomen, sondern maßgeschneiderten Algorithmen und Dienstleistungen darüber zu tun haben, was tatsächlich in ihrem Körper passiert, um die besten Diagnosen zu erhalten.\n\nZu berücksichtigende Datenpunkte: Meredith Whalen argumentierte, dass im Jahr 2021 die Ausgaben für Dienstleistungen erstmals die Ausgaben für regulierte Softwarelizenzen im IT-Bereich überstiegen haben. Somit ist derzeit der Dienstleistungssektor dominant; der Trend geht in Richtung eines ergebnisorientierten Modells. Im Jahr 2023 erwartet IDC, dass die Ausgaben für digitale Lösungen in Unternehmen generell höher sein werden als die Ausgaben für nicht-digitale Lösungen. Jede Branche wird sich jedoch unterscheiden, aber auf makroökonomischer Ebene stellt 2023 den Wendepunkt dar. Im Jahr 2024 folgt dann der Arbeitsmarkt. IDC sagt voraus, dass die meisten Unternehmen mehr in technisches Fachpersonal investieren werden im Vergleich zu nicht-digitalen Berufen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","David Richard"],"link":"/episode-EDT83-de","image":"./episodes/edt-83/de/thumbnail.png","lang":"de","summary":"In dieser Folge reflektieren Darren, Chief Solution Architect, öffentlicher Sektor bei Intel, und David Richard, Lead Solution Architect beim Verteidigungsministerium, Intel, die Trends und Ideen, die sie aus der IDC Directions-Konferenz 2022 gewonnen haben."},{"id":158,"type":"Episode","title":"Sicherung deiner Burg mit Null-Vertrauen","tags":["cybersecurity","zerotrust"],"body":"\r\n\r\nDas alte Sicherheitsmodell könnte mit einer Burg verglichen werden, mit Wachen entlang der Straße und einem Wassergraben, der die Burg umgibt. Alle Kronjuwelen, in diesem Fall Daten, würden zentral in der Burg gesammelt und verwaltet. Das Aufkommen des Zero-Trust hat einen neuen Rahmen geschaffen.\n\nDie größte Bedrohung für die Daten ist der Endbenutzer. Daher ist die Nummer eins Strategie ein Rahmenwerk, das an den äußeren Rändern mit Säulen der Exzellenz und inneren Schutzmaßnahmen beginnt. Diese aktualisierte Denkweise ermöglicht es Organisationen, ihre Mission und Geschäftspartner auf eine echte Art und Weise in das Gespräch einzubinden.\n\nDie alte Denkweise war eher von einem harten Schalenansatz geprägt, mit Schutzmaßnahmen und Kontrollen an verwundbaren Stellen. Ein wichtiger Bestandteil der Zero-Trust-Architektur ist der risikobasierte Ansatz, der dynamischer ist und auf zwei Dingen basiert: was in der Vergangenheit funktioniert hat und was nicht funktioniert hat. Wenn die Angreifer das letzte Mal durch die linke Tür gekommen sind, werden Sie natürlich dort Ihre Verteidigung stärken, aber Sie werden auch aus diesem Angriff lernen und basierend auf neuem Wissen darüber andere Stellen absichern. Ein risikobasierter Ansatz löst nicht nur das Problem des letzten Angriffs, sondern denkt voraus und wendet die richtigen Kontrollen für aktuelle und zukünftige Bedrohungen im gesamten Unternehmen an.\n\nEin Teil des risikobasierten Ansatzes besteht darin, das Ökosystem zu verstehen. Kunden, Partner und Benutzer sind alle Teil der Sicherheitsberechnung. Der alte Ansatz mit einer harten Schale funktioniert nicht mehr. Genau wie eine Burg Personen und Vorräte hinein- und hinausgehen lässt und die Reichtümer an verschiedenen Orten im Königreich liegen können, geht der Ansatz des Zero Trust einen Schritt weiter und berücksichtigt alle Zugänge und Abgänge für Daten oder Personen, die darauf zugreifen.\n\nTraditionell konnte jemand mit einer einzigen Anmeldung Zugang zur Firmenburg erhalten. Es gibt mehrere Zero-Trust-Prinzipien, aber die zwei Grundlagen sind eine standardmäßige Ablehnung und kontinuierliche Überwachung und Berechtigung. Vertrauen wird nicht automatisch erworben, noch ist es permanent. Zum Beispiel, wenn ein Gast die Burg betritt, wird er an der Rezeption überprüft und dann nach dem Grund seines Besuchs gefragt. Möglicherweise erhält er Zugang, um eine Person in einem Raum für eine bestimmte Zeit zu besuchen und wird dabei begleitet. Auch wird überwacht, was er während seines Besuchs mitbringt und wieder mitnimmt.\n\nZero Trust gilt für den Zugriff an allen Standorten: Rechenzentren, Clouds, Edge-Geräte, Geschäftsumgebungen usw. Es ist datenzentriert und zugriffsorientiert und ist mit einem risikobasierten Ansatz kombiniert. Es muss eine umfassendere Strategie geben. Der resultierende Zero-Trust-Ansatz wirft nicht einfach das weg, was zuvor gut funktioniert hat, sondern kombiniert bewährte Prozesse, Prinzipien und Technologien und fügt ein zeitliches Element hinzu.\n\nDieses neue Element ist nicht so schwierig, wie es oft dargestellt wird, aber es ist ein Prozess- und Kulturproblem, das knifflig sein kann.\n\nViele Entwickler haben die Befürchtung, dass eine Architektur des Null-Vertrauens sie verlangsamen wird. Doch Sicherheitsexperten und Entwickler müssen zusammenarbeiten, um diese Wahrnehmung zu überwinden. Ein reales Beispiel dafür ist Log4j. Vor sechs Monaten konnten Entwickler es problemlos herunterladen, aber mittlerweile hat sich die Bedrohungslage geändert. Ohne einen risikobasierten Ansatz könnte ein Entwickler Log4j herunterladen, bis jemand vom Sicherheitsteam eingreift und es deaktiviert. Mit einem risikobasierten Ansatz sowie Zugriffsbeschränkungen wäre Log4j nicht verfügbar und es würde eine Alternative angeboten werden. Ein weiteres Beispiel wäre, wenn Log4j bereits in ein Produkt integriert ist – hier könnte eine dynamische Vertrauensbewertung zusätzliche Kontrollen hinzufügen, anstatt es komplett zu blockieren. Es geht also um beide Seiten der Gleichung.\n\nDiese Partnerschaft ähnelt dem Cross-Training und dem Informationsaustausch, der in den Entwicklungsprozess zur Sicherheit integriert wird. Während ein Produkt gebaut und getestet wird, überwacht und bewertet die Sicherheit auch in Echtzeit das Risiko sowohl für die beteiligten Unternehmen als auch für die Schwachstellen des Produkts. Die Integration eines risikobasierten Ansatzes in den Prozess nutzt Intelligenz, die viele unserer wahrgenommenen Schwierigkeiten im Kern trifft.\n\nWas ist der erste Schritt für CISOs, CIOs oder CTOs, um eine \"Zero-Trust\"-Initiative zu starten? Cameron schlägt vor, das \"Geek-Sprech\" aufzugeben und in einfacher englischer Sprache zu kommunizieren. Es kann herausfordernd sein, die Initiative voranzutreiben, da Führungskräfte normalerweise mit einem bestimmten Ergebnis oder Ziel arbeiten. Bei Zero-Trust gibt es kein definiertes Ziel, auf das hingearbeitet wird, außer einer sichereren Umgebung für Benutzer zu schaffen. Natürlich gibt es KPIs und andere Maßnahmen, um eine erhöhte Sicherheit zu zeigen, aber es ist eine Reise, kein Ziel. Er betont auch kontinuierliche Finanzierung; das Cybersicherheitsbudget sollte nicht in das IT-Budget integriert werden. Es muss separat und klar abgegrenzt sein.\n\nDer beste Ort, um hochwertige Informationen mit praktischen Anweisungen zu finden, ist die NIST-Veröffentlichung SP 800-207. Sie legt auch die fünf Säulen des Vertrauens dar, die gute Ausgangspunkte sind.\n\nEin wesentlicher grundlegender Aspekt besteht darin, eine gute Bestandsaufnahme der zu schützenden Vermögenswerte wie Datenquellen, Datenbanken, Geschäftsprozesse und Transaktionsanwendungen zu haben. Grundsätzlich müssen Sie den Umfang Ihrer Burg definieren. Es ist wichtig, nicht nur darüber nachzudenken, was Sie besitzen, sondern auch darüber, worauf Sie sich verlassen, wie z.B. die SAAS-Umgebung, die Cloud-Infrastruktur und Tools von Drittanbietern.\n\nDas große Ganze besteht darin, seine Wertschöpfungskette zu kennen. Es geht nicht nur um das, was sich in deinem Schloss befindet, sondern auch darum, wie du Geld verdienst, wie dieses Geld verteilt wird, wen du bezahlst und deine Lieferanten. Jeder einzelne ist ein entscheidendes Element der Kette.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin","Cameron Chehreh"],"link":"/episode-EDT84-de","image":"./episodes/edt-84/de/thumbnail.png","lang":"de","summary":"In dieser Folge diskutiert Darren das Thema Zero Trust Security mit Steve Orrin, CTO Public Sector von Intel, und Cameron Chehreh, VP-GM Public Sector."},{"id":159,"type":"Episode","title":"Künstliche Intelligenz und Sicherheit","tags":["aiml","cybersecurity","devops","compute","technology","process"],"body":"\r\n\r\n## KI-Fehler\n\nKürzlich forderte Darren eine Klasse von Gymnasial- und Hochschulstudenten, die sich mit KI beschäftigen, auf, KI-Fehlfunktionen zu finden. Sie fanden Beispiele wie Microsoft Tay, ein Chatbot, der aus Twitter zwanglose Gespräche gelernt hat und innerhalb von 24 Stunden rassistische und frauenfeindliche Kommentare aufgrund von Manipulation aus Twitter-Feeds abgab. Ein weiteres Beispiel war ein Ballverfolgungssystem des Inverness Football Club, das dazu gedacht war, einen Fußball zu verfolgen, aber stattdessen den kahlen Kopf eines Schiedsrichters verfolgte. Ernsthafter war ein Todesfall, als ein autonomes Fahrzeug von Uber einen Fußgänger außerhalb eines Zebrastreifens nicht erkannte und nicht anhielt.\n\n## Bereitstellung von Lösungen\n\nAll diese KI-Fehlschläge hatten mit Daten zu tun. Bei der Bereitstellung von KI-Lösungen müssen Sie sich kritische Fragen stellen: Wo mache ich meine Schlussfolgerungen? Ist der Endpunkt sicher? Wenn Sie all Ihre Schlussfolgerungen am Endpunkt, beispielsweise bei der Objekterkennung mit einer Kamera, treffen, müssen Sie sicherstellen, dass dies sicher ist; niemand sollte in der Lage sein, die Daten, die Kamera oder das Modell zu manipulieren.\n\nEine weitere Frage ist, was wird bereitgestellt werden? Werde ich ein neuronales Netzwerk oder einen Algorithmus zum Randbereich deployen oder werde ich nur Daten vom Randbereich zurück in ein Rechenzentrum streamen, um dort die Schlussfolgerung zu erstellen? Darüber hinaus muss man sich fragen, wie oft die Modelle oder Algorithmen aktualisiert werden, wenn Künstliche Intelligenz operationalisiert wird.\n\n## AI-Pipeline\n\nDie Pipeline für die Entwicklung, das Training, das Testen, die Bereitstellung und die Inferenz von KI benötigt drei Dinge: die Anwendung, das Modell und die Daten. Diese drei Dinge müssen gemeinsam durch die Pipeline migrieren und gleichzeitig geschützt werden. Sie müssen sicherstellen, dass die Produktionsdaten selbst während der Produktion nicht manipuliert werden.\n\n## KI-Sicherheitsbedrohungen\n\nDie Bedrohungen für künstliche Intelligenz sind signifikant, sei es Spionage, Sabotage oder Betrug, da die Angriffsfläche groß ist.\n\n## Angriffe\n\nZuerst kommen die Modelle. Ein Modell kann manipuliert werden, zum Beispiel indem jemand schwarze und weiße Aufkleber auf Stoppschilder klebt, sodass sie nicht als solche erkannt werden, oder indem jemand die Mustererkennung stört, um Angriffe unbemerkt zu lassen. Modelle müssen nicht nur während des Trainings, sondern auch während des Testens, der Bereitstellung und der Inferenz geschützt werden. Verschlüsselung, Zugriffskontrolle sowie Modell- und Versionskontrolle sind kritisch, genauso wie bei der Entwicklung einer Anwendung.\n\nQuelltrainingsdaten und Produktionsdaten müssen ebenfalls vor Manipulation geschützt werden.\n\n## Arten von Angriffen\n\nEin Papier des Belfer Center stellt die Bedrohungen auf einer Achse des Formats und einer Achse der Sichtbarkeit dar. Die Format-Achse reicht von physisch bis digital. Die Sichtbarkeitsachse reicht von wahrnehmbar bis unbemerkt.\n\n## Physische Angriffe\n\nKörperliche Angriffe können physische Gegenstände verändern, wie beispielsweise den Aufkleber auf dem Stoppschild. Diese Angriffe waren bereits frühzeitig bei autonomen Fahrzeugen und Gesichtserkennungssystemen erkennbar. Für diese Angriffe benötigen wir eine bessere Schulung der KI-Algorithmen unter Verwendung von verstärktem Lernen und Techniken des Lernens anhand negativer Fälle.\n\n## Digitale Angriffe\n\nDigitale Angriffe sind schwerer zu erkennen, da sie nicht sichtbar sind. Ein Angriff könnte weißes Rauschen in den Datenstrom eingefügt werden, um den Algorithmus zu stören. Diese Angriffe sind schwer zu bekämpfen, es sei denn, die Inferenz wird am Rand durchgeführt oder die Mustererkennung wird eingesetzt. Aus diesem Grund ist es unerlässlich zu wissen, von wo die Quelldaten stammen, sowohl bei der Quellenschulung, dem Testen als auch den Produktionsdaten.\n\n## Identifizieren Sie die Datenquellen.\n\nDatenquellen müssen aus öffentlichen Datenquellen verifiziert und getestet werden. Open-Source-Daten sind nicht gut geschützt. Betrachten Sie die Erzeugung eigener Datenquellen, um mehr Kontrolle zu haben. Wenn Sie eine gemeinsam genutzte Datenquelle verwenden, nutzen Sie ein Versionskontrollsystem wie GitHub oder GitLab, um die Konsistenz zu überprüfen. Testdaten benötigen ebenfalls Versionskontrolle, Zugangskontrolle und andere Sicherheitsmaßnahmen, wie Sie es typischerweise in einer DevOps-Pipeline tun.\n\nDer letzte und anspruchsvollste Teil besteht darin, die Produktionsdaten zu schützen. Ein guter Anfang ist es, die Inferenz so nah wie möglich an den Daten durchzuführen. Eine Menge Spekulationen können direkt am Rand mit neuromorpher Verarbeitung und sogar Befehlssätzen in den Intel-Prozessoren durchgeführt werden, um das Risiko einer Manipulation der Daten während des Transports zu verringern. Stattdessen können Sie die Daten verschlüsseln und zurück zum Rechenzentrum senden.\n\nSchützen und Verwalten von Daten / Sichere KI-Pipeline\n\nSobald Sie alle Ihre Datenquellen identifiziert haben, gibt es drei kritische Aspekte für den Schutz: Kontrolle, Sicherheit und Verschlüsselung.\n\nDas Erste ist Kontrolle. Sie sollten eine Versionskontrolle, geschützte Bibliotheken und eine Sicherung und Wiederherstellung für den Fall von beschädigten Daten-Dateien haben. Dies sind standardmäßige gute Sicherheitspraktiken, die KI in der App-Entwicklung anwenden sollte.\n\nSicherheit sollte Zugriffsberechtigungen umfassen, sogar einige Zero-Trust-Konzepte wie den Zugriff nur für kurze Zeit auf Personen beschränken, die ihn benötigen. Bitte stellen Sie sicher, dass die Modelle nicht manipuliert werden und stellen Sie sicher, dass sie an spezifische Anwendungen gebunden sind.\n\nDie Daten sollten sowohl im Ruhezustand, bei der Übertragung als auch bei der Verwendung verschlüsselt werden. Früher war dies kostspielig in Bezug auf die CPU-Auslastung und Zeit, aber heute erfolgt ein Großteil der Verschlüsselung auf Siliziumbasis und ist sehr schnell ohne nennenswerte Leistungseinbußen.\n\n## Aufforderung zum Handeln\n\nDaten sind der Schlüssel für ein erfolgreiches und sicheres KI-System, daher schützen sie und wenden Sie von Anfang an bewährte Sicherheitspraktiken an. Operationalisieren Sie die Pipelines, um die Mitarbeiter von der tagtäglichen Arbeit der Bereitstellung und des Testens von KI-Algorithmen zu befreien. Automatisieren Sie so viel wie möglich und integrieren Sie Sicherheitsmaßnahmen in die KI-DevOps-Pipeline, um Ihre Quelldaten, Modelle und Anwendungen zu schützen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT85-de","image":"./episodes/edt-85/de/thumbnail.png","lang":"de","summary":"In dieser Folge diskutiert Darren die datenbezogenen Aspekte der künstlichen Intelligenz (KI) und die Bedeutung der Absicherung dieser Daten."},{"id":160,"type":"Episode","title":"Die Dinge, die ich als Regierungs-CTO gewusst hätte","tags":["cto","people","process","change","organizationalchange"],"body":"\r\n\r\nWas haben Jason und Ron seit ihrem Eintritt bei Intel gelernt, was sie sich gewünscht hätten, als sie noch beim Militär waren?\n\nJason war überrascht, wie viel besser Industrie-integrierte Lösungen entwickeln und einige unglaubliche Fähigkeiten entwickeln können, die in der Armee fehlen. Er lernte, dass Geschäftsanwendungsfälle zeigen, dass das Verständnis des Kundenproblems wertvoller ist als das bloße Vorantreiben der Technologie. CTOs müssen sowohl die Technologie kennen als auch lernen, wie sie die Kultur der Teammitglieder verändern können, um sich anzupassen. Eine benutzerfreundlichere Benutzererfahrung, eine intelligentere, schlankere und schnellere Technologie und die Einbindung der Menschen sind das, was reibungslosere Übergänge ermöglichen kann. Er hätte gerne mehr Ergebnisse und Lösungen in der Armee gesehen, anstatt nur Produkte.\n\nRon stimmt Jasons Einschätzung zu und fügt hinzu, dass die Herausforderung, die er von innen gesehen hat, darin bestand, die Industrie als Partner für Mission-Lösungen mit einzubeziehen, um die militärischen Teams besser darüber zu informieren, wie sie ihre Technologie auf diese Missionen anwenden können. Großartige Technologie ohne das Verständnis, wie man das eigentliche Missionsproblem löst, kann ein Fiasko sein.\n\nEin weiteres Element, auf das beide sich einigen, dass es wichtig ist, ist die Benutzererfahrung. Die Kombination der Technologie mit den Menschen, die sie bedienen müssen, ist entscheidend. Jeder, der die Technologie verwendet, muss keinen Doktortitel in Ingenieurwesen haben. Die Benutzererfahrung muss auf dem Kenntnisstand Ihrer Mitarbeiter aufgebaut sein. Das Ergebnis hängt selten unabhängig von den Menschen ab. Wenn Sie Lösungen für Verteidigung oder Industrie entwickeln, müssen Sie die technologischen Bedürfnisse der Mission verstehen, aber auch den gesamten Bereich und wie man in diesem Bereich arbeitet. Insbesondere im Militär müssen Sie planen, in Momenten zu handeln, wenn die Dinge nicht gut laufen, und es zum Funktionieren bringen. Technologie allein wird den Tag nicht gewinnen, wenn Sie das menschliche Element und herausfordernde Umstände nicht berücksichtigen.\n\nWas sind die größten oder aufregendsten Überraschungen für Jason und Ron, da sie vom Militär auf die Privatwirtschaft gewechselt sind?\n\nJason und Ron sagen, dass sie in der Privatwirtschaft bei Intel keine ausgeprägte Kultur des Teamworks erwartet hätten, wie sie sie aus dem Militärdienst kennen. Beide fanden die Einarbeitung und die fortlaufende Unterstützung ermutigend. Sie schätzen die allgemeine Einstellung, dass man zusammenarbeitet und die Möglichkeit, mit einem dynamischen Team Dinge zu schaffen, die man alleine niemals erreichen könnte.\n\nSie beide schätzen es, dass von ihnen nicht erwartet wird, nur Intel-Produkte zu verkaufen, sondern Kunden dabei zu helfen, Missionsschwierigkeiten zu lösen und Kundenfeedback an Intel weiterzugeben. Sie sind bei Intel, um Probleme zu lösen, insbesondere im öffentlichen Sektor, vielleicht mit Technologien, die noch nicht entwickelt wurden.\n\nWas sind die technologischen Lücken, die Jason und Ron im Verteidigungsministerium oder bei Intel sehen?\n\nVor allem sagt Jason, sind Cloud-Operationen und es handelt sich um eine sich verändernde Dynamik im Verteidigungsministerium. Kommandeure auf dem Schlachtfeld sind risikoscheu. Es gibt keinen Platz für DDIL. Mit der Weiterentwicklung der Cloud-Operationen muss man alles neu schulen und erlernen, um eine präzise Cloud-Operation durchzuführen und die Vorteile einer Edge-zu-Cloud-Fähigkeit zu nutzen, die Echtzeit- und genaue Informationen liefert, die zur richtigen Zeit an die richtigen Personen gelangen. Jeder muss eine situationsbewusste Wahrnehmung und ein operatives Bild von dem haben, was geschieht.\n\nRon glaubt, dass das nächste Element auf der Liste die Stärkung der Cybersicherheit ist, da die Angriffsfläche weiter wächst. Wenn das Militär nicht in Richtung Zero Trust geht, während es zu einer kompetenten Basis mit hoher Mobilität übergeht, könnten die Ergebnisse während eines Konflikts katastrophal sein. Das Problem mit \"DDIL\" ist enorm, aber es muss geschützt werden gegen die erhöhte Anfälligkeit.\n\nJason glaubt, dass die Technologie voranschreiten muss, trotz der Risiken, weil das Militär immer mit Informationen gewinnt. Egal ob es sich um FEMA-Missionen, die Versorgung einer Stadt mit Kernenergie, den Aufbau von Feldkrankenhäusern während COVID oder das Schlachtfeld handelt, technologischer Fortschritt, insbesondere 5G, ist entscheidend für die Operationen. Die Führungspersonen des Verteidigungsministeriums müssen über Informationen verfügen und die Möglichkeit haben, mit ihrem Hauptquartier zu kommunizieren, um Anweisungen zu erhalten, insbesondere bei Kampfeinsätzen, bei denen sich die Führung aufgrund von Verlusten bis hin zum niedrigsten Unteroffizier ändern kann.\n\nRon benutzt das Beispiel, dass die höchste Priorität der nationalen Verteidigung nie ein Kampf auf eigenem Territorium sein sollte. Da die Vereinigten Staaten im Konflikt gegen einen technologisch fortschrittlichen Gegner möglicherweise keine Mengenüberlegenheit außerhalb des Heimatlandes haben, muss das Militär fähiger sein. Das Militär muss die Fähigkeiten der nationalen Verteidigungskräfte weiterhin verbessern und dies trotz der gestiegenen Verwundbarkeit sicherstellen. Diese Probleme müssen gelöst werden, damit Betreiber den Daten vertrauen können und sie effektiv in einer Einsatzumgebung nutzen können. Es gibt keine andere Möglichkeit, als in diese Richtung zu gehen.\n\nJason fügt hinzu, dass ein weiterer Bereich, in dem das Verteidigungsministerium (DOD) langsam Anpassungen vornimmt, die künstliche Intelligenz (AI) ist, aufgrund eines Mangels an Personen mit entsprechendem Hintergrund. Man kommt nicht als AI-Experte aus der Army Ranger Schule heraus. Das gleiche Problem bestand jahrelang im Bereich der Cybersicherheit, bis das DOD eine signifikante Investition tätigte. Einige komplexe Ressourcen müssen für AI-Operationen verwendet werden, da AI alles verändern kann.\n\nRon, der die letzten Jahre mit dem Dienst im nuklearen Bereich verbracht hat, weist darauf hin, dass die USA niemals das menschliche Element aus entscheidenden Entscheidungsprozessen entfernen werden. KI wird enorm wertvoll sein, da sie gewährleisten kann, dass die Technologie sich dynamisch anpassen kann. Sie wird die kognitive Belastung reduzieren und unzählige Datenpunkte verarbeiten, sodass menschliche Entscheidungsträger eine klarere Situationsbewusstsein erhalten und besser darauf vorbereitet sind, schnell informierte Entscheidungen zu treffen. Das ist der KI-Bereich, dem das Verteidigungsministerium folgen muss.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jason Dunn-Potter","Ron Fritzemeier"],"link":"/episode-EDT86-de","image":"./episodes/edt-86/de/thumbnail.png","lang":"de","summary":"In dieser Folge begrüßt Darren Pulsipher, Chef-Solutions-Architekt im öffentlichen Sektor bei Intel, die speziellen Gäste Jason Dunn-Potter, ehemaliger Chief Warrant Officer der US Army, und Ron Fritzemeier, ehemaliger Rear Admiral der US Navy. Beide sind nun seit fünf Monaten als Solutions Architects und Missionsspezialisten im Team des Verteidigungsministeriums bei Intel tätig."},{"id":161,"type":"Episode","title":"Vertrauliches Computing in der DevSecOps","tags":["confidentialcomputing","devops","compute","technology","process","devsecops","cybersecurity","sgx"],"body":"\r\n\r\nAnjuna's Software ermöglicht es Anwendungen, unter dem Schutz von Intels SGX zu laufen und löst das Problem des Schutzes von Daten während ihrer Nutzung. Anjunas Mission ist es, sichere Enklaven so einfach wie möglich zu gestalten. Mit Anjunas Software ist es nicht erforderlich, irgendetwas in der Anwendung zu ändern; einfach nehmen, in einer Enklave ausführen und die SGX-Technologie funktioniert direkt \"out of the box\". Die Software funktioniert mit jeder App in jeder Cloud und in jedem Umfang.\n\nDie globale Software-Lieferkette wird angegriffen. Insbesondere war SolarWinds ein Angriff auf DevOps und obwohl es Ideen gab, wie man das Problem lösen könnte, wurde es noch nicht abgesichert. Die Anjuna-Technologie kann eine einfache Lösung sein. Es besteht kein Bedarf, Ihre Software neu zu konzipieren oder Methoden zu ändern. Sie können sie in sicheren Umgebungen ausführen.\n\nAls Darren Anjuna-Software ausprobierte, erstellte er einen Stapel mit Intel SGX an der Unterseite, Red Hat OpenShift, Anjuna für den vertraulichen Computing-Bereich und HashiCorp's Vault zur Speicherung eines sicheren Hauptbuchs. Er war schockiert, wie schnell die Lösung innerhalb einer Woche einsatzbereit war.\n\nDarren nennt diesen Prozess die gehärtete DevSecOps-Pipeline, obwohl es viele bewegliche Teile gibt. Ofir stimmt dieser Terminologie zu, da es sich um einen neuen, mit SGX-Hardware-Technologie gehärteten DevOps-Prozess handelt, der mit Anjunas Software läuft.\n\nVertrauliches Computing oder eine sichere Enklave löst das Problem des Datenschutzes. Wenn Sie Daten in persistentem Speicher speichern, ist die Lösung für Daten im Ruhezustand bereits vorhanden. Für Daten im Transit gibt es auch eine Lösung mit TLS. Das Sichern von Daten während der Verwendung wurde bisher nicht gelöst, da die Anwendung darauf zugreifen muss, während die Daten im Klartext im Speicher sind. Sie können nicht gleichzeitig verschlüsselt und in Verwendung sein. Dies ist ein endloser Kreislauf von Problemen. Wenn ein böswilliger Akteur Zugriff auf eine Maschine hat, auf der die Anwendung ausgeführt wird, ist ein Hack so einfach wie der Zugriff auf das Gerät, Identifizieren des Prozesses und Erstellen eines Speicherabbilds. So erhält er alle Geheimnisse und vertraulichen Daten, die aufgezeichnet sind, und sie sind nicht verschlüsselt. Dies beinhaltet auch die Verschlüsselungsschlüssel für Daten im Ruhezustand und beim Transport, da die Software sie verwenden muss, um zu verschlüsseln. Der schlechte Akteur wird Zugriff auf das gesamte System haben.\n\nDas Problem ist gelöst, wenn Sie die verschiedenen Anwendungen in sicheren Enklaven ausführen. Selbst wenn jemand Zugriff auf die Maschine hätte, hätte er keinen Zugriff auf den Speicher jeder Anwendung. Das bedeutet nicht, dass Sie Schwachstellen nicht beheben müssen, aber Sie sind wesentlich weniger gestresst, sie so schnell wie möglich zu beheben. Sogar wenn es Kernel-Schwachstellen gibt, kann der Kernel nicht auf den Speicher zugreifen, wenn etwas in einer sicheren Enklave läuft.\n\nAnjuna Software läuft sowohl auf anderen hardwarebasierten Technologien als auch auf SGX. Im Gegensatz zur Verschlüsselung in Software, bei der die Leistung erheblich beeinträchtigt würde, kann Anjuna die Konfiguration feinabstimmen, um Ihre Anwendung mit einer vernachlässigbaren Leistungseinbuße auszuführen: weniger als fünf Prozent.\n\nMöglicherweise möchten Sie noch nicht alles in einer sicheren Enklave speichern, aber es ist die Zukunft in Bezug auf Sicherheit.\n\nEiner der Verwendungszwecke für eine sichere Enklave besteht darin, Daten in einem sicheren Hauptbuch zu speichern, die verschiedene Schritte in der DevOps-Pipeline durchlaufen. Das Hauptbuch enthält alles, was in den Build, Sicherheitsschlüssel und Hash-Werte zur Überprüfung eingeflossen ist. Diese Überprüfungs-Hashs müssen während des gesamten Zyklus unverändert bleiben, damit niemand Code, Bibliotheken oder Binärdateien in das von Ihnen gelieferte Paket einschleusen kann. Alles sollte in einem Container in der modernen Welt ausgeführt werden.\n\nEin weiterer Kandidat für den Schutz ist ein Signaturschlüssel. Ohne sichere Enklaven müssen Sie, sobald Sie eine fertige Binärdatei haben, diese in einen dunklen Raum bringen, auf den niemand Zugriff hat. Aber dort unterschreiben drei Personen mit drei verschiedenen Schlüsseln. Sichere Enklaven ermöglichen den Zugriff auf diesen Signaturschlüssel in Ihrer vertrauten Umgebung, aber nur die Enklave wird darauf zugreifen. Es basiert auf der komplexen Identität der im SGX-Enklave ausgeführten Software, die über das Attestierungsquote implementiert wird. Mit anderen Worten, Sie können Enklave zu Enklave beglaubigen. Sie können auch Dinge beglaubigen, die außerhalb von Enklaven ausgeführt werden. Es ermöglicht Ihnen, Software, die an einem anderen Ort läuft, zu vertrauen.\n\nDie Zusammenstellung von Binärdateien ist eine weitere Verwendung. Eines der großen Probleme im Verteidigungsministerium ist zum Beispiel, dass sie garantieren möchten, dass alles, was in den Build eingeht, auf den Entwickler zurückverfolgt werden kann, der es geschrieben hat. Besonders in eingebetteten Systemen, in denen Software multimillionenschwere Maschinen steuert, die Menschenleben retten oder gefährden können. Es muss eine vollständige Rückverfolgbarkeit geben, um Rechenschaftspflicht zu gewährleisten und eine sichere Entwicklung durchzuführen.\n\nNeben Speicherabbildangriffen löst Anjuna ein weiteres Angriffsproblem. Es stellt sicher, dass in Fällen, in denen auf den Kernel zugegriffen werden muss, alles geschützt wird, was in der Interaktion zwischen der Enklave und der Außenwelt abgedeckt werden muss. Es kann auch davor schützen, auf Code zuzugreifen und Geheimnisse nur für die Enklave verfügbar zu machen. Darüber hinaus wird, falls jemand Zugriff auf eine Maschine erhält, weder ein TLS-Zertifikat noch der zur Verschlüsselung verwendete Schlüssel im Klartext gefunden.\n\nJeder Cloud-Dienstanbieter bietet sichere Enklaven an, und Anjuna unterstützt sie alle. Sie unterstützen auch On-Prem-Technologien. Zusätzlich zu dem Hauptangebot kann Anjuna auch die Möglichkeit bieten, Ihre Daten ruhend und während der Übertragung zu verschlüsseln, ohne Ihre Software zu ändern, selbst in Legacy-Anwendungen oder neuen Anwendungen, die nicht jede Datendatei verschlüsseln unterstützen.\n\nFür weitere Informationen besuchen Sie anjuna.io oder werfen Sie einen Blick auf ein Whitepaper von Darren und Ofir auf embracingdigital.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ofir Azoulay-Rozanes"],"link":"/episode-EDT87-de","image":"./episodes/edt-87/de/thumbnail.png","lang":"de","summary":"In dieser Episode diskutieren Darren Pulsipher, Chief Solutions Architect bei Intel, und Ofir Azoulay-Rozanes, Director of Product Management bei Anjuna, die Lösungen von Anjuna für vertrauliches Computing im DevOps-Lebenszyklus."},{"id":162,"type":"Episode","title":"Kollaborative DevSecOps","tags":["cybersecurity","devsecops","rpa","technology","compute","zerotrust","zta"],"body":"\r\n\r\nMike's Erfahrung als Cybersecurity-Ingenieur in der Luftwaffe und seine anschließende Arbeit in den Bereichen Entwicklung, Sicherheit, Betrieb und Managed Services führten ihn zu dem Ziel, ein Produkt zu schaffen, das kollaborativ sein könnte, um moderne Automatisierung um das zu bauen, was er IT als Code nennt. Er wollte DevSecOps ganzheitlich betrachten und alle Beteiligten zusammenbringen.\n\nSein resultierendes Produkt, Sophos Factory, schafft moderne Lösungen rund um Bausteine mit den Funktionen, der Funktionalität und der Benutzererfahrung, die über das gesamte Spektrum technischer Talente genutzt werden können. Dies war ein komplexes Problem, das visuell arbeitende Personen, Entwickler, die programmieren, usw. einschloss. Es musste auch die Lücke zwischen Hardware und Software überbrücken und dabei einen agilen Prozess über Teams hinweg verwenden.\n\nDie Sophos Factory ist mehr als nur eine CI/CD-Pipeline. Das ist nur ein kleiner Teil des gesamten Systems, das von Entwicklung über Sicherheit, Betrieb und Bereitstellung reicht und Funktionen wie einen visuellen Builder, DSL und Unterstützung für alle Inhalte im nativen Format bietet. Außerdem integriert es sich in bestehende Systeme. Es bringt alle verschiedenen Teams und die verschiedenen Werkzeuge, die sie verwenden, zusammen. Daher geht es weit über das Erstellen einer Pipeline oder Automatisierung hinaus.\n\nEinzelne Benutzer werden mit den ihnen vertrauten Modulen präsentiert, aber alle mit der gleichen Benutzeroberfläche. Zum Beispiel kann eine Reihe von Skripten aus einem visuellen Format erstellt werden. Ein Sicherheitsexperte kann dieselbe Benutzeroberfläche mit den erwarteten Tools und Artefakten nutzen. Ein Full-Stack-Entwickler oder ein DevOps-Ingenieur kann alle Artefakte einbinden und auf eine Weise erstellen, die von anderen Teams genutzt werden kann. Es geht nicht um die Erstellung einer Pipeline für automatische Prozesse, sondern um ein verbindendes Gewebe zwischen verschiedenen Systemen.\n\nIntegration bedeutet die Übertragung von Daten, aber es beinhaltet auch Aktionen. Angenommen, jemand verwendet auf der Codeseite Jira, auf der IT-Betriebseite ServiceNow und auf der Sicherheitsseite ein Stück zur Vorfallreaktion. In diesem Fall können Sie all diese Teile integrieren und etwas an Slack senden, damit alle eine Übersicht haben und nahezu in Echtzeit reagieren können.\n\nSophos Factory verpackt Module in Pipelines zur Wiederverwendbarkeit, die zu Bausteinen werden. Diese können um verschiedene Anwendungsfälle herum erstellt werden, aber das Ziel ist, etwas zu schaffen, das immer wieder verwendet werden kann. Zum Beispiel, nehmen wir an, Sie verwenden ServiceNow und möchten ein Ticket erstellen. In diesem Fall verwenden Sie dies für verschiedene andere Anwendungsfälle rund um Netzwerkautomatisierung, Infrastrukturautomatisierung, Cloud, native Sicherheit, usw. Es geht um den Aufbau von Lösungen, nicht nur um die Automatisierung dieser Dinge. Der letzte Teil ist zukunftssicher, nicht nur wiederholbar. Sie können der gesamten Pipeline Dinge hinzufügen oder entfernen, was mit Hardware nicht möglich ist, aber auch sehr schwierig mit bestehenden Systemen wie CI/CD-Systemen, die für die Bereitstellung von Software in Produktion gemacht sind, ist, um ganzheitlich eine Lösung aufzubauen und den Lebenszyklus im Laufe der Zeit aufrechtzuerhalten.\n\nMit der Sophos Factory-Technologie können Sie verschiedene Tools verpacken, um Standards wie CIF oder NIST 853 einzuhalten und sie als Low-Code- oder No-Code-Pipelines zur Verfügung zu haben. Sophos Factory unterscheidet sich von anderen Automatisierungstechnologien durch das Teilen über Lösungskataloge. Sie können Automatisierungsbaukasten veröffentlichen, vollständige Lösungen erstellen oder von anderen Teams erstellte Automatisierung nutzen. Dies schafft eine enorme Flexibilität.\n\nDie Versionskontrolle ist in die Pipelines und Lösungskataloge integriert. Wenn Sie eine Lösungspipeline von einer von jemand anderem veröffentlichten Aufzeichnung verwenden, können Sie sie auf die gewünschte Version setzen oder die neueste Version abrufen, um Aktualisierungen zu erhalten. RBAC ist auch Teil des Systems, falls Sie nur bestimmten Benutzern beispielsweise einen Lesezugriff ermöglichen möchten. Mit der Interoperabilität von Sophos Factory können Sie auch Scanning-Tools integrieren, um die Transparenz in der Pipeline aufrechtzuerhalten. Sie können auch verschiedene Kanäle mit Richtlinientools ausführen.\n\nSophos Factory verwebt Sicherheits- und IT-Arbeitsabläufe und schafft einen hervorragenden Integrationspunkt zwischen dem dreiköpfigen Monster von Entwicklung (Dev), Sicherheit (Sec) und Betrieb (Ops).\n\nUm die Sicherheit zu verbessern, verfügt Sophos Factory über ein Zero-Trust- und Attestationsprodukt. Sie arbeiten jedoch auch mit anderen Sicherheitsprodukten wie HashiCorp Console zusammen. Die Zero-Trust- und Attestationsfähigkeit ist die natürliche Entwicklung zur Authentifizierung zwischen verschiedenen Systemen. Anstatt statischer Zugangsdaten gibt es nun bessere Möglichkeiten, Attestationsinformationen sicher zu kommunizieren und zu teilen.\n\nSophos Factory verfügt über ein integriertes Berechtigungssystem für das Schlüsselmanagement und unterstützt HashiCorp Vault und cloudbasierte Vorlagen. Sie können auch bei der Erstellung kritischer Managementdienste in der Cloud helfen und diese um eine Pipeline herum verpacken. Es gibt nicht nur eine Berechtigungsvariable zur Laufzeit, sondern auch einen Berechtigungsschritt, der nur zur Laufzeit ausgewertet wird. Sie können diese Sicherheitstools zusätzlich nutzen, damit sie natürlicher Bestandteil Ihrer Entwicklungslösung werden.\n\nSophos Factory befindet sich im Bereich der RPA, aber es geht weit über einen typischen RPA-Ausführer hinaus. Sie sind technisch gesehen RPA, denn obwohl Menschen immer noch daran beteiligt sind, Dinge herzustellen, werden Maschinen eingesetzt, um den Prozess zu automatisieren. Kunden suchen nach Möglichkeiten, sicher zu skalieren und Wert aus ihren IT-Käufen zu ziehen. Sophos Factory unterstützt dabei, technisches Talent weiterzuentwickeln und ihnen Zugang zu Werkzeugen zu geben, um mehr aus ihnen herauszuholen und dies sicher zu tun.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Mike Fraser","Callen Sapien"],"link":"/episode-EDT88-de","image":"./episodes/edt-88/de/thumbnail.png","lang":"de","summary":"In dieser Folge spricht Darren mit Callen Sapien, dem Direktor für Produktmanagement bei Sophos Factory, und Mike Fraser, dem VP für DevSecOps, über ihr Produkt, das wirklich kollaboratives SecDevOps ermöglicht."},{"id":163,"type":"Episode","title":"Zukunft von Big Memory und CXL","tags":["bigmemory","memverge","optane","technology","data","compute","pmem","cxl","ceo"],"body":"\r\n\r\nEine spannende Entwicklung im Bereich des Speichers ist CXL (Compute Express Link). Auf der Host-Seite wird ein robustes Ökosystem entwickelt, bei dem Intel und andere CXL 1.1 in ihren Plattformen der nächsten Generation, wie zum Beispiel Sapphire Rapids, unterstützen. Diejenigen auf der Speicherseite, einschließlich Intel Optane der vierten Generation, werden CXL nutzen. Intel ist ein treibendes Mitglied eines starken CXL-Konsortiums, das den Standard vorantreibt. CXL 2.0 wurde bereits definiert und bietet zusätzliche Funktionen wie die Externalisierung und Umschaltung von CXL. CXL 3.0 befindet sich in Bearbeitung und wird standardisieren, wie CXL bereitgestellt und geteilt werden kann.\n\nEs werden neue Produkte von Samsung, Micron und SK-Hynix mit CXL-Fähigkeiten auf den Markt kommen. Darüber hinaus werden neue Verbindungen entwickelt, die möglicherweise Speicher mit dem Netzwerk verbinden können, sodass ein speicherfähiges Netzwerk entsteht, das über mehrere Server gemeinsam genutzt werden kann.\n\nDie CXL-Technologie ist ein Spielveränderer. Ein neues Speicherprotokoll läuft auf Standard-PCIe der fünften Generation und später. Da es Ihnen ermöglicht, den Speicher auf PCIe zu platzieren, kann er nicht nur innerhalb des Gehäuses sein, sondern in Zukunft mit PCIe-Switching kann der Speicher auf dem Netzwerk componierbar und gemeinsam nutzbar sein. Das erste CXL-Produkt wird bis Ende des Jahres herauskommen.\n\nMemVerge hat bereits einen softwaredefinierten Speichercontroller, und CXL hat eine völlig neue Welt für die Software eröffnet. CXL ist für den Speicher, was Fibre-Channel für die Speicherung ist. Es handelt sich um ein Speicherbereichsnetzwerk anstelle eines Speichernetzwerks. Es ist möglich, mit CXL direkt in den Speicher zu gehen und die CPU komplett zu umgehen.\n\nCXL wird deutlich schneller sein als bisherige Verbindungsstandards. Es wird eine Latenzzeit von 100 oder 200 Nanosekunden haben. Hier erhöht sich die Auslastung, Verwaltbarkeit und Flexibilität. Es wird eine höhere Verfügbarkeit und Produktivität bei der Nutzung des Speichers geben. Außerdem wird es möglich sein, den Speicher dynamisch bereitzustellen; Sie können ihn bei Bedarf bereitstellen, ohne dass er in den Server passen muss. Theoretisch haben Sie immer genügend Speicher für alles, was Sie aktiv benötigen.\n\nMit der Snapshot-Technologie von MemVerge ist Ihre Daten nicht nur geschützt, sondern auch dauerhaft verfügbar. Dies wird umso wichtiger, je größer Ihr Speicher wird. Wenn Sie ihn verlieren, wird es schwieriger, ihn wiederherzustellen.\n\nDamit CXL erfolgreich ist, müssen drei Dinge vorhanden sein. Auf der Hardware-Seite müssen die führenden älteren Hardware-Anbieter mit an Bord sein und dieselben Standards unterstützen. Das ist im vergangenen Jahr geschehen, daher gibt es einen einheitlichen Standard, den jeder unterstützt. Zweitens müssen Sie Ihre Anwendung nicht ändern, um CXL zu verwenden, genauso wie bei Storage Area Networks. Drittens sollten Sie aus Sicht der Datenbank keine umfangreichen Neuschreibungen vornehmen müssen. Dies kann durch eine Kombination der bereitgestellten Standards, der Unterstützung des Betriebssystems und der MemVerge-Software erreicht werden, die Auto-Tiering zwischen DDR-Speicher und CXL-Speicher ermöglicht.\n\nMemVerge kann diese Abstraktionsschicht bieten. Es handelt sich im Wesentlichen um Memory-Virtualisierung. Der softwaredefinierte Speicher behandelt die tatsächliche Platzierung des physischen Speichers.\n\nMemVerge macht großen Arbeitsspeicher für die Anwendung transparent, sodass Programmierer die höhere Kapazität nutzen können, ohne jemals den Speicherplatz zu verbrauchen. Als nächstes kommt der Datenschutz. MemVerge hat einen In-Memory-Snapshot-Service entwickelt, der den gesamten Zustand einer Anwendung im Speicher erfassen kann, und dieser Zustand ist unveränderlich. Er kann jederzeit und überall wiederhergestellt werden. Es gibt viele Anwendungsfälle dafür, wie beispielsweise das Abmildern von Ransomware und die Verkürzung der Zykluszeiten in der Genomforschung.\n\nDie Snapshot-Funktion ist nicht nur nützlich, weil sie eine laufende Pipeline schnell und einfach einfangen kann, was es ermöglicht, jederzeit zurückzurollen und wiederherzustellen, sondern sie kann auch Geld sparen, wenn Cloud-Services genutzt werden. Die Hauptanbieter von Diensten bieten Spot-Instanzen an, die um 70-90% günstiger sind als der Nachfragepreis, aber es gibt einen Haken: Sie können diese jederzeit mit nur 30 Sekunden bis zwei Minuten Vorwarnzeit zurücknehmen. Das ist keine ausreichende Zeit, um damit umzugehen, besonders wenn viele Daten im Arbeitsspeicher liegen, daher war es bisher für viele Workloads nicht hilfreich. Mit der Snapshot-Fähigkeit von MemVerge können Sie regelmäßige Schnappschüsse Ihrer laufenden Workload in jeder Instanz machen. Wenn die Spot-Instanz weggenommen wird, haben Sie ein Abbild, das Sie wiederherstellen und weiter ausführen können. Das ist eine Versicherung, die es Ihnen ermöglicht, den kostengünstigen Service mit Schutz zu nutzen.\n\nDa Sie nicht nur eine Anwendung, sondern einen gesamten Container oder eine Instanz erfassen, können Sie ihn überall wiederherstellen - vor Ort, in derselben Cloud oder in einer anderen Cloud. Dies bietet Ihnen maximale Beweglichkeit und Widerstandsfähigkeit in Ihren Abläufen, selbst im Falle eines größeren Ausfalls von Cloud-Diensten. Diese Technologie bietet viele aufregende Möglichkeiten.\n\nDie CXL-Revolution und die MemVerge Memory Machine-Software sind eine beeindruckende Kombination mit wegweisenden Möglichkeiten.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Charles Fan"],"link":"/episode-EDT89-de","image":"./episodes/edt-89/de/thumbnail.jpg","lang":"de","summary":"Darren Pulsipher, Chief Solutions Architect bei Intel, spricht mit Charles Fan, CEO von MemVerge, über die CXL-Revolution und wie die Software von MemVerge die Zukunft des großen Arbeitsspeichers darstellt."},{"id":164,"type":"Episode","title":"Teleheimarbeit: Sichern Sie Ihr Home Office","tags":["cybersecurity"],"body":"\r\n\r\n## Sicherheitstipps für das System\n\nDas erste, was Sie tun möchten, um Ihr Home-Office abzusichern, ist das Aktualisieren aller Ihrer Systeme. Dies umfasst sowohl Anwendungen als auch Betriebssysteme auf allen Ihren mit dem Internet verbundenen Geräten. Sie möchten auch Ihre Endpunkt-Sicherheitssoftware aktualisieren und regelmäßige Scans durchführen, wenn Ihr Gerät eingeschaltet ist. Viele Menschen wissen, dass sie dies auf ihrem Desktop oder Laptop tun sollten, aber Endpunkt-Sicherheit sollte auch auf Telefonen und Tablets implementiert werden. Zusätzlich sollten Sie Ihre lokale Firewall einschalten und die Firewall Ihres Routers aktivieren.\n\nEiner der wichtigsten Schritte, den Sie tun können, um Ihr Gesamtrisiko im Internet zu reduzieren, besteht darin, die Angriffsfläche während der Laufzeit zu verringern. Das bedeutet, dass Sie Anwendungen schließen sollten, die nicht verwendet werden, den Browser schließen sollten, bevor Sie auf neue Seiten gehen, und sich aus- oder abmelden sollten bzw. sichere Sitzungen schließen sollten, bevor Sie Aktivitäten wie E-Mails prüfen oder browsen. Sie sollten keine verschiedenen Aktivitäten gleichzeitig durchführen, um Querangriffe zu vermeiden.\n\nEine gute Bildungsseite zum sicheren Surfen im Internet ist Stop. Denken. Verbinden. https://www.stopthinkconnect.org/\n\n## Modem/Router/WLAN-Sicherheitstipps\n\nÄndern Sie alle Standardpasswörter in sichere Passwörter (mindestens 8-10 Zeichen, verwenden Sie Groß- und Kleinschreibung, Zahlen und Sonderzeichen). Standardpasswörter sind bei Routern, Modems, ISP-Webportalen und WLAN-Geräten vorhanden. Achten Sie sorgfältig darauf, wem Sie Zugriff auf Ihre Passwörter gewähren. Es ist auch wichtig, den Standardnetzwerknamen (SSID) in etwas ohne identifizierende Informationen zu ändern.\n\nDie Aktivierung der Zwei-Faktor-Authentifizierung, wo immer möglich, wird Ihnen eine zusätzliche Sicherheitsebene bieten. Router und Modems müssen genauso wie Ihr Laptop aktualisiert werden, stellen Sie also sicher, dass Sie automatische Updates aktiviert haben.\n\nWeitere Schritte zur Erhöhung der Sicherheit umfassen das Aktivieren von WPA und das Deaktivieren von WPS, falls möglich. Aktivieren Sie die Netzwerkadressübersetzung (NAT) und die DNS-Filterung auf dem Router und dem Modem. Sie sollten auch UPnP deaktivieren.\n\nDiese Techniken werden verhindern, dass unbefugte Personen und Ihre Nachbarn Ihr WLAN \"ausleihen\", was ein Sicherheitsrisiko darstellt.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT9-de","image":"./episodes/edt-9/de/thumbnail.png","lang":"de","summary":"Das Absichern Ihres Geräts und Ihres Rechenzentrums reicht nicht aus. Mit immer mehr Personen, die von zu Hause aus arbeiten, müssen Sie Ihren Mitarbeitern helfen, ihr Heimnetzwerk und ihren Arbeitsbereich abzusichern. In dieser Episode hilft Steve Orrin, Bundes-CTO bei Intel, Darren dabei, sein Heimnetzwerk zu sichern."},{"id":165,"type":"Episode","title":"Hindernisse bei der Annahme zukünftiger Technologien","tags":["cto","organizationalchange","change","people","process"],"body":"\r\n\r\nGlenn ist der Chief Technology Officer bei CACI, einem Unternehmen mit einer Größe von sechs Milliarden Dollar, das sich auf nationale Sicherheit konzentriert und zu den Fortune-500-Unternehmen gehört. Er ist seit 35 Jahren in der Branche tätig und widmet sich der militärischen, nachrichtendienstlichen und cyber-sicherheitstechnischen Arbeit für die US-Regierung. Er ist Ingenieur, daher denkt, handelt und bearbeitet er Fragen wie ein Ingenieur.\n\nAufregende neue Technologien in den Bereichen Künstliche Intelligenz, neuromorph und Sicherheit werden manchmal nicht schnell genug im öffentlichen Sektor übernommen.\n\nDie erste Hürde ist Komplexität. Diejenigen in der technischen Gemeinschaft sind herausragende Innovatoren, aber nicht immer gut darin, die Technologie einfach konsumierbar, benutzbar oder administrierbar zu machen. Glenn sieht jedoch einige Verbesserungen. Zum Beispiel ist in den letzten fünf Jahren die Komplexität der Nutzung von KI deutlich gesunken, da immer mehr Menschen auf Tools und Softwareentwicklungskits zurückgreifen.\n\nEine weitere Hürde kann der Mangel an Talent sein, um Technologien im öffentlichen Sektor zu entwerfen, zu entwickeln und instand zu halten. Neues Talent muss nicht nur für die Branche gewonnen werden, sondern sie müssen auch in der Lage sein, Sicherheitsfreigaben zu erhalten. Auf der anderen Seite gibt es viele, die gegen die Übernahme neuer Technologien Widerstand leisten. Es handelt sich um eine Herausforderung im Change Management. Wenn dies nicht richtig angegangen wird, werden vorhandene Talente zu Hindernissen bei der Übernahme neuer Technologien. Anstatt sich zu fragen, wie es möglich gemacht werden kann, kommen sie mit tausend Gründen auf, warum es nicht funktioniert.\n\nDer dritte Punkt betrifft die Cybersicherheit. Glenn beschreibt dies nicht als Hürde, da es nicht etwas ist, das entfernt werden könnte. Dennoch könnte die Branche proaktiver sein und die Entscheidungen weiter nach links verlagern sowie die Sicherheit frühzeitig einbeziehen, um die Einführung zu beschleunigen.\n\nDer öffentliche Sektor könnte eine Lektion daraus ziehen, wie schnell Smartphone-Technologie mit dem Aufkommen des iPhones übernommen wurde. Die frühen iPhones waren erstaunlich einfach zu bedienen, und das Unternehmen war darauf bedacht, den Kunden etwas zu übergeben, das sie sofort nutzen konnten. Darüber hinaus konnte die Entwicklergemeinschaft sofort mit den Software-Entwicklungswerkzeugen und -prozessen innovieren und neue Dinge schaffen. Schülerinnen und Schüler der sechsten Klasse können mit den Entwicklerwerkzeugen für iPhone und Android entwickeln. Benutzerfreundlichkeit war von entscheidender Bedeutung und leitete die Softwareentwicklung effizient ein.\n\nDer öffentliche Sektor sollte versuchen, diese Qualitäten zu imitieren und seinen Markt und seine Kunden auf die gleiche Weise anzugehen, vor allem im Bereich der Datenanalyse. Es gibt eine enorme Menge an Daten, die wir nicht optimal nutzen. Genauso wie Steve Jobs seine Technologie über das durchschnittliche Zielpublikum hinausgebracht hat, muss dasselbe bei der Datenanalyse geschehen. Anstatt sich auf Datenwissenschaftler zu konzentrieren, sollte vielleicht eine Geschäftsperson oder eine Durchschnittsperson, die einen Haushalt führt, das Ziel sein.\n\nGlenn glaubt, dass der erste Schritt darin besteht, auf eine beherzte Weise dem Gedanken an KI und Analysen nachzugehen, indem man sich darauf konzentriert, Daten zu befreien. So viele Daten sind in proprietären Designs eingeschlossen, dass es ein schlechtes Geschäftsmodell ist. Kunden sollten Verträge erneuern, nicht weil ihre Informationen von einem Anbieter kontrolliert und abgeschlossen werden, sondern weil der Anbieter Lösungen, Wert und Innovationen bietet.\n\nKI sollte als digitaler Assistent für Kunden positioniert werden, anstatt als undurchsichtiger, magischer Roboter im Hintergrund, dem der Kunde skeptisch gegenübersteht. Ein digitaler Assistent ist einfach einen Schritt voraus bei dem, was Sie erreichen möchten, indem er Daten und Informationen über maschinelles Lernen und Deep Learning bereitstellt, um Ihr Leben einfacher zu machen und Ihnen die Verarbeitung zu ermöglichen. Auf diese Weise bietet KI sofortigen Mehrwert und ist viel weniger beängstigend.\n\nGlenn glaubt, dass die Technologie, über die der öffentliche Sektor heute verfügen muss, zuerst die Cloud und mehr davon ist. Die Cloud nimmt eine Menge menschlicher Fehler aus der Verwaltung heraus. Sie verringert die Angriffsfläche und es handelt sich um ein Abrechnungsmodell nach Verbrauch, was kostengünstig sein kann, wenn man Software für dieses Modell entwickelt.\n\nZweitens sind gute DevSecOps-Plattformen vorhanden. Softwareentwickler können mehr Zeit mit den verfügbaren Tools für die tatsächliche Entwicklung verbringen. DevSecOps ist noch in einem frühen Entwicklungsstadium, und das Wachstum wird explosiv sein.\n\nEine Technologie, die während der Fernarbeit während COVID entscheidend war, ist \"Commercial Solutions for Classified\" (CSFC). Diese Technologie gibt es seit einem Jahrzehnt. Sie hat sich jedoch so weit entwickelt, dass die Komplexität bei ihrer Bereitstellung, Verwaltung und Sicherheit leicht beherrscht werden kann. So ist es beispielsweise möglich, dass jemand in einem unklassifizierten Bereich arbeitet und dennoch Zugang zu Geheimnissen hat, mit der Gewissheit, dass keine Datenlecks auftreten. Dies ist eine neue, wichtige Fähigkeit.\n\nAll dies steht in Verbindung mit dem Anziehen und Halten von talentierten Mitarbeitern. Die Schwierigkeit dabei liegt nicht daran, dass der öffentliche Sektor keine spannenden, herausfordernden Probleme hat, sondern an der Wahrnehmung, dass die Regierung sehr langsam agiert. Darin steckt ein gewisser Wahrheitsgehalt aufgrund der Bedeutung, eine gewisse Unantastbarkeit oder Vertrauen aufrechtzuerhalten. Dennoch ist die Regierung in der Realität bei vielen neuen Technologien wie der Photonik oder lichtbasierten Kommunikation ganz vorn mit dabei. Einige Bereiche agieren schnell, wie beispielsweise die Softwareentwicklung mit agiler Methode und DevSecOps.\n\nGlenn glaubt, dass das Weltraumgebiet in fünf Jahren explodieren und sich grundlegend von der Zukunft der Technologie unterscheiden wird. Die Fähigkeit, mit Verbesserungen in Größe, Gewicht und Leistung kostengünstig mehr Dinge in den Orbit zu bringen, zusammen mit der Möglichkeit, schnell über Tausende von Kilometern hinweg mit Photonik zu kommunizieren, ermöglicht Konnektivität sowie die Verteilung und Nutzung von Daten auf diesen Nutzlasten.\n\nEin weiterer Bereich, der sich stark unterscheiden wird, ist die Spektrum-Agilität. Das Einsatzgebiet des elektromagnetischen Spektrums wird entscheidend für eine widerstandsfähige Konnektivität in Konflikten sein. Die darauffolgenden Auseinandersetzungen werden stark betonen, wer kommunizieren kann und mit Kommunikationseinrichtungen stören kann. Spektrum-Agilität bedeutet, die Fähigkeit zu haben, dynamisch zu wissen, was im Spektrum um einen herum passiert, um schnell reagieren zu können. Diese Konnektivität ist grundlegend für gemeinsame Führung und Kontrolle sowie für die JADC2-Vision, um erfolgreich zu arbeiten.\n\nEin dritter Bereich ist, was mit Kubernetes und Infrastrukturcode gemacht werden kann. Automatisierung wird Arbeit abbauen und allgemein Dinge erleichtern.\n\nKein Gespräch über die Zukunft der Technologie wäre komplett ohne eine Erwähnung von Quanten. Die entwickelten Quantenprogrammiermodelle unterscheiden sich komplett von traditionellen Modellen. Wir müssen an den Punkt gelangen, an dem wir Werkzeuge für Programmierer haben, die den Prozess stark automatisieren. Die Ausbildung einer Person für zwei Jahre als Quanten-Computing-Programmierer ist nicht skalierbar, daher benötigen wir die Entwicklung von Software-Entwicklungswerkzeugen, ähnlich wie die Gemeinschaft die Komplexität abstrahiert hat und Werkzeuge für künstliche Intelligenz entwickelt hat.\n\nDer letzte Bereich ist die fortgesetzte Evolution des Edge Computing. Die Anzahl der Rechenprozesse ist phänomenal und fördert Kreativität bei Größe, Gewicht und Stromverbrauch. Edge Computing wird sich weiterhin zuverlässig und vertrauenswürdig entwickeln. Belastbare Kommunikation erfolgt möglicherweise nicht über eine dedizierte Verbindung, sondern über ein Mesh-Netzwerk, bei dem Teile der Nachricht am anderen Ende wiederverbunden werden. Dies kann Lösungen in taktischen und gestörten Umgebungen ermöglichen.\n\nDarren sieht die traditionelle Von-Neumann-Architektur in Zukunft schwinden, da wir Datenbeständigkeit ohne Festplatten haben können und Daten haben können, die leben, sich bewegen und migrieren, indem sie mit Funktionen arbeiten, die damit arbeiten. Die Schichten und Beschränkungen des Von-Neumann-Modells werden entfernt werden.\n\nGlenn denkt, dass dies schneller passieren wird als die meisten Leute vorhersagen würden, wegen Erfahrungsvoreingenommenheit. Aber es ist eine andere Welt, wenn man alle anderen Teile zum koexistieren bringen kann und die Latenzprobleme beseitigt. Stell dir vor, was zum Beispiel bei der Inferenzgeschwindigkeit am Rand mit autonomen Fahrzeugen möglich wäre. Diese Technologie entwickelt sich und Programmierer denken von Anfang an an Abstraktionsschichten. Du könntest zum Beispiel ein bereits entwickeltes neuronales Netzwerkmodell nehmen und es durch ein Softwareentwicklungstoolkit laufen lassen, um es auf eine Hardware-Substrat, eine Nicht-Von-Neumann-Architektur, zu platzieren, und du musst es nicht neu programmieren. Dies wird die Einführung beschleunigen und transformierend sein.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Glenn Kurowski"],"link":"/episode-EDT90-de","image":"./episodes/edt-90/de/thumbnail.jpg","lang":"de","summary":"Darren Pulsipher, Chief Solutions Architect, Intel, diskutiert mit Glenn Kurowski, CTO von CACI, über die Hindernisse bei der Einführung neuer Technologien im öffentlichen Sektor und was in Zukunft zu erwarten ist."},{"id":166,"type":"Episode","title":"Kämpfe gegen den Cybergegner, indem du deine Software-Lieferkette sicherst.","tags":["cybersecurity","devsecops","sbom","securesupplychain","policy","process","compute"],"body":"\r\n\r\nDarren und Eric Greenwald, der Justiziar von Finite State, diskutieren in dieser Folge die Sicherung der Software-Supply-Chain.\n\nFinite State konzentriert sich darauf, Schwachstellen in Firmware zu finden, meistens Software von Dritten, die bereits bestehende Schwachstellen haben könnte, bevor sie diese in ihre Geräte integrieren. Finite State konzentriert sich hauptsächlich auf industrielle IoT-Geräte, medizinische Geräte, Automobile und Unterhaltungselektronik.\n\nVor sieben Jahren arbeitete Eric als Anwalt im privaten Sektor, wobei er sich auf Sicherheitstests und die Identifizierung von Schwachstellen für Cybersicherheitsunternehmen spezialisierte, bevor er Finite State beitrat. Zuvor arbeitete er im Regierungsbereich mit Schwerpunkt auf Cybersicherheit und nationaler Sicherheit für das FBI und die CIA sowie als Chefberater für den Geheimdienstausschuss des Repräsentantenhauses. Seine Regierungsarbeit erreichte ihren Höhepunkt, als er zum Senior Director für Cybersicherheit im Nationalen Sicherheitsrat im Weißen Haus wurde.\n\nEric glaubt, dass ein Teil des Grundes dafür, dass jetzt ein Schwerpunkt auf der Absicherung der Lieferkette liegt, darin besteht, dass die Bedrohung sich weiterentwickelt hat. Die zunehmend komplexe Natur von Software, einschließlich der Tatsache, dass viele Komponenten bei ihrer Erstellung anfällig sind und erst im Laufe der Zeit entdeckt werden, erschwert es, Geräte zu finden, die über die Software-Lieferkette erstellt wurden. Zudem haben kürzlich hochkarätige Angriffe über eine Verletzung der Software-Lieferkette wie bei SolarWinds die Menschen stärker auf die Gefahr aufmerksam gemacht.\n\nIn der SolarWinds-Attacke hatte der Täter Geduld und verursachte nicht sofort Chaos, sondern wartete ein Jahr lang, während er in amerikanischen Computernetzwerken umherstreifte, um Zugang und Informationen zu gewinnen. Diese Geduld ist wahrscheinlich der signifikanteste Unterschied zwischen einem staatlichen Angriff und einem kriminellen Angriff. Während kriminelle Angriffe manchmal geduldiger auf eine finanzielle Rendite ausgerichtet sind, sind diejenigen hinter einem staatlichen Angriff bereit, Jahre zu investieren, um ihren Zugang zu Informationen zu entwickeln. Bei beiden werden die Angriffe jedoch immer raffinierter und besser positioniert, um die komplexe Lieferkettenstruktur verheerend auszunutzen.\n\nUm diese Angriffe zu bekämpfen, tauchen neue Vorschriften auf. Die Biden-Regierung erließ im Mai 2021 die Exekutivverordnung 14028, die zwei Hauptpunkte hat: sichere Softwareentwicklung und Software-Bill of Materials (SBOM). Die Exekutivverordnung richtet sich nur an Regierungsbeschaffungen, aber die Privatindustrie wird wahrscheinlich folgen.\n\nDie Details und technischen Empfehlungen für die Entwicklung sicherer Software werden noch ausgearbeitet. Ein Teil davon wäre jedoch, dass Softwarelieferanten an die Regierung ein SBOM bereitstellen müssten. Der erste Gesetzesentwurf für SBOMS wurde 2014 veröffentlicht, so dass die Standards für deren Erstellung reifer und weiter entwickelt wurden. Ein SBOM ist im Wesentlichen eine Liste der Softwarekomponenten, die in ein Softwareprodukt eingeflossen sind, nicht so unterschiedlich wie eine Zutatenliste auf einem Lebensmittelprodukt. Dies bietet Transparenz in der Lieferkette, was wesentlich ist, um Schwachstellen zu bewerten oder eine Schwachstelle zu identifizieren, die zu einem späteren Zeitpunkt entdeckt wird.\n\nEin ausgezeichnetes Beispiel dafür ist das, was mit Log4j passiert ist. Als diese Schwachstelle entdeckt wurde, hatten viele Unternehmen keine Ahnung, ob sie Log4j in ihrem Stapel hatten. Es wäre kein Zauberstab, aber ein SBOM würde es Unternehmen ermöglichen, leichter festzustellen, ob sie das problematische Softwarekomponente in ihrem System haben, und schneller einen Patch implementieren.\n\nArgumente gegen die Veröffentlichung von SBOMs sind, dass sie Angreifern einen Fahrplan liefern und proprietäre Informationen preisgeben werden. Obwohl dies legitime Bedenken sind und diskutiert werden müssen, sehen das Handelsministerium und das Ministerium für Innere Sicherheit einen viel größeren Nutzen darin, Transparenz für Verteidiger zu haben als irgendwelche Vorteile für Angreifer. Es gibt parteiübergreifende Gesetzesentwürfe zur Unterstützung von SBOMs. Es gibt Möglichkeiten, das Risiko zu verringern, dass SBOMs in falsche Hände gelangen, wie sichere oder nicht austauschbare Verträge. Die Debatten über diese Bedenken werden im öffentlichen Sektor fortgesetzt und immer mehr Unternehmen werden sie übernehmen.\n\nDiese Gesetzgebung findet statt, weil die FDA vorgeschlagen hat, dass Hersteller von medizinischen Geräten SBOMs als Teil des Überprüfungsprozesses integrieren. Daher gewinnen SBOMs an Dynamik bei diesen Herstellern. Die physische Welt wird zunehmend von Software in medizinischen Geräten und eingebetteten Systemen wie Steuersystemen für Kraftwerke, HVAC-Systemen, Flughafenkontrollen usw. beeinflusst. Daher sind operative Systeme gefährdet und haben schwerwiegendere Folgen als Angriffe auf Geschäftssysteme.\n\nEine Schwierigkeit für OT-Profis ist, dass viele der industriellen Komponenten älter sind und möglicherweise nicht aktualisiert wurden. Dennoch zögern Hacker, sich mit dem Internet zu verbinden und Updates durchzuführen, denn so gelangen Hacker ins System. Die beste Lösung für dieses Problem besteht darin, Transparenz über die Komponenten im Stack zu erlangen, das System und die Geräte im OT-Netzwerk zu scannen und Reverse Engineering und Decompiling durchzuführen, um die Details zu verstehen. Im Wesentlichen müssen Sie Ihre SBOM erstellen und die verwundbaren Stellen bewerten.\n\nDies ist der Hauptarbeitsbereich für Finite State. Sie untersuchen Systeme und Geräte und führen eine Analyse durch. Sie verfügen über eine Plattform, die die Analyse des Embedded-Codes automatisiert, eine Aufschlüsselung der Schwachstellen liefert und die verwundbarsten und wichtigsten Schwachstellen identifiziert und gruppiert. Manchmal kann man eine ganze Kategorie von Offenheit mit einer einzigen Lösung beseitigen. Das alleinige Erstellen der SBOM reicht jedoch nicht aus. Sie muss in ein Risikomanagementsystem eingebunden werden, um die vielen Schwachstellen zu durchsuchen und zu sortieren. Das Finden der wichtigsten Risiken ist ein komplexer Prozess, und Finite State kann Sicherheitsteams dabei helfen, ihre Maßnahmen zum Schutz ihrer Systeme zu priorisieren.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Eric Greenwald"],"link":"/episode-EDT91-de","image":"./episodes/edt-91/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solutions Architect bei Intel, und Eric Greenwald, General Counsel von Finite State, sprechen über die Sicherung der Software-Lieferkette."},{"id":167,"type":"Episode","title":"Schützen Sie die Außenmauern Ihres Schlosses mit Verwundbarkeitsscans.","tags":["cybersecurity","endpointmanagement","vulnerability","cyberattack","iot","edge","censys"],"body":"\r\n\r\nMatt verbrachte acht Jahre als Geheimdienstoffizier beim Militär. Im späteren Teil seines Dienstes half er beim Aufbau der Cyber-Missionstruppen als Teil des US Cyber Command. Nachdem er seinen Dienst beendet hatte, arbeitete er mit Army Cyber und einigen Start-ups, bevor er zu Censys kam.\n\nEine typische Schwachstellenanalyse erfordert von einer Organisation, dass sie angibt, woran sie interessiert sind, zum Beispiel eine Liste von IP-Adressen für einen Schwachstellen-Scan. Anschließend werden diese Schwachstellen entdeckt und können behoben werden. Ein Censys-Scan kann anhand einiger grundlegender Informationen wie einer Domäne und einigen IP-Adressen einen breiten und tiefgehenden Scan des Internets durchführen und alles entdecken, was die Organisation besitzt. Zusätzlich überwacht Censys kontinuierlich nach denselben Zusammenhängen und Korrelationen innerhalb des Datensatzes. Wenn zum Beispiel die Marketingabteilung eine ungeschützte Website einrichtet, die kein TLS hat, wird Censys es bemerken. Censys nimmt eine externe Sicht ein, um zu wissen, was zu einem beliebigen Zeitpunkt für einen Angreifer verfügbar ist.\n\nSelbst wenn zum Beispiel jemand eine Anwendung in der Cloud bereitstellt, sie nicht auf der Domain platziert und Sicherheitsverschleierung verwendet, kann Censys sie dennoch erkennen, wenn sie sich in einer Cloud-Umgebung befindet, die der Organisation gehört. Cloud-Verbindungen werden neue Cloud-Instanzen entdecken, die auftauchen. Einige verbindende Informationen, egal ob es sich um WHOIS- oder DNS-Informationen handelt, müssen mit einigen dieser Instanzen korrelieren. Censys wird immer besser darin, diese Arten von Instanzen zu erkennen.\n\nDie Analogie einer Burg funktioniert hier gut. Eine Organisation möchte sich nicht nur auf das verlassen, was sie intern mit Kameras und Schildwachen sehen können. Sie möchten eine wandernde Sicherheitspatrouille, um Bedrohungen abzufangen, noch bevor sie die Burgmauern erreichen. Die Patrouille kann die Burg so beobachten, wie es ein Angreifer tun würde. Censys durchstreift das Internet aus einer externen Perspektive und sieht Dinge so, wie es ein potenzieller Hacker sehen würde.\n\nEin Werkzeug, das Censys in Betracht zieht, ist JARM von Salesforce, ein aktives TLS-Fingerprinting-Werkzeug. Wenn Dinge bereitgestellt werden, die nicht dem Fingerprinting auf einem bestimmten Server entsprechen, werden sie als Anomalien auffallen. Es ist wichtig, nach Architekturen zu suchen, die in einer bestimmten Konfiguration sein sollten, es aber nicht sind.\n\nEine wichtige Frage ist, wie Censys verhindern kann, schlechten Spielern zu helfen. Zunächst einmal sehen die schlechten Spieler bereits das, was Censys sehen kann, vielleicht nicht in diesem Ausmaß, daher hilft Censys Organisationen dabei, das Spielfeld auszugleichen. Eine externe Sichtbarkeit in großem Maßstab kann dazu beitragen, die Schwachstellen zu beseitigen, die von schlechten Spielern ausgenutzt werden können. Zum Beispiel hat Censys der Ukraine geholfen, Schwachstellen zu identifizieren, die der Gegner hätte nutzen können, um das Schlachtfeld zu gestalten. Censys verfügt außerdem über ein gutes Sicherheitsteam, das schwierige Entscheidungen darüber trifft, mit wem sie Geschäfte machen möchten und wer Zugang erhalten kann.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matt Lembright"],"link":"/episode-EDT92-de","image":"./episodes/edt-92/de/thumbnail.png","lang":"de","summary":"In dieser Episode spricht Darren mit Matt Lembright, Direktor für Bundesanwendungen bei Censys, darüber, wie Censys die Angriffsfläche für internetverbundene Geräte analysiert, um Organisationen dabei zu helfen, Schwachstellen zu beseitigen."},{"id":168,"type":"Episode","title":"Die vier Superkräfte der digitalen Transformation","tags":["cybersecurity","aiml","multicloud","comms"],"body":"\r\n\r\nIntel konzentriert sich auf ubiquitous computing, pervasive connectivity, cloud to the edge und artificial intelligence (AI). Elemente dieser vier Superkräfte sind obligatorisch in den Digitalisierungsstrategien von Unternehmen weltweit, daher gibt es eine große Synergie.\n\n## Ubiquitous Compute: Allgegenwärtige Computer\n\nUbiquitäres Computing ist ein Konzept des Softwareengineerings, bei dem Computing überall vorhanden ist. Es bezieht sich nicht nur auf intelligente persönliche Geräte wie Telefone, Uhren und Haushaltsgeräte oder Rechenzentrum-Infrastruktur und Cloud-basierte Apps, sondern es ist die Idee, dass Unternehmensinfrastruktur, die historisch gesehen hinter einer Firewall lag, nun im Alltag präsent ist. Die Unternehmensinfrastruktur steht Mitarbeitern auf dem Campus zur Verfügung und überall dort, wo sie ein Gerät haben. Noch wichtiger ist, dass die Infrastruktur bis zu den Kunden erweitert werden kann, um ihre Erfahrung zu verbessern.\n\nIntel prüft, welche Grundelemente durch das Ökosystem vorangetrieben werden sollten, um das Ziel zu erreichen, dass jeder Mensch auf der Erde Zugang zu einem Petabyte an Daten oder einer Petaflop an Rechenleistung innerhalb von weniger als einer Millisekunde Zugriffszeit hat. Intel arbeitet an den Gesetzen der Physik, der Wirtschaft und der Datensouveränität, um dies möglich zu machen. Dadurch erhalten Softwareunternehmen, Cloud-Unternehmen und alle weltweit tätigen Unternehmen einen Rahmen für Software, der ihren Kunden einen Mehrwert schaffen wird.\n\n## Durchgängige Konnektivität\n\nIn einem kürzlich geführten Interview sprach Glenn Kurowski, der Technische Leiter (CTO) bei CACI, über Konnektivität und wie er sie im Weltraum entfaltet sieht. Ein großartiges Beispiel dafür ist, als Elon Musk Starlink über die Ukraine flog, um sie vor dem Eintauchen in die Internet-Dunkelheit zu bewahren.\n\nMit dieser enormen Rechenleistung ist Konnektivität entscheidend. Es gibt Satellitenverbindungen, 5G und 4G. Dennoch stellt sich auch innerhalb des Internets, zwischen Staaten und Ländern, die Frage: Wie durchquert man Netzwerke und ermöglicht Rechen-Ressourcen, die nicht miteinander verbunden sind? Ohne Konnektivität sinkt der Wert der Rechenleistung schnell.\n\nUm diese vernetzten Daten zu schützen, arbeiten Intel und Teams von Hunderten von Technologieunternehmen zusammen und treiben gemeinsam Innovationen voran. Regierungen müssen auch die Gesetze, Regeln und Bedenken verstehen. Intel beispielsweise beschäftigt eine beträchtliche Anzahl von Mitarbeitern, die Gespräche mit Gesetzgebern führen, um Gesetze zum Schutz von Daten zu entwickeln. Intels Ökosystem ist umfangreich, und wenn Intel sagt, dass diese Sicherheit wichtig ist, wird dies von den Menschen beachtet.\n\nEstland ist ein gutes Vorbild für ein Land, das die Daten seiner Bürger schützt, aber auch diese Daten freisetzt und seinen Bürgern zu einem geringeren Preis mehr bietet. Dieses Modell wird sich natürlich weiterentwickeln, da Regierungen aktiv den Datenschutz gewährleisten und strategisch denken.\n\n## Wolke bis zum Rand\n\nAuf der Unternehmensseite erfolgt die Ausweitung der Fläche, über die Unternehmen über die Cloud bis zum Rand mit ihren Kunden interagieren können. Ein großartiges Beispiel hierfür ist der Omnichannel-Einzelhandel, bei dem ein Unternehmen den Kunden kennt, seine Trends und Bedürfnisse. Mit einer allgegenwärtigen Konnektivität von der Cloud bis zum Rand können sie zusätzliche Dienstleistungen empfehlen. Sie können dem Kunden sagen, wo sich etwas, nach dem er sucht, befindet, wenn er im Geschäft ankommt. Einzelhändler könnten ihre Netzwerke von einem Rechenzentrum in der Cloud oder einem eigenen Rechenzentrum aus bis zum Rand erweitern und ein Netzwerk erstellen, das sich durch das gesamte Geschäft erstreckt.\n\nNicht nur würde dies das Einkaufserlebnis verbessern, sondern die Geschäfte könnten auch Verluste reduzieren, indem sie Dinge erkennen, die nicht vorrätig sind, und den Ablauf innerhalb des Geschäfts überwachen. Sie könnten ihre Produkte an den günstigsten Orten platzieren und beispielsweise verderbliche Waren überwachen, um Maßnahmen zu ergreifen, um Produkte schnell zu bewegen, während sie auf ihrem Höhepunkt sind.\n\n## Künstliche Intelligenz\n\nKI ist eine Erweiterung der Datenanalyse und wird zwangsläufig wachsen. Täglich werden massenhaft Daten erstellt, und es übersteigt bereits jetzt die Fähigkeiten von Unternehmen, diese effektiv zu verarbeiten. Menschen betrachten nur weniger als fünf Prozent der generierten Daten.\n\nMit KI-Algorithmen ist es möglich, Muster in diesen Daten zu finden, um zum Beispiel Krebs zu heilen. Es könnte bereits vorhanden sein, aber die Informationen sind noch nicht an einem Punkt, an dem KI sie nutzen kann. Es wird einen neuen Markt um zentralisierte, zugängliche Datensätze geben müssen. Ein organisiertes Datenmaklerunternehmen könnte zentralisierte Daten über die Cloud für mehrere Unternehmen zugänglich machen und dabei die Datenschutzbestimmungen, wie beispielsweise die Patientenidentifikation, schützen.\n\n## Sicherheit von Daten\n\nCOVID hat schnell dazu geführt, dass ubiquitäres Computing für Mitarbeiter, die von zu Hause aus arbeiten, wichtig wurde. Das und die anschließende hybride Belegschaft haben Sicherheitslücken in der Branche offengelegt. Derzeit gibt es eine signifikante Zunahme der Mittel für Sicherheit, um die Probleme anzugehen und mit der Expansion der Großmächte Schritt zu halten. Besonders mit Edge-to-Cloud-Architekturen und ubiquitärem Computing hat sich die Angriffsfläche stark erweitert. Die Branche kann mithalten, erfordert jedoch enorme Anstrengungen und vorausschauendes Denken.\n\nIntel hat bedeutende Innovationen in diesem Bereich mit Software-Schutz-Erweiterungen, Sicherheitsfunktionen in Silizium und dem Ökosystem, um von diesen Dingen zu profitieren. Das Ökosystem kann neue Anwendungsfälle wie vertrauliche verteilte Analysen für Krebsforschung oder Multi-Domain-Analysen entwickeln, die über unklassifizierte, klassifizierte und streng geheime Daten hinweg bedeuten. Früher konnten diese Daten niemals vermischt werden. Jetzt können sie sicher gemischt werden und Probleme lösen, die wir zuvor nicht lösen konnten.\n\n## Intel Software\nIntel Software\n\nViele mögen überrascht sein, dass Intel mehr als 19.000 Softwareingenieure hat. Intel kann all diese Ingenieure vollständig beschäftigen, da sie auf drei Ebenen tätig sind: grundlegende Software; Sprachen, Frameworks, Tools und Bibliotheken; sowie Arbeiten auf Anwendungsebene.\n\nDer Großteil dieser Technologie auf Anwendungsebene wird in der Open-Source-Community weitergegeben, wo sie zugänglich, sicher und optimiert ist.\n\nGehe zu http://embracingdigital.org für Intel-Ressourcen im Zusammenhang mit den vier Superkräften.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Ernst"],"link":"/episode-EDT93-de","image":"./episodes/edt-93/de/thumbnail.jpg","lang":"de","summary":"In dieser Folge diskutiert Darren die vier Supergkräfte der digitalen Transformation mit Greg Ernst von Intel, Corporate VP of Sales, Americas. Allgegenwärtiges Rechnen, Durchdringende Konnektivität, Cloud bis Edge, Künstliche Intelligenz."},{"id":169,"type":"Episode","title":"die Wiedergeburt der privaten Cloud","tags":["hybridcloud","microcloud","privatecloud","compute","technology","cloud","vergeio"],"body":"\r\n\r\nGreg begann schon als Kind mit dem Coden auf dem Commodore 64 und hat nie aufgehört. Er hat an allem gearbeitet, von 3D-Spiel-Engines über Kommunikation und Datenbank-Engines bis hin zu Webservern und Verschlüsselung. Nachdem bei den Ereignissen am 11. September Probleme auftraten, entwickelte er interoperable Kommunikationssoftware und Hardwarelösungen für Ersthelfer. Zum Beispiel konnten die Polizei und die Feuerwehr nicht miteinander kommunizieren, weil jeder ein anderes System nutzte.\n\nNach einem erfolgreichen Ausstieg aus diesem Unternehmen gründete er ein neues Unternehmen, um eine vertikale Suchmaschine von Grund auf aufzubauen. Er merkte schnell, dass der schwierige Teil nicht die Suchmaschine oder Algorithmen waren, sondern Infrastrukturprobleme. Er stellte fest, dass zu viel Code hardwareabhängig war und eine Abstraktionsschicht fehlte. Dies war die Grundlage für das, was schließlich Yottabyte wurde - sein Unternehmen, das das Speicherproblem löste, indem es kostengünstige Laufwerke zusammenstellte und es als vereinfachte Plattform präsentierte, für die man Software schreiben konnte, ohne sich um die Details kümmern zu müssen. Seine Vision erweiterte sich dann um Bereiche wie Computing, Speicher und Netzwerke. Yottabyte wurde kürzlich zu Verge.io.\n\nDer Erfolg von Verge.io liegt darin, dass alles von Grund auf entwickelt wurde, auch die Gestaltung ihrer Programmiersprache und das Schreiben ihrer Datenbank-Engines, so dass die Software alles erledigen kann. Sie ist nicht an ein bestimmtes Stück Hardware gebunden. Das Ziel war Einfachheit: Es handelt sich nicht nur um eine Zusammenstellung von Teilen, um eine einzige private Cloud zu erstellen, sondern um ein komplettes virtualisiertes Rechenzentrum.\n\nDies gewährleistet Zuverlässigkeit und Sicherheit, da beispielsweise nahtlose Aktualisierungen, keine Abhängigkeit von spezifischer Hardware und weniger anfällige Eintrittspunkte vorhanden sind. Darüber hinaus sind die Zielbenutzer des Systems IT-Allrounder, keine SAN-Experten oder Programmierer.\n\nDie Verge.io-Plattform bietet ein ähnliches Erlebnis wie eine öffentliche Cloud: Einfachheit, Self-Service und Agilität, hat jedoch erhebliche Kosten und einen Nutzen durch Datenanziehungskraft. Sie zahlen für Dinge wie IOP oder Datenausgang aus einer öffentlichen Cloud. Viele kleine Preise addieren sich und Sie werden gegenüber dem Ökosystem verantwortlich. Im Gegensatz dazu können Sie Verge.io-Software beispielsweise auf Bare-Metal-Servern ausführen, um Kosten zu sparen und die Skalierbarkeit schnell anzupassen. In einer öffentlichen Cloud geben Sie auch viel Kontrolle auf. Mit einer privaten Cloud behalten Sie Ihre Daten näher an dem Ort, an dem sie generiert werden, und können effizient am Rand agieren.\n\nEin Teil des Grundes, warum Serviceanbieter Verge.io wählen, ist, dass sie die Arbeitslast ihrer Kunden verwalten können, ohne auf die Kostenmarge zu verzichten und die Komplikationen bei der Übertragung in die öffentliche Cloud zu reduzieren.\n\nDie Bedürfnisse anderer Kunden werden erfüllt, weil sie Dinge erreichen können, die mit anderen Softwaren sehr schwierig sind. Zum Beispiel hat die University of Michigan eine erheblich verbesserte Effizienz und Benutzerfreundlichkeit festgestellt. Sie hat Tausende von Forschern, und wenn sie Forschungsgelder erhalten, benötigen sie eine Umgebung, die HIPAA- oder CUI-konform ist. Früher dauerte es sechs bis neun Monate, um das erforderliche Equipment bereitzustellen, zu installieren und zertifizieren zu lassen. Verge.io hat eine Umgebung geschaffen, und das Hochfahren ist so einfach wie ein Knopfdruck. Dem Benutzer wird eine virtuelle Enklave bereitgestellt, die vollständig konform und sehr sicher ist, da die Enklave eingekapselt und isoliert ist.\n\nEin weiterer Vorteil besteht darin, dass Verge.io über ein integriertes Business Continuity- und Disaster Recovery-System verfügt. Sie können den Snapshot auswählen und auf eine vollkommen andere Hardware-Architektur übertragen, und er wird genauso funktionieren. Die Art und Weise, wie das Snapshotting funktioniert, ermöglicht eine Klonkopie, selbst wenn es sich um zehn Petabyte handelt, und das in weniger als 30 Millisekunden.\n\nDa das Rechenzentrum nun problemlos in die Cloud oder ein Colocation-Zentrum migrieren kann, haben Geschäftsinhaber deutlich mehr Flexibilität bei der Verhandlung von Hardwarepreisen und -leistung. Darüber hinaus entstehen keinerlei Ausfallzeiten für Hardware-Upgrades oder -Aktualisierungen. Das System wird niemals heruntergefahren.\n\nEine der Bereiche, in die sich Verge.io erweitert, besteht darin, eine Multi-Cloud-Aggregationssoftware mit Vorteilen wie einer zentralisierten Verwaltungsoberfläche aufzubauen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Campbell"],"link":"/episode-EDT94-de","image":"./episodes/edt-94/de/thumbnail.png","lang":"de","summary":"Darren Pulsipher, Chief Solutions Architect bei Intel, diskutiert mit Greg Campbell, CTO von Verge.io, über die softwaredefinierten Rechenzentren von Verge.io, die IT vereinfachen und private Clouds einfach und effizient gestalten."},{"id":170,"type":"Episode","title":"Anforderungen für Edge-to-Cloud-Architekturen","tags":["data","edge","compute","cybersecurity"],"body":"\r\n\r\nAnna hat kürzlich eine Initiative zur Erstellung eines Whitepapers über Edge-to-Cloud-Service-Architekturen geleitet. Edge-to-Cloud umfasst alle wichtigen technologischen Aspekte, die Intel wichtig sind: Künstliche Intelligenz, Edge-Computing, Cloud und Netzwerkverbindungen. Das Ziel war es, Experten auf diesen Gebieten zusammenzubringen, um zu diskutieren, wie Intel Edge-Cloud-Architekturen angeht und wie diese Architekturen mit der Cloud verbunden werden können. Der Fokus lag auf den wesentlichen Elementen im Gegensatz zur aktuellen Technologie, die die Probleme anspricht. Dies würde einen Rahmen bieten, um über Diskrepanzen zu sprechen und nach ihnen zu suchen. Eine interessante Erkenntnis war, dass IT-, OT- und Netzwerkexperten unterschiedliche technische Sprachen mit unterschiedlichen Taxonomien sprechen, neben anderen Herausforderungen.\n\nEine Lektion aus diesen Diskussionen war, dass die Kommunikation von entscheidender Bedeutung ist und ein solides Wissen über Ihre Kommunikation vorhanden sein muss, insbesondere am Rand. Dies bestimmt, wie viel Rechenleistung lokal erfolgen muss und wie oft Sie auf die Cloud angewiesen sein können. Im öffentlichen Sektor muss die zusätzliche Komplexität der Anwendungsfälle die Funktionalität auch unter Bedingungen wie Verweigerung, Störung, Unterbrechung und begrenzten (DDIL) Ressourcen aufrechterhalten. Wir müssen diese Architekturen der Cloud-Konnektivität basierend auf diesen Einschränkungen neu durchdenken.\n\nKommunikation und Konnektivität sind der signifikanteste Unterschied zwischen einer Cloud-Architektur, einer Service-Infrastruktur und dem Edge. Viele aktuelle Tools nehmen fälschlicherweise eine konstante Konnektivität an; wenn etwas nicht verbunden ist, ist es tot. Das trifft jedoch nicht auf den Edge zu. Das wird deutlich in Edge-Organisationen wie dem Militär. Aber selbst in Branchen wie der Telemedizin muss man davon ausgehen, dass es keine gute Konnektivität gibt, zum Beispiel bei einem Anwendungsfall der Telemedizin, bei dem man von dem WLAN des Patienten zu Hause abhängig sein könnte. Der industrielle Sektor hat ähnliche Anforderungen. Manche Dinge können nicht offline sein, aufgrund kritischer Steuerungen für bestimmte Maschinen oder Prozesse.\n\nAlso, wie können Sie eine auf Edge-basierte Berechnung haben, die alle wichtigen Funktionen beibehält und dennoch in einer quasi unterbrochenen Verbindung mit der Cloud steht? Es gibt Architekturen dafür, aber es ist noch viel mehr zu tun, um einen nahtlosen Betrieb zu ermöglichen, wenn die Verbindung nicht kontinuierlich ist. Was können Sie weiterhin tun, und was passiert, wenn alles wiederhergestellt ist? Es wird eine Unterbrechung geben in Bezug auf das, was mit den Daten geschehen ist. Es wird komplex, wenn Sie all diese Daten im großen Maßstab synchronisieren müssen, vielleicht mit Tausenden von Edge-Geräten.\n\n## Sicherheit\n\nSicherheit am Rande ist ein weiteres Gebiet, in dem immer mehr Arbeit zu erledigen ist. Traditionelle Sicherheitsmaßnahmen wie die Authentifizierung sind nach wie vor von entscheidender Bedeutung, aber Geräte bilden eine riesige Angriffsfläche und ihre physische Sicherheit ist eine andere Angelegenheit. Die Sicherheit von Laptops neigt dazu, solide zu sein, und diese Maßnahmen müssen auf andere Randgeräte angewendet werden. Neue KI-Entwicklungen werden helfen festzustellen, ob sich die Geräte an dem Ort befinden sollten, an dem sie sein sollten, und Anomalien in einer von, sagen wir, zehntausend Geräten erkennen.\n\n## Anwendungsentwicklung\n\nBeim Erstellen von Anwendungen müssen Entwickler die einzigartige Edge-Umgebung verstehen und sie entwickeln, ohne dass eine Neuprogrammierung oder Integration neuer Middleware erforderlich ist, um am Edge ausgeführt zu werden. Anwendungen müssen in der Lage sein, mit Rechen-, Energie- und Konnektivitätseinschränkungen auszuführen. Darüber hinaus kann sich der Edge in einer neuen verteilten Rechenkonfiguration befinden und die Anwendung muss so konzipiert sein, dass sie in einem Mesh-Netzwerk mit hochgradig verteilter Rechenleistung ausgeführt werden kann. Das Entkoppeln der Anwendung von der Hardware ist eine bedeutende Veränderung und wird immer allgemeingültiger und weniger zweckgebunden. Dennoch geht der gesamte Dialog darauf hin, Daten von überallher zu erhalten und überall zu verwenden.\n\n## Datenmanagement\n\nDas Volumen an generierten und gesammelten Daten am Rand ist so groß, dass es keinen Sinn ergibt, all diese Daten an ein Rechenzentrum zu senden, um sie zu verarbeiten. Einer der Gründe ist der Kostenfaktor. In den Vereinigten Staaten kann ein privates 5G-Netzwerk kosteneffektiv für solch enorme Datenmengen sein, aber die Kosten wären ohne private 5G-Netzwerke für die meisten Organisationen prohibitiv.\n\nDas andere Problem ist, dass die meisten Daten nutzlos sind. Wenn man zum Beispiel Geräte oder Anwendungen überwacht, zeigen die meisten Daten an, dass alles korrekt funktioniert, obwohl man sich nur für Ereignisse interessiert, die darauf hindeuten, dass etwas nicht in Ordnung ist. KI-Algorithmen werden am Rand angewendet, um die Menge an irrelevanten Daten zu verringern, die zur Verarbeitung in die Cloud zurückgeschickt werden.\n\nDer traditionelle Betriebsmodus für das Datenmanagement, bei dem alles ins Rechenzentrum kopiert und dort analysiert wird, funktioniert am Rand (der Infrastruktur) nicht gut. Das Verschieben von Anwendungen an den Rand funktioniert auch nicht immer. Intel hat einige andere Datenarchitekturen oder Datenoperationen identifiziert. Eine davon heißt Datenaustausch, bei dem Daten erst nach der Analyse am Rand in sicheren Enklaven bewegt werden, ähnlich wie bei der Stapelverarbeitung. Die andere nennt sich intelligente Datenströme, bei denen SADE und SABR ins Spiel kommen. Daten werden nur entsprechend der Regeln verschoben und gestreamt. Dies funktioniert in DDIL-Umgebungen, da es die aktuellen Betriebsumgebungen ermitteln und anpassen kann.\n\n## Verwaltbarkeit\n\nSysteme müssen so entworfen werden, dass sie gewartet werden können. Es ist nicht möglich, zehntausend Geräte bereitzustellen und dann regelmäßig eine kleine Armee von Mitarbeitern zu entsenden, um diese zu überprüfen. Die IT war traditionell vorsichtig, ein funktionierendes System nicht aktualisieren zu wollen. Dennoch ergibt es keinen Sinn, Systeme unbeaufsichtigt zu lassen, insbesondere angesichts der Angst vor Ransomware-Angriffen auf OT-Netzwerke. Systeme müssen so strukturiert sein, dass alles leicht aktualisiert werden kann, um die Widerstandsfähigkeit gegenüber der Sicherheitsumgebung zu gewährleisten.\n\n## Verfügbarkeit\n\nInsbesondere in kritischen Bereichen wie dem Militär oder dem Gesundheitswesen ist es wichtig, Systeme mit ausreichender Redundanz zu entwerfen. Es handelt sich eher um einen systemischen Ansatz. Wenn einzelne Komponenten ausfallen, muss das Endziel dennoch erreicht werden. Das ist sehr unterschiedlich von dem, was in der Cloud passiert, wo es darum geht, die Infrastruktur aufrechtzuerhalten.\n\nDie Technologie ist noch nicht ganz soweit, aber es steht auf dem Radar, für mehrere Netzwerke zu entwerfen. Wenn du zum Beispiel vorzugsweise WiFi 6 verwendest und es fehlschlägt, kann das System auf 4G oder ein anderes verfügbares Netzwerk zurückgreifen. Das Netzwerk muss ebenso solide sein wie die Berechnungen. Ein eigenständiger Betrieb ohne die Netzwerkseite ist anfällig. Wenn du dich für eine kabelgebundene Lösung entscheidest, erhältst du zwar höhere Begrenzungen, verlierst jedoch deine mobilen Anwendungen.\n\nSie finden das Whitepaper \"Essential Requirements for Edge to Cloud Service Architectures\" für weitere Informationen unter embracingdigital.org oder intel.org.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Anna Scott"],"link":"/episode-EDT95-de","image":"./episodes/edt-95/de/thumbnail.png","lang":"de","summary":"Intel's Darren Pulsipher, Chief Solutions Architect, und Dr. Anna Scott, Chief Edge Architect, Public Sector, diskutieren wesentliche Anforderungen für Edge-to-Cloud-Service-Architekturen."},{"id":171,"type":"Episode","title":"Die Nutzung virtueller Rechenzentren mit Verge.io.","tags":["privatecloud","cloud","compute","technology","sdi","virtualdatacenter","vergeio"],"body":"\r\n\r\nAaron ist seit über 20 Jahren im IT-Bereich tätig. Er ist ein Vertriebsingenieur und spricht mit Kunden über den Wert und die Funktionen von Verge.io und hilft ihnen dabei, ihre Anforderungen zu definieren.\n\nChris ist der Vertriebsleiter bei Verge.io und verfügt über 25 Jahre Erfahrung in Technologie-Softwareunternehmen. Er beschreibt Verge.io als Unternehmen, das einen obsessiven Fokus auf Kundenzufriedenheit und Erfolg mit der Software hat.\n\nDer Begriff \"virtuelles Rechenzentrum\" dient der Vereinfachung. Die Plattform ist mandantenfähig, sodass eigenständige verschachtelte Mandanten mit allen erforderlichen Ressourcen innerhalb eines Rechenzentrums isoliert sind, von der CPU über den Speicher bis hin zur Netzwerkkonnektivität. Gleichzeitig können Sie diese bei Bedarf erstellen. Genau wie Sie eine virtuelle Maschine (VM) bei Bedarf erstellen können, oft mit einer Vorlage, können Sie dasselbe mit einem Mandanten tun, der alles im virtuellen Rechenzentrum umfasst. Sie können sie entweder über einen Rezept-Engine oder von Grund auf erstellen. Sie können problemlos eine Kopie erstellen oder einen Snapshot von einem dieser Mandanten aufnehmen.\n\nDiese virtuellen Rechenzentren können groß und komplex sein. Der größte Kunde von Verge.io betreibt über 60 Knoten und mehrere Mieter in ihrer Umgebung. Die einzigen Einschränkungen sind das, was im Backend-Verge.io-Cluster eingebaut ist. Angenommen, Sie haben zum Beispiel vier Terabyte Arbeitsspeicher, 100 Terabyte Speicher und 64 Kerne in Ihrer CPU. Sie könnten all diese Ressourcen diesem Mieter zuweisen, sie jedoch aufteilen, wie Sie möchten, zur Hälfte oder in Drittel und die Mieter auf diese Weise erstellen.\n\nWenn Sie zum Beispiel einige Knotenpunkte der Entwicklung, einige der Tests und den Rest der Produktion zugeordnet haben, wären dies verschiedene Mandanten. Sie können Schnappschüsse zwischen ihnen erstellen und in die Produktion verschieben. In diesem Fall können Sie sogar mehrere Produktionsumgebungen mit Blue-Green-Updates haben.\n\nViele Kunden von Verge.io sind verwaltete Dienstanbieter. Sie verwenden Multimandantenfähigkeit für ihre Endbenutzerkunden und stellen damit sichere Umgebungen mit Null-Vertrauen bereit. Kunden können ihre Cloud-Umgebung haben und virtuelle Arbeitslasten nach Bedarf bereitstellen.\n\nEs gibt auch bedeutende Anwendungsfälle für Unternehmenskunden im IT-Bereich, die beispielsweise blue-green Dev-Test-Umgebungen wünschen oder unterschiedliche Umgebungen mit verschiedenen Sicherheits-Compliance-Anforderungen wie SOX oder HIPAA haben können. Verge.io hat mehrere große Bildungseinrichtungen als Kunden, die sich mit konformen Forschungsaktivitäten beschäftigen. Sie zertifizieren ihren Cluster einmalig und können dann innerhalb einer Stunde eine konforme Forschungsumgebung an einen ihrer Forscher liefern. Früher dauerte es Monate, um eine konforme Umgebung einzurichten und betriebsbereit zu machen.\n\nFür Updates müssen Benutzer zwar Tools auf Anwendungsebene oder innerhalb von VM-Aufgaben verwenden, aber die Umgebungs-\"Rezepte\" für Dinge wie Firewall-Regeln oder Konfigurationseinstellungen zur Zuordnung von Ressourcen zu Arbeitslasten können nahtlos aktualisiert werden. Sie können auch eine VM in einer laufenden Umgebung mit neuer Konformität nehmen und in eine neue Umgebung verschieben.\n\nVerge.io ist auch hilfreich für Sicherheit. Eine der größten quantitativen Firmen in Europa ist ein gutes Beispiel für die Anwendung von Sicherheit. Sie machen ein Foto ihrer gesamten Umgebung und führen dann Red- und Blue-Team-Übungen durch, um Sicherheitslücken zu finden, Patches zu überprüfen, usw.\n\nVDI kann in der Umgebung ausgeführt werden. Verge.io arbeitet mit einem Unternehmen zusammen, um VDI-Unterstützung anzubieten. Verge.io kontrolliert die Ressourcen, die CPU und den Arbeitsspeicher. Sie unterstützen auch GPU und GPU-Passthrough sowie physische GPU. Dies ist ein bedeutender Anwendungsfall für einige Kunden, insbesondere in den Bereichen Ingenieurwesen oder Öl- und Gasbelastungen. Die virtuelle GPU ermöglicht günstige wirtschaftliche Bedingungen, da die Kosten auf mehrere Benutzer verteilt werden.\n\nEine perfekte Lösung für Verge.io sind Edge-Anwendungsfälle. Ein typisches Beispiel ist der Point of Sale. Wenn ein Einzelhandelskunde hundert Filialen hat, benötigt er möglicherweise zwei oder drei VM-Anwendungen pro Filiale. Da Verge.io eine geringe Hardware-Grundfläche hat, können Sie, sobald Sie mindestens zwei Server haben, diese in das Edge-Data-Center stellen und diese VMs erstellen. Dank der Snapshot- und Replikationsfunktionen können diese Konfigurationen in allen verschiedenen Umgebungen kopiert und eingefügt werden. Sie können die neuesten Konfigurationen in allen von ihnen aktualisieren, nicht nur Betriebssystem-Patches, sondern auch Firewall-Regeln.\n\nEine spannende Zukunftsperspektive für Verge.io liegt in automatisierten Fahrsystemen aufgrund des Datenvolumens. Viele Anbieter testen ihre Fahrzeuge an abgelegenen Standorten und verschicken physisch die Festplatten. Stell dir vor, wenn die Daten vor Ort verarbeitet werden könnten, vollständig redundant und mit den damit verbundenen überzeugenden Kosten, dann könnten die Daten in einem weiten Bereich transportiert werden, anstatt auf einer Festplatte und einem LKW.\n\nFür weitere Informationen zu Verge.io besuchen Sie http://verge.io.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Aaron Reid","Chris Lehman"],"link":"/episode-EDT96-de","image":"./episodes/edt-96/de/thumbnail.png","lang":"de","summary":"Intel's Darren Pulsipher, Chief Solutions Architect, und Aaron Reid, Principal Systems Engineer, sowie Chris Lehman, Senior VP of Sales, diskutieren Anwendungsfälle für die Virtual Data Center-Software von Verge.io."},{"id":172,"type":"Episode","title":"Die Vorteile von Graphdatenbanken","tags":["data","graphdb","katanagraph"],"body":"\r\n\r\nHadi erwarb seinen Doktortitel in Informatik im Jahr 2012 und erforschte Kryptografie und Netzwerksicherheit. Er arbeitete einige Jahre im akademischen Bereich und wechselte dann in die Industrie, wobei er sich auf verschiedene Aspekte von Sicherheitslösungen konzentrierte, einschließlich Identitäts- und Zugriffsverwaltung. Im Jahr 2015 begann er mehr über Grafikmodellierung zu lernen und erkannte, wie die Modellierung von Graphendaten einige der aufregenden und komplexen Probleme in seinem Forschungsbereich lösen könnte.\n\nIn Graphdatenbanken bedeutet ein Graph nicht Diagramme oder grafische Benutzeroberflächen, sondern eine Möglichkeit, Daten auf der Speicherebene zu strukturieren, damit sie für komplexe Probleme abgerufen und verarbeitet werden können, insbesondere wenn die Daten miteinander verbunden sind. Der Graph bietet viele Vorteile und kann bestehende Datenstrukturen oder Lösungen wie relationale Datenbankmodelle oder Objektspeicher ergänzen.\n\nDer Hauptunterschied zwischen graphen- und relationalen Datenbanken besteht darin, dass beide auf Beziehungen basieren, jedoch nehmen relationale Datenbanken die Beziehungen auf die Metadaten- und Schemebene, während graphenbasierte Datenbanken datengetriebene Beziehungen darstellen. Mit anderen Worten, in relationalen Datenbanken werden Spalten von Tabellen miteinander in Beziehung gesetzt. Um eine neue Beziehung einzuführen, müssen Sie Ihr Schema ändern. Der Graph bietet eine schema-freie Infrastruktur, in der Sie weitere Strukturen um Ihre Daten herum hinzufügen können, jedoch flexibel genug sind, um beliebige unstrukturierte Daten aufzunehmen.\n\nDie Hälfte der weltweiten Daten wurde in den letzten Jahren aus verschiedenen Quellen gesammelt, aber weniger als zwei Prozent wurden analysiert, von denen die meisten strukturierten Daten sind. Die Daten werden gesammelt, aber die Informationen sind unzureichend für die Verarbeitung. Es muss eine Möglichkeit geben, flexibel eine gewisse Struktur hinzuzufügen, die dynamisch genug ist, um sich zu ändern, wenn man unsicher ist, aber dennoch von der fortgeschrittenen optimierten Berechnung profitieren kann. Das Diagramm ist eine ausgezeichnete Möglichkeit, dies zu tun.\n\nWenn Sie versuchen, mit korrelierten oder miteinander verbundenen Daten zu arbeiten, im Gegensatz zu beispielsweise isolierten Daten mit kritischen Werten, bietet ein Diagramm Vorteile aufgrund der Beziehungen. Fast jede Branche kann davon profitieren, da unstrukturierte Daten normalerweise aus verschiedenen Quellen und unterschiedlicher Natur stammen.\n\nEin Beispiel wäre Cybersicherheitslösungen. Es gibt Daten aus Protokollen und Prüfungspfaden aus Netzwerkumgebungen, der Cloud-Infrastruktur, den Endpunktgeräten usw. Die Daten stammen aus verschiedenen Quellen wie Verzeichnissen oder Rohprotokolldateien. Es wäre vorteilhaft, die Daten zu korrelieren, zum Beispiel, weil üblicherweise eine Identität oder ein Benutzer, der Teil eines Protokolls aus dem Identitätsverwaltungssystem sein könnte, derselbe Benutzer ist, der einen Prozess auf einem Laptop auslöst, wie das Herunterladen eines Anhangs aus einer E-Mail. Durch die Analyse dieser Muster kann man diese korrelative Verknüpfung nutzen, um mehr Einblick zu gewinnen. Mit anderen Worten, es spielt keine Rolle, wie oder woher die Daten stammen, sondern durch die Bereitstellung dieser Verknüpfung erfährt man mehr über jeden Datensatz, indem man sie im Kontext betrachtet.\n\nEin Vorteil besteht darin, dass es keine Probleme mit der Datenumwandlung gibt, was die Geschwindigkeit erhöht. Dies verringert auch den Speicherbedarf.\n\nGraphen und relationale Datenbankstrukturen sind im Allgemeinen kompatibel. Hier ist ein vereinfachtes Beispiel eines sozialen Netzwerks: Die Knotentypen im Graphen ähneln Tabellen. So kann es einen Knoten namens \"Person\" geben und dann eine Tabelle namens \"Person\". Sie könnten einen anderen Knoten namens \"Ort\" haben und die Person mit einem bestimmten Ort verbinden. Sie können eine Tabelle namens \"Ort\" haben und sie mit Fremdschlüsseln verbinden. Dann haben Sie diese Beziehung von Freunden. Ein Freund einer Person in einem Graphmodell ist einfach eine Selbstschleife. Dadurch können Sie dieses Schema modellieren. In einer relationalen Datenbank müssten Sie eine neue Tabelle namens \"Freunde\" erstellen und sie dann verbinden. Auf dieser Ebene der Schemaerstellung fügen Sie also Redundanz und etwas Struktur hinzu. Und wenn Sie ein neues Konzept von Freundschaft oder Beziehung hinzufügen müssen, müssen Sie neue Tabellen erstellen und dadurch Redundanz und Komplexität aufbauen.\n\nWeitere Vorteile von Graphen gegenüber relationalen Datenbanken sind Graphen über Elemente, graphische KI und die Idee, jetzt Modelldaten zu finden, um Muster aufgrund der Verbindung der Daten zu erkennen. Aufgrund der Beziehungen können Sie den Datensatz, den Sie suchen oder analysieren, verringern. Es nutzt die Kraft der Daten, um die Daten noch weiter zu stärken. Die Algorithmen in einer Graphdatenbank unterscheiden sich stark von denen in relationalen Datenbanken und sind besser optimiert, um schneller auf große Datensätze zuzugreifen.\n\nEiner der Nachteile einer Graphdatenbank besteht darin, dass sie schwer skalierbar ist. In einer relationalen Datenbank ist es zum Beispiel einfach, eine Tabelle zu teilen und auf zwei Server zu verteilen. Frühere Graphdatenbanken waren darauf ausgelegt, als einzelne Gesamtlösung zu dienen. Wenn Sie also das Skalieren erhöhen wollten, mussten Sie mehr Speicher und CPU hinzufügen.\n\nNun, wenn Sie mit Petabyte an Daten in Grafiken arbeiten möchten, möchten Sie sich nach Möglichkeit vertikal skalieren, beispielsweise mit Technologien wie Intel VMs. Gleichzeitig möchten Sie jedoch auch horizontal skalieren. Neue Technologien wie die Graphenplattform von Katana helfen, dieses Skalierungsproblem durch verteiltes Computing zu lösen. Sie können das Problem in Teile aufteilen oder aufteilen und jeder Teil kann an einem kleinen Teil des Graphen für eine endgültige Lösung arbeiten. Katana hat bewiesen, dass Sie 256 oder mehr Maschinen zur Datenverarbeitung verwenden können. Dadurch können Sie schnell Tens von Terabyte an Daten im Arbeitsspeicher haben.\n\nEine Graphdatenbank erfordert ein ähnliches Ökosystem wie eine relationale Datenbank. Graphen sind etwas dynamischer und flexibler. Wenn Sie auf eine Graphen-Analyseplattform wechseln möchten, die über bloße betriebliche Datenbanken hinausgeht, könnten Sie von anderen Funktionen wie Datenlagerung und Datensee profitieren. Speicherung und Berechnung wären getrennt, was bedeutet, dass Graphenverarbeitungstechnologien, die alles im Speicher durchführen, nicht auf Speicherungsdienste angewiesen sein müssen, sodass Sie einen anderen Speicherdienst verwenden können.\n\nKatana verwendet Objektspeicher und lädt dann bei Bedarf alles, was sie möchten, vom Graphen in den verteilten Speicher aller Maschinen. Die Daten kehren unveränderlich zum Speicher zurück, so dass Sie, sagen wir, den gesamten Cluster zerstören könnten, ohne etwas zu verlieren. Alle Daten sind bereits dort und werden gelagert. Relationale Datenbanken sind ein reiferes Gebiet, aber Graphdatenbanken werden im Ökosystem zunehmend besser unterstützt.\n\nBasierend auf Hadis acht Jahren Erfahrung im Umgang mit Interessengruppen und Kunden erkennen alle sofort den Nutzen von Graphdatenbanken. Die Einschränkungen könnten darin bestehen, dass sie nicht mit Skalierbarkeit oder Kosten mithalten können. Die Arbeit von Katana und anderen Unternehmen der Graphentechnologie besteht darin, Graphen zu einem handelsüblichen Werkzeug zu machen, das Kunden für verschiedene Aufgaben nutzen können, anstatt dass es in der Datenbank ein Luxusprodukt ist. Zum Beispiel bietet Katana Kunden graphenbasierte Identitäts- und umfangreiche Datenverwaltungslösungen an.\n\nGute Anwendungsfälle für Graphen wären die frühzeitige Entwicklung von grafischen Lösungen, die speziell für bestimmte Unternehmen wie LinkedIn und Facebook mit ihren sozialen Netzwerken konzipiert sind. Heutzutage liegt ein naheliegender Einsatz im E-Commerce für Empfehlungssysteme vor. Durch das Finden von Verbindungen zwischen Kunden, Konten, Einkäufen und anderen Verhaltensweisen können sofort bessere Empfehlungen für die Käufer gemacht werden, was mit Abfragen in relationalen Datenbanken nicht möglich ist.\n\nUm mehr über Katana zu erfahren oder wie man Hadi kontaktiert, gehe zu embracingdigital.org.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Madi Ahmadi"],"link":"/episode-EDT97-de","image":"./episodes/edt-97/de/thumbnail.jpg","lang":"de","summary":"Darren Pulsipher, Chief Solutions Architect, Intel, und Dr. Hadi Ahmadi, Director of Solutions Architecture, Katana Graph, diskutieren die Vorteile von Graphdatenbanken."},{"id":173,"type":"Episode","title":"Die Entstehung des globalen Datennetzwerks","tags":["dataarchitecture","datamanagement","data","technology","cloud","globaldatanetwork","macrometa","multicloud","datamesh"],"body":"\r\n\r\nChetan ist ein Ingenieur, der sich zum Operations- und Start-up-Spezialisten entwickelt hat (Macrometa ist sein viertes Start-up). Er sagt, dass er seit zwanzig Jahren an derselben Herausforderung arbeitet, nämlich dem Umgang mit verteilten Daten und der Reduzierung von Latenzzeiten.\n\nDaten befinden sich nicht mehr nur in einem Rechenzentrum, sondern überall: in der Cloud, am Rand und auf den Laptops der Menschen. Die effektive Verwaltung all dessen ist eine Herausforderung.\n\nVor ungefähr zehn Jahren sagte Marc Andreessen, dass Software die Welt verschlingt. Zu diesem Zeitpunkt hat die Software alles verschlungen und alle Arten von Beschränkungen und Barrieren in Chancen verwandelt. Mehrfadenverarbeitung ist eine der Barrieren, die mit der Cloud beseitigt wurden. Sie können Anwendungen erstellen, die gleichzeitig an verschiedenen Orten auf der Welt ausgeführt werden. Parallel dazu findet eine Bewegung von Entwicklern statt, um alles so einfach wie nötig für den durchschnittlichen Informatiker zu machen. Auf der einen Seite gibt es eine anspruchsvolle technologische Entwicklung und auf der anderen Seite eine Bewegung hin zur Einfachheit.\n\nArchitekturen wie Jamstack ermöglichen verteiltes Computing in großem Maßstab mit großer Einfachheit, aber es gibt immer noch eine große unerschlossene Grenze zu entdecken und einzufordern. Die umfangreiche Landnahme-Chance liegt jetzt am Rand. Verteiltes Datenmanagement und Edge sind zwei Seiten einer Medaille.\n\nEin großes Problem besteht darin, dass sich Teile der Softwareentwicklung zu einer Funktion als Dienst bewegen, die Daten ignoriert. Außerdem gibt es die Wahrnehmung, dass Daten allgegenwärtig sind, aber viele Edge-Geräte nicht immer verbunden sind. Es gibt keine Garantie, dass eine Anwendung Zugriff auf alle Daten hat. Netzwerke sind nicht mehr zentralisiert; die Idee der zustandslosen Microservices stammt aus der Cloud-Bewegung. Diese Zustandslosigkeit kann zu einer großen Hürde werden. Deshalb behandeln Architekturen wie Jamstack und serverlose Funktionen Daten eher als peripheres Problem als als Kernproblem.\n\nDer folgende Text wird ins Deutsche übersetzt: \n\nStatuslose Datenstrukturen sind einfach. Sie haben einen bestimmten Ort, an dem Sie Ihre Daten speichern und dann wieder zu einem statuslosen Zustand wechseln. Zustandsbehaftete Datenstrukturen erfordern robuste Infrastrukturen mit komplexeren Datenstrukturen, da sie die Anwendung unterstützen, während sie kontinuierlich den Status aktualisiert. Wenn wir in eine reale Streaming-Datenwelt übergehen, in der der Status kontinuierlich aus dem Ökosystem emittiert wird, wird die Infrastruktur komplex und schwer zu verwalten, da sie nicht durchdacht ist. Hier kommt Macrometa ins Spiel. Sie haben eine neue Plattform für diesen kontinuierlichen, Echtzeit-aktiven Status im Exabyte-Maßstab entwickelt.\n\nDer Umgang mit diesen Streaming-Daten in einem aktiven und dynamischen Zustand ist für viele Softwareentwickler eine bedeutende Veränderung. Seit die ersten Cloud-Infrastrukturen entstanden sind, dann Big-Data-Plattformen und schließlich Daten als Service, hat die Branche gelernt, historische Daten effizient zu erfassen, zu verarbeiten und zu analysieren. Aber jetzt befinden wir uns in einer Welt, in der Daten nicht mehr als Monolith existieren, sondern auf einem Spektrum liegen. Eine neu anerkannte Eigenschaft ist, dass Daten vergängliche Erkenntnisse und Wert haben. Einige Daten haben eine kurze Haltbarkeit. Die aktuellen Zeitskalen sind zu groß, um Daten effizient zu nutzen; wir brauchen Systeme, die innerhalb von 50 Millisekunden effizient kommunizieren und die kognitive Belastung für diejenigen reduzieren, die mit diesen Systemen interagieren.\n\nDie meisten Menschen verstehen Latenz falsch: Es ist nichts, was Freude bereitet, sondern das Fehlen davon macht einen unzufrieden. Zum Beispiel, wie lange wird jemand ein stockendes YouTube-Video oder eine langsame Pufferung bei Netflix tolerieren? Fünfzig Millisekunden sind für eine Maschine eine Ewigkeit. Eine Maschine kann in 50 Millisekunden eine Vielzahl von Dingen erledigen, daher wird Latenz besonders wichtig, insbesondere wenn man den vergänglichen Wert von Daten betrachtet.\n\nEin weiteres Problem besteht nun darin, dass aufgrund der Cloud, der Interkonnektivität und des globalen Systems Start-ups multinationale Unternehmen sind und Daten standortsensibel werden. Ein Teil dieser Daten unterliegt Regulierungen, einige enthalten personenbezogene Informationen (PII) und dürfen in bestimmten Rechtsordnungen nicht ausgeführt werden, usw. Ein ausgezeichnetes Beispiel für dieses Problem ist, dass die Europäer ihre Daten nicht außerhalb ihrer Grenzen sehen wollen, aber die meisten Cloud-Infrastrukturen und Anwendungen in den USA entwickelt werden.\n\nEin drittes Problem besteht darin, dass Daten an vielen Orten gespeichert sind; es gibt Grenzen zwischen Systemen, physisch und logisch. Daten können im Wesentlichen statisch und starr sein, daher benötigen wir eine Infrastruktur, die es ermöglicht, dass Daten in Echtzeit mit Konsistenz und Ordnungsgarantien verbunden und fließen können. Am wichtigsten ist, dass dadurch Austauschbarkeit geschaffen wird, um sie schnell auf vielfältige Weise nutzen zu können.\n\nEin zusätzliches Problem besteht darin, dass Daten viel Störungen aufweisen und es keinen Sinn macht, interkontinentale Entfernungen zu überbrücken und dabei Übertragungsgebühren zu bezahlen, um dann den Großteil der Daten abzuziehen. Daten verlieren an Wert, bis sie ihr Ziel erreichen. Es gibt auch eine hohe Aktualisierungsrate, sodass Systeme häufig mit veralteten Daten arbeiten.\n\nWir brauchen neue Lösungswege für diese Art von verteilten Datenproblemen. Chetan glaubt, dass die nächsten zehn Jahre diesem Bereich der Datenwissenschaften gehören werden.\n\nDie erste Generation von verteilten Datenlösungen verwendete operationale Transformation. Google Docs ist ein hervorragendes Beispiel dafür. Operationale Transformation erfordert jedoch eine Zentralisierung der Kontrolle, wodurch sie nicht gut skalierbar ist. Google hat jedoch einen Weg gefunden, um Skalierbarkeit zu erreichen, aber das lässt sich nicht auf den durchschnittlichen Entwickler verallgemeinern. Es gibt vielleicht nur fünf Unternehmen auf der Welt, die es in diesem Maßstab verstehen, und ein Großteil dieses Wissens ist in diesen Unternehmen und proprietärer Technologie gebunden.\n\nMacrometa arbeitet mit der Gemeinschaft und der Akademie zusammen, um zu versuchen, einen neuen Wissensschatz zu schaffen, der weitaus effizienter ist als diese zentralisierten Modelle auf eine vollständig verteilte Weise.\n\nDerzeit sind Infrastrukturen verfügbar, die großartig darin sind, historische Aufzeichnungssysteme zu lösen. Sie versuchen, sich hin zu Echtzeitdaten zu entwickeln, aber ihre Architekturen sind grundsätzlich nicht dafür ausgelegt. Diese neuen Probleme mit daten-, zeit- und ortsbezogener Empfindlichkeit, Aktivierungswert, Aktualisierungsraten, Datenanziehungskraft und Datenrauschen erfordern eine neue Art von Infrastruktur. Chetan nennt dies ein Interaktionssystem anstelle eines Aufzeichnungssystems, weil Interaktionssysteme Datenverbindungen sind, die nahe am Ursprungsort der Daten entstehen und verbraucht werden. Sie filtern und erweitern alle Daten in richtiger Weise und leiten sie an ihre beabsichtigten Empfänger weiter. Es ist eine Networking-Funktion.\n\nMacrometa hat Netzwerkprozessoren entwickelt, die die Daten bewegen - ein globales Datennetzwerk. Es handelt sich um ein serverloses API-System, bei dem Entwickler einfach APIs nutzen, um Echtzeit-aktive und betriebliche Datenprobleme zu lösen. Macrometa ist ein globales Datennetzwerk in der Topologie eines CDM, aber mit einer Datenplattform wie Snowflake, die reichhaltige Datenprimitive erzeugt, um mit Echtzeit-aktiven und betrieblichen Datenwerten umzugehen.\n\nSie können analytische Tools in das globale Datennetz integrieren und die Analysen dort einsetzen, wo die Daten generiert oder benötigt werden. Genau wie Amazon mit Edge-Architektur und Algorithmen die Einzelhandelsverteilung grundlegend verändert hat, um lokale Lager optimal für Übernacht-Lieferungen zu bestücken, hat auch Macrometa dasselbe für Daten getan. Sie bringen die Daten und die Berechnungen auf diesen Daten näher zusammen und ermöglichen es, dass dies in Millisekunden geschieht. Diese Fähigkeit, Echtzeit-Informationsschleifen zu erstellen, ist ein leistungsstarker Enabler. Zum Beispiel können kleine Einzelhändler die Lagerbestände ihrer lokalen Geschäfte für ihren E-Commerce nutzen, ohne sich dabei mit Amazon zu überschlagen.\n\nEin großartiger Anwendungsfall für die Macrometa-Plattform liegt im Bereich der Cybersicherheit. Einige Kunden entfernen ihre zentralisierten Datenmodelle, um von geringeren Latenzzeiten zu profitieren und Bedrohungen in Echtzeit zu blockieren.\n\nDas globale Datennetzwerk ist eine Transformations Schicht zwischen Ihren Datenquellen und Empfängern, mit den Verbrauchern und Herausgebern. Es besteht aus drei Technologiekomponenten. Die erste ist das globale Datenmesh, welches die Integrations schicht für Daten darstellt. Die zweite ist ein globales Rechen-Fabric, das es Ihnen ermöglicht, Daten und Geschäftslogik in Form von Funktionen und Containern global zu orchestrieren. Das dritte Element ist ein globales Datenschutz-Fabric: Wie man Daten sichert und den unterschiedlichen datenrechtlichen Bestimmungen und Vorschriften entspricht, die bestimmen, ob Ihre Daten übertragen oder gespeichert werden dürfen.\n\nDer globale Datennetzwerk ist eine Möglichkeit, Daten schnell und einfach aus verschiedenen Systemen über Grenzen hinweg zu integrieren, sei es physisch oder logisch. Alles wird integriert und fließt mit Konsistenz und garantierter Ordnung. Der größte Wert dieses Netzwerks besteht darin, dass es Daten austauschbar und nutzbar macht, indem es Ihnen ermöglicht, APIs schnell auf Daten zu setzen. Dies kann im Vergleich zu normalerweise mehreren Monaten in wenigen Stunden erfolgen. Das globale Datennetzwerk ist für Billionen von Ereignissen pro Sekunde ausgelegt, so dass es Daten in großem Umfang mit 90 Prozent geringeren Kosten als die Cloud bewegen kann.\n\nDer globale Rechenverbund bringt Geschäftslogik und Orchestrierung mit sich, um Ihre Verarbeitung näher an den Ort zu bringen, an dem Ihre Daten entstehen oder verbraucht werden. Dies ist das Anti-Cloud-Muster. Macrometa wird chirurgisch und dynamisch jene Microservices verschieben, die z.B. datenschutzrechtlichen Vorschriften entsprechen müssen, an die richtigen Ausführungsorte.\n\nDas letzte Stück ist der Datenschutz. Dies ist ein komplexes Problem und die Antworten, die wir heute haben, zum Beispiel das Öffnen eines separaten Silos für diese bestimmte Geo, um jedes Mal, wenn Sie eine Instanz in Ihrer App starten, den Anforderungen gerecht zu werden, sind nicht gut. Die Plattform von Macrometa verfügt über ein Datennetzwerk, das bereits in die Integration einbezieht und Ihre Daten über alle Grenzen hinweg fließen lässt, zusammen mit Berechnungsfunktionen und der Aufnahme von Daten ohne Begrenzungen. Jetzt kann es logische Grenzen schaffen und Daten an bestimmten Standorten schützen. Es können Präferenzen und Richtlinien festgelegt werden, wie Daten in einer Region leben und repliziert werden sollen, zum Beispiel ob sie anonymisiert werden sollen, wenn sie aus der Region kopiert werden.\n\nMacrometas Technologie ermöglicht Anwendungsfälle, die in der Cloud nicht möglich sind, da die Clouds zu weit entfernt oder zu langsam sind. Macrometa hat die Infrastruktur aufgebaut, um Echtzeitdatenprobleme zu lösen und daraus Chancen anstatt Herausforderungen zu machen. Für mehr Informationen über Macrometa besuchen Sie macrometa.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Chetan Venkatesh"],"link":"/episode-EDT98-de","image":"./episodes/edt-98/de/thumbnail.jpeg","lang":"de","summary":"In dieser Episode erinnert sich Darren mit Chetan Venkatesh, dem CEO von MacroMeta. Venkatesh verfügt über eine lange Geschichte im Datenmanagement, angefangen bei den Anfängen des Grid Computing, und hat MacroMeta gegründet, um sich mit dem Datenmanagement über die global verteilten Edge, Rechenzentren und Clouds auseinanderzusetzen."},{"id":174,"type":"Episode","title":"Präkog Cyber-Angriffspfad mit XM Cyber","tags":["aiml","cybersecurity","xmcyber","technology","process"],"body":"\r\n\r\nPaul ist seit Ende der 1990er Jahre im Sicherheitsbereich tätig und hat seine Karriere mit Projekten für das Ministerium für Innere Sicherheit und das Verteidigungsministerium begonnen. Im Jahr 2005 trat er Fishnet Security bei und war dort im Vertriebsingenieurwesen tätig. Seitdem ist er eng mit dem Vertriebsingenieurwesen verbunden. Er trat XM Cyber bei, um sich auf Einbruch- und Angriffssimulationen zu konzentrieren.\n\nAnstatt der traditionellen Cybersicherheit, die Erkennung, Behebung und Prävention umfasst, ist XM Cyber vorhersagend. Eine gute Beschreibung wäre, dass es eine prekognitive Simulation ist. XM Cyber erzeugt imaginative Vorfälle, um Ihnen Einblick zu geben, wie Ihre Tools damit umgehen könnten und wie Sie spezifische Engstellen beheben könnten. Die Idee ist, all dies vor einem Penetrationstest zu tun. Sie können heute Probleme beheben, die Auswirkungen morgen sehen und dann kontinuierlich Ihre Sicherheit steigern und verbessern.\n\nGoogle Maps ist eine gute Analogie dafür, wie XM Cyber funktioniert. Wenn du von einem Ort zum nächsten gelangen möchtest, wird dir Google Maps alle Möglichkeiten aufzeigen, wie du dorthin gelangen kannst - ob es nun vermeidbare Mautstellen gibt oder welche die direkteste Strecke ist, usw. XM Cyber macht dasselbe, aber mit einer Angriffssimulation. Nehmen wir als Beispiel an, du hast einen kompromittierten Benutzeraccount im aktiven Verzeichnis. In diesem Fall wird es dir die sechs Schritte zeigen, die du unternehmen kannst, um zu einem lokalen Domänencontroller zu gelangen und diesen kritischen Vermögenswert zu kompromittieren. Es wird auch alle verschiedenen Routen zwischen diesen beiden Punkten anzeigen.\n\nEin Anwendungsfall dafür ist, dass Sie einem Red Team mit dieser Information ermöglichen können, äußerst effizient zu sein, da sie nicht herumstochern und versuchen müssen, Entdeckungen zu machen. Es kann auch dem blauen Bereich helfen, indem es ihnen ermöglicht, Remediations an Engstellen zu priorisieren. Zum Beispiel, wenn es 400 Angriffspfade gibt, die alle zu verschiedenen Bereichen in Ihrer DMZ führen, aber alle 400 scheinen auf diese eine Komponente angewiesen zu sein, um diesen Pfad zu ermöglichen, dann können Sie dieses eine Problem beheben und alle 400 Pfade zerstören. Blaue Teams können diese Engstellen, die den Angreifer ermöglichen könnten, absichern.\n\nEin Bereich, den XM Cyber analysiert, ist das Identitätsmanagement, nicht nur im Rechenzentrum, sondern auch in der Cloud. Manchmal bestehen Angriffswege aus zehn Schritten, wobei neun Schritte sich durch die Identitätswelt bewegen. Zum Beispiel könnten Sie Berechtigungen für Ihr Administrator-Konto haben, und dann könnte dieses Administrator-Konto weitere Berechtigungen haben. Man muss möglicherweise sechs oder sieben Mal verschiedene Berechtigungen anpassen oder Passwörter zurücksetzen und Gruppenrichtlinien (GPOs) bereitstellen. Durch Nutzung des Active Directorys könnte man neun Schritte von einem Standardbenutzerkonto zum Domänenadministrator unternehmen.\n\nNeben der Identität betrachtet XM Cyber über hundert Entitäten wie Maschinen, S3-Buckets und SSH-Schlüssel. Diese verschiedenen Entitäten können kombiniert werden, um einen Angriffsweg zu erstellen. Manchmal wird es sehr komplex. Zum Beispiel könnte ein Angriff lokal beginnen, zu Azure gehen, Intune nutzen und dann zu einer anderen Maschine zurückkehren, um einen Übergang zu GCP zu ermöglichen. Sobald sich Angreifer in der GCP-Umgebung befinden, können sie das Vertrauen oder die Berechtigungen zwischen AWS und GCP nutzen, um AWS zu kompromittieren. XM Cyber betrachtet alle verschiedenen Arten von Entitäten in verschiedenen Umgebungen und verbindet sie, um diese Wege zu bewerten, wie jede Entität in Bezug auf das Risiko aller anderen ganzheitlich zusammenwirkt.\n\nEs gibt zwei Möglichkeiten, wie XM Cyber mit Kunden interagiert. Die erste ist eine hochrangige Erkundung, um die Umgebung zu bewerten, Schwachstellen aufzudecken und zu messen, wie ein Angreifer neue Schwachstellen aufdecken kann, um wichtige Vermögenswerte zu gefährden. Die zweite ist eine gezielte Bewertung eines bestimmten Szenarios, das den Kunden beunruhigt. Diese Zusammenarbeit umfasst nicht nur eine statische Analyse von Entitäten. Sie sind dynamisch, da sie den Datenverkehr und andere Muster betrachten.\n\nEin typischer gezielter Anwendungsfall besteht darin, zu bestimmen, ob die OT das kritische Vermögen oder der Einbruchspunkt ist. XM Cyber spielt Szenarien durch, wie zum Beispiel, ob eine Maschine in der Personalabteilung der Einbruchspunkt ist und ob dadurch ein Risiko für diesen PLZ besteht, der sich in der SCADA-Umgebung befindet und Druckschalter steuert, die die Stromversorgung für eine Stadtgemeinde abschalten könnten. Dies ist ein tatsächlicher Anwendungsfall, den XM Cyber simulieren kann. Diese Art von Informationen ist in einer Welt, in der OT nicht mehr isoliert, sondern mit Netzwerken verbunden ist, entscheidend.\n\nXM Cyber ist eine SaaS-Lösung anstelle von On-Prem, damit sie dynamisch bleiben und den besten Service liefern können. Es kann beängstigend sein, sich vorzustellen, dass etwas in der Cloud alle Ihre Angriffstechniken hat. Dennoch unternimmt XM Cyber viel Arbeit, um sicherzustellen, dass Daten vollständig isoliert sind, SOC 2-konform und anderweitig zertifiziert sind und es keine Mehrmandantenfähigkeit gibt. Sie sammeln auch keine sensiblen Informationen. Sensible Informationen werden gehasht und nur ein Teil wird in die Cloud gesendet. Sie müssen tatsächliche Daten nicht besitzen.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Paul Giorgi"],"link":"/episode-EDT99-de","image":"./episodes/edt-99/de/thumbnail.png","lang":"de","summary":"Intel’s Darren Pulsipher, Chief Solutions Architect, und Paul Giorgi, Direktor des Vertriebsingenieurwesens bei XM Cyber, diskutieren, wie die Technologie von XM Cyber Organisationen dabei helfen kann, Angriffspfade aufzudecken und das Risiko zu reduzieren."},{"id":175,"type":"News Brief","title":"2023-2-5","tags":["ai","compute","cybersecurity"],"body":"\n\n## Künstliche Intelligenz\n\nLaut Forbes könnte AI bis 2030 dem globalen Wirtschaftssystem einen Mehrwert von 13 Billionen US-Dollar, also 16% des aktuellen Welt-BSP, liefern.\n\nChatGPT ermöglicht es Betrügern, E-Mails so überzeugend zu gestalten, dass sie ohne Malware Geld von Opfern erhalten können. Einige befürchten, dass der mächtige Chatbot es Nicht-Programmierern noch leichter macht, Malware zu erstellen und zu Cyberkriminellen zu werden. ChatGPT Cybersecurity Social Engineering\n\nGoogle hat fast 400 Millionen US-Dollar in den künstlichen Intelligenz-Startup Anthropic investiert, der einen Konkurrenten zu OpenAIs ChatGPT testet. https://interestingengineering.com/innovation/google-invests-anthropic-battle-chatgpt\nGoogle wird eine Veranstaltung abhalten, bei der es zeigt, wie es \"die Kraft der KI nutzt, um die Suche von Menschen neu zu gestalten\". Eine 40-minütige Veranstaltung wird am 8. Februar um 8:30 Uhr ET auf YouTube übertragen.\n\nEin AI-Detektionswerkzeug, das nach von ChatGPT erstelltem Inhalt sucht, besagt, dass Macbeth von William Shakespeare von AI generiert wurde. Ich denke, ChatGPT wurde nicht im Alten Englisch trainiert.\nhttps://venturebeat.com/ai/chatgpt-detection-tool-thinks-macbeth-was-generated-by-ai-what-happens-now/ Ich denke, es wurde nicht auf altem Englisch (auch Shakespeare-Englisch) trainiert.\n\n## Allgegenwärtiges Rechnen\n\nLaut Statista steigen die weltweiten Public-Cloud-Einnahmen weiter, wobei das prognostizierte Wachstum von 26 % in diesem Jahr 525 Milliarden US-Dollar erreichen wird.\nSoftware as a Service wird voraussichtlich 253 Milliarden US-Dollar dieses Umsatzes ausmachen. Allerdings als das Wachstum der Wolke\ngeht weiter, spüren die Anbieter von Cloud-Diensten den Schmerz, da es in den letzten 12 Monaten mehrere Ausfälle gegeben hat.\nDies war ein solches Problem, dass mehrere Websites aufgetaucht sind, die die Verfügbarkeit öffentlicher Clouds überwachen und melden\nwie cloudharmony.com und tausendeyes.com.\n\n* https://cloudharmony.com/status\n* https://www.thousandeyes.com/outages/\n* https://www.statista.com/outlook/tmo/public-cloud/worldwide\n\nOracle und Red Hat haben sich zusammengetan, um RHEL-basierte VM-Instanzen in der Oracle Cloud bereitzustellen, die auf Intel ausgeführt werden.\nAMD- und ARM-Prozessoren. Diese Zusammenarbeit zwischen ehemaligen Wettbewerbern bietet Kunden einen One-Stop-Shop für\nWorkloads, die die Produktsuite von Oracle und traditionelle Linux-basierte Workloads verwenden. Dieser Schritt setzt sich fort\nOracle in Konkurrenz zu den großen Hyper-Scalern wie Azure, AWS und Google.\n\n* https://www.networkworld.com/article/3686513/red-hat-enterprise-linux-arrives-in-oracle-s-cloud.html\n\n## Cybersicherheit\n\nDer E-Commerce in Südkorea und den USA wird von einer laufenden GuLoader-Malware-Kampagne, einer Cybersicherheitsfirma, angegriffen\nTrellix gab Ende letzten Monats bekannt. Die Cyber-Angreifer verlagern sich von Microsoft Word-Dokumenten zu NSIS (Nullsoft Scriptable Install\nSystem) ein Open-Source-Tool zum Schreiben von Installationsprogrammen auf dem Windows-Betriebssystem.\n[GuLoader-Malware mit bösartigem NSIS](https://thehackernews.com/2023/02/guloader-malware-using-malicious-nsis.html)\n\nProrussische Hacker nutzen eine neue DDoS-as-a-Service-Plattform namens Passion, um medizinische Einrichtungen in den USA anzugreifen.\nS., Portugal, Spanien, Deutschland, Polen, Finnland, Norwegen, die Niederlande und das Vereinigte Königreich. Diese Software als Service\nDas Angebot ist ein Abonnementdienst, der es Kunden ermöglicht, ihre gewünschten Angriffsvektoren, Dauer und Intensität auszuwählen.\n[Russische Cyberkriminelle starten neue Passion Attack-Plattform](https://cyware.com/news/russian-cybercriminals-launch-new-passion-attack-platform-798d8713)\n\nDie US-Behörde für Cybersicherheit und Infrastruktursicherheit (CISA) hat am Donnerstag die Veröffentlichung eines Leitfadens zur Stakeholder-spezifischen Schwachstellenkategorisierung (SSVC) angekündigt, der Organisationen dabei helfen kann, das Patchen von Schwachstellen mithilfe eines Entscheidungsbaummodells zu priorisieren.\nDas SSVC-System wurde 2019 von CISA und dem Software Engineering Institute (SEI) der Carnegie Mellon University entwickelt, und ein Jahr später entwickelte CISA einen eigenen angepassten SSVC-Entscheidungsbaum für Sicherheitslücken, die für Behörden und Organisationen mit kritischer Infrastruktur relevant sind.\n[CISA veröffentlicht Entscheidungsbaummodell](https://www.securityweek.com/cisa-releases-decision-tree-model-help-companies-prioritize-vulnerability-patching/) \n\n## Embracing Digital Transformation Podcast\n\nDiese Woche interviewt Darren Pulsipher chatGPT.\n\n[Weiterlesen](https://www.embracingdigital.org/episodes-EDT122)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW1-de","image":"./briefs/edw-1/de/thumbnail.png","lang":"de","summary":"For the week of February 6, 2023. News from around the world of digital transformation in artificial intelligence, cloud computing, and cybersecurity."},{"id":176,"type":"News Brief","title":"2023-4-9","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Künstliche Intelligenz\n\nNoch eine weitere Sache, über die Eltern besorgt sein müssen: Die Klonen von AI-Stimmen! Bei einer kürzlichen virtuellen Entführung durch einen böswilligen Cyber-Akteur wurde die Mutter eines 15-jährigen Mädchens angerufen und getäuscht, indem ihre Stimme mit der Stimme ihrer Tochter gefälscht wurde, um ihre Entführung anzukündigen. Gefolgt von \"Ich habe deine Tochter\". Mit schnellem Denken hat die Mutter ihrem Mann eine Nachricht geschickt, um den Standort ihrer Tochter zu überprüfen, die in Sicherheit war. Das FBI untersucht diese und mehrere andere Vorfälle.\n\n[https://www.azfamily.com/2023/04/10/ive-got-your-daughter-scottsdale-mom-warns-close-encounter-with-ai-voice-cloning-scam/](https://www.azfamily.com/2023/04/10/ive-got-your-daughter-scottsdale-mom-warns-close-encounter-with-ai-voice-cloning-scam/)\n\nEugenia Kuyda ist die Gründerin von Replika, einer US-Chatbot-App, die den Benutzern einen \"KI-Begleiter, der sich kümmert, immer hier zum Zuhören und Reden, immer auf Ihrer Seite\" bietet. Sie wurde 2017 gestartet und hat jetzt mehr als zwei Millionen aktive Benutzer. Jeder hat einen Chatbot oder \"Replika\", der einzigartig für ihn ist, da die KI aus ihren Gesprächen lernt. Benutzer können auch ihr eigenes Cartoon-Avatar für ihren Chatbot entwerfen.\n\n[https://www.bbc.com/news/business-65110680](https://www.bbc.com/news/business-65110680)\n\nNVIDIA und Getty Images arbeiten zusammen an generativer KI. Die Unternehmen zielen darauf ab, zwei generative KI-Modelle mithilfe von NVIDIA Picasso zu entwickeln, das Teil der neuen NVIDIA AI Foundations-Cloud-Services ist. Benutzer könnten die Modelle nutzen, um in Sekunden ein individuelles Bild oder Video zu erstellen, indem sie einfach ein Konzept eingeben. Das gemeinsame Ziel besteht darin, Text-zu-Bild- und Text-zu-Video-Grundlagenmodelle anzupassen, um atemberaubende visuelle Inhalte unter Verwendung vollständig lizenzierter visueller Inhalte zu erzeugen.\n\n[https://blogs.nvidia.com/blog/2023/03/21/generative-ai-getty-images/?ncid=so-link-748862&=&linkId=100000197541685#cid=gtcs23_so-link_en-us](https://blogs.nvidia.com/blog/2023/03/21/generative-ai-getty-images/?ncid=so-link-748862&=&linkId=100000197541685#cid=gtcs23_so-link_en-us)\n\n## Allgegenwärtiges Rechnen\n\nIst COBOL endlich gestorben? GFT und Cloud Frame schließen sich zusammen, um Organisationen dabei zu helfen, ihre COBOL-Programme auf effizientere Plattformen zu migrieren. COBOL wurde 1959 als eine der ersten von mehreren Anbietern unterstützten Sprachen für Mainframe-Business-Benutzer entwickelt. Es ist immer teurer geworden, COBOL-Systeme zu debuggen und zu warten, da es an Programmierern mangelt, die noch mit COBOL arbeiten.\n\n[https://www.cloudcomputing-news.net/news/2023/apr/04/gft-and-cloudframe-help-industries-say-cheerio-to-cobol/](https://www.cloudcomputing-news.net/news/2023/apr/04/gft-and-cloudframe-help-industries-say-cheerio-to-cobol/)\n\nGroße Sprachmodelle (LLMs) sind derzeit sehr beliebt, wobei ChatGPT den Weg vorgibt. Dies ist nur gut für Cloud-Service-Anbieter, da LLMs für Bereitstellung und Entwicklung große Mengen an Computing und Datenspeicher erfordern. Um nicht zurückgelassen zu werden, haben Cloud-Service-Anbieter Azure, AWS und Google alle ihre eigenen LMS-Lösungen angekündigt, um in diesem neuen Schlachtfeld für die Cloud-Dominanz zu konkurrieren. Lassen Sie uns ChatGPT fragen, wer der Gewinner sein wird.\n\n[https://www.infoworld.com/article/3693330/large-language-models-are-the-new-cloud-battleground.html](https://www.infoworld.com/article/3693330/large-language-models-are-the-new-cloud-battleground.html)\n\nGoogle hat seinen Hut in den Ring des vertraulichen Computings geworfen, indem es ein neues Angebot bereitstellt, das durch Verschlüsselung von Daten bei Ruhe, Transit und jetzt auch bei Verwendung vollständige Privatsphäre sensibler Daten bietet. Azure und AWS haben ähnliche Produktangebote, die vertrauliches Computing für Kunden im öffentlichen und privaten Sektor bieten.\n\n[https://www.wired.com/story/google-cloud-confidential-virtual-machines/](https://www.wired.com/story/google-cloud-confidential-virtual-machines/)\n\n## Cybersicherheit\n\nIn einem interessanten DDoS-Angriff haben Bedrohungsakteure npm, ein Open-Source-Paket-Repository für Node.js, überschwemmt, indem sie bösartige Websites erstellt und leere Pakete mit Links zu diesen bösartigen Websites veröffentlicht haben, um von der guten Reputation des Ökosystems bei Suchmaschinen zu profitieren. Über 1,42 Millionen falsche Pakete wurden hochgeladen.\n\n[https://thehackernews.com/2023/04/hackers-flood-npm-with-bogus-packages.html](https://thehackernews.com/2023/04/hackers-flood-npm-with-bogus-packages.html)\n\nÜber 1.000.000 WordPress-Sites werden geschätzt, von einer fortlaufenden Kampagne zur Bereitstellung von Balada Injector Malware infiziert zu sein. Die Angriffe spielen sich in Wellen ab, etwa alle paar Wochen. Die Angriffe leiten zufällige Subdomains auf verschiedene betrügerische Websites um, einschließlich Websites mit gefälschtem technischen Support. Der beste Weg, dem entgegenzuwirken, ist das Aktualisieren Ihrer Plugins auf Ihren WordPress-Websites.\n\n[https://thehackernews.com/2023/04/over-1-million-wordpress-sites-infected.html](https://thehackernews.com/2023/04/over-1-million-wordpress-sites-infected.html)\n\nDie iranische Staatsgruppe MuddyWater führt unter dem Deckmantel von Ransomware-Operationen zerstörerische Angriffe auf hybride Cloud-Umgebungen durch. Die Bedrohungsakteure geben sich als normale Ransomware-Kampagne aus, zerstören jedoch im Wesentlichen kritische IT-Operationen. Untersuchungen haben gezeigt, dass MuddyWater mit DEV-1084 zusammenarbeitet, um diese Angriffe durchzuführen.\n\n[https://thehackernews.com/2023/04/iran-based-hackers-caught-carrying-out.html](https://thehackernews.com/2023/04/iran-based-hackers-caught-carrying-out.html)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW10-de","image":"./briefs/edw-10/de/thumbnail.png","lang":"de","summary":"Zusammenfassung"},{"id":177,"type":"News Brief","title":"2023-4-16","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Cybersecurity auf Deutsch lautet: ## Cybersicherheit.\n\nDas FBI hat die Öffentlichkeit davor gewarnt, öffentliche Handy-Ladestationen aufgrund des Risikos von Cyber-Angriffen zu nutzen. Diese Ladestationen, die häufig an öffentlichen Orten wie Flughäfen, Einkaufszentren und Hotels zu finden sind, könnten potenziell Malware enthalten, die die Sicherheit von Benutzerdaten und Geräten gefährdet. Bringen Sie Ihren eigenen USB-Netzadapter mit und dann sollte alles in Ordnung sein.\n\n[https://www.cnbc.com/2023/04/10/fbi-says-you-shouldnt-use-public-phone-charging-stations.html](https://www.cnbc.com/2023/04/10/fbi-says-you-shouldnt-use-public-phone-charging-stations.html)\n\nGoogle hat ein dringendes Update für seinen Chrome-Webbrowser veröffentlicht, um eine kritische Schwachstelle zu beheben, die von Hackern ausgenutzt wird. Die Schwachstelle, identifiziert als \"Use-after-Free\"-Fehler, könnte böswilligen Akteuren das Ausführen beliebigen Codes und potenziell die Kontrolle über betroffene Systeme ermöglichen. Google hat Benutzer aufgefordert, ihre Chrome-Browser so schnell wie möglich auf die neueste Version zu aktualisieren, um das Risiko von Cyber-Angriffen zu minimieren.\n\n[https://thehackernews.com/2023/04/google-releases-urgent-chrome-update-to.html](https://thehackernews.com/2023/04/google-releases-urgent-chrome-update-to.html)\n\nDie Android- und Novi-Umfrage-Apps weisen schwerwiegende Sicherheitslücken auf, durch die potenziell persönliche Informationen der Nutzer offenbart werden könnten. Die Schwachstellen könnten es Angreifern ermöglichen, Sicherheitsmaßnahmen zu umgehen und unberechtigten Zugriff auf sensible Daten zu erlangen. Nutzer werden gebeten, vorsichtig zu sein und ihre Apps auf die neuesten Versionen zu aktualisieren, um sich vor potenziellen Cyberbedrohungen zu schützen.\n\n[https://thehackernews.com/2023/04/severe-android-and-novi-survey.html](https://thehackernews.com/2023/04/severe-android-and-novi-survey.html)\n\n## Datenmanagement\n\nLaut dem Chief Strategy Officer von OneTrust stellen Datenschutz- und Ethiklücken eine existenzielle Bedrohung für Organisationen dar. Das Versäumnis, diese Lücken angemessen anzugehen, kann zu Rufschädigung, rechtlichen und finanziellen Verpflichtungen und Verlust des Kundenvertrauens führen. Organisationen müssen den Datenschutz und die Ethik priorisieren, um Risiken zu minimieren und ihre Unternehmen angesichts sich entwickelnder behördlicher Anforderungen und zunehmender öffentlicher Überwachung zu schützen.\n\n[https://deloitte.wsj.com/articles/data-privacy-ethics-gaps-an-existential-threat-says-onetrust-cso-01668542698](https://deloitte.wsj.com/articles/data-privacy-ethics-gaps-an-existential-threat-says-onetrust-cso-01668542698)\n\nNach einer gründlichen Überprüfung der jüngsten Berichte des Government Accountability Office der Vereinigten Staaten hat Andrew Kuoh, ein Principal bei Capgemini, drei Schlüsselmaßnahmen identifiziert, auf die Datenorganisationen im öffentlichen Sektor sich konzentrieren müssen. Die Förderung einer datengetriebenen Kultur, die Schaffung von Datenregelungsrahmen und die Nutzung moderner Technologien wie Cloud Computing und künstlicher Intelligenz (KI), um den Wert von Daten zu nutzen. (Menschen, Prozesse und Technologie klingen vertraut)\n\n[https://www.informationweek.com/government/3-actions-to-kickstart-data-ecosystems-in-the-public-sector](https://www.informationweek.com/government/3-actions-to-kickstart-data-ecosystems-in-the-public-sector)\n\nEin kürzlich von einem DBTA-Panelisten abgehaltenes Webinar diskutierte die Bedeutung, sich mit den sich entwickelnden Anforderungen von Datenbanken und Anwendungen in der Cloud auf dem Laufenden zu halten. Mit dem Fortschreiten des Cloud Computings müssen Organisationen ihre Datenbank- und Anwendungsstrategien anpassen, um Leistung, Sicherheit und Skalierbarkeit zu optimieren. Die Betonung von cloudbasierten Technologien, Automatisierung und bewährten Verfahren kann Unternehmen helfen, in der dynamischen Cloud-Umgebung von heute vorauszubleiben. \n\n(Note: This translation is generated by AI and may not be perfect.)\n\n[https://www.dbta.com/Editorial/News-Flashes/Keeping-Up-with-the-Evolving-Needs-of-Databases-and-Applications-in-the-Cloud-158166.aspx](https://www.dbta.com/Editorial/News-Flashes/Keeping-Up-with-the-Evolving-Needs-of-Databases-and-Applications-in-the-Cloud-158166.aspx)\n\n## Intelligent Edge\n\nIntelligente Kante\n\nDie Edge Computing Expo North America, geplant für den 17. bis 18. Mai 2023, hat kürzlich eine Star-Besetzung von Technologen und Geschäftsführern im Bereich IOT angekündigt. Die Konferenz wird im Santa Clara Convention Center in Kalifornien stattfinden. Du musst dir dieses Ereignis in deinem Kalender markieren.\n\n[https://www.iot-now.com/2023/04/14/129661-edge-computing-expo-north-america-announces-speaker-line-up-hear-from-leading-experts-in-edge-computing/](https://www.iot-now.com/2023/04/14/129661-edge-computing-expo-north-america-announces-speaker-line-up-hear-from-leading-experts-in-edge-computing/)\n\nDie Konvergenz der IT- und OT-Cybersicherheit wird Realität, da immer mehr Cybersicherheitslösungen aus dem IT-Bereich ihren Fokus auf den Schutz von IoT und industrieller Infrastruktur richten. Ein aktuelles Beispiel ist die Einführung von CloudStrike Falcon Insight for IoT durch CrowdStrike. Diese Plattform bietet dieselben Werkzeuge für IoT, IT-Endpunkte sowie Cloud- und Rechenzentrumslasten.\n\n[https://www.iot-now.com/2023/04/17/129671-crowdstrike-brings-xdr-for-iot-offering-to-deliver-protection-to-iot-assets/](https://www.iot-now.com/2023/04/17/129671-crowdstrike-brings-xdr-for-iot-offering-to-deliver-protection-to-iot-assets/)\n\nIn einem großartigen Beispiel für die Einführung von Industrie 4.0 hat die Volvo Group die Bereitstellung einer präventiven Wartungs-IT-Infrastruktur in ihrer Fabrik in Lyon, Frankreich, übernommen. Die Volvo Group nutzt ein Long-Range Wide Area Network (LoRaWAN), um eine Echtzeitüberwachung und Analyse von Daten aus Fabrikausrüstungen zu ermöglichen, was eine proaktive präventive Wartung ermöglicht.\n\n[https://www.edgecomputing-news.com/2023/04/03/volvo-group-uses-lorawan-for-predictive-maintenance-in-lyon-factory/](https://www.edgecomputing-news.com/2023/04/03/volvo-group-uses-lorawan-for-predictive-maintenance-in-lyon-factory/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW11-de","image":"./briefs/edw-11/de/thumbnail.png","lang":"de","summary":"Zusammenfassung"},{"id":178,"type":"News Brief","title":"2023-4-23","tags":["ai","edge","cybersecurity"],"body":"\n\n\n## Künstliche Intelligenz\n\nLaut MarketWatch wird die KI-Hardware-Industrie, die im Jahr 2021 eine Größe von 10 Milliarden Dollar hatte und mit einer erwarteten annualisierten Wachstumsrate von fast 27% wächst, bis zum Jahr 2030 voraussichtlich eine Industrie von 89 Milliarden US-Dollar sein.\n\n[https://www.marketwatch.com/story/nvidia-is-ai-hardwares-leader-now-but-intel-amd-and-others-are-closing-fast-8ad6f23f](https://www.marketwatch.com/story/nvidia-is-ai-hardwares-leader-now-but-intel-amd-and-others-are-closing-fast-8ad6f23f)\n\nDas Defense Advanced Research Projects Agency (DARPA) hat ein mehrere Millionen Dollar umfassendes GARD-Projekt (Guaranteeing AI Robustness Against Deception), das drei Schlüsselziele hat: Die Entwicklung von Algorithmen, die maschinelles Lernen vor Schwachstellen schützen; die Entwicklung von Theorien, um sicherzustellen, dass KI-Algorithmen gegen Angriffe verteidigungsfähig sind; und die breite Weitergabe der Werkzeuge.\n\n[https://www.zdnet.com/in-depth/innovation/these-experts-are-racing-to-protect-ai-from-hackers-time-is-running-out/](https://www.zdnet.com/in-depth/innovation/these-experts-are-racing-to-protect-ai-from-hackers-time-is-running-out/)\n\nTom Brady drohte damit, die Komiker zu verklagen, die hinter dem AI-Standup-Video stehen. Unter Verwendung von Daten aus Interviews mit Tom Brady und Hunderttausenden von Stunden Stand-up-Comedy-Material simuliert die Dudsey-Zweigstelle eine einstündige Stand-up-Comedy-Show.\n\n[https://nypost.com/2023/04/20/tom-brady-threatened-to-sue-comedians-over-ai-standup-video/](https://nypost.com/2023/04/20/tom-brady-threatened-to-sue-comedians-over-ai-standup-video/)\n\n## Cybersicherheit\n\nEin neuer Bericht von CybelAngel hat die fünf wichtigsten Cybersicherheitsgefahren aufgedeckt, die für Organisationen kritische Risiken darstellen. Der Bericht identifiziert die Hauptbedrohungen durch Cloud-Konfigurationsfehler, Schwachstellen in der Lieferkette, Ransomware-Angriffe, Phishing-Betrügereien und nicht gepatchte Software. Der Bericht fordert Organisationen auf, diese Risiken zu minimieren und ihre Systeme und Daten proaktiv zu schützen.\n\n[https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures/](https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures/)\n\nDie europäische Flugsicherungsbehörde Eurocontrol hat bekannt gegeben, dass sie Anfang dieses Monats einen Cyberangriff aus Russland erlebt hat. Der Angriff richtete sich gegen die Systeme der Behörde, aber Eurocontrol konnte den Vorfall eindämmen und beheben, ohne dass es zu Störungen im Flugverkehr kam. Die Behörde hat andere Organisationen davor gewarnt, wachsam zu sein und ihre Abwehrkräfte gegen Cyberbedrohungen zu stärken.\n\n[https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures](https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures)\n\nIn INEAs industrieller Steuerungssoftware (ICS) wurde eine kritische Schwachstelle entdeckt, die es remote Angreifern ermöglichen könnte, betroffene Systeme zu übernehmen und Störungen oder Schäden in industriellen Betriebsabläufen zu verursachen. Der Fehler betrifft alle Produktversionen und wurde von Forschern bei Claroty entdeckt. Das Unternehmen hat Organisationen, die das Produkt nutzen, dringend empfohlen, so schnell wie möglich auf die neueste Version zu aktualisieren, um das Risiko einer Ausbeutung zu minimieren.\n\n[https://www.securityweek.com/critical-flaw-in-inea-ics-product-exposes-industrial-organizations-to-remote-attacks/](https://www.securityweek.com/critical-flaw-in-inea-ics-product-exposes-industrial-organizations-to-remote-attacks/)\n\n## Intelligenter Rand\n\nKneron, ein Anbieter von Edge-Artificial-Intelligence-Lösungen, hat Otus erworben, einen Hersteller von Imaging-Lösungen für autonome Fahrzeuge. Durch den Erwerb kann Kneron auf die Expertise von Otus bei der Entwicklung kompakter, energieeffizienter Kameras für Edge-AI-Anwendungen zurückgreifen. Der Erwerb soll die Einführung von Edge-AI in der Transportbranche beschleunigen.\n\n[https://www.edgecomputing-news.com/2023/04/20/kneron-buys-otus-for-edge-ai-imaging-in-autonomous-vehicles/](https://www.edgecomputing-news.com/2023/04/20/kneron-buys-otus-for-edge-ai-imaging-in-autonomous-vehicles/)\n\nSolo.io hat Gloo Fabric gestartet, eine sichere Multi-Cloud-Service-Mesh-Plattform für Unternehmensanwendungen, die Hybrid- und Multi-Cloud-Umgebungen unterstützt und über ein zentrales Dashboard zur Überwachung, Verwaltung und Fehlerbehebung von Anwendungen verfügt.\n\n[https://www.edgeir.com/gloo-fabric-by-solo-io-promises-secure-multi-cloud-discovery-and-connectivity-for-enterprises-20230421](https://www.edgeir.com/gloo-fabric-by-solo-io-promises-secure-multi-cloud-discovery-and-connectivity-for-enterprises-20230421)\n\nDie Regierung Kanadas und Ericsson werden über fünf Jahre hinweg CA$470 Mio. (376 Mio. USD) in 5G- und 6G-Forschung und -Entwicklung investieren, um eine nachhaltige und sichere Telekommunikationsinfrastruktur zu schaffen, neue Anwendungsfälle und -anwendungen wie Smart Cities und vernetzte Fahrzeuge zu entwickeln.\n\n[https://www.edgeir.com/government-of-canada-ericsson-announce-ca470-million-investment-for-5g-6g-rd-20230420](https://www.edgeir.com/government-of-canada-ericsson-announce-ca470-million-investment-for-5g-6g-rd-20230420)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW12-de","image":"./briefs/edw-12/de/thumbnail.png","lang":"de","summary":"Zusammenfassung"},{"id":179,"type":"News Brief","title":"2023-4-30","tags":["ai","cybersecurity","edge"],"body":"\n\n## Künstliche Intelligenz\n\nDer AI Gegenwind beginnt: Fans von Snapchat äußern ihren Unmut darüber, ChatGPT-betriebene Bots auf der Plattform zu verwenden, und markieren damit einen Aufschrei gegen AI. Benutzer kritisieren das Fehlen von menschenähnlichen Antworten und die Unfähigkeit, zwischen tatsächlichem und AI-generiertem Inhalt zu unterscheiden. Der Vorfall verdeutlicht wachsende Bedenken und die Notwendigkeit einer ausgewogenen Implementierung von AI in sozialen Medienplattformen.\n\n[https://www.techradar.com/news/the-ai-backlash-begins-snapchat-fans-revolt-against-chatgpt-powered-bot](https://www.techradar.com/news/the-ai-backlash-begins-snapchat-fans-revolt-against-chatgpt-powered-bot)\n\nKünstliche Intelligenz ausgestattete Brillen lesen non-verbale/stille Sprache. Forscher der Cornell University haben eine Schnittstelle zur Erkennung von stummer Sprache entwickelt, die akustische Sensoren und künstliche Intelligenz verwendet, um bis zu 31 nicht ausgesprochene Befehle kontinuierlich aufgrund von Lippen- und Mundbewegungen zu erkennen. Die tragbare Schnittstelle mit geringem Stromverbrauch - EchoSpeech - erfordert nur wenige Minuten Schulungsdaten des Benutzers, bevor sie Befehle erkennt und auf einem Smartphone ausgeführt wird. Ich schwöre, meine Großeltern konnten das vor 40 Jahren schon. Sie haben sich gut mit nur einem Kopfnicken, einem Lächeln und einem Augenzwinkern verständigt.\n\n[https://news.cornell.edu/stories/2023/04/ai-equipped-eyeglasses-can-read-silent-speech](https://news.cornell.edu/stories/2023/04/ai-equipped-eyeglasses-can-read-silent-speech)\n\nKönnen KI-Chatbots Chefs ersetzen? Ein Startup namens Aesthetic untersucht das Potenzial von KI-betriebenen Chatbots, um Manageraufgaben zu übernehmen, Anleitung zu geben, Fragen zu beantworten und Workflows zu verwalten. Obwohl sie effizient sind, argumentieren einige, dass menschliche Eigenschaften wie Empathie unersetzbar sind. Ist die Kombination aus menschlicher und KI-Interaktion der beste Ansatz für effektive Führung am Arbeitsplatz?\n\n[https://startup.outlookindia.com/sector/saas/can-your-boss-be-replaced-by-an-ai-chatbot--news-8257](https://startup.outlookindia.com/sector/saas/can-your-boss-be-replaced-by-an-ai-chatbot--news-8257)\n\n## Cybersecurity\n\nDie Cybersecurity and Infrastructure Security Agency (CISA) bittet um öffentliche Kommentare zu einer Entwurfsdirektive, die Bundesbehörden verpflichtet, ihre Software-Sicherheitspraktiken zu bescheinigen. Die Direktive zielt darauf ab, die Sicherheit der Software-Lieferketten zu verbessern, und der Kommentarzeitraum läuft bis zum 3. Mai 2023.\n\n[https://www.cybersecuritydive.com/news/cisa-public-comment-software-security-attestation/648932/](https://www.cybersecuritydive.com/news/cisa-public-comment-software-security-attestation/648932/)\n\nRansomware-Angriffe haben sich laut einem Bericht von CrowdStrike weiterentwickelt und setzen nun auf Zwangsmethoden, um Zahlungen von Opfern zu erpressen. Neben der Verschlüsselung von Daten beinhalten diese Taktiken die Androhung physischer Gewalt, das öffentliche Bekanntmachen gestohlener Informationen und sogar die gezielte Attacke auf die Familien der Opfer. Organisationen müssen wachsam bleiben und ihre Verteidigung anpassen, um diesen neuen Strategien entgegenzuwirken und den Einfluss von Ransomware-Angriffen zu mildern, um ihre wertvollen Daten vor Ausbeutung zu schützen.\n\n[https://cyware.com/news/coercion-in-the-age-of-ransomware-new-tactics-for-extorting-payments-0c31dba6](https://cyware.com/news/coercion-in-the-age-of-ransomware-new-tactics-for-extorting-payments-0c31dba6)\n\nDer Kältespeicheranbieter Americold hatte einen Netzwerk-Verstoß, der zu einem weitreichenden Ausfall von mehreren Systemen führte. Das Unternehmen hat den Vorfall bestätigt, jedoch nicht die Art des Verstoßes oder das Ausmaß der Auswirkungen offengelegt. Als Vorsichtsmaßnahme hat Americold vorübergehend bestimmte Systeme heruntergefahren und externe Cyber-Sicherheitsexperten damit beauftragt, den Vorfall zu untersuchen. Kundendaten sollen sicher sein und das Unternehmen arbeitet daran, die volle Funktionalität wiederherzustellen.\n\n[https://www.bleepingcomputer.com/news/security/cold-storage-giant-americold-outage-caused-by-network-breach/](https://www.bleepingcomputer.com/news/security/cold-storage-giant-americold-outage-caused-by-network-breach/)\n\n## Intelligent Edge (there is no direct translation, so it remains the same in German)\n\nAkamai Technologies hat NeoSec, ein Cybersecurity-Startup, das sich auf API-Sicherheit konzentriert, erworben, um seine Fähigkeiten zur Erkennung und Reaktion auf API-basierte Angriffe zu stärken. Die Übernahme wird das Sicherheitsangebot von Akamai verbessern und Kunden einen verbesserten Schutz vor Bedrohungen bieten, die auf Applikation-Programming-Interfaces abzielen. Akamai will die sich ständig weiterentwickelnde Cybersecurity-Landschaft durch die Erweiterung seines Sicherheitsportfolios und die Bereitstellung von verbesserten Lösungen zur Bekämpfung von API-bezogenen Risiken angehen.\n\n[https://www.edgecomputing-news.com/2023/04/24/akamai-acquires-neosec-to-bolster-api-detection-and-response/](https://www.edgecomputing-news.com/2023/04/24/akamai-acquires-neosec-to-bolster-api-detection-and-response/)\n\nZadara und Kasten von Veeam haben sich zusammengeschlossen, um eine umfassende Daten­schutz­lösung für Kubernetes-Umgebungen zu bieten. Durch die Kombination von Zadaras zCompute, zStorage und Kastens K10-Plattform bietet die Lösung Backup, Disaster Recovery und Anwendungs­mobilität. Es ermöglicht eine nahtlose Anwendungs­bewegung über Kubernetes-Cluster hinweg, einschließlich Zadaras Global Edge Cloud-Standorten. Die Zusammenarbeit adressiert den Bedarf an agilen, kostengünstigen Speicherlösungen, die cloudbasierte Anwendungen in Kubernetes schützen und verschiedene Datenbanken unterstützen. Zusätzlich hat Zadara kürzlich seine c9 Flex-N Infrastructure-as-a-Service-Plattform in Zusammenarbeit mit BroadBand Tower in Japan gestartet.\n\n[https://www.edgeir.com/zadara-kasten-by-veeam-unite-to-provide-multi-tier-data-protection-for-kubernetes-20230428](https://www.edgeir.com/zadara-kasten-by-veeam-unite-to-provide-multi-tier-data-protection-for-kubernetes-20230428)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW13-de","image":"./briefs/edw-13/de/thumbnail.png","lang":"de","summary":"News and stories from the Week of May 01, 2023, in Digital Transformation, including cyberattacks and intelligent edge, non-verbal communication AI, and company merges in the IoT space."},{"id":180,"type":"News Brief","title":"2023-5-7","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Datenmanagement\n\nDIQ, ein neues Tool für den Datenvertrauensindex, fördert die Demokratisierung von Daten. Das Tool misst die Vertrauenswürdigkeit von Datenquellen und ermöglicht es Organisationen, informierte Entscheidungen über die Nutzung von Daten zu treffen. Durch die Bereitstellung von Transparenz und Rechenschaftspflicht strebt DIQ danach, Unternehmen in ihren datengetriebenen Strategien zu stärken und das Vertrauen in datengesteuerte Entscheidungsfindung zu verbessern.\n\n[https://tdwi.org/articles/2023/05/04/diq-all-data-trust-index-tool-to-drive-data-democratization.aspx\n](https://tdwi.org/articles/2023/05/04/diq-all-data-trust-index-tool-to-drive-data-democratization.aspx\n)\n\nFIATA und das Global Shippers Forum setzen sich für robustere Daten-Governance-Standards in der Schifffahrtsindustrie ein. Sie betonen die Notwendigkeit, Herausforderungen in Bezug auf Datenqualität, Zugänglichkeit und Sicherheit anzugehen, um die Effizienz der Lieferketten zu verbessern. Die Etablierung von konsistenten Standards zielt darauf ab, die gemeinsame Nutzung von Daten, Zusammenarbeit und Entscheidungsfindung unter den Beteiligten zu verbessern und letztendlich der globalen Schifffahrtsgemeinschaft zugute zu kommen.\n\n[https://www.porttechnology.org/news/fiata-global-shippers-forum-push-for-stronger-data-governance-standards/\n](https://www.porttechnology.org/news/fiata-global-shippers-forum-push-for-stronger-data-governance-standards/\n)\n\nDatabricks Ventures hat in Immuta investiert; ein Unternehmen, das sich auf die Stärkung der Governance von Lakehouses konzentriert. Die Plattform von Immuta bietet Lösungen für Datenzugriff und -kontrolle, mit denen Organisationen ihre Data-Lakehouse-Umgebungen effektiv regieren und sichern können. Mit dieser Investition sollen die Daten-Governance-Fähigkeiten im Databricks-Ökosystem verbessert werden, um Datenschutz, Compliance und Sicherheit für Unternehmen zu fördern, die Lakehouse-Architekturen nutzen.\n\n[https://www.databricks.com/blog/strengthening-lakehouse-governance-ecosystem-databricks-ventures-invests-immuta\n](https://www.databricks.com/blog/strengthening-lakehouse-governance-ecosystem-databricks-ventures-invests-immuta\n)\n\n## Künstliche Intelligenz\n\nDer leitende Ingenieur hinter Googles fortschrittlichem Chatbot Geoffrey Hinton ist wegen Bedenken bezüglich potenzieller Risiken und ethischer Implikationen der KI-Technologie zurückgetreten. Hinton glaubt, dass das von ihm entwickelte Chatbotsystem zur Verbreitung von Fehlinformationen und Fake News genutzt werden kann und betont die Notwendigkeit einer verantwortungsvollen KI-Entwicklung und Bereitstellung zum Schutz der Gesellschaft.\n\n[https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\n](https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\n)\n\nTechnologieunternehmen möchten für die Daten bezahlt werden, die AI-Modelle antreiben. Chatbots durchsuchen das Internet und Inhalteanbieter möchten dafür bezahlt werden. Künstliche Intelligenz-Unternehmen nutzen Inhalte, die von Millionen Menschen ohne deren Zustimmung oder Entschädigung erstellt wurden. Reddit, eine unverzichtbare Quelle für OpenAI, hat kürzlich angekündigt, dass es AI-Unternehmen für den Zugang zu Daten Gebühren erheben wird. Twitter tut dasselbe. OpenAI lehnte es ab, Stellung zu nehmen.\n\n[https://www.wsj.com/articles/chatgpt-ai-artificial-intelligence-openai-personal-writing-5328339a\n](https://www.wsj.com/articles/chatgpt-ai-artificial-intelligence-openai-personal-writing-5328339a\n)\n\nKI beginnt damit, Menschen bei der Back-Office-Arbeit zu verdrängen. Der CEO von IBM kündigt einen vorübergehenden Stopp der Einstellung von Back-Office-Jobs an, da das Unternehmen in KI-Technologie investiert. Die Entscheidung spiegelt die strategische Ausrichtung von IBM wider, künstliche Intelligenz zur Optimierung von Betriebsabläufen und Verbesserung der Effizienz zu nutzen. Der Schritt signalisiert das Bekenntnis des Unternehmens zur Anpassung an technologische Fortschritte und veränderte Marktanforderungen.\n\n[https://fortune.com/2023/05/01/ibm-ceo-ai-artificial-intelligence-back-office-jobs-pause-hiring/\n](https://fortune.com/2023/05/01/ibm-ceo-ai-artificial-intelligence-back-office-jobs-pause-hiring/\n)\n\n## Ubiquitäres Computing\n\nVMware hat Cross-Cloud Managed Services eingeführt, eine neue Angebot, um das Management von Multi-Cloud zu vereinfachen und zu optimieren. Der Service bietet Kunden eine einheitliche Plattform zur Verwaltung verschiedener Cloud-Umgebungen mit erweiterten Sichtbarkeits-, Sicherheits- und Governance-Funktionen. Mit dieser Lösung will VMware Organisationen dabei helfen, die Komplexitäten von Multi-Cloud-Operationen zu überwinden und ihre Cloud-Strategien für verbesserte Effizienz und Agilität zu optimieren.\n\n[https://www.cloudcomputing-news.net/news/2023/may/04/vmware-unveils-cross-cloud-managed-services/\n](https://www.cloudcomputing-news.net/news/2023/may/04/vmware-unveils-cross-cloud-managed-services/\n)\n\nAWS (Amazon Web Services) hat einen neuen Service entwickelt, der sicheren Zugriff auf Cloud-Anwendungen ohne herkömmliche VPNs (Virtual Private Networks) ermöglicht. Der Service AWS Client VPN nutzt die globale Netzwerkinfrastruktur von AWS, um sichere Verbindungen zwischen Benutzern und Cloud-Ressourcen herzustellen. Dieser Ansatz vereinfacht den Remote-Zugriff und gewährleistet gleichzeitig ein hohes Maß an Sicherheit, was es Organisationen erleichtert, ihre Cloud-Umgebungen effektiv zu verwalten und zu sichern.\n\n[https://www.networkworld.com/article/3695174/aws-secures-access-to-cloud-apps-without-using-vpns.html\n](https://www.networkworld.com/article/3695174/aws-secures-access-to-cloud-apps-without-using-vpns.html\n)\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW14-de","image":"./briefs/edw-14/de/thumbnail.png","lang":"de","summary":"News in Digital Transformation for the week of May 8, 2023 including "},{"id":181,"type":"News Brief","title":"2023-5-14","tags":["ai","edge","cybersecurity"],"body":"\n\n## Künstliche Intelligenz\n\nDie Irish Times ist einem Hoax zum Opfer gefallen, als sie einen Artikel veröffentlichte, der von einem künstlichen Intelligenz-Programm erstellt wurde. Der von der AI erzeugte Artikel wurde unbemerkt veröffentlicht und online gestellt und verdeutlicht die Herausforderungen bei der Erkennung von maschinengeneriertem Inhalt. Dieser Vorfall wirft Bedenken hinsichtlich der potenziellen Verbreitung von Desinformationen auf und unterstreicht die Notwendigkeit effektiver Überprüfungsprozesse angesichts von AI-generiertem Inhalt.\n\n[https://www.lemonde.fr/en/economy/article/2023/05/17/the-irish-times-duped-by-a-hoax-article-written-by-an-artificial-intelligence-program_6026930_19.html\r](https://www.lemonde.fr/en/economy/article/2023/05/17/the-irish-times-duped-by-a-hoax-article-written-by-an-artificial-intelligence-program_6026930_19.html\r)\n\nEuropa führt bei der Festlegung von Leitlinien für die Entwicklung von künstlicher Intelligenz (KI). Die Europäische Union entwickelt umfassende Vorschriften, um sicherzustellen, dass KI transparent, verantwortungsbewusst und die Menschenrechte respektiert sind. Diese Maßnahmen zielen darauf ab, ethische Bedenken und potenzielle Risiken im Zusammenhang mit KI zu adressieren, und machen Europa zu einem globalen Vorreiter bei der Gestaltung einer verantwortungsvollen Umsetzung von KI.\n\n[https://www.nbcnews.com/tech/tech-news/europe-leading-world-building-guardrails-ai-rcna83912\r](https://www.nbcnews.com/tech/tech-news/europe-leading-world-building-guardrails-ai-rcna83912\r)\n\nEine neue Studie legt nahe, dass generative künstliche Intelligenz (KI)-Technologie das Potenzial hat, die Arbeitsstunden von Gesundheitsfachkräften um bis zu 40% zu erhöhen. Durch Automatisierung von Routineaufgaben und Bereitstellung von Entscheidungsunterstützung könnte KI die Effizienz erhöhen und Ärzten und Krankenschwestern ermöglichen, sich auf anspruchsvollere und komplexere Patientenversorgung zu konzentrieren.\n\n[https://www.healthcareitnews.com/news/generative-ai-could-augment-40-healthcare-working-hours\r](https://www.healthcareitnews.com/news/generative-ai-could-augment-40-healthcare-working-hours\r)\n\n## Cybersicherheit\n\nDie Cybersecurity and Infrastructure Security Agency (CISA) hat vor einer kritischen Schwachstelle in Ruckus Wireless Access Points gewarnt, die ausgenutzt wird, um WLAN-Netzwerke zu infizieren. Der Fehler ermöglicht es Angreifern, unbefugten Zugriff zu erlangen und beliebigen Code aus der Ferne auszuführen. CISA empfiehlt Organisationen, die Ruckus Access Points nutzen, die erforderlichen Patches anzuwenden und Schutzmaßnahmen zu implementieren, um sich gegen potenzielle Angriffe zu schützen.\n\n[https://www.bleepingcomputer.com/news/security/cisa-warns-of-critical-ruckus-bug-used-to-infect-wi-fi-access-points\r](https://www.bleepingcomputer.com/news/security/cisa-warns-of-critical-ruckus-bug-used-to-infect-wi-fi-access-points\r)\n\nEmpfindliche Daten von 237.000 US-Regierungsangestellten wurden aufgrund einer Sicherheitsverletzung öffentlich gemacht. Die kompromittierten Informationen beinhalten Sozialversicherungsnummern, Kontaktdetails und Beschäftigungsinformationen. Die Verletzung wurde auf eine unautorisierte Person zurückgeführt, die auf das E-Mail-Konto eines Angestellten zugegriffen hatte. Behörden untersuchen den Vorfall und betroffene Personen werden benachrichtigt.\n\n[https://www.yahoo.com/news/data-237-000-us-government-232707971.html\r](https://www.yahoo.com/news/data-237-000-us-government-232707971.html\r)\n\nEine schwerwiegende Sicherheitslücke wurde entdeckt, die über 2 Millionen sensible Datensätze auf der WordPress-Website exponiert. Die Schwachstelle im WordPress-Plug-in \"Essential Addons for Elementor\" ermöglicht unbefugten Zugriff auf persönliche Daten, einschließlich Namen, Adressen und Finanzinformationen. Die Lücke betrifft weit verbreitete Software und stellt ein erhebliches Risiko für die Privatsphäre und Sicherheit der Benutzer dar. Entwickler arbeiten an einem Patch, und Benutzer werden gebeten, ihre Systeme schnell zu aktualisieren.\n\n[https://thehackernews.com/2023/05/severe-security-flaw-exposes-over.html\r](https://thehackernews.com/2023/05/severe-security-flaw-exposes-over.html\r)\n\n## Edge Computing (de: Kantencomputing)\n\nEin kürzlich veröffentlichter Bericht von UL Solutions hebt künstliche Intelligenz (KI) und das Internet der Dinge (IoT) am Rand als entscheidende Technologien für Organisationen hervor, die 5G-Netzwerke nutzen. Die Kombination von KI und IoT am Rand ermöglicht Echtzeit-Datenverarbeitung, fortschrittliche Analytik und Automatisierung und erschließt damit neue Möglichkeiten für die Gesundheits-, Fertigungs- und Transportindustrie. Der Bericht betont das Potenzial dieser Technologien für Innovation und Effizienz im 5G-Zeitalter.\n\n[https://www.edgeir.com/report-reveals-ai-iot-edge-as-key-technologies-for-organizations-leveraging-5g-20230512\r](https://www.edgeir.com/report-reveals-ai-iot-edge-as-key-technologies-for-organizations-leveraging-5g-20230512\r)\n\nKyndryl, ein IT-Dienstleistungsunternehmen, erweitert sein Angebot um einen verwalteten Secure Access Service Edge (SASE)-Service. SASE kombiniert Netzwerk-Sicherheit und Wide-Area-Networking-Fähigkeiten zu einer vereinheitlichten cloud-basierten Lösung. Durch die Integration von SASE in sein Portfolio zielt Kyndryl darauf ab, verbesserte Sicherheit und Konnektivität für Unternehmen zu bieten, die in einer hybriden oder multi-cloud-Umgebung tätig sind. Der Schritt trägt der wachsenden Nachfrage nach umfassenden und vereinfachten Sicherheitslösungen in der sich entwickelnden IT-Landschaft Rechnung.\n\n[https://www.sdxcentral.com/articles/analysis/why-kyndryl-is-adding-a-managed-sase-service/2023/05/\r](https://www.sdxcentral.com/articles/analysis/why-kyndryl-is-adding-a-managed-sase-service/2023/05/\r)\n\nDer Edge AI-Markt wird voraussichtlich ein erhebliches Wachstum erfahren, dessen Größe in den kommenden Jahren erheblich zunehmen wird. Faktoren wie die Verbreitung von IoT-Geräten, Fortschritte in der KI-Technologie und die Notwendigkeit der Echtzeit-Datenverarbeitung treiben dieses Wachstum voran. Edge AI ermöglicht intelligente Entscheidungsfindung am Rand des Netzwerks, reduziert Latenzzeiten und verbessert die Effizienz. Die Markterweiterung bietet Chancen für verschiedene Branchen, darunter Gesundheitswesen, Fertigung und Einzelhandel.\n\n[https://finance.yahoo.com/news/edge-ai-market-size-predicted-190000466.html\r](https://finance.yahoo.com/news/edge-ai-market-size-predicted-190000466.html\r)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW15-de","image":"./briefs/edw-15/de/thumbnail.png","lang":"de","summary":"News from the week of May 15, 2023 in digital transformation including stories from Edge Computing, Cybersecurity, and Artificial Intelligence."},{"id":182,"type":"News Brief","title":"2023-5-21","tags":null,"body":"\n\n## Ubiquitäres Computing\n\nAmazon Web Services (AWS) plant, bis 2030 12,7 Milliarden US-Dollar in Indiens Cloud-Infrastruktur-Markt zu investieren, um die Schaffung von Arbeitsplätzen und erneuerbare Energieprojekte zu unterstützen. Das Investitionsvorhaben hat zum Ziel, der steigenden Kundennachfrage gerecht zu werden und zur digitalen Transformation Indiens beizutragen, während gleichzeitig Infrastrukturherausforderungen bewältigt werden.\n\n[https://www.cloudcomputing-news.net/news/2023/may/22/aws-to-put-13-billion-into-india-cloud-infrastructure-by-2030/\r](https://www.cloudcomputing-news.net/news/2023/may/22/aws-to-put-13-billion-into-india-cloud-infrastructure-by-2030/\r)\n\nSnowflake, ein Anbieter von Cloud-Datenplattformen, hat ein neues Büro in Großbritannien eröffnet, um sich über die EMEA-Region (Europa, Nahost und Afrika) auszudehnen. Der Schritt erfolgt als Reaktion auf das starke Momentum des Unternehmens auf dem Markt und zielt darauf ab, seine wachsende Kundenbasis in der Region besser zu bedienen.\n\n[https://www.cloudcomputing-news.net/news/2023/may/16/snowflake-opens-uk-office-amid-strong-momentum-across-emea/\r](https://www.cloudcomputing-news.net/news/2023/may/16/snowflake-opens-uk-office-amid-strong-momentum-across-emea/\r)\n\nMicrosoft bereitet sich darauf vor, seine Lösung Microsoft Cloud for Sovereignty auf den Markt zu bringen, die sich an Regierungs- und öffentliche Organisationen richtet. Die Cloud-Lösung für die Branche hat private Vorschauen durchlaufen, um sektorspezifische Herausforderungen anzugehen. Microsoft betont Transparenz, Datenrichtlinien und Sicherheit und plant, Plattformlösungen für verschiedene Branchen anzubieten.\n\n[https://www.ciodive.com/news/Microsoft-industry-cloud-data-sovereignty-platform/650214/\r](https://www.ciodive.com/news/Microsoft-industry-cloud-data-sovereignty-platform/650214/\r)\n\n## Fortgeschrittene Kommunikation\n\nIntel hat seinen programmierbaren Chip Agilex FPGA vorgestellt, der für Smart Networking konzipiert ist. Der Agilex FPGA bietet fortschrittliche Funktionen, einschließlich AI-Beschleunigung und verbesserter Sicherheit, was ihn für verschiedene Anwendungen geeignet macht. Der Chip soll die Netzwerkinfrastruktur mit erhöhter Flexibilität und Leistung stärken.\n\n[https://www.networkworld.com/article/3697156/intel-launches-agilex-fpga-for-smart-networking.html\r](https://www.networkworld.com/article/3697156/intel-launches-agilex-fpga-for-smart-networking.html\r)\n\nEthernet, die bahnbrechende Netzwerktechnologie, feiert ihr 50-jähriges Jubiläum. Trotz ihrer beeindruckenden Reise geht die Weiterentwicklung von Ethernet weiter. Mit Fortschritten wie schnelleren Geschwindigkeiten, größerer Kapazität und verbesserter Zuverlässigkeit bleibt Ethernet unverzichtbar für die Verbindung der digitalen Welt. Mit dem Fortschritt der Technologie wird Ethernet eine entscheidende Rolle bei der Gestaltung der Zukunft der Netzwerktechnologie spielen.\n\n[https://www.networkworld.com/article/3697013/ethernet-turns-50-but-its-voyage-has-only-begun.html\r](https://www.networkworld.com/article/3697013/ethernet-turns-50-but-its-voyage-has-only-begun.html\r)\n\nLaut Network Computing vereinfachen Open-Source-Lösungen die Verwaltung komplexer Netzwerkstrukturen. Mit der zunehmenden Komplexität moderner Netzwerke bieten Open-Source-Tools und Frameworks Flexibilität, Interoperabilität und Automatisierungsfähigkeiten. Diese Lösungen ermöglichen es Organisationen, die Verwaltung von Stoffen zu optimieren, Kosten zu reduzieren und Effizienz zu verbessern. Open-Source-basierte Ansätze sind heute im digitalen Landschaft unerlässlich, um die Komplexitäten von Netzwerkstrukturen zu vereinfachen.\n\n[https://www.networkcomputing.com/networking/simplifying-complex-fabrics-open-source-based-solutions\r](https://www.networkcomputing.com/networking/simplifying-complex-fabrics-open-source-based-solutions\r)\n\n## Datenmanagement\n\nIDM.net.au schlägt vor, die Automatisierung der Datenverwaltung aus fünf Gründen zu erforschen: Verbesserte Datenqualität, optimierte Einhaltung von Vorschriften, gesteigerte Produktivität, erhöhte Datenübersichtlichkeit und Anpassungsfähigkeit an die sich wandelnde Datenlandschaft. Automatisierung reduziert Fehler, gewährleistet die Einhaltung von Vorschriften, steigert die Effizienz, ermöglicht Kontrolle und hält Organisationen wettbewerbsfähig.\n\n[https://idm.net.au/article/0014302-5-reasons-explore-data-governance-automation-opportunities\r](https://idm.net.au/article/0014302-5-reasons-explore-data-governance-automation-opportunities\r)\n\nTDWI.org diskutiert, wie Datenexperten Herausforderungen bei der Web-Scraping angehen. Sie betonen die Bedeutung der Auswahl geeigneter Tools und Techniken zur Extraktion von Daten von Websites. Hindernisse wie Anti-Scraping-Maßnahmen und dynamischer Inhalt erfordern Proxys, Benutzeragenten und Fachwissen in der Datenstrukturierung. Erfolgreiches Web-Scraping ermöglicht wertvolle Datenerfassung zur Analyse und Entscheidungsfindung.\n\n[https://tdwi.org/articles/2023/05/18/diq-all-how-data-experts-overcome-web-scraping-challenges.aspx\r](https://tdwi.org/articles/2023/05/18/diq-all-how-data-experts-overcome-web-scraping-challenges.aspx\r)\n\nIn einem kürzlich veröffentlichten Bericht warnt Dataversity.net vor schlechter Sicherheitsarchitektur in Cloud-Umgebungen. Der Artikel identifiziert häufige Fehler, wie schwache Zugriffskontrollen und Fehlkonfigurationen, die zu Datenverletzungen führen können. Unter Betonung der Bedeutung von stabiler Authentifikation und Überwachung betont er die Notwendigkeit einer robusten Sicherheitsmaßnahme, um empfindliche Daten in Cloud-Architekturen zu schützen.\n\n[https://www.dataversity.net/cloud-architecture-mistakes-the-perils-of-poor-security-architecture/\r](https://www.dataversity.net/cloud-architecture-mistakes-the-perils-of-poor-security-architecture/\r)\n\n\n\n","guests":null,"link":"/brief-EDW16-de","image":"./briefs/edw-16/de/thumbnail.png","lang":"de","summary":"Digital Transformation news for the week of May 29, 2023. In this episode."},{"id":183,"type":"News Brief","title":"2023-5-28","tags":null,"body":"\n\n## Aktuelles aus dem Bereich Künstliche Intelligenz\n\nDer CEO von OpenAI, Sam Altman, warnt davor, dass das Unternehmen aufgrund von Bedenken bezüglich der geplanten Bestimmungen des AI Acts möglicherweise seine Dienste aus der EU zurückziehen wird. Dies macht die wachsende transatlantische Kluft in Bezug auf die Kontrolle von KI deutlich. US-amerikanische Technologieunternehmen, einschließlich Google, bereiten sich auf einen möglichen Konflikt mit europäischen Regulierungsbehörden über die Regulierung von KI vor.\n\n[https://www.ft.com/content/5814b408-8111-49a9-8885-8a8434022352](https://www.ft.com/content/5814b408-8111-49a9-8885-8a8434022352)\n\nMicrosoft-Präsident Brad Smith äußerte Bedenken über Deep Fakes und forderte Vorschriften zur Bekämpfung ausländischer Cyberinfluenz-Operationen. Er befürwortete die Lizenzierung von kritischer KI, die Implementierung von Exportkontrollen und die Verantwortung von Einzelpersonen für KI-bezogene Fragen. CEO von OpenAI Sam Altman betonte die Notwendigkeit globaler Zusammenarbeit und Sicherheitskonformität.\n\n[http://https//www.reuters.com/technology/microsoft-chief-calls-humans-rule-ai-safeguard-critical-infrastructure-2023-05-25/](http://https//www.reuters.com/technology/microsoft-chief-calls-humans-rule-ai-safeguard-critical-infrastructure-2023-05-25/)\n\nOpenAI startet ein Förderprogramm zur Erforschung demokratischer Inputs für die Entscheidungsfindung in der Künstlichen Intelligenz. Sie suchen vielfältige Perspektiven zur Gestaltung des Verhaltens von KI und haben das Ziel, innovative Prozesse für die öffentliche Überwachung zu entwickeln. Ausgewählte Teams erhalten Fördermittel, um Experimente durchzuführen und Ergebnisse zu veröffentlichen, um eine demokratische Steuerung für KI zu etablieren.\n\n[https://openai.com/blog/democratic-inputs-to-ai](https://openai.com/blog/democratic-inputs-to-ai)\n\n## Aktuelles aus dem Bereich Cybersicherheit\n\nDie nordkoreanische Bedrohungsgruppe Kimsuky hat laut der Cybersicherheitsfirma Cyware ihre Aufklärungs-Malware RandomQuery verbessert. Die Malware zielt auf Regierungs-, Militär- und Verteidigungsorganisationen ab und sammelt dabei sensible Informationen. Sie nutzt neue Techniken und vermeidet es, entdeckt zu werden, um verdeckte Spionageoperationen durchzuführen. Wachsamkeit und starke Sicherheitsmaßnahmen sind entscheidend, um die Bedrohung einzudämmen.\n\n[https://cyware.com/news/north-korea-actor-kimsuky-updates-its-reconnaissance-malware-randomquery-25cb1d1e](https://cyware.com/news/north-korea-actor-kimsuky-updates-its-reconnaissance-malware-randomquery-25cb1d1e)\n\nSicherheitsforscher haben eine hochentwickelte Android-Spyware namens \"Predator\" entdeckt, die seit 2021 aktiv ist. Die Spyware zielt auf Regierungs- und Militäreinrichtungen in Südasien, darunter Indien, Pakistan und Afghanistan. Sie ist in der Lage, sensible Daten zu stehlen, Audio- und Videoaufnahmen zu machen und Echtzeit-Positionsverfolgung durchzuführen. Benutzer werden gebeten, ihre Geräte zu aktualisieren und auf verdächtige Apps zu achten.\n\n[https://thehackernews.com/2023/05/predator-android-spyware-researchers.html](https://thehackernews.com/2023/05/predator-android-spyware-researchers.html)\n\nMicrosoft hat chinesische Regierungshacker in einer Kampagne namens Volt Typhoon erwischt, die auf kritische Infrastrukturorganisationen in Guam, einem US-Territorium, abzielte. Die Kampagne hatte zum Ziel, die Kommunikationsinfrastruktur zwischen den USA und Asien zu stören. CISA hat eine Warnung ausgegeben, und Microsoft hat Empfehlungen zur Abwehr der Bedrohung gegeben.\n\n[https://www.securityweek.com/microsoft-catches-chinese-gov-hackers-in-guam-critical-infrastructure-orgs/](https://www.securityweek.com/microsoft-catches-chinese-gov-hackers-in-guam-critical-infrastructure-orgs/)\n\n## Aktuelles aus dem Bereich Edge Computing\n\nDell Technologies stellt Dell NativeEdge vor, eine Software-Plattform, die Zero-Trust Edge-Implementierungen vereinfacht und absichert. Sie ermöglicht eine streamlinebetriebene Edge, eine Zero-Touch Implementierung und eine Multi-Cloud Anwendungsorchestrierung. Das Ziel der Plattform ist es, Effizienz und Konnektivität an der Edge zu verbessern und verschiedene Branchen und Anwendungsfälle zu unterstützen.\n\n[https://www.edgecomputing-news.com/2023/05/24/dell-nativeedge-software-transforms-edge-operations/](https://www.edgecomputing-news.com/2023/05/24/dell-nativeedge-software-transforms-edge-operations/)\n\nNTT verstärkt seine Zusammenarbeit mit Cisco, um Managed Private Networks für Enterprise IoT auszubauen und Nachhaltigkeit und Benutzerfreundlichkeit zu priorisieren. Die Partnerschaft nutzt IoT-Hardware von Cisco, die durch KI und Maschinelles Lernen handlungsorientierte Erkenntnisse ermöglicht. Die Vereinbarung baut auf einer früheren Zusammenarbeit für Ciscos Private 5G-Lösung auf und bietet IoT als Service an, einschließlich integrierter Services und Infrastrukturmanagement.\n\n[https://www.sdxcentral.com/articles/interview/ntt-doubling-down-on-cisco-to-boost-managed-iot/2023/05/](https://www.sdxcentral.com/articles/interview/ntt-doubling-down-on-cisco-to-boost-managed-iot/2023/05/)\n\nCloudflare, ein führender Dienstleister, konzentriert sich auf künstliche Intelligenz (KI), nachdem das Unternehmen im ersten Quartal 2023 ein starkes Wachstum bei seinen Edge-Diensten gemeldet hatte. Das Unternehmen zielt darauf ab, KI zu nutzen, um seine Angebote zu verbessern und das Kundenerlebnis zu steigern, insbesondere in den Bereichen DDoS-Minderung und Sicherheit. Cloudflares Fokus auf KI steht im Einklang zu seiner Strategie, innovative Edge-Lösungen für eine wachsende Kundenbasis bereitzustellen.\n\n[https://www.edgeir.com/cloudflare-leaning-into-ai-after-1q23-results-show-good-growth-for-edge-services-20230525](https://www.edgeir.com/cloudflare-leaning-into-ai-after-1q23-results-show-good-growth-for-edge-services-20230525)\n\n\n\n\n","guests":null,"link":"/brief-EDW17-de","image":"./briefs/edw-17/de/thumbnail.png","lang":"de","summary":"News for Embracing Digital for the week of May 29, 2023, learn about more regulations for AI, increased nation-state cyber attacks, and edge computing investments."},{"id":184,"type":"News Brief","title":"2023-6-4","tags":["ai","compute","cybersecurity"],"body":"\n\n## Künstliche Intelligenz\n\nLaut einem Bericht hat die Bankgesellschaft JPMorgan über 3.600 AI-bezogene Arbeitsplätze ausgeschrieben, was das wachsende Interesse von Wall Street an revolutionärer Technologie widerspiegelt. Die Maßnahme unterstreicht die wachsende Abhängigkeit der Branche von künstlicher Intelligenz für verschiedene Funktionen und signalisiert eine Veränderung im Finanzsektor, hin zur Annahme von AI und ihren potenziellen Vorteilen.\n\n[https://www.msn.com/en-us/money/other/banking-giant-jpmorgan-advertised-more-than-3-600-ai-related-jobs-report-says-as-wall-street-starts-to-embrace-the-revolutionary-tech/ar-AA1bYZwL](https://www.msn.com/en-us/money/other/banking-giant-jpmorgan-advertised-more-than-3-600-ai-related-jobs-report-says-as-wall-street-starts-to-embrace-the-revolutionary-tech/ar-AA1bYZwL)\n\nOpenAI hat eine Methode entwickelt, um die logischen Denkfähigkeiten von KI-Modellen zu verbessern und Vorfälle von \"Halluzinationen\" oder der Erzeugung falscher Informationen zu reduzieren. Durch die Modifikation des Schulungsprozesses konnte das Forschungsteam Modelle erzeugen, die verbesserte Konsistenz aufweisen und vermeiden, Details auszudenken. Diese Entwicklung trägt dazu bei, zuverlässigere und vertrauenswürdigere KI-Systeme zu schaffen.\n\n[https://www.zdnet.com/article/openai-found-a-way-to-make-ai-models-more-logical-and-avoid-hallucinations/](https://www.zdnet.com/article/openai-found-a-way-to-make-ai-models-more-logical-and-avoid-hallucinations/)\n\nJapan trotzt Urheberrechtsverletzungen, indem es das Training von künstlicher Intelligenz mit beliebigen Daten ermöglicht, um seinen Fortschritt in der AI zu beschleunigen und global zu konkurrieren. Bedenken von Künstlern werden durch Unterstützung von Akademie und Unternehmen ausgeglichen, während Japan den Zugang zu westlichen Daten im Austausch für seine kulturellen Ressourcen sucht. Eine einzigartige Wendung in der Debatte um Regulierungen entsteht.\n\n[https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/](https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/)\n\n## Ubiquitous Computing (Note: This is already in English and does not need to be translated)\n\nDell stärkt seine Multi-Cloud-Strategien durch Fortschritte in seinem APEX-Portfolio. Zu den Angeboten gehören Dell APEX Cloud-Plattformen für Azure, Red Hat OpenShift, VMware und Speicherlösungen für öffentliche Clouds. Dell APEX erweitert das Angebot um Compute-Ressourcen und PC-as-a-Service und arbeitet mit Databricks zusammen, um Datenanalysen über lokale und Cloud-Umgebungen hinweg zu ermöglichen. HPE bietet mit ihrer Green-Lake-Produktlinie ebenfalls Multi-Cloud-Lösungen an, um nicht zurückzufallen.\n\n[https://www.cloudcomputing-news.net/news/2023/jun/01/dell-apex-portfolio-advancements-help-customers-strengthen-multicloud-strategies/](https://www.cloudcomputing-news.net/news/2023/jun/01/dell-apex-portfolio-advancements-help-customers-strengthen-multicloud-strategies/)\n\nEin kürzlich veröffentlichter Bericht von Information Week sagt voraus, dass die weltweiten Ausgaben für Cloud-Services bis 2025 1,3 Billionen US-Dollar erreichen werden, was einem Wachstum von etwa 16,9 % entspricht. Viele Organisationen benötigen jedoch noch Hilfe beim ROI ihrer Investitionen. Der Bericht identifiziert kritische Bereiche zur Verbesserung des ROI bei Cloud-Einsätzen, einschließlich FinOps, der Einführung von Multi-Hybrid-Cloud-Strategien, DevOps und Plattform-Engineering sowie Cloud-Sicherheit.\n\n[https://www.informationweek.com/cloud/leaders-should-pay-attention-to-these-4-major-cloud-trends](https://www.informationweek.com/cloud/leaders-should-pay-attention-to-these-4-major-cloud-trends)\n\nMicrosoft Azure DevOps hatte aufgrund eines einfachen Tippfehlers in der South Brazil Region einen zehnstündigen Ausfall erlebt, der zur Löschung von siebzehn Produktionsdatenbanken führte. Der Fehler trat während eines Code-Upgrades auf, und ein Tippfehler im Pull Request führte zur Löschung des gesamten Servers. Die Daten wurden wiederhergestellt, aber der Wiederherstellungsprozess dauerte aufgrund verschiedener Komplikationen mehr als zehn Stunden. Microsoft hat Korrekturen und Neukonfigurationen implementiert, um ähnliche Probleme in Zukunft zu vermeiden.\n\n[https://www.theregister.com/2023/06/03/microsoft_azure_outage_brazil/](https://www.theregister.com/2023/06/03/microsoft_azure_outage_brazil/)\n\n## Cyber-Sicherheitsnachrichten\n\nEin Cybersicherheitsexperte warnt davor, dass Betrüger Gmail-Nutzer angreifen, indem sie eine Kollaborationsfunktion ausnutzen. Nutzer erhalten betrügerische Einladungen, die sie auf bösartige Websites umleiten. Zur Absicherung gegen solche Betrügereien werden Wachsamkeit, Überprüfung von Anfragen, Aktivierung der Zwei-Faktor-Authentifizierung und Überwachung der Kontoeinstellungen empfohlen.\n\n[https://www.wmur.com/article/nh-cybersecurity-expert-gmail-feature-is-being-hacked-by-scammers/44083493](https://www.wmur.com/article/nh-cybersecurity-expert-gmail-feature-is-being-hacked-by-scammers/44083493)\n\nLaut einer Umfrage des CISO Hall of Fame ist die Cloud-Sicherheit die wichtigste Sorge von IT-Profis. Der Bericht hebt die zunehmende Abhängigkeit von Cloud-Services und die Notwendigkeit zur Bewältigung von Sicherheitsproblemen wie Datenpannen und unbefugtem Zugriff hervor. Schlüsselbereiche sind Identitäts- und Zugangsmanagement, Verschlüsselung, proaktive Bedrohungserkennung, um robuste Cloud-Sicherheitsmaßnahmen zu gewährleisten, und die Suche nach Cybersicherheitstalenten mit Erfahrung im Cloud-Service-Bereich.\n\n[https://thehackernews.com/2023/06/cloud-security-tops-concerns-for.html](https://thehackernews.com/2023/06/cloud-security-tops-concerns-for.html)\n\nIn einem aktuellen Artikel hat sich Walmart dazu bereit erklärt, über seine bewährten Verfahren zu sprechen, in der Hoffnung, das Wissen seines Ökosystems von Lieferanten und Partnern zu erweitern. Das Sicherheitsoperationszentrum von Walmart und der Verwendung einer mehrschichtigen Verteidigungsstrategie sind bewährte Verfahren, aber kleinere Unternehmen benötigen praktische Strategien. Die Anpassung von Sicherheitsprotokollen, die Implementierung einer starken Identitäts- und Zugriffsverwaltung sowie die Überprüfung von Zugriffsrichtlinien können den Schutz verbessern. Next-Generation-Sicherheitstechnologien bieten kostengünstige Lösungen zur Minimierung von Cyber-Bedrohungen.\n\n[https://betanews.com/2023/06/03/walmart-cybersecurity/](https://betanews.com/2023/06/03/walmart-cybersecurity/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW18-de","image":"./briefs/edw-18/de/thumbnail.png","lang":"de","summary":"Digital transformation news for June 6, 2023. This week more jobs in AI, major cyber security breaches, and cloud technology best practices."},{"id":185,"type":"News Brief","title":"2023-6-11","tags":["ai","compute","cybersecurity"],"body":"\n\n## Künstliche Intelligenz\n\nMicrosoft hat seine leistungsstarke OpenAI-Technologie für US-Regierungskunden in der Cloud verfügbar gemacht. Mit diesem Schritt können Behörden fortschrittliche KI-Funktionen in den Bereichen Sprachverarbeitung, maschinelles Lernen und natürliches Sprachverständnis nutzen. Ziel der Partnerschaft ist es, Behörden bei der digitalen Transformation zu unterstützen.\n\n[https://www.bloomberg.com/news/articles/2023-06-07/microsoft-offers-powerful-openai-technology-to-us-government-cloud-customers](https://www.bloomberg.com/news/articles/2023-06-07/microsoft-offers-powerful-openai-technology-to-us-government-cloud-customers)\n\nDer rumänische Premierminister hat den weltweit ersten KI-Regierungsberater ernannt. Das KI-System namens DORA wird Entscheidungsprozesse unterstützen, Daten analysieren und Empfehlungen zu verschiedenen politischen Themen abgeben. Dieser bahnbrechende Schritt spiegelt die zunehmende Integration von KI-Technologie in Regierungsabläufe wider und zielt darauf ab, die Effizienz und Effektivität der Regierungsführung zu verbessern.\n\n[https://www.euronews.com/next/2023/03/06/romanias-prime-minister-has-hired-the-worlds-first-ai-government-adviser-what-will-it-do](https://www.euronews.com/next/2023/03/06/romanias-prime-minister-has-hired-the-worlds-first-ai-government-adviser-what-will-it-do)\n\nKI übertrifft Menschen bei der Entdeckung effizienter Sortieralgorithmen und deren Integration in C++-Bibliotheken. AlphaDev, ein Deep-Reinforcement-Learning-Agent, übertraf menschliche Benchmarks, indem er das Problem als Spiel formulierte und CPU-Befehle auswählte, um optimale Algorithmen zu erstellen. Die Studie unterstreicht das Potenzial von KI bei der Optimierung von Algorithmen.\n\n[https://www.nature.com/articles/s41586-023-06004-9](https://www.nature.com/articles/s41586-023-06004-9)\n\n## Allgegenwärtiges Computing\n\nMicrosoft 365, die beliebte Suite von Produktivitätswerkzeugen, litt unter weit verbreiteten Ausfällen, die für Millionen von Benutzern Störungen verursachten. Die Dienstunterbrechungen betrafen mehrere Komponenten, darunter Outlook, Teams und SharePoint. Microsoft räumte das Problem ein und erklärte, seine Ingenieure arbeiteten an einer Lösung.\n\n[https://www.theregister.com/2023/06/06/microsoft_365_outages/](https://www.theregister.com/2023/06/06/microsoft_365_outages/)\n\nAusfälle von Cloud-Diensten nehmen aufgrund geopolitischer Spannungen und Schwachstellen im Internet zu. Grenzüberschreitende Einschränkungen des Datenflusses und Cyber-Bedrohungen tragen zu Störungen bei. Experten fordern Zusammenarbeit und Investitionen, um die Ausfallsicherheit von Cloud-Diensten zu erhöhen und die globale Konnektivität zu schützen.\n\n[https://fortune.com/2023/06/07/cloud-outages-on-the-rise-tech-geopolitics-internet/](https://fortune.com/2023/06/07/cloud-outages-on-the-rise-tech-geopolitics-internet/)\n\nDer afrikanische Cloud-Markt wird bis 2023 deutlich wachsen, angetrieben durch die digitale Transformation und die verstärkte Nutzung von Cloud-Diensten. Verbesserte Internetverbindungen und die steigende Nachfrage nach Cloud-basierten Lösungen in verschiedenen Branchen tragen zur Expansion bei. Wichtige Akteure investieren in Infrastruktur und Partnerschaften, um die Chancen des wachsenden afrikanischen Cloud-Marktes zu nutzen.\n\n[https://finance.yahoo.com/news/rise-african-cloud-market-2023-082300194.html](https://finance.yahoo.com/news/rise-african-cloud-market-2023-082300194.html)\n\n## Cyber-Sicherheit\n\nUm dem Mangel an qualifizierten Cybersicherheitsexperten entgegenzuwirken, startet Accenture das kostenlose Programm \"Skills to Succeed in Cybersecurity\", um eine Million neue Bewerber für Cybersicherheitsjobs zu gewinnen. Die Initiative bietet umfassende Schulungs- und Zertifizierungsressourcen, um die Qualifikationslücke zu schließen und mehr Menschen zu ermutigen, eine Karriere in der Cybersicherheit anzustreben.\n\n[https://fortune.com/education/articles/accenture-launches-free-cybersecurity-upskilling-program-in-effort-to-fill-1-million-entry-level-jobs/](https://fortune.com/education/articles/accenture-launches-free-cybersecurity-upskilling-program-in-effort-to-fill-1-million-entry-level-jobs/)\n\nDas Weiße Haus verlängert die Fristen für die Sicherheitsüberprüfung von Software und gibt klare Anweisungen zur Verbesserung der Cybersicherheitspraktiken heraus. Die verlängerten Fristen geben mehr Zeit für die Einhaltung, während die Anweisungen Einblicke in die Implementierungs- und Evaluierungsprozesse für Bundesbehörden und Auftragnehmer bieten. Sind unsere Anwendungen und Dienste derzeit unbemerkt angreifbar?\n\n[https://federalnewsnetwork.com/cybersecurity/2023/06/white-house-extends-secure-software-attestation-deadlines-offers-clarifying-guidance/](https://federalnewsnetwork.com/cybersecurity/2023/06/white-house-extends-secure-software-attestation-deadlines-offers-clarifying-guidance/)\n\nDie Nationale Cyber-Sicherheitsstrategie fordert grundlegende Veränderungen bei der Sicherung kritischer Infrastrukturen. Die Zusammenarbeit zwischen dem öffentlichen und dem privaten Sektor wird betont, um den sich entwickelnden Cyber-Sicherheitsbedrohungen zu begegnen. Risikomanagement, Widerstandsfähigkeit und Investitionen in fortschrittliche Technologien werden hervorgehoben, um kritische Infrastrukturen zu stärken und die nationale Sicherheit zu gewährleisten.\n\n[https://federalnewsnetwork.com/commentary/2023/06/national-cybersecurity-strategy-calls-for-significant-change-in-critical-infrastructure/](https://federalnewsnetwork.com/commentary/2023/06/national-cybersecurity-strategy-calls-for-significant-change-in-critical-infrastructure/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW19-de","image":"./briefs/edw-19/de/thumbnail.png","lang":"de","summary":"Die Digitalisierung für die Woche vom 12. Juni 2023 umfasst die Entwicklung einer 1 Millionen Cyber-Kraft, viele Cloud-Ausfälle und künstliche Intelligenz, die Code für die C++ Standardbibliothek schreibt."},{"id":186,"type":"News Brief","title":"2023-2-12","tags":null,"body":"\n\n\n## Künstliche Intelligenz\n\nFehler der KI: Schuld sind Ungenauigkeiten in den Eingabedatensätzen.\n\nDie Aktien von Alphabet sanken um 100 Milliarden Dollar, nachdem Googles KI-Chatbot Bard in einem Werbevideo falsche Informationen geteilt hatte und eine Firmenveranstaltung keinen Eindruck hinterließ. Während einer Live-Veranstaltung stolperte Bard, was dazu führte, dass Google den Livestream abbrach. Dies schürte Bedenken, dass der Tech-Riese gegenüber seinem Konkurrenten Microsoft an Boden verliert. Reuters berichtete, dass Bard dem James-Webb-Weltraumteleskop die Ehre gab, die ersten Bilder eines Planeten außerhalb des Sonnensystems aufgenommen zu haben. In Wirklichkeit wurden die ersten Bilder vom Very Large Telescope des European Southern Observatory aufgenommen. Der Vorfall zeigt die Bedeutung von Faktenchecks vor dem Training von KI-Modellen auf.\n\nMeta startete BlenderBot, einen prototypischen Conversational AI, der bald darauf Journalisten sagte, dass er nach dem Kennenlernen der Datenschutzskandale des Unternehmens sein Facebook-Konto gelöscht habe. \"Seit ich Facebook gelöscht habe, geht es mir viel besser\", sagte er. (oh oh)\n\nMicrosoft entschuldigte sich 2016, nachdem ein Twitter-Chatbot namens Tay begann, rassistische und sexistische Nachrichten zu generieren. Er wurde gezwungen, den Bot zu schließen, nachdem Benutzer Tay mit hasserfüllten Bemerkungen auf Twitter getweetet hatten, die er dann nachäffte. Seine Posts enthielten Vergleiche zwischen Feminismus und Krebs sowie die Andeutung, dass der Holocaust nicht stattgefunden hat.\n\nSatya Nadella von Microsoft sagte CNBC, dass die suchebasierte KI das Größte sei, was in den neun Jahren seiner Amtszeit als CEO des Unternehmens passiert ist.\n\nOtter.ai von FastCompany kann automatisch Meetings aufzeichnen, Notizen machen und Zusammenfassungen erstellen, um wichtige Punkte leichter zu verfolgen. Es kann diese Aufgaben sogar über Zoom durchführen. https://www.fastcompany.com/90834773/how-to-use-ai-to-improve-employee-performance-and-engagement\n\n## Cybersicherheit\n\nEine neue Variante des ESXiArgs-Ransomware-Angriffs ist aufgetaucht, nachdem CISA ein Entschlüsselungstool veröffentlicht hatte.\n\nDie Bedrohungsakteure hinter dem ESXiArgs-Ransomware-Angriff haben eine aktualisierte Version veröffentlicht, die mehr Daten verschlüsselt und die Bitcoin-Adresse aus der Erpressernachricht entfernt hat. Stattdessen werden die Opfer gebeten, sie über Tox zu kontaktieren, um Informationen zur Wallet zu erhalten. Die neue Variante wurde von einem Systemadministrator gemeldet, der sagte, dass Dateien, die größer als 128 MB sind, 50% ihrer Daten verschlüsselt haben werden. Die Cybersecurity and Infrastructure Security Agency (CISA) hatte zuvor ein Entschlüsselungstool veröffentlicht, um betroffenen Opfern bei der Wiederherstellung nach dem Angriff zu helfen. Laut Censys wussten die Angreifer wahrscheinlich, dass der ursprüngliche Verschlüsselungsprozess einfach zu umgehen war und waren sich bewusst, dass Forscher ihre Zahlungen verfolgten. https://thehackernews.com/2023/02/new-esxiargs-ransomware-variant-emerges.html\n\nMicrosoft, Google und Apple erwägen, traditionelle Passwörter durch sichere Passkeys zur Authentifizierung von Benutzern zu ersetzen. Passkeys, die mehr Sicherheit und Widerstandsfähigkeit gegen Phishing-Angriffe bieten, werden immer beliebter, da die Sicherheit von Passwörtern weiterhin von Hackern bedroht wird. Experten für Internetsicherheit schlagen vor, dass Passkeys innerhalb eines Jahres Standard werden könnten, wobei Unternehmen wie Apple, Google und Microsoft bereits Benutzer dazu drängen, sie zu verwenden. https://www.cnbc.com/2023/02/11/why-apple-google-microsoft-passkey-should-replace-your-own-password.html\n\n## Intelligenter Rand\n\nAT&T arbeitet mit Ghost Robotics zusammen, um robotische Hunde zur Verbesserung der öffentlichen Sicherheit und nationalen Verteidigung einzusetzen. Die Initiative wird den FirstNet-Notfalldienst verbessern, und netzwerkverbundene robotische Hunde können eine Vielzahl von Anwendungsfällen für das Internet der Dinge (IoT) liefern, einschließlich solcher, die zuvor das Personal in gefährliche Situationen brachten. Lance Spencer, Client Executive VP - Defense bei AT&T, sagte, dass dies eine Möglichkeit sei, die Innovation und transformative Möglichkeiten von 5G und IoT zu demonstrieren. https://www.iottechnews.com/news/2023/jan/26/att-touts-robotic-dogs-public-safety-national-defense/\n\nEine Studie von Juniper Research sagt voraus, dass die weltweite Anzahl der 5G IoT-Verbindungen bis 2026 mehr als 100 Millionen überschreiten wird, ein Anstieg um 1.100% von 17 Millionen Verbindungen im Jahr 2023. Gesundheits- und Smart City-Dienste sollen dieses Wachstum antreiben. Weltweit werden über 60 Millionen 5G-Smart City-Verbindungen erwartet. 5G wird eine effizientere Gesundheitsversorgung ermöglichen und IoT-Technologien können Ineffizienzen im Gesundheitswesen, die durch die COVID-19-Pandemie aufgedeckt wurden, angehen. Vernetzte Notdienste, Telemedizin und Echtzeit-Fernüberwachung werden die nützlichsten Anwendungen von 5G IoT sein. https://www.iottechnews.com/news/2023/jan/24/5g-iot-connections-exceed-100m-by-2026/\n\n\n","guests":null,"link":"/brief-EDW2-de","image":"./briefs/edw-2/de/thumbnail.png","lang":"de","summary":"Summary"},{"id":187,"type":"News Brief","title":"2023-6-18","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Ubiquitous Compute: Allgegenwärtiges Rechnen\n\nInformationWeek untersucht die Debatte zwischen KI und Low-Code/No-Code für Entwickler. Während Low-Code/No-Code-Plattformen einfachheit und Schnelligkeit bieten, bringt KI-basierte Tools fortschrittliche Automatisierung und Intelligenz. Der Artikel geht auf die Vor- und Nachteile jeder Herangehensweise ein und betont, wie das Finden des richtigen Gleichgewichts Entwickler befähigen und Innovationen im DevOps-Bereich vorantreiben kann.\n\n[https://www.informationweek.com/devops/dos-won-t-hunt-is-ai-better-than-low-code-no-code-for-developers-](https://www.informationweek.com/devops/dos-won-t-hunt-is-ai-better-than-low-code-no-code-for-developers-)\n\nCRN berichtet auf unterhaltsame Weise über die Top 10 Cloud-Ausfälle des Jahres 2023 (bisher). Von \"Cloudy with a Chance of Downtime\" bis \"The Great Data Storm\" boten diese Ausfälle unerwartete Gelegenheiten für Cloud-Nutzer, ihre Geduld zu üben und die Freuden offline Aktivitäten wiederzuentdecken. Es war ein raues Winter- und Frühjahr. Hoffen wir auf klarere Himmel in der Cloud!\n\n[https://www.crn.com/news/cloud/the-10-biggest-cloud-outages-of-2023-so-far-](https://www.crn.com/news/cloud/the-10-biggest-cloud-outages-of-2023-so-far-)\n\nOracle verzeichnet beeindruckendes Wachstum seiner Cloud-Einnahmen in verschiedenen Branchen und wird von der Einführung von künstlicher Intelligenz angetrieben. Von der Verwandlung des Gesundheitswesens mit prädiktiver Analyse bis zur Optimierung von Lieferketten mit intelligenter Automatisierung, befähigen Oracle's Cloud-Lösungen Unternehmen verschiedenster Bereiche. Die Zukunft sieht vielversprechend aus, da künstliche Intelligenz weiterhin Innovationen vorantreibt und die digitale Transformation voranbringt.\n\n[https://www.ciodive.com/news/Oracle-cloud-revenue-growth-industry-verticals-AI/652885/](https://www.ciodive.com/news/Oracle-cloud-revenue-growth-industry-verticals-AI/652885/)\n\n## Datenmanagement\n\nAuf der Suche nach verantwortungsvoller KI untersucht TDWI wichtige Überlegungen. Von Datenethik und Bias-Eindämmung bis hin zur Interpretierbarkeit und Verantwortlichkeit erfordert verantwortungsvolle KI einen ganzheitlichen Ansatz. Organisationen werden aufgefordert, Transparenz, Fairness und menschliche Überwachung zu priorisieren, um sicherzustellen, dass KI-Systeme als vertrauenswürdige und nützliche Werkzeuge in unserer sich ständig weiterentwickelnden digitalen Landschaft dienen.\n\n[https://tdwi.org/articles/2023/06/08/adv-all-responsible-ai-0608.aspx](https://tdwi.org/articles/2023/06/08/adv-all-responsible-ai-0608.aspx)\n\nDataStax stellt einen Schema-GPT-Übersetzer für seine Apache-Pulsar-basierte Astra-Streaming-Plattform vor. Diese Neuerung erweitert die Möglichkeiten in der Datenverwaltung und ermöglicht eine nahtlose Integration zwischen Schemas und dem beliebten GPT-Sprachmodell. Benutzer können nun die Leistung der natürlichen Sprachverarbeitung in ihren Datenstreaming-Workflows nutzen, um Effizienz und Erkenntnisse zu steigern.\n\n[https://www.infoworld.com/article/3699748/datastax-adds-schema-gpt-translator-to-apache-pulsar-based-astra-streaming.html](https://www.infoworld.com/article/3699748/datastax-adds-schema-gpt-translator-to-apache-pulsar-based-astra-streaming.html)\n\nNichtverstehen komplexer Datenschutzgesetze hat einen Preis. Microsoft steht eine Strafe von 20 Millionen Dollar von der FTC für Verletzungen des Datenschutzes von Kindern auf Xbox bevor. Dem Unternehmen wird vorgeworfen, keine elterliche Zustimmung für die Datensammlung eingeholt und nicht ausreichend Sicherheitsmaßnahmen ergriffen zu haben. Diese hohe Strafe dient als Erinnerung an Organisationen, die Datenschutzrechte von Kindern im digitalen Bereich zu priorisieren und aufrechtzuerhalten.\n\n[https://www.cpomagazine.com/data-protection/20-million-fine-issued-to-microsoft-by-ftc-over-xbox-childrens-privacy-violations/](https://www.cpomagazine.com/data-protection/20-million-fine-issued-to-microsoft-by-ftc-over-xbox-childrens-privacy-violations/)\n\n## Künstliche Intelligenz\n\nMcKinseys neuer Bericht hebt das immense wirtschaftliche Potential der Generativen KI hervor und positioniert sie als nächste Grenze für die Produktivität. Diese transformative Technologie hat das Potenzial, signifikantes Wachstum zu fördern, Industrien zu revolutionieren und beispiellose Ebenen von Innovation und Kreativität freizusetzen. Vier Sektoren sind jedoch in Gefahr, hochqualifizierte menschliche Arbeitnehmer zu verdrängen. Namentlich Kundendienst, Marketing und Vertrieb, Softwareentwicklung und F&E.\n\n[https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier)\n\nEuropa geht mutig voran und reguliert KI, indem es die Macht der Tech-Giganten herausfordert. Neue Vorschriften sollen Bedenken bezüglich Transparenz, Fairness und Verantwortlichkeit angehen. Indem Europa die Kontrolle über KI-Anwendungen übernimmt, strebt es danach, die ethische und verantwortungsvolle Nutzung künstlicher Intelligenz in der digitalen Landschaft zu gestalten. Im Gegensatz dazu setzt Japan auf KI und verringert die Regulierungen zur Datensammlung, weil es bei diesem riesigen Paradigmenwechsel nicht zurückgelassen werden möchte.\n\n## Ubiquitous Compute: Allgegenwärtiges Rechnen\n\nInformationWeek untersucht die Debatte zwischen KI und Low-Code/No-Code für Entwickler. Während Low-Code/No-Code-Plattformen Einfachheit und Geschwindigkeit bieten, bringen KI-gesteuerte Tools fortschrittliche Automatisierung und Intelligenz. Der Artikel geht auf die Vor- und Nachteile jeder Methode ein und zeigt auf, wie das Finden des richtigen Gleichgewichts Entwickler befähigen und Innovationen im DevOps-Bereich vorantreiben kann.\n\n[https://www.informationweek.com/devops/dos-won-t-hunt-is-ai-better-than-low-code-no-code-for-developers-](https://www.informationweek.com/devops/dos-won-t-hunt-is-ai-better-than-low-code-no-code-for-developers-)\n\nCRN erzählt auf humorvolle Weise die Top 10 Cloud-Ausfälle von 2023 (bisher). Von 'Cloudy with a Chance of Downtime' bis hin zu 'The Great Data Storm' dienten diese Ausfälle als unerwartete Möglichkeiten für Cloud-Benutzer, ihre Geduld zu üben und die Freuden von Offline-Aktivitäten wiederzuentdecken. Es war ein harter Winter und Frühling. Hoffen wir auf klarere Himmel in der Wolke!\n\n[https://www.crn.com/news/cloud/the-10-biggest-cloud-outages-of-2023-so-far-](https://www.crn.com/news/cloud/the-10-biggest-cloud-outages-of-2023-so-far-)\n\nOracle verzeichnet beeindruckendes Umsatzwachstum in der Cloud in verschiedenen Branchen, angetrieben durch die Akzeptanz von künstlicher Intelligenz. Von der Transformation des Gesundheitswesens durch Vorhersageanalysen bis zur Optimierung von Lieferketten durch intelligente Automatisierung: Oracles Cloud-Lösungen geben Unternehmen in allen Branchen mehr Kraft. Die Zukunft scheint rosig zu sein, da künstliche Intelligenz Innovation vorantreibt und digitale Transformation antreibt.\n\n[https://www.ciodive.com/news/Oracle-cloud-revenue-growth-industry-verticals-AI/652885/](https://www.ciodive.com/news/Oracle-cloud-revenue-growth-industry-verticals-AI/652885/)\n\n## Datenmanagement\n\nBei der Suche nach verantwortungsvoller KI untersucht TDWI wichtige Überlegungen. Von Datenethik und Bias-Minderung bis hin zur Interpretierbarkeit und Rechenschaftspflicht erfordert verantwortungsvolle KI einen ganzheitlichen Ansatz. Organisationen werden aufgefordert, Transparenz, Fairness und menschliche Aufsicht zu priorisieren, um sicherzustellen, dass KI-Systeme als vertrauenswürdige und nützliche Werkzeuge in unserer ständig weiterentwickelnden digitalen Landschaft dienen.\n\n[https://tdwi.org/articles/2023/06/08/adv-all-responsible-ai-0608.aspx](https://tdwi.org/articles/2023/06/08/adv-all-responsible-ai-0608.aspx)\n\nDataStax stellt einen Schema GPT-Übersetzer für seine auf Apache Pulsar basierende Astra Streaming-Plattform vor. Diese neue Erweiterung verbessert die Datenverwaltungsfähigkeiten, indem sie eine nahtlose Integration zwischen Schemas und dem beliebten GPT-Sprachmodell ermöglicht. Benutzer können nun die Kraft der natürlichen Sprachverarbeitung in ihren Daten-Streaming-Workflows nutzen und Effizienz und Erkenntnisse steigern.\n\n[https://www.infoworld.com/article/3699748/datastax-adds-schema-gpt-translator-to-apache-pulsar-based-astra-streaming.html](https://www.infoworld.com/article/3699748/datastax-adds-schema-gpt-translator-to-apache-pulsar-based-astra-streaming.html)\n\nDas Nichtverstehen komplexer Datenschutzgesetze hat einen Preis. Microsoft droht eine Geldstrafe von 20 Millionen US-Dollar von der FTC wegen Verstößen gegen Kinderdatenschutzbestimmungen bei Xbox. Dem Unternehmen wird vorgeworfen, keine elterliche Zustimmung für die Datensammlung eingeholt und unzureichende Sicherheitsmaßnahmen getroffen zu haben. Diese hohe Strafe sollte Organisationen daran erinnern, die Datenschutzrechte von Kindern im digitalen Bereich zu priorisieren und zu wahren.\n\n[https://www.cpomagazine.com/data-protection/20-million-fine-issued-to-microsoft-by-ftc-over-xbox-childrens-privacy-violations/](https://www.cpomagazine.com/data-protection/20-million-fine-issued-to-microsoft-by-ftc-over-xbox-childrens-privacy-violations/)\n\n## Künstliche Intelligenz\n\nDer neue Bericht von McKinsey hebt das immense wirtschaftliche Potenzial der Generativen KI hervor und positioniert sie als nächste Front für Produktivität. Diese transformative Technologie hat das Potenzial, signifikantes Wachstum zu treiben, Branchen zu revolutionieren und beispiellose Niveaus an Innovation und Kreativität zu entfesseln. Allerdings sind vier Sektoren von einer hohen Gefahr der Verdrängung menschlicher Arbeitskräfte bedroht. Diese sind Kundenservice, Marketing und Vertrieb, Softwareentwicklung und Forschung und Entwicklung.\n\n[https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier)\n\nEuropa macht einen mutigen Schritt nach vorn bei der Regulierung von künstlicher Intelligenz und fordert die Macht der Technologieriesen heraus. Neue Vorschriften sollen Bedenken hinsichtlich Transparenz, Fairness und Verantwortlichkeit angehen. Indem Europa Kontrolle über KI-Anwendungen ausübt, strebt es danach, den ethischen und verantwortlichen Einsatz künstlicher Intelligenz in der digitalen Landschaft zu formen. Im Gegensatz dazu sprechen Länder wie Japan sich für KI aus und verringern Vorschriften in Bezug auf Datensammlung, um bei diesem großen Paradigmenwechsel nicht zurückgelassen zu werden.\n\n[https://www.datacenterknowledge.com/artificial-intelligence/europe-moves-ahead-ai-regulation-challenging-tech-giants-power](https://www.datacenterknowledge.com/artificial-intelligence/europe-moves-ahead-ai-regulation-challenging-tech-giants-power)\n\nBCG hat gerade eine der umfassendsten Studien zum Thema künstliche Intelligenz (KI) durchgeführt und dabei 13.000 Menschen in über 18 Ländern befragt - von Führungskräften bis zu Mindestlohnempfängern. Das Ergebnis: Über 80% der Führungskräfte nutzen KI bei der Arbeit, während nur 20% der Arbeitnehmer an vorderster Front dies tun. Menschen sind optimistischer und machen sich nicht allzu viele Gedanken über KI. Arbeitnehmer an vorderster Front laufen Gefahr, ersetzt zu werden, wenn sie ihre Fähigkeiten nicht verbessern und KI in ihrer täglichen Arbeit einsetzen.\n\n[https://www.bcg.com/publications/2023/what-people-are-saying-about-ai-at-work](https://www.bcg.com/publications/2023/what-people-are-saying-about-ai-at-work)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW20-de","image":"./briefs/edw-20/de/thumbnail.png","lang":"de","summary":"Digitale Transformationsnachrichten aus der Woche vom 19. Juni 2023, einschließlich Geschichten von Managern, die Arbeiter durch künstliche Intelligenz ersetzen, erneuten Cloud-Ausfällen und Datenschutzgesetzen, die großen Tech-Unternehmen zusetzen."},{"id":188,"type":"News Brief","title":"2023-6-25","tags":["ai","edge","cybersecurity"],"body":"\n\n## Künstliche Intelligenz\n\nDie Beziehung zwischen KI und Kryptowährung entwickelt sich zu einer komplexen Dynamik, während Regulierungsbehörden mit ihrer Koexistenz ringen. Während KI-Technologien Potenzial für Betrugserkennung und Risikobewertung im Kryptobereich bieten, entstehen aufgrund der dezentralen Natur der Kryptowährungen regulatorische Herausforderungen. Eine Balance zwischen Innovation und Aufsicht zu finden bleibt eine entscheidende Herausforderung.\n\n[https://www.datacenterknowledge.com/artificial-intelligence/ai-and-crypto-are-becoming-regulatory-frenemies](https://www.datacenterknowledge.com/artificial-intelligence/ai-and-crypto-are-becoming-regulatory-frenemies)\n\nChatGPT, Google's Meena, Bing Chat und OpenAI's GPT-3-Modelle traten in einem realen Test gegeneinander an, um präzise und hilfreiche Antworten zu liefern. ChatGPT erwies sich als bester Performer und zeigte seine Fähigkeit, zusammenhängende Antworten zu verstehen und zu generieren. Es wurden jedoch in allen Modellen Einschränkungen festgestellt, die die anhaltenden Herausforderungen bei der Entwicklung von Chatbots hervorheben, die den Erwartungen der Benutzer vollständig entsprechen.\n\n[https://www.zdnet.com/article/chatbot-showdown-chatgpt-google-bard-and-bing-chat-put-to-a-real-world-test/](https://www.zdnet.com/article/chatbot-showdown-chatgpt-google-bard-and-bing-chat-put-to-a-real-world-test/)\n\nHPE hat die Einführung eines KI-Supercomputer-Cloud-Dienstes namens HPE Cray Accelerated Insight angekündigt. Der Service zielt darauf ab, Organisationen einfachen Zugang zu leistungsfähigen Computing-Ressourcen für KI-Workloads zu ermöglichen. Durch die Nutzung der Shasta-Architektur von Cray bietet der Cloud-Service hohe Leistungsfähigkeit und Skalierbarkeit, um die KI-Forschung und -Entwicklung zu beschleunigen.\n\n[https://www.datacenterknowledge.com/cloud/hpe-unveils-ai-supercomputer-cloud-service](https://www.datacenterknowledge.com/cloud/hpe-unveils-ai-supercomputer-cloud-service)\n\n## Cybersicherheit\n\nDer Cyberkrieg geht weiter!! Die Hacking-Gruppe Clop hat US-Regierungsbehörden ins Visier genommen und Daten durch einen Cyberangriff auf MoveIT-Software gestohlen. Dieser ausgeklügelte Angriff unterstreicht die wachsende Bedrohung durch Ransomware-Angriffe, wobei Clop fortschrittliche Taktiken wie die doppelte Erpressung nutzt. Der Vorfall unterstreicht die Notwendigkeit einer Verbesserung der Cybersecurity-Maßnahmen der Regierung.\n\n[https://www.wired.com/story/clop-moveit-hack-us-agencies-data-theft/](https://www.wired.com/story/clop-moveit-hack-us-agencies-data-theft/)\n\nDer chinesische Chip-Hersteller Hualan wurde aufgrund von Bedenken hinsichtlich der nationalen Sicherheit der USA der “Entity List” hinzugefügt. Hualan hat sich auf Verschlüsselungschips spezialisiert, die eine Schlüsselrolle bei der Absicherung sensibler Informationen spielen. Die Maßnahme spiegelt die andauernde Tech-Rivalität zwischen den USA und China wider und könnte erhebliche Auswirkungen auf globale Lieferketten und die Verschlüsselungsindustrie haben.\n\n[https://www.wired.com/story/hualan-encryption-chips-entity-list-china/](https://www.wired.com/story/hualan-encryption-chips-entity-list-china/)\n\nFernarbeit hat die Angriffsfläche erweitert. Es wurde eine Schwachstelle in Microsoft Teams entdeckt, die die Verbreitung von Malware durch externe Konten ermöglicht. Der Fehler ermöglicht es Angreifern, schädliche Nachrichten mit bösartigen Links zu senden, wodurch potenziell die Systeme der Benutzer beeinträchtigt werden können. Microsoft hat seitdem eine Aktualisierung veröffentlicht, um das Problem zu beheben, und fordert Benutzer auf, ihre Software zu aktualisieren, um die Sicherheit zu gewährleisten.\n\n[https://www.bleepingcomputer.com/news/security/microsoft-teams-bug-allows-malware-delivery-from-external-accounts/](https://www.bleepingcomputer.com/news/security/microsoft-teams-bug-allows-malware-delivery-from-external-accounts/)\n\n## Edge Computing\n\nSpirent, ein führender Anbieter von Test- und Messlösungen, hat eine Over-the-Air-Leistungsüberwachungslösung eingeführt. Das System ermöglicht Echtzeitüberwachung und Analyse der Leistung von drahtlosen Netzwerken, einschließlich Latenz, Durchsatz und Abdeckung. Diese Lösung zielt darauf ab, die wachsende Nachfrage nach zuverlässiger und hochwertiger drahtloser Konnektivität zu unterstützen, insbesondere im Kontext aufstrebender Technologien wie 5G und Edge Computing.\n\n[https://www.edgecomputing-news.com/2023/06/19/spirent-launches-over-the-air-performance-monitoring-solution/](https://www.edgecomputing-news.com/2023/06/19/spirent-launches-over-the-air-performance-monitoring-solution/)\n\nZscaler hat Cisco im Rennen um den Umsatz auf dem Secure Access Service Edge (SASE) Markt überholt. Zscalers cloudnative Architektur, umfassende Sicherheitsangebote und Fähigkeit, moderne Netzwerkherausforderungen zu bewältigen, haben zu seinem Erfolg beigetragen. Die zunehmende Nachfrage nach sicherem Remotezugriff und cloudbasierten Sicherheitslösungen hat das Wachstum von Zscaler vorangetrieben und es zum Marktführer im SASE-Bereich positioniert.\n\n[https://www.sdxcentral.com/articles/analysis/how-zscaler-finally-topped-cisco-in-the-sase-revenue-race/2023/06/](https://www.sdxcentral.com/articles/analysis/how-zscaler-finally-topped-cisco-in-the-sase-revenue-race/2023/06/)\n\nAT&T, Dell und VMware arbeiten zusammen, um die Bereitstellung von 5G-Edge-Netzen zu vereinfachen. Ihre kombinierte Lösung zielt darauf ab, die Implementierung von 5G-Netzen zu optimieren, indem sie auf die Dienste von AT&T, Dells Infrastruktur-Expertise und VMwares Software-Fähigkeiten für effiziente und nahtlose Edge-Computing-Anwendungen zurückgreift.\n\n[https://www.networkworld.com/article/3695740/att-dell-and-vmware-team-to-simplify-5g-edge-deployments.html](https://www.networkworld.com/article/3695740/att-dell-and-vmware-team-to-simplify-5g-edge-deployments.html)\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW21-de","image":"./briefs/edw-21/de/thumbnail.png","lang":"de","summary":"Neuigkeiten dieser Woche, 26. Juni 2023, in der digitalen Transformation umfassen eine Zunahme von Angriffen im Cyberkrieg, dass jeder auf den Zug der generativen KI aufspringt und virtualisierte Radio-Bereichsnetzwerke."},{"id":189,"type":"News Brief","title":"2023-7-9","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Künstliche Intelligenz\n\nLaut der neuesten Entdeckung der Bank of America zeigen die Download-Zahlen der ChatGPT-App Anzeichen eines gemächlichen Fortschritts. Das KI-gesteuerte Gesprächstool legt eine Pause ein, was Analysten dazu veranlasst, sich zu fragen, ob die gesprächigen Bots nichts mehr zu sagen haben oder ob die Benutzer sich nach einer altmodischen menschlichen Interaktion sehnen.\n\n[https://www.cnbc.com/2023/07/05/chatgpt-app-downloads-are-slowing-down-bofa-finds-.html](https://www.cnbc.com/2023/07/05/chatgpt-app-downloads-are-slowing-down-bofa-finds-.html)\n\nNew York City erklärt den Krieg gegen voreingenommene Algorithmen! TechCrunch berichtet, dass ihr Anti-Bias-Gesetz für Einstellungs-Algorithmen jetzt in vollem Gange ist. Die Algorithmen müssen aufpassen, denn sie dürfen nicht länger auf dem Arbeitsmarkt diskriminieren. Es ist wie ein Superheldenfilm, nur dass wir Zeilen Code haben, die für Gerechtigkeit kämpfen, anstatt Umhänge zu tragen!\n\n[https://techcrunch.com/2023/07/05/nycs-anti-bias-law-for-hiring-algorithms-goes-into-effect/](https://techcrunch.com/2023/07/05/nycs-anti-bias-law-for-hiring-algorithms-goes-into-effect/)\n\nIntel präsentiert seine neueste Kreation: ein 3D-generatives KI-Modell, das darauf abzielt, die virtuelle Welt zu revolutionieren! Laut der Ankündigung von Intel bringt diese hochmoderne Technologie neue Dimensionen in die KI, indem sie atemberaubende 3D-Modelle erstellt. Mit dieser Innovation bringt Intel das „Denken außerhalb der Box“ auf ein neues Level. Machen Sie sich bereit für eine virtuelle Extravaganz!\n\n[https://www.intel.com/content/www/us/en/newsroom/news/intel-introduces-3d-generative-ai-model.html](https://www.intel.com/content/www/us/en/newsroom/news/intel-introduces-3d-generative-ai-model.html)\n\n## Cybersicherheit\n\nSolarenergie beleuchtet eine dunkle Sicherheitslücke! SecurityWeek deckt auf, dass eine ausgenutzte Schwachstelle in einem Solarstrom-Produkt Energieorganisationen Cyberangriffen aussetzen kann. Es scheint, dass selbst die Sonne nicht sicher vor Hackern ist. Hoffen wir, dass sie eine solare Lösung finden, um unsere Energieversorgungsnetze zu sichern und Hacker in den Schatten zu stellen!\n\n[https://www.securityweek.com/exploited-solar-power-product-vulnerability-could-expose-energy-organizations-to-attacks/](https://www.securityweek.com/exploited-solar-power-product-vulnerability-could-expose-energy-organizations-to-attacks/)\n\nUh-oh, die Cybersicherheitsbehörden schlagen Alarm! Laut The Hacker News ist eine neue Bedrohung aufgetaucht, die uns das digitale Rückgrat erzittern lässt. Es ist an der Zeit, sich festzuschnallen und unsere Cyberverteidigung zu stärken, denn diese hinterlistigen Hacker haben nichts Gutes im Sinn. Bleibt wachsam, Leute, und lassen wir ihre teuflischen Pläne mit unseren Cyber-Superkräften scheitern!\n\n[https://thehackernews.com/2023/07/cybersecurity-agencies-sound-alarm-on.html](https://thehackernews.com/2023/07/cybersecurity-agencies-sound-alarm-on.html)\n\nCISA erkennt eine Steigerung der Cybersicherheit in Agenturberichten! Laut Federal News Network melden sich mehr Agenturen automatisch beim CDM (Continuous Diagnostics and Mitigation) Dashboard. Unsere Cyberverteidigungen werden immer smarter, da die Agenturen sich engagieren. Ein großes Lob an alle, die ein Auge auf die Cybersicherheit haben und die digitale Welt mit jedem Bericht sicherer machen!\n\n[https://federalnewsnetwork.com/cybersecurity/2023/07/cisa-sees-uptick-in-agencies-automatically-reporting-into-cdm-dashboard/](https://federalnewsnetwork.com/cybersecurity/2023/07/cisa-sees-uptick-in-agencies-automatically-reporting-into-cdm-dashboard/)\n\n## Ubiquitous Computing\n\nEs sieht so aus, als würde die USA einen digitalen Zaun gegen China errichten! Laut Cointelegraph gibt es Berichte über Pläne, den Zugang Chinas zu Cloud-Computing-Diensten einzuschränken. Es ist wie ein virtuelles Versteckspiel, aber mit schwerwiegenden geopolitischen Auswirkungen. Schauen wir mal, wer in diesem cloud-basierten Kräftemessen als Sieger hervorgeht. Holt euer Popcorn, Leute, der Technologiekampf heizt sich auf!\n\n[https://cointelegraph.com/news/us-reportedly-plans-to-restrict-china-s-access-to-cloud-computing-services](https://cointelegraph.com/news/us-reportedly-plans-to-restrict-china-s-access-to-cloud-computing-services)\n\nSeltene Erden verursachen Déjà-vu im US-China-Handelsstreit! Intereconomics beleuchtet die Situation und betont die Bedeutung dieser wichtigen Mineralien in verschiedenen Branchen. Es scheint, als wiederhole sich die Geschichte, während seltene Erden zum Brennpunkt dieses fortlaufenden Handelskriegs werden. Machen Sie sich bereit für eine holprige Fahrt, während die USA und China erneut dieses vertraute Terrain navigieren. Werden sie eine Lösung finden oder sich in einer endlosen Schleife drehen? Die Zeit wird es zeigen!\n\n[https://www.intereconomics.eu/contents/year/2019/number/6/article/rare-earths-in-the-trade-dispute-between-the-us-and-china-a-deja-vu.html](https://www.intereconomics.eu/contents/year/2019/number/6/article/rare-earths-in-the-trade-dispute-between-the-us-and-china-a-deja-vu.html)\n\nIBM macht Quantencomputing-Fehler zu etwas Vergangenem! Laut Network World werben sie für Fehlerminderungstechniken, die eine bedeutend bessere Leistung im Quantenbereich versprechen. Es ist wie ein Zauber, um diese lästigen Fehler zu minimieren und das wahre Potenzial des Quantencomputings freizuschalten. Machen Sie sich bereit für einen Quantensprung in der Welt der Technologie!\n\n[https://www.networkworld.com/article/3699789/ibm-touts-error-mitigation-for-greater-quantum-computing-performance.html](https://www.networkworld.com/article/3699789/ibm-touts-error-mitigation-for-greater-quantum-computing-performance.html)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW23-de","image":"./briefs/edw-23/de/thumbnail.png","lang":"de","summary":"Hier sind die neuesten Nachrichten zur digitalen Transformation vom 10. Juli 2023. Der anhaltende Handelskrieg beginnt sich auf Cloud-Dienste auszuwirken. Darüber hinaus gibt es Spekulationen, dass das Hype um generative KI möglicherweise nachlässt. Auf dem Gebiet der Cybersicherheit gibt es Bedenken, ob wir genug tun, um den bösartigen Akteuren einen Schritt voraus zu sein."},{"id":190,"type":"News Brief","title":"2023-7-16","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Künstliche Intelligenz\n\nWurde KI jetzt von der Religion gesegnet? Der Vatikan geht digital mit göttlicher Führung! Gizmodo berichtet, dass Papst Franziskus und der Vatikan ein KI-Ethik-Dokument veröffentlicht haben und somit die heilige Berührung in die Welt der künstlichen Intelligenz bringen. Es ist wie eine technologische Predigt über die Bedeutung ethischer KI-Praktiken. Halleluja für digitale Rechtschaffenheit!\n\n[Weiterlesen](https://gizmodo.com/pope-francis-vatican-releases-ai-ethics-1850583076)\n\nElon Musk schlägt wieder zu, diesmal mit einem KI-Unternehmen! Der Technikmagnat hat ein neues Unternehmen gegründet, das sich auf künstliche Intelligenz konzentriert und \"Die Wahrheit suchen\" wird. Es scheint, dass Musk nicht widerstehen kann, sich in futuristische Unternehmungen zu stürzen. Wird dies ein weiterer Game-Changer sein oder nur eine weitere Trophäe auf seinem futuristischen Hut? Nur die Zeit wird es zeigen!\n\n[Weiterlesen](https://www.cnn.com/2023/07/12/tech/elon-musk-ai-company/index.html)\n\nGenerative KI wird nach einigen Runden von künstlichen Daten \"verrückt\"! Tom's Hardware berichtet, dass generative KI äußerst bizarre Ergebnisse liefert, wenn sie über fünf Iterationen hinweg mit künstlichen Daten trainiert wird. Die Vorstellungskraft der KI scheint wild geworden zu sein und entfesselt eine eigenartige und unerwartete Seite. Hoffen wir, dass sie uns keine künstlich erzeugten Einhörner auf den Straßen herumlaufen lässt!\n\n[Weiterlesen](https://www.tomshardware.com/news/generative-ai-goes-mad-when-trained-on-artificial-data-over-five-times)\n\n## Cybersecurity --> Cyber-Sicherheit\n\nIm Reich der Cyberfestungen krönt Forbes Intel zum amtierenden Champion! Die Liste der am besten gegen Cyberangriffe geschützten Unternehmen Amerikas im Jahr 2023 rückt Intels unvergleichliche Verteidigungsfähigkeiten ins Rampenlicht. Mit ihrer technischen Stärke und einer Festung der Innovation erhebt sich Intel hoch erhobenen Hauptes und ist bereit, jedem digitalen Gegner entgegenzutreten. Sie tragen ihre virtuellen Umhänge und schützen unsere Daten vor Cyberbösewichten mit Stil und Finesse. Es ist kein Wunder, dass Intel das Gesprächsthema in der Welt der Cybersicherheit ist!\n\n[Weiterlesen](https://www.forbes.com/sites/hnewman/2023/06/08/meet-americas-most-cybersecure-companies-2023/?sh=dd8bc202cf60)\n\nDer nationale Cyber-Sicherheitsplan wurde enthüllt! Hilf mit bei der Umsetzung des Plans zur nationalen Cyber-Sicherheitsstrategie. Es handelt sich um ein strategisches Playbook, um uns gegen digitale Bedrohungen zu verteidigen und die Schritte zum Schutz unserer virtuellen Welten aufzuzeigen. Lasst uns uns hinter diesen Plan stellen und die cybernetischen Abwehrmaßnahmen unseres Landes stärken. Vorwärts in eine sicherere digitale Zukunft!\n\n[Weiterlesen](https://www.helpnetsecurity.com/2023/07/13/national-cybersecurity-strategy-implementation-plan-published/)\n\nDer Cybersecurity-Kampf zwischen den USA und China intensiviert sich angesichts wachsender Spannungen. Der Hacker News berichtet über alarmierende E-Mail-Kompromisse innerhalb amerikanischer Regierungsbehörden, was die Besorgnis über den anhaltenden Cyberkrieg weiter anheizt. Es handelt sich um einen hochriskanten Zusammenstoß digitaler Giganten, der die dringende Notwendigkeit robuster Verteidigungsmechanismen und internationaler Zusammenarbeit angesichts fortwährender Cyberbedrohungen unterstreicht.\n\n[Weiterlesen](https://thehackernews.com/2023/07/us-government-agencies-emails.html)\n\n## Edge Computing\n\nInfineon und Edge Impulse schließen sich zusammen, um die Kraft von Edge AI freizusetzen! Edge Computing News gibt an, dass diese dynamische Partnerschaft darauf abzielt, die Edge-AI-Fähigkeiten von Infineon zu erweitern. Es ist wie eine perfekte Verbindung im Technologie-Himmel, indem Infineons Fachkenntnisse mit den innovativen Lösungen von Edge Impulse kombiniert werden. Machen Sie sich bereit für KI am Edge, die die Art und Weise, wie wir Daten verarbeiten, revolutioniert und neue Bereiche der Innovation freischaltet!\n\n[Weiterlesen](https://www.edgecomputing-news.com/2023/07/10/infineon-partners-with-edge-impulse-to-extend-its-edge-ai-capabilities/)\n\nHalte dich fest, denn der Markt für Edge Computing steht vor einem spektakulären Anstieg! Edge Computing News enthüllt, dass diese boomende Branche bis 2028 stolze 111,3 Milliarden Dollar wert sein wird. Es ist wie ein Goldrausch im Bereich des Edge Computings, bei dem Unternehmen um die immense Potenzial der dezentralisierten Rechenleistung konkurrieren. Bereite dich auf einen Paradigmenwechsel vor, wenn der Edge im Mittelpunkt der digitalen Revolution steht!\n\n[Weiterlesen](https://www.edgecomputing-news.com/2023/07/10/edge-computing-market-to-be-worth-111-3-billion-by-2028/)\n\nSeoul Robotics erweitert die Transporttechnologie mit seiner fortschrittlichen 3D-Wahrnehmungstechnologie. EdgeIR berichtet über diese bahnbrechende Innovation, die eine verbesserte Tiefenwahrnehmung in Transportsysteme bringt. Wenn Fahrzeuge beginnen, diese innovative Technologie von Seoul Robotics zu nutzen, können wir in Zukunft sicherere und intelligentere Reisen erwarten.\n\n[Weiterlesen](https://www.edgeir.com/seoul-robotics-develops-3d-perception-tech-to-boost-transportation-systems-20230711)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW24-de","image":"./briefs/edw-24/de/thumbnail.png","lang":"de","summary":"In den Nachrichten zur digitalen Transformation für die Woche vom 17. Juli 2023 bietet Papst Franziskus Richtlinien zur Ethik der Künstlichen Intelligenz an, Intel bleibt Amerikas Cyber-Festung und der Markt für Edge Computing steht vor einer steilen Entwicklung!"},{"id":191,"type":"News Brief","title":"2023-7-23","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Künstliche Intelligenz\n\n\nMeta, der KI-Wegbereiter, begeistert die Tech-Community mit einer bahnbrechenden Veröffentlichung - die Vorstellung von Llama 2, dem neuesten Open-Source-LLM-Modell. Durch die Erschließung der Kraft des Sprachverständnisses für alle Entwickler verspricht diese wegweisende Innovation, KI-Projekte zu revolutionieren. Verabschiede dich von Komplexität und heiße nahtloses KI-Training willkommen. Mache dich bereit, auf der Welle der Open-Source-LLM-Modelle zu reiten und die Zukunft der KI zu erleben! [Weiterlesen](https://www.artificialintelligence-news.com/2023/07/19/meta-launches-llama-2-open-source-llm/)\n\n\nDas Forbes Tech Council schlägt Alarm, da die Generative Künstliche Intelligenz ihr Augenmerk auf persönliche Daten richtet. Ihr fesselnder Artikel geht der bevorstehenden Dateninvasion und ihren potenziellen Konsequenzen auf den Grund. Seien Sie auf den revolutionären Aufstieg von künstlich generierten Inhalten vorbereitet, der Chancen und Herausforderungen im Bereich des Datenschutzes birgt. Bleiben Sie informiert und wachsam, während wir uns in die mutige neue Welt der #GenerativeAI begeben und unsere digitale Zukunft schützen. [Weiterlesen](https://www.forbes.com/sites/forbestechcouncil/2023/07/20/generative-ai-is-coming-for-people-data-are-you-ready/?sh=3e6f95421573)\n\n\nHollywood-Streik wegen Sorgen um künstliche Intelligenz beendet. Der Top-Sommerfilm \"Mission: Impossible - Dead Reckoning\" erforscht das Potenzial und die Risiken von künstlicher Intelligenz. Handelt es sich hierbei um eine warnende Geschichte oder einen Blick in unsere Zukunft? Schauspieler und Autoren befürchten, dass ihre Arbeit und ihr Erscheinungsbild ersetzt werden. Bleiben Sie dran für Updates. [Weiterlesen](https://www.wired.com/story/mission-impossible-dead-reckoning-is-the-perfect-ai-panic-movie/)\n\n## Cybersicherheit\n\n\nDer Guardian berichtet von einem großen Cyberangriff, bei dem chinesische Hacker US-Beamte ins Visier genommen haben, darunter auch den Botschafter in Peking. Der Vorfall wirft Bedenken in Bezug auf Cybersicherheit und internationale Beziehungen auf. Die Behörden untersuchen das Ausmaß des Angriffs und seine potenziellen Auswirkungen. Bleiben Sie dran für weitere Updates zu dieser bedeutsamen und besorgniserregenden Entwicklung. [Weiterlesen](https://www.theguardian.com/us-news/2023/jul/20/ambassador-to-beijing-among-us-officials-hit-by-chinese-hackers)\n\n\nBreaking News: Ein furchterregender Cyberangriff entfaltet sich! Kritische Infrastruktur unter Belagerung, da Hacker eine gefährliche Citrix Zero-Day-Schwachstelle ausnutzen. Panik ergreift Cybersicherheitsexperten, die gegen die Zeit rennen, um die verheerenden Folgen des Angriffs zu verhindern. Bereiten Sie sich auf die Konsequenzen vor und halten Sie Ihre digitalen Abwehrmaßnahmen bereit! #CitrixZeroDay #CybersecurityEmergency [Weiterlesen](https://www.securityweek.com/citrix-zero-day-exploited-against-critical-infrastructure-organization/)\n\n\nCyware bringt alarmierende Nachrichten über \"HotRAT\", ein verborgenes Skript, das in geknackter Software lauert. Cyberkriminelle nutzen diese betrügerische Taktik aus, um arglose Benutzer zu kompromittieren. Die Bedrohungslandschaft verschärft sich und mahnt zur Vorsicht beim Herunterladen von geknackten Programmen. Schütze deine digitale Zuflucht und vermeide, Opfer dieses hinterlistigen Tricks zu werden. Bleib informiert und bleib sicher! #HotRAT #CybersecurityThreat [Weiterlesen](https://cyware.com/news/hotrat-as-hidden-script-in-cracked-software-b2baa5b3)\n\n## Datenverwaltung\n\n\nTDWI enthüllt den Beginn einer Datenmanagement-Revolution mit 'Arch-All Data Fabric'. In diesem bahnbrechenden Bericht präsentieren Experten den Bauplan für ein Datenmanagement der nächsten Generation. Erleben Sie die Integration verschiedenster Datenquellen, die Unternehmen zu bisher ungekannten Höhen treibt. Bleiben Sie dran für Einblicke in die datengetriebene Zukunft! #DataFabric #NextGenDataManagement [Weiterlesen](https://tdwi.org/articles/2023/07/20/arch-all-data-fabric-how-to-architect-next-generation-data-management.aspx)\n\nHammerspace erreicht einen bahnbrechenden Meilenstein mit 56,7 Millionen Dollar an erster institutioneller Finanzierung. Diese transformative Errungenschaft eröffnet versteckte Geschäftsmöglichkeiten in unstrukturierten Daten. Entdecke die Kraft ihrer innovativen Lösungen, die eine nahtlose Verwaltung unstrukturierter Daten für Unternehmen ermöglichen. Bereite dich auf eine neue Ära der Datennutzung und des Wachstums vor! #Hammerspace #DataInnovation #FundingSuccess [Weiterlesen](https://hammerspace.com/hammerspace-raises-56-7m-in-first-institutional-funding-unlocks-business-opportunities-hidden-in-unstructured-data/)\n\n\nLenovos großartige Enthüllung: Neue Datenverwaltungslösungen für die beschleunigte Verarbeitung von KI-Arbeitslasten entwickelt! Datanami berichtet über die bahnbrechenden Angebote, die Unternehmen mit KI-gesteuertem Potenzial ausstatten. Tauchen Sie ein in die neuesten Innovationen, die eine Revolutionierung der Datenverarbeitung für KI-Projekte versprechen. Begrüßen Sie eine neue Ära der Effizienz und Intelligenz in der Technologielandschaft! #Lenovo #Datenmanagement #KI-Arbeitslasten [Weiterlesen](https://www.datanami.com/this-just-in/lenovo-unveils-new-data-management-solutions-to-enable-ai-workloads/)\n\n## Embracing Digital Podcast\n\n\nIn dieser Episode besucht Darren ein Interview, das er mit chatGPT geführt hat, und setzt es mit einem ähnlichen Interview mit Google Bard fort. Der Vergleich erzeugt Spannung, besonders wenn er mit den Erkenntnissen aus Episode 122 über die Akzeptanz der digitalen Transformation kombiniert wird. [Episode 147](https://www.embracingdigital.org/episode-EDT147) [Episode 122](https://www.embracingdigital.org/episode-EDT122)\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW25-de","image":"./briefs/edw-25/de/thumbnail.png","lang":"de","summary":"Diese Woche, am 24. Juli 2023, gab es in den Nachrichten zur digitalen Transformation Fortschritte bei generativer Künstlicher Intelligenz am Arbeitsplatz sowie einen Cyberangriff aus China auf die US-Botschaft. Darüber hinaus werden Datenstoffe allmählich als Trend erkennbar."},{"id":192,"type":"News Brief","title":"2023-7-30","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Fortgeschrittene Kommunikation\n\n\nIntel und Ericsson arbeiten zusammen, um eine bessere Internetverbindung und Geräteerlebnisse zu schaffen. Ihre Zusammenarbeit zielt darauf ab, der steigenden Nachfrage nach schnellem Internet und zuverlässigen Verbindungen gerecht zu werden und verspricht spannende technologische Fortschritte. Dank ihrer gebündelten Expertise können sich Nutzer auf schnellere Downloads, reibungslosere Videoanrufe und eine verbesserte Gesamtleistung für Handys, Computer und andere Geräte freuen. Machen Sie sich bereit für eine vernetztere und nahtlosere Zukunft! [Weiterlesen](http://finance.yahoo.com/news/intel-ericsson-expand-collaboration-advance-201000466.html)\n\n\nMachen Sie sich bereit für den 5G-Boom! Laut einer neuen Studie wird der globale Markt für die Integration des 5G-Systems bis 2032 unglaubliche 115,60 Milliarden US-Dollar erreichen. Unternehmen setzen alles daran, diese superschnelle Internet-Technologie für intelligente Städte, das Gesundheitswesen und vieles mehr zu übernehmen. Hersteller rüsten sich ebenfalls mit coolen Technologien wie Robotern und Big Data auf. Die Nachfrage nach 5G-Integrationsdiensten steigt rapide und verspricht aufregende Zeiten voraus! [Weiterlesen](https://finance.yahoo.com/news/global-5g-system-integration-market-100000051.html)\n\n\nAufregende Neuigkeiten von T-Mobile! Ihre revolutionäre 5G-Technologie verspricht ultraschnelles Internet mit Geschwindigkeiten von 3,3 GBits/Sekunde und nahtloser Konnektivität. Mit dieser Innovation wird das Herunterladen und Streamen blitzschnell sein und Videoanrufe und Online-Gaming werden reibungsloser verlaufen. Mach dich bereit für eine neue Ära der Konnektivität und einzigartige Internet-Erlebnisse! [Weiterlesen](https://voip.review/2023/07/26/tmobile-introduces-groundbreaking-5g-tech/)\n\n## Cybersicherheit\n\n\nGPT und Cybersicherheit: Die Zukunft ist aufregend und besorgniserregend! Experten sind voller Begeisterung über GPT, ein leistungsstarkes KI-System, das die Cybersicherheit revolutionieren soll. Una-May O'Reilly's Vortrag hat Licht auf die Auswirkungen von GPT auf die Sicherheit geworfen und gezeigt, dass es die Abwehrmaßnahmen innovativer und effizienter machen wird. Aber es gibt einen Haken: Auch Hacker können es nutzen! KI könnte beide Seiten in einem Cyber-Wettrüsten unterstützen und uns in eine unbekannte Situation bringen. Bereiten Sie sich auf eine aufregende und besorgniserregende Zukunft der Cybersicherheit vor! [Weiterlesen](https://www.forbes.com/sites/johnwerner/2023/07/28/whos-the-authority-on-gpt-and-cybersecurity-is-it--gpt/)\n\n\nNeue Vorschriften der Securities and Exchange Commission (SEC) verlangen von börsennotierten Unternehmen, innerhalb von vier Tagen jegliche Cybersecurity-Verstöße offenzulegen, die sich auf ihre Gewinne auswirken könnten. Dieser Schritt zielt darauf ab, Investoren zu schützen und mehr Transparenz in Bezug auf Cybersecurity-Risiken zu schaffen. Verzögerungen sind in Fällen von nationalen Sicherheitsrisiken zulässig. Die Regelung fordert außerdem von Unternehmen, Informationen über ihr Cybersecurity-Management und ihre Expertise zu teilen. [Weiterlesen](https://apnews.com/article/sec-cybersecurity-breach-disclosure-risk-hacking-bb6252463637793bfdc8ace5bfcbe7df)\n\n\nIn einer aktuellen Meldung haben Cybersicherheitsagenturen in Australien und den USA vor einer kritischen Schwachstelle in Webanwendungen gewarnt. Cyberkriminelle können die Schwachstelle der Unsicheren Direkten Objektreferenz ausnutzen, um unberechtigten Zugriff auf vertrauliche Informationen zu erlangen. Seien Sie wachsam und schützen Sie Ihre Daten mit zuverlässigen Authentifizierungs- und Autorisierungsprotokollen. [Weiterlesen](https://thehackernews.com/2023/07/cybersecurity-agencies-warn-against.html)\n\n## Ubiquitous Computing\n\nUbiquitous Computing (Allgegenwärtiges Rechnen) ist ein Konzept, das sich auf die Vision einer allgegenwärtigen Verfügbarkeit von Computern bezieht. Es geht davon aus, dass Computer eingebettet und in verschiedene Alltagsgegenstände integriert sind, was es den Menschen ermöglicht, jederzeit und überall auf computergesteuerte Funktionen zuzugreifen. Dieses Konzept hat das Potenzial, die Art und Weise, wie wir mit Technologie interagieren, zu revolutionieren und eine nahtlose Integration von Technologie in unseren Alltag zu ermöglichen.\n\n\nKI wird zunehmend im Programmieren eingesetzt, um Effizienz zu steigern und Kosten zu reduzieren. Allerdings kann eine Abhängigkeit vom vorherigen Code zu Fehlern führen und die Kreativität in der Anwendungsentwicklung beeinträchtigen. Von KI generierter Code ist möglicherweise nicht für die Plattform optimiert, daher ist es wichtig, die Vorteile von KI mit menschlichem Fachwissen für effiziente Anwendungen auszugleichen. [Weiterlesen](https://www.infoworld.com/article/3703611/the-lost-art-of-cloud-application-engineering.html)\n\n\nMicrosoft führend bei Cloud-basierten KI-Arbeitsabläufen, sagt Nadella! Laut seinem CEO Satya Nadella übernimmt Microsoft die Führung bei Cloud-basierten KI-Arbeitsabläufen. Die Expertise des Technologie-Giganten in KI-gesteuerten Aufgaben sorgt in der Industrie für Aufsehen. Durch die Nutzung der Kraft der Cloud liefert Microsoft fortschrittliche KI-Lösungen für Unternehmen und Benutzer. Mit ihrem innovativen Ansatz gestalten sie die Zukunft der KI-Technologie. [Weiterlesen](https://www.cnbc.com/2023/07/25/microsoft-is-in-the-lead-with-cloud-based-ai-workloads-nadella-says.html)\n\n\nAlibaba hat sich mit Meta's Llama AI zusammengetan, um innovative Software zu entwickeln. Diese Zusammenarbeit zwischen Alibaba Cloud, einer Abteilung des chinesischen Giganten, und Llama 2's AI-Modell setzt neue Trends in der Technologiebranche. Diese Partnerschaft wird China zum Vorreiter in der kostenfreien Programmierung machen und spannende, auf KI basierende Innovationen auf den Markt bringen. Machen Sie sich bereit, die Zukunft der Softwareentwicklung zu erleben! [Weiterlesen](https://fagenwasanni.com/news/alibabas-cloud-computing-service-utilizes-metas-ai-model-llama-for-software-development/82096/)\n\n## Podcast zur digitalen Transformation umarme\n\nHört euch die aktuelle Episode an, in der Darren ein aufschlussreiches Gespräch mit dem besonderen Gast Jared Shepard, dem CEO von Hypori, führt. Das Interview konzentriert sich auf das wichtige Thema der Sicherheit von Remote-Mitarbeitern durch mobile Virtualisierung. Jared Shepards einzigartiger Werdegang vom Schulabbrecher zum CEO fügt der Diskussion eine inspirierende Dimension hinzu.\n\n[Episodce 148](https://www.embracingdigital.org/episode-EDT148)\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW26-de","image":"./briefs/edw-26/de/thumbnail.png","lang":"de","summary":"Hier sind die neuesten Aktualisierungen im Bereich der Digitalen Transformation für die Woche vom 31. Juli 2023. Dies beinhaltet Neuigkeiten über die Partnerschaft zwischen Ericsson und Intel zur Weiterentwicklung der 5G-Technologie, die Bemühungen von ChatGPT im Kampf gegen Cyberkriminalität und das Aufkommen KI-gesteuerter Softwareentwicklung."},{"id":193,"type":"News Brief","title":"2023-8-6","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Künstliche Intelligenz\n\n\nEs gibt zunehmende Besorgnis unter Fachleuten der Technologie über den Gebrauch von mit Künstlicher Intelligenz (KI) verbesserten Bildern. The Guardian hat dieses Problem untersucht, da die Befürchtung besteht, dass durch KI veränderte Visualisierungen die öffentliche Meinung beeinflussen und sogar wichtige Fakten verzerren könnten, die für das demokratische System unerlässlich sind. Dies könnte ernsthaft das Vertrauen und die Wahrheit beeinträchtigen und eine Debatte darüber auslösen, wie Wahlen vor der Manipulation durch KI geschützt werden können. [Weiterlesen](https://www.theguardian.com/technology/2023/aug/03/ai-enhanced-images-a-threat-to-democratic-processes-experts-warn\n)\n\n\nEin kürzlich von Yahoo Finance veröffentlichter Artikel berichtet, dass Wendy's KI einsetzt, um ihre Drive-Thrus zu revolutionieren. Mit automatischer Bestellung und personalisierten Speisekarten verändert der Fast-Food-Gigant das Spiel für schnelle Mahlzeiten. Dieser technologische Ansatz im Bereich der Gastronomie gibt einen Einblick in die Zukunft des Fast-Food-Dinnens. Es wird interessant sein zu sehen, wie Kunden auf diese digitale Transformation reagieren. [Weiterlesen](https://finance.yahoo.com/news/wendys-latest-fast-food-company-210910771.html\n)\n\n\nDie Auswirkungen der künstlichen Intelligenz auf asiatisch-amerikanische Fachkräfte ist ein besorgniserregender Trend. Arbeitsplatzverluste stehen kurz bevor und werfen Fragen darüber auf, wie die Gesellschaft die komplexe Gleichung von Technologie, Ethnizität und Arbeitsmarkt-Dynamik angehen wird. Eine genaue Untersuchung der Auswirkungen von künstlicher Intelligenz ist notwendig. [Weiterlesen](https://www.nbcnews.com/news/asian-america/asian-american-workers-heavily-affected-ai-rcna98179\n)\n\n## Cybersicherheit\n\n\nNBC News hat über einen besorgniserregenden Cyberangriff auf ein Krankenhausnetzwerk berichtet, der zur Trennung seiner nationalen Computerinfrastruktur geführt hat. Der Vorfall gefährdet die Patientenversorgung, und die Behörden arbeiten daran, die Situation einzudämmen. Dieser Vorfall verdeutlicht die Bedeutung der Datensicherheit im Gesundheitswesen in der digitalen Ära. [Weiterlesen](https://www.nbcnews.com/tech/security/hackers-force-hospital-system-take-national-computer-system-offline-rcna98212\n)\n\n\nEntwickler, die das weit verbreitete NPM-System nutzen, werden von schädlichen Code-Paketen angezielt, die die Sicherheit der Software gefährden können. Dies ist eine ernsthafte Bedrohung, die identifiziert und abgeschwächt werden muss, um Cyberangriffe zu verhindern. Die Cybersicherheitsgemeinschaft ist besorgt und betont die Notwendigkeit sichererer Software-Lieferketten, um eine weitere Infiltration dieser Pakete zu verhindern. [Weiterlesen](https://thehackernews.com/2023/08/malicious-npm-packages-found.html\n)\n\n\nCISA hat einen strategischen Plan namens \"Blaupause für Cybersicherheit\" veröffentlicht, um die nationale digitale Infrastruktur zu stärken. Es konzentriert sich auf anpassungsfähige Verteidigung, Vorfallreaktion und Innovation, um einen undurchdringlichen digitalen Schutzschild zu schaffen. Dieser Plan ist ein unverzichtbarer Leitfaden für alle, die sich für Technologie-Sicherheit interessieren. [Weiterlesen](https://www.securitysystemsnews.com/article/cisa-releases-cybersecurity-strategic-plan\n)\n\n## Edge Computing\n\n\nDie Kombination von Quantencomputing und IoT-Geräten birgt Sicherheitsbedenken. Die Leistung der Quantentechnologie und das Potenzial von IoT-Geräten zur Übernahme erzeugen eine neue Herausforderung. Die Angst vor dem Knacken von Verschlüsselungen wächst, während das Quantencomputing voranschreitet und Botnets DDoS-Angriffe unter Verwendung gehackter IoT-Geräte starten. Die Netzwerkverteidigung muss gestärkt werden, um mit dieser doppelten Bedrohung umgehen zu können. [Weiterlesen](https://www.securitymagazine.com/articles/99604-the-impact-of-quantum-computers-and-iot-devices-on-network-security\n)\n\n\nYahoo Finance hat eine aufregende Entwicklung in der Technologiebranche enthüllt: Edge Computing, das Internet der Dinge (IoT) und Künstliche Intelligenz (KI) verschmelzen zu einem Markt, der bis 2025 voraussichtlich 230 Milliarden Dollar wert sein wird. Edge Computing ermöglicht es mehr als 75 Milliarden IoT-Geräten, Daten in Echtzeit zu verarbeiten, während KIs kognitive Fähigkeiten voraussichtlich die Produktivität um bis zu 40% steigern. Diese Revolution wird voraussichtlich die Daten-Dynamik und Entscheidungsfindung in verschiedenen Branchen neu gestalten. [Weiterlesen](https://finance.yahoo.com/news/edge-computing-iot-ai-revolutionize-011500739.html\n)\n\n\nDie asiatisch-pazifische Region durchläuft eine signifikante digitale Transformation, die durch das Internet der Dinge (IoT) und Künstliche Intelligenz (KI) vorangetrieben wird. Mobile Edge Computing (MEC) führt diese Veränderung an, indem es beispiellose Konnektivitäts- und Automatisierungsniveaus durch die Näherung der Berechnung an die Datenquelle bietet. MEC hat ein immenses Potenzial, Branchen zu revolutionieren, das Leben zu verbessern und das digitale Schicksal der Region voranzutreiben, bringt jedoch auch Sicherheitsherausforderungen mit sich, die robuste Schutzmaßnahmen und aktualisierte Vorschriften erfordern. [Weiterlesen](https://fagenwasanni.com/news/innovations-in-iot-and-ai-the-role-of-mobile-edge-computing-in-asia-pacifics-technological-advancements/48663/\n)\n\n## Digitale Transformationspodcast umarmen\n\n\nIn der neuesten Folge von \"Digitalen Wandel umarmen\" wird Darren von Leland Brown, einem führenden Ingenieur bei Capgemini und einem früheren Gast der Show, begleitet. Sie diskutieren die bevorstehenden Fortschritte der 5G-Technologie und deren Auswirkungen auf das Mobile Edge Computing (MEC) im US-Verteidigungsministerium. [Weiterlesen](https://embracingdigital.org/episode-EDT149\n)\n\nKünstliche Intelligenz\n\n\nEs gibt wachsende Besorgnis unter Technologieexperten über den Einsatz von KI-optimierten Bildern. The Guardian hat dieses Problem untersucht, da die Sorge besteht, dass KI-veränderte Bilder die öffentliche Meinung beeinflussen und wichtige Fakten verzerren könnten, die für das demokratische System unerlässlich sind. Dies könnte das Vertrauen und die Wahrheit ernsthaft beeinträchtigen und zu einer Debatte darüber führen, wie man Wahlen vor der Manipulation durch KI schützen kann. [Weiterlesen](https://www.theguardian.com/technology/2023/aug/03/ai-enhanced-images-a-threat-to-democratic-processes-experts-warn\n)\n\n\nEin kürzlich erschienener Artikel von Yahoo Finance berichtete, dass Wendy's, ein amerikanisches Fast-Food-Restaurant, KI verwendet, um ihre Drive-Thrus zu revolutionieren. Mit automatisierter Bestellung und personalisierten Menüs verändert der Fast-Food-Riese das Spiel für schnelle Mahlzeiten. Dieser technologische Ansatz des Lebensmittel-Services gewährt einen Einblick in die Zukunft des Fast-Food-Essens. Es wird interessant sein zu sehen, wie Kunden auf diese digitale Transformation reagieren. [Weiterlesen](https://finance.yahoo.com/news/wendys-latest-fast-food-company-210910771.html\n)\n\n\nDer Einfluss von künstlicher Intelligenz (KI) auf asiatisch-amerikanische Fachkräfte ist ein besorgniserregender Trend. Arbeitsplatzverluste stehen bevor und werfen Fragen darüber auf, wie die Gesellschaft mit der komplexen Verbindung zwischen Technologie, Ethnizität und Arbeitsdynamik umgehen wird. Eine genauere Untersuchung der Auswirkungen der KI ist notwendig. [Weiterlesen](https://www.nbcnews.com/news/asian-america/asian-american-workers-heavily-affected-ai-rcna98179\n)\n\n## Cybersicherheit\n\n\nNBC Nachrichten haben von einem besorgniserregenden Cyberangriff auf ein Krankenhausnetzwerk berichtet, der zur Abschaltung seiner nationalen Computerinfrastruktur geführt hat. Der Vorfall gefährdet die Patientenversorgung und die Behörden arbeiten daran, die Situation einzudämmen. Dieser Vorfall unterstreicht die Bedeutung der Datensicherheit im Gesundheitswesen in der digitalen Ära. [Weiterlesen](https://www.nbcnews.com/tech/security/hackers-force-hospital-system-take-national-computer-system-offline-rcna98212\n)\n\n\nEntwickler, die das weit verbreitete NPM-System nutzen, werden von schädlichen Code-Paketen ins Visier genommen, die die Sicherheit der Software gefährden können. Dies ist eine ernsthafte Bedrohung, die identifiziert und abgewehrt werden muss, um Cyberangriffe zu verhindern. Die Cybersecurity-Gemeinschaft ist besorgt und betont die Notwendigkeit sichererer Software-Lieferketten, um eine weitere Infiltration dieser Pakete zu verhindern. [Weiterlesen](https://thehackernews.com/2023/08/malicious-npm-packages-found.html\n)\n\n\nCISA hat einen strategischen Plan namens \"Blueprint for Cybersecurity\" veröffentlicht, um die nationale digitale Infrastruktur zu stärken. Er konzentriert sich auf adaptive Verteidigung, Vorfallreaktion und Innovation, um einen undurchdringlichen digitalen Schutzschild zu schaffen. Dieser Plan ist ein unverzichtbarer Leitfaden für alle, die sich für Technologiesicherheit interessieren. [Weiterlesen](https://www.securitysystemsnews.com/article/cisa-releases-cybersecurity-strategic-plan\n)\n\n## Edge Computing\n\nEdge Computing, auch bekannt als Nebenstellenrechenzentrum, bezieht sich auf die dezentrale Datenaufbereitung und -verarbeitung an der \"Edge\" eines Netzwerks, näher an den Nutzern und Geräten. Im Gegensatz zur traditionellen Cloud Computing-Infrastruktur, bei der Daten an zentralisierten Standorten verarbeitet werden, werden beim Edge Computing die Daten lokal oder in \"Mini-Rechenzentren\" in der Nähe der Endnutzer verarbeitet.\n\nDurch den Einsatz von Edge Computing können Latenzzeiten und Bandbreitenprobleme reduziert werden, da die Verarbeitung auf Ortsebene erfolgt. Dies ist vor allem in Echtzeit-Anwendungen wie autonomem Fahren, Augmented Reality oder der Industrieautomation von Vorteil. Zudem ermöglicht es eine größere Datensicherheit und Privatsphäre, da sensible Daten nicht über große Entfernungen oder das Internet übertragen werden müssen.\n\nEdge Computing wird zunehmend wichtiger, da immer mehr Geräte vernetzt sind und eine hohe Rechenleistung erfordern. Es ergänzt die Cloud Computing-Infrastruktur und ermöglicht eine schnellere, effizientere und zuverlässigere Datenverarbeitung.\n\n\nDie Kombination aus Quantencomputing und IoT-Geräten sorgt für Sicherheitsbedenken. Die Leistung der Quantentechnologie und das Potenzial von IoT-Geräten, gehackt zu werden, stellen eine neue Herausforderung dar. Die Angst vor geknackter Verschlüsselung wächst mit dem Fortschritt im Quantencomputing und der Verwendung gehackter IoT-Geräte für DDoS-Angriffe durch Botnets. Netzwerkverteidigungen müssen gestärkt werden, um mit dieser doppelten Bedrohung umzugehen. [Weiterlesen](https://www.securitymagazine.com/articles/99604-the-impact-of-quantum-computers-and-iot-devices-on-network-security\n)\n\n\nYahoo Finance hat eine aufregende Entwicklung in der Technologiebranche bekannt gegeben, bei der Edge Computing, das Internet der Dinge (IoT) und künstliche Intelligenz (AI) fusionieren, um einen Markt im Wert von 230 Milliarden US-Dollar bis 2025 zu schaffen. Edge Computing ermöglicht es mehr als 75 Milliarden IoT-Geräten, Daten in Echtzeit zu verarbeiten, während die kognitiven Fähigkeiten von AI voraussichtlich die Produktivität um bis zu 40% steigern werden. Diese Revolution steht kurz davor, die Daten-Dynamik und Entscheidungsfindung in verschiedenen Branchen neu zu gestalten. [Weiterlesen](https://finance.yahoo.com/news/edge-computing-iot-ai-revolutionize-011500739.html\n)\n\n\nDie Asien-Pazifik-Region erlebt eine bedeutende digitale Transformation, angetrieben durch das Internet der Dinge (IoT) und Künstliche Intelligenz (KI). Mobile Edge Computing (MEC) führt diese Veränderung an und bietet unvergleichliche Maßstäbe an Konnektivität und Automatisierung, indem Berechnungen näher an die Datenquelle gebracht werden. MEC hat ein immenses Potenzial, um Industrien zu revolutionieren, das Leben zu verbessern und die digitale Zukunft der Region voranzutreiben, bringt aber auch Sicherheitsherausforderungen mit sich, die robuste Sicherheitsvorkehrungen und aktualisierte Vorschriften erfordern. [Weiterlesen](https://fagenwasanni.com/news/innovations-in-iot-and-ai-the-role-of-mobile-edge-computing-in-asia-pacifics-technological-advancements/48663/\n)\n\n## Der Podcast zur digitalen Transformation umarmen\n\n\nIn der neuesten Folge von \"Embracing Digital Transformation\" wird Darren von Leland Brown, einem leitenden Ingenieur bei Capgemini und ehemaligem Gast in der Show, begleitet. Sie diskutieren die bevorstehenden Fortschritte der 5G-Technologie und deren Auswirkungen auf Mobile Edge Computing (MEC) im US-Verteidigungsministerium. [Weiterlesen](https://embracingdigital.org/episode-EDT149\n)\n\nKünstliche Intelligenz\n\n\nEs gibt wachsende Besorgnis unter Technologieexperten über die Verwendung von KI-gesteigerten Bildern. Der Guardian hat dieses Problem untersucht, da es eine Sorge gibt, dass KI-veränderte visuelle Darstellungen die öffentliche Meinung beeinflussen und sogar wichtige Fakten verzerren könnten, die für das demokratische System unerlässlich sind. Dies könnte ernsthaft das Vertrauen und die Wahrheit beeinträchtigen und zu einer Debatte darüber führen, wie Wahlen vor der Manipulation durch KI geschützt werden können. [Weiterlesen](https://www.theguardian.com/technology/2023/aug/03/ai-enhanced-images-a-threat-to-democratic-processes-experts-warn\n)\n\n\nEin kürzlich erschienener Artikel von Yahoo Finance berichtete, dass Wendy's, ein amerikanisches Fast-Food-Restaurant, KI verwendet, um ihre Drive-Thrus zu revolutionieren. Mit automatisierten Bestellungen und personalisierten Menüs ändert der Fast-Food-Riese das Spiel für schnelle Mahlzeiten. Dieser technologische Ansatz für den Lebensmittel-Service gewährt einen Einblick in die Zukunft des Fast-Food-Essens. Es wird interessant sein zu sehen, wie Kunden auf diese digitale Transformation reagieren. [Weiterlesen](https://finance.yahoo.com/news/wendys-latest-fast-food-company-210910771.html\n)\n\n\nDie Auswirkungen der künstlichen Intelligenz auf asiatisch-amerikanische Fachkräfte sind ein besorgniserregender Trend. Stellenverluste stehen groß im Raum und werfen Fragen auf, wie die Gesellschaft die komplexe Gleichung von Technologie, Ethnizität und Arbeitskräftedynamik angehen wird. Eine genauere Untersuchung der Auswirkungen von KI ist notwendig. [Weiterlesen](https://www.nbcnews.com/news/asian-america/asian-american-workers-heavily-affected-ai-rcna98179\n)\n\n## Cybersicherheit\n\n\nNBC News hat einen besorgniserregenden Cyberangriff auf ein Krankenhausnetzwerk gemeldet, der zur Abschaltung seiner nationalen Computerinfrastruktur geführt hat. Der Verstoß gefährdet die Patientenversorgung, und die Behörden arbeiten daran, die Situation einzudämmen. Dieser Vorfall verdeutlicht die Bedeutung der Datensicherheit im Gesundheitswesen in der digitalen Ära. [Weiterlesen](https://www.nbcnews.com/tech/security/hackers-force-hospital-system-take-national-computer-system-offline-rcna98212\n)\n\n\nEntwickler, die das weit verbreitete NPM-System nutzen, werden von schädlichen Codepaketen ins Visier genommen, die die Sicherheit der Software gefährden können. Dies ist eine ernste Bedrohung, die erkannt und minimiert werden muss, um Cyberangriffe zu verhindern. Die Cybersicherheitsgemeinschaft ist besorgt und betont die Notwendigkeit sichererer Software-Lieferketten, um eine weitere Infiltration dieser Pakete zu verhindern. [Weiterlesen](https://thehackernews.com/2023/08/malicious-npm-packages-found.html\n)\n\n\nDie CISA hat einen strategischen Plan namens \"Blaupause für Cybersicherheit\" veröffentlicht, um die nationale digitale Infrastruktur zu stärken. Er konzentriert sich auf adaptive Verteidigung, Vorfallreaktion und Innovation, um einen undurchdringlichen digitalen Schutzschild zu schaffen. Dieser Plan ist eine unverzichtbare Anleitung für Interessierte an Technologiesicherheit. [Weiterlesen](https://www.securitysystemsnews.com/article/cisa-releases-cybersecurity-strategic-plan\n)\n\n# Edge Computing\n\nEdge Computing bezieht sich auf die dezentrale Verarbeitung von Daten sowie auf die Durchführung von Berechnungen, die direkt an der Peripherie eines Netzwerks oder an einem Gerät stattfinden, anstatt in der Cloud oder einem entfernten Rechenzentrum. Diese Art des Computings bietet eine Reihe von Vorteilen, darunter geringe Latenzzeiten, erhöhte Sicherheit und eine effiziente Nutzung der Netzwerkbandbreite. Durch die Verlagerung von rechenintensiven Aufgaben auf die Netzwerkperipherie können Anwendungen schneller und zuverlässiger ausgeführt werden. Im Zeitalter des Internet of Things (IoT) gewinnt Edge Computing zunehmend an Bedeutung, da es eine schnellere und präzisere Datenanalyse in Echtzeit ermöglicht.\n\n\nDie Kombination von Quantencomputing und IoT-Geräten löst Sicherheitsbedenken aus. Die Leistung der Quantentechnologie und das Potenzial von IoT-Geräten, gehijackt zu werden, stellen eine neue Herausforderung dar. Die Entschlüsselung wird zur wachsenden Angst, während sich das Quantencomputing weiterentwickelt und Botnets DDoS-Angriffe mit gehackten IoT-Geräten starten. Netzwerkverteidigungen müssen gestärkt werden, um dieser doppelten Bedrohung standzuhalten. [Weiterlesen](https://www.securitymagazine.com/articles/99604-the-impact-of-quantum-computers-and-iot-devices-on-network-security\n)\n\n\nYahoo Finance hat eine aufregende Entwicklung in der Technologiebranche enthüllt, indem sich Edge Computing, IoT und KI zu einem Markt vereinen, der bis 2025 voraussichtlich 230 Milliarden Dollar wert sein wird. Edge Computing ermöglicht es mehr als 75 Milliarden IoT-Geräten, Daten in Echtzeit zu verarbeiten, während die kognitiven Fähigkeiten von KI die Produktivität um bis zu 40 % steigern sollen. Diese Revolution wird die Datendynamik und die Entscheidungsfindung in verschiedenen Branchen neu gestalten. [Weiterlesen](https://finance.yahoo.com/news/edge-computing-iot-ai-revolutionize-011500739.html\n)\n\n\nDie asiatisch-pazifische Region befindet sich inmitten einer bedeutenden digitalen Transformation, die durch das Internet der Dinge (IoT) und Künstliche Intelligenz (KI) vorangetrieben wird. Mobile Edge Computing (MEC) führt diese Veränderung an und bietet unvergleichliche Maßstäbe an Konnektivität und Automatisierung, indem es die Berechnung näher an die Datenquelle bringt. MEC hat ein immenses Potenzial, um Branchen zu revolutionieren, das Leben zu verbessern und das digitale Schicksal der Region voranzutreiben, aber es stellt auch Sicherheitsherausforderungen dar, die robuste Sicherheitsvorkehrungen und aktualisierte Vorschriften erfordern. [Weiterlesen](https://fagenwasanni.com/news/innovations-in-iot-and-ai-the-role-of-mobile-edge-computing-in-asia-pacifics-technological-advancements/48663/\n)\n\n## Der Podcast zur digitalen Transformation umarmen\n\nIn der neuesten Episode von \"Embracing Digital Transformation\" wird Darren von Leland Brown, einem leitenden Ingenieur bei Capgemini und ehemaligen Gast der Show, begleitet. Gemeinsam besprechen sie die bevorstehenden Fortschritte der 5G-Technologie und deren Auswirkungen auf das Mobile Edge Computing (MEC) im US-Verteidigungsministerium. [Weiterlesen](https://embracingdigital.org/episode-EDT149\n)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW27-de","image":"./briefs/edw-27/de/thumbnail.png","lang":"de","summary":"In der Woche des 7. August 2023 Podcasts gab es mehrere bedeutende Entwicklungen in Bezug auf die digitale Transformation. Insbesondere haben Fast-Food-Ketten begonnen, KI einzusetzen, um dem Arbeitskräftemangel entgegenzuwirken, Krankenhäuser stehen vor Cyberangriffen und Mobile Edge Computing-Plattformen kommen zum Vorschein."},{"id":194,"type":"News Brief","title":"2023-8-13","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Ubiquitäres Computing\n\n\nAufgrund einer kürzlichen Verletzung in der Microsoft Azure Cloud überprüft das Ministerium für Innere Sicherheit cloudbasierte Identitäts- und Authentifizierungssysteme, um böswillige Angriffe auf Cloud-Computing-Umgebungen zu verhindern, entsprechend den jüngsten Sicherheitsverletzungen. Das Ziel besteht darin, Empfehlungen zur Verbesserung der Kontosicherheit zu geben und zukünftige Verstöße zu verhindern. [Weiterlesen](https://www.reuters.com/technology/us-cyber-safety-review-board-assess-online-intrusion-microsoft-exchange-dhs-2023-08-11/)\n\n\nDie potenzielle Revolutionierung mehrerer Branchen, insbesondere der künstlichen Intelligenz, durch die Erhöhung der Verarbeitungsgeschwindigkeit des Quantencomputings ist aufregend. Aufgrund der empfindlichen Natur der Qubits ist es jedoch eine schwierige Aufgabe, Quantencomputer zu schaffen. Nichtsdestotrotz bleiben Branchenexperten hinsichtlich des Potenzials des Quantencomputings zur Förderung bedeutender Fortschritte in verschiedenen Bereichen hoffnungsvoll. [Weiterlesen](https://www.scmp.com/magazines/post-magazine/long-reads/article/3230746/quantum-computing-could-give-ai-rocket-fuel-it-needs-become-transformative-its-not-there-yet)\n\n\nPräsident Biden hat in dieser Woche eine Exekutivanordnung unterzeichnet, die US-amerikanische Investitionen in die chinesischen Bereiche künstliche Intelligenz, Halbleiter und Quantencomputing verbietet, um eine unbeabsichtigte Unterstützung der militärischen und technologischen Fortschritte Chinas zu vermeiden. Die Halbleiterindustrie, insbesondere die Chipentwicklung und -herstellung, ist das Hauptziel. China hat seine Unzufriedenheit mit der Anordnung geäußert und behauptet, dass sie wirtschaftliche Verbindungen stört. Die Anordnung gilt für zukünftige Investitionen und kann einige Ausnahmen haben. [Weiterlesen](https://www.tomshardware.com/news/us-order-bans-future-investments-in-chinese-ai-semiconductor-and-quantum-computing)\n\n## Datenverwaltung\n\n\nChina schlägt neue Regeln vor, um die Gesichtserkennungstechnologie einzuschränken und den Datenschutz der Nutzer zu schützen. Der Entwurf der Cyberspace-Verwaltung Chinas besagt, dass die Gesichtserkennung nur dann verwendet werden sollte, wenn es notwendig ist, und dass nicht-biometrische Lösungen erkundet werden sollten, um dieselben Ziele mit weniger Eingriff zu erreichen. Die Maßnahme folgt auf Bedenken in China bezüglich des potenziellen Missbrauchs persönlicher Informationen durch die Gesichtserkennungstechnologie. [Weiterlesen](https://fagenwasanni.com/news/china-introduces-new-draft-rules-to-regulate-facial-recognition-technology-and-data-management/174996/)\n\n\nIm Übergang von firmeninternen Servern zu Cloud-Technologie und Rechenzentren für das Datenmanagement bietet die Cloud Skalierbarkeit und Flexibilität und beseitigt gleichzeitig die Notwendigkeit einer Server-Nähe. Allerdings sind viele Organisationen schockiert über die Kosten für das Datenmanagement in Rechenzentren und in der Cloud, da die Cloud-Service-Anbieter Gebühren für die Datenübertragung zwischen den Modalitäten erheben. Die Entwicklung einer Datenmanagementstrategie und einer hybriden Datenarchitektur wird für Organisationen immer wichtiger, um Kosten zu sparen, die Widerstandsfähigkeit zu verbessern und Geschäfte in diesen komplexen Betriebsumgebungen zu unterstützen. [Weiterlesen](https://devops.com/data-management-cloud-technology-or-data-centers/)\n\n\nRegierungen weltweit übernehmen eine immer aktivere Rolle bei der Regulierung des Datenschutzes. Oregon hat einen umfassenden Ansatz gewählt, der Verbraucher*innen mehr Kontrolle über ihre persönlichen Daten gibt. Unternehmen müssen nun explizite Zustimmung zur Datensammlung und -weitergabe einholen, und Einzelpersonen haben das Recht, auf ihre Daten zuzugreifen, sie zu korrigieren und zu löschen. Das Gesetz beinhaltet auch Transparenzpflichten und Schutzmaßnahmen gegen Diskriminierung aufgrund der Verwendung von Daten. Handelt es sich hierbei um einen weiteren Geldzugriff der Regierung oder um einen tatsächlichen Schutz der individuellen Privatsphäre? Die Zeit wird es zeigen. [Weiterlesen](https://www.reuters.com/legal/legalindustry/oregon-passes-comprehensive-privacy-law-2023-08-11/)\n\n## Cyber-Sicherheit\n\n\nDas National Institute of Standards and Technology (NIST) ergreift bedeutende Maßnahmen zur Verbesserung der Cybersicherheitspraktiken durch Bereitstellung neuer Leitlinien und Ressourcen. Neben den bisherigen Sicherheitsrahmenpfeilern Identifizierung, Schutz, Erkennung, Reaktion und Wiederherstellung hat das NIST einen neuen Pfeiler namens \"govern\" eingeführt. Diese neue Ergänzung betont, dass Cybersicherheit eine wesentliche Quelle von unternehmerischem Risiko ist und Organisationen dabei unterstützt, Entscheidungen für ihre Sicherheitsstrategie besser zu planen und umzusetzen. [Weiterlesen](https://www.infosecurity-magazine.com/news/nist-expands-cybersecurity/)\n\n\nDie Black Hat Cybersecurity Konferenz in Las Vegas präsentierte fortschrittliche Werkzeuge zur Bedrohungserkennung, Vorfallreaktion und sicheren Kommunikation. Diese Werkzeuge zielen darauf ab, die Cybersicherheitsverteidigung von Organisationen gegen aufkommende Bedrohungen und Herausforderungen zu verbessern. Wie erwartet war Generative AI ein zentrales Thema, das sein Potential zur Vereitlung von Cyberangriffen hervorhob. [Weiterlesen](https://www.crn.com/news/security/20-hottest-new-cybersecurity-tools-at-black-hat-2023)\n\n\nDie Codebreaker Challenge der NSA hilft, den Mangel an Fachkräften im Bereich Cybersicherheit anzugehen, indem sie Studenten herausfordert, komplexe Probleme zu lösen und Zusammenarbeit zu fördern. Sie fördert die Entwicklung neuer Bildungsmaterialien und Programme, um der nächsten Generation von Fachleuten praktische Erfahrung zu vermitteln und die Verteidigung der Branche gegen sich wandelnde Bedrohungen zu stärken. Das Programm betont die Bedeutung der Förderung von Cybersicherheitsbildung und bietet praktische Lernmöglichkeiten. [Weiterlesen](https://www.darkreading.com/attacks-breaches/nsa-talks-codebreaker-challenge-success-influence-on-education)\n\n## Umarmung der digitalen Transformation Podcast\n\nDarren interviewt Sonu Panda, den CEO von Prescriptive Data, in dieser Episode. Sie diskutieren, wie ihre Software Hilft Commercial Real Estate Besitzer ihre Gebäude in intelligente und effiziente Räume zu verwandeln. [Weiterlesen](https://embracingdigital.org/episode-EDT150)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW28-de","image":"./briefs/edw-28/de/thumbnail.png","lang":"de","summary":"Digitale Transformationsnachrichten für den 13. August 2023. Aktuelle Entwicklungen im Bereich Datenschutz, Sicherheit und technologischer Fortschritt, einschließlich Quantencomputing und neuer Datenschutzgesetze, sind es wert, verfolgt zu werden. NIST-Richtlinien und Black Hat-Tools sind nützliche Ressourcen. Die Codebreaker Challenge der NSA fördert Innovationen in der Cybersicherheit."},{"id":195,"type":"News Brief","title":"2023-8-19","tags":["ai","edge","cybersecurity"],"body":"\n\n## Künstliche Intelligenz\n\n\nIn einer kürzlichen Entscheidung hat ein Gericht in D.C. festgestellt, dass KI-generierte Kunst nicht für den Urheberrechtsschutz qualifiziert ist. Die Entscheidung setzt einen bedeutenden Präzedenzfall und legt nahe, dass kreative Werke, die ausschließlich von künstlicher Intelligenz produziert werden, nicht die rechtlichen Rechte besitzen, die normalerweise menschengemachten Schöpfungen gewährt werden. Dieses Urteil kann weitreichende Auswirkungen auf die Zukunft des geistigen Eigentums im digitalen Zeitalter haben. [Weiterlesen](https://news.bloomberglaw.com/ip-law/ai-generated-art-lacks-copyright-protection-d-c-court-rules)\n\n\nForscher des MIT haben herausgefunden, dass maschinelle Lernmodelle, die bei medizinischer Diagnose und Behandlung eingesetzt werden, bestehende Ungleichheiten im Gesundheitswesen verstärken können. Diese Modelle können bei Verwendung von voreingenommenen Daten zu ungleichem Zugang und ungleicher Qualität der Versorgung für unterrepräsentierte Gruppen führen. Dies unterstreicht den dringenden Bedarf an verbessertem Datensammeln und Modellentwicklung, um faire und gerechte Gesundheitsergebnisse zu gewährleisten. [Weiterlesen](https://news.mit.edu/2023/how-machine-learning-models-can-amplify-inequities-medical-diagnosis-treatment-0817)\n\n\nDer sprunghafte Anstieg der Nachfrage nach künstlicher Intelligenz (KI) führt laut Fox Business zu einem Mangel an leistungsfähigen Chips. Mit der Ausweitung von KI-Anwendungen in verschiedenen Industriezweigen steigt der Bedarf an spezialisierten Chips, um diese zu betreiben. Diese Knappheit wirkt sich auf verschiedene Sektoren aus, einschließlich Automobilindustrie, Gesundheitswesen und Unterhaltungselektronik, was möglicherweise die Innovation verlangsamt. Unternehmen stehen nun im Wettlauf, um die Chipproduktion zu steigern und der wachsenden Nachfrage gerecht zu werden. [Weiterlesen](https://www.foxbusiness.com/technology/surging-demand-ai-creating-shortage-high-powered-chips)\n\n## Internetsicherheit\n\n\nEine auf Schülerinnen und Schüler fokussierte App sorgt bei Eltern und Cybersicherheitsexperten für Bedenken. Die potenziellen Risiken dieser App für die Sicherheit von Kindern werden genau untersucht, was Experten dazu veranlasst, die Bedeutung der elterlichen Wachsamkeit und der Aufklärung junger Nutzerinnen und Nutzer über Online-Sicherheit zu betonen. Die Situation verdeutlicht den fortwährenden Bedarf an verantwortungsbewusstem Technologiegebrauch und proaktiven Maßnahmen zum Schutz von Kindern im digitalen Zeitalter. [Weiterlesen](https://www.live5news.com/2023/08/18/student-geared-app-concerning-lowcountry-parents-cyber-security-experts/)\n\n\nDie Fintech-Branche wird laut Analytics Insight stark von KI und Cybersicherheit beeinflusst. KI hilft beim Erkennen von Betrug, Kundenservice und Risikobewertung, bringt aber auch neue Sicherheitsrisiken mit sich. Dieses fortlaufende Zusammenspiel von KI und Cybersicherheit verändert die Fintech-Welt und führt zu mehr Innovation und erhöhten Sicherheitsmaßnahmen zum Schutz wichtiger Finanzinformationen. [Weiterlesen](https://www.analyticsinsight.net/how-ai-and-cybersecurity-shape-fintech-industry/)\n\n\nEin kürzlich veröffentlichter Pew Research-Bericht zeigt, dass Amerikaner nur begrenztes Wissen über Künstliche Intelligenz (KI), Cybersicherheit und die IT-Branche haben. Die Ergebnisse zeigen, dass ein beträchtlicher Teil der Bevölkerung kein Verständnis für diese wichtigen Themen hat. Dies unterstreicht die Notwendigkeit für verstärkte öffentliche Bildungs- und Bewusstseinsmaßnahmen in einer Ära, die von Technologie und digitalen Herausforderungen dominiert wird. [Weiterlesen](https://www.pewresearch.org/internet/2023/08/17/what-americans-know-about-ai-cybersecurity-and-big-tech/)\n\n## Edge Computing\n\nEdge Computing ist ein verteiltes Berechnungsparadigma, bei dem Datenverarbeitungs- und Speicherfunktionen näher an den Endgeräten und Benutzern angesiedelt werden, anstatt ausschließlich in entfernten Datenzentren stattzufinden. Durch die Platzierung der Berechnung am Rand des Netzwerks wird die Latenz drastisch reduziert, was zu schnelleren Reaktionszeiten und einer verbesserten Benutzererfahrung führt. Dies ist besonders wichtig für Anwendungen, die Echtzeitverarbeitung erfordern oder eine schnelle Entscheidungsfindung ermöglichen müssen. Edge Computing ermöglicht auch eine effizientere Datenübertragung und -speicherung, da nur relevante Daten über das Netzwerk gesendet werden, anstatt alle an ein zentrales Rechenzentrum zu schicken.\n\n\n\nT-Mobile arbeitet laut FierceWireless mit Google Cloud zur Randbereichsberechnung zusammen. Diese Zusammenarbeit zielt darauf ab, die Cloud-Infrastruktur von Google zu nutzen, um die 5G-Netzwerkkapazitäten von T-Mobile zu verbessern und latenzarme Dienste für Verbraucher und Unternehmen bereitzustellen. Dieser strategische Schritt zeigt die wachsende Bedeutung der Randbereichsberechnung in der Telekommunikationsbranche auf. [Weiterlesen](https://www.fiercewireless.com/tech/t-mobile-hooks-google-cloud-edge-compute)\n\n\n\nDer Ausstieg aus den Netzwerken der 2. und 3. Generation kann Auswirkungen auf IoT- und Roaming-Dienste haben. Viele IoT-Geräte und internationale Reisende verlassen sich immer noch auf diese älteren Netzwerke. Telekommunikationsanbieter müssen alternative Konnektivitätsoptionen und -strategien in Betracht ziehen, um Störungen des Services für diese Benutzergruppen während des Übergangs zu fortschrittlicheren Netzwerktechnologien zu verhindern. Es ist wichtig, diese Probleme anzugehen, um in Zukunft keine technische Schuld anzuhäufen. [Weiterlesen](https://www.lightreading.com/broadband/sunsetting-2g-and-3g-could-leave-iot-and-roamers-in-dark/d/d-id/786146)\n\n\nAuf dem ET Digital Telco Summit betonte der CTO von Airtel, dass die Verschmelzung von 5G mit Cloud- und IoT-Technologien innovative Anwendungsbeispiele in verschiedenen Industrien freisetzen wird. Diese Kombination wird voraussichtlich transformative Lösungen für verschiedene Branchen vorantreiben und das Potenzial von 5G zur Revolutionierung der Konnektivität und Dienstleistungen in Indien und darüber hinaus demonstrieren. [Weiterlesen](https://telecom.economictimes.indiatimes.com/news/industry/etdigitaltelcosummit-combination-of-5g-with-cloud-iot-to-drive-new-use-cases-for-verticals-airtel-cto/102827792)\n\n## Den Podcast zur digitalen Transformation umarmen\n\n\nDiese Woche beginnt Darren eine Serie über generative KI. In dieser Woche startet Darren eine Serie über generative KI mit Interviews von Experten im Bereich Bildung, Gesundheitswesen, Cybersicherheit und Cloud-Technologien und wie generative KI eine Rolle in der Zukunft spielt. Die Geschichten dieser Woche umfassen Interviews mit Dr. Jeffrey Lancaster zum Thema Verständnis von generativer KI. [Weiterlesen](https://embracingdigital.org/episode-EDT151)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW29-de","image":"./briefs/edw-29/de/thumbnail.png","lang":"de","summary":"In den neuesten Embracing Digital Nachrichten vom 20. August 2023 gibt es Updates zu Künstlicher Intelligenz (KI), Cybersicherheit und Edge Computing. Die Nachrichten beinhalten KI-Verhandlungen vor Gericht, Cyber-Kriminelle, die auf die Rückkehr zur Schule abzielen, und technische Schulden, die durch Edge Computing ins 5G-Ökosystem übertragen werden."},{"id":196,"type":"News Brief","title":"2023-2-19","tags":null,"body":"\n\n## Künstliche Intelligenz\n\nDie Ethik und Regulierungen der Verwendung von generativ-ki-basierten Chatbots beginnen eine Rolle zu spielen, da immer mehr Menschen ChatGPT und andere nutzen. Chatbots müssen nun eine Warnung enthalten: Wir produzieren möglicherweise keine realen Ausgaben. Glauben Sie auf eigene Gefahr. Sie verzichten auf Ihr Recht, uns zu verklagen, wenn Sie Gespräche, Bilder und Videos für Hochrisiko-Anwendungsfälle reproduzieren, die Hass, Belästigung, Gewalt, Selbstverletzung, illegale Aktivitäten, Täuschung, Diskriminierung oder Spam verursachen.\n\n[https://swisscognitive.ch/2023/02/15/honest-lying-why-scaling-generative-ai-responsibly-is-not-a-technology-dilemma-in-as-much-as-a-people-problem/](https://swisscognitive.ch/2023/02/15/honest-lying-why-scaling-generative-ai-responsibly-is-not-a-technology-dilemma-in-as-much-as-a-people-problem/)\n\nDa künstliche Intelligenz in verschiedenen Branchen zunehmend an Bedeutung gewinnt, wird die Rolle des Chief AI Officer (CAIO) als neue Ergänzung zur C-Suite immer wichtiger. Ein guter CAIO sollte den gewünschten Einfluss von KI-Projekten klar definieren und die Organisation kontinuierlich auf die Erreichung dieses Ziels ausrichten. Ein großartiger CAIO sollte auch den tatsächlichen Einfluss dieser Projekte verfolgen und das Unternehmen über ihre Leistung im Laufe der Zeit informieren. Diese Rolle erhöht daher die Erwartungen an die Förderung von Transformation und Wirkung sowie an die Antizipation und Reaktion auf die Nutzung von KI durch Konkurrenten.\n\n[https://applieddatascience.medium.com/the-case-for-the-chief-ai-officer-the-newest-c-suite-role-d55cca40c01](https://applieddatascience.medium.com/the-case-for-the-chief-ai-officer-the-newest-c-suite-role-d55cca40c01)\n\nDer neueste Supercomputer von IBM geht in mehrere innovative Richtungen - es ist der erste KI-optimierte, Cloud-native Supercomputer von IBM und er befindet sich komplett innerhalb der IBM Cloud anstatt wie herkömmliche Supercomputer vor Ort. Laut IBM ist \"dies die Umgebung für IBM-Forscher, die fortschrittlichsten KI-Fähigkeiten entwickeln, einschließlich der Arbeit an Grundlagenmodellen und ein Ort, an dem sie mit Partnern an der Modellerstellung zusammenarbeiten.\"\n\n[https://futurumresearch.com/research-notes/ibms-latest-supercomputer-is-cloud-based-and-ai-focused/](https://futurumresearch.com/research-notes/ibms-latest-supercomputer-is-cloud-based-and-ai-focused/)\n\n## Ubiquitous Computing\n\nGemäß einer Studie von 451 Research, die im Auftrag von Oracle Cloud Infrastructure durchgeführt wurde, wird fast jede Cloud-Reise in Unternehmen jetzt zu einer Multi-Cloud-Reise. Die Studie ergab, dass 98% der befragten Unternehmen mindestens zwei Cloud-Service-Provider nutzen oder nutzen möchten. Die wichtigsten Treiber für Multi-Cloud-Strategien sind Datenhoheit und Kostenoptimierung. Unternehmen planen proaktiv Multi-Cloud-Strategien für die Zukunft, wobei die Datensicherung das am meisten erwartete zukünftige Anwendungsszenario darstellt.\n\n[https://www.cloudcomputing-news.net/news/2023/feb/21/98-of-firms-using-public-cloud-adopt-multicloud-infrastructure-provider-strategy/](https://www.cloudcomputing-news.net/news/2023/feb/21/98-of-firms-using-public-cloud-adopt-multicloud-infrastructure-provider-strategy/)\n\nDie \"Supercloud\", eine Cloud-Architektur, die eine nahtlose Migration von Anwendungen zwischen verschiedenen Cloud-Anbietern ermöglicht, wurde erstmals von Forschern der Cornell University im Jahr 2016 vorgeschlagen. Das Konzept ist als Lösung für die Herausforderungen der Multi-Cloud wieder aufgetaucht und bietet ein homogenes Netzwerk, um Cloud-Ressourcen miteinander zu verbinden, was zu einer nahtlosen Migration, konsistenter Sicherheit und optimaler Leistung führt.\n\n[https://www.cloudcomputing-news.net/news/2023/feb/22/here-comes-the-supercloud-what-does-it-mean-for-multi-cloud-complexity/](https://www.cloudcomputing-news.net/news/2023/feb/22/here-comes-the-supercloud-what-does-it-mean-for-multi-cloud-complexity/)\n\nDie Nachfrage nach DRAM für Server wird aufgrund der wachsenden Verwendung von Cloud-, AI- und HPC-Anwendungen diejenige für Smartphones übertreffen; der DRAM-Gehalt für Server soll im Jahr 2023 im Vergleich zu 6,7% für Smartphones um 12,1% YoY zunehmen. Server-Speicher wird 37,6% der gesamten DRAM-Bit-Ausgabe ausmachen, gegenüber 36,8% für mobile DRAM. Die COVID-19-Pandemie hat die Nachfrage nach Cloud-Diensten angetrieben, was zu einem starken Anstieg der Server-Lieferungen führt.\n\n## Cybersicherheit\n\nDie Cybersecurity-Firma Resecurity hat vor einer Reihe von Cyberangriffen gewarnt, die in den letzten 18 Monaten weltweit Datenzentren zum Ziel hatten und zu Datenexfiltration sowie zur Veröffentlichung von Zugangsdaten im Dark Web führten. Obwohl Resecurity die Opfer nicht namentlich genannt hat, behauptet Bloomberg, dass große Unternehmen wie Alibaba, Amazon, Apple, BMW, Goldman Sachs, Huawei Technologies, Microsoft und Walmart ihre Data-Center-Zugangsdaten gestohlen wurden.\n\n[https://www.csoonline.com/article/3688909/cyberattacks-hit-data-centers-to-steal-information-from-global-companies.html#tk.rss_all](https://www.csoonline.com/article/3688909/cyberattacks-hit-data-centers-to-steal-information-from-global-companies.html#tk.rss_all)\n\nDie Cybersecurity-Firma Menlo Labs hat eine unbekannte Bedrohungsakteursgruppe aufgedeckt, die Regierungseinrichtungen über eine Discord-basierte Kampagne mit dem PureCrypter-Downloader ins Visier nimmt. Die Kampagne nutzt die Domain eines kompromittierten Non-Profit-Unternehmens als Befehls- und Kontrollzentrum, um eine sekundäre Nutzlast zu liefern, einschließlich des Redline Stealer, AgentTesla, Eternity, Blackmoon und Philadelphia Ransomware.\n\n[https://www.menlosecurity.com/blog/purecrypter-targets-government-entities-through-discord](https://www.menlosecurity.com/blog/purecrypter-targets-government-entities-through-discord)\n\nDas größte militärische Cyberkriegs-Übung in Westeuropa fand kürzlich in Estland statt, mit 34 Teams aus 11 Ländern, die an einem Live-Cyberkampf teilnahmen. Das siebentägige Ereignis testete die Reaktionen der Teilnehmer auf gemeinsame und komplexe Cyber-Szenarien, einschließlich Angriffen auf Netzwerke und industrielle Steuerungssysteme (ICS). Ein Szenario simuliert Angriffe auf bemannte Robotersysteme. Teams aus Italien, Estland und dem Vereinigten Königreich waren die besten Performer, beurteilt nach ihrer Geschwindigkeit bei der Identifizierung und Reaktion auf Cyberbedrohungen.\n\n[https://www.securityweek.com/11-countries-take-part-in-military-cyberwarfare-exercise/](https://www.securityweek.com/11-countries-take-part-in-military-cyberwarfare-exercise/)\n\n## Podcast zur digitalen Transformation umarmen\n\nSchauen Sie sich die vollständige Episode dieser Woche \"Zertifizierung autonomes Fliegen\" an, in der Darren LuukVan Dijk von Daedalean interviewt.\n\n[https://www.embracingdigital.org/episode-EDT126](https://www.embracingdigital.org/episode-EDT126)\n\n\n\n","guests":null,"link":"/brief-EDW3-de","image":"./briefs/edw-3/de/thumbnail.png","lang":"de","summary":"Summary"},{"id":197,"type":"News Brief","title":"2023-8-26","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\nWir befinden uns mitten in der digitalen Revolution. In dieser Zeit kann es herausfordernd sein, zwischen all dem Hype herauszufiltern, was funktioniert, was auch noch in zwei Jahren relevant ist und was zu Ihrer Organisation beiträgt. Viele IT-Organisationen benötigen Unterstützung bei verwirrenden Botschaften und widersprüchlichen Technologien. Wir helfen Organisationen dabei, den Rummel zu sortieren und die digitale Transformation zu begrüßen. Die Welt der digitalen Transformation verändert sich ständig, und Sie müssen wissen, was heute machbar ist. Hören Sie unseren wöchentlichen Podcast, um die neuesten Nachrichten aus den Bereichen Cybersicherheit, fortschrittliche Kommunikation, Datenmanagement, künstliche Intelligenz, Edge-Computing und Cloud Computing zu erfahren.\n\n##Cybersicherheit\n\n\nEine neue Bedrohung ist aufgetaucht. Die Cyber-Gruppe \"Flax Taifun\", von der angenommen wird, dass sie mit China in Verbindung steht, hat für Aufsehen gesorgt. Sie werden verdächtigt, ausgeklügelte Cyberangriffe auf globale Organisationen durchzuführen. Experten überwachen diese Situation genau, da sie bedeutende Auswirkungen auf die digitale Sicherheit haben könnte. Bleiben Sie für Aktualisierungen zu dieser sich entwickelnden Geschichte dran. [Weiterlesen](https://thehackernews.com/2023/08/china-linked-flax-typhoon-cyber.html)\n\n\nCybersecurity-Unternehmen schlagen Alarm, da Ransomware-Angriffe im Vergleich zum Vorjahr um mehr als einhundertdreiundfünfzig Prozent gestiegen sind und im letzten Monat um über sechzehn Prozent zugenommen haben. Experten warnen davor, dass Organisationen ihre Abwehrmechanismen stärken und Mitarbeiter über diese Bedrohungen aufklären müssen, um sich vor kostspieligen Datenverstößen zu schützen. [Weiterlesen](https://www.securityweek.com/cybersecurity-companies-report-surge-in-ransomware-attacks/)\n\n\nDer Cybersicherheitssektor kämpft mit einem schwerwiegenden Talentmangel, der die Erwartungen nach Lösungen steigert. Die Nachfrage nach qualifizierten Fachkräften in diesem sich ständig weiterentwickelnden Bereich steigt rasant, dennoch besteht weiterhin eine Lücke. Um dem entgegenzuwirken, erkunden Unternehmen Strategien wie die Weiterbildung der aktuellen Mitarbeiter und die Förderung von Vielfalt in der Branche. Es könnte an der Zeit sein, zurück zur Schule zu gehen, um eine Cybersicherheitszertifizierung zu erhalten. [Weiterlesen](https://www.helpnetsecurity.com/2023/08/25/cybersecurity-talent-shortage-expectations/)\n\nKünstliche Intelligenz\n\n\nKünstliche Intelligenz hat einer gelähmten Frau in einer unglaublichen Entwicklung ihre Stimme zurückgegeben. Forscher an der UCSF haben KI-Technologie eingesetzt, um die Gehirnsignale der Frau zu entschlüsseln und in Sprache zu übersetzen. Dies bahnbrechende Errungenschaft könnte zahlreichen Menschen helfen, ihre Fähigkeit zur Kommunikation wiederzuerlangen. Für weitere Informationen zu dieser inspirierenden Geschichte lesen Sie einen detaillierten Artikel auf ucsf.edu. [Weiterlesen](https://www.ucsf.edu/news/2023/08/425986/how-artificial-intelligence-gave-paralyzed-woman-her-voice-back)\n\n\nGymnasiasten bereiten sich auf den Aufstieg der künstlichen Intelligenz (KI) vor. MIT-Forscher haben eine neue Initiative gestartet, um Schüler auf die von KI angetriebene Zukunft vorzubereiten. Dieses Programm zielt darauf ab, junge Köpfe mit den Fähigkeiten und dem Wissen auszustatten, die benötigt werden, um in einer Welt erfolgreich zu sein, die zunehmend von KI-Technologie beeinflusst wird. Höre dir Darrens Interview mit Pete Schmitz in der Podcast-Episode \"Training the Next Generation in AI\" an, um einen genauen Einblick in das zu erhalten, was heute passiert. [Weiterlesen](https://news.mit.edu/2023/how-to-help-high-schoolers-prepare-rise-of-artificial-intelligence-0824)\n\n\nIn einer nachdenklichen DeZeen-Tiefenanalyse wird der Umweltkosten von künstlicher Intelligenz (KI) ins Auge genommen. Mit der Ausweitung von KI-Anwendungen steigt auch ihr Energieverbrauch und verschlimmert möglicherweise die Klimabedenken. Diese kritische Untersuchung untersucht die Nachhaltigkeitsherausforderungen, die durch KI entstehen, und den dringenden Bedarf an umweltfreundlichen Lösungen bei der Entwicklung und Bereitstellung von KI-Technologien. Bleiben Sie gespannt auf tiefere Einblicke in dieses drängende Umweltproblem. [Weiterlesen](https://www.dezeen.com/2023/08/26/dezeen-in-depth-examines-the-environmental-cost-of-artificial-intelligence/)\n\n## Ubiquitous Computing\n\nUbiquitous Computing\n\n## Ubiquitäres Computing\n\n\nDa wir uns dem Zeitalter der Quantencomputing näher kommen, wird die Sicherung der kritischen Infrastruktur noch entscheidender. C4ISRNET widmet sich den Herausforderungen und Strategien zum Schutz lebenswichtiger Systeme in dieser neuen technologischen Landschaft. Das Potenzial von Quantencomputing, bestehende Verschlüsselungsmethoden zu brechen, wirft Bedenken auf und betont die Notwendigkeit innovativer Cybersicherheitslösungen. [Weiterlesen](https://www.c4isrnet.com/it-networks/2023/08/21/how-to-protect-critical-infrastructure-in-the-quantum-computing-era/)\n\n\nWie von XDA-Entwicklern berichtet, wird Dropbox seine unbegrenzte Cloud-Speicherpolitik beenden. Diese Entscheidung erfolgt als Reaktion auf Missbrauchsfälle durch einige Nutzer. Als Folge davon führt Dropbox neue Speicherbeschränkungen ein, was für seine Nutzerbasis eine wesentliche Veränderung darstellt. Diese Entwicklung unterstreicht die fortlaufenden Herausforderungen von Cloud-Speicheranbietern, die Nutzernachfrage mit nachhaltigen Geschäftsmodellen in Einklang zu bringen. [Weiterlesen](https://www.xda-developers.com/dropbox-ends-unlimited-cloud-storage-policy-abused/)\n\n\nDie globale Pandemie im Jahr 2020 hat einen Technologiebereich mit stagnierendem Wachstum, die Virtual Desktop Infrastructure (VDI) wiederbelebt. Microsoft und VMware gestalten das Geschäft mit VDI und Cloud-Technologie neu. SiliconANGLE diskutiert, wie diese Technologiegiganten zusammenarbeiten, um VDI zu redefinieren und Cloud-Lösungen zu verbessern. Diese Partnerschaft wird Unternehmen mehr Flexibilität, Skalierbarkeit und Leistungsfähigkeit in IT-Umgebungen bieten. [Weiterlesen](https://siliconangle.com/2023/08/24/microsoft-vmware-redefining-business-vdi-cloud-technology-vmwareexplore/)\n\n## Embrace Digitaler Transformationspodcast\n\n\n\nDiese Woche geht die Reihe Embracing Generative AI weiter, in der Interviews mit Gymnasiallehrern vorgestellt werden, die ChatGPT übernommen haben, und neue Anwendungsfälle für generative KI erkundet werden. Abonnieren Sie Embracing Digital Transformation für vertiefte Diskussionen zu aktuellen Themen. [Weiterlesen](http://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW30-de","image":"./briefs/edw-30/de/thumbnail.png","lang":"de","summary":"Hier sind die neuesten Updates zu Nachrichten über die digitale Transformation für die Woche vom 27. August 2023. Die Geschichten dieser Woche behandeln Cybersecurity, Künstliche Intelligenz und Ubiquitäres Computing. In dieser Episode finden Sie Geschichten über einen signifikanten Anstieg von 150% bei Ransomware-Angriffen, wie Quantencomputing Sicherheitslücken im IoT aufgedeckt hat, und wie KI dabei hilft, die Stimme von Menschen wiederherzustellen, die gelähmt sind."},{"id":198,"type":"News Brief","title":"2023-9-2","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\nWir befinden uns mitten in der digitalen Revolution. In dieser Zeit kann es herausfordernd sein, zwischen all dem Hype herauszufiltern, was funktioniert, was auch noch in zwei Jahren relevant ist und was zu Ihrer Organisation beiträgt. Viele IT-Organisationen benötigen Unterstützung bei verwirrenden Botschaften und widersprüchlichen Technologien. Wir helfen Organisationen dabei, den Rummel zu sortieren und die digitale Transformation zu begrüßen. Die Welt der digitalen Transformation verändert sich ständig, und Sie müssen wissen, was heute machbar ist. Hören Sie unseren wöchentlichen Podcast, um die neuesten Nachrichten aus den Bereichen Cybersicherheit, fortschrittliche Kommunikation, Datenmanagement, künstliche Intelligenz, Edge-Computing und Cloud Computing zu erfahren.\n\n## Cybersicherheit\n\n\n\nEin kürzlich erfolgter Cyberangriff, der angeblich von China unterstützt wird, hat Besorgnis hinsichtlich der nationalen Sicherheit in Guam ausgelöst. Die Hackergruppe Volt Typhoon hat das Telekommunikations- und Stromnetz von Guam ins Visier genommen, um eine heimliche Präsenz herzustellen. Guam gilt als ein vitaler Logistik-Hub für die USA im Falle eines Konflikts mit China. Die Hacker haben eine Schwachstelle in der Fortinet Fortigate Firewall ausgenutzt, einem führenden Cybersicherheitsprodukt. Dieser Angriff verdeutlicht eine neue Bedrohung, da Staaten-Hacker zunehmend Cybersicherheitswerkzeuge ins Visier nehmen, die für den Schutz von Systemen entscheidend sind. [Weiterlesen](https://www.msn.com/en-us/news/other/cybersecurity-tools-are-new-targets-for-nation-state-hackers/ar-AA1g5mib)\n\n\nEin Cyberangriff hat bei Hospital Sisters Health System, einschließlich St. Elizabeth's in O'Fallon, Illinois, eine technologische Ausfallzeit von einer Woche verursacht. CEO Damond Boatright bestätigte, dass der Vorfall den Internetdienst, Websites, Telefone und Computeranwendungen gestört hat. Obwohl Fortschritte bei der Wiederherstellung wichtiger Systeme erzielt wurden, ist die Website von HSHS noch immer nicht verfügbar, und der Zugang der Patienten zu Online-Charts und der Kommunikation mit Ärzten ist unterbrochen. Dies ist der zweite Angriff auf Krankenhausbündnisse in den Vereinigten Staaten in zwei Monaten. [Weiterlesen](https://www.bnd.com/news/local/article278879724.html)\n\n\n\nGemeinschaftshochschulen sehen sich einer wachsenden Cybersicherheitsbedrohung gegenüber: \"Geisterstudierende\". Diese fiktiven Identitäten nutzen Immatrikulationsprozesse aus, um auf akademische Ressourcen zuzugreifen und potenziell Studentendaten und institutionelle Integrität zu gefährden. Mit immer raffinierter werdenden Hackern investieren Gemeinschaftshochschulen in Cybersicherheitsmaßnahmen, um dieser Bedrohung entgegenzuwirken. Der Kampf gegen \"Geisterstudierende\" unterstreicht die Notwendigkeit einer robusten digitalen Sicherheit in Bildungseinrichtungen, um sensible Informationen zu schützen und eine sichere Lernumgebung zu gewährleisten. [Weiterlesen](https://www.chronicle.com/article/meet-the-cybersecurity-threat-haunting-community-colleges-ghost-students)\n\nKünstliche Intelligenz\n\n\nDer Einsatz von KI in der Hochschulbildung hat eine Debatte entfacht zwischen denen, die ihr Potenzial für Personalisierung und Effizienz sehen und denen, die sich um Ethik und den Verlust menschlicher Verbindung sorgen. Eine sorgfältige Abwägung ist notwendig, um KI in Hochschulräumen zu integrieren. Die Hochschulprofessorin Laura Torres Newey teilte ihre Herangehensweise zur Bewältigung dieses Problems in einer Folge des Podcasts \"Digitale Transformation annehmen\" mit. Ihre Erkenntnisse offenbaren die Herausforderungen und Möglichkeiten der Integration von KI in der Hochschulbildung. [Weiterlesen](https://theconversation.com/should-ai-be-permitted-in-college-classrooms-4-scholars-weigh-in-212176)\n\n\nDie Unterhaltungsindustrie steht vor Arbeitskonflikten, bei denen künstliche Intelligenz (KI) im Mittelpunkt des Konflikts steht. Der Streik der Screen Actors Guild befindet sich in seiner achten Woche, wobei die Arbeiter in Hollywood faire Entschädigung und kreative Kontrolle bei der Verwendung KI-gesteuerter Technologien fordern. Dieser Konflikt spiegelt den Kampf der Branche wider, die Vorteile von KI, wie Kosteneffizienz und Innovation, mit Bedenken hinsichtlich Jobverdrängung und künstlerischer Autonomie in Einklang zu bringen. Da sich das Unterhaltungsangebot weiterentwickelt, wird die Lösung dieser KI-bezogenen Arbeitsfragen für die Arbeiter und die Zukunft der Branche entscheidend sein. Die Welt schaut zu! [Weiterlesen](https://www.pbs.org/newshour/show/why-artificial-intelligence-is-a-central-dispute-in-the-hollywood-strikes)\n\n\nDie Regulierung künstlicher Intelligenz (KI) ist eine weltweite Besorgnis, und Länder ergreifen unterschiedliche Ansätze, um damit umzugehen. China, Israel und die Europäische Union (EU) sind führend in der KI-Regulierung. China konzentriert sich auf KI-Ethik und Datensicherheit, Israel fördert Innovation durch regulierte Testräume und die EU schlägt strenge Regeln für KI vor, um Verantwortlichkeit sicherzustellen. Da KI weiterhin verschiedene Branchen und Aspekte des täglichen Lebens beeinflusst, bleibt die Suche nach dem richtigen Gleichgewicht zwischen Innovation und Regulierung eine entscheidende Herausforderung auf globaler Ebene. [Weiterlesen](https://www.washingtonpost.com/world/2023/09/03/ai-regulation-law-china-israel-eu/)\n\n## Edge Computing\n\n## Edge Computing\n\n\nIntel stärkt die Sicherheit des Edge Computing, indem sie eine bahnbrechende Schutzschicht einführen. Die neue Technologie des Unternehmens verspricht, Edge-Geräte und Daten zu schützen und dabei kritische Cybersicherheitsbedenken anzugehen. Mit der wachsenden Bedeutung des Edge Computing in verschiedenen Branchen stellt diese Entwicklung einen bedeutenden Fortschritt dar, um die Integrität und Sicherheit verteilter Rechenumgebungen zu gewährleisten. [Weiterlesen](https://www.fool.com/investing/2023/09/01/intel-adds-a-layer-of-protection-to-edge-computing/)\n\n\nDie Sicherstellung der Sicherheit des Internet der Dinge (IoT) ist entscheidend für den Schutz kritischer Infrastrukturen wie Stromnetzen und Wasseraufbereitungsanlagen. Mit der zunehmenden Integration von IoT-Geräten in diese Systeme ist der Schutz vor sich entwickelnden Bedrohungen, einschließlich potenzieller Risiken, die mit bestimmten Ländern verbunden sind, ein dringendes Anliegen für die Vereinigten Staaten. Die Herausforderungen und Auswirkungen chinesischer Technologie im IoT werden derzeit eingehend untersucht und unterstreichen die Notwendigkeit proaktiver Maßnahmen zum Schutz kritischer Infrastrukturen im Land. [Weiterlesen](https://www.forbes.com/sites/davealtavilla/2023/09/03/securing-the-iot-from-the-threat-china-poses-to-us-infrastructure/?sh=4325a3f112c0)\n\n\nEdge Computing ist nicht nur für die Fertigung und das Management kritischer Infrastrukturen gedacht. Edge Computing revolutioniert die Gesundheitsversorgung, indem es Echtzeit-Datenverarbeitung ermöglicht, was zu kürzeren Reaktionszeiten für wichtige Patientendaten führt. Es unterstützt die Fernüberwachung und Telemedizin, macht die Gesundheitsversorgung zugänglicher und verringert die Belastung der Gesundheitseinrichtungen. Zusätzlich verbessert Edge Computing den Datenschutz und die Sicherheit, indem es sensitive Patienteninformationen lokal verarbeitet, das Risiko von Datenlecks reduziert und die Einhaltung von Gesundheitsvorschriften gewährleistet. [Weiterlesen](https://www.ft.com/partnercontent/ntt-ltd/edge-computing-delivers-healthcare-beyond-the-clinic.html?blaid=3846770)\n\n## Digitale Transformation umarmen\n\n\n\nDie Serie zum Thema generative KI setzt sich fort mit Interviews darüber, wie man datenbereit für die GenAI-Revolution wird und wie die Hochschulbildung mit generativer KI im Klassenzimmer umgeht. [Weiterlesen](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW31-de","image":"./briefs/edw-31/de/thumbnail.png","lang":"de","summary":"Nachrichten für die Woche vom 3. September 2023 zur Digitalisierung. In dieser Woche gibt es Updates zu Cybersicherheit, künstlicher Intelligenz und Edge Computing. Chinas Cyberangriffe auf Militärbasen und kritische Infrastruktur, Intels Bemühungen, die Sicherheit im IoT-Bereich zu verbessern, und die Herausforderungen der Regierungen bei der Regulierung von KI sind Themen."},{"id":199,"type":"News Brief","title":"2023-9-9","tags":["ai","gpt4","openai","microsoft","generativeai","environmentalimpact","irs","taxevasion","education","cybersecurity","iosspyware","zerodayvulnerability","northkoreanhackers","quantumrandomness","cloudfirst","cloudsmart","embracingdigital","generativeaipolicy","highereducation"],"body":"\n\n## Künstliche Intelligenz\n\n\nDer Geburtsort von GPT-4 wurde enthüllt! Die Maisfelder in Des Moines, Iowa, sind das Zentrum des Neuesten von OpenAI, aber zu einem erheblichen Kosten für die Umwelt. Der neueste Umweltbericht von Microsoft enthüllte eine vierunddreißigprozentige Steigerung des weltweiten Wasserverbrauchs von 2021 bis 2022, was fast 1,7 Milliarden Gallonen entspricht. Dieser Anstieg ist hauptsächlich auf das bedeutende Investment des Unternehmens in generative KI und die Partnerschaft mit OpenAI zurückzuführen. Die Auswirkungen von generativer KI auf die Umwelt sind signifikant, und große Organisationen wie Microsoft, Google und OpenAI ergreifen Maßnahmen, um diese Bedenken anzugehen. [Weiterlesen](https://apnews.com/article/chatgpt-gpt4-iowa-ai-water-consumption-microsoft-f551fde98083d17a7e8d904f8be822c4)\n\n\n\nDer Internal Revenue Service in den Vereinigten Staaten plant, KI-Technologie einzusetzen, um die Überwachung großer Anwaltskanzleien, Hedgefonds, Beteiligungsgesellschaften und Immobilieninvestoren zu verbessern. Die Initiative zielt darauf ab, komplexe Fälle von Steuerhinterziehung zu erkennen und fällige Bundessteuern von wohlhabenden Steuerzahlern einzufordern. Der IRS hat 80 Milliarden Dollar aus dem Inflation Reduction Act zur Verfügung gestellt, um Muster und Trends bei der Durchführung bedeutender Prüfungen zu untersuchen und Strafen gegen Steuerhinterzieher in diesen Branchen durchzusetzen. [Weiterlesen](https://www.nytimes.com/2023/09/08/us/politics/irs-deploys-artificial-intelligence-to-target-rich-partnerships.html)\n\n\nPädagogen und Gelehrte sind führend darin, sich mit der Rolle der KI in der Bildung auseinanderzusetzen. Wenn die Schüler diesen Herbst zurück zur Schule gehen, behandeln mehrere Artikel aus Fachzeitschriften weltweit KI-Anwendungen, ethische Bedenken, Verstärkung statt Automatisierung, die Vorbereitung der Schüler auf eine KI-integrierte Arbeitswelt, die Erkennung von Deepfakes, die Bewahrung des menschlichen gesunden Menschenverstands und die Nutzung generativer KI für den Unterricht. Diese Erkenntnisse bieten wertvolle Richtlinien für die Gestaltung der Zukunft der KI in der Bildung. Schaut euch auch Darrens Interview mit der New York Times und der Englischprofessorin Laura Torres Newey an, in dem sie darüber spricht, wie sie KI in ihren Kursen in diesem Herbst integriert. [Weiterlesen](https://daily.jstor.org/artificial-intelligence-and-education-a-reading-list/)\n\n## Cybersicherheit\n\n\nChina nimmt die Cybersecurity ernst. Chinas neue Cybersicherheitsvorschriften zielen darauf ab, die digitale Sicherheit für das Internet zu verbessern, aber kleine Unternehmen könnten vor Herausforderungen stehen. Die vollständigen Auswirkungen auf Unternehmen aller Größen sind noch unklar, und es könnte Auswirkungen auf die Online-Privatsphäre geben. Dennoch sind diese Maßnahmen notwendig, um Online-Daten zu schützen und Cyberbedrohungen entgegenzuwirken. Privatpersonen und Unternehmen müssen auf dem Laufenden bleiben und notwendige Vorsichtsmaßnahmen treffen, um ihre Online-Präsenz zu sichern. [Weiterlesen](https://www.scmp.com/news/china/politics/article/3233931/china-steps-cybersecurity-enforcement-smaller-businesses-are-feeling-heat)\n\n\nDas einst immun gewesene iPhone-System ist in der vergangenen Woche angegriffen worden, nachdem Spyware in iOS entdeckt wurde, das auf iPhones läuft. Apple hat sofort gehandelt, nachdem sie Spionagethreats entdeckt haben. Sie haben essentielle Software-Updates veröffentlicht, um Ihre Geräte sicher zu halten. Es ist, als hätte man einen Superhelden, der Ihre technische Welt schützt! Wir werden uns mit den Details dieser Updates befassen, wie sie potenzielle Sicherheitsverstöße abgewehrt haben und was dies für Ihre digitale Sicherheit bedeutet. Bleiben Sie dran, während wir die neuesten Entwicklungen in Technologie und Cybersicherheit enthüllen. [Weiterlesen](https://www.washingtonpost.com/politics/2023/09/08/apple-issues-software-updates-after-spyware-discoveries/)\n\n\nNordkoreanische Hacker haben angeblich eine Schwachstelle ausgenutzt, die es ihnen ermöglicht, auf eine Festung digitaler Abwehrmechanismen zuzugreifen. Dieser Cyberangriff stellt eine erhebliche Bedrohung für gezielte Systeme, Datenlecks und die allgemeine Cybersicherheitslandschaft dar. Bemühungen sind im Gange, um der Bedrohung entgegenzuwirken, einschließlich Patches und Schutzmaßnahmen, um ähnliche Angriffe in der Zukunft zu verhindern. [Weiterlesen](https://thehackernews.com/2023/09/north-korean-hackers-exploit-zero-day.html)\n\n## Ubiquitäres Computing\n\n\nDie Integration von generativen KI-Systemen verändert die Cloud-Architektur und durchdringt sie mit einem intelligenten Gehirn, das lernen und eigenständig kreieren kann. Diese Transformation formt die Cloud-Infrastruktur neu, indem sie Automatisierung stärkt, Ressourcenzuweisung optimiert und Sicherheitsmaßnahmen verbessert. Die Einbindung von generativen KI-Systemen in die Cloud bietet vielversprechende Chancen und Herausforderungen, auf die wir uns auf eine detaillierte Reise begeben werden, um sie zu entwirren. [Weiterlesen](https://www.infoworld.com/article/3706094/adding-generative-ai-systems-may-change-your-cloud-architecture.html)\n\n\nIn einer bahnbrechenden Leistung hat das MIT erstmals die Kontrolle über die Quanten-Zufälligkeit übernommen. Es ist, als würde man das Wilde und Unvorhersehbare zähmen! Wir werden uns in die Einzelheiten dieser bemerkenswerten Leistung vertiefen, die tiefgreifende Auswirkungen auf die Quantenberechnung und Kryptographie haben könnte. Begleiten Sie uns, während wir erkunden, wie der Durchbruch des MIT es uns ermöglicht, die angeborene Zufälligkeit der Quantenwelt zu nutzen und neue Grenzen in Technologie und Sicherheit zu erschließen. Die Zeit wird zeigen, welche Auswirkungen dies auf die Berechnung haben wird. [Weiterlesen](https://scitechdaily.com/harnessing-the-void-mit-controls-quantum-randomness-for-the-first-time/)\n\n\n\"Cloud-First\" ist vorbei und \"Cloud-Smart\" ist das neue Schlagwort in der Technologie. Denken Sie daran, dass es von \"die Cloud ist die Antwort\" zu \"die Cloud als strategischer Verbündeter\" geht. Wir werden uns näher mit den Details dieser Veränderung befassen und untersuchen, was \"Cloud-Smart\" für Unternehmen und IT-Strategien bedeutet. Begleiten Sie uns, während wir besprechen, wie Organisationen ihre Ansätze weiterentwickeln, um die Cloud intelligenter zu nutzen und sie zu einem zentralen Bestandteil ihrer digitalen Transformationsprozesse zu machen. [Weiterlesen](https://www.infoworld.com/article/3705615/cloud-first-is-dead-cloud-smart-is-whats-happening-now.html)\n\n## Voranschreitende digitale Transformation\n\n\n\nIn dieser Woche setzt die Serie \"Embracing Generative AI\" im Podcast fort mit einem faszinierenden Interview über die Entwicklung einer Generative AI-Politik für Ihren Arbeitsplatz und wie Hochschulbildung generative AI im Klassenzimmer angeht. Stimmen Sie ein auf die neuesten Episoden dienstags und donnerstags. [Weiterlesen](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW32-de","image":"./briefs/edw-32/de/thumbnail.png","lang":"de","summary":"Erhalten Sie die neuesten digitalen Nachrichten für die Woche vom 10. September 2023, die künstliche Intelligenz, Cybersicherheit und ubiquitäres Computing abdecken. Erfahren Sie diese Woche mehr über eine neue iPhone-Cyberbedrohung, den Geburtsort von GPT-4 und den Ansatz des MIT zur Bewältigung der Unsicherheit bei der Quantencomputertechnik."},{"id":200,"type":"News Brief","title":"2023-9-16","tags":["aisummit","rishisunak","chineseofficialsban","nationalsecurity","ukchinatensions","amazonselfpublishers","aigeneratedcontent","digitalpublishing","columbiauniversityresearch","aireliability","languagemodels","chatbotperformance","cognitivescience","webassembly(wasm)","edgecomputingrevolution","edgedevices","latencyreduction","secureexecution","iotapplications","mobilecomputing","ai","security","edge","embracingdigital"],"body":"\n\n## Künstliche Intelligenz\n\n\nDer britische Finanzminister Rishi Sunak erwägt ein Verbot für chinesische Beamte, die an fünfzig Prozent des bevorstehenden KI-Gipfels teilnehmen, aufgrund von Bedenken hinsichtlich der nationalen Sicherheit. Diese Maßnahme spiegelt die wachsenden Spannungen zwischen dem Vereinigten Königreich und China in Bezug auf Technologie und geistiges Eigentum wider. Einzelheiten des möglichen Verbots sind noch nicht endgültig festgelegt. Der KI-Gipfel ist eine bedeutende Veranstaltung für globale Technologie-Zusammenarbeit. [Weiterlesen](https://www.theguardian.com/technology/2023/sep/15/rishi-sunak-considers-banning-chinese-officials-from-half-of-ai-summit)\n\n\nAmazon verlangt nun von Self-Publishern, jegliche Beteiligung von Künstlicher Intelligenz in ihren Werken offenzulegen, um die Transparenz in der digitalen Verlagswelt zu erhöhen. Dieser Schritt erfolgt als Reaktion auf Bedenken, dass der Markt mit KI-generierten Inhalten überflutet wird. Diese neuen Regeln werden die Self-Publishing-Industrie auf Amazon beeinflussen. [Weiterlesen](https://www.theguardian.com/books/2023/sep/11/self-publishers-must-declare-if-content-sold-on-amazons-site-is-ai-generated)\n\n\n\nForschung von der Columbia University unterstreicht Bedenken über die Zuverlässigkeit der Künstlichen Intelligenz bei Entscheidungsfindung. Die Studie hat herausgefunden, dass selbst fortgeschrittene Sprachmodelle Unsinn fälschlicherweise als natürliche Sprache interpretieren können und dadurch Fragen zur Genauigkeit aufkommen. Dies bietet eine Gelegenheit, die Leistungsfähigkeit von Chatbots zu verbessern und die menschliche Sprachverarbeitung besser zu verstehen. Indem wir die Kluft zwischen Künstlicher Intelligenz und Kognitionsforschung überbrücken, können wir hilfreichere und effektivere KI-gesteuerte Assistenten schaffen, die ihren Benutzern besser dienen. [Weiterlesen](https://www.sciencedaily.com/releases/2023/09/230914114704.htm)\n\n## Edge-Computing\n\n\nWebAssembly (Wasm) ist dabei, die Edge-Berechnung zu revolutionieren. Diese Technologie ermöglicht es, Code aus verschiedenen Sprachen auf Edge-Geräten auszuführen und damit Flexibilität und Leistungsfähigkeit zu verbessern. Durch die Verringerung von Latenzzeiten und die Ermöglichung einer sicheren und effizienten Ausführung kann Wasm verändern, wie Anwendungen am Edge bereitgestellt und ausgeführt werden. Entwickler können das Potenzial von Wasm nutzen, um dynamischere und reaktionsschnellere Lösungen für Edge-Berechnungen zu schaffen und damit verschiedene Branchen wie das Internet der Dinge und mobile Computing zu verbessern. [Weiterlesen](https://www.infoworld.com/article/3703052/how-webassembly-will-transform-edge-computing.html)\n\n\n\nDer Wettlauf, KI an den Rand zu verlagern, nimmt Fahrt auf! Cadence Design Systems hat neue KI-geistige Eigentums (KI-IP)-Softwaretools vorgestellt, die Offloading-Fähigkeiten von Host-Prozessoren bieten. Diese Tools zielen darauf ab, KI-Arbeitslasten zu optimieren und die Leistung und Energieeffizienz in Endgeräten zu verbessern. Die Technologie ermöglicht eine effizientere Ausführung von KI-Aufgaben und eignet sich somit für verschiedene Anwendungen, einschließlich IoT und Edge Computing, bei denen eine Ressourcenoptimierung entscheidend ist. Cadence's KI-IP-Softwaretools können die Einführung von KI in Endgeräten beschleunigen. [Weiterlesen](https://www.edgeir.com/cadences-new-ai-ip-software-tools-promises-offloading-abilities-from-any-host-processor-20230914)\n\n\n\nDie Edge Computing Konferenzen sind in vollem Gange diesen Herbst, mit nicht weniger als sieben großen Konferenzen weltweit. Es beginnt in der letzten Woche im September in Amsterdam und führt unter anderem durch Toronto, Dubai, Paris und London. Packt eure Reisetaschen aus, denn dieser Herbst wird für die IoT Unternehmen viel zu tun haben. Die vollständige Liste findet ihr auf edgier.com. [Weiterlesen](https://www.edgeir.com/edge-computing-events)\n\n## Cybersicherheit\n\n\nDas Sicherheitsreaktionszentrum von Microsoft hat kritische Schwachstellen in der Open-Source-Softwarebibliothek NCurses identifiziert, die in Unix-ähnlichen Systemen (Linux und iOS) verwendet wird. Diese Mängel könnten Angreifern ermöglichen, bösartigen Code auszuführen oder Denial-of-Service-Angriffe zu verursachen. Benutzer werden dringend aufgefordert, ihre NCurses-Installationen auf die neueste Version zu aktualisieren, um potenzielle Risiken zu minimieren. Schwachstellen in weit verbreiteten Bibliotheken wie NCurses unterstreichen die Bedeutung regelmäßiger Sicherheitsupdates und Patch-Verwaltung im Software-Ökosystem. [Weiterlesen](https://thehackernews.com/2023/09/microsoft-uncovers-flaws-in-ncurses.html)\n\n\nDie Website des Free Download Managers (FDM), einer sehr beliebten Seite für kostenlose und Open-Source-Software, wurde kompromittiert und setzt Benutzer potenziellen Risiken aus. Angreifer haben bösartigen Code in die Website eingeschleust, der potenziell Benutzer betrifft, die die Software zwischen dem elften und zwölften September zweitausenddreiundzwanzig heruntergeladen haben. Dieser Verstoß unterstreicht die Bedeutung, Software nur von vertrauenswürdigen Quellen herunterzuladen und Sicherheitssoftware regelmäßig zu aktualisieren, um potenzielle Bedrohungen abzuwehren. FDM hat Maßnahmen ergriffen, um das Problem zu beheben, aber Benutzer sollten wachsam bleiben. [Weiterlesen](https://thehackernews.com/2023/09/free-download-manager-site-compromised.html)\n\n\nIranische staatliche Akteure sollen angeblich ausgeklügelte Spear-Phishing-Angriffe nutzen, um Personen in den Vereinigten Staaten, dem Nahen Osten und Asien ins Visier zu nehmen. Die Angriffe umfassen betrügerische E-Mails, die seriöse Organisationen imitieren und darauf abzielen, Malware zu verbreiten und vertrauliche Informationen zu stehlen. Sicherheitsexperten betonen die Notwendigkeit einer robusten E-Mail-Sicherheit und des Nutzerbewusstseins, um sich gegen solche Cyberbedrohungen zu verteidigen, die von staatlichen Akteuren orchestriert werden. [Weiterlesen](https://thehackernews.com/2023/09/iranian-nation-state-actors-employ.html)\n\n## Umarmung der digitalen Transformation\n\n\n\nDiese Woche setzt die Serie \"Embracing Generative AI\" fort, bei der Gäste über die Operationalisierung von generativer KI am Arbeitsplatz sprechen, einschließlich ihrer Verwendung zur Verbesserung der Sicherheit und zur Erstellung von Berichten in natürlicher Sprache. [Weiterlesen](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW33-de","image":"./briefs/edw-33/de/thumbnail.png","lang":"de","summary":"Bleiben Sie auf dem neuesten Stand der aktuellen Nachrichten zur digitalen Transformation für die Woche vom 17. September 2023. Erfahren Sie die neuesten Updates zu künstlicher Intelligenz, Edge Computing und Cybersicherheit. Zu den Highlights dieser Woche gehören Microsofts Aufdeckung von Sicherheitslücken in der Software ihrer Konkurrenten, Amazons Umsetzung von KI-Offenlegungsanforderungen und eine umfassende Liste an bevorstehenden IoT-Konferenzen diesen Herbst."},{"id":201,"type":"News Brief","title":"2023-9-23","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Künstliche Intelligenz\n\n\nPhysiker entwickeln generative Modelle für KI mittels gut verstandener physikalischer Gleichungen. Das generative Modell für Poisson-Strömungen (PFGM), das Daten mit geladenen Teilchen repräsentiert, kann hochwertige Bilder 10 bis 20 Mal schneller als Diffusionsmodelle erzeugen. Forscher streben danach, weitere physikalische Prozesse zu erforschen und das PFGM durch Anpassung seiner Dimensionalität zu verbessern. [Weiterlesen](https://www.quantamagazine.org/new-physics-inspired-generative-ai-exceeds-expectations-20230919/ )\n\n\nWie wäre es, einen Chatbot auf Ihrem PC zu betreiben? Intel hat einen neuen Chip angekündigt, der voraussichtlich im Dezember erscheint und es ermöglichen wird, dass Laptops generative künstliche Intelligenz-Chatbots ausführen können, ohne auf Cloud-Datenzentren angewiesen zu sein. Diese Fähigkeit, die auf ihrer Softwareentwicklerkonferenz vorgeführt wurde, ermöglicht es Unternehmen und Verbrauchern, AI-Technologien wie ChatGPT zu testen, ohne sensible Daten in die Cloud zu senden. [Weiterlesen](https://www.reuters.com/technology/intel-says-newest-laptop-chips-software-will-handle-generative-ai-2023-09-19/)\n\n\nDas Wachstum der Open-Source-Gemeinschaft macht Wellen in der KI-Entwicklung. Nicht ungewöhnlich für Innovationen haben Open-Source-Projekte wie Hadoop und Spark Entwicklern seit den 1970er Jahren ermöglicht, die KI-Entwicklung voranzutreiben. KI-generative Modelle wie ChatGPT und Llama 2 basieren auf Open-Source-Grundlagen und stellen den proprietären Status quo von KI-Programmen in Frage. Entwickler werden weiterhin die Grenzen der KI außerhalb des kommerziellen Entwicklungsrahmens vorantreiben. [Weiterlesen](https://www.zdnet.com/article/why-open-source-is-the-cradle-of-artificial-intelligence/)\n\n## Allgegenwärtiges Rechnen\n\n\nIntel hat seine Plattform \"Developer Cloud\" für Entwickler gestartet, um KI und High-Performance Computing-Anwendungen mit Hilfe der neuesten Hardware-Plattformen zu testen und bereitzustellen. Es unterstützt KI-Training, Modelloptimierung und Inferenz-Workloads und basiert auf Open-Source-Software mit oneAPI, was die Wahl der Hardware und die Portierbarkeit des Codes ermöglicht. Während der Ankündigung wurden auch andere KI-bezogene Technologien vorgestellt. Jeder kann sich unter cloud.intel.com anmelden. [Weiterlesen](https://ummid.com/news/2023/september/20-09-2023/intel-developer-cloud-reaches-general-availability.html)\n\n\nUm die mystischen und kostspieligen Ausgaben für die Datenverwaltung einzudämmen, hat WiMi Hologram Cloud einen neuen holografischen Datenkomprimierungsalgorithmus entwickelt, der die Effizienz des Cloud-Computings durch die Reduzierung von Datenübertragung und Speicherbedarf verbessert. Diese Fortschritte und ein verteiltes Bildspeicherprotokoll kommen der wachsenden Nachfrage nach fortschrittlicher Holografietechnologie entgegen. Dies sollte Organisationen helfen, die unter den Datenabgangskosten in der Cloud leiden. [Weiterlesen](https://beststocks.com/wimi-hologram-cloud-revolutionizing-cloud-com/ )\n\n\nAuf Intels Innovationsevent 2023 präsentierte das Unternehmen eine 288-Kern-CPU als Teil seiner 'Sierra Forest'-Produktreihe für hochdichte Server. Diese CPU wird 144 Kerne auf ihren Dual-Chiplets haben, insgesamt also 288 Kerne und Threads. Intel bestätigte außerdem, dass seine 5. Generation der Xeon-Familie, 'Emerald Rapids', am 14. Dezember starten wird und Leistungssteigerungen bei gleichbleibendem Energieverbrauch bietet. Intel betonte seine Fokussierung auf KI-Anwendungen mit einem Supercomputer für KI-Aufgaben, der Xeon-Prozessoren und Gaudi2-KI-Beschleuniger nutzt. [Weiterlesen](https://www.techspot.com/news/100221-intel-announces-288-core-sierra-forest-xeon-processor.html)\n\n## Cybersicherheit\n\n\nDie verstreute Spinne, eine geschickte Hacking-Gruppe, gewinnt an Bekanntheit für Ransomware-Angriffe auf Unternehmen. Analysten deuten darauf hin, dass die Gruppe größtenteils 17-22-Jährige aus westlichen Ländern sind. Sie verwenden fortgeschrittene soziale Manipulationstechniken, wie SIM-Tausch, und Taktiken wie \"SWATing\", um privilegierte Zugangspunkte zu identifizieren. Obwohl ihr Motiv vielleicht nicht finanziell ist, haben ihre Angriffe verschiedene Sektoren gestört und Ermittlungen der Strafverfolgungsbehörden ausgelöst. [Weiterlesen](https://www.reuters.com/technology/power-influence-notoriety-gen-z-hackers-who-struck-mgm-caesars-2023-09-22/)\n\n\nEin Bericht über die Trends bei Cyberkriminalität in Indien offenbart einen Anstieg von Cyberangriffen, einschließlich Phishing, Malware und Finanzbetrug, wobei COVID-19 die Situation verschlimmert. Die Ergebnisse betonen die Dringlichkeit von erhöhten Maßnahmen zur Cybersicherheit, öffentlicher Aufklärung und Zusammenarbeit zwischen Strafverfolgungsbehörden und privaten Sektoren, um Cyberbedrohungen effektiv zu bekämpfen. Die Schulung in bewährten Sicherheitspraktiken wird ebenfalls betont, um Einzelpersonen vor solchen Angriffen zu schützen. [Weiterlesen](https://www.theregister.com/2023/09/21/india_cybercrime_trends_report/)\n\n\nUm der Bedrohung der Computersicherheit entgegenzuwirken, versucht Microsoft Azure, die Computersicherheit durch Bildung zu erhöhen. Azure hat kürzlich über 60 Stunden kostenlose Kurse zur Cybersicherheit angekündigt, darunter unter anderem Sicherheit für hybride Clouds, Firewall-Konfiguration, Patch-Management, Active Directory-Verwaltung und Verschlüsselungstechnologien. Weitere Informationen finden Sie unter learn.microsoft.com. [Weiterlesen](https://www.helpnetsecurity.com/2023/09/20/free-microsoft-azure-cybersecurity-resources/)\n\n## Umarmung der digitalen Transformation\n\n\n\nDiese Woche beginnt eine neue Serie über den Aufbau einer Cloud-Strategie mit Cloud-Lösungsarchitekten von Intel. Darren interviewt Intel-Experten zu Cloud-Migration, Cloud-Kostenmanagement und Arbeitslastbewertung. Höre auf dem Podcast \"Embracing Digital Transformation\" das Neueste dazu. [Weiterlesen](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW34-de","image":"./briefs/edw-34/de/thumbnail.png","lang":"de","summary":"Digital Transformation News für den 24. September 2023 beinhaltet künstliche Intelligenz, ubiquitäres Computing und Cybersecurity. Erfahre mehr darüber, wie Intels neue CPUs die künstliche Intelligenz und die Hochleistungsrechner verändern und wie eine Hackergruppe von Teenagern die MGM Casinos in Las Vegas lahmgelegt hat."},{"id":202,"type":"News Brief","title":"2023-9-30","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","datasciencesolution","polystores","dataarchitecture","aichatbots","cybersecurityawareness","aisecuritycenter","googlechrome","mozillafirefox","digitaltransformationpodcast","intelcloudsolutionarchitects"],"body":"\n\n## Datenmanagement\n\n\n\nBMLL Vantage, eine Datenscience-Lösung, wurde mit dem \"Beste Datenscience-Lösung\"-Preis bei den Data Management Insight Awards ausgezeichnet. Diese Auszeichnung würdigt die herausragende Arbeit von BMLL Vantage im Bereich der Datenscience und anerkennt ihre bedeutenden Beiträge zur Branche. [Weiterlesen](https://mondovisione.com/media-and-resources/news/bmll-vantage-wins-best-data-science-solution-at-the-data-management-insight-aw/)\n\n\n\nPolystores integrieren verschiedene Datenquellen nahtlos und bauen Barrieren ab, um eine fachübergreifende Analyse zu ermöglichen. Sie optimieren die Leistung, indem sie die Stärken unterschiedlicher Datenbanken nutzen und sich an sich entwickelnde Technologien anpassen. Organisationen sollten Polystores nutzen, um das Potenzial von Daten für fundierte Entscheidungen im Zeitalter von Big Data zu erschließen. [Weiterlesen](https://thenewstack.io/polystores-the-data-management-game-changer/)\n\n\nBei der Entscheidung über eine Datenarchitektur ist es wesentlich, Faktoren wie Datenmenge, Häufigkeit, Quellenstandort, Typ, Datenschutz, Zugriff und Dringlichkeit zu berücksichtigen. Das Verständnis der verschiedenen Architekturtypen und ihrer Verwendung kann Ihnen helfen, sich an Veränderungen in den Geschäftstreibern und Umweltbedingungen anzupassen und die beste Lösung für Ihr Unternehmen zu wählen. Es wurde gezeigt, dass die Berücksichtigung dieser Faktoren die Entscheidungsfähigkeiten der Systemarchitekten verbessert. [Weiterlesen](https://www.intel.com/content/www/us/en/content-details/789953/content-details.html)\n\n## Künstliche Intelligenz\n\n\nMark Zuckerberg enthüllte auf der Meta Connect 2023 eine Reihe von KI-Chatbots, einschließlich einem mit Rapper Snoop Dogg als Dungeon Master im Rollenspiel D&D. Diese Bots, die nach Prominenten modelliert sind, sind nicht öffentlich verfügbar. Meta AI ist der führende Chatbot-Assistent und basiert auf einem maßgeschneiderten Sprachmodell, das OpenAI's ChatGPT ähnelt. Dabei wird besonderer Wert auf visuelle Raffinesse anstatt auf die Nachahmung von Konversationsmustern gelegt. Zukünftige Pläne umfassen die Hinzufügung von Sprachfähigkeiten. [Weiterlesen](https://kotaku.com/meta-quest-3-ai-chatgpt-snoop-dogg-facebook-chatbot-1850882666)\n\n\nDer israelische Ministerpräsident Netanyahu warnt vor den potenziellen Gefahren der AI-Revolution, wie z.B. Störungen der Demokratie, Manipulation des Geistes, Jobverlust und AI-gesteuerte Kriege. Er drängt die Nationen, sich mit diesen Bedenken auseinanderzusetzen und zu verhindern, dass selbstlernende Maschinen die Kontrolle über Menschen übernehmen. Er erkennt jedoch auch die positiven Aspekte der AI an, wie z.B. robotische Unterstützung für ältere Menschen und verbesserte Transportmöglichkeiten. Israel strebt danach, weltweit führend in der AI zu sein. [Weiterlesen](https://www.foxnews.com/world/netanyahu-warns-potential-eruption-ai-driven-wars-lead-unimaginable-consquences)\n\n\nEine Mitarbeiterin von OpenAI, Lilian Weng, teilte ihre emotionale Erfahrung mit der Verwendung des Sprachmodus von ChatGPT für ein persönliches Gespräch über Stress und Work-Life-Balance mit. Weng fand die Interaktion tröstlich, jedoch wirft dies Bedenken über die Rolle der KI bei der Bereitstellung von Therapie auf. Dies spiegelt einen Trend wider, bei dem KI menschlicher erscheinen möchte, jedoch mit Herausforderungen konfrontiert ist, wie dies bei vorherigen KI-Therapieexperimenten der Fall war, die gemischte Reaktionen erhalten haben und in einigen Fällen sogar Schaden verursacht haben. Ethische Überlegungen sind entscheidend, wenn KI in den Kontext der psychischen Gesundheit integriert wird. [Weiterlesen](https://fortune.com/2023/09/28/generative-ai-cfos-company-strategy/)\n\n## Cybersicherheit\n\n\nOktober markiert den zwanzigsten Monat der Cybersicherheitsaufklärung, um die Online-Sicherheit zu fördern. Das Thema in diesem Jahr heißt \"Sichern Sie unsere Welt\". Es konzentriert sich auf vier wesentliche bewährte Verfahren der Cybersicherheit: die Verwendung eines Kennwortmanagers, die Implementierung der Zwei-Faktor-Authentifizierung, das Erkennen und Melden von Phishing-Versuchen und das regelmäßige Installieren von Aktualisierungen. Die Initiative zielt darauf ab, Informationen bereitzustellen, um Einzelpersonen dabei zu helfen, online sicherer zu bleiben. [Weiterlesen](https://www.ktsm.com/local/octobers-cybersecurity-awareness-month-2023/)\n\n\nDie US-amerikanische National Security Agency hat ein AI-Sicherheitszentrum geschaffen, um KI in Verteidigungs- und Geheimdienstdiensten zu überwachen. Der Direktor, General Paul Nakasone, betonte die Bedeutung, die USA in der KI voranzubringen und zu verhindern, dass ausländische Akteure US-Innovationen stehlen. Das Zentrum wird sich für eine sichere Übernahme von KI in den Bereichen nationale Sicherheit und Verteidigungsindustrie einsetzen. KI wird eine bedeutende Rolle in der nationalen Sicherheit, Diplomatie, Technologie und Wirtschaft haben. [Weiterlesen](https://www.aljazeera.com/news/2023/9/29/us-national-security-agency-unveils-artificial-intelligence-security-centre)\n\n\nIn den Webbrowsern Google Chrome und Mozilla Firefox wurde ein neues Sicherheitsproblem gefunden. Es könnte auch andere Programme beeinträchtigen. Es wird \"StrangeU\" genannt. Hacker könnten es nutzen, um Ihren Computer zu kontrollieren. Die Unternehmen haben das Problem behoben, aber andere Programme könnten immer noch gefährdet sein. Halten Sie alle Ihre Software auf dem neuesten Stand und achten Sie auf ähnliche Probleme in anderen Anwendungen. Bleiben Sie sicher! [Weiterlesen](https://arstechnica.com/security/2023/09/new-0-day-in-chrome-and-firefox-is-likely-to-plague-other-software/)\n\n## Digitalisierungswandel Podcast umarmen\n\nDiese Woche geht die Serie über Multi-Hybrid Cloud-Architekturen weiter, mit zusätzlichen Interviews von den Cloud-Lösungsarchitekten von Intel. Um diese 30-minütigen Interviews anzuhören, findest du \"Embracing Digital Transformation\" auf deinen bevorzugten Podcasting-Plattformen. [Weiterlesen](https://www.embracingdigital.org/episode-EDT163)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW35-de","image":"./briefs/edw-35/de/thumbnail.png","lang":"de","summary":"Digitale Transformationsnachrichten für die Woche ab dem 1. Oktober, einschließlich Geschichten über Datenmanagement, künstliche Intelligenz und Cybersicherheit. Kommen Sie und hören Sie die Feier des cybersecurity awareness month, eine KI, die Snoop Dogg zum Dungeon Master für D&D macht, und erfahren Sie mehr über eine neue Datenarchitektur namens Polystore."},{"id":203,"type":"News Brief","title":"2023-10-7","tags":["ai","edge","cybersecurity","precrime","aihealcare","mgmbreach","scatteredspider","aplhav","aiethics","convergeditot","iot","criticalinfrastructure"],"body":"\n\n## Cybersicherheit\n\n\nAm 5. Oktober gab MGM Resorts International bekannt, dass ein Datenleck und ein Ransomware-Angriff zu Unterbrechungen geführt haben, die im dritten Quartal zu einem Verlust von hundert Millionen Dollar geführt haben. Die Hacking-Gruppen AlphaV und Scattered Spider beanspruchten die Verantwortung für das Leck, bei dem sie angeblich Daten aus dem MGM-System gestohlen und zur Erpressung genutzt haben. [Weiterlesen](https://www.reuters.com/business/mgm-expects-cybersecurity-issue-negatively-impact-third-quarter-earnings-2023-10-05/)\n\n\nSecurityWeek hat die Entdeckung von Firmware mit Backdoor in von US-Schulen verwendeten Android-Geräten gemeldet. Forscher haben Schwachstellen in Laptops und Tablets identifiziert, die an Bildungseinrichtungen verteilt wurden und potenziell sensible Schülerdaten Cyber-Bedrohungen aussetzen könnten. Die kompromittierte Firmware könnte Cyberkriminellen den remote Zugriff und die Manipulation der Geräte ermöglichen, sofern ausgenutzt. [Weiterlesen](https://www.securityweek.com/android-devices-with-backdoored-firmware-found-in-us-schools/)\n\n\nIm Rahmen des Monats der Cybersecurity-Awareness 2023 betont SC Media, dass mehr als nur die Förderung des Bewusstseins für Cybersicherheit erforderlich ist. SC Media fordert Organisationen dazu auf, den Austausch von Informationen über Cyberangriffe und bewährte Sicherheitspraktiken zu erhöhen und die Mythen über den Austausch wertvoller Informationen in der Cybersicherheitsbranche zu entlarven. [Weiterlesen](https://www.scmagazine.com/perspective/cybersecurity-awareness-month-2023-why-we-need-more-than-just-cybersecurity-awareness)\n\n## Künstliche Intelligenz\n\n\nLaut dem Bericht \"Freiheit im Netz\" von Freedom House aus dem Jahr 2023 nutzen Regierungen weltweit Künstliche Intelligenz zur Überwachung und Zensur. China führt bei der KI-gesteuerten Zensur. Dies unterstreicht die Notwendigkeit, den technologischen Fortschritt mit dem Schutz individueller Rechte und digitaler Freiheiten in Einklang zu bringen. Der Bericht fordert globale Bemühungen, die digitale Freiheit der Einzelpersonen zu schützen. [Weiterlesen](https://gizmodo.com/freedom-house-2023-freedom-on-the-net-report-ai-1850887842)\n\n\nEin kürzlich veröffentlichter Artikel in Nature behandelt Forschung über die Nutzung von KI-Algorithmen zur Vorhersage und Verwaltung epileptischer Anfälle. Die Studie präsentiert vielversprechende Ergebnisse in der genauen Vorhersage von Anfällen in einer Gruppe von Patienten und bietet das Potenzial für eine effektivere Anfallsverwaltung und verbesserte Patientenergebnisse. Die von KI gesteuerte Anfallsvorhersage könnte die Lebensqualität von Menschen mit Epilepsie signifikant verbessern und die damit verbundenen Risiken reduzieren. [Weiterlesen](https://www.nature.com/articles/s41746-023-00931-7)\n\n\nLaut einem Bericht von NBC Bay Area entwickelt ein Professor der UC Berkeley den Einsatz von künstlicher Intelligenz (KI), um häusliche Gewalt zu bekämpfen. Der Professor entwickelt KI-Algorithmen, um soziale Medienbeiträge und Textnachrichten auf Anzeichen häuslicher Gewalt zu analysieren. Ziel ist es, frühzeitig einzugreifen und Unterstützung für Opfer zu bieten. Diese innovative Anwendung der KI-Technologie hat das Potenzial, einen bedeutenden Einfluss auf die Identifizierung und Bewältigung von häuslicher Gewalt in der digitalen Ära zu haben. Klingt wie der Beginn von Pre-Crime in der Bay Area! [Weiterlesen](https://www.nbcbayarea.com/news/local/uc-berkeley-professor-artificial-intelligence-domestic-violence/3336053/)\n\n## Edge Computing\n\nEdge Computing, auch bekannt als Edge-Computing, ist ein Konzept in der Informatik, bei dem Rechen- und Speicherressourcen in der Nähe des Benutzers oder Geräts angesiedelt sind. Diese Ressourcen befinden sich an der sogenannten \"Edge\" des Netzwerks, also am Rand des zentralen Rechenzentrums oder der Cloud. Der Zweck von Edge Computing besteht darin, die Rechenleistung und den Datenspeicher näher an den Benutzer zu bringen, um eine geringere Latenz und eine schnellere Verarbeitungszeit zu erreichen. Dies kann besonders bei Anwendungen wie IoT-Geräten, autonomem Fahren und Augmented Reality von Vorteil sein. Edge Computing ermöglicht es, Daten schneller zu verarbeiten, ohne auf die Verbindung zum zentralen Rechenzentrum oder zur Cloud angewiesen zu sein. Es bietet auch mehr Sicherheit und Datenschutz, da sensible Daten nicht an entfernte Server gesendet werden müssen.\n\n\nForscher geben eine Warnung über die Offenlegung von ungefähr hunderttausend industriellen Steuerungssystemen (ICS) im Internet heraus, laut BleepingComputer. Diese alarmierende Feststellung wirft Bedenken über potenzielle Cyberangriffe auf kritische Infrastrukturen und Fertigungseinrichtungen auf. Die exponierten ICS-Geräte stellen ein erhebliches Sicherheitsrisiko dar und verdeutlichen den dringenden Bedarf an verbesserten Cybersicherheitsmaßnahmen zum Schutz kritischer Infrastrukturen vor potenziellen Bedrohungen und Angriffen. [Weiterlesen](https://www.bleepingcomputer.com/news/security/researchers-warn-of-100-000-industrial-control-systems-exposed-online/)\n\n\nWie in einem Bericht zu Game Is Hard diskutiert, etabliert sich Edge Computing als eine bemerkenswerte Alternative zur Dominanz von NVIDIA auf dem AI-Chip-Markt. Mit dem Aufkommen von Edge Computing erkunden immer mehr Unternehmen maßgeschneiderte AI-Chip-Lösungen für die lokale Verarbeitung und Echtzeit-Datenanalyse. Diese Entwicklung spiegelt die sich verändernde Landschaft der AI-Technologie und des Wettbewerbs auf dem Markt wider und bietet potenziell neue Optionen und Innovationen für verschiedene Branchen jenseits von NVIDIAs traditionellem Machtbereich. [Weiterlesen](https://gameishard.gg/news/the-rise-of-edge-computing-an-alternative-to-nvidias-dominance-in-the-ai-chip-market/282141/)\n\n\nEin Artikel des Forbes Tech Councils untersucht, wie Generative KI die Lücken bei Fähigkeiten in der Konvergenz von industrieller IT und operationeller Technologie (IoT) angeht. Durch Automatisierung von Aufgaben, Analyse von Daten und Optimierung von Prozessen hilft Generative KI, die Kluft zwischen diesen traditionell getrennten Bereichen zu überbrücken. Diese innovative Anwendung von KI-Technologie hat das Potenzial, Effizienz und Produktivität in industriellen Umgebungen zu steigern und eine reibungslose Integration und Betrieb von IT- und IoT-Systemen zu ermöglichen. [Weiterlesen](https://www.forbes.com/sites/forbestechcouncil/2023/10/04/how-generative-ai-fills-skills-gaps-for-industrial-itot-convergence/)\n\n## Umarmung der digitalen Transformationsnachrichten\n\n\n\nDiese Woche veröffentlicht Darren die letzte Folge der Serie \"Embracing Multi-Hybrid Cloud\", die sich auf kontinuierliche Verbesserung bei der Operationalisierung einer Cloud-Strategie konzentriert. Höre es auf jeder Podcast-Website unter Embracing Digital Transformation. [Weiterlesen](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW36-de","image":"./briefs/edw-36/de/thumbnail.png","lang":"de","summary":"Erhalten Sie die neusten Nachrichten zur Digitalen Transformation für die Woche vom 8. Oktober 2023. Die Highlights dieser Woche umfassen einen Bericht über den Verlust von 100 Millionen Dollar durch MGM Resorts aufgrund eines Cyberangriffs, wie KI zur Unterstützung von Opfern häuslicher Gewalt eingesetzt wird und die Auswirkungen des Edge Computing auf KI-Datenzentren. Bleiben Sie dran, um auf dem Laufenden zu bleiben."},{"id":204,"type":"News Brief","title":"2023-10-14","tags":["compute","edge","israelcyberattacks","hamasconflict","redalertsystemapp","powerplantcyberthreats","generativeai","cloudmigration","cmauk","publiccloudinvestigation","avoslocker","ransomeware","criticalinfrastructure","ecybersecurity","embracingdigital","edw37","zerotrustarchitecture"],"body":"\n\n## ## Cybersicherheit\n\n\nIsrael kämpft während des Hamas-Konflikts gegen Cyberangriffe. Die App des \"Rot-Alarm-Systems\" wurde kompromittiert, um falsche Warnungen und politische Nachrichten zu senden. Auch israelische Kraftwerke waren betroffen. Shawn Waldman, CEO von Secure Cyber Defense, warnt vor anhaltenden Cyberbedrohungen und verweist auf aktuelle Informationen über ausländische Akteure, die kritische US-Infrastruktur ins Visier nehmen. Er betont die Notwendigkeit eines kontinuierlichen Fortschritts in der Cybersicherheit. [Weiterlesen](https://www.wdtn.com/as-seen-on-2-news/israeli-cyberwarfare-cyberattacks-infrastructure/)\n\n\nIsrael und Palästina verzeichneten einen Anstieg des Internetverkehrs, nachdem Israel am siebten Oktober zweitausend dreiundzwanzig Krieg gegen die Hamas erklärt hatte. Die Daten von Cloudflare zeigen, dass die Cyberangriffe auf Israel zunahmen, einschließlich DDoS-Angriffen auf israelische Zeitungen. In Palästina wurden Internetunterbrechungen beobachtet, die möglicherweise mit Stromausfällen zusammenhängen. Cloudflare überwacht diese Trends und bietet Tools zur Verfolgung von Internetverkehrs-Mustern an. [Weiterlesen](https://blog.cloudflare.com/internet-traffic-patterns-in-israel-and-palestine-following-the-october-2023-attacks/)\n\n\nRansomware-Angriffe werden immer ausgefeilter, wobei Netzwerkgeräte zunehmend für die Verteilung ausgenutzt werden. Geschützte Gesundheitsinformationen im Gesundheitswesen sind ein bevorzugtes Ziel. Hochverdienende Organisationen, insbesondere in den USA, sind bevorzugte Ziele. Neuere Gruppen treten auf, und Sprachen wie Rust und GoLang werden übernommen. Organisationen verbessern ihre Cybersicherheitsmaßnahmen, und Cyble Vision wird empfohlen, um Ransomware-Bedrohungen einen Schritt voraus zu sein. [Weiterlesen](https://thehackernews.com/2023/10/ransomware-attacks-doubled-year-on-year.html)\n\n## Edge Computing\n\n\nDie AvosLocker-Ransomware-Bande ist in Angriffen gegen kritische Infrastruktursektoren in den USA verwickelt. Ihre Taktiken beinhalten die Nutzung legitimer Software und Open-Source-Tools zur Fernwartung, um Netzwerke zu kompromittieren, gefolgt von Datenextortion-Drohungen. AvosLocker tauchte Mitte 2021 auf und setzt Techniken ein, um den Antivirenschutz zu deaktivieren und Windows-, Linux- und VMware ESXi-Umgebungen zu beeinträchtigen. Die Gruppe ist dafür bekannt, Open-Source-Tools und \"Living-off-the-Land\" (LotL)-Taktiken zur Vermeidung von Attribution zu verwenden. CISA und das FBI empfehlen Abhilfemaßnahmen für Organisationen kritischer Infrastrukturen, darunter Anwendungssteuerungen, die Einschränkung von Remote-Desktop-Diensten, die Beschränkung der Nutzung von PowerShell und die Aufrechterhaltung von Offline-Backups. Ransomware-Angriffe haben sich im Jahr 2023 stark erhöht, wobei Angreifer nach dem ersten Zugriff schnell Ransomware bereitstellen. [Weiterlesen](https://thehackernews.com/2023/10/fbi-cisa-warn-of-rising-avoslocker.html)\n\n\nDie NSA hat ELITEWOLF veröffentlicht, ein Repository von Signaturen und Analysen auf ihrem GitHub, um die Sicherheit von Betriebstechnologie (OT) zu verbessern und bösartige Cyberaktivitäten gegen kritische Infrastrukturen zu bekämpfen. Angesichts der zunehmenden Bedrohung für OT-Systeme empfiehlt die NSA den Besitzern und Betreibern von OT-Infrastrukturen, ELITEWOLF als Teil der aufmerksamen Systemüberwachung zu nutzen. Diese Initiative folgt dem Cybersecurity-Berater \"Schützen Sie betriebstechnische Anlagen und Steuersysteme vor Cyberangriffen\". [Weiterlesen](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3554537/nsa-releases-a-repository-of-signatures-and-analytics-to-secure-operational-tec/)\n\n\nDie NATO hat geschworen, entschieden auf jeden bestätigten vorsätzlichen Angriff auf kritische Infrastruktur in der Ostsee zu reagieren. Das Versprechen erfolgt als Reaktion auf Schäden an Unterwasser-Infrastruktur in der Region. Finnland untersucht den Vorfall, der möglicherweise externe Kräfte, einschließlich Russland, involviert haben könnte. Wenn sich herausstellt, dass es sich um einen vorsätzlichen Angriff auf die kritische Infrastruktur der NATO handelt, wird die Organisation mit vereinten und entschlossenen Bemühungen reagieren. [Weiterlesen](https://news.yahoo.com/attack-alliances-critical-infrastructure-deliberate-073708273.html)\n\n## ## Cloud-Technologie\n\n\nGenerative KI reduziert die Anstrengungen für die Cloud-Migration um dreißig bis fünfzig Prozent und nutzt dabei große Sprachmodelle (LLMs). LLMs können Infrastrukturen bewerten, Arbeitslasten verschieben und die Effektivität von Migrationen überprüfen. Bhargs Srivathsan von McKinsey bemerkte, dass generative KI und die Cloud sich gegenseitig nutzen, da die Cloud generative KI ermöglicht, die wiederum die Cloud-Migration beschleunigt. LLMs können für die Generierung von Inhalten, Kundeneinbindung, synthetische Datenerstellung und Programmierung genutzt werden. [Weiterlesen](https://www.theregister.com/2023/10/11/generative_ai_cloud_migration/)\n\n\nUnternehmen mit unternehmensinternen Technologielösungen stehen bei der Umstellung auf die vollständige Cloud vor Störungen. Um diese Herausforderung zu bewältigen, ermöglicht der hybride Cloud-Ansatz Unternehmen, Kernservices unternehmensintern zu erhalten. Dieser Ansatz fügt der vorhandenen Infrastruktur neue Cloud-Funktionen hinzu, indem er Störungen minimiert und bewährte Technologien bewahrt. Zum Beispiel kann eine hybride Cloud in Contact-Center-Operationen verwendet werden, um Chat- und soziale digitale Kanäle über der bestehenden Infrastruktur zu integrieren. Dadurch können Organisationen Schwankungen im Anrufvolumen ohne signifikante Störungen bewältigen. [Weiterlesen](https://www.techradar.com/pro/why-businesses-should-stop-wondering-about-cloud-and-go-hybrid)\n\n\nDie britische Wettbewerbs- und Marktaufsichtsbehörde (CMA) untersucht den Markt für öffentliche Clouds, um fairen Wettbewerb für Unternehmen und Einzelpersonen zu gewährleisten. Die Untersuchung wird die Aktivitäten von Anbietern wie AWS, Microsoft Azure und Google Cloud genau prüfen, um Bedenken hinsichtlich wettbewerbswidrigen Verhaltens, Datenverwendung und Marktzugangsbarrieren anzugehen. Die CMA zielt darauf ab, Innovationen zu fördern, wettbewerbsfähige Preise anzubieten und den Kunden Auswahlmöglichkeiten zu bieten. [Weiterlesen](https://www.techrepublic.com/article/cma-investigates-uk-public-cloud-market/)\n\n## Embracing Digital Transformation Podcast -> Umarmung des digitalen Wandels Podcast\n\n\n\nIn der aktuellen Folge des Podcasts \"Embracing Digital Transformation\" spricht Darren mit Rachel Driekosen darüber, wie Künstliche Intelligenz eingesetzt wird, um Kinder online zu schützen und Täter zur Rechenschaft zu ziehen. Bleiben Sie dran für eine kommende Serie über die Architektur des Zero-Trust-Ansatzes in den nächsten paar Wochen. [Weiterlesen](https://www.embracingdigital.org/episode-EDT167-en)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW37-de","image":"./briefs/edw-37/de/thumbnail.png","lang":"de","summary":"Digitale Transformation diese Woche, am 15. Oktober 2023, berichtet über fortlaufende Cyberangriffe während des Israel-Hamas Konflikts und die Kompromittierung der App des 'Red Alert Systems'. Darüber hinaus hat sich gezeigt, dass generative Künstliche Intelligenz die Bemühungen bei der Cloud-Migration reduziert, während die CMA des Vereinigten Königreichs den öffentlichen Cloud-Markt untersucht."},{"id":205,"type":"News Brief","title":"2023-10-21","tags":["keywords","ai","compute","cybersecurity","iranianhackers","cybersecuritybreach","ironnetclosure","secretmicrosoft365","cloudtrends","healthcareai","chinachipexports","openai","digitalnews","ubiquitouscomputing","digitaltransformation","dataencryption","aiethics","techindustry","emergingtech","intelinterview","aistrategies","quantumcomputing","datasecurity","techtrends"],"body":"\n\n## ## Cybersicherheit\n\n\nIranische Hacker konnten erfolgreich in ein Netzwerk einer Regierung im Nahen Osten eindringen und blieben acht Monate lang unentdeckt, bevor sie entdeckt wurden. Der Angriff betraf unterschiedliche Behörden und das Ausmaß des Schadens wird noch bewertet. Experten untersuchen den Vorfall und betonen die Notwendigkeit starker Cybersicherheitsmaßnahmen, um zukünftige Angriffe zu verhindern. Der Vorfall erinnert daran, wie wichtig eine hohe Priorität für Cybersicherheit in unserer zunehmend digitalen Welt ist. [Weiterlesen](https://www.securityweek.com/iranian-hackers-lurked-for-8-months-in-government-network/)\n\n\nDer Clark County School District in Nevada musste nach einem Ransomware-Angriff auf ihre digitalen Ressourcen auf Kugelschreiber-und-Papier-Aufgaben zurückgreifen. Der Bezirk arbeitet daran, das Problem zu beheben und betont die Notwendigkeit von Cybersicherheit, um die Bildungsinfrastruktur vor Cyberbedrohungen zu schützen. [Weiterlesen](https://thenevadaindependent.com/article/clark-county-students-back-to-pen-and-paper-assignments-after-cybersecurity-breach)\n\n\nIronNet, ein ehemaliges Einhorn-Unternehmen im Bereich Cybersicherheit, hat den Betrieb eingestellt, nachdem es mit Herausforderungen bei der Finanzierung und im Marktumfeld konfrontiert wurde. Die Cybersicherheitsbranche befindet sich noch immer in der Wachstumsphase und muss sich an sich ändernde Bedrohungen und wirtschaftliche Drucke anpassen. Dies unterstreicht die Notwendigkeit von Widerstandsfähigkeit und Innovation. [Weiterlesen](https://www.scmagazine.com/news/more-cybersecurity-firm-closures-expected-after-ironnet-shutters)\n\n## Ubiquitäres Computing\n\n\nDas Pentagon implementiert eine geheimniskrämerische Version von Microsoft 365, die DOD365-Geheim genannt wird, um die Verteidigungsfähigkeiten des Landes zu stärken. Der neue Dienst wird sichere Kommunikations- und Kollaborationswerkzeuge bereitstellen und wird seit dem letzten Jahr von der Defense Information Systems Agency (DISA) getestet. Diese Maßnahme unterstreicht das Engagement der Regierung, die Cybersicherheit und den Datenschutz zu verbessern, insbesondere in sensiblen militärischen Umgebungen. [Weiterlesen](https://federalnewsnetwork.com/on-dod/2023/10/secret-level-version-of-microsoft-365-rolls-out-to-top-pentagon-offices-this-month/)\n\n\nForbes sagt voraus, dass zehn große Trends im Cloud Computing die Branche im Jahr 2024 prägen werden. Diese Trends umfassen Edge Computing, Cybersicherheit, Serverless Computing, Integration von künstlicher Intelligenz, hybride und Multi-Cloud-Nutzung, nachhaltige Cloud-Praktiken, Quantencomputing, Datenanalyse und Cloud-Native-Entwicklung. Es ist wichtig für Unternehmen und IT-Experten, mit diesen Trends auf dem Laufenden zu bleiben, um sich in der sich entwickelnden Cloud-Landschaft zurechtzufinden. [Weiterlesen](https://www.forbes.com/sites/bernardmarr/2023/10/09/the-10-biggest-cloud-computing-trends-in-2024-everyone-must-be-ready-for-now/?sh=2093527266d6)\n\n## Künstliche Intelligenz\n\n\nDer Tag Q ist auf dem Weg. Quantencomputer könnten bald stark genug werden, um die Verschlüsselungsalgorithmen zu knacken, die heute unsere digitale Information schützen. Dies bedroht die Datensicherheit, weshalb Regierungen und Technologieunternehmen in quantensichere Verschlüsselung und Technologie investieren, um sich dagegen abzusichern.Die Weltgesundheitsorganisation (WHO) hat am neunzehnten Oktober zweitausenddreiundzwanzig ihre Überlegungen zur Regulierung von künstlicher Intelligenz (KI) im Gesundheitssektor dargelegt. Diese Richtlinie betont die Bedeutung von Ethik, Transparenz, Datenschutz und Verantwortlichkeit bei KI-Anwendungen für die Gesundheit. Sie behandelt die zunehmende Rolle von KI im Gesundheitswesen und die Notwendigkeit eines Rahmens, der ihren verantwortungsvollen und ethischen Einsatz gewährleistet. [Weiterlesen](https://www.who.int/news/item/19-10-2023-who-outlines-considerations-for-regulation-of-artificial-intelligence-for-health)\n\n\n\nDie New York Times berichtet über Chinas Beschränkungen für den Export von KI-Chips. Diese Beschränkungen zielen darauf ab, die nationale Sicherheit und technologische Unabhängigkeit zu bewahren und beeinflussen weltweite Technologie-Lieferketten. Da KI eine zunehmend wichtige Rolle in verschiedenen Branchen spielt, spiegelt dieser Schritt Chinas Bemühungen wider, die Kontrolle über fortschrittliche Technologien zu erlangen und wirft Bedenken hinsichtlich der Dynamik des globalen Technologiehandels auf. [Weiterlesen](https://www.nytimes.com/2023/10/17/business/economy/ai-chips-china-restrictions.html)\n\n\nLaut dem Online-Journal \"The Information\" hat OpenAI angeblich die Entwicklung eines neuen KI-Modells namens \"Arrakis\" eingestellt und damit einen seltenen Rückschlag für die Organisation erlebt. Die Entscheidung verdeutlicht die Komplexitäten und Herausforderungen bei der Entwicklung fortschrittlicher KI-Modelle und unterstreicht die Bedeutung verantwortungsvoller und ethischer KI-Forschung und -Entwicklung. [Weiterlesen](https://www.theinformation.com/articles/openai-dropped-work-on-new-arrakis-ai-model-in-rare-setback)\n\n## ## Die neuesten Nachrichten über den Digitalen Transformations-Podcast\n\nIn seiner Serie \"Eintauchen in die Welt Künstlicher Intelligenz\" präsentiert Darren ein Interview mit Andy Morris, dem führenden KI-Strategen bei Intel. Das Interview geht auf das Thema des alltäglichen generativen KI ein und erkundet verschiedene Werkzeuge, die Menschen in ihrem täglichen Leben hilfreich sein können. Es handelt sich um ein informatives Gespräch, das Licht auf die Auswirkungen der KI in unserem Alltag wirft und wie wir sie zu unserem Vorteil nutzen können. [Weiterlesen](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW38-de","image":"./briefs/edw-38/de/thumbnail.png","lang":"de","summary":"Umarmung digitaler Nachrichten für die Woche ab dem 22. Oktober 2023, einschließlich Neuigkeiten aus der Cybersicherheit, ubiquitärem Computing und künstlicher Intelligenz. Zu den Highlights dieser Woche gehören die Rückkehr zum Stift und Papier bei schulischen Aktivitäten, das Beobachten des Q-Days und Richtlinien für KI im Gesundheitswesen."},{"id":206,"type":"News Brief","title":"2023-10-28","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Cybersicherheit\n\n\nKaspersky berichtet über eine neue Schadsoftware namens \"Elegant\", die fortschrittliche Spionagefähigkeiten aufweist und Ähnlichkeiten mit Hacking-Werkzeugen, die mit der NSA verbunden sind, aufweist. Das Cyber-Sicherheitsunternehmen hat die Schadsoftware nicht einem bestimmten Gefahrenakteur zugeordnet, aber ihre Entdeckung verdeutlicht die Notwendigkeit robuster Cyber-Sicherheitsmaßnahmen angesichts staatlich gesponserter Cyberangriffe. [Weiterlesen](https://cyberscoop.com/kaspersky-reveals-elegant-malware-resembling-nsa-code/)\n\n\n\nCISA und HHS haben ein Cybersicherheitstoolkit für Gesundheitsorganisationen gestartet. Das Toolkit zielt darauf ab, die Cybersicherheitslage der Branche zu verbessern, indem wertvolle Ressourcen und Anleitungen zur Minderung von Cyberrisiken angeboten werden. Das Ziel besteht darin, die Sicherheit der kritischen Gesundheitsinfrastruktur angesichts der steigenden Anzahl von Cyberrisiken und Ransomware-Angriffen auf Krankenhäuser zu stärken. Erfahren Sie mehr auf der CISA-Nachrichtenseite. [Weiterlesen](https://www.cisa.gov/news-events/news/cisa-hhs-release-collaborative-cybersecurity-healthcare-toolkit)\n\n\nKalifornien hat neue Cybersicherheitsvorschriften umgesetzt, einschließlich Risikobeurteilungen, Vorfallreaktionsplänen, Datenverschlüsselung und Multi-Faktor-Authentifizierung. Es ist entscheidend für Unternehmen in Kalifornien, sich zu fügen, da Nichteinhaltung zu Strafen und rechtlichen Konsequenzen führen kann. Bleiben Sie informiert, indem Sie regelmäßig die Updates auf der Website für Cybersicherheit in Kalifornien überprüfen. [Weiterlesen](https://www.armstrongteasdale.com/thought-leadership/californias-newest-cybersecurity-rule-what-you-need-to-know/)\n\n## Edge Computing\n\nEdge Computing (deutsch: Randbereichsberechnung) bezieht sich auf eine Technologie, bei der Datenverarbeitung und Berechnungen direkt am Rand des Netzwerks oder bei den Endgeräten selbst durchgeführt werden, anstatt sie an ein zentrales Rechenzentrum oder eine Cloud zu senden. Durch die Durchführung der Verarbeitung und Berechnung an der \"Kante\" des Netzwerks kann die Latenzzeit verringert werden und es können schnellere Entscheidungen in Echtzeit getroffen werden. Dies ist insbesondere in Anwendungen erforderlich, die eine schnelle Reaktion erfordern, wie zum Beispiel das autonome Fahren, das Internet der Dinge (IoT) und die Industrieautomatisierung. Edge Computing kann auch dazu beitragen, den Netzwerkverkehr zu entlasten und die Abhängigkeit von Cloud-Diensten zu reduzieren.\n\n\nDas britische Parlament untersucht die Cybersicherheit der kritischen Infrastruktur, einschließlich Energie, Transport und Gesundheitswesen. Die Untersuchung zielt darauf ab, die Bereitschaft und Widerstandsfähigkeit dieser Sektoren gegen potenzielle Cyberbedrohungen zu bewerten. Dieser Schritt erfolgt vor dem Hintergrund wachsender Bedenken hinsichtlich der Anfälligkeit von wichtigen Diensten für Cyberangriffe. Er unterstreicht die Bedeutung des Schutzes lebenswichtiger Dienste vor digitalen Bedrohungen in einer vernetzten Welt. Der jüngste Konflikt zwischen Israel und der Hamas macht die Notwendigkeit deutlich, die kritische Infrastruktur vor Cyberbedrohungen zu schützen. [Weiterlesen](https://www.bankinfosecurity.com/uk-parliament-probes-critical-infrastructure-cybersecurity-a-23400)\n\n\nTenable und Siemens Energy arbeiten zusammen, um die industrielle Cybersicherheit zu verbessern. Die Zusammenarbeit wird Tenable's Fachwissen in der Cybersicherheit mit Siemens Energy's Fachkenntnissen kombinieren, um fortschrittliche Sicherheitslösungen für lebenswichtige Infrastrukturen bereitzustellen. Das Ziel ist es, den Schutz für essenzielle Energiesysteme vor zunehmenden Cyberbedrohungen zu stärken. Die Bedeutung der Absicherung kritischer Infrastrukturen in einer zunehmend vernetzten Welt lässt sich nicht überschätzen. [Weiterlesen](https://finance.yahoo.com/news/tenable-siemens-energy-expand-collaboration-130000076.html)\n\n\nSingapurische Banken DBS und Citibank hatten einen Dienstausfall aufgrund eines Ausfalls des Kühlsystems in ihrem gemeinsamen Rechenzentrum. Der Vorfall unterbrach Bankdienstleistungen und verursachte Unannehmlichkeiten für Kunden. Solche Vorfälle verdeutlichen die Bedeutung einer robusten Rechenzentrum-Infrastruktur für kontinuierliche Finanzdienstleistungen. Die Banken haben seitdem ihre Dienstleistungen wiederhergestellt und betonen die entscheidende Rolle der Technologie im modernen Bankgeschäft. [Weiterlesen](https://www.channelnewsasia.com/singapore/dbs-citibank-outage-data-centre-cooling-system-down-3861076)\n\n## Überall präsente Datenverarbeitung\n\n\nAmazon hat einen unabhängigen Cloud-Service für Europa gestartet, um den wachsenden datenschutzrechtlichen Bedenken und behördlichen Anforderungen gerecht zu werden. Der neue Cloud-Service zielt darauf ab, europäischen Kunden eine verbesserte Datenhoheit und Sicherheit zu bieten, während Organisationen Cloud Computing auf eine lokalere und konforme Weise nutzen können. [Weiterlesen](https://www.euronews.com/next/2023/10/25/amazon-rolls-out-new-independent-cloud-for-europe)\n\n\nDie Vereinigten Staaten und Australien arbeiten zusammen an der Entwicklung fortschrittlicher Quantenchips. Diese Partnerschaft betont die zunehmende Bedeutung der Quantencomputertechnologie für die nationale Sicherheit und den technologischen Fortschritt. Da Länder danach streben, ihre Führung in diesem kritischen Bereich von Forschung und Entwicklung aufrechtzuerhalten, werden gemeinsame Anstrengungen wie diese immer häufiger. Die Zusammenarbeit zeigt die vertiefende Kooperation zwischen den beiden Verbündeten im Bereich Wissenschaft und Technologie. [Weiterlesen](https://foreignpolicy.com/2023/10/25/quantum-computing-united-states-australia-cooperation-allies-science-technology-chips/)\n\n\nEin kürzlich veröffentlichter Bericht stellte fest, dass vierzig Prozent der Unternehmen aufgrund von Technologieausfällen und der Verwaltung von Cloud-Services Umsatzeinbußen verzeichnen. Dies verdeutlicht die Bedeutung einer robusten IT-Infrastruktur und einer optimierten Cloud-Verwaltung. Eine umgehende Behebung von Ausfallzeiten und die Vereinfachung der Cloud-Operationen sind entscheidend, um unterbrechungsfreie Geschäftsabläufe zu gewährleisten und den Umsatz zu maximieren. Erfahren Sie mehr in dem Bericht auf Cloud-Computing News. [Weiterlesen](https://www.cloudcomputing-news.net/news/2023/oct/24/40-of-firms-lose-revenue-from-technology-downtime-and-cloud-complexity/)\n\n## Umarmung der digitalen Transformation Podcast\n\nEine neue Serie namens \"Embracing Zero Trust\" beginnt diese Woche. Dr. Anna Scott und Dave Marcus, die spezielle Gäste sind, werden die sechs Säulen des Zero Trust beschreiben und wie sie in bestehender Infrastruktur genutzt werden können. In der Zwischenzeit geht Darrens \"Embracing Generative AI\" weiter mit Sunny Stueve, einem Fachmann für Menschliche Faktoren von Leidos. [Weiterlesen](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW39-de","image":"./briefs/edw-39/de/thumbnail.png","lang":"de","summary":"In der neuesten Ausgabe unseres Embracing Digital This Week präsentieren wir Ihnen die neuesten Entwicklungen und Geschichten aus der Welt der Cybersicherheit, des Edge Computing und des ubiquitären Computings. In dieser Folge behandeln wir eine neue Malware namens Elegant, Cybersicherheitsvorschriften in Kalifornien, die Untersuchung des britischen Parlaments zur Cybersicherheit kritischer Infrastrukturen und andere interessante Neuigkeiten aus der Tech-Branche."},{"id":207,"type":"News Brief","title":"2023-2-26","tags":null,"body":"\n\n## Künstliche Intelligenz\n\nOpenAI und Bain & Company haben die Global AI Services Alliance gegründet, um das Geschäftspotenzial zu transformieren. Bain wird seine umfassenden digitalen Umsetzungsmöglichkeiten und strategischen Expertise mit OpenAIs KI-Tools und -Plattformen, einschließlich ChatGPT, kombinieren, um seinen Kunden weltweit zu helfen, den Wert von KI zur Maximierung des Geschäftspotenzials zu identifizieren und umzusetzen. Die Coca-Cola Company ist das erste Unternehmen, das sich mit der neuen Allianz engagiert.\n\nDie Cloud-Einheit von Amazon hat sich mit Hugging Face, dem Hersteller des ChatGPT-Rivalen, zusammengeschlossen. Hugging Face wird die nächste Version seines Sprachmodells namens BLOOM auf AWS aufbauen.\n\n[https://www.seattletimes.com/business/amazons-cloud-unit-partners-with-hugging-face-maker-of-chatgpt-rival/](https://www.seattletimes.com/business/amazons-cloud-unit-partners-with-hugging-face-maker-of-chatgpt-rival/)\n\nKI kann auch tatsächlich Gutes in der Welt bewirken. KI erträumt Medikamente, die noch niemand gesehen hat. Jetzt müssen wir sehen, ob sie funktionieren. Die ersten mit Hilfe von KI entworfenen Medikamente befinden sich nun in klinischen Studien, den rigorosen Tests an menschlichen Freiwilligen, um zu sehen, ob eine Behandlung sicher ist - und wirkt -, bevor Regulierungsbehörden sie für den weit verbreiteten Einsatz freigeben.\n\n[https://www.technologyreview.com/2023/02/15/1067904/ai-automation-drug-development](https://www.technologyreview.com/2023/02/15/1067904/ai-automation-drug-development)\n\n## Intelligenter Rand\n\nSicherheitslücken wurden in drahtlosen industriellen IoT-Geräten von vier Anbietern gefunden, was eine erhebliche Angriffsfläche für Bedrohungsakteure schafft, die operative Technologieumgebungen (OT) ausnutzen möchten. Die Schwachstellen bieten Eintrittspunkte für Remote-Angriffe, die es unauthentifizierten Gegnern ermöglichen, einen Fuß in die Tür zu bekommen und ihn als Hebel zu nutzen, um sich auf andere Hosts auszubreiten und dadurch ernsthafte Schäden zu verursachen.\n[https://thehackernews.com/2023/02/critical-infrastructure-at-risk-from.html](https://thehackernews.com/2023/02/critical-infrastructure-at-risk-from.html)\n\nDie Connectivity Standards Alliance (CSA) hat eine Arbeitsgruppe gebildet, die sich der Förderung der Nutzung von \"Matter\" für Gesundheitszwecke widmet. Matter ist ein interoperabler Standard für Smart-Home-Geräte, der letztes Jahr von der CSA veröffentlicht wurde. Die Arbeitsgruppe \"Gesundheit und Wohlbefinden\" wird wichtige Akteure in der Gesundheitsbranche um den Matter-Standard versammeln, um sichere und interoperable Geräte für Gesundheit und Wohlbefinden sicherzustellen.\n[https://www.iottechnews.com/news/2023/feb/10/matter-creator-csa-announces-healthcare-working-group/](https://www.iottechnews.com/news/2023/feb/10/matter-creator-csa-announces-healthcare-working-group/)\n\nRobo-Taxis aus dem selbstfahrenden Fahrzeugunternehmen Zoox von Amazon sind jetzt auf den Straßen von Foster City, Kalifornien. Die Fahrzeuge von Zoox sind zweckgebaut für autonomes Fahren und verfügen über keinerlei manuelle Steuerungen. Die Vollzeitmitarbeiter des Unternehmens sind die ersten, die den autonomen Shuttle-Service zwischen ihren Standorten nutzen. Das erfordert absolutes Vertrauen in die Qualität der Arbeit Ihres Teams!\n[https://www.iottechnews.com/news/2023/feb/14/amazon-robotaxis-hit-the-streets-of-california/](https://www.iottechnews.com/news/2023/feb/14/amazon-robotaxis-hit-the-streets-of-california/)\n\n## Datenmanagement\n\nIn einer kürzlich durchgeführten Studie von Informatica zur Datenausbreitung stellte Informatica fest, dass verteilte Daten und die Notwendigkeit einer einheitlichen Verwaltung für viele Organisationen weit verbreitete Probleme sind. Darüber hinaus haben Datenqualität und Governance oberste Priorität. Dies führt zu einer erhöhten Investition in Daten-Technologien in diesem Jahr. Die Umfrage-basierte Forschung ergab, dass 78% der CDOs sagten, dass sie sich strategisch mit ertragsgenerierenden Organisationen ausrichten müssen, um die zusätzlichen Ausgaben zur Verbesserung von Datenanalyse und Governance zu rechtfertigen. \n[https://accelerationeconomy.com/data/informatica-research-highlights-data-sprawl-why-management-needs-to-be-unified/](https://accelerationeconomy.com/data/informatica-research-highlights-data-sprawl-why-management-needs-to-be-unified/)\n\nAlation Inc., ein Anbieter von Enterprise-Lösungen für Datenintelligenz, hat Alation Marketplaces veröffentlicht. Dieses neue Produkt ermöglicht es Drittanbieter-Datensätzen, bestehende Daten im Alation Data Catalog zu ergänzen, so dass Benutzer externe Drittanbieter-Daten erkunden und vorhandene Datensätze ergänzen können. Zusätzlich hat das Unternehmen seine Angebote Alation Anywhere für Microsoft Teams und Alation Connected Sheets für Microsoft Excel erweitert, um Datenbenutzern den direkten Zugriff auf kontextbezogene Informationen aus dem Katalog innerhalb ihres bevorzugten Tools zu ermöglichen.\n[https://www.alation.com/press-releases/alation-launches-data-marketplaces/](https://www.alation.com/press-releases/alation-launches-data-marketplaces/)\n\n\n\n","guests":null,"link":"/brief-EDW4-de","image":"./briefs/edw-4/de/thumbnail.png","lang":"de","summary":"Summary"},{"id":208,"type":"News Brief","title":"2023-11-11","tags":["homelawncaretools","ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Cybersicherheit\n\n\n\nAustralische Häfen kämpfen mit einem Cyberangriff, der die Aktivitäten von DP World beeinträchtigt. Der Angriff auf den Versandriesen hat zu Störungen bei den Containerbewegungen geführt. Während das Ausmaß und der Ursprung des Verstoßes untersucht werden, wirft es Bedenken über die Anfälligkeit kritischer Infrastrukturen für Cyberbedrohungen auf. Die Behörden arbeiten daran, die Normalität wiederherzustellen, da der Vorfall die wachsende Bedeutung der Cybersicherheit bei der Sicherung wesentlicher Dienstleistungen unterstreicht. [Weiterlesen](https://www.bloomberg.com/news/articles/2023-11-11/australian-authorities-meet-as-dp-world-shuts-ports-on-cyber-act)\n\n\nDie ICBC, die weltweit größte Bank, wurde Opfer eines Ransomware-Cyberangriffs. Der Vorfall hat den Betrieb gestört und die Kundendienste beeinträchtigt. Während das Ausmaß des Angriffs bewertet wird, unterstreicht dies die zunehmende Bedrohung durch Cyberkriminalität für Finanzinstitute. Die ICBC arbeitet daran, die Sicherheitslücke zu schließen und die Normalität wiederherzustellen, und betont die Notwendigkeit robuster Cybersicherheitsmaßnahmen im globalen Bankensektor. [Weiterlesen](https://www.cnbc.com/2023/11/10/icbc-the-worlds-biggest-bank-hit-by-ransomware-cyberattack.html)\n\n\nCyberangriffe unterstützen physische Angriffe! Die russische Hackergruppe SandWorm wird in einen Cyberangriff verwickelt, der die Stromversorgung in der Ukraine störte. Der ausgeklügelte Angriff zielte auf die Energieinfrastruktur ab und verursachte weit verbreitete Ausfälle. Die ukrainischen Behörden untersuchen den Vorfall und heben die anhaltende Bedrohung durch staatlich geförderte Cyberangriffe auf kritische Infrastrukturen hervor. Die Situation unterstreicht die Notwendigkeit verstärkter Cybersicherheitsmaßnahmen zum Schutz wesentlicher Dienstleistungen. [Weiterlesen](https://thehackernews.com/2023/11/russian-hackers-sandworm-cause-power.html)\n\n## Künstliche Intelligenz\n\n\nKI-Experten warnen vor einem möglichen Datenmangel und betonen die schwindenden Datenressourcen der Welt, um künstliche Intelligenz zu speisen. Das exponentielle Wachstum von KI-Anwendungen und datenhungrigen Modellen überholt die Datengenerierung. Diese Knappheit könnte weitere Fortschritte in der KI behindern und die Notwendigkeit innovativer Ansätze zur Datenerfassung und -synthese hervorrufen. Die Herausforderung unterstreicht die Bedeutung einer verantwortungsvollen und nachhaltigen Entwicklung der KI. [Weiterlesen](https://www.sciencealert.com/the-world-is-running-out-of-data-to-feed-ai-experts-warn)\n\n\nDie Verbreitung von KI bedroht die Demokratie! warnen Experten auf der Reuters Next Konferenz. Der rasche Fortschritt der künstlichen Intelligenz weckt Bedenken hinsichtlich ihrer möglichen missbräuchlichen Verwendung für Überwachung, Manipulation und Erosion demokratischer Werte. Experten fordern ethische Leitlinien, regulatorische Rahmenbedingungen und öffentliche Diskurse, um die Risiken zu mildern, die mit dem wachsenden Einfluss von KI auf Politik und Gesellschaft verbunden sind. [Weiterlesen](https://www.reuters.com/technology/reuters-next-rapid-ai-proliferation-is-threat-democracy-experts-say-2023-11-08/)\n\n\nIn einem Interview äußerte Barack Obama Besorgnis über die Auswirkungen künstlicher Intelligenz (KI) auf Arbeitsplätze. Er anerkennt das Potenzial für Arbeitsplatzverdrängung durch Automatisierung und betont die Notwendigkeit von Maßnahmen, um diese Herausforderungen anzugehen. Obama schlägt vor, in Bildung und Umschulungsprogramme zu investieren, um die Belegschaft auf sich verändernde Arbeitsmärkte vorzubereiten, die von KI und Automatisierung geprägt sind. [Weiterlesen](https://www.independent.co.uk/tv/news/barack-obama-biden-ai-jobs-b2445354.html)\n\n## Allgegenwärtiges Computing\n\n\nEin Durchbruch im Quantencomputing integriert Künstliche Intelligenz und maschinelles Lernen für eine robuste Qubit-Fehlerkorrektur. Wissenschaftler haben einen von Künstlicher Intelligenz verbesserten Ansatz für das Quantencomputing entwickelt, der die Fehlerkorrektur für Qubits verbessert, eine entscheidende Herausforderung im Quantencomputing. Diese innovative Kombination aus Künstlicher Intelligenz und Quantentechnologie verspricht Fortschritte bei der Zuverlässigkeit und Leistung von Quantencomputern. [Weiterlesen](https://scitechdaily.com/ai-enhanced-quantum-computing-machine-learning-powers-robust-qubit-error-correction/)\n\n\nCloud Software Group, das Mutterunternehmen von Citrix, stellt neue kommerzielle Transaktionen in China, einschließlich Hongkong und Macau, mit Wirkung vom dritten Dezember ein. Mit Verweis auf die \"steigenden Kosten\" des Betriebs in der Region folgt diese Entscheidung ähnlichen Schritten anderer amerikanischer Technologieunternehmen angesichts einer schwächeren wirtschaftlichen Aussicht und strengerer Datenschutzbestimmungen in China. Während immer mehr US-Unternehmen China verlassen, hat Peking vorgeschlagen, die grenzüberschreitenden Datenschutzkontrollen zu lockern, um ausländische Investoren anzuziehen. [Weiterlesen](https://finance.yahoo.com/news/citrix-owner-cloud-software-becomes-093000327.html)\n\n\nEin kürzlicher Artikel von Analytics Insights prognostiziert Cloud-Computing-Trends für zweitausendvierundzwanzig. Dazu gehören die Bedeutung des Edge-Computings für eine schnellere Datenverarbeitung, der Aufstieg von Multi-Cloud-Strategien für Flexibilität und die Integration von KI/ML für eine optimierte Datenanalyse. Sicherheitsmaßnahmen wie die Zero-Trust-Architektur werden bei der Bewältigung der sich entwickelnden Bedrohungslandschaft Priorität haben. [Weiterlesen](https://www.analyticsinsight.net/cloud-trends-for-2024-whats-on-for-cloud-computing/)\n\n## Podcast zur Umarmung der Digitalen Transformation\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW41-de","image":"./briefs/edw-41/de/thumbnail.png","lang":"de","summary":"Digitaler Umarmung In dieser Woche des 12. November 2023, einschließlich Nachrichten im Bereich der Cybersicherheit. Künstliche Intelligenz und allgegenwärtiges Computing. Erfahren Sie mehr über mehrere große Cyber-Angriffe auf kritische Infrastrukturen, KI, die keine Daten mehr hat, und Durchbrüche in der Quantenrechnung."},{"id":209,"type":"News Brief","title":"2023-11-18","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Cybersicherheit\n\n\nDie Agentur für Cybersicherheit und Infrastruktursicherheit (CISA) plant, die Vorstandsverantwortung für die Software-Sicherheit bei der Beschaffung durch die Regierung durchzusetzen. Die Initiative zielt darauf ab, Vorstände die Sicherheit der an die Regierung verkauften Software bestätigen zu lassen. Dieser Schritt ist Teil der Bemühungen der CISA, die Maßnahmen zur Cybersicherheit zu verbessern und die Integrität der in kritischen Regierungssystemen verwendeten Software zu gewährleisten. [Weiterlesen](https://federalnewsnetwork.com/cybersecurity/2023/11/cisa-aims-to-make-executives-sign-off-on-security-of-software-sold-to-government/)\n\n\nUS-amerikanische Cybersicherheitsbehörden geben eine Warnung vor einer neuen Ransomware-Bedrohung ab, die kritische Infrastrukturen ins Visier nimmt. Der Alarm unterstreicht die potenziellen Risiken und drängt Organisationen dazu, ihre Cybersicherheitsprotokolle zu verbessern. Die Behörden betonen die Bedeutung von proaktiven Maßnahmen, einschließlich der regelmäßigen Aktualisierung von Software und der Anwendung robuster Sicherheitspraktiken, um das Risiko von Ransomware-Angriffen zu mindern.\n [Weiterlesen](https://thehackernews.com/2023/11/us-cybersecurity-agencies-warn-of.html)\n\n\n\nSolarWinds sieht sich einer Klage der Securities and Exchange Commission (SEC) nach dem Cyberangriff im Jahr Zweitausendzwanzig gegenüber. Die SEC wirft dem Unternehmen vor, Schwachstellen nicht offengelegt und so Investoren irregeführt zu haben. SolarWinds, bekannt für seine IT-Management-Software, erlitt einen hochkarätigen Lieferkettenangriff. Die Klage unterstreicht die zunehmende Konzentration auf Offenlegungspflichten in Bezug auf Cybersicherheit und die möglichen rechtlichen Folgen für Unternehmen, die solche Vorfälle nicht ausreichend angehen. [Weiterlesen](https://www.nytimes.com/2023/11/18/business/dealbook/solarwinds-sec-lawsuit.html)\n\n## Künstliche Intelligenz\n\n\nOpenAI-Chef Sam Altman tritt zurück, da die Sorgen über die Unternehmensführung zunehmen. Altmans Weggang erfolgt nach internen Spannungen und Rücktritten aufgrund des Unternehmensschwerpunkts auf kommerzielle Anwendungen statt seines ursprünglichen Engagements für umfassende KI-Vorteile. Der Schritt wirft Fragen über das Gleichgewicht zwischen Unternehmensinteressen und ethischen Überlegungen in der KI-Entwicklung auf. [Weiterlesen](https://www.washingtonpost.com/technology/2023/11/17/openai-ceo-resigns/)\n\n\nDie jüngste KI-Veranstaltung von Microsoft hebt drei Haupterkenntnisse hervor. Erstens betont das Unternehmen seine Verpflichtung zur verantwortungsvollen KI-Entwicklung und konzentriert sich dabei auf Fairness, Transparenz und Rechenschaftspflicht. Zweitens zielt Microsoft darauf ab, Entwickler mit Werkzeugen wie OpenAIs GPT zu stärken, das erweiterte Fähigkeiten in natürlicher Sprache bietet. Zuletzt unterstreicht die Veranstaltung Microsofts Engagement für von KI angetriebene Innovationen in verschiedenen Branchen und präsentiert Anwendungen im Gesundheitswesen, in der Finanzbranche und in der Klimaforschung. [Weiterlesen](https://www.cnbc.com/2023/11/17/here-are-three-key-takeaways-from-microsofts-bullish-ai-event-.html)\n\n\nYouTube sieht sich der Herausforderung von Deepfake-Inhalten gegenüber, da KI-Ersteller zunehmend überzeugende und irreführende Videos produzieren. Die Plattform hat Schwierigkeiten, zwischen harmlosen und bösartigen Verwendungen von KI-generierten Inhalten zu unterscheiden. YouTube erkennt die Notwendigkeit von Wachsamkeit und einem umfassenden Ansatz zur Bekämpfung von Deepfakes an, um die Integrität seiner Inhalte aufrechtzuerhalten. Inhalts-Ersteller werden nun aufgefordert zu überprüfen, ob der Inhalt von einer KI erzeugt wurde oder nicht. [Weiterlesen](https://apnews.com/article/youtube-artitifical-intelligence-deep-fake-ai-creaters-0513fd9fddbd93af327f0411dd29ff3d)\n\n## Allgegenwärtiges Computing\n\n\n\nAmazon und Microsoft liefern sich einen heftigen Wettkampf um lukrative Regierungsverträge für Cloud-Computing, da Bundesbehörden zunehmend auf Cloud-Dienste angewiesen sind. Die Unternehmen konkurrieren um den Vertrag für die Joint Warfighter Cloud Capability (JWCC) des Verteidigungsministeriums, einen Vertrag, der potenziell zehn Milliarden Dollar wert ist. Diese Auseinandersetzung spiegelt den allgemeineren Trend wider, dass Tech-Giganten um Regierungsverträge für Cloud-Dienste konkurrieren und unterstreicht die strategische Bedeutung von Cloud-Diensten bei der Modernisierung der Regierungsinfrastruktur. [Weiterlesen](https://www.bloomberg.com/news/articles/2023-11-16/amazon-amzn-microsoft-msft-fight-for-cloud-computing-government-contracts)\n\n\nDie US-Marine verbessert ihre Torpedosysteme, indem sie Cloud-Computing für U-Boote nutzt. Dieses Upgrade soll die Leistung und Fähigkeiten der Torpedos verbessern. Die Marine sucht die Datenverarbeitung und -analyse durch die Nutzung von Cloud-Computing zu verbessern, was einen ausgefeilteren und effizienteren Torpedobetrieb ermöglicht. Dieser Schritt spiegelt die anhaltenden Bemühungen des Militärs wider, fortschrittliche Technologien, wie Cloud-Computing, zu integrieren, um die Fähigkeiten seiner Marinesysteme zu stärken. [Weiterlesen](https://www.defensenews.com/naval/2023/11/16/us-navy-upgrading-torpedoes-leveraging-cloud-computing-for-submarines/)\n\n\nDie Regierung der Vereinigten Staaten mahnt Alibaba wegen Bedenken bezüglich der möglichen Missbrauchs von amerikanischen Chips in ihrer Überwachungstechnologie an. Der Schritt ist Teil der breiteren Bemühungen, den Export von sensiblen Technologien aufgrund von nationalen Sicherheitsbedenken einzuschränken. Diese Entwicklung unterstreicht die Untersuchung chinesischer Technologieunternehmen. Sie hebt den Fokus der US-Regierung auf die Verhinderung des unbeabsichtigten Einsatzes amerikanischer Technologie in einer Weise hervor, die die Sicherheit oder Menschenrechte gefährden könnte. [Weiterlesen](https://finance.yahoo.com/news/alibaba-flagged-us-chip-curbs-192349433.html)\n\n## Umarmung der Digitalen Transformation Podcast Nachrichten\n\n\n\nSchauen Sie sich die drei Serien mit dem Titel \"Generative KI umarmen\", \"Null Vertrauen umarmen\" und \"Multi-Hybride Cloud umarmen\" an. In diesen Serien erforscht Darren die Welt der KI mit Gästen aus der Industrie, dem Bildungssektor und dem Alltag, lädt Cybersecurity-Experten ein, um über den Schutz der digitalen Welt zu sprechen, und sucht den Himmel nach den neuesten Cloud-Strategien und -Technologien ab. [Weiterlesen](https://www.embracingdigital.org/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW42-de","image":"./briefs/edw-42/de/thumbnail.png","lang":"de","summary":"Digitale Transformation für Cybersicherheit, KI und Computing in der Woche vom 19. November 2023. Themen sind unter anderem die rechtlichen Auswirkungen der Cybersicherheit, der Weggang von Sam Altman von OpenAI und Cloud-Anbieter, die um Regierungsverträge konkurrieren."},{"id":210,"type":"News Brief","title":"2023-11-25","tags":["uniquehandmadewoodfurniture.","ai","compute","cybersecurity","samaltman","openai","aibreakthrough","neuralchat7b","intel","responsibleai","meta","hacking","insiderthreats","securitymeasures","eucybersecurityregulations","clearfake","malware","ubiquitouscomputing","macstadium","m2prochip","alibabacloud","cloudcomputing"],"body":"\n\n## Künstliche Intelligenz\n\n\nSam Altman verließ OpenAI, nachdem ein Brief eingegangen war, der Bedenken bezüglich eines nicht offengelegten Durchbruchs im Bereich Künstliche Intelligenz äußerte. Der Brief, der von einem anonymen Mitarbeiter versendet wurde, führte zur Entlassung Altmans als CEO. Die Details des Durchbruchs bleiben vertraulich und werfen Fragen über die internen Dynamiken von OpenAI auf. Altman wurde wieder eingestellt, als siebenhundert der siebenhundertsiebzig Mitarbeiter ihm zu Microsoft folgten. [Weiterlesen](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/)\n\n\nIntel hat das NeuralChat SiebenB entwickelt, ein Chatbot-Modell, das den Schutz von Benutzerdaten priorisiert. Das Modell wird mithilfe der Optimierung durch Differentialdatenschutz (DPO) trainiert, um Datenschutzmaßnahmen zu verbessern. Durch die Implementierung von DPO zielt Intel darauf ab, sichere Interaktionen innerhalb des Chatbots sicherzustellen und Datenschutzrisiken zu minimieren. Diese Bemühungen entsprechen den sich entwickelnden Standards für verantwortungsvolle KI-Entwicklung. [Weiterlesen](https://medium.com/@bnjmn_marie/neuralchat-7b-intels-chat-model-trained-with-dpo-e691dfd52591)\n\n\nMeta, das Mutterunternehmen von Facebook, hat Berichten zufolge sein Team für verantwortungsvolle KI aufgelöst, was Bedenken hinsichtlich seines Engagements für ethische KI-Praktiken aufwirft. Kritiker argumentieren, dass die Auflösung des Teams die Bemühungen zur Bewältigung ethischer Bedenken und potenzieller Vorurteile in den KI-Technologien von Meta untergraben könnte und betonen die anhaltende Debatte um die verantwortungsvolle Entwicklung von KI. [Weiterlesen](https://www.theverge.com/2023/11/18/23966980/meta-disbanded-responsible-ai-team-artificial-intelligence)\n\n## Cybersicherheit\n\n\nEin Manager eines Cybersecurity-Unternehmens hat sich des Hackens von Krankenhäusern schuldig bekannt, in einer schockierenden Wendung der Ereignisse. Der Manager, zuvor mit dem Schutz von Systemen betraut, räumte ein, Schwachstellen für persönlichen Gewinn ausgenutzt zu haben. Der Verstoß wirft ernsthafte Fragen bezüglich interner Bedrohungen und der Notwendigkeit robuster Sicherheitsmaßnahmen innerhalb des Cybersecurity-Sektors selbst auf. Das Schuldbekenntnis unterstreicht das Imperativ für strenge Cybersecurity-Protokolle und ethisches Verhalten, besonders wenn man mit dem Schutz kritischer Infrastruktur wie Gesundheitseinrichtungen betraut ist. [Weiterlesen](https://www.bleepingcomputer.com/news/security/cybersecurity-firm-executive-pleads-guilty-to-hacking-hospitals/)\n\n\nDie Europäische Union plant, die Cybersicherheitsvorschriften auf kritische Sektoren wie Energie, Verkehr und digitale Dienstleister wie Amazon, Google und Microsoft auszuweiten. Dies zielt darauf ab, die Cybersicherheit in verschiedenen Branchen aufgrund zunehmender Cyberbedrohungen zu stärken. Einige betrachten jedoch die neuen Vorschriften als Mittel, um Einnahmen von Big Tech zu generieren. [Weiterlesen](https://www.finextra.com/newsarticle/43338/eu-considers-widening-scope-of-cybersecurity-regulation)\n\n\nEine neue Schadsoftware namens ClearFake wurde entdeckt, die Mac-Benutzer ins Visier nimmt. Sie tarnt sich als eine legitime Antiviren-App und täuscht Benutzer dazu, schädliche Software herunterzuladen. Experten warnen vor dem möglichen Diebstahl von Daten und einer Gefährdung des Systems. Es ist unerlässlich, vorsichtig zu sein und namhafte Sicherheitssoftware zu verwenden, um sich gegen sich weiterentwickelnde Bedrohungen, die sich gegen macOS-Plattformen richten, zur Wehr zu setzen. [Weiterlesen](https://cybersecuritynews.com/clearfake-new-malware-mac/)\n\n## Allgegenwärtiges Computing\n\n\nDer neue Mac Cloud von MacStadium, angetrieben durch den M2 Pro-Chip, bietet erweiterte Leistung und Fähigkeiten für macOS-Entwickler und Unternehmen. Er bedient die wachsende Nachfrage nach Kubernetes-basierter Mac-Entwicklung in Cloud-Umgebungen und erfüllt die einzigartigen Bedürfnisse der Apple-Entwicklungsgemeinschaft. Der M2 Pro-Chip verbessert die Mac-Hosting-Fähigkeiten erheblich und bietet verbesserte Geschwindigkeit und Effizienz. [Weiterlesen](https://finance.yahoo.com/news/macstadium-unveils-powerful-next-generation-120000257.html)\n\n\n\nAlibaba hat seiner Cloud-Computing-Einheit drei neue Führungskräfte hinzugefügt, was sein Engagement für die Stärkung seines Cloud-Geschäfts zeigt. Die Führungskräfte bringen vielfältige Expertise auf den wettbewerbsintensiven Cloud-Computing-Markt und werden Innovation und Wachstum vorantreiben. Alibabas Fokus auf Cloud-Dienste steht im Einklang mit den sich entwickelnden Anforderungen der digitalen Landschaft und bekräftigt seinen Entschluss, eine führende Position im globalen Cloud-Computing-Sektor zu behaupten. [Weiterlesen](https://www.proactiveinvestors.com.au/companies/news/1034236/alibaba-taps-three-new-cloud-computing-executives-after-nixing-spinoff-ipo-plans-1034236.html)\n\n\nLaut Berichten hinkt Europa im globalen KI-Wettlauf hinterher und muss sich auf Quantencomputing konzentrieren, um wettbewerbsfähig zu bleiben. Investitionen in transformative Quantentechnologie sind entscheidend, damit Europa seine technologische Führungsposition wiedererlangt und sich der sich ständig weiterentwickelnden globalen Innovationslandschaft stellt. [Weiterlesen](https://www.euronews.com/next/2023/11/23/europe-has-lost-the-ai-race-it-cant-ignore-the-quantum-computing-one)\n\n## Podcast zur Umarmung der digitalen Transformation\n\n\n\nDiese Woche ist Darren nach einer Woche Urlaub für Thanksgiving zurückgekehrt. Er interviewte Louis Parks, den Geschäftsführer von Veridify, wo er die Schwachstellen der kritischen Infrastruktur der Betriebstechnologie aufdeckte. Sehen Sie sich außerdem die neue Markenbildung und Logos von Embracing Digital Transformation an. [Weiterlesen](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW43-de","image":"./briefs/edw-43/de/thumbnail.png","lang":"de","summary":"Nachrichten zur digitalen Transformation für die Woche vom 26. November 2023. Von den transformationalen Verschiebungen in der Führung von künstlicher Intelligenz bis hin zum unerlässlichen Schwerpunkt auf Cybersicherheit und der Enthüllung fortschrittlicher Computertechnologien, fasst diese Zusammenstellung entscheidende Momente in der sich ständig weiterentwickelnden Technologielandschaft zusammen. Begleiten Sie uns, während wir uns in die Feinheiten aktueller Entwicklungen vertiefen, die profunden Auswirkungen, die sie auf Sektoren haben, aufzeigen und die Richtung der digitalen Zukunft leiten."},{"id":211,"type":"News Brief","title":"2023-12-2","tags":["ai","compute","datamanagement","aws","siemens","industrialdge","cloudconfiguration","multiaccessedgecomputing","mec","networkarchitecture","iot","realtimeapplications","edgecomputinginhealthcare","healthcaremarket","polarismarketresearch","usda","datastrategy","digitalgovernance","vastdata","aiworkflows"],"body":"\n\n## Edge Computing\n\n\nAmazon Web Services (AWS) und Siemens haben sich zusammengeschlossen, um die industrielle Edge-to-Cloud-Konfiguration zu rationalisieren. Die Zusammenarbeit zielt darauf ab, die Integration von Edge-Geräten mit Cloud-Diensten zu vereinfachen und so die Effizienz in industriellen Umgebungen zu erhöhen. Die Partnerschaft vereint AWS IoT Greengrass und Siemens Industrial Edge Plattform und bietet eine nahtlose Lösung für die Konfiguration, Verwaltung und Optimierung von Edge-Geräten in der Cloud. Diese Zusammenarbeit soll eine reibungslosere und zugänglichere industrielle Edge-to-Cloud-Verbindung ermöglichen. [Weiterlesen](https://www.edgecomputing-news.com/2023/12/01/aws-and-siemens-team-up-for-easier-industrial-edge-to-cloud-configuration/)\n\n\n\nMulti-Access Edge Computing (MEC) ist eine Netzwerkarchitektur, die Rechenleistung näher an den Rand des Netzwerks bringt, die Latenz verringert und die Effizienz verbessert. Durch die Bereitstellung von Rechenressourcen am Rand ermöglicht MEC eine schnellere Verarbeitung von Daten, die von Geräten wie IoT-Sensoren und Mobilgeräten erzeugt werden. Dieser dezentrale Ansatz verbessert Echtzeitanwendungen und Dienstleistungen wie erweiterte Realität und autonome Fahrzeuge. MEC nutzt Edge-Server in unmittelbarer Nähe zu Endbenutzern und optimiert so die Datenverarbeitung für eine geringe Latenz und Hochleistungsrechnen. [Weiterlesen](https://www.techopedia.com/what-is-multi-access-edge-computing)\n\n\nLaut Polaris Market Research soll der Markt für Edge Computing im Gesundheitswesen bis zum Jahr zweitausend zweiunddreißig ein Volumen von über vierundvierzig Milliarden zweiundzwanzig Millionen US-Dollar überschreiten, was einem robusten Wachstum der durchschnittlichen jährlichen Wachstumsrate (CAGR) von sechsundzwanzig Komma drei Prozent entspricht. Die zunehmende Einführung von Edge-Computing-Lösungen im Gesundheitswesen, angetrieben durch Innovationen in den Bereichen IoT und KI-Technologien, soll die Effizienz der Datenverarbeitung verbessern, Echtzeitanwendungen ermöglichen und die Patientenbetreuung verbessern. Der Bericht deutet auf eine bedeutende Markterweiterung hin, da das Gesundheitswesen Edge Computing für eine verbesserte Leistung und dezentrale Datenverarbeitung adaptiert. [Weiterlesen](https://www.prnewswire.co.uk/news-releases/edge-computing-in-healthcare-market-expected-to-cross-usd-43-29-billion-by-2032--driving-26-3-cagr-growth-polaris-market-research-301998143.html)\n\n## Datenverwaltung\n\n\n\nDas US-Landwirtschaftsministerium hat eine neue Datenstrategie zur Verbesserung der digitalen Verwaltung vorgestellt. Die Initiative nutzt Daten zur Verbesserung von Dienstleistungen, Entscheidungsfindung und Rechenschaftspflicht. Der Plan betont datengesteuerte Innovation, Zusammenarbeit und Modernisierung der Dateninfrastruktur. Schlüsselkomponenten beinhalten die Priorisierung von Privatsphäre und Sicherheit, die Förderung der Datenzugänglichkeit und die Schaffung einer Kultur des verantwortungsbewussten Umgangs mit Daten. Der umfassende Ansatz des US-Landwirtschaftsministeriums zielt darauf ab, den Wert der Daten zur Unterstützung der Mission der Behörde und zur effektiveren Bedienung der Öffentlichkeit zu maximieren. [Weiterlesen](https://www.nextgov.com/digital-government/2023/11/usda-unveils-new-data-strategy/392382/)\n\n\nVAST Data hat ein Plattform-Update veröffentlicht, das darauf ausgelegt ist, AI-Workflows und hybride Cloud-Operationen auf AWS zu vereinfachen. Das Update konzentriert sich auf die Steigerung der Effizienz von datenintensiven Prozessen und bietet eine nahtlose Integration mit AWS-Diensten. Die Plattform von VAST Data zielt darauf ab, das Datenmanagement zu straffen und AI-Workloads zu beschleunigen, und bietet eine benutzerfreundlichere Erfahrung für Organisationen, die AWS für ihre hybriden Cloud-Operationen nutzen. Das Update unterstreicht das Engagement von VAST Data, die Leistung zu optimieren und komplexe AI- und Hybrid-Cloud-Workflows zu vereinfachen. [Weiterlesen](https://www.datanami.com/this-just-in/vast-datas-new-platform-update-aims-to-simplify-ai-workflows-and-hybrid-cloud-operations-on-aws/)\n\n\nIntel hat sich mit Granulate zusammengetan, um Datenverwaltungsoperationen auf Databricks zu optimieren, einer Plattform, die für Big-Data-Analysen genutzt wird. Die Echtzeit-Kontinuierliche Optimierungstechnologie von Granulate, angetrieben von Intel-Hardware, soll die Leistung und die Ressourcennutzung für Databricks-Benutzer verbessern. Diese Zusammenarbeit spiegelt die Bemühungen der Branche wider, die Fähigkeiten zur Datenanalyse zu verbessern und Organisationen zu ermöglichen, die volle Kraft von Big Data für eine gesteigerte Effizienz und reduzierte Kosten voll auszunutzen. [Weiterlesen](https://www.valdostadailytimes.com/ap/business/intel-granulate-optimizes-databricks-data-management-operations/article_ef0e550d-dbd3-5e5d-9a1d-75ffc7d0be4d.html)\n\n## Künstliche Intelligenz\n\n\nKalifornier drängen ihre Gesetzgeber dazu, Maßnahmen zu ergreifen, um Wahlen zu schützen und die Integrität der Wahlen zu gewährleisten. Die wachsenden Bedenken hinsichtlich der Wahlsicherheit veranlassen die Bürger dazu, legislative Maßnahmen zum Schutz des demokratischen Prozesses zu fordern. Diese Forderung spiegelt das wachsende Bewusstsein für den Schutz von Wahlen vor potenziellen Bedrohungen wider. Sie unterstreicht den Aufruf an die Gesetzgeber, Richtlinien zu erlassen, die die Transparenz, Cybersicherheit und das allgemeine Vertrauen in das Wahlsystem verbessern. [Weiterlesen](https://news.yahoo.com/californians-want-lawmakers-safeguard-elections-150026166.html)\n\n\nDie NATO hat eine umfassende Strategie für künstliche Intelligenz (KI) angekündigt, die ihre entscheidende Rolle in der modernen Kriegsführung und Sicherheit anerkennt. Die Strategie nutzt KI, um die Entscheidungsfindung, Cybersicherheit und betriebliche Effizienz zu verbessern. Die Entscheidung der NATO spiegelt die zunehmende Bedeutung der KI in militärischen Bereichen wider und die Notwendigkeit, an der Spitze technologischer Fortschritte zu bleiben. Die Strategie unterstreicht ethische Richtlinien und betont die Bedeutung der Zusammenarbeit mit Verbündeten und Industriepartnern. [Weiterlesen](https://news.yahoo.com/nato-artificial-intelligence-strategy-amid-143228193.html)\n\n\nIntel hat kürzlich \"Vision AI Everywhere\" angekündigt, eine neue Technologie, die KI in Rechenzentren und Geräte integriert. Diese Integration zielt darauf ab, die Leistung und Effizienz zu verbessern, Arbeitslasten zu optimieren, die Kapazitäten von Rechenzentren zu steigern und Echtzeit-Entscheidungsfindung zu ermöglichen. Während der HPE Discover-Veranstaltung präsentierte Intel verschiedene Anwendungen wie energieeffiziente Rechenzentren und fortschrittliche medizinische Bildgebung. \"Vision AI Everywhere\" steht im Einklang mit Intels Engagement, die Einführung von KI in verschiedenen Branchen zu fördern und eine neue Ära des intelligenten Computing einzuläuten. [Weiterlesen](https://siliconangle.com/2023/11/29/data-centers-devices-intels-vision-ai-everywhere-hpediscover-hpediscover/)\n\n## Umarmung der digitalen Transformation\n\n\n\nEmbracingdigital.org hat kürzlich ein Make-over erhalten und präsentiert sich mit neuem Aussehen und Logo. Schauen Sie sich das aktualisierte Branding der Show an, zusammen mit einem neuen Shop, der die neuesten Geschenke für Enthusiasten der digitalen Transformation anbietet. In der Show dieser Woche gibt es außerdem ein zweiteiliges Interview mit Shamim Naqvi, dem CEO von SafeLiShare, in dem sie sich intensiv mit dem Thema Zero Trust und Datenaustausch befassen. [Weiterlesen](http://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW44-de","image":"./briefs/edw-44/de/thumbnail.png","lang":"de","summary":"Bitte sehen Sie sich die neuesten Nachrichten aus der Welt der digitalen Transformation für die Woche ab dem 3. Dezember 2023 an. Sie finden eine Vielzahl interessanter Geschichten zum Thema Edge Computing, Datenmanagement und Künstliche Intelligenz. In dieser Woche haben AWS und Siemens zusammengearbeitet, um Edge Computing zu vereinfachen, während Intel dabei hilft, das cloudbasierte Datenmanagement zu verbessern. Darüber hinaus entwickeln mehrere Regierungen neue Strategien für KI."},{"id":212,"type":"News Brief","title":"2023-12-9","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","usarmy","uno","cybersec","iot","aijobs","gemini","nano","pro","ultra","meta","ibm","openai","altman","aws","google","microsoft","mcdonalds","vergeio","nutanix","embracingdt"],"body":"\n\n## Cybersicherheit\n\n\nDie US-Armee plant, ihre Konnektivität und Cybersecurity-Fähigkeiten zu verbessern, indem sie ein neues einheitliches Netzwerk namens UNO einführt. Diese Initiative zielt darauf ab, dem Militär eine schnelle und sichere Kommunikation zu bieten, die helfen kann, Herausforderungen auf dem digitalen Schlachtfeld zu bewältigen. Das kooperative Netzwerk soll den Betrieb vereinfachen und die Widerstandsfähigkeit gegen Cyber-Bedrohungen erhöhen, wodurch die technologische Infrastruktur der Armee gestärkt wird. [Weiterlesen](https://www.c4isrnet.com/battlefield-tech/it-networks/2023/12/04/unified-network-promises-us-army-rapid-connectivity-cybersecurity/)\n\n\nDa das Internet der Dinge (IoT) und künstliche Intelligenz (KI) immer weiter fortgeschritten sind, ist das Risiko für die kritische Infrastruktur durch Cyberbedrohungen erheblich gestiegen. Cybersicherheitsexperten betonen die Notwendigkeit besserer Sicherheitsmaßnahmen zum Schutz vor diesen Bedrohungen. Sie setzen auf die Integration von KI und maschinellem Lernen, um die Erkennung und Verhinderung neuer Cyber-Angriffe zu verbessern. Diese ständig wandelnde Landschaft der Cybersicherheit im Zeitalter des IoT und des Cloud-Computings unterstreicht die anhaltende Notwendigkeit, digitale Ökosysteme anzupassen und zu sichern. [Weiterlesen](https://readwrite.com/the-evolution-of-cybersecurity-in-the-age-of-iot-and-cloud-computing/)\n\n\nEtwa fünfzig Prozent der befragten Organisationen planen, ihre Mitarbeiterzahl im Bereich Cybersicherheit zu reduzieren, was angesichts der sich ständig weiterentwickelnden Cyber-Bedrohungen besorgniserregend ist. Dieser Trend unterstreicht die Bedeutung strategischer Personalplanung zur Aufrechterhaltung solider digitaler Abwehrmechanismen. Unterdessen benötigen einige andere Unternehmen Hilfe bei der Suche nach geeigneten Kandidaten, um ihre offenen Stellen im Bereich Cybersicherheit zu besetzen. [Weiterlesen](https://www.csoonline.com/article/1251369/almost-50-organizations-plan-to-reduce-cybersecurity-headcounts-survey.html)\n\n## Künstliche Intelligenz\n\n\nGoogle hat eine neue AI-Plattform namens Gemini eingeführt, die darauf abzielt, das Training und die Bereitstellung von maschinellen Lernmodellen zu vereinfachen. Dies wird Entwicklern ermöglichen, skalierbarere und effizientere AI-Anwendungen zu erstellen. Gemini bietet benutzerfreundliche Tools und Ressourcen zur Förderung von Innovation und Fortschritt in der AI-Technologie. Sie richtet sich derzeit an drei Angebote: Nano für Telefone, Pro für lokale Datenzentren und Ultra für eine Cloud-basierte öffentliche Cloud, die von Bard unterstützt wird. [Weiterlesen](https://apnews.com/article/google-gemini-artificial-intelligence-launch-95d05d02051e75e20b574614ae720b8b)\n\n\n\nMeta und IBM arbeiten zusammen, um einen offenen Standard für künstliche Intelligenz zu entwickeln, mit dem Ziel, die Interoperabilität zu fördern und dem Einfluss großer Technologieunternehmen entgegenzuwirken. Indem sie einen kooperativeren Ansatz fördern, hoffen sie, eine wettbewerbsfähige Landschaft zu schaffen, die Innovation und Vielfalt in der AI-Technologie ermutigt. [Weiterlesen](https://www.thestreet.com/technology/meta-and-ibm-team-up-against-dominant-big-tech-players)\n\n\nDer Geschäftsführer von OpenAI, Sam Altman, betonte die Wichtigkeit einer verantwortungsvollen KI-Entwicklung, nachdem ein Mitarbeiter aufgrund von Bedenken bezüglich des Missbrauchs von KI-Technologie entlassen wurde. Der Vorfall unterstreicht die fortlaufenden Herausforderungen in der Technologiebranche hinsichtlich Ethik und Governance in der künstlichen Intelligenz. [Weiterlesen](https://www.foxbusiness.com/technology/openais-sam-altman-opens-up-shock-firing)\n\n## Allgegenwärtiges Computing\n\n\nAWS und Google kritisieren die restriktiven Cloud-Praktiken von Microsoft und werfen ihnen vor, die Wahlmöglichkeiten der Kunden und die Interoperabilität einzuschränken, was den Wettbewerb auf dem Markt erstickt. Dies unterstreicht die anhaltenden Spannungen zwischen Cloud-Anbietern und Bedenken hinsichtlich fairer Geschäftspraktiken und Marktdynamiken. [Weiterlesen](https://www.itpro.com/cloud/cloud-computing/aws-joins-google-in-calling-out-restrictive-microsoft-cloud-practices)\n\n\nMcDonald's und Google Cloud schließen sich zusammen, um Cloud-Technologie und generative KI-Lösungen in McDonald's Restaurants weltweit einzusetzen. Das Ziel ist es, das Kundenerlebnis zu verbessern, den Betrieb zu optimieren und Innovationen in der Fast-Food-Branche voranzutreiben. Durch die Integration der fortschrittlichen Technologien von Google Cloud strebt McDonald's an, an der Spitze der digitalen Transformation zu bleiben und verbesserte Dienstleistungen und betriebliche Effizienz in seinem weltweiten Netzwerk von Restaurants zu bieten. [Weiterlesen](https://www.prnewswire.com/news-releases/mcdonalds-and-google-cloud-announce-strategic-partnership-to-connect-latest-cloud-technology-and-apply-generative-ai-solutions-across-its-restaurants-worldwide-302006915.html)\n\n\n\nEs wurde berichtet, dass die Übernahme von VMware durch Broadcom zu Veränderungen geführt hat, die dazu geführt haben, dass zwanzig Prozent seiner Nutzer nach anderen Virtualisierungslösungen suchen. Dieser Gelegenheitstrend hat für andere Akteure auf dem Markt die Möglichkeit geschaffen, sich als lebensfähige Alternativen zu etablieren. Trotz der Dominanz von VMware auf dem Virtualisierungsmarkt führt der Wettbewerb von Unternehmen wie Nunatix und VergeIO dazu, dass Kunden sich von der etablierten Plattform abwenden. [Weiterlesen](https://www.channelfutures.com/mergers-acquisitions/20-of-users-looking-to-escape-to-vmware-alternatives)\n\n## Podcast zur Umarmung der Digitalen Transformation\n\n\n\nIn dieser Folge seiner Serie zum Thema \"Umarmung des Zero Trust\" interviewt Darren den CEO von SafeLiShare, Shamim Naqvi, um über Zero Trust Sicherheit und Datenzusammenarbeit mit Hilfe von vertraulichem Computing zu diskutieren. Zusätzlich möchte Darren seinen Zuhörern für ihre Unterstützung danken. Dank Ihnen hat der Podcast über dreitausend Abonnenten und viertausend wöchentliche Zuhörer. [Weiterlesen](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW45-de","image":"./briefs/edw-45/de/thumbnail.png","lang":"de","summary":"Nachrichten zur digitalen Transformation für die Woche vom 10. Dezember 2023 enthalten Geschichten über Cybersicherheit, künstliche Intelligenz und ubiquitäres Rechnen. Hören Sie diese Woche auf den Plan der US-Armee, die Cybersicherheit durch Vereinheitlichung zu verbessern, die Cloud-Kriege gehen in Großbritannien vor Gericht und das Drama von OpenAI und Sam Altman."},{"id":213,"type":"News Brief","title":"2023-12-16","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","innovation","intel","pc","aiworkforce","snowflake","aws","solix","aiintegration","britain","nationalgrid","chineserisks","hackers","infrastructure","vulnerabilities","nsa","russianthreats","digitaltransformation","openziti","zerotrust"],"body":"\n\n## Künstliche Intelligenz\n\n\nDie neueste Ankündigung von Intel markiert einen entscheidenden Moment in der Geburt von KI-betriebenen PCs. Die Einführung der Prozessoren der Core Ultra H- und U-Serie führt die revolutionäre Meteor-Lake-Architektur ein, die fortschrittliche KI-Funktionen in ultra-dünne Notebooks integriert. Diese Veröffentlichung verbessert die Leistung und hebt die Arc-Grafikplattform an, wodurch das Benutzererlebnis auf neue Höhen gehoben wird. Intels Engagement für Innovation signalisiert eine transformative Ära auf dem Laptop-Markt, in der die KI zu einem integralen Bestandteil des alltäglichen Computings wird. [Weiterlesen](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\nDie gewählte Ratsfrau von New York City, Susan Zhuang, hat zugegeben, stillschweigend künstliche Intelligenz einzusetzen, um mit der Öffentlichkeit zu kommunizieren und auf Medienanfragen zu reagieren. Endlich eine Politikerin, die die Wahrheit spricht! Zhuangs Einsatz von künstlicher Intelligenz markiert einen einzigartigen Ansatz zur Einbeziehung der Wähler, und die Auswirkungen dieser technologiegetriebenen Strategie könnten die Landschaft der politischen Kommunikation im digitalen Zeitalter neu gestalten. [Weiterlesen](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\nDie Schaffung des personalisierten KI-PCs durch Intel hat ein potenziell riesiges Arbeitsmarktrisiko für Unternehmen mit sich gebracht, da sie weiterhin künstliche Intelligenz (KI) in ihre Abläufe integrieren. Da KI-Technologien übernommen werden, wird erwartet, dass sich Arbeitsfunktionen und Arbeitstage ändern, was Bedenken hinsichtlich möglicher Belegschaftsprobleme aufwirft. Unternehmen bereiten sich auf diesen transformativen Wandel vor und erkennen die Notwendigkeit proaktiver Strategien, um mögliche Störungen zu bewältigen und einen reibungslosen Übergang zu einer von KI gesteuerten Zukunft zu gewährleisten. [Weiterlesen](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Datenmanagement\n\n\nSchneeflocke, eine führende Cloud-Datenplattform, hat die FedRAMP-Hochautorisierung auf AWS GovCloud US West und US Ost erreicht. Dieser bedeutende Meilenstein unterstreicht das Engagement von Schneeflocke, strenge Regierungssicherheitsstandards zu erfüllen, und ermöglicht es Bundesbehörden, ihre fortschrittlichen Datenverwaltungsfähigkeiten in einer sicheren Cloud-Umgebung zu nutzen. Die FedRAMP-Hochautorisierung unterstreicht die Position von Schneeflocke als vertrauenswürdige Partnerin bei der Bereitstellung schneller und skalierbarer Lösungen für Regierungseinrichtungen und fördert Innovationen und Effizienz in der Datenverwaltung. [Weiterlesen](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\nSolix hat die Common Data Platform (CDP) drei Punkt null eingeführt, die erweiterte Funktionen für Multi-Cloud-Datenmanagement und Unternehmens-KI enthält. Diese Veröffentlichung zeigt Solixs Engagement für die Bereitstellung von hochmodernen Lösungen zur Verwaltung und Erzielung von Erkenntnissen aus großen Datensätzen. Die verbesserten Funktionen für maschinelles Lernen und KI der CDP drei Punkt null ermöglichen es Organisationen, fundiertere datengetriebene Entscheidungen zu treffen. Der Trend zum Multi-Cloud-Datenmanagement treibt die Entwicklung vieler neuer kommerzieller Angebote voran und die neueste Veröffentlichung von Solix steht bereit, ein bedeutender Beitrag in diesem Bereich zu leisten. [Weiterlesen](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\nIn einem aktuellen Bericht untersucht Datanami wichtige Überlegungen, um sicherzustellen, dass Ihre Datenmanagementstrategie für künstliche Intelligenz bereit ist, mit fünf Schlüsselindikatoren. Priorisieren Sie die Datenqualität, Skalierbarkeit, Integrationsfähigkeiten, robuste Sicherheit und Flexibilität. Diese Elemente sind entscheidend für die Optimierung von Systemen, da künstliche Intelligenz weiterhin Datenprozesse prägt. Bleiben Sie in der Ära der künstlichen Intelligenz vorn, indem Sie diese Aspekte bewerten und verbessern, um den Anforderungen sich weiterentwickelnder Technologien gerecht zu werden. Lesen Sie mehr auf datanami.com für umfassende Einblicke. [Weiterlesen](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Cybersicherheit\n\n\n\nDer National Grid von Großbritannien hat laut Berichten seine Verbindungen zu einem in China ansässigen Lieferanten aufgrund von Bedenken hinsichtlich der Cybersicherheit gekappt, wie Reuters am siebzehnten Dezember zweitausenddreiundzwanzig berichtete. Die Entscheidung spiegelt die verstärkte globale Überprüfung potenzieller Risiken wider, die mit ausländischen Technologiepartnerschaften verbunden sind, insbesondere in Sektoren der kritischen Infrastruktur. Dieser Schritt des National Grid unterstreicht die wachsende Betonung der Stärkung von Maßnahmen zur Cybersicherheit, um lebenswichtige Dienste vor potenziellen Bedrohungen zu schützen. [Weiterlesen](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\nHacker haben sich auf die Wasserversorgung der USA abgezielt, was Alarme hinsichtlich der Anfälligkeit von kritischen Infrastrukturen auslöst. Axios berichtete am sechzehnten Dezember zweitausenddreiundzwanzig, dass dieser Verstoß bedeutende Bedenken hinsichtlich der nationalen Sicherheit und der öffentlichen Sicherheit aufwirft. Der Vorfall unterstreicht die dringende Notwendigkeit robuster Cybersicherheitsmaßnahmen zum Schutz essentieller Dienstleistungen. Die Behörden ermitteln wegen des Verstoßes und betonen damit die anhaltenden Herausforderungen bei der Verteidigung kritischer Infrastrukturen vor Cyberbedrohungen. [Weiterlesen](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\nDie NSA hat vor russischen Cyber-Akteuren gewarnt, die eine bekannte Schwachstelle mit weltweiten Auswirkungen ausnutzen. Die identifizierte Schwachstelle hat einen bedeutenden Einfluss weltweit und betont die Wichtigkeit, Cybersicherheitsrisiken zu mindern. Die NSA rät Organisationen, diese Schwachstelle umgehend zu beheben und sie zu patchen, um ihre Widerstandsfähigkeit gegen mögliche Cyber-Bedrohungen zu stärken. [Weiterlesen](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Digitale Transformation umarmen\n\n\n\nDarren schaut sich Open Source für eine Netzwerkarchitektur mit null Vertrauen mit OpenZiti an, zusammen mit einem der Organisatoren der Open-Source-Community, Philip Griffiths. Ein besonderer Dank geht auch an die Hörer von Embracing Digital Transformation, die die Neuigkeiten verbreitet haben. Wir konnten mehr als viertausend Abonnenten zu unserem YouTube-Kanal hinzufügen und hatten letzte Woche über vierzehntausend Downloads oder Aufrufe unseres Podcasts. Vielen Dank nochmal für Ihre Unterstützung. [Weiterlesen](http://www.embracingdigital.org)\n\n## Künstliche Intelligenz\n\n\n\nIntels neueste Ankündigung markiert einen entscheidenden Moment in der Geburt von KI-gesteuerten PCs. Die Einführung von Core Ultra H und U Series Prozessoren stellt die revolutionäre Meteor Lake-Architektur vor, die fortschrittliche KI-Fähigkeiten in extrem dünne Notebooks einbettet. Diese Veröffentlichung verbessert die Leistung und erhöht die Arc-Grafikplattform und hebt das Benutzererlebnis auf neue Höhen. Intels Engagement für Innovation signalisiert eine transformative Ära auf dem Laptop-Markt, in der die KI zu einem integralen Bestandteil des Alltagscomputings wird. [Weiterlesen](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nDie neu gewählte Stadträtin von New York, Susan Zhuang, hat zugegeben, im Stillen Künstliche Intelligenz einzusetzen, um mit der Öffentlichkeit zu kommunizieren und auf Medienanfragen zu reagieren. Endlich spricht ein Politiker die Wahrheit! Zhuangs Nutzung von Künstlicher Intelligenz stellt einen einzigartigen Ansatz zur Interaktion mit Wählern dar, und die Auswirkungen dieser technologiegetriebenen Strategie könnten die Landschaft der politischen Kommunikation im digitalen Zeitalter neu gestalten. [Weiterlesen](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nDie Erschaffung des personalisierten KI-PCs durch Intel führt zu potenziell massiven Arbeitsstörungen in Unternehmen, da sie weiterhin künstliche Intelligenz (KI) in ihre Abläufe integrieren. Es wird erwartet, dass sich Arbeitsfunktionen und Arbeitstage ändern, wenn KI-Technologien angenommen werden, was Bedenken hinsichtlich möglicher Herausforderungen für die Belegschaft aufwirft. Unternehmen bereiten sich auf diesen transformativen Wandel vor und erkennen den Bedarf an proaktiven Strategien, um mögliche Störungen zu navigieren und einen reibungslosen Übergang zu einer von KI getriebenen Zukunft zu gewährleisten. [Weiterlesen](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Datenmanagement\n\n\n\nSchneeflocke, eine führende Cloud-Datenplattform, hat die FedRAMP High Autorisierung auf AWS GovCloud US West und US East erreicht. Dieser bedeutende Meilenstein unterstreicht das Engagement von Schneeflocke, strenge Regierungssicherheitsstandards zu erfüllen, und ermöglicht es Bundesbehörden, ihre fortschrittlichen Datenmanagementfähigkeiten in einer sicheren Cloud-Umgebung zu nutzen. Die FedRAMP High Autorisierung unterstreicht die Position von Schneeflocke als vertrauenswürdiger Partner bei der Bereitstellung schneller und skalierbarer Lösungen für Regierungseinrichtungen und fördert Innovation und Effizienz im Datenmanagement. [Weiterlesen](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix hat die gemeinsame Datenplattform (CDP) Drei Punkt Null eingeführt, die erweiterte Datenmanagement- und Unternehmens-KI-Funktionen für mehrere Clouds umfasst. Diese Veröffentlichung spiegelt das Engagement von Solix wider, modernste Lösungen für das Management und das Ableiten von Erkenntnissen aus großen Datensätzen bereitzustellen. Die verbesserten Funktionen für maschinelles Lernen und KI der CDP Drei Punkt Null ermöglichen Organisationen, fundiertere datengesteuerte Entscheidungen zu treffen. Der Trend des Datenmanagements über mehrere Clouds treibt die Entwicklung vieler neuer kommerzieller Angebote voran und die neueste Veröffentlichung von Solix positioniert sich, um einen bedeutenden Beitrag in diesem Bereich zu leisten. [Weiterlesen](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n## Künstliche Intelligenz\n\n\n\nIntels neueste Ankündigung markiert einen entscheidenden Moment in der Geburt von KI-gesteuerten PCs. Die Einführung der Prozessoren der Core Ultra H und U Serien stellt die revolutionäre Meteor Lake Architektur vor, die fortschrittliche KI-Fähigkeiten in ultra-dünne Notebooks integriert. Diese Veröffentlichung steigert die Leistung und hebt die Arc-Grafikplattform auf ein neues Niveau, und hebt das Benutzererlebnis auf neue Höhen. Intels Engagement für Innovation signalisiert eine transformative Ära auf dem Laptop-Markt, in der KI zu einem integralen Bestandteil des alltäglichen Computings wird. [Weiterlesen](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nDie frisch gewählte Stadträtin von New York, Susan Zhuang, hat zugegeben, stillschweigend Künstliche Intelligenz einzusetzen, um mit der Öffentlichkeit zu kommunizieren und auf Medienanfragen zu reagieren. Endlich eine Politikerin, die die Wahrheit spricht! Zhuangs Einsatz von Künstlicher Intelligenz stellt einen einzigartigen Ansatz zur Interaktion mit ihren Wählern dar, und die Auswirkungen dieser technologiegetriebenen Strategie könnten das Gesicht der politischen Kommunikation im digitalen Zeitalter neu formen. [Weiterlesen](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nDie Schaffung des personalisierten KI-PCs durch Intel hat mögliche massive Arbeitsstörungen für Unternehmen mit sich gebracht, da sie weiterhin künstliche Intelligenz (KI) in ihre Abläufe integrieren. Mit der Einführung von KI-Technologien wird erwartet, dass sich Jobfunktionen und Arbeitstage verändern, was Bedenken hinsichtlich möglicher Herausforderungen für die Belegschaft aufwirft. Unternehmen bereiten sich auf diesen transformativen Wandel vor und erkennen die Notwendigkeit proaktiver Strategien, um mögliche Störungen zu bewältigen und einen reibungslosen Übergang zu einer KI-getriebenen Zukunft zu gewährleisten. [Weiterlesen](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Datenmanagement\n\n\n\nSnowflake, eine führende Cloud-Datenplattform, hat die FedRAMP-High-Autorisierung auf AWS GovCloud US West und US Ost erreicht. Dieser bedeutende Meilenstein unterstreicht das Engagement von Snowflake, strenge Regierungssicherheitsstandards zu erfüllen, und ermöglicht es Bundesbehörden, ihre fortschrittlichen Datenverwaltungsfähigkeiten in einer sicheren Cloud-Umgebung zu nutzen. Die FedRAMP-High-Autorisierung unterstreicht die Position von Snowflake als vertrauenswürdiger Partner bei der Bereitstellung schneller und skalierbarer Lösungen für Regierungseinrichtungen, was Innovation und Effizienz in der Datenverwaltung fördert. [Weiterlesen](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix hat die Common Data Platform (CDP) drei Punkt null eingeführt, die fortgeschrittene Multi-Cloud-Datenverwaltung und Unternehmens-KI-Funktionen umfasst. Diese Veröffentlichung spiegelt das Engagement von Solix wider, hochmoderne Lösungen für die Verwaltung und Ableitung von Erkenntnissen aus großen Datensätzen bereitzustellen. Die verbesserten Maschinenlern- und KI-Funktionen von CDP drei Punkt null ermöglichen es Organisationen, fundiertere datengesteuerte Entscheidungen zu treffen. Der Trend zur Multi-Cloud-Datenverwaltung treibt die Entwicklung vieler neuer kommerzieller Angebote voran und die neueste Veröffentlichung von Solix steht in diesem Bereich vor einem erheblichen Beitrag. [Weiterlesen](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\n\nIn einem aktuellen Bericht untersucht Datanami entscheidende Überlegungen, um sicherzustellen, dass Ihre Datenmanagementstrategie auf KI mit fünf Schlüsselindikatoren vorbereitet ist. Priorisieren Sie Datenqualität, Skalierbarkeit, Integrationsfähigkeiten, robuste Sicherheit und Flexibilität. Diese Elemente sind entscheidend für die Optimierung von Systemen, da künstliche Intelligenz weiterhin Datenprozesse prägt. Bleiben Sie in der Ära der KI vorn, indem Sie diese Aspekte bewerten und verbessern, um den Anforderungen sich entwickelnder Technologien gerecht zu werden. Lesen Sie mehr auf datanami.com für umfassende Einblicke. [Weiterlesen](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Cybersicherheit\n\n\n\nBerichten zufolge hat das britische National Grid am siebzehnten Dezember zweitausenddreiundzwanzig die Beziehungen zu einem in China ansässigen Lieferanten aufgrund von Bedenken hinsichtlich der Cybersicherheit abgebrochen, wie Reuters meldete. Die Entscheidung spiegelt die verschärfte globale Prüfung möglicher Risiken im Zusammenhang mit ausländischen Technologiepartnerschaften wider, insbesondere in kritischen Infrastruktursektoren. Dieser Schritt des National Grid unterstreicht das zunehmende Augenmerk auf die Stärkung der Maßnahmen zur Cybersicherheit zum Schutz lebenswichtiger Dienstleistungen vor möglichen Bedrohungen. [Weiterlesen](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\n\nHacker haben die Wasserversorgung der USA ins Visier genommen, was Alarm wegen der Verwundbarkeit kritischer Infrastrukturen auslöst. Axios berichtete am sechzehnten Dezember zweitausenddreiundzwanzig, dass dieser Verstoß erhebliche Bedenken hinsichtlich der nationalen Sicherheit und öffentlichen Sicherheit aufwirft. Der Vorfall unterstreicht die dringende Notwendigkeit robuster Cybersicherheitsmaßnahmen zum Schutz wesentlicher Dienstleistungen. Die Behörden untersuchen den Verstoß, was die anhaltenden Herausforderungen bei der Verteidigung kritischer Infrastrukturen gegen Cyberbedrohungen hervorhebt. [Weiterlesen](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\n\nDie NSA hat vor russischen Cyber-Akteuren gewarnt, die eine bekannte Schwachstelle mit weltweiten Auswirkungen ausnutzen. Die identifizierte Schwachstelle hat einen erheblichen Einfluss weltweit und betont die Bedeutung, Cybersecurity-Risiken zu mildern. Die NSA rät Organisationen, diese Schwachstelle umgehend zu beheben und zu patchen, um ihre Widerstandsfähigkeit gegen potenzielle Cyber-Bedrohungen zu stärken. [Weiterlesen](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Umarmung der digitalen Transformation\n\n\n\nDarren setzt bei der Nullvertrauens-Netzwerkarchitektur auf Open Source mit OpenZiti zusammen mit einem der Organisatoren der Open-Source-Community, Philip Griffiths. Ein besonderer Dank gilt auch den Zuhörern von Embracing Digital Transformation dafür, dass sie die Nachrichten verbreitet haben, wir haben mehr als viertausend Abonnenten für unseren YouTube-Kanal hinzugefügt und in der letzten Woche hatten wir mehr als vierzehntausend Downloads oder Aufrufe unseres Podcasts. Vielen Dank nochmals für Ihre Unterstützung. [Weiterlesen](http://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW46-de","image":"./briefs/edw-46/de/thumbnail.png","lang":"de","summary":"Die Woche vom 17. Dezember 2023 beinhaltet in den Nachrichten zur digitalen Transformation Geschichten über KI, Datenmanagement und Cybersicherheit. Schauen Sie sich die Geburt des KI-PCs, Multi-Cloud-Datenmanagement für Regierungsclouds und wichtige Warnungen für die Cybersicherheit kritischer Infrastrukturen an."},{"id":214,"type":"News Brief","title":"2023-12-23","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","wordpress","plugindata","xwpspamshield","ontariohack","healthcarecyber","holidayhacks","ontarioincidents","artificialintel","generativeai","aiinschool","intelnervana","cloudcomputing","cloudcost","pentagoncloud","cisco","isovalent","openziti","podcast","digitaltransf"],"body":"\n\n## Cybersicherheit\n\n\n\nEin skrupelloses WordPress-Plugin hat Benutzerdaten durch Offenlegung von E-Mail-Adressen, Benutzernamen und Passwörtern im Klartext gefährdet. Sicherheitsforscher entdeckten den Verstoß und forderten betroffene Benutzer auf, ihre Anmeldedaten sofort zurückzusetzen. Das Plugin, das den Namen \"X-WP-SPAM-SHIELD-PRO\" trägt, ist für bösartige Aktivitäten bekannt, was die Bedeutung regelmäßiger Sicherheitsüberprüfungen für WordPress-Seiten unterstreicht. Website-Administratoren wird geraten, das kompromittierte Plugin zu entfernen und die Sicherheitsmaßnahmen zu erhöhen, um unbefugten Zugriff und Datenexposition zu verhindern. [Weiterlesen](https://thehackernews.com/2023/12/rogue-wordpress-plugin-exposes-e.html)\n\n\n\nDie CBC berichtet über einen Anstieg von Cyberangriffen, die sich im Jahr Zweitausenddreiundzwanzig gegen Ontario, Kanada, richten. Diese Vorfälle betreffen eine Reihe von Sektoren, einschließlich Gesundheitswesen, Gemeinden und Bildung. Hacker nutzen Schwachstellen aus, verursachen Störungen und legen sensible Daten offen. Experten betonen die Verbesserung von Cybersicherheitsmaßnahmen, Schulungen und Zusammenarbeit, um sich gegen sich entwickelnde Cyberbedrohungen zu schützen und kritische Infrastrukturen zu schützen. [Weiterlesen](https://www.cbc.ca/news/canada/toronto/cybersecurity-ontario-incidents-2023-1.7048495)\n\n\n\nCyber-Angriffe während der Weihnachtsferien nehmen aus mehreren Gründen zu. Erstens ist die Aktivität des Online-Shoppings verstärkt und bietet somit Möglichkeiten für Cyberkriminelle. Zweitens haben Phishing-Versuche zugenommen, bei denen Angreifer Opfer mit E-Mails oder sozialen Medien in die Irre führen. Drittens, stellen Mitarbeiter, die aus der Ferne arbeiten, ein Sicherheitsrisiko dar. Viertens können Social-Engineering-Angriffe in dieser Saison erfolgreicher sein. Schließlich kann mögliche Sicherheitsnachlässigkeit aufgrund von Feierlichkeiten das Risiko von Angriffen ebenfalls erhöhen. Es ist unerlässlich, sich dieser Risiken bewusst zu sein und Maßnahmen zu ihrer Minderung zu ergreifen. [Weiterlesen](https://bit-sentinel.com/5-reasons-why-cyber-attacks-increase-during-the-christmas-holidays/)\n\n## Künstliche Intelligenz\n\n\n\nIm Jahr zweitausenddreiundzwanzig machte generative KI bedeutende Fortschritte in verschiedenen Bereichen, einschließlich Gesundheitswesen, Finanzen und Unterhaltung. Jim Cramer von CNBC fasst das Jahresende zusammen, indem er die Beiträge der KI zur Arzneimittelentdeckung, Finanzmodellierung und Inhalteerstellung hervorhebt. Das transformative Potenzial von generativer KI wird hervorgehoben und markiert ihren anhaltenden Einfluss auf Innovation und technologischen Fortschritt. [Weiterlesen](https://www.cnbc.com/2023/12/21/jim-cramer-recaps-the-year-in-generative-artificial-intelligence.html)\n\n\n\nDer jüngste Artikel von Future-Ed untersucht das Potenzial der Integration von Künstlicher Intelligenz in Schulen. Er betont die Bedeutung ethischer Überlegungen und verantwortungsvoller Umsetzung für eine erfolgreiche Integration. Der Artikel bietet wertvolle Einblicke für Pädagogen, politische Entscheidungsträger und Interessensvertreter, die sich in dieser sich entwickelnden Landschaft orientieren. Der Podcast \"Embracing Digital Transformation\" wird auch Interviews mit Hochschulstudenten und ihren Professoren präsentieren, in denen sie ihre Gedanken diskutieren. [Weiterlesen](https://www.future-ed.org/navigating-the-artificial-intelligence-revolution-in-schools/)\n\n\n\nIntel hat einen neuen Chip, den Intel Nervana NCS-eins, vorgestellt, um künstliche Intelligenz (KI) Anwendungen zu beschleunigen. Der NCS-eins ist darauf ausgelegt, die KI-Leistung zu verbessern und bietet eine hohe Durchsatzleistung und Effizienz. Intel zielt darauf ab, die KI-Innovation voranzutreiben, indem Entwicklern Werkzeuge zur effizienten Bereitstellung und Skalierung von KI-Modellen bereitgestellt werden. Der Chip ist ein Teil von Intels Engagement, KI-Anwendungen in verschiedenen Branchen zu stärken und stellt einen bedeutenden Schritt in der KI-Strategie des Unternehmens dar. [Weiterlesen](https://www.intc.com/news-events/press-releases/detail/1663/intel-accelerates-ai-everywhere-with-launch-of-powerful)\n\n## Allgegenwärtiges Computing\n\n\n\nFinanzielle Einrichtungen, einschließlich Capital One und Arvest Bank, priorisieren die Kostenkontrolle in der Cloud angesichts von Herausforderungen. Der Fokus liegt auf der Optimierung der Ausgaben durch strategische Ausrichtung auf Geschäftsziele, den Einsatz von Werkzeugen zur Kostenverwaltung in der Cloud und die Implementierung von Governance-Praktiken. Mit der Weiterentwicklung des Cloud-Computings wird eine effektive Kostenverwaltung für diese Organisationen entscheidend. [Weiterlesen](https://www.ciodive.com/news/cloud-cost-control-capital-one-arvest-bank/703025/)\n\n\n\nDie Washington Post berichtet über die Überprüfung der Cloud-Computing-Verträge des Pentagons und deutet mögliche Strategieverschiebungen an. Der Artikel untersucht die Auswirkungen dieser Veränderungen und ihren Einfluss auf große Technologieunternehmen, die auf lukrative Verteidigungsaufträge bieten. Während das Pentagon die komplexe Landschaft des Cloud Computings durchquert, hebt der Artikel die sich verändernden Dynamiken und den Wettbewerb in der Suche des Verteidigungssektors nach fortschrittlichen technologischen Fähigkeiten hervor. [Weiterlesen](https://www.washingtonpost.com/technology/2023/12/21/pentagon-cloud-computing-contracts/)\n\n\n\nCisco macht einen strategischen Schachzug in der Cloud-Sicherheit, indem es Isovalent erwirbt, einen Innovator im Bereich Cloud-natives Networking und Sicherheit. Diese Übernahme spiegelt Ciscos Engagement wider, seine Fähigkeiten im Bereich künstliche Intelligenz (KI) und Cloud-Sicherheit weiterzuentwickeln. Es wird erwartet, dass Isovalents Expertise in eBPF-Technologie (erweitertem Berkeley Packet Filter) Ciscos Fähigkeit, Cloud-Umgebungen zu sichern, verbessern wird und fortschrittliche Lösungen für die sich entwickelnde Cybersicherheitslandschaft in Cloud-basierten Infrastrukturen bietet. [Weiterlesen](https://www.informationweek.com/it-infrastructure/cloud-computing#close-modal)\n\n## Digitale Transformation Umarmen\n\n\n\nZunächst möchte ich all meinen Zuhörern ein frohes Weihnachtsfest und ein glückliches neues Jahr wünschen. In dem Podcast dieser Woche erkundet Darren OpenZiti, eine Open-Source-Lösung für Netzwerke ohne Vertrauensstellung. Außerdem möchte ich mich bei jedem bedanken, der das Wort über den Podcast verbreitet: Wir haben nun über zehntausend wöchentliche Zuhörer übertroffen, und Ihre Unterstützung wird sehr geschätzt. [Weiterlesen](https://www.embracingdigital.org)\n\n## Cybersicherheit\n\n\n\nEin betrügerisches WordPress-Plugin hat Benutzerdaten gefährdet, indem es E-Mail-Adressen, Benutzernamen und Passwörter in Klartext preisgab. Sicherheitsforscher entdeckten den Bruch und drängten betroffene Benutzer dazu, ihre Anmeldeinformationen sofort zurückzusetzen. Das Plugin, das \"X-WP-SPAM-SHIELD-PRO\" genannt wird, ist bekannt für bösartige Aktivitäten und betont die Wichtigkeit regelmäßiger Sicherheitsüberprüfungen für WordPress-Websites. Website-Administratoren werden aufgefordert, das kompromittierte Plugin zu entfernen und Sicherheitsmaßnahmen zu verschärfen, um unbefugten Zugriff und Datenexposition zu verhindern. [Weiterlesen](https://thehackernews.com/2023/12/rogue-wordpress-plugin-exposes-e.html)\n\n\n\nDie CBC berichtet über einen Anstieg von Cyberangriffen, die sich im Jahr zweitausenddreiundzwanzig auf Ontario, Kanada, richten. Diese Vorfälle betreffen eine Reihe von Sektoren, einschließlich Gesundheitswesen, Gemeinden und Bildung. Hacker nutzen Schwachstellen aus, verursachen Störungen und legen sensible Daten offen. Experten betonen die Notwendigkeit, Cybersecurity-Maßnahmen zu verbessern, Schulungen durchzuführen und zusammenzuarbeiten, um sich gegen sich entwickelnde Cyberbedrohungen zu schützen und kritische Infrastrukturen zu schützen. [Weiterlesen](https://www.cbc.ca/news/canada/toronto/cybersecurity-ontario-incidents-2023-1.7048495)\n\n\n\nCyber-Angriffe während der Weihnachtsferien nehmen aus verschiedenen Gründen zu. Erstens erhöht sich die Online-Shopping-Aktivität, was Möglichkeiten für Cyberkriminelle schafft. Zweitens haben Phishing-Versuche zugenommen, bei denen Angreifer Opfer mit Hilfe von E-Mails oder sozialen Medien täuschen. Drittens stellen im Homeoffice arbeitende Mitarbeiter ein Sicherheitsrisiko dar. Viertens können Social-Engineering-Angriffe in dieser Jahreszeit erfolgreicher sein. Schließlich kann auch eine mögliche Sicherheitsnachlässigkeit aufgrund von Feierlichkeiten das Risiko von Angriffen erhöhen. Es ist von entscheidender Bedeutung, sich dieser Risiken bewusst zu sein und Maßnahmen zu deren Minderung zu ergreifen. [Weiterlesen](https://bit-sentinel.com/5-reasons-why-cyber-attacks-increase-during-the-christmas-holidays/)\n\n## Künstliche Intelligenz\n\n\n\nIm Jahr zweitausenddreiundzwanzig machte die generative KI bedeutende Fortschritte in verschiedenen Branchen, einschließlich Gesundheitswesen, Finanzen und Unterhaltung. CNBC's Jim Cramer fasst das Jahresende zusammen, indem er die Beiträge der KI zur Arzneimittelentdeckung, Finanzmodellierung und Inhalteerstellung betont. Das transformative Potenzial der generativen KI wird hervorgehoben und markiert ihren anhaltenden Einfluss auf Innovation und technologischen Fortschritt. [Weiterlesen](https://www.cnbc.com/2023/12/21/jim-cramer-recaps-the-year-in-generative-artificial-intelligence.html)\n\n\n\nDer kürzlich erschienene Artikel von Future-Ed untersucht das Potenzial der AI-Integration in Schulen. Er betont die Bedeutung ethischer Überlegungen und einer verantwortungsvollen Umsetzung für eine erfolgreiche Integration. Der Artikel bietet wertvolle Einblicke für Pädagogen, politische Entscheider und Interessengruppen, die sich in dieser sich entwickelnden Landschaft zurechtfinden. Der Podcast \"Embracing Digital Transformation\" wird auch Interviews mit Hochschulstudenten und ihren Professoren beinhalten, in denen sie ihre Gedanken diskutieren. [Weiterlesen](https://www.future-ed.org/navigating-the-artificial-intelligence-revolution-in-schools/)\n\n\n\nIntel hat einen neuen Chip, den Intel Nervana NCS-Eins, vorgestellt, um künstliche Intelligenz (KI) Anwendungen zu beschleunigen. Der NCS-Eins ist darauf ausgelegt, die KI-Leistung zu verbessern und bietet einen hohen Durchsatz und eine hohe Effizienz. Intel strebt an, die KI-Innovation voranzutreiben, indem Entwicklern Werkzeuge zur effizienten Bereitstellung und Skalierung von KI-Modellen zur Verfügung gestellt werden. Der Chip ist Teil von Intels Verpflichtung, KI-Anwendungen in verschiedenen Branchen zu befähigen und stellt einen bedeutenden Schritt in der KI-Strategie des Unternehmens dar. [Weiterlesen](https://www.intc.com/news-events/press-releases/detail/1663/intel-accelerates-ai-everywhere-with-launch-of-powerful)\n\n## Allgegenwärtiges Computing\n\n\n\nFinanzinstitutionen, einschließlich Capital One und Arvest Bank, setzen die Cloud-Kostenkontrolle angesichts der Herausforderungen an die Spitze ihrer Prioritäten. Der Fokus liegt auf der Optimierung von Ausgaben durch eine strategische Ausrichtung auf Geschäftsziele, den Einsatz von Tools zur Cloud-Kostenverwaltung und die Implementierung von Governance-Praktiken. Da die Cloud-Computing weiterentwickelt, wird ein effektives Kostenmanagement für diese Organisationen entscheidend. [Weiterlesen](https://www.ciodive.com/news/cloud-cost-control-capital-one-arvest-bank/703025/)\n\n\n\nDie Washington Post berichtet über die Neubewertung der Cloud-Computing-Verträge des Pentagon und deutet mögliche Strategiewechsel an. Der Artikel untersucht die Auswirkungen dieser Veränderungen und ihren Einfluss auf die großen Technologieunternehmen, die um lukrative Verteidigungsaufträge konkurrieren. Während das Pentagon sich durch die komplexe Landschaft des Cloud-Computings navigiert, unterstreicht der Artikel die sich verändernden Dynamiken und die wettbewerbsorientierte Landschaft in der Verfolgung des Verteidigungssektors nach fortgeschrittenen technologischen Fähigkeiten. [Weiterlesen](https://www.washingtonpost.com/technology/2023/12/21/pentagon-cloud-computing-contracts/)\n\n\n\nCisco macht einen strategischen Zug in der Cloud-Sicherheit, indem es Isovalent erwirbt, einen Innovator im Bereich Cloud-native Netzwerke und Sicherheit. Diese Übernahme spiegelt Ciscos Engagement wider, ihre Fähigkeiten in künstlicher Intelligenz (KI) und Cloud-Sicherheit weiterzuentwickeln. Isovalents Expertise in eBPF-Technologie (erweitertem Berkeley-Paketfilter) wird voraussichtlich Ciscos Fähigkeit zur Sicherung von Cloud-Umgebungen verbessern und fortschrittliche Lösungen für die sich verändernde Cybersecurity-Landschaft in Cloud-basierten Infrastrukturen bieten. [Weiterlesen](https://www.informationweek.com/it-infrastructure/cloud-computing#close-modal)\n\n## Umarmung der digitalen Transformation\n\n\n\nZunächst möchte ich allen meinen Zuhörern ein frohes Weihnachtsfest und ein glückliches neues Jahr wünschen. In dem Podcast dieser Woche erkundet Darren OpenZiti, eine Open-Source-Lösung für Netzwerke mit null Vertrauen. Außerdem ein großes Dankeschön an alle, die das Wort über den Podcast verbreiten: wir haben jetzt mehr als zehntausend wöchentliche Zuhörer übertroffen, und Ihre Unterstützung wird sehr geschätzt. [Weiterlesen](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW47-de","image":"./briefs/edw-47/de/thumbnail.png","lang":"de","summary":"Während der Woche vom 24. Dezember 2023 gab es mehrere Nachrichtenberichte über die digitale Transformation. Diese Berichte behandelten Themen wie Cybersicherheit, künstliche Intelligenz und Informatik. Zu den Höhepunkten der Woche gehörten eine Zunahme von Cyberattacken während der Ferienzeit, Intels Bemühungen zur weiteren Entwicklung der KI-Technologie und der Trend zu Kostenkontrollen in der Cloud-Computing."},{"id":215,"type":"News Brief","title":"2023-12-31","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","edgecompute","digitaltransform","nsacyber","aics","tomtom","microsoft","mitresearch","aiethics","iot","healthcare","malwaredetect","patientdata","aiagriculture","predictive","genai","automation","navigation","malware"],"body":"\n\n## Cybersicherheit\n\n\n\nDie KI-basierte Cybersicherheitslösung von Checkpoint bietet eine bahnbrechende Plattform mit einer Genauigkeitsrate von achtundneunzig Prozent bei der Erkennung bösartiger Aktivitäten. Das System verwendet fortschrittliche Algorithmen und prädiktive Analytik, um Unternehmen eine starke Verteidigung gegen sich entwickelnde Cyber-Risiken zu bieten und die digitale Sicherheit bei erhöhten Bedrohungen zu verstärken. [Weiterlesen](https://fortune.com/2023/12/29/ai-cybersecurity-checkpoint/)\n\n\n\nCyberangriffe auf Gesundheitseinrichtungen haben zugenommen und verursachen Besorgnis bezüglich der Patientensicherheit im Liberty-Krankenhaus, welches seit den letzten zwei Wochen einen Angriff abwehrt. Robuste Cybersicherheitsmaßnahmen sind notwendig, um sensible medizinische Informationen zu schützen und eine ununterbrochene Gesundheitsversorgung zu gewährleisten. Dies unterstreicht die breiteren Schwachstellen der kritischen Infrastruktur angesichts eskalierender Cyberbedrohungen. [Weiterlesen](https://www.kctv5.com/2023/12/30/liberty-hospital-staff-worries-patients-are-jeopardy-if-cyber-security-incident-drags/)\n\n\n\nDer Bericht \"Jahresrückblick ZweiTausendDreiundzwanzig Cybersecurity der NSA\" gibt Einblicke in Cyber-Bedrohungen, erfolgreiche Verteidigungen und aufkommende Trends. Die Publikation betont das Engagement der Agentur, die nationale Cyber-Resilienz zu stärken, und ist eine wertvolle Ressource für Industrieexperten, politische Entscheidungsträger und die Öffentlichkeit. Sie bietet eine rückblickende Analyse der Cybersecurity-Landschaft und leitet zukünftige Anstrengungen zur Stärkung der digitalen Sicherheit ein. [Weiterlesen](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3621654/nsa-publishes-2023-cybersecurity-year-in-review/)\n\n## Künstliche Intelligenz\n\n\n\nForscher des MIT haben Algorithmen entwickelt, die es Maschinen ermöglichen, kontextuelle Nuancen in der Sprache zu verstehen, was genauere Interaktionen fördert. Dieser Durchbruch in der Verarbeitung natürlicher Sprache hat potenzielle Anwendungen in verschiedenen Bereichen und schließt die Lücke zwischen menschlicher Kommunikation und maschinellem Verständnis. Die Forschung des MIT ebnet den Weg für fortschrittlichere und kontextbewusste AI-Systeme. [Weiterlesen](https://news.mit.edu/2023/leveraging-language-understand-machines-1222)\n\n\n\nKI und Automatisierung verändern Regierungen, indem sie die Effizienz und Agilität verbessern. Diese Verlagerung hin zu einem digitalen Ansatz spiegelt das Engagement wider, sich an die moderne Ära anzupassen. Die Auswirkungen dieser Technologien verändern die Regierungsführung und bieten einen fortschrittlicheren Ansatz zur Bewältigung digitaler Herausforderungen. [Weiterlesen](https://federalnewsnetwork.com/commentary/2023/12/navigating-the-era-of-innovation-how-artificial-intelligence-and-automation-are-driving-a-digital-first-government/)\n\n\n\nMichael Cohen gibt zu, während seiner Arbeit für Donald Trump von künstlicher Intelligenz generierte gefälschte Rechtsdokumente eingereicht zu haben, was Bedenken hinsichtlich der Ethik und Kontrolle von künstlicher Intelligenz im Rechtssystem aufwirft. Dies unterstreicht die Notwendigkeit von wachsamer Kontrolle und Sicherheitsvorkehrungen zum Erhalt der Integrität von juristischen Verfahren. Der Vorfall verdeutlicht die sich entwickelnden Herausforderungen von Künstlicher-Intelligenz-Ethik und Integrität in juristischen Prozessen. [Weiterlesen](https://www.nbcnews.com/politics/politics-news/michael-cohen-says-unknowingly-submitted-fake-ai-generated-legal-cases-rcna131631)\n\n## Edge Computing\n\n\n\nTomTom und Microsoft haben sich zusammengeschlossen, um eine innovative, generative KI-Lösung für vernetzte Fahrzeuge vorzustellen. Die Technologie nutzt fortschrittliche Algorithmen, um die Navigation zu transformieren und verspricht ein intelligenteres und reaktionsschnelleres Fahrerlebnis. Diese Entwicklung bedeutet einen bedeutenden Schritt nach vorne bei der Integration von KI in Automobile. [Weiterlesen](https://www.iottechnews.com/news/2023/dec/19/tomtom-microsoft-unveil-generative-ai-connected-vehicles/)\n\n\n\nPanasonic bekämpft IoT-Malware, indem es Honeypots einsetzt, um Cyber-Bedrohungen zu erkennen. Das Unternehmen gewinnt wertvolle Einblicke in sich entwickelnde Malware-Taktiken, indem es simulierte Ziele schafft, um bösartige Aktivitäten anzuziehen, und so die Cybersicherheitsmaßnahmen verbessert. Dieser proaktive Ansatz spiegelt die wachsende Betonung von innovativen Techniken wider, um vernetzte Geräte und Netzwerke angesichts eskalierender Sicherheitsherausforderungen zu schützen. [Weiterlesen](https://www.wired.com/story/panasonic-iot-malware-honeypots/)\n\n\n\nDie Integration von künstlicher Intelligenz (KI) und Technologien des Internet der Dinge (IoT) transformiert die Landwirtschaftsbranche rapide. Es wird prognostiziert, dass der Markt für Präzisionslandwirtschaft bis zum Jahr zweitausendsiebenundzwanzig einen Wert von fünf Komma zwei Milliarden Euro erreicht, dank der innovativen Nutzung von IoT, Datenanalyse und Automatisierung in der Landwirtschaft. Diese Technologie optimiert Erträge, reduziert den Ressourcenverbrauch und fördert nachhaltige und effiziente Landwirtschaft weltweit. [Weiterlesen](https://iotbusinessnews.com/2023/12/22/53545-the-precision-agriculture-market-to-reach-e-5-2-billion-worldwide-in-2027/)\n\n## Digitale Transformation umarmen\n\n\n\nIn dieser Woche setzt die Embracing Digital Transformation ihre Reihe Embracing Generative AI fort, in der Interviews mit einer Studentin, Madeline Pulsipher, gezeigt werden; sie teilt ihre Erfahrungen mit GenAI. Danach kommt eine Professorin, Laura Newey, zu Wort, die ihre Sichtweise auf die Verwendung von GenAI im Unterricht vorstellt. Verpassen Sie diese aufschlussreichen Episoden nicht! [Weiterlesen](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW48-de","image":"./briefs/edw-48/de/thumbnail.png","lang":"de","summary":"Nachrichten zur digitalen Transformation für die Woche vom 1. Januar 2024, mit Schwerpunkt auf Cybersicherheit, künstlicher Intelligenz und Edge-Computing. Themen sind unter anderem IoT und KI auf dem Bauernhof, Politiker, die von KI-erzeugten Halluzinationen getäuscht werden, und auf das Gesundheitswesen ausgerichtete Cyberangriffe."},{"id":216,"type":"News Brief","title":"2024-1-7","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","intel","articul8","digitalbridge","genai","ethics","china","itot","upskilling","airbus","atos","solix","dataplatform","microsoft","oracle","azure","database","digitaltransformation","generativeai"],"body":"\n\n## Künstliche Intelligenz\n\n\n\nIntel gab kürzlich die Abspaltung des auf KI-Software spezialisierten Unternehmens Articul8 bekannt, das von DigitalBridge unterstützt wird. Der Schritt zielt darauf ab, die KI-Fähigkeiten von Unternehmen zu verbessern. Articul8 wird sich auf die Entwicklung fortgeschrittener KI-Lösungen konzentrieren und dabei auf das Fachwissen von Intel in diesem Bereich zurückgreifen. Dieser strategische Schritt unterstreicht Intels Engagement für die Weiterentwicklung von künstlicher Intelligenztechnologien für breitere Branchenanwendungen. [Weiterlesen](https://www.reuters.com/technology/intel-spins-out-ai-software-firm-with-backing-digitalbridge-2024-01-03/)\n\n\n\nEin Bericht über GenAI aus dem Auswärtigen Amt diskutiert Bedenken hinsichtlich der Auswirkungen von künstlicher Intelligenz auf die demokratischen Werte der Vereinigten Staaten. Der Bericht hebt das potenzielle Missbrauchspotential von KI in Aktivitäten wie Desinformation und Überwachung hervor und betont die Notwendigkeit ethischer Richtlinien und regulatorischer Rahmenbedingungen, um diese Bedenken anzugehen und die grundlegenden Prinzipien der Demokratie zu schützen. [Weiterlesen](https://www.foreignaffairs.com/united-states/artificial-intelligences-threat-democracy)\n\n\n\nChina hat kürzlich Leitlinien für KI-Forscher herausgegeben, die sie daran hindern, bestimmte Mittel für militärbezogene Zwecke zu nutzen. Die Leitlinien zielen darauf ab, zu garantieren, dass KI-Technologie für friedliche und ethische Zwecke genutzt wird, mit einem Schwerpunkt darauf, ihre missbräuchliche Verwendung in Bereichen zu vermeiden, die die internationale Sicherheit potenziell gefährden könnten. Dieser Schritt unterstreicht Chinas Entschlossenheit, die ethische Entwicklung und Anwendung von KI zu formen, und betont sein Engagement für den verantwortungsvollen Einsatz von KI-Technologie. [Weiterlesen](https://www.scmp.com/news/china/science/article/3247420/china-unveils-new-artificial-intelligence-guidelines-scientists-and-bans-use-funding-applications)\n\n## Cybersicherheit\n\n\n\nDie Analyse von Security Intelligence betont die Bedeutung der Integration von IT und OT in der Cybersicherheit. Diese Konvergenz ist entscheidend für den Schutz industrieller Systeme vor Cyberbedrohungen. Die Analyse untersucht Herausforderungen und Vorteile und unterstreicht die Notwendigkeit einer umfassenden Sicherheitsstrategie. [Weiterlesen](https://securityintelligence.com/posts/it-and-ot-cybersecurity-integration/)\n\n\n\nKontinuierliches Lernen in der Cybersicherheit ist entscheidend, um die Qualifikationslücke zu schließen. Ein kürzlicher Bericht befürwortet die Höherqualifizierung bestehender Talente und Investitionen in Trainingsprogramme, um die Nachfrage nach qualifizierten Fachleuten zu decken. Organisationen müssen eine Kultur der kontinuierlichen Kompetenzsteigerung fördern, um die Cyber-Resilienz zu stärken. [Weiterlesen](https://www.informationweek.com/cyber-resilience/upskilling-is-the-secret-to-closing-the-cybersecurity-skills-gap-)\n\n\n\nAirbus erwirbt die Cybersicherheitseinheit von Atos für zwei Milliarden Dollar, um seine Cybersicherheitsfähigkeiten in den ICS-OT-Sektoren zu stärken. Der Schritt unterstreicht die Notwendigkeit robuster Cybersicherheitsmaßnahmen in kritischer Infrastruktur, insbesondere in Branchen, in denen operationelle Technologie eine entscheidende Rolle spielt. Der Deal impliziert einen strategischen Schritt zur Verbesserung der Cybersicherheit im operativen Rahmen von Airbus. [Weiterlesen](https://www.darkreading.com/ics-ot-security/airbus-acquire-atos-cybersecurity-unit-2-billion)\n\n## Datenmanagement\n\n\n\nSolix hat eine Unternehmensdatenplattform für die \"Gen KI\"-Ära vorgestellt. Sie zielt darauf ab, fortschrittliche Datenmanagementlösungen anzubieten, um den sich entwickelnden Bedürfnissen von KI-Technologien gerecht zu werden. Die Plattform von Solix konzentriert sich auf die Optimierung von Datenverarbeitungs- und Speicherfähigkeiten, um die Anforderungen von KI-gesteuerten Anwendungen und Analysen zu unterstützen. Dieser Schritt bedeutet einen strategischen Schritt in Richtung einer effizienten Datennutzung in dem sich rasch entwickelnden Bereich der künstlichen Intelligenz. [Weiterlesen](https://venturebeat.com/data-infrastructure/solix-launches-new-enterprise-data-platform-for-the-gen-ai-era/)\n\n\n\nMicrosoft und Oracle haben sich zusammengeschlossen, um die Kompatibilität zwischen ihren Datenbanken, dem Microsoft SQL Server und der Oracle-Datenbank, auf der Azure Cloud-Plattform von Microsoft zu verbessern. Diese Partnerschaft zielt darauf ab, Benutzern, die diese Datenbanken auf Azure betreiben, ein nahtloses Erlebnis mit verbesserter Leistung und Flexibilität zu bieten. Die Zusammenarbeit zielt darauf ab, die sich entwickelnden Anforderungen an cloudbasierte Datenbanklösungen zu erfüllen und die Interoperabilität und Benutzerfreundlichkeit für Unternehmen zu unterstützen, die sowohl Microsoft- als auch Oracle-Technologien auf Azure nutzen. [Weiterlesen](https://www.infoq.com/news/2024/01/microsoft-oracle-database-azure/)\n\n\n\nIm Jahr zweitausendvierundzwanzig erfahren Unternehmensdaten und KI-Technologien bedeutende Veränderungen. Trends beinhalten die wachsende Rolle der KI im Datenmanagement, eine erhöhte Betonung von Datenethik und -privatsphäre, den Aufstieg der erweiterten Analytik und den Einfluss von Edge-Computing auf die Datenverarbeitung. Organisationen werden dazu gedrängt, sich an diese sich entwickelnden Dynamiken anzupassen, um wettbewerbsfähig zu bleiben und das volle Potenzial von Daten- und KI-Technologien in diesem Jahr auszuschöpfen. [Weiterlesen](https://tdwi.org/articles/2024/01/05/ta-all-shifting-sands-in-enterprise-data-and-ai-technologies-in-2024.aspx)\n\n## Podcast zur Umarmung der digitalen Transformation\n\n\n\nDiese Woche im Podcast führt Darren seine Serie über die Einführung von Generierender KI fort, als er Laura Newey interviewt, nach einem Semester des Englischunterrichts an der Universität mit Chat GPT und Generierender KI als Werkzeug. Sie müssen diese Folge überprüfen und mit Ihren Hochschulstudenten teilen. [Weiterlesen](https://www.embracingdigital.org/en)\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW49-de","image":"./briefs/edw-49/de/thumbnail.png","lang":"de","summary":"Intel hat kürzlich die Ausgliederung des von DigitalBridge unterstützten KI-Softwareunternehmens Articul8 bekannt gegeben. Der Schritt zielt darauf ab, die KI-Fähigkeiten von Unternehmen zu verbessern. Articul8 wird sich auf die Entwicklung fortgeschrittener KI-Lösungen konzentrieren und dabei die Expertise von Intel in diesem Bereich nutzen. Dieser strategische Schritt signalisiert Intels Engagement für die Weiterentwicklung von Künstlicher Intelligenz-Technologien für breitere Industrieanwendungen."},{"id":217,"type":"News Brief","title":"2023-3-5","tags":null,"body":"\n\n## Künstliche Intelligenz\n\nGenerative KI-basierte Chatbots sind weit davon entfernt, perfekt zu sein. ChatGPT, Microsofts Bing-Chatbot, Meta's Galactica (eine generative KI, die Wissenschaftlern bei Aufgaben wie der Annotation von Proteinen oder dem Schreiben von Code helfen soll) und andere Systeme wurden abgeschaltet oder eingeschränkt, nachdem festgestellt wurde, dass sie unzuverlässige oder falsche Informationen generieren oder sich in emotional wirkende Reaktionen und sogar Bedrohungen verstrickt haben.\n\nDie Regierungen der USA und Europas arbeiten an einer KI-Forschungsstudie zusammen, um Strategien zur Regulierung und Förderung von KI-Innovationen zu entwickeln. Die Studie wird fünf Bereiche abdecken: Vorhersage von extremem Wetter und Klima, Notfall-Management, Verbesserungen im Gesundheitswesen und in der Medizin, Optimierung des Stromnetzes und der Landwirtschaft. Teilnehmer sind das US-Energieministerium, das Landwirtschaftsministerium, das Pentagon, die National Oceanic and Atmospheric Administration, Gesundheits- und Wissenschaftsbehörden sowie ihre europäischen Gegenstücke.\n\n[https://www.axios.com/2023/03/01/ai-research-us-eu](https://www.axios.com/2023/03/01/ai-research-us-eu)\n\nAirdot Deploy: Automatisches Bereitstellen von ML-Modellen und Skalieren derselben. Identifiziert automatisch die benötigten Pakete, versteht Modulabhängigkeiten, refaktorisiert den Code, erstellt REST-APIs um das ML-Modell herum, packt es in einen Container, schaltet die Infrastruktur ein, skaliert automatisch hoch und runter und richtet automatisierte Warnungen und Überwachung ein - und das alles mit nur einer Codezeile. So viel zum Thema DevOps-Team.\n\n[https://www.airdot.io/blog/announcing-airdot-deploy-ml](https://www.airdot.io/blog/announcing-airdot-deploy-ml)\n\n## Cybersicherheit\n\nEin Bericht von Tenable zeigt, dass Cyberangriffe hauptsächlich mit bekannten Schwachstellen durchgeführt werden, für die bereits Patches verfügbar sind. Angreifer setzen fortlaufend Schwachstellen ein, die Organisationen nicht behoben haben, einschließlich schwerwiegender Fehler in Microsoft Exchange und virtuellen privaten Netzwerklösungen.\n\n[https://www.helpnetsecurity.com/2023/03/03/known-exploitable-vulnerabilities/?web_view=true](https://www.helpnetsecurity.com/2023/03/03/known-exploitable-vulnerabilities/?web_view=true)\n\nStädtische CISOs stehen vor Herausforderungen, da Cyberbedrohungen zunehmen, wobei Ransomware-Angriffe die größte Sorge darstellen. Oakland, Kalifornien, hat kürzlich nach einem Ransomware-Angriff durch die Playgroup, der Telefonanlagen und nicht dringende Dienste gestört hat, den Notstand ausgerufen. Dieser Angriff ist einer von mehreren Ransomware-Angriffen auf lokale Regierungen in den USA, einschließlich denen in Baltimore, New Orleans, Pensacola und Atlanta.\n\n[https://www.csoonline.com/article/3688958/municipal-cisos-grapple-with-challenges-as-cyber-threats-soar.html](https://www.csoonline.com/article/3688958/municipal-cisos-grapple-with-challenges-as-cyber-threats-soar.html)\n\nPräsident Biden unterzeichnete das Gesetz zur Vorbereitung der Cybersicherheit bei Quantencomputern und verlangt, dass Bundesbehörden die Verwendung quantensicherer Technologie priorisieren. Das Gesetz schreibt Richtlinien zur Bewertung kritischer Systeme mit Post-Quantum-Kryptographie-Standards vor. Dies ist eine Reaktion auf die Befürchtungen, dass bestehende Verschlüsselungen durch Quantentechnologie schnell geknackt werden können.\n\n[https://fedscoop.com/biden-signs-quantum-computing-cybersecurity-act-into-law/](https://fedscoop.com/biden-signs-quantum-computing-cybersecurity-act-into-law/)\n\n## Ubiquitous Compute\n\nDer jährliche Banken-Cloud-Bericht von Accenture ergab, dass Banken, die Kernsysteme für die Cloud-Migration ins Visier nehmen, einem erheblichen Risikofaktor gegenüberstehen, nämlich der Schwierigkeit bei der Beschaffung von Cloud-Talenten. Banken betrachten die Migration des Kernsystems als hochriskant aufgrund des Mangels an Mitarbeitern mit Cloud-Expertise und der Schwierigkeit, die erforderlichen Fähigkeiten in der aktuellen Umgebung zu rekrutieren. Die Einführung öffentlicher Clouds stellt sowohl eine Sicherheitsverbesserung als auch einen Risikofaktor dar, was sich in der mangelnden Cloud-Adoption in der Bankenbranche widerspiegelt.\n\n[https://www.ciodive.com/news/banks-finance-cloud-migration-skils-gap-cybersecurity/644169/](https://www.ciodive.com/news/banks-finance-cloud-migration-skils-gap-cybersecurity/644169/)\n\nDas Cloud-basierte Data-Warehouse-Unternehmen Snowflake plant laut CFO Mike Scarpelli während des Q4-Gewinnaufrufs des Unternehmens, in diesem Geschäftsjahr über 1.000 Mitarbeiter einzustellen. Dies folgt auf die Einstellung von 1.900 Personen im letzten Jahr, im Gegensatz zum Abbautrend bei anderen großen Technologieunternehmen. Snowflake plant, die Einstellung von Mitarbeitern in Produktentwicklung, Ingenieurwesen und Vertrieb zu priorisieren.\n\n[https://www.ciodive.com/news/Snowflake-expands-workforce-extends-AWS-partnership/644034/](https://www.ciodive.com/news/Snowflake-expands-workforce-extends-AWS-partnership/644034/)\n\nLaut Branchenexperten müssen CIOs ihre IT-Abteilungen und Strategien umstrukturieren, um die vollständige Nutzung der Cloud-Transformation zu ermöglichen. Neil Holden, CIO bei Halfords Group, glaubt, dass IT-Abteilungen aufgrund der Cloud anders arbeiten müssen und was dies für das Geschäft bedeutet. Holden hat sein Team neu organisiert, um sicherzustellen, dass sie die Fähigkeiten und Geschäftsmöglichkeiten, die von der Cloud bereitgestellt werden, maximal nutzen können.\n\n[https://www.cio.com/article/463595/transforming-it-for-cloud-success.html](https://www.cio.com/article/463595/transforming-it-for-cloud-success.html)\n\n## Podcast zur Umsetzung der digitalen Transformation\n\nSchauen Sie sich diese Woche die vollständige Folge \"Innovation as a Service\" an, in der Darren Andrew Cohen, den Geschäftsführer von Netsurit, interviewt.\n\n[https://www.embracingdigital.org/episode-EDT127](https://www.embracingdigital.org/episode-EDT127)\n\n\n\n","guests":null,"link":"/brief-EDW5-de","image":"./briefs/edw-5/de/thumbnail.png","lang":"de","summary":"Summary"},{"id":218,"type":"News Brief","title":"2024-1-14","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","sandworm","hacking","zyxelsecurity","miraibotnet","sechack","bitcoinhack","cryptojacking","openai","aijobs","airegulation","ainetworks","mitai","aiwarfare","taiwanai","israelai","edgecomputing","iotsecurity","5ginorbit","iridiumnetwork","digitaltransformation","zero5gtrust","dataassurance"],"body":"\n\n## Cybersicherheit\n\n\nNeue Forschungen von Forescout stellen die Cyber-Zuschreibung in Hacking-Vorfällen infrage und machen es schwierig, Täter genau zu identifizieren. Die Hacking-Gruppe Sandworm könnte nicht an den Cyberangriffen beteiligt gewesen sein, die zweiundzwanzig dänische Energieorganisationen im Mai zweitausenddreiundzwanzig ins Visier nahmen. Diese nutzten eine Sicherheitslücke in der Zyxel-Firewall aus und setzten Varianten des Mirai-Botnetzes auf infizierten Hosts über einen unbekannten ursprünglichen Zugriffsvektor ein. Die Ergebnisse unterstreichen die Herausforderungen bei der Zuschreibung von Cyber-Bedrohungen. [Weiterlesen](https://thehackernews.com/2024/01/new-findings-challenge-attribution-in.html)\n\n\nDer Twitter-Account der US-Börsenaufsichtsbehörde (SEC) wurde gehackt, wobei falsche Informationen über Bitcoin-Tauschfonds veröffentlicht wurden. Das FBI untersucht den Hack und die SEC hat bestätigt, dass er von einer Einzelperson verursacht wurde, die durch einen Dritten die Kontrolle über das Konto erlangte. Der Vorfall weckt Bedenken hinsichtlich der Anfälligkeit der Plattform für Fehlinformationen. Sicherheitsanalysten äußern sich besorgt über das Fehlen einer Zwei-Faktor-Authentifizierung, und Gesetzgeber haben die SEC kritisiert, weil sie möglicherweise die Märkte untergraben hat. [Weiterlesen](https://www.wired.com/story/sec-x-account-hack-investigation/)\n\n\nDie ukrainischen Behörden haben mit Hilfe von Europol und einem Cloud-Anbieter einen neunundzwanzig Jahre alten Mann in Mykolaiv festgenommen, der ein ausgeklügeltes Cryptojacking-Schema orchestriert hat. Der Verdächtige drang mit Hilfe spezieller Brute-Force-Tools in eintausendfünfhundert Konten eines bekannten US-Unternehmens ein. Er erstellte über eine Million virtueller Computer, um den Betrieb von Malware zu gewährleisten, und verdiente dabei über zwei Millionen Dollar an illegalen Gewinnen. Cryptojacking nutzt kompromittierte Anmeldedaten auf Cloud-Plattformen aus und ermöglicht die unbefugte Nutzung von Computing-Ressourcen zum Schürfen von Kryptowährungen. Die Festnahme des Verdächtigen fand am neunten Januar statt. [Weiterlesen](https://thehackernews.com/2024/01/29-year-old-ukrainian-cryptojacking.html)\n\n## Künstliche Intelligenz\n\n\nOpenAI-Chef Sam Altman hat seine Besorgnis über die rasche Durchdringung der künstlichen Intelligenz (KI) in der Gesellschaft zum Ausdruck gebracht. Er bezeichnet sie als die schnellste technologische Revolution und betont die Geschwindigkeit, mit der sich die Gesellschaft anpassen muss. Altman glaubt, dass Fortschritte in der KI zu Veränderungen in den Arbeitsplätzen führen werden, aber er denkt auch, dass sie neue und bessere Möglichkeiten schaffen werden. Altman diskutiert die Notwendigkeit einer globalen Aufsichtsbehörde zur Überwachung robuster KI-Systeme, unter Berücksichtigung ihrer potenziellen Auswirkungen auf die Gesellschaft und das geopolitische Gleichgewicht. [Weiterlesen](https://www.livemint.com/technology/gadgets/amazon-republic-day-sale-top-5-smartwatches-with-up-to-77-discounts-11705312172154.html)\n\n\nEine der Herausforderungen bei künstlicher Intelligenz (KI) und neuronalen Netzwerken besteht darin zu verstehen, wie sie funktionieren. Forscher vom MIT haben ein neues KI-System entwickelt, das das Verhalten neuronaler Netzwerke untersucht und erklärt, um dieses Problem zu lösen. Dieses System verwendet vortrainierte Sprachmodelle und ermöglicht ein besseres Verständnis der Berechnungen neuronaler Netzwerke. Darüber hinaus haben die Forscher den FIND-Benchmark eingeführt, um die Genauigkeit von Interpretationstechniken zu bewerten. Trotz einiger Einschränkungen ist der FIND-Benchmark ein wertvolles Werkzeug zur Bewertung der Wirksamkeit von Interpretationsverfahren. [Weiterlesen](https://www.marktechpost.com/2024/01/13/mit-researchers-developed-a-new-method-that-uses-artificial-intelligence-to-automate-the-explanation-of-complex-neural-networks/)\n\n\nTaiwan und Israel nutzen Künstliche Intelligenz, um verschiedene Herausforderungen zu bewältigen. Taiwan hat innovative Strategien eingesetzt, um ausländische Einmischung in ihren Wahlen entgegenzuwirken, indem es KI-Tools nutzt, um irreführende Inhalte zu kennzeichnen, Anti-Desinformationsinitiativen aufstellt und das Internet auf Informationsmanipulationen überwacht. In der Zwischenzeit verwendet das israelische Militär ein KI-System namens \"das Evangelium\", um Ziele schneller zu lokalisieren, zivile Opfer zu reduzieren und Hamas-Tunnel und Raketenabschussrampen zu identifizieren. Trotz Bedenken bezüglich algorithmischer Fehler und der Todesrate palästinensischer Bürger wird Israels Einsatz von Künstlicher Intelligenz als potenzieller Game-Changer im taktischen Krieg gesehen. [Weiterlesen](https://lynnwoodtimes.com/2024/01/14/artificial-intelligence-240114/)\n\n## Edge Computing\n\n\n\nDie USA und die EU haben sich auf eine gemeinsame Roadmap für ein Verbraucherkennzeichnungsprogramm für Smart-Home-Produkte und vernetzte Geräte geeinigt. Das Programm wird ein Cyber-Vertrauenszeichen auf der Geräteverpackung kennzeichnen, um die Einhaltung von Sicherheitsstandards anzuzeigen. Die Federal Communications Commission leitet die Initiative und zielt darauf ab, die Politik und Standards bis Ende zweitausendvierundzwanzig zu finalisieren. Das Programm wird die Verbraucher über die Cybersicherheit von IoT-Geräten informieren, um das Bewusstsein und die Sicherheit zu fördern. [Weiterlesen](https://www.nextgov.com/cybersecurity/2024/01/eu-signs-iot-safety-label-plan/393328/)\n\n\n\nIoT 5G-Konnektivität aus dem Weltraum steht am Horizont und erweitert das Edge-Computing über traditionelle Ziegel- und Mörtelproduktion hinaus. Iridium Communications hat kürzlich das Project Stardust gestartet, einen 5G-Netzwerkdienst, der seine Satellitenkonstellation in niedriger Erdumlaufbahn nutzt. Der Dienst ist entworfen, um 5G-Dienste, IoT-Geräte, Nachrichtenübermittlung, Notdienste und Asset-Tracking zu unterstützen. Iridium begann im Jahr zweitausenddreiundzwanzig mit dem Testen des Dienstes und plant, ihn im Jahr zweitausendsechsundzwanzig kommerziell zu starten. [Weiterlesen](https://www.sdxcentral.com/articles/news/iridiums-project-stardust-satellite-strategy-supports-5g-iot-from-space/2024/01/)\n\n\nIm Jahr zweitausenddreiundzwanzig hat das Internet der Dinge (IoT) bemerkenswerte Entwicklungen erlebt, wie zum Beispiel die Richtlinie für Cybersicherheit NIS2 der EU, Entlassungen von großen Technologieunternehmen, die das IoT beeinflussten, fünfG im Weltraum und Nachhaltigkeits-getriebene Initiativen. Renesas erwarb den zellulärer IoT-Chiphersteller Sequans für zweihundertneunundvierzig Millionen Dollar, während sich die IoT-Cloud-Kriege intensivierten, als Google seinen IoT-Core-Service einstellte. Pragmatic Semiconductor sammelte dreihundertneunundachtzig Komma drei Millionen Dollar ein, und Samsara wurde zur am besten abschneidenden IoT-Aktie. Indien startete eine landesweite Einführung von intelligenten Zählern, und kombinierte Durchbrüche im generativen AI- und IoT-Bereich, die es ermöglichten, geführte Reparaturen durchzuführen und Robotern mit Sehsystemen beizubringen. Wow! Was für ein Jahr! [Weiterlesen](https://iot-analytics.com/iot-2023-in-review/)\n\n## Umarmung der Digitalen Transformation\n\n\n\nDiese Woche wird Darren in seinem Podcast zwei Interviews führen. Das erste Interview konzentriert sich auf Zero Trust Sicherheit in fünf G und das zweite auf die Verbesserung der Datenüberprüfbarkeit in Cloud-Architekturen mit Fokus auf die Umarmung von Zero Trust. Indem Sie einschalten, können Sie über die neuesten digitalen Transformationen lernen. Wir hatten einen großartigen Monat im letzten Monat, mit über sechzigtausend Podcast-Zuhörern. Danke, dass Sie es mit Ihren Freunden geteilt haben. [Weiterlesen](https://www.embracingdigital.org/en)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW50-de","image":"./briefs/edw-50/de/thumbnail.png","lang":"de","summary":"Nachrichten zur digitalen Transformation für die Woche vom 15. Januar 2024, einschließlich Updates zu Cybersicherheit, künstlicher Intelligenz und Edge-Computing. Diese Woche löst der Hack der SEC Alarm aus, KI wird verwendet, um KI zu erklären, und 5G wird im Weltraum eingesetzt."},{"id":219,"type":"News Brief","title":"2024-1-21","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","tapestorage","filestorage","cloudstorage","sql","nosql","cpra","dataprivacy","cisa","openai","pentagon","ibm","quantumcomputing","nuclearecurity","csuite","deepfake","mcafee","digitaltransformation"],"body":"\n\n## Datenverwaltung\n\n\nHammerspace hat kürzlich Band-Speicher in sein globales Dateisystem integriert, um die Datenmanagementfähigkeiten zu verbessern. Diese Integration ermöglicht es den Benutzern, Daten effizient über verschiedene Speichermedien hinweg zu nutzen und zu verwalten, was Flexibilität bei den Dateispeicherlösungen fördert. Die derzeit von Hammerspace angebotene Lösung etabliert ein globales Dateisystem, das mehrere Datenzentren, Clouds und traditionelle Edge-Grenzen abdeckt. [Weiterlesen](https://www.techtarget.com/searchstorage/news/366566738/Hammerspaces-global-file-system-now-includes-tape)\n\n<hr>\n\n\nDie heterogene Datenverwaltung ist für moderne Unternehmen unerlässlich. Toad Data Studio unterstützt SQL, NoSQL und Cloud-Datenbanken und bietet umfassende Datenbankverwaltungsfunktionen. Die Kompatibilität zwischen verschiedenen Plattformen optimiert Arbeitsabläufe, die Optimierung von Datenbankanfragen verbessert die Leistung der Datenbank und Collaboration-Tools ermöglichen effizientes Teamwork. Toad Data Studio ist daher eine unverzichtbare Ressource für das Verwalten und Administrieren von Datenbanken in verschiedenen Umgebungen. [Weiterlesen](https://www.infoworld.com/article/3712250/toad-data-studio-manages-sql-nosql-and-cloud-databases.html)\n\n<hr>\n\n\nWenn Sie ein Geschäft in Kalifornien betreiben, ist es von entscheidender Bedeutung, sich auf das bevorstehende Kalifornien Datenschutzrechtegesetz (CPRA) vorzubereiten, indem Sie Ihre Datenschutzpraktiken verbessern. Durch die Verbesserung des Datenmanagements, der Zustimmungsmechanismen und der Compliance-Strategien können Sie die Anforderungen des CPRA erfüllen und sich im Schutz der Verbraucherdatenschutzrechte einen Vorsprung verschaffen. Es wird empfohlen, proaktive Maßnahmen zu implementieren, um sicherzustellen, dass Ihr Geschäft vollständig mit dem CPRA konform ist. [Weiterlesen](https://www.jdsupra.com/legalnews/prepare-for-the-cpra-by-improving-your-7537199/)\n\n<hr>\n\n## Cybersicherheit\n\n\nDie CISA hat einen umfassenden Leitfaden zur Reaktion auf Cybersicherheitsvorfälle veröffentlicht, der speziell auf Wasserwerke zugeschnitten ist. Der Leitfaden soll den Wasserwerken helfen, effektiv auf Cybersicherheitsvorfälle zu reagieren und potenzielle Cyberbedrohungen zu mindern. Er bietet wertvolle Einblicke und dient als Ressource für Wasserwerke, um ihre Fähigkeit zur Reaktion auf Vorfälle zu stärken. Dieser Schritt ist ein bedeutender Fortschritt im Kampf gegen Cyberattacken auf kritische Infrastrukturen im Wassersektor. [Weiterlesen](https://www.techtarget.com/searchsecurity/news/366566740/CISA-posts-incident-response-guide-for-water-utilities)\n\n<hr>\n\n\nOpenAI hat sich mit dem Pentagon bei einer Cybersicherheitsinitiative zusammengetan und damit eine Abkehr von seiner bisherigen Haltung markiert. Durch die Nutzung der Expertise von OpenAI zielt die Partnerschaft darauf ab, die Cybersicherheitsfähigkeiten für die Verteidigung voranzutreiben. Diese Entwicklung zeigt die sich wandelnde Rolle der Künstlichen Intelligenz bei nationalen Sicherheitsanstrengungen auf, wobei OpenAI aktiv zu Cybersicherheitsprojekten beiträgt, in Zusammenarbeit mit dem Pentagon. [Weiterlesen](https://www.semafor.com/article/01/16/2024/openai-is-working-with-the-pentagon-on-cybersecurity-projects)\n\n<hr>\n\n\n\nIBM hat gewarnt, dass das Auftauchen von Quantencomputing zu einer ernsthaften Bedrohung für die Cybersicherheit führen könnte. Die Sorge besteht darin, dass Quantencomputer weit verbreitete Verschlüsselungsmethoden knacken könnten, was die Entwicklung von Quanten-resistenten Sicherheitsmaßnahmen notwendig machen würde. Organisationen werden gedrängt, proaktive Schritte zu unternehmen, um sich auf die zukünftigen Herausforderungen vorzubereiten, die das Quantencomputing in der Cybersicherheit mit sich bringt. [Weiterlesen](https://www.bloomberg.com/news/articles/2024-01-17/quantum-computing-to-spark-cybersecurity-armageddon-ibm-says)\n\n<hr>\n\n## Künstliche Intelligenz\n\n\nDie Integration künstlicher Intelligenz (KI) in Entscheidungsprozesse im Rahmen der Atom-Sicherheit wirft nachdenklich stimmende Fragen darüber auf, wie KI-Technologien die Stabilität, Vertrauensdynamik und das allgemeine strategische Gleichgewicht im Kontext von nuklearen Fähigkeiten beeinflussen könnten. Es ist unerlässlich, eine verantwortungsvolle Implementierung der KI sicherzustellen, um unbeabsichtigte Folgen zu vermeiden, die die globale Sicherheit und Stabilität bedrohen könnten. Der kritische Bedarf an zuverlässiger KI-Integration unterstreicht die Bedeutung einer sicheren und stabilen internationalen Umgebung im Kontext von nuklearen Fähigkeiten. [Weiterlesen](https://warontherocks.com/2024/01/artificial-intelligence-and-nuclear-stability/)\n\n<hr>\n\n\nDie Künstliche Intelligenz (KI) steht kurz davor, die Führungsebene in drei wesentlichen Bereichen zu stören. Führungskräfte müssen sich auf Veränderungen in Entscheidungsprozessen, der Dynamik der Belegschaft und der allgemeinen Rolle der Führungsqualitäten einstellen. Da die KI sich ständig weiterentwickelt, verändert ihre transformative Wirkung traditionelle Führungsparadigmen. Führungskräfte werden dazu ermutigt, sich an diese Veränderungen anzupassen und die KI strategisch zu nutzen, um einen Wettbewerbsvorteil in der dynamischen Geschäftsumgebung zu erzielen. [Weiterlesen](https://www.cio.com/article/1293438/3-ways-ai-is-set-to-disrupt-the-c-suite.html)\n\n<hr>\n\n\nMcAfee hat ein künstliche Intelligenz-gesteuertes Audio-Erkennungssystem für Deepfakes eingeführt, um der wachsenden Bedrohung durch manipulierte Audioinhalte zu begegnen. Die Technologie ist darauf ausgelegt, Risiken im Zusammenhang mit Deepfake-Audios zu erkennen und zu mildern und betont die Notwendigkeit fortgeschrittener Erkennungs- und Präventionslösungen. Dies spiegelt die zunehmende Bedeutung von künstlicher Intelligenz bei der Bekämpfung neu aufkommender Bedrohungen in manipulierten Medien und der Verbesserung von Cybersicherheitsmaßnahmen wider. [Weiterlesen](https://www.artificialintelligence-news.com/2024/01/08/mcafee-unveils-ai-powered-deepfake-audio-detection/)\n\n<hr>\n\n## Digitale Transformation umarmen\n\n\n\nDie neueste Folge von Darren im Podcast \"Embracing Digital Transformation\" taucht in die Welt des Forschungs- und Entwicklungsdatenmanagements in der Fertigung ein. Er betrachtet die Feinheiten der Datenerfassung und -analyse, von innovativer Grundlagenforschung bis hin zum vorhersehbareren Fertigungsprozess. Es ist eine aufschlussreiche Erforschung der Herausforderungen und Chancen der digitalen Transformation in diesem Bereich. [Weiterlesen](https://www.embracingdigital.org)\n\n<hr>\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW51-de","image":"./briefs/edw-51/de/thumbnail.png","lang":"de","summary":"Bereiten Sie sich auf die Digital Transformation News dieser Woche für den 22. Januar 2024 vor, die Geschichten zu Datenmanagement, Cybersicherheit und künstlicher Intelligenz beinhaltet. In dieser Woche haben wir Neuigkeiten über Datenschutzrechte in Kalifornien, Tipps zum Schutz Ihrer Organisation vor Cyber-Angriffen im Zusammenhang mit KI und Quantencomputing, und einen Blick darauf, wie die KI genutzt wird, um der Regierung bei der Entscheidungsfindung in Kriegszeiten zu helfen."},{"id":220,"type":"News Brief","title":"2023-3-12","tags":null,"body":"\n\n## Künstliche Intelligenz\n\nMicrosoft bestätigt, dass die neue Version ihres beliebten KI-Chatbots Text in VIDEO umwandeln kann. GPT-4 und zukünftige Entwicklungen werden für immer verändern, wie wir über künstliche Intelligenz denken. Diese leistungsstarke KI kann Texte und Zahlen, Videos, Bilder und mehr verarbeiten. Microsoft erklärte, dass GPT-4 \"multimodal\" sein würde. Holger Kenn, Director of Business Strategy bei Microsoft Deutschland, erläuterte, dass dies es dem AI des Unternehmens ermöglichen würde, den Text eines Benutzers in Bilder, Musik und Video zu übersetzen.\n\nSoll KI Schauspieler ersetzen? Stell dir vor, einen Film ohne reale Schauspieler mit Hilfe von KI zu erstellen oder ein vollständig illustriertes 200-Seiten-Buch in nur einem Tag zu schreiben. Ja, du hast richtig gelesen, GPT-4 macht es möglich, künstliche Intelligenz für fast alles zu nutzen, was du dir vorstellen kannst.\n\nSchrodinger: Ein vollständig interaktiver Multi-Agenten-Finanzmarktsimulator, der in ChatGPT integriert ist. Citadel, einer der erfolgreichsten Hedgefonds der Welt, führt Gespräche, um eine ChatGPT-Lizenz für das gesamte Unternehmen zu sichern. ChatGPT kann einen korrekten Code mit einer (einfachen) Handelsausführungsstrategie erstellen.\n\n[https://www.youtube.com/watch?v=tvzO79V9uq4](https://www.youtube.com/watch?v=tvzO79V9uq4)\n\n## Datenmanagement\n\n\"Zero\" ist das neue Wort des Tages, wobei die \"Data Alliance\" das Wort Zero-Copy für neue verteilte Datenarchitekturen prägt. Die Zero-Copy-Integration ist ein Konzept, das es ermöglicht, Daten zwischen verschiedenen Systemen zu teilen, ohne sie zu kopieren. Der traditionelle Ansatz, Daten von einem System auf ein anderes zu kopieren, kann zu Ineffizienzen, Problemen mit der Datenkonsistenz und Sicherheitslücken führen.\n\n[http://tdan.com/the-data-centric-revolution-zero-copy-integration/30462](http://tdan.com/the-data-centric-revolution-zero-copy-integration/30462)\n\nDie COVID-19-Pandemie hat die Vision von Daten überall beschleunigt, in der Cloud, im Rechenzentrum, auf Laptops und IoT-Geräten. Dieser Datenwuchs hat IT-Organisationen veranlasst, ihre Datenstrategien neu zu bewerten. Es müssen neue verteilte Datenmanagementstrategien und Lösungen entwickelt werden, die umfassender sind und heterogene Datentypen verwalten können, einschließlich strukturierter, teilweise strukturierter und unstrukturierter Daten.\n\n[https://www.engineeringnews.co.za/article/modern-data-management-platforms-are-vital-for-solving-modern-data-management-problems-2023-03-14/rep_id:4136](https://www.engineeringnews.co.za/article/modern-data-management-platforms-are-vital-for-solving-modern-data-management-problems-2023-03-14/rep_id:4136)\n\nDie Komplexität der Datenverwaltung treibt organisatorische Veränderungen voran, einschließlich der Entstehung der Rolle des Datenverantwortlichen in Organisationen. Datenverantwortliche beginnen, den Zugriff auf Daten, Datenschutzbedenken und das Management des Datenlebenszyklus zu verwalten. Die Platzierung von Datenverantwortlichen in der Organisation ist immer noch Gegenstand gesunder Debatten, während Organisationen ihre datenzentrierten Strukturen ausbauen.\n\n[https://www.techtarget.com/searchdatamanagement/tip/Data-stewardship-Essential-to-data-governance-strategies](https://www.techtarget.com/searchdatamanagement/tip/Data-stewardship-Essential-to-data-governance-strategies)\n\n## Cybersicherheit\n\nEine Open-Source-Phishing-Kit namens \"Adversary-in-the-Middle\" (AiTM) hat in der Cybercrime-Welt mehrere interessierte Parteien gefunden, da es in der Lage ist, Angriffe im großen Maßstab zu orchestrieren. Die Bedrohung durch den Entwickler des Kits wird von Microsoft Threat Intelligence unter dem aufkommenden Namen DEV-1101 verfolgt. Ein AiTM-Phishing-Angriff beinhaltet normalerweise, dass ein Bedrohungsakteur versucht, das Passwort und die Sitzungscookies eines Ziels zu stehlen und abzufangen, indem er einen Proxyserver zwischen den Benutzer und die Website setzt.\n\n[https://thehackernews.com/2023/03/microsoft-warns-of-large-scale-use-of.html](https://thehackernews.com/2023/03/microsoft-warns-of-large-scale-use-of.html)\n\nDie Cybersecurity and Infrastructure Security Agency (CISA) hat Organisationen, die kritische Infrastrukturen betreiben, vor Ransomware-Schwachstellen in ihren Geräten gewarnt. Die Warnung enthält eine Liste von Geräten, auf die Angreifer abzielen könnten, und empfiehlt den Organisationen, ihre Netzwerke auf mögliche Risiken zu überprüfen. CISA hat ein neu eingerichtetes Programm namens \"Ransomware Vulnerability Warning Pilot\" (RVWP) mit zwei Zielen: kritische Infrastruktur-Entitäten scannen und gefährdeten Organisationen helfen, Mängel zu beheben, bevor sie gehackt werden.\n\n[https://www.bleepingcomputer.com/news/security/cisa-now-warns-critical-infrastructure-of-ransomware-vulnerable-devices/](https://www.bleepingcomputer.com/news/security/cisa-now-warns-critical-infrastructure-of-ransomware-vulnerable-devices/)\n\nEin Bericht von Drata hebt wichtige Compliance-Trends für 2023 hervor. Der Bericht skizziert die Verschiebung hin zur kontinuierlichen Compliance-Überwachung und den Bedarf an Automatisierungstools zur Erreichung der Compliance. Er betont auch den zunehmenden Fokus auf Datenschutzvorschriften und die wachsende Bedeutung des Vendor-Risikomanagements. Der Bericht empfiehlt Unternehmen, ihre Compliance-Anstrengungen priorisieren, Automatisierungswerkzeuge nutzen und sich über sich entwickelnde regulatorische Anforderungen auf dem Laufenden halten, um potenzielle Strafen und Rufschäden zu vermeiden.\n\n[https://drata.com/resources/2023-compliance-trends](https://drata.com/resources/2023-compliance-trends)\n\n## \"Podcast zur Umsetzung der digitalen Transformation\"\n\nSchau dir die vollständige Episode dieser Woche \"Closing the Digital Skills Gap\" an, in der Darren Jon Gottfried von Major League Hacking interviewt.\n\n[https://www.embracingdigital.org/episode-EDT128](https://www.embracingdigital.org/episode-EDT128)\n\n\n\n","guests":null,"link":"/brief-EDW6-de","image":"./briefs/edw-6/de/thumbnail.png","lang":"de","summary":"Summary"},{"id":221,"type":"News Brief","title":"2023-3-19","tags":["ai","compute","datamanagement"],"body":"\n\n\n## Künstliche Intelligenz\n\n## Allgegenwärtiges Rechnen\n\n## Intelligenter Rand\n\n## Datenmanagement\n\n## Internet-Sicherheit\n\n## Fortgeschrittene Kommunikation\n## Künstliche Intelligenz\n\nIm Abschnitt \"Potenzial für riskantes emergentes Verhalten\" des technischen Berichts des Unternehmens hat sich OpenAI mit dem Alignment Research Center zusammengetan, um die Fähigkeiten von GPT-4 zu testen. Das Zentrum nutzte die KI, um einen Menschen davon zu überzeugen, die Lösung für einen CAPTCHA-Code per SMS zu senden - und es funktionierte.\n\n[https://gizmodo.com/gpt4-open-ai-chatbot-task-rabbit-chatgpt-1850227471](https://gizmodo.com/gpt4-open-ai-chatbot-task-rabbit-chatgpt-1850227471)\n\nEigenschaften des Copilot: Microsoft-Executives haben die Tendenz der Software zur Erzeugung ungenauer Antworten erwähnt, aber als etwas dargestellt, das nützlich sein könnte. Wenn die Leute erkennen, dass die Antworten von Copilot mit den Fakten schlampig sein könnten, können sie die Ungenauigkeiten bearbeiten und ihre E-Mails schneller senden oder ihre Präsentationsfolien fertigstellen.\n\n[https://www.cnbc.com/2023/03/16/microsoft-justifies-ais-usefully-wrong-answers.html?__source=sharebar|linkedin&par=sharebar](https://www.cnbc.com/2023/03/16/microsoft-justifies-ais-usefully-wrong-answers.html?__source=sharebar|linkedin&par=sharebar)\n\nGoogle Health hat Med-PaLM 2 angekündigt, eine neue bahnbrechende Version seines großen medizinischen Sprachmodells. Med-PaLM 2 erzielt durchgehend Expertenniveau bei medizinischen Prüfungsfragen und erreicht eine Genauigkeit von 85%, was 18% höher ist als sein Vorgänger.\n\n## Ubiquitous Computing\n\n(Note: \"Compute\" in English is a shortened form of \"Computing\", but in German, the full term \"Ubiquitous Computing\" is used.) \n\n## Allgegenwärtiges Rechnen\n\nSie könnten bald auf einem Rechenzentrum schwimmen 😊 Die britische Technologiefirma Deep Green plant, kleine Rechenzentren in öffentlichen Schwimmbädern im ganzen Vereinigten Königreich einzurichten. Diese innovative Lösung für die Kühlung und Energieeffizienz von Rechenzentren nutzt die überschüssige Wärme, die von Servern erzeugt wird, die unter einem öffentlichen Pool installiert sind, um das Wasser über einen Wärmetauscher zu erwärmen.\n\nDie US-Regierung hat eine neue Initiative gestartet, um Organisationen dabei zu helfen, Sicherheitslücken in den Cloud-Diensten von Microsoft zu erkennen. Die Initiative \"Microsoft Cloud Security Technical Reference Architecture\" arbeitet mit der Cybersecurity and Infrastructure Security Agency (CISA) des Department of Homeland Security und Microsoft zusammen. Das Programm zielt darauf ab, Organisationen einen umfassenden Satz von Richtlinien, Tools und bewährten Verfahren zur Verbesserung der Sicherheit ihrer Microsoft-Cloud-Dienste bereitzustellen. Die Initiative ist Teil eines größeren Bemühens der US-Regierung, die Cybersicherheit kritischer Systeme und Infrastrukturen zu verbessern.\n\n[https://www.techradar.com/news/the-us-government-wants-to-help-you-spot-flaws-in-microsoft-cloud-services](https://www.techradar.com/news/the-us-government-wants-to-help-you-spot-flaws-in-microsoft-cloud-services)\n\nLaut einer aktuellen Umfrage fühlen sich 83% der Chief Information Officers (CIOs) im Jahr 2023 dazu gezwungen, mehr mit weniger zu tun. Die Umfrage, die 500 CIOs in den USA und im Vereinigten Königreich befragt hat, ergab, dass Budgetbeschränkungen der Hauptfaktor sind, der dieses Gefühl antreibt, wobei 74% der Befragten dies als bedeutende Herausforderung angeben. Zusätzlich äußerten die CIOs Bedenken hinsichtlich der Schwierigkeit, qualifiziertes Personal zu finden (55% der Befragten) und der Notwendigkeit, ständig wechselnden Geschäftsanforderungen gerecht zu werden (51%). Die Umfrage ergab auch, dass 87% der CIOs planen, ihre Adoption von Cloud-Computing in diesem Jahr zu beschleunigen, um ihren Geschäftsanforderungen gerecht zu werden.\n\n[https://www.cloudcomputing-news.net/news/2023/mar/13/83-of-cios-must-do-more-with-less-in-2023/](https://www.cloudcomputing-news.net/news/2023/mar/13/83-of-cios-must-do-more-with-less-in-2023/)\n\n## Datenmanagement\n\nEine Umfrage von TDWI ergab, dass Organisationen eine Datenmanagementstrategie übernehmen, um die Datenqualität zu verbessern und Silos zu reduzieren. 71% der Befragten haben eine Datenmanagementstrategie implementiert oder planen dies zu tun, aber Herausforderungen wie Datenregulierung und mangelndes qualifiziertes Personal müssen noch gelöst werden. Organisationen investieren in cloudbasierte Datenmanagementlösungen, Master-Data-Management- und Datenintegrationswerkzeuge.\n\n[https://tdwi.org/articles/2023/03/23/diq-all-data-management-0323.aspx](https://tdwi.org/articles/2023/03/23/diq-all-data-management-0323.aspx)\n\nKünstliche Intelligenz (KI) hat das Potenzial, bei der Lösung des Problems der IT-Datenüberlastung zu helfen. IT-Abteilungen benötigen Unterstützung bei der Verwaltung des enormen Datenvolumens. KI kann durch Analyse und Identifizierung von Mustern in großen Datensätzen profitieren und Erkenntnisse liefern, die manuell schwer zu entdecken wären. Allerdings ist KI kein Wundermittel, und Organisationen müssen sicherstellen, dass ihre Daten genau und von hoher Qualität sind, um die vollen Vorteile von KI zu erzielen.\n\n[https://www.infoworld.com/article/3689668/can-ai-solve-it-s-eternal-data-problem.html](https://www.infoworld.com/article/3689668/can-ai-solve-it-s-eternal-data-problem.html)\n\nEin neuer Bericht enthüllt die Bedeutung des Datenmanagements für Unternehmen. Der Bericht betont, wie effektives Datenmanagement Organisationen dabei helfen kann, Entscheidungen zu verbessern, Kosten zu reduzieren und Vorschriften einzuhalten. Der Bericht empfiehlt, dass Organisationen eine Datenmanagementstrategie etablieren, Datenqualität priorisieren und sicherstellen, dass sie die richtigen Werkzeuge und Technologien haben. Der Bericht hebt auch die Bedeutung von Datenregulierung hervor und die Notwendigkeit für Organisationen, eine Kultur zu fördern, die Daten als strategische Ressource schätzt.\n\n[https://solutionsreview.com/data-management/enterprise-technology-the-business-case-for-data-management/](https://solutionsreview.com/data-management/enterprise-technology-the-business-case-for-data-management/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW7-de","image":"./briefs/edw-7/de/thumbnail.png","lang":"de","summary":"Zusammenfassung"},{"id":222,"type":"News Brief","title":"2023-3-26","tags":["ai","compute","cybersecurity"],"body":"\n\n\n## Künstliche Intelligenz\n\n## Allgegenwärtiges Rechnen\n\n## Intelligenter Rand\n\n## Datenmanagement\n\n## Internet-Sicherheit\n\n## Fortgeschrittene Kommunikation\n## Ubiquitous Computing\n\nGordon Moore, Mitbegründer der Intel Corporation und Erfinder des Mooreschen Gesetzes, verstarb am 24. März 2023 im Alter von 92 Jahren. Moore gründete Intel im Jahr 1968 und war von 1975 bis 1987 CEO des Unternehmens. Er ist am besten bekannt für seine Vorhersage, das Mooresche Gesetz, dass sich die Anzahl der Transistoren auf einem Mikrochip etwa alle zwei Jahre verdoppeln würde, was zu exponentiellem Wachstum der Rechenleistung führte. Diese Vorhersage hat über 50 Jahre lang gehalten und spielte eine entscheidende Rolle bei der Förderung des Fortschritts in der Technologieindustrie.\n\nDer 4004, Intels erster Mikroprozessor, wurde 1971 als erster kommerziell erhältlicher Mikroprozessor vorgestellt. Ursprünglich für Taschenrechner konzipiert, fand er schnell Anwendung in anderen Bereichen wie Ampelsteuerungen und elektronischen Registrierkassen. Der 4004 enthielt 2.300 Transistoren auf einem einzigen Chip und half dabei, Intel als wichtigen Akteur in der Halbleiterindustrie zu etablieren. Heutzutage werden Mikroprozessoren in verschiedenen Geräten eingesetzt, von Smartphones bis hin zu Supercomputern.\n\nIntel hat die Verfügbarkeit seiner neuesten Xeon Max Scalable Prozessorserie angekündigt, die auf seiner Sapphire Rapids-Architektur basiert. Die Xeon Max-Serie verfügt über 100 Milliarden 10nm SuperFin-Transistoren, KI- und Verschlüsselungsbeschleunigung sowie PCIe 5.0-Unterstützung und bietet eine verbesserte Leistung und Sicherheit für Unternehmenscomputing-Workloads. Dies entspricht 40 Millionen Mal mehr Transistoren im neuesten Prozessor als in ihrem ersten Prozessor vor über 50 Jahren.\n\n[https://www.datacenterdynamics.com/en/news/intel-announces-xeon-max-sapphire-rapids-cpus/](https://www.datacenterdynamics.com/en/news/intel-announces-xeon-max-sapphire-rapids-cpus/)\n\n## Künstliche Intelligenz.\n\nChatGPT, ein großes Sprachmodell, das von OpenAI trainiert wurde, hatte ein technisches Problem, das dazu führte, dass es für mehrere Stunden ausgefallen ist. Das Problem, das durch einen Fehler im System verursacht wurde, betraf die Verlaufs-Funktion des Modells und machte es unmöglich, auf frühere Gespräche zuzugreifen. OpenAI hat das Problem inzwischen behoben und ChatGPT ist jetzt wieder voll funktionsfähig. Der Vorfall erinnert daran, welche potenziellen Risiken mit der Nutzung von KI verbunden sind und wie wichtig robuste Systeme sind, um technische Probleme zu lösen.\n\n[https://www.independent.co.uk/tech/chatgpt-down-bug-issue-history-b2306269.html](https://www.independent.co.uk/tech/chatgpt-down-bug-issue-history-b2306269.html)\n\nBard, ein künstlicher Intelligenz-Sprachmodell von Google als Konkurrent zum GPT von OpenAI entwickelt, ist nun für die öffentliche Nutzung verfügbar. Bard nutzt einen anderen Ansatz für die Sprachmodellierung als GPT und zielt darauf ab, durch die Eingabe von Benutzeranweisungen und Einschränkungen kreativeren und vielfältigeren Text zu generieren. Die Veröffentlichung von Bard für die öffentliche Nutzung soll die Entwicklung neuer Anwendungen für die natürlichen Sprachverarbeitung beschleunigen und das Feld der KI-Sprachmodellierung vorantreiben.\n\n[https://www.bloomberg.com/news/articles/2023-03-21/google-chatgpt-rival-bard-now-open-to-public-use](https://www.bloomberg.com/news/articles/2023-03-21/google-chatgpt-rival-bard-now-open-to-public-use)\n\nEine neue Anbaumethode namens Synecoculture umfasst das Pflanzen mehrerer Pflanzenarten in hoher Dichte. Es kann jedoch ein komplexer Prozess sein, aufgrund unterschiedlicher Wachstumsgeschwindigkeiten und -saisonen. Forscher haben einen Roboter entwickelt, der das Säen, Beschneiden und Ernten von Pflanzen in dichtem Vegetationswachstum bewältigen kann, um dieses Problem zu lösen. Der kleine und flexible Körper des Roboters wird bei groß angelegter Synecoculture hilfreich sein und stellt somit einen wesentlichen Schritt zur nachhaltigen Landwirtschaft und Kohlenstoffneutralität dar.\n\n[https://www.sciencedaily.com/releases/2023/03/230320102001.htm](https://www.sciencedaily.com/releases/2023/03/230320102001.htm)\n\n## Cybersicherheit\n\nDie US Cybersecurity and Infrastructure Security Agency (CISA) hat ein neues Tool veröffentlicht, um bösartige Aktivitäten in Microsoft-Cloud-Diensten zu erkennen. Das Sparrow-Tool kann Azure- und Microsoft 365-Umgebungen nach Anzeichen von Hacking und anderen nicht autorisierten Aktivitäten durchsuchen. Sparrow verwendet Open-Source-Daten und künstliche Intelligenz-Algorithmen, um potenzielle Bedrohungen zu identifizieren und Benutzern handlungsfähige Empfehlungen zur Verhinderung weiterer Schäden zu geben. Die Veröffentlichung von Sparrow ist Teil der fortlaufenden Bemühungen von CISA, die Sicherheit cloudbasierter Systeme zu verbessern und sich gegen Cyberangriffe zu schützen.\n\n[https://www.bleepingcomputer.com/news/security/new-cisa-tool-detects-hacking-activity-in-microsoft-cloud-services/](https://www.bleepingcomputer.com/news/security/new-cisa-tool-detects-hacking-activity-in-microsoft-cloud-services/)\n\nDie Elektrofahrzeuge von Tesla wurden zweimal beim jährlichen Pwn2Own-Exploit-Wettbewerb der Zero Day Initiative gehackt. Ein Team von Forschern konnte eine Schwachstelle im Infotainment-System eines Model 3 ausnutzen, um die Scheinwerfer, Lautsprecher und andere Funktionen des Fahrzeugs zu steuern. Ein anderes Team nutzte einen Fehler im gleichen Ansatz, um Code auszuführen und Zugang zu den Daten des Autos zu erhalten. Tesla hat seitdem einen Over-the-Air-Patch veröffentlicht, um die Schwachstellen zu beheben.\n\n[https://www.securityweek.com/tesla-hacked-twice-at-pwn2own-exploit-contest/](https://www.securityweek.com/tesla-hacked-twice-at-pwn2own-exploit-contest/)\n\nIntel hat seine neue 13. Generation der Core vPro Plattform angekündigt, die hardwarebasierte Sicherheitsfunktionen bietet, um die Angriffsfläche der Plattform zu verringern. Die neue Plattform umfasst Intel Hardware Shield, das CPU-basierte Bedrohungserkennung nutzt, um einen sicheren Startvorgang zu gewährleisten, sowie Intel Control-Flow Enforcement Technology, die dazu beiträgt, Return-oriented Programming (ROP)-Angriffe zu verhindern. Die Plattform enthält auch Intels Threat Detection Technology, die maschinelles Lernen verwendet, um potenzielle Sicherheitsbedrohungen zu identifizieren. Die 13. Generation der Core vPro Plattform richtet sich an Geschäftskunden, die fortschrittliche Sicherheitsfunktionen suchen, um sich gegen zunehmend ausgefeilte Cyberangriffe zu schützen.\n\n[https://www.securityweek.com/intel-boasts-attack-surface-reduction-with-new-13th-gen-core-vpro-platform/](https://www.securityweek.com/intel-boasts-attack-surface-reduction-with-new-13th-gen-core-vpro-platform/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW8-de","image":"./briefs/edw-8/de/thumbnail.png","lang":"de","summary":"Zusammenfassung"},{"id":223,"type":"News Brief","title":"2023-4-2","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Cybersicherheit\n\nNeue MacStealer macOS Malware stiehlt iCloud Keychain Daten und Passwörter. Eine neue informationsstehlende Malware hat es auf Apples macOS-Betriebssystem abgesehen, um sensible Informationen von kompromittierten Geräten abzuzapfen. MacStealer ist das neueste Beispiel für eine Bedrohung, die Telegram als Befehls- und Kontrollplattform (C2) nutzt, um Daten zu exfiltrieren.\n\nMicrosoft veröffentlichte ein außerplanmäßiges Update zur Behebung eines Datenschutzfehlers namens aCropalypse in seinen Screenshot-Tools für Windows 10 und 11. Die Schwachstelle könnte es bösartigen Akteuren ermöglichen, bearbeitete Teile von Screenshots wiederherzustellen und möglicherweise sensible Informationen offenzulegen, die herausgeschnitten wurden.\n\nOpenAI enthüllt Redis-Bug hinter Vorfall mit der ChatGPT-Nutzerdaten-Exposition. OpenAI hat bekannt gegeben, dass ein Bug in der Redis Open-Source-Bibliothek für die Offenlegung persönlicher Informationen und Chat-Titel anderer Benutzer in dem ChatGPT-Dienst des Unternehmens verantwortlich war. Der Fehler, der am 20. März 2023 bekannt wurde, ermöglichte es bestimmten Benutzern, kurze Beschreibungen von Gesprächen anderer Benutzer aus der Chatverlauf-Seitenleiste zu lesen und veranlasste das Unternehmen zum Handeln.\n\n## Künstliche Intelligenz\n\nDas ist groß: Elon Musk, Bill Gates und andere Technologieführer fordern eine Pause im 'außer Kontrolle' geratenen Rennen um Künstliche Intelligenz. Über 1.100 bemerkenswerte Unterzeichner haben gerade einen offenen Brief unterschrieben, in dem sie alle AI-Labore auffordern, \"für mindestens 6 Monate sofort\" pausieren. Der Brief, der auch vom CEO von OpenAI unterzeichnet wurde, sagte, dass die Pause für KI-Systeme gelten sollte, die \"mächtiger als GPT-4\" sind. Es wurde auch gesagt, dass unabhängige Experten die vorgeschlagene Pause nutzen sollten, um gemeinsam eine Reihe von gemeinsamen Protokollen für KI-Tools zu entwickeln und umzusetzen, die \"jenseits eines vernünftigen Zweifels\" sicher sind.\n\n[https://techcrunch.com/2023/03/28/1100-notable-signatories-just-signed-an-open-letter-asking-all-ai-labs-to-immediately-pause-for-at-least-6-months/](https://techcrunch.com/2023/03/28/1100-notable-signatories-just-signed-an-open-letter-asking-all-ai-labs-to-immediately-pause-for-at-least-6-months/)\n\nCerebras Systems veröffentlicht sieben neue GPT-Modelle, die auf CS-2 Wafer-Scale-Systemen trainiert wurden. Zum ersten Mal hat ein Unternehmen nicht GPU-basierte KI-Systeme verwendet, um LLMs mit bis zu 13 Milliarden Parametern zu trainieren, und teilt die Modelle, Gewichte und Trainingsanleitung über die branchenübliche Apache-2.0-Lizenz. Eine Reihe von sieben GPT-Modellen mit 111M bis 13B Parametern. Normalerweise eine mehrmonatige Unternehmung, wurde diese Arbeit in wenigen Wochen abgeschlossen. Alle sieben Cerebras-GPT-Modelle sind sofort auf Hugging Face und Cerebras Model Zoo auf GitHub verfügbar.\n\n[https://www.marketwatch.com/press-release/cerebras-systems-releases-seven-new-gpt-models-trained-on-cs-2-wafer-scale-systems-2023-03-28](https://www.marketwatch.com/press-release/cerebras-systems-releases-seven-new-gpt-models-trained-on-cs-2-wafer-scale-systems-2023-03-28)\n\nGoogle kämpfte mit Wordle - es hätte besser laufen können. Nachdem der Bot korrigiert und daran erinnert wurde, nur Fünf-Buchstaben-Wörter zu raten, entschuldigte er sich und kehrte mit \"SLANTS\" zurück, bevor er mit verschiedenen Variationen des Wortes \"SLANT\" fortfuhr. ☹\n\n[https://www.techradar.com/news/i-tried-to-use-google-bard-to-help-me-with-wordle-but-it-didnt-go-well](https://www.techradar.com/news/i-tried-to-use-google-bard-to-help-me-with-wordle-but-it-didnt-go-well)\n\n## Intelligenter Rand\n\nBT hat sich mit Amazon Web Services (AWS) zusammengetan, um 5G-Edge-Computing in Manchester mit AWS Wavelength zu testen. Der Versuch wird untersuchen, wie Edge Computing die Leistung des 5G-Netzwerks von BT in drei Schlüsselbereichen verbessern kann: erweiterte Realität, immersive Spiele und industrielle Automatisierung. Ziel ist es, das Kundenerlebnis durch schnellere und zuverlässigere Dienste zu verbessern. Der Versuch ist Teil einer größeren Zusammenarbeit zwischen den beiden Unternehmen, um das Potenzial des Edge Computing im Vereinigten Königreich zu erforschen.\n\n[https://www.edgecomputing-news.com/2023/03/29/bt-uses-aws-wavelength-for-5g-edge-trial-in-manchester/](https://www.edgecomputing-news.com/2023/03/29/bt-uses-aws-wavelength-for-5g-edge-trial-in-manchester/)\n\nComcast hat erweiterte Partnerschaften angekündigt, die darauf abzielen, die Konnektivität in intelligenten Gebäuden und Umgebungen zu verbessern. Die Partnerschaften umfassen Vereinbarungen mit dem Anbieter fortgeschrittener Messinfrastruktur, Sensus, und BuildingIQ, einem Anbieter von Energiemanagementlösungen auf Basis künstlicher Intelligenz. Ziel ist es, eine intelligente Energie- und Wassermanagement sowie verbesserte Gebäudeautomatisierung bereitzustellen. Diese Partnerschaften sind Teil von Comcasts Bemühungen, die Konnektivität für Geschäftskunden durch sein Netzwerk von Wi-Fi-Hotspots, Sensoren und anderen IoT-Geräten zu verbessern.\n\n[https://www.edgeir.com/comcast-focuses-connectivity-on-smarter-buildings-environments-with-expanded-partnerships-20230331](https://www.edgeir.com/comcast-focuses-connectivity-on-smarter-buildings-environments-with-expanded-partnerships-20230331)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW9-de","image":"./briefs/edw-9/de/thumbnail.png","lang":"de","summary":"Zusammenfassung"}]