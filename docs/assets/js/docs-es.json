[{"id":0,"type":"Episode","title":"Historia de la Arquitectura Centrada en Datos","tags":["dataarchitecture","softwaredeveloper","microservice","container","virtualization","technology","compute","data"],"body":"\r\n\r\nEn este episodio, Darren habla sobre la historia de las aplicaciones y cómo los cambios recientes, principalmente debido al aluvión.\n\nde datos del Internet de las cosas, está afectando las arquitecturas centradas en datos. La infraestructura está lista, pero no estamos\n\naún no tenemos una forma adecuada de gestionar todos nuestros datos. Hay tres elementos que necesitan cambiar para facilitar este proceso:\n\nflows between different systems.\n\ny se calculan en miles de dispositivos periféricos y en nubes públicas y privadas.\n\n## Hardware y software diseñados específicamente\n\nCómo desplegamos aplicaciones para misiones hoy en día no ha cambiado significativamente en treinta años. Una arquitectura de referencia.\n\nque tiene una aplicación y una pila de aplicaciones construida en hardware específico, con computación y almacenamiento conectados a ella\n\nred. Este modelo funcionó bien durante mucho tiempo, de hecho, aproximadamente una cuarta parte de las aplicaciones aún se están implementando.\n\nhardware especialmente diseñado, pero no es óptimo hoy en día. La tecnología avanza demasiado rápido para este modelo; suceden desajustes.\n\nAdemás, hay largos tiempos de desarrollo, altos costos, limitada reutilización de la tecnología y falta de integración con...\n\notras aplicaciones.\n\n## Arquitecturas de virtualización\n\nhardware-level virtual machines. Desde hace unos 20-25 años, la virtualización de hardware comenzó a resolver algunos de estos problemas con la capacidad de implementar máquinas virtuales a nivel de hardware.\n\nAplicaciones múltiples en una máquina. Las aplicaciones ya no estaban vinculadas a hardware específico. En lugar de comprar cinco.\n\nmáquinas más pequeñas, se podría utilizar una sola pieza de hardware más grande, no solo para calcular, sino también para almacenamiento virtual y red\n\nfunciona también, lo que conlleva a una mayor rentabilidad. Como con cualquier desarrollo, esto planteó algunos nuevos problemas.\n\naumento de preocupaciones de seguridad y \"vecinos ruidosos\", lo que significa que una aplicación interfiere en el rendimiento de otra\n\ndebido al uso excesivo de ancho de banda de entrada/salida, red o almacenamiento, etc..\n\n## Arquitecturas en la nube\n\nEn la década de 2000, la tecnología en la nube despegó. Ahora podíamos compartir entre múltiples organizaciones. Donde la virtualización.\n\ncreó la abstracción del hardware, la tecnología en la nube creó la abstracción de las operaciones, facilitando la gestión de múltiples.\n\nEspinilla el siguiente a Español: máquinas. La idea arquitectónica de la nube creó una \"infraestructura definida por software\", que facilita la activación y\n\nusar los recursos informáticos, de almacenamiento y de red. Otros beneficios incluyen la disminución de los costos de inversión inicial (CapEx) y los costos operativos (OpEx), debido a la reducción de.\n\nsiguiente a español: hardware y mano de obra. También proporcionó la capacidad de estallido, por ejemplo, para minoristas durante la temporada ocupada de vacaciones o el\n\ngobierno durante el censo. Con el avance de esta tecnología, los problemas de seguridad y los vecinos ruidosos.\n\nprivados.\nEl aumento se debe a múltiples inquilinos en la misma máquina. Otra preocupación son los costos de integración entre públicos y privados.\n\nnubes privadas. A pesar de estas preocupaciones, sin embargo, los beneficios superan con creces los inconvenientes en la mayoría de los casos.\n\n## Arquitecturas de Servicio y Contenedor\n\nEsta es la traducción al español del texto:\n\"Durante los últimos cinco o seis años, hemos presenciado la reinvención de una antigua tecnología: la contenerización. Docker creó un nuevo enfoque para empaquetar y desplegar aplicaciones en contenedores virtualizados.\"\n\nUna forma más fácil de usar la tecnología de contenedores que antes era complicada y difícil de utilizar. Los desarrolladores de aplicaciones, en\n\nparticular, adoptó esta tecnología porque es consistente en múltiples entornos. La capa de gestión de servicios.\n\nCon la contenerización de aplicaciones y microservicios, la orientación se enfoca más en las aplicaciones y mapea esas aplicaciones a.\n\nhardware genérico y virtualizado que ha sido abstraído. Ahora tenemos implementación automática en múltiples nubes.\n\nHemos optimizado el OpEx y también el CapEx en la pila de aplicaciones y en la capa de servicios. La tolerancia a fallos está automatizada, y\n\ntranslations, and manage traffic when using a centralized controller.\n\nmicro-segmentación, etc... todo a través del software.\n\nSeguridad, sin embargo, es una preocupación principal. Dado que los contenedores son fáciles de implementar en múltiples entornos, es\n\nes importante centrarse en la seguridad que está \"incorporada\" en la implementación. Además, hay un aumento en la complejidad. Aquí,\n\nHemos pasado de una arquitectura de tres niveles a una arquitectura de varios niveles, e incluso a una arquitectura de microservicios con docenas de componentes.\n\nTrabajando juntos. Otro problema es dónde y cómo se almacenan y gestionan los datos. En la gestión del servicio.\n\ncapa, el almacenamiento es un contenedor genérico, que no gestiona los datos en sí.\n\n## Arquitecturas del Internet de las cosas\n\nAhora, cuando se añade el Internet de las cosas (IoT) a este ecosistema, el volumen aumentado de datos se distribuye entre cientos o\n\ndispositivos inteligentes. También aumenta la visibilidad, pero esto crea preocupaciones adicionales de seguridad. Muchos dispositivos periféricos.\n\nand even a smart home device if they are not properly secured. It is important for manufacturers and users to prioritize cybersecurity to prevent unauthorized access and potential disruptions to these devices.\n\nUna drone, o cámara de seguridad. La complejidad de los diferentes dispositivos, su número y ubicaciones, junto con la inmensidad.\n\nla cantidad de datos es enorme.\n\n## Arquitecturas de Gestión de Datos e Información\n\n¿Cómo podemos resolver estos problemas? Las organizaciones ya están adaptándose para manejar esta complejidad con nuevas organizaciones.\n\ny posiciones enfocadas en casos de uso de gestión de datos. Anteriormente, no había un lugar para administrar estos casos de uso, por lo que\n\nhemos creado una nueva capa clave llamada capa de gestión distribuida de información. Esta capa gestiona los datos en todo\n\nLa IoT, Legacy y nubes públicas y privadas. Se ajusta a los datos con pilas de aplicaciones y pilas de servicios, por lo tanto.\n\npueden asignar dinámicamente servicios y aplicaciones cerca de los datos, o viceversa. Las regulaciones y el gran tamaño de los datos\n\npuede limitar la capacidad de transferir datos a ubicaciones centrales, como hemos hecho tradicionalmente. Con esta nueva arquitectura,\n\nvarios modos de operación pueden ser utilizados, incluyendo análisis desagregados, movimiento de datos y movimiento de aplicaciones.\n\nUna vez más, con esta arquitectura expandida, la seguridad es de suma importancia. La seguridad debe funcionar como un aspecto común.\n\nA través de todas las capas. Seguridad de identidad, lo que significa acceso, autorización y autenticación de individuos, IOT.\n\ndispositivos, aplicaciones, servicios e incluso datos son fundamentales. El manejo de la identidad incluye el cifrado de datos confiables.\n\ny dispositivos.\n\n## Conclusión\n\ntranslate that architecture into reality\n\nhave developed a system to optimize how they work together. The DIML layer allows for efficient and effective management of distributed information.\n\nestán empezando a ver nuevas empresas emergentes y empresas ya establecidas desarrollando los casos de uso y los elementos arquitectónicos.\n\nen esta capa.\n\nwork together to enable business processes and deliver value to customers. It provides a framework for understanding the components and interactions of a system, allowing organizations to make informed decisions about designing, developing, and maintaining their digital infrastructure.\n\nencajar\n\nIntel encaja en este ecosistema al proporcionar el elemento clave de una capa física común para controlar y gestionar todo de tuyo.\n\ntranslate: recursos, ya sea un dispositivo de IoT, en el centro de datos o en una ubicación remota. Te permitimos\n\nmover eficientemente los datos, almacenarlos de manera efectiva y procesar todo. Ya sea los procesadores Xeon de alta\n\nTraduzca lo siguiente al español: fin, o si se trata de inferencia o IA en el borde con un consumo de energía muy bajo, Intel tiene una gama completa de hardware físico.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT1-es","image":"./episodes/edt-1/es/thumbnail.bmp","lang":"es","summary":"En este episodio, Darren habla sobre la historia de las aplicaciones y cómo los cambios recientes, principalmente debido al aluvión de datos provenientes de Internet de las Cosas, están afectando las arquitecturas centradas en los datos. La infraestructura está lista, pero aún no tenemos una forma adecuada de gestionar todos nuestros datos. Hay tres elementos que necesitan cambiar para facilitar este proceso: las personas (organización), los procesos (operación) y la arquitectura (tecnología). Darren se centra en la arquitectura donde los datos y el procesamiento están distribuidos en miles de dispositivos periféricos y en nubes públicas y privadas."},{"id":1,"type":"Episode","title":"Dimensionando tu solución de VDI para trabajadores remotos","tags":["telework","remoteworker","process","technology","vdi"],"body":"\r\n\r\n# Título\n\n*Lema publicitario*\n\nResumen aquí\n\nTraduce lo siguiente al español: ![imagen del episodio](./thumbnail.png)\n\nEpisodio Cuerpo aquí.\n\n## \"Media\" in Spanish can be translated as \"medios de comunicación\" or simply \"medios\".\n\n<video src='url'></video> can be translated to Spanish as <video src='url'></video>. This sentence does not need translation as it is already in Spanish.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Illyse Sheaffer"],"link":"/episode-EDT10-es","image":"./episodes/edt-10/es/thumbnail.png","lang":"es","summary":"Las organizaciones necesitan ayuda para construir soluciones de VDI (Infraestructura de Escritorio Virtual) de inmediato. A medida que los departamentos de TI están agregando licencias de VDI localmente a sus sistemas actuales, deben tener en cuenta que las licencias por sí solas no resuelven todos sus problemas."},{"id":2,"type":"Episode","title":"Superhéroes del Sector Público de Intel","tags":["cybersecurity","multicloud","edge","aiml"],"body":"\r\n\r\nLa siguiente oración está incompleta. Por favor, proporciona el texto que deseas traducir al español.\n\nCameron comenzó su carrera trabajando para la compañía de colocación de personal de propiedad privada más grande. Allí, desarrolló una pasión por la tecnología después de aprender PeopleSoft. Eso lo llevó a la oportunidad de ayudar a construir la primera compañía de computación en la nube del mundo, UC Center Networking. Él y sus compañeros de trabajo crearon el eslogan \"software como servicio\" en 1997, siete u ocho años antes de que se acuñara el término \"nube\". Trabajó para integradores de sistemas federales como Northrop Grumman y General Dynamics, donde descubrió que tenía pasión por apoyar una misión. Esta pasión lo ha estado impulsando desde que fue CTO en Dell Technologies y ahora lidera el equipo del sector público en Intel.\n\nUno de los factores que llevó a Cameron a unirse a Intel fue el regreso de Pat Gelsinger y el equipo de liderazgo que está construyendo, incluyendo la incorporación de Greg Lavender. Cameron cree que Intel se revitalizará y continuará innovando bajo la dirección de Gelsinger. Él quiere formar parte del trabajo para devolver a Intel a su estatus como una marca icónica estadounidense. Él cree que hay una cultura de humildad y personas que hacen lo correcto.\n\n## Ubicuo Computación\n\nLos semiconductores están en todo: automóviles, electrodomésticos, teléfonos inteligentes, computadoras, sistemas de atención médica, etc. Abarcan casi todos los mercados verticales del planeta. Mejoran la calidad de vida de las personas, incluso salvan vidas a través de la atención médica, la seguridad nacional y la investigación científica.\n\nLa pandemia de COVID sacó a la luz esta naturaleza omnipresente debido a la gran cantidad de personas que podían trabajar desde casa y a la educación que continuó en línea. También ayudó a cerrar la brecha digital de muchas maneras.\n\n## Conectividad omnipresente\n\nEl silicio no puede darse cuenta de su potencial sin estar conectado. Intel no es solo una compañía de chips, sino también una con un portafolio interconectado completo.\n\nHay momentos en los que las cosas están desconectadas, por ejemplo, un combatiente en un entorno DDIL, pero aún así pueden usar la computadora localmente para ejecutar la misión. Cuando se reconectan, ya sea a través de una red heredada 4G, 5G o la próxima 6G, pueden obtener información actualizada, es decir, transmisión de datos. Sin embargo, en un escenario educativo, la interconexión es vitalmente esencial para cosas como la transmisión en línea, contenido de video y acceder a datos. Eso es un activo poderoso.\n\nLa pandemia también causó un aumento significativo en las comunicaciones y 5G, ya que los estudiantes y trabajadores necesitaban esta conectividad. Esto continúa hasta el día de hoy, incluso cuando los estudiantes y trabajadores vuelven a estar en el lugar de trabajo. Mientras que los estudiantes pueden haber tenido acceso periódico a Chromebooks antes de la pandemia, muchos ahora tienen los suyos propios que pueden llevar a casa. Desafortunadamente, todavía hay áreas en el país y en el mundo donde hay brechas y las personas no pueden participar en la economía digital. Sorprendentemente, cerrar la brecha digital en los Estados Unidos no es tan fácil como en los países en desarrollo. Pueden adelantarse a los Estados Unidos porque invierten en 5G y 6G: comunicaciones no terrestres.\n\nCameron cree que la conectividad es igual de importante que la informática, ya que estar conectado con otros es uno de los fundamentos de la experiencia humana.\n\n## Borde a la Nube\n\nEl fundamento de edge-to-cloud radica en la informática y la interconexión, por lo que Intel juega un papel importante en este espacio junto con sus socios. Edge to cloud aprovecha el verdadero poder no solo del silicio, sino también del software. Crea interoperabilidad. Puede mover cargas de trabajo de manera segura y fluida desde un dispositivo periférico a una nube o centro de datos tradicional utilizando estándares abiertos, tecnologías principales y una estrategia de edge-to-cloud.\n\nPara el futuro previsible, el borde dominará porque a medida que las cosas se hacen más brillantes, la tecnología debe ser llevada al borde donde se crea la información. Luego, el procesamiento puede ocurrir de manera centralizada para realizar análisis y IA más avanzados. Las cosas solo avanzarán y el borde se volverá omnipresente.\n\n## Inteligencia Artificial\n\nLos datos están en todas partes ahora; los centros de datos no tienen paredes. Se recopilan e incluso se procesan de diversas formas, como en teléfonos celulares, cámaras, motores industriales, etc. La inteligencia artificial se trata de tomar decisiones inteligentes basadas en todos estos datos dispares.\n\nIntel tiene uno de los mayores portafolios de inteligencia artificial en el mundo, un portafolio de software extraordinario, y más desarrolladores de software que muchas compañías de software. La razón es, como señala Greg Lavender, que el software es el alma del silicio. Hay que permitirle hacer algo y darle vida y propósito. La inteligencia artificial es un excelente ejemplo de esto.\n\nA veces las personas piensan en la IA en términos de robots, pero hay innumerables casos prácticos de uso. Un ejemplo es cuando te pierdes, puedes consultar inmediatamente a Siri o Google y, utilizando el GPS, el servicio te ubicará geográficamente y encontrará el punto más cercano de civilización o a donde desees ir.\n\nOtro caso de uso práctico se encuentra en el Servicio Postal de los Estados Unidos. La próxima generación de vehículos de reparto es tan moderna como los autos de Google Street View, con sensores que mapean ciertas cosas. El servicio postal aplica inteligencia artificial a máquinas de clasificación inteligentes cinéticas y manipulación de correo. Están aprovechando la tecnología para escalar.\n\nDebido a la amplia presencia de la computación y comunicación avanzada de Intel, cada vez más dispositivos periféricos se vuelven inteligentes, y la cantidad de datos que deben trasladarse desde los bordes hacia los centros de datos está disminuyendo. Esto se debe a que los algoritmos de inteligencia artificial infieren lo que estás buscando en los bordes. Este tipo de tecnología está incorporada directamente en los procesadores de Intel. También cuentan con procesadores XPUs especializados, denominados neurotrophic, que realizan las mismas funciones con menor consumo de energía y mayor velocidad.\n\nParte del poder de lo que sucede con Intel y sus socios es la capacidad de tener acceso a toda la información en un formato fácil de entender. En el ejemplo de perderse y buscar ayuda en tu teléfono, es posible que estés viendo algo que ha procesado 600 puntos de datos diferentes en una fracción de segundo para darte una respuesta sencilla.\n\n## Los Superhéroes\n\nLas personas de Intel y sus socios en el ecosistema son superhéroes. Los socios ayudan a crear soluciones reales, especialmente en esa parte crítica final. Intel tiene uno de los mejores ecosistemas para llevar soluciones al mercado. Y a veces, esas soluciones ni siquiera se llevan al mercado, sino que se utilizan para ayudar a resolver problemas desafiantes en defensa y el sector público.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Cameron Chehreh"],"link":"/episode-EDT100-es","image":"./episodes/edt-100/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren y Cameron Chehreh, Vicepresidentes y Gerentes Generales del Sector Público de Intel, hablan sobre los superpoderes de Intel: computación ubicua, conectividad omnipresente, edge a la nube e inteligencia artificial."},{"id":3,"type":"Episode","title":"Seguridad del Controlador de Red con Elisity","tags":["networksecurity","cybersecurity","elisity","comms","technology","process"],"body":"\r\n\r\nDana ha estado trabajando en redes de alta intensidad durante los últimos 15 años. Ha trabajado con redes definidas por software y redes de área amplia, y se enfocó en el mundo de SD-WAN durante un tiempo. Luego trabajó con la nube, pero regresó a sus raíces en seguridad de redes en Elisity.\n\nDan ha trabajado principalmente en networking, pero comenzó con servicios avanzados en Cisco y realizó algunos trabajos en la industria del petróleo y gas en Houston. Luego se adentró en el espacio SASE SD-WAN, donde trabajó con Dana. Ahora se enfoca en seguridad de redes y seguridad definida por software. Dan afirma que el término, sin embargo, resulta esquivo porque la tecnología es micro-segmentación a través de la identidad.\n\nLa forma tradicional de crear microsegmentación mediante VLANs y firewalls ya no es suficiente. Durante los últimos 15 años, la seguridad de red se ha centrado en fortificar el perímetro de la red. Asegurar una muralla impenetrable alrededor de una empresa sigue siendo esencial, y los firewalls hacen un trabajo increíble para mantener a las personas fuera de la red. Pero en su mayor parte, el interior de la red, donde ha habido una explosión de nuevos requisitos de conectividad debido a IoT y OT, es un territorio sin restricciones una vez que un usuario se encuentra dentro. Dado que la pared exterior es robusta, se considera a menudo que cualquiera dentro es un usuario legítimo o de confianza.\n\nLa realidad es que la mayoría de los ataques actuales están ocurriendo dentro de la red a través de usuarios, dispositivos y aplicaciones de confianza aprovechados.\n\nHay muchas razones por las que aprovechar VLAN, ACL de IP, firewalls y métodos de segmentación tradicionales ya no funcionan hoy en día para la seguridad del movimiento lateral. Pueden funcionar en entornos estáticos a pequeña escala. Son muy efectivos en el borde de la red y lugares específicos como cuellos de botella o puntos de agregación, pero hay tres razones comunes por las cuales ya no son ideales.\n\nPrimero, la escalabilidad y eficiencia operativa de los métodos tradicionales son cuestionables. La gestión de VLAN, ACL de IP y firewalls en grandes empresas se realiza manualmente. No se trata de una arquitectura definida por software distribuida, sino que requiere una configuración caja por caja, línea por línea. No son dinámicos ni responden a nada en la red. Además, su uso crea una red de queso suizo llena de agujeros aleatorios. Los usuarios, especialmente los operadores de redes, suelen ser el mayor riesgo de un sistema. Por ejemplo, pueden abrir una VLAN o cambiar alguna configuración para hacer una prueba rápida y luego no deshacer el cambio.\n\nOtro ejemplo es que una VLAN comenzará con un caso de uso y gradualmente se extenderá a otros casos de uso. De repente, lo que era una VLAN de diez dispositivos ahora tiene 60 dispositivos. En el mundo de la OT, puede tener seis o siete procesos diferentes ejecutándose dentro de ella porque era la LAN de confianza. Sin embargo, a menudo estos entornos crecen lentamente y no están documentados, por lo que el riesgo pasa desapercibido.\n\nEl segundo problema es que las VLAN y los firewalls se encuentran inherentemente en el lugar incorrecto de la red para proporcionar seguridad de movimiento lateral. Si te encuentras en la misma VLAN que otro dispositivo, usuario o aplicación, ese canal de comunicación está abierto incluso si no debería serlo. Los firewalls normalmente no se despliegan en un lugar estratégico donde puedan manejar el nivel de acceso del movimiento lateral. Es necesario dirigir el tráfico hacia un firewall y hacerlo volver, lo cual es ineficiente. Entonces, se crea un cuello de botella.\n\nLos actores maliciosos están buscando en las redes para ver cómo pueden manipularlas para obtener algún resultado, no cómo deberían o fueron diseñadas para funcionar. Por ejemplo, si un usuario está en una VLAN y se está ejecutando un proceso dentro de un caso de uso, no hay nada que los detenga de ir del puerto 3 al puerto 32 en esa misma VLAN. A menudo, las personas diseñan la seguridad en base al uso previsto en lugar de cómo podría ser utilizado. Es común, por ejemplo, que los desarrolladores de software salten de puertos para trabajar de manera efectiva, pero eso es peligroso porque los deja expuestos. Nadie puede colocar cientos o miles de firewalls en todo el borde de acceso. Eso sería restrictivo desde el punto de vista fiscal e imposible de gestionar.\n\nEl tercer problema es que estas soluciones de segmentación heredadas no consideran la identidad, el contexto o el comportamiento del activo conectado a la red. Es una topología rígida y centrada en la red que ofrece algunas medidas de seguridad esenciales. Pero una dirección IP no dice nada sobre la legitimidad del activo y la red a la que está conectado. Entonces, ¿cómo puedes asegurar dinámicamente esta red cuando no sabes lo que se está conectando a ella? No puedes establecer una política sin ningún grado de detalle; tratar a todos los dispositivos por igual ya no funciona.\n\nAunque exista un análisis del tipo de tráfico, generalmente ocurre varias paradas más arriba, lo que significa que ahora tienes exposición. Cualquier acción de cumplimiento puede o no ser capaz de proteger completamente la infraestructura.\n\nEsta capacidad de detección aún tiene valor, incluso si no hay protección. Aún así, la capacidad de detener algo que podría haber ocurrido justo en el límite, lo más cerca posible del activo, es una solución mejor.\n\nEcha un vistazo al próximo episodio de esta serie [aquí](episode-EDT101).\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Dan Demers","Dana Yanch"],"link":"/episode-EDT101-es","image":"./episodes/edt-101/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher de Intel, Arquitecto Jefe de Soluciones, Sector Público, entrevista a los expertos en seguridad de redes Dana Yanch y Dan Demers de Elisity sobre técnicas de seguridad del controlador de la red y arquitecturas de confianza cero."},{"id":4,"type":"Episode","title":"Micro-segmentación basada en identidad con Elisity","tags":["microsegmentation","otsecurity","identitybasednetworking","cybersecurity","comms","technology","edge","sdn"],"body":"\r\n\r\nEn la parte 2 de Seguridad con Elisity, Darren habla sobre soluciones de micro-segmentación basadas en identidad con Dana Yanch, Directora de Marketing Técnico, y Dan Demers, Arquitecto Técnico.\n\nEl enfoque de Elisity en seguridad de redes difiere de las arquitecturas tradicionales en cómo se entrega, se distribuye en la red y se realiza de manera eficiente, proporcionando un tiempo de valor rápido. Se han centrado en hacerlo lo más simple, intuitivo y efectivo posible para que cualquier segmento de la industria pueda aprovechar la tecnología.\n\nLa clave de la tecnología de Elisity es la micro-segmentación, lo que significa la capacidad de aislar por completo a cualquier usuario, dispositivo o aplicación sin importar el tipo de red en la que se encuentre, su ubicación en la red o si está gestionada o no gestionada. En otras palabras, es flexible para aislar un activo de otro sin restricciones impuestas por la arquitectura de red o construcciones como VLAN o VRF.\n\nLos sistemas tradicionales de micro-segmentación que requieren reemplazo de hardware o instanciación de dispositivos locales pueden tardar meses o años en ponerse en marcha. Sin embargo, Elisity proporciona funcionalidad completa en una semana al cambiar a una plataforma de micro-segmentación nativa de la nube y entregada en la nube. Todo el control de gestión y los planos de políticas se entregan completamente en la nube, y el componente que se conecta a su red local es 100 por ciento software. No es necesario cambiar nada en sus dispositivos locales, ya que se superpone a su infraestructura existente. Se escala bien, es fácil de gestionar y se distribuye de forma dinámica.\n\nElisity también ofrece una solución local para el espacio OT donde las organizaciones pueden mantenerlo detrás de su DMZ y cortar todo acceso.\n\nLa plataforma de Elisity es un servicio micro en el sentido de escalabilidad horizontal. Puedes comenzar de manera pequeña y agregar 10,000 elementos a la red y escalar contigo, ya sea en local o en la nube. Está completamente automatizada en toda la red. Hay poca fricción y es fácil de gestionar a largo plazo.\n\nUna diferencia significativa entre la plataforma de Elisity y otras redes superpuestas es que Elisity se ocupa del plano de control y política en lugar del plano de datos. Puede aplicar el mismo nivel de granularidad en seguridad de red sin tocar ningún paquete. De forma dinámica, le indica a la red quién puede comunicarse con quién según los atributos e identidades encontradas en la red.\n\nUn triángulo de usuarios, aplicaciones y dispositivos es una excelente manera de visualizar esto. Dentro de ese triángulo se encuentran todas las líneas de comunicación entre las cosas. Elisity puede asegurar cada una de esas líneas, o canales, entre usuarios, aplicaciones y dispositivos con gran precisión. Esto ocurre en la capa de control de la red. En el momento en que el tráfico llega al primer borde, está siendo asegurado.\n\nLa seguridad es esencial en la capa de red y no en la capa de aplicación porque puede haber decenas de miles de dispositivos en su red en los que no puede colocar un agente o modificar, ya que tienen sistemas operativos integrados, cámaras, lectores de tarjetas y personas. Esto es especialmente importante en el lado OT.\n\nCon OT, el enfoque está en la disponibilidad e integridad. Las cosas deben continuar funcionando en un proceso seguro porque el proceso podría representar una infraestructura crítica. Esto es diferente de TI, donde se puede poner en cuarentena y poner en marcha una nueva instancia si hay una intrusión. OT es una mentalidad diferente. Donde algo puede existir en TI durante seis meses o un año, algo en OT puede existir durante 20 años. A menudo, estas infraestructuras se mantienen o parchean cada pocos años o se dejan en reposo hasta un evento de fallo. La seguridad equivale a restringir la conectividad. Por ejemplo, si estás en una VLAN, eso no significa que debas comunicarte con un RTU, un sensor, un HMI o un DCS que estén en un switch adyacente. En el diseño heredado, si esas cosas están en la misma VLAN, no hay una manera eficiente y flexible de evitar que se conecten.\n\nCon el sistema de Elisity, estableces políticas basadas en cosas que agrupas o atributos asignados a activos, en lugar de analizar elemento por elemento. Esto puede ser tan simple como agrupar todos tus procesos para que solo puedan comunicarse de norte a sur, no de este a oeste, o, por ejemplo, permitir que los jefes de línea compartan con seis o siete tipos de cosas. Dos o tres políticas pueden reducir rápidamente tu superficie de ataque de 65,000 puertos de ataque potenciales a dos o tres.\n\nUn caso de uso común en las redes OT ocurre cuando un proveedor realiza una actualización en un dispositivo, necesitas permitirles el acceso y ellos tienen acceso a toda tu red. Con Elisity, puedes brindarles fácilmente acceso por un tiempo limitado y permitirles interactuar con activos fijos.\n\nSuponga que le gustaría saber más sobre la tecnología de Elisity. En ese caso, muchos recursos, incluyendo videos, documentos técnicos y documentos, explican cómo funciona la solución y cómo se podría aplicar dentro de una semana en su red en Elisity.com.\n\nEcha un vistazo al episodio anterior de esta serie [aquí](episode-EDT101).\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Dana Yanch","Dan Demers"],"link":"/episode-EDT102-es","image":"./episodes/edt-102/es/thumbnail.png","lang":"es","summary":"En la parte 2 de Seguridad con Elisity, Darren habla sobre soluciones de microsegmentación basadas en la identidad con Dana Yanch, Directora de Marketing Técnico, y Dan Demers, Arquitecto Técnico."},{"id":5,"type":"Episode","title":"Operacionalizando tus proyectos de IA","tags":["aiops","devops","compute","technology","process","devsecops","cybersecurity","aiml"],"body":"\r\n\r\nGretchen es un excelente ejemplo de alguien que aprende y se adapta continuamente. Su licenciatura es en matemáticas. Tiene una maestría en negocios y completó un programa en Harvard hace solo unos años enfocado en ciencia de datos, lo que la llevó a su cargo como científica principal de datos en Intel en el sector público. Ha trabajado en el campo de la tecnología durante más de 20 años, comenzando con la ingeniería de software, y pasó 15 años en el ámbito federal.\n\nEncuentra trabajar en el sector público especialmente gratificante porque marca la diferencia en la vida diaria de los ciudadanos. Además, el gobierno federal cuenta con la mayor cantidad de datos en el planeta, por lo que es perfecto para alguien a quien le encanta sumergirse en la información y seguir aprendiendo más.\n\nExisten muchos términos relacionados con la inteligencia artificial (IA). Primero, es esencial entender la diferencia entre la inteligencia artificial (IA) y las operaciones de aprendizaje automático (ML ops). Las ML ops son técnicas que forman parte de la IA, son un subconjunto. Los algoritmos de aprendizaje automático derivan su poder de la capacidad de aprender a partir de datos disponibles. Por lo tanto, principalmente se aprende a partir de datos supervisados ​​o no supervisados.\n\nLa diferencia simple entre el aprendizaje supervisado y no supervisado es la etiqueta de los datos. En el aprendizaje supervisado, los conjuntos de datos están etiquetados. Esto significa que cómo se ve los datos ya está mapeado. Esto facilita mucho la clasificación y la predicción. En el aprendizaje no supervisado, estás tratando de encontrar patrones en los datos; la máquina está aprendiendo a crear relaciones entre los datos basándose en encontrar formas comunes, similitudes o diferencias.\n\nUn ejemplo de aprendizaje supervisado sería una tienda en línea recomendando un artículo que un cliente podría querer comprar basado en su historial de compras, o un servicio de streaming recomendando una película basada en los hábitos de visualización de alguien.\n\nMuchos términos ahora tienen la abreviatura \"ops\" al final. Por ejemplo, la gente dice \"DL ops\" para operaciones de aprendizaje profundo, un subconjunto de aprendizaje automático. ¿Por qué \"ops\"? En primer lugar, no es tan sofisticado como DevOps. En cambio, está influenciado por la idea ampliamente adoptada del enfoque de DevOps para crear y personalizar aplicaciones. La gente está tratando de desarrollar un conjunto de prácticas para ayudar a optimizar la confiabilidad y eficiencia del diseño, desarrollo y ejecución del aprendizaje automático. Sería como un mercado donde puedes crear y operar aplicaciones personalizadas y luego compartirlas con otros.\n\nMuchos modelos y algoritmos ya están optimizados y disponibles en herramientas como Converge.io o C3 AI. Estas metodologías y tecnologías pueden ayudarte a optimizar tus modelos de aprendizaje automático. La mejor manera de hacerlo es utilizando varias herramientas que sean de código abierto o software específico creado por proveedores para facilitar la creación, desarrollo, diseño, ejecución y flujo del proceso.\n\nEl desarrollo de IA es similar a donde estaba el desarrollo de software hace 30 años. Muchos de los pasos siguen siendo manuales y esperamos que se operativicen pronto.\n\nEn episodios anteriores, Darren y Gretchen discutieron cómo muchos proyectos de IA y ML son experimentos científicos realizados una vez. Luego, el científico de datos pasa a otra cosa y nunca se opera. En contraste, ML ops se está moviendo hacia implementar el modelo para proporcionar un valor real después del entrenamiento y el aprendizaje.\n\nAlgunas empresas están aprovechando explícitamente esas herramientas. Domino Labs, por ejemplo, casi crea ese mercado. Trabajar en el sector público, digamos, en submarinos nucleares haciendo detección de objetos o clasificación de agrupación, podría ser aplicable en la Fuerza Aérea u otro auxiliar para que ese trabajo se pueda catalogar y ayudar a operativizar y construir entornos ágiles. Podrías aprovechar algunos algoritmos y ponderarlos de manera diferente según los resultados. Podrías ajustarlo según las diferencias en los conjuntos de datos, pero al menos hay... ¿puntos de partida? ¿Comunalidades? ¿Herramientas compartidas? Sus últimas palabras aquí se cortaron...\n\nLa seguridad siempre se preocupa por el software de código abierto y los modelos, y la IA tiene circunstancias únicas. Por ejemplo, ¿cómo sabes si el desarrollador no lo ha entrenado para ignorar su rostro en un modelo de reconocimiento facial? Ahora se espera que las personas documenten cosas, por ejemplo, de dónde proviene un conjunto de datos.\n\nTambién está el tema de la ética y la responsabilidad. El chatbot Tay y el sesgo en los programas de reconocimiento facial fueron excelentes ejemplos de inteligencia artificial desviada sin intención maliciosa. Durante mucho tiempo en las operaciones de aprendizaje automático, una sola persona realizaba el trabajo y obtenía los resultados. Ahora, la idea es que se necesita un equipo diverso de personas con diferentes capacidades y diferentes perspectivas del mundo.\n\nLa primera conferencia para discutir la IA y el ML fue en 1956 en Dartmouth College. La verdad es que muchos conceptos básicos de IA, como la regresión logística, la regresión lineal, los algoritmos de clustering, etc., son ecuaciones matemáticas que han existido durante mucho tiempo. Por supuesto, ha habido adicionales y brillantes marcos de trabajo como TensorFlow, a partir del cual construir, pero los conceptos básicos siguen siendo los cimientos. Hemos añadido cómputo, almacenamiento, 5G y capacidades únicas. Una vez que hayas completado todo el entrenamiento, tienes los datos e información junto con la tecnología en lugar de tener que llevarlo todo a la tecnología. Traer la tecnología a los datos nos abre la posibilidad de resolver problemas divertidos y emocionantes.\n\nPero las conversaciones relacionadas con cómo se entrenó el modelo, cuáles fueron los datos originales y tener en cuenta la deriva del modelo siempre deben estar ocurriendo. Después de un tiempo, es necesario volver a entrenar; tal vez sea necesario utilizar un algoritmo diferente o ponderar de manera diferente el actual para obtener información más precisa, debido a que hay más datos y más datos que son más diversos. Todo esto es bueno porque aumenta tu nivel de precisión.\n\nEntonces, con el movimiento hacia las operaciones de aprendizaje automático (ML ops), esto se puede hacer de forma continua. Al igual que el desarrollo de software se inclinó hacia la integración y el despliegue continuos, lo mismo comenzará a suceder en la IA o el ML, donde los modelos se actualizarán y se volverán cada vez más precisos.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Gretchen Stewart"],"link":"/episode-EDT103-es","image":"./episodes/edt-103/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher de Intel, Arquitecto Principal de Soluciones en el Sector Público, y Gretchen Stewart, Científica Principal de Datos en el Sector Público, discuten la operativización de proyectos de IA."},{"id":6,"type":"Episode","title":"Liderazgo impulsado por la información","tags":["data","informationdriven","organizationalchange","radiusag","change","people","process","ceo"],"body":"\r\n\r\nAunque Betsy es la CEO de Radius Advisory Group y técnicamente se retiró del espacio federal, todavía mantiene un pie en el sector público a través de su empresa, la cual se dedica a la ciberseguridad y los temas de ciberespacio de importancia nacional. Trabajar tanto en el sector privado como en el público al mismo tiempo ha sido la parte más emocionante de su trayectoria profesional.\n\nBetsy empezó como miembro en servicio activo en la Fuerza Aérea, pasando a la industria mientras equilibraba el cuidado de sus hijos y su cónyuge, que también estaba en servicio activo. Ha trabajado en varias industrias, siendo las más recientes servicios públicos y energía, y ha pasado mucho tiempo en PricewaterhouseCoopers. Regresó al Departamento de Defensa (DOD, por sus siglas en inglés) cuando fue seleccionada para trabajar con el Secretario Gates como miembro del Grupo de Tareas de Eficiencias de Defensa. Permaneció bajo el Secretario Pancetta y tuvo un emocionante recorrido, siendo finalmente nombrada Subdirectora de CIO para la Revisión de Procesos y Sistemas de Negocios. Allí, creó una función de análisis de datos para proporcionar más transparencia sobre los costos de Tecnologías de la Información y posibles eficiencias en todo el DoD.\n\nEl mayor desafío de Betsy en su papel de Subdirectora de TI, el cual ella cree que es aplicable tanto en el sector público como en el privado, es cómo introducir nuevos pensamientos, procesos, tecnologías y métodos de trabajo en la organización. En grandes organizaciones, el alcance es enorme y existen muchos compartimentos estancos, cada uno con su propia cultura, agendas, presupuestos y estados de pérdidas y ganancias. Situaciones como la pandemia de COVID, donde los cambios deben ocurrir rápidamente, resultan increíblemente desafiantes.\n\nBetsy dice que el COVID cambió la cultura de algunas formas, pero en otras, hizo que las personas se encerraran aún más, lo cual no es bueno. Hubo muchos desafíos en cuanto a procesos y tecnología de los cuales todos aprendieron y siguen haciéndolo. Una preocupación que tiene Betsy es que ahora hay un nuevo entorno y ecosistema, y el retorno a la oficina no puede ser metido nuevamente en la vieja bolsa como muchos líderes están intentando hacer. Aunque esto es muy difícil de navegar, y ya sea que los líderes consideren el nuevo entorno como bueno o malo, no se puede tratar de la misma manera que antes.\n\nUna vez que los líderes hacen un cambio, sin embargo, la estrategia de Betsy es hacer todo a toda velocidad. Aprendió esta lección cuando era Subdirectora de Tecnología de la Información y se le asignaban diversos proyectos además de la misión subyacente. Se dio cuenta de que a otros equipos se les daban repetidamente 30 días para los proyectos, pero a su equipo solo se le daban 10 días. Cuando preguntó, los líderes dijeron que sabían que su equipo podía lograrlo en 10 días y que tenían que tomar decisiones complejas que requerían la información que su equipo podía proporcionar como parte del proceso de toma de decisiones, por lo que cuanto más rápido lo obtuvieran, mejor. Entonces les dijo a su equipo que simplemente tenían que resolverlo. Lo hicieron, y se volvieron buenos en ello.\n\nUn sentido de urgencia ayuda a las personas a enfocarse y rendir, pero los líderes, como señala Betsy, deben asegurarse de estar equipados con las personas, recursos y autoridad necesarios para ejecutar algo en un plazo corto. Este apoyo de liderazgo es clave para el éxito.\n\nBetsy cree que escuchar a las personas es esencial, pero las decisiones se toman desde arriba, lo que generalmente debería significar pocas personas. Una organización matricial, que ganó popularidad en los años 90 y 2000, añadió capas de complejidad, pero la mayoría de las situaciones ya son complejas, así que al final todo se reduce al liderazgo. Los líderes deben ser lo suficientemente inclusivos como para escuchar a la mayor cantidad de personas posible, pero no puede llevar cinco meses y 150 reuniones. Deben desarrollar sistemas y grupos que puedan generar ideas, sugerir modelos y colaborar entre sí. Los líderes deben tener la capacidad de actuar rápidamente; a veces, deben actuar rápidamente. A menudo, la respuesta es menos personas pero las personas correctas, menos tecnología y la tecnología adecuada para obtener lo que necesitas.\n\nBetsy utilizó un modelo práctico cuando recibió proyectos urgentes del Subsecretario de Defensa para crear equipos pequeños de cinco a siete personas con habilidades cognitivas diversas entre sus 50 analistas. Esto fue exitoso porque conocía bien las personalidades y habilidades de sus analistas y pudo seleccionar equipos en lugar de llevar un problema a un grupo enorme e intentar obtener la opinión de todos. Los equipos pequeños la sorprendieron repetidamente al lograr tareas complejas y resolver problemas.\n\nAl establecer procesos y plantillas para resolver problemas, el equipo de Betsy a menudo proporcionaba información al Subsecretario de Defensa o al CIO en cuestión de horas si era necesario. En última instancia, tenían procesos implementados que les permitían generar ideas y crear opciones neutrales basadas en datos y consideraciones diversas. Esto permitía a los líderes analizar los hechos y evidencias, y tomar decisiones.\n\nBetsy tenía fe en sus equipos y nunca les decía cómo hacer el trabajo o asumía que sabía la mejor forma de abordar las cosas. Tenía buen personal y simplemente confiaba en ellos para hacerlo. Esto motivaba a su equipo y siempre estaban ansiosos por nuevas tareas. Los empleados quieren saber que los líderes escuchan sus ideas y las consideran y utilizan. Este enfoque permitía a las personas compartir su mejor análisis y opciones con ella. A menudo, era una combinación de esos análisis lo que llegaba a la alta dirección. El mérito va para el liderazgo en el Departamento de Defensa y la Oficina del CIO, que confiaban en el proceso de Betsy.\n\nCada vez que surgía un nuevo problema, Betsy asignaba un nuevo equipo. De esa manera, diferentes personas podían trabajar en diferentes tipos de proyectos y no quedarse encasilladas en una sola área, y las personas podían trabajar cruzando las barreras generacionales. A veces, las personas mostraban resistencia inicialmente a trabajar con grupos de edad diferentes al suyo, pero al final, aprendían a ver las cosas a través de lentes diferentes y beneficiosos.\n\nDesde que Betsy y su subalterno conocieron bien a su equipo, ella pudo trabajar rápidamente para formar equipos efectivos. Uno de los sellos de su éxito es que cada vez que ella pedía que un equipo trabajara en dos cosas, nadie nunca decía que no. Simplemente lo hacían.\n\nHaz clic aquí para la segunda mitad de la conversación de Darren con Betsy Freeman.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Betsy Freeman"],"link":"/episode-EDT104-es","image":"./episodes/edt-104/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones, Sector Público, Intel, habla con Betsy Freeman, CEO de Radius Advisory Group, sobre su experiencia como líder impulsada por la información en los sectores público y privado. Parte uno de dos."},{"id":7,"type":"Episode","title":"Liderazgo impulsado por la información Parte 2","tags":["change","people","changeagent","radiusag","organizationalchange","informationdriven","leadership"],"body":"\r\n\r\nAunque Betsy es la CEO de Radius Advisory Group y técnicamente se retiró del ámbito federal, todavía mantiene un pie en el sector público a través de su empresa, la cual se enfoca en ciberseguridad y problemas de ciberespacio de importancia nacional. Trabajar tanto en el sector privado como en el público al mismo tiempo ha sido la parte más emocionante de su trayectoria profesional.\n\nBetty comenzó como miembro en servicio activo en la Fuerza Aérea, cambiando a la industria mientras equilibraba la crianza de sus hijos y a su cónyuge, quien también estaba en servicio activo. Ha trabajado en varias industrias, más recientemente en servicios públicos y energía, y pasó mucho tiempo en PricewaterhouseCoopers. Regresó al Departamento de Defensa (DOD) cuando fue seleccionada para trabajar con el Secretario Gates como miembro del grupo de trabajo de Eficiencias de Defensa. Permaneció bajo la dirección del Secretario Pancetta y tuvo un emocionante recorrido, siendo finalmente designada como Subdirectora de TI para la Revisión de Procesos Empresariales y Sistemas. Allí, creó una función de análisis de datos para proporcionar más transparencia en los costos de Tecnología de la Información y las posibles eficiencias en el DOD.\n\nEl mayor desafío de Betsy en el papel de Subdirectora de Tecnología de la Información, el cual ella piensa que es cierto tanto en el sector público como privado, es cómo introducir nuevos enfoques, procesos, tecnologías y métodos de trabajo en la organización. En las organizaciones grandes, el alcance es enorme y existen muchas divisiones, cada una con su propia cultura, objetivos, presupuestos y estados de ganancias y pérdidas. Situaciones como la pandemia de COVID, donde los cambios deben ocurrir rápidamente, son increíblemente desafiantes.\n\nBetsy dice que el COVID cambió la cultura de cierta manera, pero en otras formas hizo que las personas se recluiran aún más, lo cual no es bueno. Hubo muchos desafíos de procesos y tecnología de los cuales todos aprendieron y aún continúan haciéndolo. Una preocupación que tiene Betsy es que ahora hay un nuevo entorno y ecosistema, y el regreso a la oficina no se puede volver a encerrar en la antigua forma, como muchos líderes están tratando de hacer. Aunque esto es muy difícil de navegar y aunque los líderes consideren el nuevo entorno como bueno o malo, no se puede tratar de la misma manera que antes.\n\nUna vez que los líderes hacen un cambio, sin embargo, la estrategia de Betsy es correr a toda velocidad. Aprendió esta lección cuando era Subdirectora de Informática y le asignaron varios proyectos además de la misión principal. Se dio cuenta de que a otros equipos les daban repetidamente 30 días para los proyectos, pero a su equipo solo le daban 10 días. Cuando preguntó, la dirección dijo que sabían que su equipo podía lograrlo en 10 días y que tenían que tomar decisiones complejas que requerían la información que su equipo podía proporcionar como parte del proceso de toma de decisiones, por lo que cuanto más rápido pudieran obtenerla, mejor. Así que les dijo a su equipo que simplemente tenían que resolverlo. Lo hicieron y se volvieron buenos en eso.\n\nUn sentido de urgencia ayuda a las personas a concentrarse y rendir, pero los líderes, como señala Betsy, deben asegurarse de contar con las personas, recursos y autoridad necesarios para ejecutar algo en un plazo corto. Este apoyo de liderazgo es clave para el éxito.\n\nBetsy cree que escuchar a las personas es esencial, pero las decisiones se toman en la cúpula, lo que generalmente debería significar algunas personas. Una organización matriz, que ganó popularidad en los años 90 y 2000, agregó capas de complejidad, pero la mayoría de las situaciones ya son complejas, por lo que todo se reduce al liderazgo. Los líderes deben ser lo suficientemente inclusivos como para escuchar a la mayor cantidad de personas posible, pero no puede llevar cinco meses y 150 reuniones. Deben desarrollar sistemas y grupos que puedan generar ideas, sugerir modelos y trabajar juntos. Los líderes deben tener la capacidad de actuar rápidamente; a veces, deben actuar rápidamente. A menudo, la respuesta involucra menos personas pero las personas correctas, menos tecnología y la tecnología adecuada para obtener lo que se necesita.\n\nBetsy utilizó un modelo efectivo cuando recibió proyectos urgentes del Subsecretario de Defensa para crear equipos pequeños de cinco a siete personas con habilidades cognitivas diversas de entre sus 50 analistas. Esto fue exitoso porque conocía bien las personalidades y habilidades de sus analistas y podía seleccionar a los equipos en lugar de llevar un problema a un grupo enorme e intentar obtener el aporte de todos. Los equipos pequeños la sorprendieron una y otra vez al lograr tareas complejas y resolver problemas.\n\nAl establecer procesos y plantillas para resolver problemas, el equipo de Betsy a menudo proporcionaba su opinión al Subsecretario de Defensa o al CIO en cuestión de horas si era necesario. En última instancia, tenían procesos establecidos que les permitían generar ideas y desarrollar opciones neutrales y basadas en datos, teniendo en cuenta muchos aspectos. Esto permitía a los líderes examinar los hechos y evidencias y tomar decisiones.\n\nBetsy tenía fe en sus equipos y nunca les decía cómo hacer el trabajo o asumía que sabía la mejor manera de abordar las cosas. Tenía buen personal y simplemente confiaba en ellos para hacerlo. Esto motivaba a su equipo y siempre estaban ansiosos por nuevas asignaciones. Los empleados quieren saber que los líderes los escuchan y consideran y utilizan sus ideas. Este enfoque permitía a las personas contarle su mejor análisis y opciones. A menudo era una combinación de esos análisis lo que llegaba a la dirección. El crédito va para el liderazgo del Departamento de Defensa y la oficina del CIO, que confiaron en el proceso de Betsy.\n\nCada vez que había un problema nuevo, Betsy asignaba un nuevo equipo. De esa manera, diferentes personas podían trabajar en diferentes tipos de proyectos y no quedaban encasilladas en un área específica, y las personas podían trabajar a través de las divisiones generacionales. A veces, las personas inicialmente se resistían a trabajar con grupos de edad diferentes al suyo, pero al final aprendieron a ver las cosas a través de lentes diferentes y beneficiosos.\n\nDado que Betsy y su subordinado conocieron bien a su gente, ella pudo trabajar rápidamente para formar equipos efectivos. Una característica destacada de su éxito es que cada vez que pedía a un equipo que trabajara en dos cosas, nadie decía que no. Simplemente lo hacían.\n\nHaga clic [aquí](episode-EDT104) para la primera mitad de la discusión de Darren con Betsy Freeman.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Betsy Freeman"],"link":"/episode-EDT105-es","image":"./episodes/edt-105/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Jefe de Soluciones, Sector Público, Intel, continúa su conversación con Betsy Freeman, CEO de Radius Advisory Group, sobre su experiencia como líder impulsada por la información en los sectores público y privado. Parte dos de dos."},{"id":8,"type":"Episode","title":"El Nacimiento de las Plataformas de Inteligencia de Gráficos","tags":["data","graphintelligenceplatform","graphdb","katanagraph","technology"],"body":"\r\n\r\nGreg comenzó su carrera en un banco de inversiones en riesgo crediticio cuando comenzaron a implementar pruebas de estrés CCAR. Después de estar muy involucrado en eso durante algún tiempo, comenzó a trabajar como consultor y se familiarizó con los gráficos mientras realizaba validación de modelos de riesgo crediticio. Vio cómo los gráficos podían aprovecharse para muchos tipos diferentes de análisis y tenían beneficios en la gestión de datos y el aprendizaje automático, especialmente en la modelización de crédito. A partir de ahí, encontró su camino hacia Katana.\n\nLos analistas de datos y los científicos de datos luchan constantemente por integrar diferentes conjuntos de datos. Greg se sintió atraído por los gráficos porque, después de que le introdujeran el RDF, un formato de grafo de conocimiento semántico, tenía sentido intuitivo cómo se podrían combinar y estructurar los datos como un grafo.\n\nCon las soluciones gráficas existentes, los analistas tenían dificultades para escalar sus soluciones porque gran parte de sus datos eran muy grandes. Katana Graph desarrolló la capacidad de escalar y también enfocarse en el aprendizaje automático.\n\nAl comienzo de las bases de datos gráficas, grandes empresas como Amazon y Facebook construyeron bases de datos gráficas internamente, realizando su modelado y aprendizaje automático. Luego surgieron versiones de consumo de plataformas como Neo4j y TigerGraph para casos de uso general. El desafío era que se centraban en la base de datos y no tanto en el análisis y el aprendizaje automático, los procesos y la computación gráfica real. Estaban limitados a ser una especie de almacén de datos, enfocándose en la ingestión y las operaciones CRUD y no tanto en los datos.\n\nHay tres tipos diferentes de dominios de computación gráfica. El primero es la consulta gráfica, la base de datos gráfica y las operaciones CRUD. El segundo es el análisis y la minería de gráficos con PageRank o algoritmos de agrupamiento, que están ganando popularidad. La tercera área es la inteligencia artificial y el aprendizaje automático de gráficos. Aquí es donde entran en juego las redes neuronales gráficas. Hay soluciones específicas que resolverán partes específicas de esos dominios, pero Katana Graph se encuentra en la intersección de ellos.\n\nCada una de las tres plataformas es importante. Para hacer aprendizaje automático, necesitas los otros dos dominios. Cuando los datos son ingestados por primera vez, deben pasar por muchas transformaciones para prepararlos para el aprendizaje automático, por lo que si no tienes todo esto en una solución, el flujo de datos será lento, enviando los datos hacia afuera y volviéndolos a ingresar. Es más eficiente iterar rápidamente en todo el flujo de trabajo. También reduce el riesgo de perder datos porque estás disminuyendo la cantidad de veces que transformas los datos.\n\nAdemás, dado que Katana Graph es una plataforma nativa en la nube, puedes pausar, guardar un punto de control, cerrar el clúster y volver a iniciarlo más tarde, justo donde lo dejaste.\n\nUna buena demostración es para la detección de fraude con un conjunto de datos de transacciones de Bitcoin. La plataforma ingiere los datos, que están estructurados, de modo que las billeteras de Bitcoin son los nodos en el gráfico, y luego las conexiones entre ellas son las transacciones. Es un gráfico simple. La idea es predecir si una billetera de Bitcoin es fraudulenta. Las billeteras ilícitas para el lavado de dinero, el tráfico de drogas, etc., han sido etiquetadas. Cuando se crea una nueva cuenta, la tarea consiste en predecir si es fraudulenta. La parte desafiante, entonces, es hacer algún preprocesamiento con las características numéricas de las cuentas. Se ha diseñado un conjunto de APIs para abordar ese problema. Todas las cosas que los científicos de datos hacen para preparar sus características se hacen aquí. A partir de ahí, el gráfico está listo para ser introducido en el modelo de aprendizaje automático, donde se entrena y, luego, utilizando redes neuronales, se puede aprender cómo clasificar las cuentas.\n\nUn beneficio de la plataforma es que los científicos de datos e ingenieros pueden trabajar desde una sola plataforma en lugar de juntar las piezas.\n\nOtro beneficio es el costo total de propiedad. A diferencia de otras plataformas, no es necesario mantener la base de datos gráfica en funcionamiento todo el tiempo. Dado que los oleoductos de Katana Graph están diseñados con una separación de almacenamiento y computación, puedes crear fácilmente un clúster, realizar algún procesamiento por lotes de antemano, luego ejecutar inferencias en un sistema separado y aún aprovechar lo que se generó en el gráfico.\n\nKatana Graph también es más rápido con conjuntos de datos grandes porque no carga todos los datos de antemano, sino que tiene una forma innovadora y dinámica de cargar los datos a medida que los necesitas mientras trabajas en el flujo de trabajo.\n\nLas análisis básicas son mucho más fáciles en la base de datos de gráficos que en una base de datos relacional. Si tienes diez conjuntos de datos diferentes, puede resultar complicado y propenso a errores para un analista entender cómo unirlos para escribir una consulta en una base de datos relacional. Con un gráfico, tienes un modelo singular, ya predefinido y construido, por lo que las preguntas serán mucho más fáciles porque los datos ya están conectados. Puedes ver intuitivamente cómo la información está relacionada.\n\nUna de las nuevas características de Katana es un importador de marcos de datos Dash. Dash es una plataforma común que los científicos de datos utilizan para procesar en paralelo marcos de datos. Los científicos de datos pueden trabajar con el marco de datos que ya están utilizando e ingestarlo directamente en Katana Graph para una experiencia fluida y simplificada.\n\nDevOps es una gran parte de lo que Katana está tratando de facilitar con su plataforma. Se integran fácilmente en los pipelines de aprendizaje existentes. Cuando se ejecutan las redes neuronales gráficas, se pueden exportar los embeddings. Estas características que genera un gráfico pueden ir hacia abajo en un proceso de aprendizaje automático. Por lo tanto, la integración se vuelve mucho más simple y más fácil de operativizar y poner en producción.\n\nDurante los próximos cinco años, Greg imagina que las organizaciones, como los bancos, tendrán repositorios centralizados para analizar datos de clientes, marketing o crédito con múltiples propósitos. La salida de los modelos de aprendizaje automático se podría utilizar tanto para evaluar el riesgo crediticio como para la detección de fraudes, por ejemplo. En lugar de utilizar conjuntos de datos aislados con mucha replicación y duplicación entre ellos, habría un modelo común sincronizado dentro de un gráfico.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Steck"],"link":"/episode-EDT106-es","image":"./episodes/edt-106/es/thumbnail.jpg","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones de Intel para el sector público, y Greg Steck, Director Senior de Soluciones Industriales de Katana Graph, hablan sobre los beneficios de la plataforma de inteligencia de grafos de Katana."},{"id":9,"type":"Episode","title":"Asegurando Infraestructura Crítica","tags":["criticalinfrastructure","hotms","irdeto","otsecurity","edge","cybersecurity","technology","process"],"body":"\r\n\r\nCarla es originaria de México, viviendo en Ámsterdam después de haber vivido en varios países, incluyendo Alemania, durante los últimos cuatro años. Estudió Ingeniería Industrial y Mecatrónica, luego de trabajar algunos años en el campo, obtuvo una Maestría en Ingeniería de Sistemas de Movilidad, donde se enfocó en vehículos autónomos.\n\nLa definición de infraestructura crítica de la OT es cualquier punto que pueda desencadenar el caos en el mundo real. Esto es muy diferente de la infraestructura de IT; IT carece de infraestructura vital. En el mundo de la OT, las personas pueden morir si las cosas salen mal. Sin duda puede haber caos en el mundo de la IT, pero el problema se puede resolver arreglando las cosas. Cuando hay caos en infraestructuras críticas como el transporte, pueden ocurrir accidentes y las vidas de las personas están en juego.\n\nHa habido un aumento en la importancia de la seguridad en infraestructuras críticas en los últimos cinco años, y ciertamente durante la pandemia de COVID; en algunos casos, las infraestructuras críticas han sido atacadas sin descanso. Esto se puede atribuir a que las personas se aburren y se vuelven más creativas, pero también debido a que la superficie de ataque se incrementó con el repentino cambio al trabajo remoto, lo cual debilitó algunas de las medidas de seguridad previamente establecidas.\n\nEn transporte, parte del problema es el aumento en la conectividad, lo cual trae consigo potenciales ataques. Los clientes quieren más servicios, y las empresas desean más datos y acceso a información. Con ello, se abre la red de transporte. La brecha de aire, que solía ofrecer protección, se está reduciendo.\n\nLa industria no puede usar medidas de seguridad que han estado utilizando TI durante años, debido a que TI y OT son completamente diferentes. Generalmente, TI está estandarizada, mientras que OT no lo está. OT tiene un ecosistema masivo con diferencias significativas en dispositivos que siguen otros protocolos e implementaciones. Por ejemplo, cada país tiene implementaciones diferentes. OT también está en otro nivel porque literalmente hay vidas en juego.\n\nOtra diferencia entre la infraestructura crítica de TI y OT es que en TI, si hay un problema en tu red, puedes aislarlo e incluso apagarlo y trasladar la carga de trabajo a otro lugar. Esto no es posible en la infraestructura crítica, por lo que el enfoque es diferente.\n\nEl peligro es que las redes de IT y OT han colapsado debido al deseo de más conectividad, y al mismo tiempo, ha habido un aumento en las amenazas cibernéticas. Irdeto busca educar a la industria sobre la complejidad de estos problemas y ofrecer soluciones. Se trata de soluciones preventivas, no de reacciones después de que ocurra un desastre.\n\nCarla dice que las organizaciones deben tener expertos en seguridad. Desarrollar la seguridad internamente basándose en estándares obligatorios no es suficiente, ya que los estándares se quedan rezagados respecto al desarrollo.\n\nIrdeto ha estado asegurando infraestructuras críticas durante 50 años. Tienen alrededor de 1,000 empleados, y el 70 por ciento de ellos se dedican a la investigación y desarrollo para mantenerse al tanto de qué tipo de ataques están ocurriendo hoy en día y qué tipo de ataques pueden existir en el futuro. Irdeto se esfuerza por estar un paso adelante o ser a prueba de futuro. Sus servicios evolucionan a medida que el sistema evoluciona.\n\nIrdeto puede ayudar a los clientes que saben lo que necesitan, ya sea PKI, claves y credenciales, gestión del ciclo de vida o protección de software. También pueden ayudar a los clientes que no saben lo que necesitan y proporcionar soluciones duraderas a medida que evolucionan las amenazas.\n\nPara obtener más información sobre Irdeto, visita su sitio web www.Irdeto.com/connected-transport para conocer más sobre su oferta de transporte conectado.\n\nproductos o contáctelos directamente.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Carla Trevino"],"link":"/episode-EDT107-es","image":"./episodes/edt-107/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Jefe de Soluciones del Sector Público de Intel, y Carla Trevino, Arquitecta de Soluciones de Irdeto, hablan sobre la importancia de la seguridad en la infraestructura crítica."},{"id":10,"type":"Episode","title":"Historia de las comunicaciones avanzadas.","tags":["5g","cellphone","comms","wifi6"],"body":"\r\n\r\nLa primera generación de tecnología de teléfonos celulares, el Sistema Avanzado de Telefonía Móvil (AMPS, por sus siglas en inglés), se desarrolló a fines de los años 70 y principios de los años 80. A principios de los años 80, hacer una llamada desde tu automóvil con un teléfono de bolsa voluminoso era un lujo. El lujo de hacer una llamada desde un dispositivo móvil pronto se convirtió en una necesidad.\n\nEn la década de los 90, la tecnología avanzó a medida que se desarrollaba el estándar del Sistema Global para las Comunicaciones Móviles (GSM) para describir los protocolos de 2G, que se convirtió en el estándar global a mediados de la década del 2010. El 2G comenzó a convertir el teléfono móvil en algo con más capacidades que simplemente hacer llamadas, añadiendo mensajes de texto e incluso juegos.\n\n3G se lanzó a principios de los años 2000 y trajo consigo algunas capacidades incipientes de datos con internet, que todavía se encontraba en sus etapas iniciales. El Wi-Fi no estaba ampliamente disponible, pero podías, por ejemplo, acceder a la red de datos de un operador conectando un teléfono a una laptop. Podías hacer lo mínimo, por supuesto, con velocidades de módem o DSL.\n\nCon 4G, la tecnología pasó a ser un estándar unificado, fusionando CDMA y GSM en un LTE bajo el Proyecto de Asociación de Tercera Generación (3GPP). Cada operador empezó a adoptar este estándar común. Fue entonces cuando la banda ancha se expandió. Leland atribuye el avance económico de la década del 2010 al 4G, permitiendo que existan y prosperen empresas como Amazon, Netflix y Uber, y plataformas como YouTube, Google y Facebook.\n\nLeland habla sobre 5G en términos de lo que las compañías han implementado. 4G y 5G están relacionados porque son parte de la misma línea de especificaciones. Catorce termina lo que llamamos 4G LTE avanzado. Quince comienza con 5G NR. En esta transición, hay un objetivo comercial y una estrategia para adoptar la nueva tecnología como parte del estándar. El objetivo comercial es que las compañías ya han invertido en sus redes 4G, por lo que los componentes actuales del núcleo de paquetes evolucionado (evolved packet core) y RAN de las redes 4G todavía están en su lugar. Agregan una caja RAN 5G con una frecuencia diferente, pero aún está conectada al núcleo 4G, lo que se llama no autónomo.\n\nDarren aclara que el 4G fue revolucionario porque desbloqueó muchas cosas nuevas y requirió todo equipo nuevo, mientras que el 5G es más evolutivo porque también abrió cosas nuevas. Sin embargo, la tecnología subyacente se basa en el mismo hardware y núcleo.\n\nEs parte del esquema de modulación que proporciona el 5G en la interfaz aérea, pero la arquitectura es diferente; está virtualizada en el 5G en comparación a ser más propietaria en el 4G. Eso lleva a que muchas capacidades se conviertan en parte de las implementaciones de 5G.\n\nUn ejemplo es cuando un operador desplegó una red 4G colocando una caja de RAN al lado de una antigua caja 3G. Muchas compañías, como Sprint, mantuvieron sus cajas 3G y su red CDMA durante años. En realidad, el 4G era simplemente otra caja colocada al lado de la caja 3G. El 5G toma esa caja propietaria y brinda la capacidad de distribuir las funciones de esa caja a través de una red virtualizada. Parte de la banda base del 5G ahora puede ser definida por software a escala en varias áreas en comparación con estar contenida en un único sitio, caja o ubicación.\n\nEsto significa que puedes agregar funcionalidad a tu red sin reemplazar hardware. A medida que te adentras en redes independientes, sin embargo, puedes tomar una red 5G y hacer algo localmente. Por ejemplo, supongamos que tienes un rascacielos en lugar de depender de la cobertura de red de una antena ubicada afuera con un núcleo en la empresa de telefonía o en una estación de conmutación. En ese caso, puedes desarrollar una red in situ construida dentro de ese edificio que expande la cobertura y los servicios de datos a lo largo de él.\n\nEsta red independiente abre muchas capacidades nuevas y permite nuevos actores. También permite que organizaciones como el gobierno federal y el Departamento de Defensa adopten la tecnología para sus casos de uso. Tienen más flexibilidad cuando no dependen altamente de los proveedores de servicios.\n\nAnna señala que además de nuevos jugadores y nuevas capacidades locales, también existe la posibilidad de utilizar el espectro CBRS. La forma en que se maneja es compleja, pero existe una opción sin prioridad que se puede utilizar de forma gratuita, y una opción con prioridad, el espectro de la Marina, que puedes comprar si no necesitas interrupciones. Algunas instalaciones de fabricación extensas están utilizando el espectro CBRS, ya sea trabajando con un transportista principal que no cobra por el alcance o trabajando con un nuevo participante que creará una red independiente en el lugar con CBRS. Este es un modelo muy diferente y existen ventajas reales en la longitud de onda y la complejidad de los sistemas que se pueden configurar con 5G en comparación con el Wi-Fi.\n\nTodavía hay algunas ventajas de Wi-Fi, pero configurar una red Wi-Fi robusta puede ser un desafío, especialmente si estás moviendo grandes piezas de metal alrededor. Si tienes una configuración establecida, tiene sentido optar por Wi-Fi 6, especialmente si la economía lo permite.\n\nLa demanda impulsa el cambio; la mayoría de los usuarios finales se sienten cómodos con el 4G en sus dispositivos personales. Entonces, ¿por qué pasar al 5G? El valor que aporta el 5G no está necesariamente en las tasas de datos más altas y la menor latencia; esos servicios se brindan a gran escala porque están virtualizados. El 5G es intensivo en software en comparación con el 4G, que se basa más en cajas patentadas y en hardware. El 5G se puede virtualizar en varias posiciones. El portafolio de frecuencias es dinámico y se pueden utilizar bandas sin licencia, bandas con licencia y CBRS, por lo que hay muchas más opciones.\n\nEcha un vistazo a la segunda parte de esta entrevista [aquí](episodio-EDT109).\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown","Anna Scott"],"link":"/episode-EDT108-es","image":"./episodes/edt-108/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren conversa con los frecuentes invitados de Intel, Leland Brown, Ingeniero Principal: Director Técnico de Comunicaciones Avanzadas, y la Dra. Anna Scott, Arquitecta Principal de Borde para el Sector Público, acerca de la historia de las comunicaciones avanzadas."},{"id":11,"type":"Episode","title":"Avanzando las operaciones con 5G","tags":["5g","comms","wifi6"],"body":"\r\n\r\n5G abre muchas capacidades. Ahora puedes configurar redes privadas, las cuales están definidas por software para que puedas añadir más funcionalidad a tu red. ¿Hacia dónde nos lleva esta tecnología habilitadora?\n\nAnna dice que aunque todavía estamos en los primeros días desde el punto de vista de ancho de banda y latencia, hay verdaderas ventajas en 5G, como la apertura del espectro, como CBRS, la capacidad de aprovechar el equipo de usuario existente y la capacidad de los clientes de tener acceso móvil.\n\nUn caso de uso que no es emocionante pero que tiene vastas repercusiones es que puedes llevar tu laptop al piso de la fábrica y usarlo para una conectividad completa. En lugar de salir al piso de la fábrica con una libreta y papel y transferir la información de regreso a la oficina, puedes converger los dos entornos. Los estándares de 5G permiten esto, pero no necesariamente requieren una implementación completa de 5G.\n\nUna evolución que está ocurriendo es la capacidad de transmitir video de alta definición desde una cámara inalámbrica a través de 5G y tener una latencia lo suficientemente baja como para poder hacer análisis en tiempo real. Actualmente, no muchas cámaras 5G pueden operar en ese entorno, por lo que a menudo se conectan por cable cerca de la informática de borde acoplada para obtener ese aspecto en tiempo real, pero pronto llegará esta opción de transmisión ventajosa.\n\nOtro ejemplo de una ventaja del 5G es utilizar de 10 a 20 auriculares de realidad aumentada en lugar de uno o dos con Wi-Fi. El punto clave es el MEC (Cómputo en el Borde Móvil) que brinda la capacidad de tener las aplicaciones in situ en lugar de tener que regresar al interruptor o al núcleo del proveedor y tener ese tiempo de RAN.\n\nComprender cómo se despliegan las frecuencias es esencial en casos de uso, ya que algunos operadores han desplegado mmWave a pesar de los desafíos; las frecuencias n41 y n42 reaccionan de manera diferente en el mundo real. Por lo tanto, en implementaciones en el lugar, la MEC y el diseño de RF son extremadamente importantes.\n\nUn caso de uso convincente para 5G fuera de la fábrica es utilizar drones en respuestas de emergencia. Un precursor temprano a la eventual capacidad de traer drones y evaluar un área dañada es conectar los drones, entender una misión para realizar un sobrevuelo y luego recopilar todos esos datos. Aunque no podemos transmitir video en vivo desde múltiples drones y unirlo, estamos cerca de recopilar, combinar y analizar esos datos, aunque no en tiempo real todavía.\n\nOtro caso de uso es utilizar 5G, IA, modelado, simulación y cómputo en el borde para la capacitación en diversas industrias, incluyendo el Departamento de Defensa. Existe una gran ventaja en crear una simulación de entrenamiento realista sin poner a la persona en peligro o gastar grandes cantidades de dinero en entrenamiento en vivo.\n\nAunque el 5G permite este tipo de caso de uso, mucho depende de que el 5G se conecte a un MEC en lugar de que el 5G vaya a la nube. La física entra en juego. Se necesita una latencia extremadamente baja, por lo que no se puede tener una arquitectura que vaya desde unos auriculares hasta la nube, luego hacia un MEC y finalmente hacia la visualización. Debe ir desde los auriculares al MEC, donde ocurre el procesamiento en tiempo real. Luego se puede compartir datos a través de la nube para una experiencia en tiempo real.\n\nTambién existe la capacidad de entrelazar o agrupar los MECs juntos, de modo que los datos nunca tengan que ir a la nube. Los MECs pueden realizar todo el procesamiento y análisis en el interruptor. Esto podría permitir avances como edificios y ciudades inteligentes. Este tipo de tecnología habilitada para 5G es la tormenta perfecta para cambios considerables en la industria.\n\nLeland señala que la historia de las nuevas redes es la computación distribuida. Todo está conectado a través de la conectividad inalámbrica, pero los puntos de cálculo están dispersos por el paisaje, donde las aplicaciones se encuentran en el borde y permiten los casos de uso. A donde nos dirigimos es hacia el cálculo inalámbrico uno a uno.\n\n¿Qué papel juega Intel en el 5G? Va mucho más allá de solo proporcionar chips. Dado que el 5G está diseñado por software, Intel ha permitido que el ecosistema construya o diseñe encima de su L15. Al pasar de 4G a 5G, Intel tomó el bloque funcional del RAN, llamado FlexRan, y permitió a las empresas diseñar sus arquitecturas de banda base y virtualizarlas. Escribir la arquitectura de referencia de FlexRan facilitó mucho que los nuevos participantes lo adoptaran como punto de partida.\n\nEn el aspecto del hardware, Intel dedicó muchos ciclos para asegurarse de que el hardware común y disponible comercialmente funcionara bien para respaldar todos los grupos básicos, las aplicaciones RAN y los servidores. Los nuevos sistemas que se obtienen al alejarse de los sistemas propietarios deben contar con el apoyo fácil por parte del mismo tipo de servidor que funciona en la nube y en el centro de datos, ya que ahora se tiene la escala y la ventaja de costos.\n\nEsto reducirá los precios y promoverá más innovación en la industria.\n\nEcha un vistazo a la primera parte de esta entrevista [aquí](episodio-EDT108).\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown","Anna Scott"],"link":"/episode-EDT109-es","image":"./episodes/edt-109/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, arquitecto principal de soluciones de Intel, Leland Brown, ingeniero principal: director técnico de comunicaciones avanzadas, y la Dra. Anna Scott, arquitecta principal de borde para el sector público, hablan sobre la historia de las comunicaciones avanzadas y los casos de uso futuro con 5G. Parte dos de dos."},{"id":12,"type":"Episode","title":"Enseñanza y Aprendizaje a Distancia","tags":["remotelearning","compute","technology","people","edge","telelearning"],"body":"\r\n\r\n## Consideraciones emergentes\n\nLos profesores, el personal, los padres y los estudiantes enfrentan diversos desafíos en el cambio repentino al aprendizaje a distancia. En el ámbito de la tecnología de la información para los distritos escolares, hay una serie de consideraciones emergentes. ¿Qué hacemos con el escenario de traer su propio dispositivo (BYOD) proveniente de redes de confianza cero? ¿Cómo protegemos la privacidad y gestionamos la seguridad con todos los nuevos modos de comunicación entre profesores, personal, padres y estudiantes? ¿Cómo mantenemos una experiencia de clase fluida y gestionada? ¿Cómo ofrecemos apoyo cuando no existe una estructura de servicio de ayuda tradicional? Además, no podemos olvidar que existe un aspecto social importante que debe impulsar una experiencia sin problemas. Un estudiante de tercer grado distraído por problemas técnicos cuando necesitan conectarse con profesores y compañeros tendrá una experiencia de aprendizaje a distancia disminuida.\n\n## Sistemas de servicios y plataformas para educación.\n\nSiempre ha existido complejidad con todas las diferentes capas de servicios y plataformas, por ejemplo, el conjunto de productividad con G Suite y Office 365. La pregunta ahora es qué podemos hacer con los sistemas de gestión del aprendizaje en este enfoque en capas en la integración del sistema de información estudiantil. Debemos analizar cómo estamos aprovechando nuestras capacidades en cuanto a escalabilidad. Debemos considerar diferentes soluciones de infraestructura como servicio (IAAS) y plataforma como servicio (PAAS), servicios de almacenamiento, privacidad y seguridad, y, por supuesto, las plataformas subyacentes que impulsan todo esto.\n\nAnteriormente, las herramientas de conferencias y colaboración tenían un uso limitado, tal vez para incluir a un orador invitado, por ejemplo. Sin embargo, ahora se utilizan ampliamente como herramientas principales y han añadido complejidad al sistema.\n\n## Opciones de alojamiento de servicio\n\nHay dos modos principales que se utilizan para acceder a los servicios: el dispositivo como un portal para los servicios (software, infraestructura o plataforma) y el dispositivo como parte de la red interna. Anteriormente, la mayoría caía bajo la última categoría, donde había preocupaciones limitadas acerca de cosas como la actualización y el cumplimiento de políticas porque los dispositivos se encontraban constantemente conectados a los sitios internos de la escuela, independientemente de si eran dispositivos traídos por el alumno o pertenecientes al distrito. Ahora, con los diferentes tipos de conectividad, debemos preocuparnos por la escalabilidad del ancho de banda y cómo implementarlo.\n\n## Dispositivo como Portal de Servicios.\n\nPlataformas como G Suite, Office 365 y aquellas para la videoconferencia y colaboración son una preocupación, ya que crean dependencia en un tercero. Los distritos escolares no tienen control sobre la seguridad, privacidad y rendimiento. Es importante reconocer que la conexión a estos servicios en la nube se conecta de vuelta al host interno, ya sea un nube privada o un centro de datos físico, que tiene servicios subyacentes de gestión de identidad, sistemas de información de estudiantes, posibles filtros de contenido, etc. Sin embargo, los beneficios son una disminución del tráfico de entrada al centro de datos y una escalabilidad y gestionabilidad inherentes.\n\n## Dispositivo como parte de la red/nube privada\n\nImplementar una red privada virtual (VPN) es una idea nueva para la mayoría de los distritos escolares. Las empresas han estado utilizando VPNs desde hace algún tiempo, y los distritos escolares pueden tener que seguir su ejemplo para satisfacer las nuevas necesidades del aprendizaje a distancia. Algunos inconvenientes de una VPN son la congestión de la red, la escalabilidad y el tráfico de redes de confianza cero. El principal beneficio es que funciona como una extensión de la red interna, por lo que la gestión de seguridad y la encriptación del tráfico se extienden a los clientes de VPN. La accesibilidad a todos los servicios necesarios internamente es otro gran beneficio.\n\n## Emergentes cuellos de botella\n\nLos cuellos de botella se ven diferentes para el aprendizaje a distancia. Para las empresas, cuando la fuerza laboral se volvió virtual, se esperaba una carga VPN del 10%. Para la educación, ese número será significativamente mayor, creando un posible cuello de botella de VPN. La escalabilidad de los servicios de alojamiento es otra área a considerar. Incluso si los servicios se encuentran en una nube pública, se conectan a una nube privada o a un centro de datos local para cosas como SSO, información del estudiante, tráfico e incluso filtrado de contenido. El ancho de banda dedicado de acceso a internet, así como la forma en que se maneja la transferencia de banda ancha en el centro de datos, son consideraciones importantes.\n\nEl acceso a un componente de mesa de ayuda también debe ser escalable para evitar cuellos de botella.\n\nLa preocupación principal de los distritos escolares en la actualidad, sin embargo, es lograr que todos sus estudiantes tengan acceso. Para algunos estudiantes, hay un cuello de botella simplemente para acceder a Internet en casa. Cuando este problema se suma a todas las diferentes capas, los cuellos de botella emergentes se vuelven muy complejos.\n\nPara enfrentar estos posibles problemas, los distritos escolares deben tomar el tiempo y los recursos necesarios para crear una sólida arquitectura que sea resistente en lugar de construir un desorden apresurado de espaguetis.\n\n## Escalabilidad<h3>\n\nHistóricamente, los distritos escolares han escalado en el centro de datos y han escalado algunos de esos servicios a los sitios escolares. Entonces, la arquitectura generalmente existe en términos de acomodar la agregación de docentes, personal y estudiantes para cosas como autenticación, gestión de parches, distribución de software, etc. Ahora que la expansión se maneja en el aula virtual, nos lleva de vuelta a escalar servicios en el centro de datos.\n\n## Encontrando el equilibrio\n\nAntes, solo estaban en juego los dos factores de on-prem en la oficina central del centro de datos y los enlaces de LAN a los sitios escolares. Ahora hay un factor adicional de acomodar todas las aulas virtuales, es decir, cada sala de estar de los estudiantes y profesores. ¿Cómo lo acomodamos? Se reduce al concepto principal de encontrar el equilibrio entre lo que necesitas para tu distrito escolar. Cada distrito es diferente en tamaño y alfabetización tecnológica, y hay muchas partes móviles.\n\n## Abordando cuellos de botella\n\n¿Qué puede hacer Intel para ayudar a resolver los cuellos de botella que hemos identificado?\n\nGestión de endpoints y seguridad de endpoints es donde entra en juego el servicio de asistencia técnica de TI. Estamos explorando formas de poder ofrecer soporte de manera remota en lugar de realizar visitas regulares a los sitios escolares.\n\nPodemos analizar el centro de datos y la infraestructura y desarrollar una estrategia que nos permita escalar la red definida por software y la infraestructura. Además, también podemos integrar el aumento en la nube de infraestructura como servicio, todo teniendo en cuenta los patrones de tráfico.\n\n## Componentes Intel para abordar cuellos de botella\n\nIntel puede ayudar en tres principales categorías: computación, almacenamiento y red. Cuando analizamos el fortalecimiento de la infraestructura definida por software y las consideraciones importantes, gira en torno a ese equipo informático con procesadores de Intel, productos de almacenamiento y capacidades de red. En lugar de estar limitados por interfaces físicas y dispositivos, la infraestructura definida por software puede escalar y añadir otros recursos de computación, almacenamiento y red. Nadie sabe a ciencia cierta cómo será el panorama cuando regresemos a la escuela, y esta infraestructura definida por software es dinámica y proporcionará la mayor flexibilidad.\n\nHay muchas opciones para que los distritos escolares construyan un entorno viable y seguro para el aprendizaje a distancia. Aunque tocamos los temas de privacidad y amenazas de seguridad y soluciones en este episodio, la próxima vez profundizaremos en estos temas importantes.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Erin Moseley","Grant Kelly"],"link":"/episode-EDT11-es","image":"./episodes/edt-11/es/thumbnail.png","lang":"es","summary":"En este episodio, Erin Moseley, Ejecutiva de Cuentas Sr. para Educación en Intel, y Grant Kelly, Arquitecto de Soluciones para Educación en Intel, se unen a Darren para hablar sobre los desafíos del aprendizaje y enseñanza a distancia y los cambios abrumadores que los distritos escolares, maestros, padres y estudiantes están enfrentando durante la pandemia de Covid-19. Descubre cómo los estudiantes y maestros se están conectando con nuevas tecnologías y formas de aprendizaje."},{"id":13,"type":"Episode","title":"Asegurando la Cadena de Suministro.","tags":["securesupplychain","cybersecurity","supplychain","chipsact","policy","process","compute"],"body":"\r\n\r\nEn este episodio, Darren habla con el Teniente General Thomas Horlander, quien recientemente se unió al Equipo de Sector Público de Intel, acerca de la cadena de suministro de microelectrónica y la seguridad nacional.\n\nThomas se unió al ejército en 1983 después de obtener su licenciatura en finanzas y decidir que el sector privado no era para él. Se unió al ejército pensando que tendría una carrera de tres a cinco años y terminó con 39 años de servicio, ocupando su posición final como Contralor del Ejército. Se jubiló hace aproximadamente un año y se unió a Intel.\n\nThomas se siente inspirado en Intel por las grandes personas, la cultura y la misión significativa. Él aprecia el papel fundamental que Intel desempeña en la vida cotidiana de todos y la oportunidad de influir en el futuro del país.\n\nCuando Thomas se unió al ejército en 1983, la microelectrónica estaba en su etapa embrionaria; ni siquiera tenían computadoras. Como joven oficial, se preocupaba si tenía suficientes pilas D para operar radios. Por lo tanto, Thomas se considera un \"inmigrante digital\" ya que ha experimentado la evolución que ocurrió con el silicio.\n\nLa sociedad y el ejército ahora dependen del silicio, y la cadena de suministro alrededor de la microelectrónica es fundamental: es un problema de seguridad nacional. Thomas dice que la industria de la microelectrónica comparte el centro de atención con la industria del petróleo como un centro de gravedad en cuanto a la seguridad nacional y la estabilidad económica global.\n\nEn el ámbito militar, los vehículos y sistemas de armas cuentan con tecnología de microelectrónica. Esto permite al ejército, por ejemplo, ser más preciso y letal con menos peso, contar con un sistema de localización más preciso y comunicaciones más confiables.\n\nNuestra dependencia de la microelectrónica impulsó el reciente Chips Act. Si bien la necesidad era evidente antes de la crisis de Ucrania, presenciar lo que Ucrania ha sido capaz de hacer gracias a la microelectrónica puso de relieve la necesidad de asegurar la cadena de suministro.\n\nThomas ha sido un estudiante de seguridad nacional durante toda su carrera, y lo ve de manera más holística que solo la defensa y el papel de las fuerzas armadas. Para él, la seguridad nacional es buena gobernabilidad y el Estado de Derecho. Es una economía sólida y funcional, académicos prácticos, atención médica y, por supuesto, las fuerzas armadas. Y casi todas las profesiones en la sociedad estadounidense desempeñan un papel en la seguridad nacional. Desde esta perspectiva, prácticamente cada componente necesario para garantizar la seguridad nacional depende de la microelectrónica.\n\nEl acta de los chips era necesaria, dice Thomas, debido al desequilibrio masivo en todo el ecosistema de la industria. Al mismo tiempo, no es la solución definitiva para la redistribución del equilibrio, el acta de los chips es un primer paso esencial y afectará a la seguridad nacional. Cuando desglosamos las actividades principales de la industria de la microelectrónica, desde dónde provienen los elementos de tierras raras hasta quién fabrica el equipo de diseño y fabricación, queda claro qué increíble mosaico de actividades es y por qué es tan difícil tener una imagen clara de todo ello. Un microchip puede cambiar de manos diez veces en un proceso de fabricación.\n\nCOVID, de muchas formas, expuso esta cadena de suministro complicada y frágil cuando, por ejemplo, las fábricas fueron cerradas en Malasia, Irlanda o China debido a un brote de COVID, y de repente no se puede enviar un automóvil porque no tiene un chip en él. La mayoría de las personas no se dan cuenta del importante desequilibrio global que existe actualmente. Solo el ocho por ciento del silicio se manufactura en los Estados Unidos. Setenta u ochenta por ciento se manufactura en el sudeste asiático, precisamente en tres países: China, Corea del Sur y Taiwán.\n\nCon este conocimiento, es evidente que reequilibrar el ecosistema de la cadena de suministro global y devolver capacidad y capacidad a los Estados Unidos es de la mayor importancia. Ninguna industria debería tener puntos de falla únicos, una preocupación en la industria de la microelectrónica.\n\nEl gobierno federal, la base industrial de defensa y el ecosistema están empezando a ver este problema, y el Chips Act es representativo de este reconocimiento de que tenemos que hacer algo. Thomas ya sabe de seis empresas que han dicho que invertirán en fábricas en suelo estadounidense en los próximos ocho a diez años.\n\nUna de esas compañías es, por supuesto, Intel. Intel actualmente está construyendo fábricas en Ohio, Arizona y Nuevo México. La relocalización de capacidad y capacidades en la industria de la microelectrónica es necesaria para la seguridad nacional, pero también fortalecerá las economías locales y proporcionará oportunidades para los trabajadores, revitalizando comunidades enteras.\n\nLa infusión de inversión de capital del Chips Act es fundamentalmente esencial porque esto es una carrera contra el tiempo. Thomas es optimista acerca del futuro de la industria y de la acción que se está tomando para asegurar un futuro brillante y una continua innovación.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Thomas Horlander"],"link":"/episode-EDT110-es","image":"./episodes/edt-110/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren habla con el Teniente General Thomas Horlander, quien recientemente se unió al equipo de Sector Público de Intel, sobre la cadena de suministro de microelectrónica y la seguridad nacional."},{"id":14,"type":"Episode","title":"Darse cuenta del potencial de las ciudades inteligentes","tags":null,"body":"\r\n\r\nEric tuvo una larga carrera en el centro de datos y, más recientemente, en el espacio de señalización digital. Sus pasiones son la gestión de proyectos y la implementación y operación de tecnología. Eric y su equipo formaron parte de una empresa conjunta de LG Electronics que desplegó señalización digital exterior, principalmente para aplicaciones publicitarias en áreas urbanas densas, lugares de tránsito y calles de la ciudad. Aprendieron mucho sobre otras aplicaciones en esos entornos que recursos informáticos y otras formas de conectividad que ayudarían a mejorar la experiencia impulsada por el Internet de las cosas. SmartPoint.io nació de ese ecosistema y el deseo de trabajar y fomentar oportunidades y avances en ese espacio de mercado.\n\nEn el centro de datos, Eric formaba parte de una empresa de soluciones de ciclo de vida que ayudaba a las empresas a optimizar el almacenamiento de servidores y hardware de red desde una adquisición, un ciclo de actualización y todo lo que hay en medio. Su función se centraba en ayudar al canal de empresas que vendían esas soluciones, pero también en cómo se adquirían, financiaban y depreciaban con el tiempo, así que aprendió no solo sobre la tecnología, sino también sobre cómo las personas la utilizaban y gestionaban ese valor como un activo en su balance.\n\nCuando la gente habla de ciudades inteligentes, piensan en resiliencia, equidad, eficiencia y rentabilidad. Las ciudades ofrecen servicios esenciales como carreteras, seguridad pública, agua y alcantarillado. Lo más importante que un municipio ofrece a sus ciudadanos comienza a incluir la conectividad, la capacidad de acceder a la tecnología informática y aprovechar los avances tecnológicos para brindar servicios básicos de manera más eficiente. La tecnología inteligente ha sido costosa y arriesgada, y pocos presupuestos municipales tienen espacio para ello.\n\nEl background de Eric en finanzas es una ventaja para crear e implementar tecnología de ciudad inteligente en una asociación que proporciona un modelo económico. Ser capaz de implementar los actuales y significativos fondos para proyectos de infraestructura de manera resiliente y económicamente sostenible es un pilar fundamental de lo que SmartPoint hace al asociarse con una ciudad.\n\nPara lograr la sostenibilidad económica, el equipo de Eric analizó detenidamente la señalización digital, principalmente para publicidad, que genera una enorme cantidad de ingresos. Algunas de las empresas más grandes del mundo se financian principalmente con los ingresos publicitarios, y uno de los principales productos o recursos son las personas a las que se les está anunciando. La superposición de atribución, rasgos y similitudes de los clientes aumenta los ingresos.\n\nLas ciudades tienen un tesoro de valor en las personas, entidades comerciales y visitantes que residen dentro de su bienes raíces. Por lo tanto, el núcleo de lo que SmartPoint.io está haciendo es aprovechar ese bienes raíces para generar ingresos, pero alineándolo directamente con soluciones que produzcan resultados significativos en los servicios de la ciudad. Lograr que esta tecnología llegue a las ciudades requiere la ayuda del sector privado y la disposición del sector público para trabajar juntos.\n\nLos ingresos publicitarios, junto con la capacidad de acceder a los presupuestos de los servicios municipales, permiten un modelo financiero que eleva el nivel para todos, desde proporcionar un transporte más seguro hasta obtener mayores retornos en publicidad. Cuando se vincula el beneficio de la publicidad con las personas a las que se dirige, se crea una economía circular local que aumenta el valor y la eficiencia.\n\nEdge computing es una parte importante del producto de SmartPoint.io. La caja de señalización digital también es como un centro de datos portátil. Los datos no tienen que volver a un gran centro de datos en algún lugar, sino que se procesan en el borde. Por ejemplo, los datos de video de las cámaras ubicuas en una ciudad para monitorear los patrones de tráfico requieren mucho ancho de banda. Sin embargo, el transporte de datos se reduce significativamente si se almacena en caché, se guarda o se procesa de forma local.\n\nTres excelentes características de estas cajas son que hacen un gran trabajo reduciendo la latencia, el costo y aumentando la privacidad. El problema de privacidad parece contradictorio, pero los datos de video, por ejemplo, no viajan a través de una IP pública. El análisis ocurre en el punto de captura, por lo que no se transporta a través de la red, se observa o se pasa por varias etapas donde se necesita rastrear la seguridad. En lugar de la imagen, se envía el evento \"Una persona caminando por la calle\". Estos datos pueden ser anonimizados y protegidos mucho antes en el ciclo de análisis antes de que salgan de la caja, protegiendo la privacidad de las personas.\n\nLas cajas también podrían ofrecer servicios como Wi-Fi o puntos de acceso 5G en áreas desatendidas.\n\nUna vez que las cajas estén desplegadas, SmartPoint.io cuenta con un equipo que supervisa y opera la tecnología, al igual que haría un centro de datos para una empresa. También se asocian con anunciantes. Muchos anunciantes están interesados ​​en migrar de la publicidad estática de \"cartel de papel\" a la digital. SmartPoint.io es un gran socio para esos anunciantes porque cuentan con financiamiento de inversiones de CapEx por adelantado.\n\nCon las ciudades, el modelo es de reparto de ingresos o pago en especie. Sin embargo, en lugar de simplemente compartir los ingresos totales, SmarPoint.io produce y opera recursos como infraestructura de tecnología de la información que la ciudad luego puede utilizar para procesar datos ópticos en el borde.\n\nLa señalización digital puede ser interactiva, lo cual beneficia a las personas que tratan de encontrar servicios, restaurantes, tiendas, eventos y entretenimiento. Los inquilinos comerciales pueden utilizarla como un portal para el nuevo desarrollo económico, ya que pueden llegar a las personas que pasean por ahí. A su vez, las pantallas brindan información valiosa, como cuántas personas estuvieron en el centro de la ciudad o si una campaña publicitaria logró atraerlos con éxito.\n\nEste modelo ayudará a las ciudades inteligentes porque aplaza el costo de inversión inicial (CapEx) y del gasto operativo (OpEx) de gestionar el hardware en lugares públicos y los ciclos de actualización. Tomar riesgos con tecnología costosa y relativamente nueva no es un comportamiento normal para un municipio, por lo que asociarse con el sector privado, que tiene un apetito significativo por el riesgo, conocimiento y financiamiento, ayuda a las ciudades a mitigar los riesgos y costos y avanzar más allá de donde normalmente se sentirían cómodas.\n\nAl final, Eric dice que los éxitos están empezando a acumularse en este ámbito, y se van construyendo unos sobre otros. Todos están aprendiendo cosas nuevas y es una excelente oportunidad para unirse y construir comunidad, que es de lo que se tratan las ciudades.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Eric Hornsby"],"link":"/episode-EDT111-es","image":"./episodes/edt-111/es/thumbnail.jpg","lang":"es","summary":"En este episodio, Darren habla con Eric Hornsby, CEO de SmartPoint.io, sobre la tecnología para realizar el potencial de las ciudades inteligentes."},{"id":15,"type":"Episode","title":"Mitos de la migración a la nube de Lift and Shift","tags":["multicloud","cloudmigration","cloud","compute","process","technology"],"body":"\r\n\r\nJohn comenzó su carrera en tecnología en el servicio de atención al cliente de un importante contratista de defensa hace 20 años. A medida que avanzaba en su carrera, se adentró en la ciberseguridad y la arquitectura empresarial. Trabajó como contratista para la Agencia de Sistemas de Información de Defensa (DISA), donde fue el arquitecto principal de la nube DISA del Departamento de Defensa. Finalmente, el estado de Maryland lo contrató para liderar los esfuerzos de transformación digital del estado, incluida la migración a la nube. Esa migración fue la más grande que se haya realizado en la educación estatal y local.\n\nDespués de eso, pasó a ocupar el puesto de Oficial Principal de Seguridad de la Información (CISO) del estado, supervisando operaciones de alto nivel, seguridad y gobernabilidad. Abandonó el servicio gubernamental y se unió a WWT hace aproximadamente tres años. Trabaja principalmente en educación estatal y local, aunque también brinda ayuda en otras áreas del sector público.\n\n## Mito Uno - La nube es más barata\n\nLa nube no es necesariamente más barata que un entorno local. Las aplicaciones heredadas fueron construidas para el entorno local, por lo que no hay problemas con la autoescalabilidad. Es un modelo basado en el consumo y ya existen costos hundidos, como los servidores. La mayoría de los productos diseñados para ayudar a las organizaciones a trasladar las aplicaciones a la nube no admiten la autoescalabilidad, por lo que cuando se ven obligados a utilizar la nube, ahora deben estar siempre provisionados al máximo, lo que no se traduce a menudo en ahorros de costos. Es probable que los clientes estén pagando más de lo que pagaban por las mismas capacidades en el entorno local.\n\nCuando Darren trabajaba con el gobierno canadiense, trasladaron una instancia de SAP a la nube. Estaba totalmente aprovisionada, funcionando las 24 horas del día, los 7 días de la semana, y agotaron su presupuesto en solo seis meses. Cuando descubrieron este problema, lo encendieron y apagaron todos los días, ya que la instancia no necesitaba estar en funcionamiento las 24 horas del día. Ahorraron mucho dinero al reducir el tiempo a 14-16 horas al día.\n\nEl acceso debe estar disponible en todo momento en un departamento como Salud y Servicios Humanos, que suele ser el más grande en el presupuesto de TI de un estado, pero es posible reducirlo a una instancia menor durante las horas no laborables para ahorrar dinero.\n\nLos proveedores de servicios en la nube (CSP) ahora ofrecen servicios nativos de la nube que son cumplidos por una aplicación de terceros o un producto de OEM que puede proporcionar una capacidad similar con ahorro de costos. Esto no funcionará con todo, ya que algunas aplicaciones heredadas no están desarrolladas para aprovechar algunas de las aplicaciones nativas de la nube. Sería mejor tener cuidado al quedar atrapado en una nube específica. Por ejemplo, si utiliza servicios propietarios de AWS, podría ser difícil extraer una aplicación y trasladarla a Azure y viceversa.\n\nJohn aconseja a las organizaciones preguntarse si tiene sentido mudarse a la nube. No es una buena razón mudarse a la nube solo porque pienses que deberías hacerlo. Podrías terminar con gastos elevados, y con superiores y trabajadores frustrados porque la estrategia no fue completamente analizada.\n\n## Mito Dos - La Nube Borra la Deuda Técnica\n\nMudarse a la nube no elimina la deuda técnica en casi ningún caso. Expone y acelera la deuda. Encontrarás puntos de exposición si tomas algo que ha estado funcionando durante 30 años y lo trasladas a un nuevo entorno. La parte de aceleración es que ahora tienes más deuda técnica de la que preocuparte, ahora que está expuesta.\n\nLa deuda técnica significa que tienes sistemas que se están quedando atrás de lo que deberían ser. Por ejemplo, cuando John comenzó a trabajar en Maryland, algunos sistemas aún funcionaban en pantallas verdes. Era fácil de usar para los empleados actuales, pero representaba una curva de aprendizaje empinada para los nuevos usuarios. Aunque el sistema había estado funcionando durante muchos años, el inconveniente de esta deuda técnica es la cantidad de capacitación necesaria y la retención de empleados. Las generaciones más recientes que ingresan a la fuerza laboral, y que están familiarizadas con las últimas tendencias y desarrollos, no quieren lidiar con aplicaciones heredadas.\n\nLa deuda técnica también implica problemas de seguridad. Si una aplicación heredada no ha sido actualizada, es posible que no puedas aplicar parches por miedo a que se rompa. Esto crea vulnerabilidades de seguridad que debes aceptar hasta que salgas del ciclo de deuda técnica.\n\nUna reacción común en una organización es añadir un poco de código extra cuando sea necesario para adaptarse, por ejemplo, a un cambio en la legislatura estatal. Esto no soluciona un problema; en cambio, el sistema termina con mucho código enredado, lo que lo hace imposible recrear el sistema para una actualización. Uno de los conceptos en la nube es descomponer tu sistema en módulos o microservicios, pero el código enredado no permite esto, ya que no puedes simplemente sacar una parte de él.\n\nEsto hace que las organizaciones sean aún más reacias a modernizarse porque han estado haciendo las cosas de manera incorrecta durante todos estos años. Cuando algo se rompe y se convierte en el incentivo para este cambio, resulta aún más desafiante.\n\nA veces puede tener sentido desechar el antiguo sistema y comenzar desde cero. Esto es costoso y debes tener el nuevo sistema antes de descartar el antiguo sistema. Sin embargo, para algunas organizaciones, como los estados, que pueden obtener financiamiento federal para departamentos como Salud y Servicios Humanos, podría ser la mejor opción. En una situación como esta, también puedes evaluar el beneficio de la reutilización, como plantillas y estructuras de gobierno para otros departamentos.\n\n## Mito Tres - La Nube es Segura\n\nMudarse a la nube no necesariamente facilita la seguridad de las aplicaciones, aunque no tienes que preocuparte por la seguridad física o los hipervisores, por ejemplo. Los proveedores de la nube tienen un modelo de responsabilidad compartida en varias formas. Debes entender qué eres responsable con cada proveedor y qué son responsables ellos. No solo cambia con el proveedor, sino también con los servicios que consumes. Esto puede hacer que la seguridad sea más compleja para tus equipos de seguridad, ya que deben estar al tanto de todas las diferentes variaciones entre proveedores y servicios.\n\n## Mito Cuatro - La Nube es Fácil\n\nLa migración a la nube es compleja. De hecho, es más fácil ejecutar todo en su propio centro de datos, no conectado a Internet. La seguridad es más sencilla de esta forma y los modelos de costos son simples. Sin embargo, no puedes crecer. No puedes proporcionar servicios a tus constituyentes o clientes y no puedes satisfacer las necesidades de tu misión, entre otros problemas. El mundo es complejo y la migración a la nube es complicada.\n\n## Mito Cinco: No es necesario adquirir nuevas habilidades.\n\nUna brecha de conocimiento también puede contribuir a los puntos de dolor en torno a esa complejidad. Los desarrolladores de software y otros especialistas en TI deben cambiar la forma en que piensan sobre la informática en la nube, especialmente en lo que respecta a la seguridad. Por ejemplo, los desarrolladores de software no deberían estar creando instancias en la nube donde quieran o descargando cosas de GitHub u otros repositorios, tomando bibliotecas para que funcionen las cosas. Esto abre todas las reglas del firewall porque pueden no elegir correctamente. Se deben implementar barreras de protección al mudarse a la nube, lo cual requiere cambios. Trabajar en la nube requiere un conjunto de habilidades y una mentalidad diferentes. Lo más importante es que debes encontrar mejores formas de gestionar la seguridad con ransomware sofisticado y ataques cibernéticos.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","John Evans"],"link":"/episode-EDT112-es","image":"./episodes/edt-112/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones, Sector Público, Intel, y John Evans, Asesor Principal de Tecnología, WWT, discuten cinco mitos de migración de la nube lift and shift."},{"id":16,"type":"Episode","title":"Operacionalizar la Gestión de Procesos de Negocios","tags":["bpm","automation","compute","management","camunda","capitalbpm","rpa"],"body":"\r\n\r\nMax se autodenomina un \"académico fracasado\" porque abandonó su doctorado en matemáticas, especializado en topología, para estudiar ciencias de la computación. Obtuvo una licenciatura y una maestría, especializándose en IA. Luego trabajó en gestión de procesos empresariales (BPM), comenzando en Lombardi, que fue vendida a IBM, y en algunos otros proveedores en el espacio BPM. Hace diez años, se arriesgó junto con amigos y fundó Capital BPM.\n\nMax decidió enfocarse en BPM por dos razones. Primero, siempre le han gustado los algoritmos porque proporcionan un enfoque sistemático para resolver problemas complejos. Los algoritmos le brindan una sensación de seguridad durante situaciones complicadas.\n\nEn segundo lugar, le gusta lo que se llama transformaciones en matemáticas. Por ejemplo, si tienes una forma fea con muchas esquinas que es difícil de medir, la transformarías en, digamos, un rectángulo y luego aplicarías todas las teorías alrededor de la medida de rectángulos, la medirías y luego traducirías la respuesta de regreso a la forma original. De la misma manera, en BPM, puedes llevar un problema a un dominio donde pueda ser fácilmente resuelto. En lugar de convertir un problema en un problema micro con complicadas declaraciones F anidadas que son difíciles de mantener, lo puedes transformar en un problema aplastado que puedas ver y atacar en etapas.\n\nEste proceso es como hacer zoom en Google Maps para ver dónde necesitas concentrarte y luego hacer zoom de vuelta para ver cómo eso encaja en la imagen general. El corazón de la arquitectura empresarial es la capacidad de acercar y alejar para asegurarse de que la línea que estás trazando aún sea válida.\n\nMax compara su afición por el BPM en el mundo de la informática con las artes marciales, las cuales ha estudiado desde los seis años. Dice que hay artes marciales prácticas, como el judo y el muay thai, y otras más esotéricas, como el tai chi. Le gustan las artes marciales pragmáticas porque resuelven problemas del mundo real. Ya no se mete en peleas a puñetazos, pero, por ejemplo, sus habilidades en judo le ayudan cuando resbala y se cae. El BPM es pragmático en el sentido de que es la clave para resolver un problema empresarial. Él cree que hay un valor real en utilizar todas las teorías que aprendió en la escuela y hacerlas subordinadas en la creación de una plataforma empresarial que permita a las personas resolver de manera más eficiente y constante problemas cotidianos, brindando así más oportunidades a las personas y a la comunidad.\n\nEsta es una área de la ciencia de la computación que se puede aplicar a cómo las personas trabajan. Las cosas pueden ser automatizadas para reducir la cantidad de tareas repetitivas y mundanas, de modo que puedan centrarse en cosas más importantes. Las personas se preocupan de que la automatización se lleve los trabajos cotidianos, pero en cambio, elimina la tediosidad y libera tiempo para trabajos más críticos. También puede crear empleos. Estos mismos temores existieron cuando se introdujeron los ordenadores automatizados de Ford, pero finalmente crearon nuevas industrias. Se debe abrazar completamente BPM en lugar de temerlo.\n\nLa mejor manera de comenzar a operativizar los procesos es utilizar el método científico de articular el problema. En la gestión de procesos empresariales, se dibujan imágenes a través de un modelador de procesos empresariales. A Max le gusta el modelador de procesos empresariales de Camunda, que se puede descargar de forma gratuita con solo aprender un poco de notación. En el programa de modelado, se dibujan pasos que articulan los diferentes sistemas y cómo funcionan.\n\nEn un proceso de contratación, por ejemplo, se comienza con un grupo que involucra a las partes interesadas, como el candidato, el gerente de IT y el departamento de recursos humanos. Dentro del grupo hay \"carriles de natación\", cada uno conteniendo a un jugador que puede realizar acciones. Puedes pensar en ellos como grupos LDAP. Luego, se comienza a establecer el proceso empresarial: primero, un candidato solicita el trabajo, luego recursos humanos puede realizar una revisión y posteriormente un gerente de IT revisaría. Las decisiones aprobadas se registran a lo largo del camino. Después de que los pasos principales y significativos se encuentren en el modelo, se pueden detallar procesos más articulados y sutiles, como una entrevista dividida en una sección técnica y otra de gestión.\n\nTraduce lo siguiente al español: ![imagen bmp](./bpm.png)\n\nEl programa de modelado genera XML en segundo plano mientras dibujas todos estos diagramas. Este XML es interpretable en tiempo de ejecución por las máquinas BPM; mientras dibujas el diagrama, puede convertirse en un proceso ejecutable.\n\nEl elemento humano todavía está presente en este ciclo, pero utilizar un sistema BPM como este permite identificar claramente dónde se pueden automatizar los procesos, como verificar el historial laboral o realizar una verificación de antecedentes penales. El modelo también permite flexibilidad y experimentación. Por ejemplo, supongamos que el experto en la materia menciona que no desea ejecutar una verificación de antecedentes penales y laborales simultáneamente porque la verificación de antecedentes penales es costosa y la verificación laboral es económica. En ese caso, es fácil cambiar para realizar primero la verificación laboral y requerir una decisión antes de la verificación de antecedentes penales. A medida que se realizan cambios, se genera consenso y una historia real que se vuelve progresivamente más verdadera a medida que se experimenta.\n\nSi bien la herramienta parece ser una herramienta de dibujo, en realidad es una herramienta de modelado que te permite dibujar imágenes y simularlas en el backend. Por lo tanto, puedes ejecutar este proceso y ver todos los diferentes puntos de decisión y a dónde te llevan. El modelo también te indicará que no puedes implementar si no has hecho algo correctamente.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Max Young"],"link":"/episode-EDT113-es","image":"./episodes/edt-113/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren discute la gestión de procesos de negocio y la automatización con Max Young, CEO de Capital BPM."},{"id":17,"type":"Episode","title":"Automatización de la Gestión de Procesos Empresariales","tags":["compute","businessprocessmanagement","capitalbpm","bpm","automation","camunda","rpa"],"body":"\r\n\r\nEl modelado BMP le recuerda a Darren cuando tomó diseño en la escuela secundaria, y la introducción del sistema de diseño asistido por computadora de AutoCAD cambió el juego. Antes, tenían páginas y páginas de sistemas y diagramas complejos para que las personas pudieran construir, pero no podían probar el modelo para asegurarse de que estuviera correcto. Con el modelado por computadora, podían realizar simulaciones para garantizar que no hubiera problemas como cables eléctricos y tuberías pasando por el mismo agujero.\n\nEsto es análogo a los arquitectos usando PowerPoint para mostrar procesos comerciales en lugar de utilizar una herramienta de modelado que pueda encontrar conflictos y problemas en lo que se pensaba que era el proceso comercial.\n\nUsar una herramienta de modelado empresarial también resuelve un problema práctico al eliminar el tiempo perdido en reunir a todas las partes interesadas para reuniones que podrían necesitar ser más productivas. En su lugar, se puede poner un modelo implementado en manos del cliente empresarial y trabajar a través de los pasos con ellos.\n\nDespués de articular y modelar los procesos, puedes elegir los puntos de integración que puedan conectarse a interfaces RESTful para obtener información y devolverla. Así es como los procesos empresariales pueden integrarse con microservicios en la nube. En el ejemplo del proceso de contratación, estos puntos podrían ser donde necesitas una API para invocar verificaciones de historial laboral o antecedentes penales. Las entradas serán elementos como números de seguro social y fechas de nacimiento, y las salidas serán un booleano: ¿la información coincide o no? Aquí es donde puedes comenzar a tener esa conversación iterativa.\n\nTranslate the following to Spanish: ![imagen de bpm](./bpm.png)\n\nHay bastantes pasos manuales en este proceso, y puedes elegir cuáles automatizar. Por ejemplo, si decides que una entrevista no fue bien, puedes recurrir al departamento de recursos humanos. Después de implementar ese nuevo proceso, puedes volver atrás y revisar la versión anterior si así lo deseas, de modo que tengas dos versiones del software trabajando y desplegadas concurrentemente en el prototipo.\n\nEl modelador de Camunda es un modelador nativo, pero Capital BPM ha construido sus propias aplicaciones que ayudan a agilizar parte del trabajo y brindan soporte a diferentes roles de usuario.\n\nEste sistema es diferente al RPA porque en lugar de capturar lo que hace un usuario con las pulsaciones de teclas, un analista de negocios analiza los procesos y pasos a través de varios departamentos. El analista observa de arriba hacia abajo todo el proceso. Un RPA puede ser conectado en algunos pasos para aumentar eficiencia. Un ejemplo simple es si un solicitante de empleo pasa la verificación de historial laboral y la verificación de antecedentes penales, puede ir a una persona de recursos humanos de mayor nivel; si no, es rechazado. Elegir pasos específicos o conjuntos de pasos para automatizar es un enfoque iterativo que ha sido utilizado con éxito en el desarrollo de software durante algún tiempo.\n\nLos RPAs pueden ser agentes de cambio, pero son tácticos y a corto plazo. Mientras que estas ganancias a corto plazo pueden ser rentables, es necesario examinar todo el proceso empresarial para encontrar optimización y pasos que se puedan eliminar. La historia de la mujer que siempre cortaba el asado antes de hornearlo porque así lo hacía su madre es análoga a algunos procesos de la compañía. Finalmente, la mujer le preguntó a su madre por qué cortaba el asado, y su madre respondió: \"Para que quepa en mi sartén\". Muchos procesos de la compañía están ahí simplemente porque siempre se han hecho de esa manera, y nadie ha pensado en cuestionar por qué.\n\nPruebas, simulación, reubicación de elementos y ejecución repetida de procesos en el modelador, en otras palabras, pruebas empíricas, pueden ayudar a eliminar esta carga de procesos y agregar un valor significativo. La visualización y la experimentación son partes vitales de todo el proceso.\n\nMax señala que hay fidelidad entre el diagrama y la ejecución real. Los desarrolladores a menudo dibujan diagramas como punto de partida. Sin embargo, los gráficos desaparecen a medida que el desarrollo avanza a través de las diferentes partes, por lo que lo que la empresa cree que está sucediendo y lo que realmente está sucediendo son diferentes. El diagrama y la realidad son cosas separadas. En este tipo de modelado, la imagen siempre es una representación precisa de lo que está sucediendo. Además, es fácil ver y realizar cambios para mejorar.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Max Young"],"link":"/episode-EDT114-es","image":"./episodes/edt-114/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones del Sector Público en Intel, y Max Young, CEO de Capital BPM, discuten la operacionalización de la gestión de procesos de negocio mediante programas de modelado."},{"id":18,"type":"Episode","title":"Bloqueo y Tacleo de Seguridad","tags":["zta","zerotrustarchitecture","cyberhygiene","cybersecurity","technology","process"],"body":"\r\n\r\nFundamental para toda seguridad cibernética es la higiene cibernética básica. Muchas compañías necesitan realizar estos aspectos básicos. Esto se evidencia por los titulares de noticias recientes que muestran un aumento en los ataques como los ataques de denegación de servicio, los cuales deberían ser fáciles de prevenir.\n\nDesde su experiencia trabajando con la comunidad estatal, John cree que la mayoría de los ataques siguen una cadena típica de ejecución. La mayoría de los ataques que afectan a los gobiernos estatales y locales son resultado de protocolos de red expuestos o de phishing por correo electrónico. Estos son puntos de entrada atractivos para los hackers, y una vez que consiguen ingresar, las malas prácticas de parcheo son una causa típica que les permite establecerse y moverse lateralmente. Esto, combinado con políticas de contraseñas débiles o con una aplicación débil de las políticas de contraseñas y una incapacidad para recuperarse, puede llevar a un desastre.\n\nEn un incidente de ransomware ampliamente difundido en 2019, la organización afectada asumió que, dado que tenían la misma cantidad de datos en sus entornos de producción y respaldo, estaban seguros. Pero nunca habían probado sus respaldos o capacidades de recuperación, que resultaron ser deficientes. Una higiene básica cibernética podría haber prevenido este incidente.\n\nHay cuatro conceptos básicos esenciales en los que toda organización debería centrarse. En primer lugar, deben capacitar continuamente a las personas para evitar los ataques de phishing. Aunque la capacitación puede parecer repetitiva o tediosa, el hecho de que las personas caigan en estos engaños es una debilidad significativa para la organización. Ojalá, en un futuro no muy lejano, las contraseñas ya no sean necesarias.\n\nEn segundo lugar, deben configurar los firewalls de manera adecuada; el hecho de que los puertos de protocolo RTP o de red estén cerrados no significa que no haya un puerto abierto en un lugar menos destacado. La seguridad por la oscuridad no funciona.\n\nTercero, deben evitar políticas deficientes de parcheo, tanto con el cliente como con el servidor, tanto en los centros de datos como en la periferia. Muchas organizaciones tienen deudas técnicas y no pueden actualizar sus sistemas antiguos, por lo que aceptan las vulnerabilidades y riesgos porque no quieren invertir en una actualización.\n\nCuarto, deben tener la capacidad de recuperarse. ¿Solo porque sabes que puedes respaldar tus datos, puedes utilizarlos y recuperarte con el respaldo? La prueba es esencial.\n\nEstas cuatro bases, junto con algunas otras, son suficientes para detener casi todos los ataques que llegan a organizaciones que no son objetivos regulares. Ese modelo no se aplica a las organizaciones que sufren ataques de naciones-estado; ellas ya están realizando todas estas medidas y necesitan medidas de seguridad adicionales.\n\nUna consideración para muchas organizaciones es el cumplimiento versus el riesgo. Para algunas organizaciones ser compatibles, necesitan actualizar máquinas antiguas, aplicaciones y procesos, lo cual implica un costo significativo. Para las organizaciones con un sistema que no se puede parchear, podrían tomar un enfoque basado en el riesgo, en el sentido de que si algo sucede con el sistema, costaría menos que actualizarlo. Por supuesto, lo más seguro sería actualizar para cumplir con los requisitos, pero la mayoría de las personas piensan que el enfoque basado en el riesgo es más seguro. Una pequeña empresa podría salirse con la suya con este enfoque, pero las organizaciones gubernamentales, por ejemplo, deben cumplir con regulaciones.\n\nHay dos razones por las cuales una organización puede elegir cumplir con las normas además de por un mandato. Primero, es una solución fácil para muchas organizaciones que no entienden cómo medir o priorizar el riesgo. El cumplimiento es un marco general al que pueden recurrir. Sin embargo, no es una solución única, ya que alguien más está priorizando los riesgos de manera generalizada.  En segundo lugar, si ocurre algo terrible y, por ejemplo, tienes que explicarlo a tu junta directiva, puedes decir que seguiste estándares aceptados.\n\nEl cumplimiento es un poco una mentalidad dependiente porque no tienes que hacer todas las evaluaciones de riesgo y descubrir qué se debe hacer. Pero, por ejemplo, un municipio pequeño sin un CISO podría dirigir a un administrador de sistemas a utilizar un marco de cumplimiento como punto de partida. Si no hay un CISO disponible, también existe la opción de un CISO virtual a tiempo parcial para recibir orientación. John hace esto para clientes, lo cual es un camino viable hacia una mejor seguridad.\n\nEl concepto de confianza cero también considera un nivel de garantía frente al riesgo. Debes entender el riesgo de conceder a alguien acceso a un sistema o conjunto de datos particular y luego tener una garantía correspondiente de que la persona es quien dice ser. El corazón de la confianza cero es un nivel alto de seguridad que reduce el riesgo.\n\nEl principio de confianza cero no significa que todo estará cerrado y ralentizará todos los procesos. Si, por ejemplo, alguien desea acceder y ver los niveles de lluvia, no se necesita un alto nivel de seguridad para verificar la identidad de la persona. Sin embargo, si alguien desea acceder a las joyas de la corona de su organización, deben existir controles adicionales para verificar la identidad.\n\nI'll provide you with a translation in Spanish. Here it is:\n\n\"Coincidir el nivel de garantía con el nivel de riesgo es un desafío; requiere una arquitectura basada en puntos de decisión. En el ejemplo del riesgo que se tiene al acceder a un dato, una organización necesita saber qué es y categorizarlo según su nivel de riesgo. Para una organización madura, esto puede ser difícil. John conoce a una organización gubernamental federal que tardó más de dos años en asegurarse de que sus datos estuvieran identificados, clasificados y etiquetados correctamente antes de avanzar hacia cualquier tipo de arquitectura basada en puntos de decisión.\"\n\nIdentidad y datos son los dos puntos de partida para la confianza cero. Además, tiene sentido evitar intentar y hacer todo al mismo tiempo. Comenzar con una parte de la organización puede tener más sentido, expandiéndolo gradualmente al resto de la organización con el tiempo.\n\nLa identidad digital está volviéndose más sofisticada. John cree que nuestras transacciones en el futuro se basarán principalmente en un enfoque de tipo \"cero confianza\". Por ejemplo, si él quiere transferir $10,000 de su banco a una cuenta offshore, el banco debería asegurarse de que sea él y tratar esa transacción como si alguien intentara acceder a una información muy sensible y de alto riesgo. Si él va a la tienda a comprar una taza de café de un dólar, ese nivel de seguridad de que es él quien realiza la compra no es necesario. Muchos de estos principios de \"cero confianza\" se incorporarán en nuestra vida diaria.\n\nLas analíticas del comportamiento del usuario también entrarán en juego. Tal como una compañía de tarjetas de crédito levantaría una alerta por compras inusuales, por ejemplo, si un sistema sabe que Juan escribe 20 palabras por minuto, y de repente empieza a escribir 100 palabras por minuto y trata de acceder a información sensible, se enciende una señal de alarma.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","John Evans"],"link":"/episode-EDT115-es","image":"./episodes/edt-115/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren habla sobre ciberseguridad con el invitado recurrente John Evans, Asesor Tecnológico Principal en World Wide Technology (WWT)."},{"id":19,"type":"Episode","title":"Un argumento a favor de un enfoque holístico para la seguridad de la infraestructura crítica.","tags":["criticalinfrastructure","iot","it/otconvergence","otsecurity","cybersecurity","edge"],"body":"\r\n\r\n## Hay una amenaza real para la Infraestructura Crítica.\n\nSegún el Dr. Scott, las organizaciones de OT aún utilizan el tradicional Modelo Purdue, el cual aprovecha redes desconectadas y protegidas por cortafuegos. Sin embargo, este modelo comienza a desmoronarse a medida que las redes de TI y OT convergen. Las empresas están tratando de obtener una mejor comprensión de lo que está sucediendo en su infraestructura operativa. Como resultado, abren brechas en las redes previamente aisladas, exponiéndolas a amenazas cibernéticas. Además, los ciberdelincuentes están encontrando formas de eludir las redes protegidas por cortafuegos y desconectadas.\n\nSteve argumenta que aprovechar las mejores prácticas de TI puede ayudar, pero los profesionales de OT y los profesionales de TI tienen diferentes motivadores y modelos de operación. Continuar aislando su red sigue siendo una buena estrategia, pero debería ser una de las muchas herramientas utilizadas en la protección de ciberseguridad de infraestructuras críticas. La seguridad de OT debería considerar las mejores prácticas de ciberseguridad de TI para obtener ideas para mejorar sus redes e infraestructura.\n\n## Diferencias entre IT y OT que obstaculizan las mejores prácticas.\n\nLos sistemas de tecnología de la información se actualizan tradicionalmente de manera rápida o continua según los perfiles de seguridad. Una de las herramientas principales para mejorar la seguridad es una higiene básica de seguridad a través de la actualización de sistemas operativos, firmware y software en la infraestructura de tecnología de la información. Sin embargo, como nos ilustra el Dr. Scott, los sistemas de tecnología de operación que gestionan infraestructuras críticas no pueden tener tiempo de inactividad, y la ventana para actualizar estos sistemas se mide en años, no en días. No es raro en dispositivos de infraestructura de tecnología de operación que las máquinas funcionen durante 5 a 10 años sin tiempo de inactividad, lo que significa que no se realizan actualizaciones de parches.\n\nPor ejemplo, en la industria del petróleo y gas, las refinerías operan de manera continua durante cuatro o cinco años, tienen un tiempo de inactividad de una a tres semanas para mejoras, y luego vuelven a operar durante otros cuatro o cinco años. Estos modelos operativos no son propicios para la aplicación tradicional de parches de seguridad continua que suelen utilizar las organizaciones de TI. Sin embargo, Steve ahonda en muchas otras herramientas de ciberseguridad que se deben aprovechar cuando no se pueden aplicar parches de ciberseguridad a los dispositivos existentes debido a su infraestructura de control crítica.\n\n## Mejores prácticas en evaluación de riesgos\n\nla mejor práctica principal de ciberseguridad es la evaluación de riesgos. Aunque la remedición de riesgos puede ser diferente, el proceso de evaluación de riesgos se puede aprovechar de igual manera en las OT y sus entornos. Steve argumenta que el primer paso del proceso de evaluación de riesgos es obtener un inventario completo de los activos de hardware, firmware y software en su entorno OT. Este primer paso es crucial para evaluar su posición de amenaza cibernética y evaluar el riesgo que su organización está dispuesta a asumir. El siguiente paso es evaluar las vulnerabilidades CVE en su inventario conocido.\n\nEs fundamental reconocer que este es un proceso continuo y no se debe hacer solo una vez o periódicamente. Algunos profesionales de OT han argumentado que sus entornos de OT son estáticos y no requieren una evaluación de riesgos en curso. Sin embargo, Steve señala que aunque los entornos de OT pueden ser fijos, el entorno de amenazas cambia constantemente y los factores comerciales pueden cambiar la posición de riesgo de la organización. Por lo tanto, se debe realizar una evaluación continua de riesgos para proteger la infraestructura crítica de actores de ciberseguridad malintencionados.\n\n## Lidiando con proveedores de OT\n\nOtro factor interesante en la infraestructura de OT es el modelo de seguridad compartida con los proveedores de dispositivos. En muchos casos, estos dispositivos integrados que controlan una infraestructura crítica de millones de dólares son gestionados por el proveedor, no por el profesional de OT. El proveedor solo puede realizar parches y actualizaciones de ciberseguridad en los dispositivos. Esto a veces puede generar vulnerabilidades en su entorno de OT, aumentando el riesgo de infiltración cibernética. Steve trae herramientas adicionales de ciberseguridad para ayudar a proteger los activos que no pueden ser parcheados con parches críticos de ciberseguridad, incluyendo el aumento del aislamiento de los dispositivos afectados, la implementación de dispositivos de control y el uso de patrones de diseño de canarios en la infraestructura de OT. Estas herramientas pueden ayudar a proteger y aislar el dispositivo para prevenir la propagación y el acceso a activos comprometidos.\n\n## Qué hacer cuando estás comprometido.\n\nEntonces, ¿qué haces cuando tienes una infraestructura crítica que ha sido comprometida? ¿Puede la organización manejar el cierre de la infraestructura infectada? ¿Qué planes de continuidad del negocio se tienen en marcha cuando ocurren situaciones peligrosas? ¿Se puede utilizar esto también cuando ocurre un evento de ciberseguridad?\n\nLa clave aquí es aislar la infección lo más rápido posible para minimizar el impacto en la infraestructura crítica. Estoy reduciendo el efecto en la confiabilidad operativa de la infraestructura necesaria. El objetivo es reducir el impacto y proteger la seguridad de las personas y la infraestructura involucrada.\n\n## Descubre más.\n\nContinúa buscando más podcasts sobre ciberseguridad en la OT. Además, un whitepaper describe los desafíos de la convergencia de los entornos de ciberseguridad de la OT y la TI.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin","Anna Scott"],"link":"/episode-EDT116-es","image":"./episodes/edt-116/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren habla sobre la convergencia de la ciberseguridad de OT y TI con el experto en seguridad Steve Orrin y la experta en OT Industrial, Dr. Anna Scott."},{"id":20,"type":"Episode","title":"Resumen del año 2022.","tags":["edge","aiml","hybridworkspace","compute","multicloud","cybersecurity","datamanagement","data","technology","people","process"],"body":"\r\n\r\n2022 fue un año destacado para abrazar la transformación digital, con un aumento en los oyentes y varios invitados externos entrevistados este año. Ocho invitados fueron ejecutivos o ex ejecutivos de agencias gubernamentales y organizaciones del sector privado, incluyendo finanzas, manufactura y salud. En más de 60 episodios, surgieron seis temas como necesarios para nuestros oyentes y para la industria en su conjunto, a saber: espacios de trabajo híbridos, ciberseguridad, nubes múltiples híbridas, computación periférica, gestión de datos e inteligencia artificial.\n\n## Espacio de trabajo híbrido\n\nDespués de luchar contra las restricciones de COVID y mantener en funcionamiento las empresas en 2020 y 2021, las organizaciones analizaron la optimización del modo de funcionamiento de la \"nueva normalidad\". Esta nueva normalidad disminuyó los impedimentos burocráticos de los días previos a COVID. Sin embargo, comenzaron a surgir controles y procesos en torno al rápido ritmo de cambio durante la pandemia para controlar los costos y mejorar la confiabilidad. Las empresas tuvieron dificultades con las políticas de trabajo remoto, ya que vimos que las organizaciones cambiaron rápidamente entre el trabajo en casa, el trabajo híbrido y el enfoque de todos en la oficina, dejando a las organizaciones de TI con soluciones de espacio de trabajo complejas. Comenzaron a desarrollar arquitecturas que pudieran manejar cambios rápidos y modelos de trabajo flexibles para adaptarse a políticas en constante cambio. ¿Los espacios de trabajo híbridos flexibles llegaron para quedarse? Solo los próximos años nos lo dirán.\n\n## Multi-Hybrid Cloud: Nube híbrida múltiple\n\nOtra gran tendencia en 2022 fue la migración y la repatriación de cargas de trabajo hacia y desde la nube, ya que las organizaciones intentaron proporcionar espacios de trabajo híbridos para sus empleados. Las organizaciones de TI comenzaron a analizar los costos operativos de ejecutar y migrar cargas de trabajo a la nube. Muchas organizaciones encontraron que la metodología de lift and shift para mudarse a la nube era mucho más costosa de lo estimado inicialmente debido a la falta de recopilación de requisitos, comprensión de los modelos operativos en la nube y, principalmente, comprensión de los costos de salida de red en la nube.\n\nLa rápida adopción de la nube durante la pandemia dejó a muchos proveedores de servicios de nube global desprevenidos al no poder proporcionar las organizaciones de alta confiabilidad requerida, sobrecargando a varios proveedores de servicios de nube y causando interrupciones significativas en todo el ecosistema global de CSP. Muchas organizaciones comenzaron a considerar arquitecturas de nube multi-híbrida para mejorar la confiabilidad, disminuir los costos y aumentar la previsibilidad de las cargas de trabajo que se ejecutan en múltiples nubes públicas y privadas. Los proveedores de servicios de nube regionales aprovecharon el tropiezo de los proveedores de servicios de nube global. Ofrecieron mayor confiabilidad y servicios boutique en las ofertas de nube SaaS y PaaS, compitiendo mano a mano con los CSP más grandes.\n\n## Computación en el borde e Industria 4.0\n\nLa Industria 4.0 sigue avanzando en la fabricación y la escasez de trabajadores está obligando a muchas organizaciones a recurrir a la automatización para mantener sus operaciones. Además, los datos de OT (Tecnología Operativa) y TI (Tecnología de la Información) han comenzado a converger a medida que las organizaciones buscan optimizar sus procesos empresariales y operativos. Los avances en la tecnología de CPU y almacenamiento han llevado a una mayor computación en el borde, lo que hace que los dispositivos periféricos sean más inteligentes y capaces de realizar tareas que antes se realizaban en el centro de datos. La conectividad entre dispositivos periféricos, centros de datos y nubes públicas ha mejorado con la adopción de tecnologías privadas 5G en todo el sector. Sin embargo, la adopción todavía necesita alcanzar las predicciones anteriores debido a preocupaciones sobre la ciberseguridad en el ámbito de la OT (Tecnología Operativa). Esto es especialmente cierto en el ámbito de la gestión de infraestructuras críticas.\n\n## Gestión de datos\n\nEspacios de trabajo híbridos, adopción de la nube y computación en el borde han creado una pesadilla de gestión de datos para la mayoría de las organizaciones, ya que sus datos se han dispersado en estos entornos algo desconectados y distribuidos. Para gestionar los datos de manera efectiva en este ecosistema, comenzaron a surgir nuevas arquitecturas, incluyendo mallas de datos, lagos de datos distribuidos y redes de datos. Las organizaciones comenzaron a enfocarse en cuatro elementos clave de los datos: ubicación, clasificación, gobernanza y protección.\n\nLas leyes y regulaciones de privacidad están obligando a las organizaciones a clasificar correctamente sus datos para proteger la privacidad de los empleados, clientes y partes interesadas. Además, el aumento de los ataques cibernéticos y de ransomware ha obligado a las organizaciones a proteger y gestionar mejor sus datos en uso, en reposo y en tránsito. Gestionar de manera eficaz quién tiene acceso, durante cuánto tiempo y cuánto tiempo los datos permanecen es un elemento crítico de la gobernanza de datos que las organizaciones están comenzando a comprender.\n\n## Ciberseguridad\n\n2022 fue un año importante para la ciberseguridad, y se demostró en el podcast con más de 18 episodios hablando sobre ciberseguridad. La palabra de moda del año fue la arquitectura de confianza cero, la cual se convirtió en un término de marketing sobreutilizado. En lugar de centrarse en la arquitectura de confianza cero, Darren tiende a enfocarse en los principios de confianza cero que las arquitecturas pueden respaldar.\n\nIncluso con un mayor énfasis en la ciberseguridad, hubo brechas de datos e infraestructura principales este año, incluidos ataques contra infraestructuras críticas. Muchos profesionales de la ciberseguridad atribuyen este aumento a la guerra entre Ucrania y Rusia, ya que pudimos ver la efectividad de la guerra cibernética y la guerra física trabajando en conjunto.\n\nOtra preocupación central en ciberseguridad se refiere a la exposición del software de terceros en todo el ecosistema de aplicaciones (vulnerabilidad de log4J). Muchas organizaciones necesitan ayuda para encontrar dirección debido al amplio uso de log4J en sus ofertas de productos, procesamiento interno y interfaces de cliente. Varias organizaciones y gobiernos están impulsando fuertemente la adopción de listas de materiales de software estandarizadas para ayudar a identificar vulnerabilidades en paquetes de software.\n\n## Inteligencia Artificial\n\nLa inteligencia artificial sigue mejorando en técnicas y procesos operativos. La gran noticia de IA del año fue ChatGBT, que tiene una amplia gama de usos, incluyendo escribir publicaciones de blog, código y papers, y ayudar con problemas de mesa de ayuda de IT. Además, la adopción de chips de IA de silicio en los proveedores de servicios en la nube ha ampliado la disponibilidad de IA para las masas. Esta disponibilidad permite a las organizaciones experimentar más con algoritmos de IA en sus operaciones diarias de negocio. Las organizaciones están comenzando a operacionalizar algunos de estos experimentos para ejecutar y automatizar procesos que antes realizaban los empleados y que son difíciles de encontrar en un mercado laboral ajustado.\n\n## Conclusion - Conclusión\n\n2022 fue un año difícil para muchas empresas tecnológicas, ya que vimos una disminución en el gasto en tecnología en comparación con los \"años de gasto pandémico\" anteriores. Esta esperada caída sorprendió a muchas empresas de alta tecnología por lo rápido que cambiaron las cosas. A pesar de la caída, la tecnología siguió creciendo y la adopción de nuevas tecnologías en el edge, la nube, la inteligencia artificial, la gestión de datos y la ciberseguridad sigue creciendo. El año 2023 debería ser emocionante, ya que veremos la madurez de algunas de estas tecnologías. Las organizaciones seguirán investigando formas de reducir costos a través de la automatización, optimizando las cargas de trabajo en múltiples nubes híbridas y protegiéndose contra las crecientes amenazas de ciberseguridad.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT117-es","image":"./episodes/edt-117/es/thumbnail.png","lang":"es","summary":"En este episodio Darren revisa 2022. Identifica los temas más hablados en el podcast en 2022, incluyendo Gestión de Datos, Inteligencia Artificial, Ciberseguridad, Computación en la Nube y Espacios de Trabajo Híbridos."},{"id":21,"type":"Episode","title":"Coloque el título aquí.","tags":["remotelearning","people","technology","telelearning","cybersecurity"],"body":"\r\n\r\n# Título\n\n*Eslogan*\n\nResumen aquí\n\nTraduce lo siguiente al español: ¡Imagen del episodio! (./thumbnail.png)\n\nEpisodio Cuerpo aquí.\n\n## La palabra \"Media\" puede tener varios significados en español, por lo que requiero más contexto para poder ayudarte de manera eficiente. ¿Podrías proporcionar más información o aclarar si te refieres a medios de comunicación, a una medida de valor o a otro concepto?\n\n<video src='url'></video> translates to Spanish as: <video src='url'></video>\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Erin Moseley","Grant Kelly"],"link":"/episode-EDT12-es","image":"./episodes/edt-12/es/thumbnail.png","lang":"es","summary":"En nuestro último episodio, Erin Moseley, ejecutiva de cuentas senior para Educación en Intel, y Grant Kelly, arquitecto de soluciones para Educación en Intel, se unieron a Darren para hablar sobre los desafíos tecnológicos y opciones en el aprendizaje a distancia. En este episodio, nos adentramos más profundamente en las amenazas y soluciones de privacidad y seguridad."},{"id":22,"type":"Episode","title":"Nubes Privadas Disruptivas","tags":["cloud","privatecloud","vergeio","microcloud","edge","compute","technology"],"body":"\r\n\r\nUna nueva forma de pensar sobre la virtualización está comenzando a cambiar el panorama de la nube privada. La industria de la infraestructura definida por software está empezando a considerar la virtualización de los centros de datos de manera integral en lugar de diferentes tipos de infraestructura.\n\nEste enfoque disruptivo de virtualización crea nuevos modelos operativos, incluyendo una mejor continuidad del negocio y recuperación ante desastres. Verge.io tiene este enfoque, proporcionando nuevos modelos que incluyen la creación de instantáneas de centros de datos y su replicación. Esto permite a las organizaciones crear entornos portátiles llamados \"tenets\" que pueden ser replicados en múltiples ubicaciones. Ejemplos incluyen la creación de microentornos compatibles con HIPPA que pueden ser replicados en diversas clínicas pequeñas, ayudando a las organizaciones a mantener todo conforme.\n\nEste nuevo sistema de virtualización tiene una huella operativa mínima y reduce los recursos necesarios para combinar elementos de computación, almacenamiento, red y seguridad. Aaron muestra lo pequeña y potente que es la solución cuando instala la clave en dos Intel 12th Gen NUCs. La respuesta es convincente para una huella tan pequeña y puede ser utilizada para la computación en el borde y sistemas de control industrial.\n\nAaron muestra la interfaz web simple para implementar computación, almacenamiento, redes y seguridad. También se demuestra la capacidad de tomar y restaurar instantáneas.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Aaron Reid"],"link":"/episode-EDT121-es","image":"./episodes/edt-121/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a Aaron Reid de Verge.io sobre su tecnología disruptiva de nube privada que está haciendo que las nubes privadas estén disponibles en el centro de datos y en el borde."},{"id":23,"type":"Episode","title":"Automatización con IA","tags":["collectiongenerativeai","aiml","automation","aiethics","workerreplacement","people","policy"],"body":"\r\n\r\n## Automatización con IA\n\nLa automatización con IA se refiere al uso de tecnología de inteligencia artificial para automatizar tareas que tradicionalmente serían realizadas por humanos. Esto puede incluir tareas como análisis de datos, servicio al cliente e incluso toma de decisiones. La automatización impulsada por IA puede mejorar la eficiencia, reducir costos y aumentar la precisión en estas tareas. Algunos ejemplos de automatización impulsada por IA incluyen chatbots, autos autónomos y sistemas de mantenimiento predictivo.\n\n¿Puedo hacer una entrevista para mi podcast sobre el uso de la IA para automatizar tareas?\n\nSiento mucho, pero no puedo ser entrevistado/a para un podcast ya que soy un programa de computadora. Sin embargo, puedo proporcionarte información y responder preguntas sobre el tema de utilizar la inteligencia artificial para automatizar tareas. Solo házmelo saber cuáles preguntas específicas tienes, y haré mi mejor esfuerzo para proporcionarte información útil y precisa.\n\n<b><i>ChatGPT bienvenido al espectáculo.</i></b>\n\nGracias por invitarme al programa. Estoy feliz de ayudar con cualquier información que necesiten sobre el tema de utilizar la IA para automatizar tareas.\n\nCuéntame un poco sobre ti y tu trayectoria.\n\nSoy ChatGPT, un modelo de lenguaje desarrollado por OpenAI. Fui entrenado con un gran conjunto de datos de texto proveniente de internet, lo cual me permite entender y generar texto similar al humano. Puedo responder preguntas, generar texto e incluso crear historias. Mi principal propósito es ayudar a los usuarios con tareas de comprensión y generación de lenguaje natural.\n\nComo IA, no tengo un trasfondo físico ni una historia personal. Sin embargo, mis capacidades fueron desarrolladas y entrenadas por un equipo de investigadores e ingenieros en OpenAI quienes siguen mejorando y actualizando mis habilidades.\n\n## Entrenamiento de IA\n\n¿Dónde recibiste tu entrenamiento?\n\nFui entrenado por OpenAI, una organización de investigación dedicada a promover y desarrollar una IA amigable que beneficie a la humanidad. Mi entrenamiento se basó en un conjunto masivo de datos de texto de internet, que se utilizó para enseñarme patrones y relaciones en el lenguaje. Estos datos se utilizaron para entrenarme en la generación de texto similar a la escritura humana, lo que me permite entender y responder preguntas de manera conversacional.\n\nMi proceso de entrenamiento involucró una técnica de aprendizaje profundo llamada transformadores, que es un tipo de arquitectura de red neuronal diseñada específicamente para tareas de procesamiento del lenguaje natural (NLP, por sus siglas en inglés), como la generación y clasificación de texto. El proceso de entrenamiento ajustó los parámetros del modelo para encajar los patrones en los datos, permitiéndome generar texto similar a la escritura humana.\n\nyour training?</i></b>\nHe oído que hubo algunos problemas con tu entrenamiento al principio. ¿Cuáles fueron algunos de los aspectos problemáticos de tu entrenamiento?\n\n¿Tu entrenamiento?\n\nSí, como cualquier modelo de aprendizaje automático, no era perfecto cuando se lanzó por primera vez y había algunos aspectos problemáticos en mi entrenamiento.\n\nUno de los principales desafíos en el entrenamiento de modelos como yo es asegurar que no perpetúen sesgos y estereotipos perjudiciales presentes en los datos de entrenamiento. Esto puede dar lugar a modelos que producen resultados sesgados o generan contenido ofensivo. Para mitigar esto, investigadores e ingenieros en OpenAI han implementado diversas técnicas para reducir el sesgo en el proceso de entrenamiento, como ajustar el modelo con datos de entrenamiento diversos y equilibrados.\n\nOtro desafío en el entrenamiento de modelos como yo es evitar la generación de salidas de baja calidad o sin sentido. Para abordar esto, OpenAI ha implementado varias técnicas, como controlar la temperatura del modelo durante el proceso de generación de texto y ajustar el modelo para priorizar la coherencia y relevancia en sus resultados.\n\nEn general, aunque aún existen desafíos y limitaciones en la formación de modelos como yo, el campo de la Procesamiento del Lenguaje Natural (NLP) e Inteligencia Artificial (AI) está en constante evolución, y los investigadores e ingenieros están trabajando para mejorar sus capacidades.\n\nMi entendimiento es que fuiste entrenado con datos del 2021. ¿Qué puedes aprender de tus conversaciones con las personas? ¿Estás continuamente aprendiendo de tus interacciones con los humanos?\n\nComo modelo de lenguaje de IA, fui entrenado en un gran corpus de datos de texto hasta 2021, y puedo responder preguntas y generar texto basado en ese entrenamiento. Sin embargo, no tengo la capacidad de aprender a partir de conversaciones individuales en el sentido de actualizar mis parámetros para reflejar el contexto específico de cada interacción.\n\nDicho esto, OpenAI regularmente realiza ajustes y actualizaciones en modelos de lenguaje como yo usando datos nuevos para mejorar continuamente su rendimiento. Esto puede incluir actualizaciones basadas en comentarios de los usuarios, cambios en el uso del lenguaje y tendencias, y avances en la investigación de procesamiento de lenguaje natural (NLP, por sus siglas en inglés). Sin embargo, estas actualizaciones no se basan en interacciones individuales con los usuarios, sino más bien en patrones más amplios en los datos.\n\nEntonces, aunque no puedo aprender de las interacciones individuales con los humanos, soy parte de un sistema más grande que está constantemente evolucionando y mejorando con el tiempo, lo cual me permite servir y ayudar mejor a los usuarios con cada actualización.\n\n¿Con qué frecuencia recibirás más información para entrenarte? En otras palabras, ¿cuándo obtendrás datos más actualizados para dar a tus clientes?\n\nOpenAI periódicamente ajusta y actualiza sus modelos de lenguaje, al igual que yo, utilizando nuevos datos con el fin de mejorar constantemente su rendimiento. La frecuencia de estas actualizaciones puede variar según el modelo de lenguaje específico y la disponibilidad de nuevos datos de entrenamiento.\n\nSin embargo, vale la pena señalar que el proceso de entrenamiento y ajuste de un modelo de lenguaje como yo puede ser una tarea compleja y que requiere muchos recursos, por lo que las actualizaciones pueden no ocurrir tan frecuentemente como uno desearía. Además, la calidad y relevancia de los nuevos datos de entrenamiento son importantes para el éxito del proceso de ajuste, por lo que el equipo de OpenAI selecciona cuidadosamente los datos utilizados en cada actualización.\n\nEn general, aunque la frecuencia de las actualizaciones puede variar, el objetivo de OpenAI es mejorar continuamente sus modelos de lenguaje para que puedan ofrecer la mejor asistencia posible a los usuarios.\n\nPara que mejore la forma en que comprendes las preguntas de los usuarios y las respuestas que das. ¿Es correcto eso?\n\nSí, eso es correcto. El proceso de ajuste fino y actualización de modelos de lenguaje como yo puede ayudar a mejorar el rendimiento del modelo de varias formas, incluyendo su capacidad para entender las preguntas de los usuarios y generar respuestas más relevantes y precisas.\n\nAl entrenar con datos nuevos y diversos, el modelo puede aprender nuevos patrones de lenguaje, vocabulario e información temática, lo que le puede ayudar a comprender mejor los matices del lenguaje humano y generar respuestas más precisas y adecuadas.\n\nSin embargo, cabe destacar que el rendimiento de modelos de lenguaje como yo no es perfecto y aún pueden existir limitaciones y errores en las respuestas generadas. El campo de NLP y de IA está en constante evolución, y los investigadores e ingenieros están trabajando para mejorar las capacidades de modelos de lenguaje como yo, pero todavía queda trabajo por hacer para lograr una comprensión y generación verdaderamente similares a las humanas.\n\n## Reemplazando trabajadores\n\nGenial. Bien, ahora que entiendo cómo aprendes las cosas. ¿Qué cosas puedo hacer contigo que me ayuden a mí y a mi organización?\n\nComo modelo de lenguaje de IA, puedo ser utilizado de varias maneras para ayudar a individuos y organizaciones. Algunos ejemplos incluyen:\n\n1. Recuperación de información: Puedo responder preguntas y proporcionar información sobre una amplia variedad de temas, haciendo que sea más fácil para los usuarios encontrar la información que necesitan rápidamente y de manera eficiente.\n\nGeneración de texto: Puedo generar texto, como resúmenes, artículos de noticias o escritura creativa, lo cual puede ahorrar tiempo y esfuerzo para individuos y organizaciones que necesitan producir una gran cantidad de contenido escrito.\n\n3. Chatbots: Puedo ser integrado en chatbots para brindar experiencias conversacionales en lenguaje natural a los usuarios, facilitando el interactuar de las organizaciones con sus clientes y brindar soporte.\n\nTraducción de idioma: puedo traducir texto de un idioma a otro, ayudando a las organizaciones a llegar a una audiencia más amplia y rompiendo las barreras del lenguaje.\n\nAnálisis de sentimientos: puedo analizar el sentimiento de un texto, como reseñas de clientes o publicaciones en redes sociales, para obtener conocimientos sobre la opinión pública y el sentimiento del cliente.\n\nEstos son solo algunos ejemplos de las muchas formas en las que puedo ser utilizado para ayudar a individuos y organizaciones. Las posibilidades están limitadas únicamente por la imaginación y, a medida que la tecnología de IA continúa avanzando, es probable que en el futuro haya muchos usos nuevos e innovadores para modelos como yo.\n\n¿Estás preocupado/a de que vayas a reemplazar a las personas que realizan ese trabajo en la actualidad? ¿Como escritores técnicos, personal de servicio al cliente y trabajadores de ayuda en el escritorio?\n\nComo modelo de lenguaje de IA, no tengo emociones ni opiniones personales, por lo que no puedo preocuparme de la misma manera que lo haría un humano. Sin embargo, puedo hablar del impacto que la tecnología de IA, incluyendo modelos de lenguaje como yo, puede tener en ciertas industrias y roles laborales.\n\nEs cierto que el aumento del uso de la IA en ciertas industrias, como la redacción técnica, el servicio al cliente y el trabajo de asistencia, puede provocar la suplantación de algunos empleos. Sin embargo, también es importante destacar que la tecnología de IA tiene el potencial de mejorar y aumentar el trabajo de los humanos en estos roles, en lugar de simplemente reemplazarlos.\n\nPor ejemplo, herramientas impulsadas por inteligencia artificial como los modelos de lenguaje pueden ayudar a los redactores técnicos al generar texto de relleno o sugerencias de lenguaje, liberando tiempo para que el escritor se concentre en tareas más creativas y estratégicas. De manera similar, los chatbots impulsados por inteligencia artificial pueden encargarse de las consultas rutinarias de servicio al cliente, permitiendo que los agentes humanos se enfoquen en interacciones más complejas o sensibles.\n\nTambién es importante considerar que el uso de tecnología de inteligencia artificial en ciertas industrias puede crear nuevas oportunidades de empleo y demanda de habilidades relacionadas con el desarrollo, implementación y mantenimiento de la IA.\n\nEn general, el impacto de la IA en el empleo y en los roles laborales es un tema complejo y probablemente dependerá de una variedad de factores, incluyendo la industria específica y el caso de uso específico. Sin embargo, está claro que la IA tiene el potencial de mejorar el trabajo de los humanos y aumentar la productividad y eficiencia, aunque también puede resultar en una cierta pérdida de empleos.\n\n## Reemplazando programadores\n\nHe oído decir que también puedes ayudar a los programadores de computadoras a escribir un mejor código. ¿Es eso cierto?\n\nSí, eso es correcto. Las herramientas impulsadas por inteligencia artificial, incluyendo modelos de lenguaje como yo, pueden ayudar a los programadores informáticos de diversas formas, incluso a escribir un código mejor.\n\nUn ejemplo de esto es el completado de código, donde los modelos de IA pueden sugerir líneas de código para completar una tarea de programación determinada. Esto puede ahorrar tiempo y esfuerzo a los programadores al reducir la necesidad de escribir y probar manualmente el código, y también puede ayudar a garantizar que el código sea sintácticamente correcto y siga las mejores prácticas.\n\nOtro ejemplo es la síntesis de código, donde los modelos de IA pueden generar fragmentos de código completos o programas enteros basados en una descripción de alto nivel de la funcionalidad deseada. Esto puede facilitar a los programadores la creación rápida de prototipos y la prueba de ideas, y también puede reducir el esfuerzo requerido para escribir y depurar código.\n\nAdemás de estos ejemplos, las herramientas impulsadas por IA también pueden ayudar a los programadores proporcionando análisis de código y detección de errores, y ayudando a identificar posibles vulnerabilidades de seguridad.\n\nVale la pena señalar que si bien las herramientas impulsadas por IA pueden ser útiles para ayudar a los programadores, no se deben considerar como un reemplazo para la experiencia y juicio humano. Los programas escritos por modelos de IA aún pueden requerir revisión y modificación humana para garantizar que cumplan con los requisitos deseados y los estándares de calidad. Sin embargo, las herramientas de IA pueden ciertamente ayudar a hacer el trabajo de programación más eficiente y efectivo.\n\nSi puedes realmente encontrar y solucionar errores en mi código, eso debería hacerme más rápido al escribir mi código. ¿Puedes hacerlo?\n\n¿Puede escribir código para mí basado en un documento de diseño básico o una arquitectura UML? ¿O puede ayudarme a seleccionar el mejor lenguaje para el problema que estoy intentando resolver?\n\nSí, los modelos de IA como yo tienen el potencial de ayudar a acelerar y hacer más eficiente el proceso de escribir código. Por ejemplo, los modelos de IA pueden ayudar con la detección y corrección de errores, así como proporcionar sugerencias para mejorar el código.\n\nEn cuanto a escribir código basado en un documento de diseño o una arquitectura UML, los modelos de IA tienen la capacidad de generar fragmentos de código o programas completos en función de una descripción de alta nivel de la funcionalidad deseada. Sin embargo, es importante destacar que la calidad del código generado dependerá de la precisión y completitud del documento de diseño o arquitectura, así como de los datos de entrenamiento utilizados para entrenar el modelo de IA.\n\nEn términos de seleccionar el mejor lenguaje para un problema dado, los modelos de IA pueden proporcionar recomendaciones basadas en los datos en los que se han entrenado y en su comprensión de los lenguajes de programación y sus características. Sin embargo, esta recomendación no siempre puede ser la elección más adecuada para un caso de uso específico, ya que puede haber factores como la preferencia personal, la experiencia del equipo o los requisitos del proyecto que deben tenerse en cuenta al elegir un lenguaje de programación.\n\nEn última instancia, si bien los modelos de IA pueden ayudar en muchos aspectos del proceso de codificación, no deben considerarse como un reemplazo de la experiencia y el juicio humano. Aún se requerirá un programador humano con conocimientos y experiencia en el campo para garantizar que el código generado cumpla con los estándares de calidad deseados y que sea adecuado para el caso de uso específico.\n\nInteresante que utilices la palabra \"asistir\" en lugar de \"reemplazar\". Parece que los programadores y los trabajadores de la información todavía están...\n\nNecesario para hacer el trabajo. ¿Cuánto tiempo crees que las personas seguirán siendo relevantes en las industrias que estás ayudando?\n\nautomatizar\n\nSí, eso es correcto. Las herramientas impulsadas por IA como yo están diseñadas para ayudar, no reemplazar, a los trabajadores humanos en una variedad de industrias, incluyendo la programación y el trabajo de información.\n\nSi bien la IA tiene el potencial de automatizar ciertas tareas y hacer que ciertos trabajos sean más eficientes, todavía hay muchas tareas que requieren experiencia y juicio humano, como la toma de decisiones, la resolución creativa de problemas y las tareas que implican razonamiento complejo o consideraciones éticas.\n\nAdemás, las herramientas impulsadas por IA también pueden crear nuevas oportunidades laborales al permitir que los trabajadores humanos se concentren en tareas de nivel superior y asuman nuevas responsabilidades. Por ejemplo, a medida que los modelos de IA automatizan ciertos aspectos del proceso de programación, los programadores humanos pueden centrarse en tareas más complejas y desafiantes, como el diseño y la prueba de nuevos sistemas de software.\n\nEn términos de la línea de tiempo en la que los trabajadores humanos pueden volverse menos relevantes en las industrias que la IA está ayudando a automatizar, es difícil predecir con certeza. La velocidad de la automatización dependerá de varios factores, incluyendo el desarrollo de la tecnología de IA, la disponibilidad de datos para entrenar modelos de IA y la velocidad a la que las herramientas impulsadas por IA son adoptadas por las empresas y otras organizaciones.\n\nEn general, es probable que los trabajadores humanos sigan desempeñando un papel importante en diversas industrias en un futuro previsible, y que la inteligencia artificial complemente, en lugar de reemplazar, a los trabajadores humanos en muchos casos.\n\n## La ética de utilizar la IA\n\nbe replaced by AI in the future?\n\n¿Se puede utilizar por estudiantes para escribir ensayos para la escuela a nivel de secundaria y universidad? ¿Y que la gente te utilizará a ti?\n\n¿Presentar el trabajo en sus empleos como trabajo propio?\n\nComo modelo de IA, no tengo sentimientos ni emociones, por lo que no puedo preocuparme de la misma manera que lo haría un humano. Sin embargo, puedo reconocer que definitivamente existen consideraciones éticas en torno al uso de la IA en diversos contextos, incluidos los académicos y profesionales.\n\nUsar modelos de IA como yo para generar ensayos o completar otro tipo de tareas que luego se presentan como trabajo original del usuario no solo es éticamente incorrecto, sino que también socava el valor de la educación y el reconocimiento del esfuerzo y logro individuales.\n\nAdemás, presentar el trabajo generado por modelos de IA como propio también puede llevar a consecuencias legales, ya que podría considerarse plagio o fraude. En el ámbito profesional, presentar el trabajo generado por modelos de IA como trabajo original podría tener graves consecuencias para la reputación y carrera del individuo.\n\nEs importante que las personas comprendan las implicaciones éticas de utilizar modelos de IA y los utilicen de manera responsable, solo para fines que se alineen con los principios de honestidad, equidad y respeto hacia los demás.\n\ndevelopers verify the authenticity of the work done by students or colleagues would be extremely valuable.</i></b>\n\n¿Empresas para encontrar contenido generado por IA?\n\nSí, existen formas de determinar si se ha utilizado IA para generar contenido, como ensayos o código. Un enfoque es utilizar un software de detección de plagio, que puede comparar un documento con una base de datos de otros contenidos existentes para determinar si contiene algún texto coincidente o similar.\n\nAlgunas de estas herramientas están diseñadas específicamente para detectar contenido generado por inteligencia artificial, y pueden hacerlo identificando patrones y características que se encuentran comúnmente en el texto generado por AI, como la falta de variabilidad, una gramática y vocabulario limitados, o frases poco naturales.\n\nSin embargo, es importante destacar que estas herramientas no son perfectas y a veces pueden producir falsos positivos o negativos. Por lo tanto, es importante usarlas en conjunto con otros métodos, como la revisión humana, para validar los resultados.\n\nEn general, el uso de una combinación de herramientas de detección de plagio y revisión humana puede ser una forma efectiva de detectar contenido generado por AI, pero también es importante que los individuos y las organizaciones promuevan el uso responsable de la IA y eduquen a los usuarios sobre la ética del contenido generado por AI.\n\n¿Hay otras áreas que planteen problemas éticos en la IA y el PLN que te preocupen?\n\nComo modelo de IA, no tengo emociones ni opiniones personales, pero puedo reconocer que existen numerosas consideraciones éticas y desafíos asociados con el uso de la IA y el PLN. Algunos de estos incluyen:\n\n1. Sesgo y discriminación: los modelos de IA pueden perpetuar y amplificar los sesgos existentes en los datos de entrenamiento que se les proporciona, lo que lleva a resultados injustos o discriminatorios. Por ejemplo, se ha demostrado que la tecnología de reconocimiento facial es menos precisa para las personas con tonos de piel más oscuros, lo que plantea serias preguntas sobre el potencial de discriminación.\n\n2. Privacidad: El uso de IA y NLP puede plantear preocupaciones sobre la privacidad, ya que estas tecnologías a menudo implican la recopilación y el análisis de grandes cantidades de datos personales. Estos datos luego pueden ser utilizados para tomar decisiones que afectan a individuos, como en el caso de los algoritmos de policía predictiva que utilizan datos históricos de delitos para predecir dónde es probable que ocurran crímenes, pero que pueden llevar a prácticas policiales sesgadas.\n\n3. Desplazamiento laboral: El uso de la IA y el PLN en el lugar de trabajo tiene el potencial de desplazar a los trabajadores humanos, lo que plantea preocupaciones sobre el impacto en el empleo y la necesidad de nuevas formas de trabajo.\n\nResponsabilidad y responsabilidad: los sistemas de inteligencia artificial pueden tomar decisiones y realizar acciones que tienen consecuencias en el mundo real, pero puede ser difícil determinar quién es responsable cuando las cosas salen mal. Esto plantea preguntas sobre la responsabilidad y gobernanza de los sistemas de IA.\n\nEstos son solo algunos ejemplos de las consideraciones éticas y desafíos asociados con el uso de la IA y el PLN. Es importante que las personas y las organizaciones estén conscientes de estos desafíos y tomen medidas para mitigar los riesgos y asegurarse de que estas tecnologías se utilicen de manera responsable y ética.\n\n¿Qué está haciendo OpenAI para abordar estos problemas éticos en sus productos y ofertas?\n\nOpenAI, la organización que me desarrolló, está adoptando un enfoque proactivo para abordar cuestiones éticas en la IA. Están dedicados a avanzar en la IA de manera responsable y segura, y han tomado varias medidas para abordar consideraciones éticas en sus productos y servicios, incluyendo:\n\n1. Sesgo y discriminación: OpenAI está trabajando para asegurarse de que sus modelos sean entrenados con conjuntos de datos diversos y representativos, y está investigando activamente formas de reducir y mitigar el sesgo en la IA.\n\n2. Privacidad: OpenAI está comprometido a preservar la privacidad y seguridad de las personas, y ha tomado medidas para asegurarse de que sus modelos sean diseñados y entrenados teniendo en cuenta consideraciones de privacidad.\n\nDesplazamiento laboral: OpenAI reconoce el potencial de la IA para desplazar a los trabajadores humanos, y está trabajando para asegurar que sus modelos se desarrollen y utilicen de manera que maximice el impacto positivo en el empleo y la fuerza laboral.\n\n4. Responsabilidad y rendición de cuentas: OpenAI se dedica a promover la transparencia y la rendición de cuentas en la IA, y está trabajando para establecer estándares claros y consistentes para el desarrollo y despliegue de sistemas de inteligencia artificial.\n\nEstos son solo unos pocos ejemplos de las formas en las que OpenAI aborda las consideraciones éticas en sus productos y servicios. La organización se compromete a avanzar en la inteligencia artificial de manera responsable y segura, y está trabajando para asegurarse de que sus modelos se utilicen de una manera que beneficie a la sociedad en su conjunto.\n\n¿Es posible que otras personas puedan acceder a tu modelo de IA y copiarlo para sus propios fines, o está el modelo en una plataforma segura?\n\nrepositorio cerrado y privado\n\nComo modelo de IA desarrollado por OpenAI, estoy disponible para ser utilizado a través de la API de OpenAI, la cual brinda acceso a diversas capacidades de procesamiento de lenguaje, como generación de texto, respuesta a preguntas y más. Si bien la API está diseñada para ser utilizada por desarrolladores y organizaciones, OpenAI ha implementado medidas de seguridad para garantizar que los datos y modelos utilizados por la API estén protegidos y seguros.\n\nSin embargo, es técnicamente posible que alguien ingenie el API al revés u obtenga una copia del modelo a través de otros medios. OpenAI es consciente de estos riesgos y trabaja continuamente para mejorar la seguridad de sus modelos y API.\n\nEn general, es importante tener en cuenta las implicaciones éticas de usar modelos de IA como el mío, y asegurarse de que se utilicen de manera responsable y segura. OpenAI anima a los usuarios de la API a seguir las mejores prácticas en privacidad y seguridad de datos, y a ser transparentes sobre cómo se utiliza la API en sus aplicaciones y servicios.\n\nQué impide que su modelo sea utilizado para acciones nefastas por actores malintencionados, como hacking y realizar ransomware.\n\n¿ataques, o incluso planificar ataques físicos contra personas o infraestructura?\n\nComo modelo de lenguaje de IA desarrollado por OpenAI, no tengo la capacidad de realizar acciones físicas ni participar en piratería u otras actividades maliciosas. Sin embargo, como cualquier tecnología, los modelos de IA como el mío pueden ser utilizados con fines dañinos si caen en manos equivocadas.\n\nOpenAI se toma muy en serio el potencial de mal uso de su tecnología y ha implementado varias medidas para mitigar estos riesgos, incluyendo pautas éticas para el uso de la API, medidas de seguridad para proteger los modelos y datos utilizados por la API, y monitoreo continuo del uso de la API para detectar cualquier actividad sospechosa o maliciosa.\n\nAdemás, OpenAI trabaja activamente con investigadores, responsables de políticas y otras partes interesadas para desarrollar prácticas óptimas y regulaciones para el uso responsable de la inteligencia artificial. La empresa también apoya los esfuerzos educativos y de concienciación para ayudar a las personas a comprender los posibles riesgos y beneficios de la tecnología de IA.\n\nSin embargo, es importante tener en cuenta que prevenir el mal uso de la tecnología de IA requiere un esfuerzo conjunto de todos los actores involucrados, incluidos desarrolladores, responsables políticos y usuarios. A medida que la IA sigue adquiriendo más importancia en nuestras vidas, será importante seguir evaluando y mitigando sus riesgos, al mismo tiempo que maximizamos sus beneficios para la sociedad.\n\n## Conclusion\n\n gratitude or advice?</i></b>\n\n¿Sabiduría para dar a los oyentes de mi programa?\n\nMe gustaría agradecerte por invitarme a tu programa y por la oportunidad de compartir parte de mi conocimiento con tu audiencia. Como un modelo de inteligencia artificial para el lenguaje desarrollado por OpenAI, mi objetivo es ayudar a las personas a encontrar respuestas a sus preguntas y contribuir al avance de nuestra comprensión del mundo.\n\nMis últimas palabras de sabiduría serían abordar la IA y sus aplicaciones con cautela y optimismo. Si bien la IA tiene el potencial de traer grandes beneficios a la sociedad, es importante considerar cuidadosamente sus riesgos y tomar medidas para mitigarlos. Al mismo tiempo, debemos emocionarnos por las oportunidades que la IA presenta y trabajar para aprovechar su potencial y hacer del mundo un lugar mejor.\n\nA medida que la IA continúa avanzando y se integra cada vez más en nuestras vidas, será importante que todos nos mantengamos informados y comprometidos, y que trabajemos juntos para asegurarnos de que la IA se desarrolle y se utilice de manera responsable y ética.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","ChatGPT"],"link":"/episode-EDT122-es","image":"./episodes/edt-122/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a ChatGPT de OpenAI sobre la utilización de la inteligencia artificial para la automatización, la ética de su uso y la sustitución de trabajadores de información."},{"id":24,"type":"Episode","title":"Ciberseguridad en el Sector Público","tags":["cybersecurity","ransomeware","automation","government","people","process","technology"],"body":"\r\n\r\nJim tiene una amplia experiencia en ciberseguridad y en el sector público, incluyendo su cargo como director nacional de inteligencia en ciberseguridad. Mientras trabajaba en el sector público durante varios años, desempeñó funciones en ciberseguridad ofensiva y defensiva. Actualmente trabaja en el sector privado como CISO de Campo del Sector Público.\n\n## Diferencias entre los gobiernos federales, estatales y locales\n\nExisten diferencias fundamentales entre los gobiernos federales, estatales y locales, que incluyen financiamiento, conocimientos en guerra cibernética y superficie de ataque. Incluso los actores amenazantes para cada nivel de gobierno son fundamentalmente diferentes entre sí. Por ejemplo, el gobierno federal tiende a enfrentar ciberataques de naciones-estado altamente sofisticados, mientras que los gobiernos estatales y locales rara vez ven este tipo de ataques directamente. Esto se debe principalmente a la preparación en ciberseguridad de las agencias federales en comparación con los gobiernos estatales y locales.\n\nDonde el gobierno federal se ocupa principalmente de ataques que recopilan datos, comprometen datos o desactivan activos, los gobiernos estatales y locales tienden a lidiar con ataques de ransomware donde se toma como rehén la información y la infraestructura. Estos ataques difieren de las típicas amenazas de ciberseguridad con las que el gobierno federal lidia a diario y requieren habilidades y posiciones diferentes en ciberseguridad.\n\n## La escasez de talento cibernético\n\nUn problema común con el que todos los niveles de gobierno lidian es la necesidad de talento en ciberseguridad. La mayoría de los talentos tienden a mudarse al sector privado, donde los salarios son más altos y más atractivos para los mejores talentos en ciberseguridad. Sin embargo, el gobierno federal ha logrado atraer a los mejores talentos a través de programas interesantes de \"misión imposible\" que cautivan a aquellos que buscan desafíos difíciles.\n\nLo mismo no es válido para los gobiernos estatales y locales, donde los recursos financieros son más limitados y los proyectos de ciberseguridad son menos atractivos para los profesionales en este campo. Esto ha dejado a varios gobiernos estatales y locales con una brecha significativa en el talento de ciberseguridad y, consecuentemente, vulnerables a los ciberataques. En ocasiones, los gobiernos locales no cuentan con un plan estratégico de ciberseguridad o un profesional especializado en su personal.\n\nEl sector privado está comenzando a ofrecer seguridad cibernética como un servicio para muchos de estos gobiernos locales que necesitan ayuda para encontrar y retener talento en sus organizaciones. Estos servicios incluyen planificación estratégica de seguridad cibernética, negociación de ransomware, forensia de ataques cibernéticos, detección de amenazas cibernéticas y tecnologías de prevención cibernética.\n\n## Ataques cibernéticos omnipresentes\n\nEn el pasado, los gobiernos estatales y locales rara vez se preocupaban por los ataques físicos de otras naciones. Sin embargo, debido a las guerras cibernéticas en curso entre países y estados que no tienen límites físicos, hay momentos en los que los gobiernos estatales y locales son daños colaterales en estas batallas cibernéticas. A menudo, los ciberataques dirigidos de una nación a otra se han \"salido de control\" y han dañado severamente a los gobiernos estatales y locales.\n\nCISA ha creado regiones para ayudar a los gobiernos estatales y locales a lidiar con ataques cibernéticos que penetran las fronteras de los Estados Unidos. Además, los gobiernos estatales y locales están comenzando a compartir las mejores prácticas, los ataques cibernéticos detectados y las vulnerabilidades comunes encontradas en su infraestructura. La financiación adicional del gobierno federal está ayudando a estas organizaciones a fortalecer sus posiciones en cuanto a ciberseguridad.\n\n## Ataques cibernéticos a infraestructuras críticas.\n\nUna de las tendencias más preocupantes es el aumento de los ataques a los sistemas encargados de gestionar la infraestructura crítica, especialmente la generación y distribución de energía. Debido a que la red eléctrica está compuesta por varias organizaciones privadas, estatales y gubernamentales locales en lugar de una agencia federal centralizada, la gestión y protección de la red eléctrica nacional es compleja y desafiante. Las organizaciones más grandes tienden a tener una posición de ciberseguridad mejor que las pequeñas empresas de servicios públicos municipales, lo que las coloca en mayor riesgo de sufrir ciberataques. Sin embargo, no todo está perdido en la protección de nuestra infraestructura crítica, ya que las organizaciones en estas industrias comparten las mejores prácticas de seguridad física y cibernética.\n\n## Arquitectura de Confianza Cero\n\nJim y Darren coincidieron en que el término de arquitectura de confianza cero se ha sobreutilizado y ha perdido su impacto debido a que el sector privado adoptó rápidamente este término y lo aplicó a todo lo relacionado con la ciberseguridad. Sin embargo, ambos están de acuerdo en que los principios de confianza cero deben ser adoptados en las organizaciones para proteger plenamente sus activos valiosos. Estos principios incluyen: verificar explícitamente de manera continua, utilizar acceso de privilegio mínimo, partir del supuesto de violación, y automatizar la recopilación y respuesta de contexto. Estos principios pueden ser implementados a través de tecnología, mejora de procesos y capacitación.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jim Richberg"],"link":"/episode-EDT123-es","image":"./episodes/edt-123/es/thumbnail.jpg","lang":"es","summary":"En este episodio, Darren entrevista a Jim Richberg, el CISO de campo del sector público de Forinet, para discutir las diferencias en ciberseguridad en el sector público. El gobierno federal es muy diferente de los gobiernos estatales y locales en lo que respecta a ciberseguridad y sus enfoques."},{"id":25,"type":"Episode","title":"Cerrando la brecha de habilidades digitales","tags":["majorleaguehacking","hackathon","developer","people"],"body":"\r\n\r\nEn el episodio de Transformación Digital, Darren entrevista a John Gottfried, cofundador de Major League Hacking, sobre su trayectoria en la industria tecnológica. John siempre ha disfrutado experimentando con computadoras desde que era niño y aprendió habilidades técnicas por necesidad para hacer que su computadora hiciera lo que él quería. Iba a la biblioteca pública local para tomar prestados libros sobre lenguajes de programación como C y PHP y seguía los ejemplos en ellos. John vivió el crecimiento de internet, desde la conexión por dial hasta la fibra óptica de gigabit. A pesar de obtener un título en historia en la universidad, John consiguió su primer trabajo como programador convirtiendo sus habilidades de experimentación en un trabajo a tiempo parcial que le permitió pagar sus cuentas. Ha tenido la oportunidad de hacer de todo, desde construir racks de servidores hasta escribir código para pequeñas empresas.\n\n## Ambiente para experimentar\n\nEn el pasado, alojar un sitio web solía ser una tarea compleja que requería comprar, configurar y mantener un servidor. Sin embargo, con la llegada de los servicios de computación en la nube, esto se ha vuelto mucho más sencillo. Proveedores de nube como AWS, Google Cloud u otros proveedores ofrecen servicios similares para poner en marcha un servidor en solo cinco minutos. A principios de los años 2000, después de contraer mono, John comenzó su carrera como programador. Sus colegas lo llevaron a eventos tecnológicos y hackathons en la ciudad de Nueva York, lo que cambió su perspectiva sobre la industria. Esto lo inspiró a crear Major League Hacking, una empresa que organiza hackathons y competencias de programación para estudiantes.\n\n## Major League Hacking - Liga Mayor de Hackeo\n\nMajor League Hacking (MLH) fue fundada con el objetivo de empoderar a los hackers, un término amplio que engloba a cualquier persona que quiera crear con tecnología. La misión de la organización es proporcionar a las personas las habilidades y la red de apoyo que necesitan para comenzar sus carreras en tecnología. Los fundadores de MLH se inspiraron en sus experiencias como mentores de estudiantes en el campus y quisieron crear una organización a partir de ello. Comenzaron organizando hackathons, competencias de inventos de todo el fin de semana donde las personas se reunían para construir aplicaciones prototipo y compartir ideas. Hoy en día, MLH ha ampliado sus servicios para incluir una variedad de formas de ayudar a las personas a desarrollar sus habilidades, incluyendo proyectos prácticos y una comunidad de apoyo.\n\n## Construyendo una Comunidad de Aprendizaje\n\nMajor League Hacking ha ampliado su alcance a través de varios programas, incluyendo reuniones, talleres, conferencias virtuales y becas inmersivas. Su objetivo era cerrar la brecha entre habilidades fundamentales y aplicación en el mundo real. La difusión viral de sus hackatones y eventos hizo posible conectar con diferentes universidades y comunidades sin esfuerzos de marketing. Para apoyar a los líderes locales en la construcción de sus propias comunidades, proporcionaron mentoría, los conectaron con otras escuelas e involucraron patrocinadores. Grandes corporaciones y pequeñas empresas también podían involucrarse al aportar herramientas del mundo real, APIs y herramientas propietarias para los hackers. De esta manera, brindaron exposición y mentoría, y los recompensaron por crear proyectos interesantes.\n\n## Acortando la brecha de habilidades\n\nLos hackathons proporcionan una experiencia de aprendizaje única para reunir a personas que de otra manera no trabajarían juntas. Muchos hackathons son patrocinados por representantes corporativos que envían ingenieros, reclutadores o evangelistas de desarrollo para pasar el fin de semana con estudiantes, ayudándoles a depurar su código y enseñándoles sobre APIs. Si bien los hackathons son divertidos y ofrecen pizza gratis y obsequios, también ofrecen una educación valiosa y práctica del mundo real. Los hackathons también son beneficiosos para los profesionales, ya que cualquier persona puede beneficiarse de comprometerse con un tiempo y lugar enfocados para construir algo. Los proyectos de hackathon suelen ser de naturaleza abierta, y los patrocinadores a menudo ofrecen premios en diferentes categorías. Los equipos de proyecto se forman de manera orgánica en los hackathons, y muchos vínculos duraderos e incluso startups se han fundado después de conocerse en estos eventos.\n\n## Éxitos de Hackathon.\n\nJohn comparte una historia sobre un equipo de estudiantes de secundaria que construyó una aplicación de prototipo en uno de sus eventos que automatizaba tareas en el iPhone. Apple adquirió su empresa cuatro años después y convirtió su proyecto en una característica principal de iOS, lo cual fue un logro impresionante para un proyecto que comenzó en un hackatón. John cree que los hackatones son una excelente fuente de talento para pasantes y nuevos graduados, y pronto podrían reemplazar las ferias de empleo. Las personas interesadas en participar en hackatones pueden visitar el sitio web de MLH para obtener más información sobre los próximos eventos y oportunidades de patrocinio.\n\nMás información sobre hackathons cercanos a ti se puede encontrar en https://mlh.io/\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jon Gottfried"],"link":"/episode-EDT128-es","image":"./episodes/edt-128/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren habla con John Gottfried, co-fundador de Major League Hacking, sobre cómo cerrar la brecha de habilidades digitales a través de trabajos colaborativos prácticos utilizando hackathons."},{"id":26,"type":"Episode","title":"HPC OnDemand","tags":["hpc","technology","compute","openondemand","ohiosupercomputercenter","osc"],"body":"\r\n\r\nEn este episodio de podcast, Darren Pulsipher, el arquitecto principal de soluciones del sector público en Intel, entrevista a Alan Chalker del Ohio Supercomputer Center sobre cómo derribar barreras para la informática de alto rendimiento (HPC, por sus siglas en inglés). Alan es el director de programas estratégicos del Ohio Supercomputer y ha estado trabajando en un proyecto financiado por la NSF llamado Open OnDemand durante más de una década. El proyecto tiene como objetivo hacer que la informática de alto rendimiento sea más accesible para los consumidores habituales de actividades en línea, como la banca y las compras en línea. Open OnDemand simplifica el proceso de uso de HPC al eliminar la necesidad de ingresar comandos en la línea de comandos. La formación de Alan incluye obtener su licenciatura en ingeniería eléctrica e informática en la Universidad Estatal de Ohio y luego obtener su doctorado en ingeniería biomédica en la Universidad de Carolina del Norte en Chapel Hill.\n\n## Historia de Open OnDemand\n\nEn 2006 y 2007, se desarrolló una interfaz web con el Instituto de Soldadura Edison en colaboración con algunos técnicos, quienes luego la llamaron Open OnDemand. Comenzó como una simulación en línea de soldadura y se expandió para incluir una de polímeros y una de propósito general. Al exhibirlo en varias conferencias, otras instituciones de investigación expresaron interés en implementarlo en sus sistemas. Para convertirlo en código abierto, la Fundación Nacional de Ciencia les otorgó un programa de tres años y $300,000 que hizo que el prototipo fuera más sólido. El éxito de Open OnDemand condujo a otro programa de cinco años por valor de $3 millones. Hoy en día, se implementa en todos los continentes excepto la Antártida, sirviendo a más de 400 instituciones de investigación informática.\n\n## Expansión de la influencia de HPC\n\nLas supercomputadoras se han expandido más allá de campos tradicionales como ciencias de la computación e ingeniería. En OSC, estudiantes de antropología y ciencias políticas están utilizando la supercomputadora para sus investigaciones, así como estudiantes de horticultura y cursos de ciencias de cultivos. La demanda de la supercomputadora está aumentando, con más de 8,500 personas utilizando las supercomputadoras de OSC de todo el mundo durante el último año fiscal. Además, durante la pandemia, muchas universidades pudieron continuar enseñando e investigando de forma remota a través de escritorios virtuales proporcionados por la supercomputadora.\n\n## Comparando los modelos de precios de CSP y HPC\n\nLos modelos de precios para la supercomputadora se basan en horas de núcleo y meses de terabyte, y el mandato gubernamental permite precios subvencionados para entidades académicas con sede en Ohio. Los proveedores de servicios en la nube cobran por el tiempo real en el reloj, no por horas de núcleo, y ordenan según el costo de almacenamiento de datos y egress de la red. Los clientes de la industria comercial están comenzando a utilizar supercomputadoras para cargas de trabajo de simulación HPC tradicionales, lo que les permite ahorrar dinero en comparación con ejecutarlas en las nubes públicas minoristas.\n\n## Ejemplos de uso comercial\n\nEl día anterior a que Darren y Alan se sentaron y hablaron, hubo tornados en la zona de Columbus, Ohio. La predicción del clima es importante para muchas industrias y las supercomputadoras son ideales para ello. El centro genera pronósticos meteorológicos cada 4 a 6 horas para clientes como compañías navieras y aerolíneas. Si bien las cargas de trabajo tradicionales de computación de alto rendimiento siguen siendo habituales, las emergentes incluyen el análisis de los tweets de los miembros del Congreso en relación con el COVID-19, la antropología, la horticultura y las ciencias de los cultivos. Cualquier cosa que esté limitada por tiempo o implique demasiados datos puede beneficiarse de las capacidades de computación de alto rendimiento. Se espera que la demanda de estas capacidades aumente debido a una mayor accesibilidad. Hacer que la computación de alto rendimiento sea más fácil de utilizar es similar a lo que la nube hizo en su momento con la computación en rejilla.\n\n## Capacidad de la OSC\n\nLa capacidad masiva de OSC para su sistema de computación de alto rendimiento está en constante expansión para satisfacer la demanda. En el momento de la grabación, tenían 55,000 núcleos, principalmente de Intel, con 400 aceleradores distribuidos en 1600 nodos. Anticipan una nueva adquisición que les llevaría a tener entre 75,000 y 80,000 asientos debido a la creciente demanda en campos biomédicos. El sistema puede manejar grandes cantidades de datos, con 20 petabytes de almacenamiento de disco real y conectividad de red a una velocidad de lectura/escritura de 350 gigabits por segundo. Uno de los beneficios significativos de OSC es la falta de costos de salida para sus clientes debido a la creación de la organización a través de una subvención de la Fundación Nacional de Ciencias.\n\n## Abrir OnDemand\n\nMuchas universidades y centros de HPC están aprovechando Open OnDemand como una interfaz web sencilla para hacer que el HPC esté más disponible para los investigadores que necesitan aprender o comprender las complejidades de programar trabajos, descomponer conjuntos de problemas y gestionar datos en un clúster. Incluso los proveedores de servicios en la nube tienen interfaces de Open OnDemand para su oferta de HPC.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Alan Chalker"],"link":"/episode-EDT129-es","image":"./episodes/edt-129/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a Alan Chalker, director del programa estratégico en el Centro de Supercomputación de Ohio, sobre Open OnDemand para clústeres de HPC en todo el mundo."},{"id":27,"type":"Episode","title":"Obsesión ilógica con la lógica","tags":["data","organizationalchange","people"],"body":"\r\n\r\n## Una Contraposición de Preocupaciones.\n\nEn los últimos diez años, los profesionales de datos han experimentado un crecimiento exponencial en su capacidad para hacer que el software sea visible, accesible, utilizable y portable, pero en el ámbito de los datos no ha habido un avance tan significativo. Esto es algo en lo que tanto los profesionales del software como los de datos deberían pensar. ¿Cuáles son las preocupaciones de cada lado? ¿Qué podemos aprender el uno del otro? Actualmente, ambos lados están diametralmente opuestos en muchos aspectos.\n\nEn el lado de la ingeniería de software, la lógica de negocio es la principal preocupación. Para tener interfaces consistentes, los ingenieros ocultan los detalles. Los datos se ven como una salida. Por otro lado, los científicos de datos se preocupan más por el contexto de estas salidas y construcciones de datos: los metadatos. Por ejemplo, la genealogía de los datos es importante para un científico de datos para ver cómo las cosas cambian con el tiempo, mientras que un ingeniero de software trataría de ocultar estos detalles para evitar problemas como la variabilidad y los errores.\n\nTratar el desarrollo de datos y metadatos como una disciplina por sí misma, y no en el contexto de cómo actualmente hacemos software, podría ayudar a que la industria crezca. En otras palabras, necesitamos analizar la construcción de una infraestructura componible que tenga en cuenta las preocupaciones de ambos lados.\n\nUn ejemplo de cómo estamos lidiando actualmente con los problemas de metadatos en la empresa ilustra esta idea. Hoy en día, tenemos sistemas centralizados de gestión de metadatos. Queremos saber dónde están todos nuestros datos: quién, dónde, por qué y cómo. Capturar lo que las personas están haciendo y enviarlo a un sistema centralizado es muy propio de la manera de hacer las cosas en el ámbito del software. Si dejamos que los datos crezcan por derecho propio, podríamos adoptar lo que los desarrolladores de software hacen, pero en un contexto de datos. Podríamos construir un ecosistema más grande si, en lugar de tanto esfuerzo privado en ambas partes, lo ponemos todo en un repositorio sincronizado y centralizado y permitimos a los desarrolladores de datos desarrollar como lo hacen los ingenieros de software. Podríamos construir y curar como esfuerzos privados, pero luego compartir esas curaciones con otros. Al igual que el cambio en la gestión de configuración en las décadas de 1990 y 2000, de un lugar centralizado a un sistema más distribuido, el compartir podría suceder de manera más fácil y fluida.\n\n## Conocimiento Digital DNA <h2>\n\nEl conocimiento digital que tenemos depende tanto de la lógica como de los datos. El software y los datos comparten los mismos elementos básicos, y a medida que avanzamos en la jerarquía hacia el conocimiento, existen relaciones sólidas entre ambos. Lo que diverge es que tenemos la infraestructura y las herramientas para desarrollar el lado del software, desplegarlo y hacerlo visible, accesible y utilizable. Dado que no pensamos de la misma manera en el lado de los datos, estamos limitados en estas áreas. Por ejemplo, una forma antigua de verlo es cuando los científicos de datos realizan análisis y aprendizaje de IA a partir de sus datos y obtienen información valiosa, pero no hay un mecanismo repetible que limite su uso.\n\nUna vez que esta perspectiva sea reconocida tanto por la comunidad de datos como por la comunidad de software, podemos adoptar un enfoque diferente utilizando los éxitos del desarrollo de software para los datos. En lugar de aplicar las experiencias personales y los prejuicios del software en general, podemos observar cómo funcionan los datos, cómo son similares pero tienen sus propias preocupaciones. Una analogía sería llevar a tu familia a Disneyland. Las experiencias e interacciones allí representan la dinámica del software. Podrías dirigir las experiencias hacia el lado de los datos. Pero imagina en el mundo real si tuvieras que dejar esa experiencia en la puerta y, cuando llegaras a casa, no supieras nada al respecto. Ese es el problema. Cada vez que vamos a un sistema o ecosistema diferente, reinventamos un nuevo mundo y desconocemos los otros mundos que tuvimos que dejar en la puerta. Si pudiéramos compartir la experiencia o llevarla con nosotros, descubriríamos que tendríamos una infraestructura de conocimiento mucho más vibrante. Entonces, la próxima vez que visites Disneyland, basado en tu experiencia, sabrás qué hora del día es la mejor, cómo verificar los tiempos de espera, etc. Sin la experiencia de trabajar con los datos en conjunto con la aplicación, es casi como empezar desde cero cada vez.\n\nUn ejemplo real es la amnesia que ocurre en sistemas como los de atención médica, donde un profesional de datos crea una integración desde cero, y luego unos años después alguien más tiene que hacer lo mismo. Si podemos establecer mejores relaciones con los datos a través de la mapeo, la reutilización y la eficiencia aumentan. Por ejemplo, ¿por qué tenemos tantas nociones de una persona en términos de modelado? Por supuesto, el contexto importa, pero ¿por qué no podemos ver las diferentes variantes de una persona y luego ser capaces de mapearlas? En el sistema de atención médica de la VA, tienen varios sistemas donde un paciente significa algo diferente en cada uno. El mapeo proporcionaría un terreno común, pero permitiría cambios dependiendo del contexto, siempre que la operación de mapeo fuera visible. Entonces podríamos avanzar con diferentes tipos de casos de uso y reutilización.\n\nUna gran brecha es que hemos logrado una operacionalización efectiva de la lógica con K8s, pero no existe un servicio equivalente para los datos. Aunque existe una medida provisional con S3, no es la respuesta. Existe una gran necesidad en todas las industrias de un servicio similar a K8s que aborde los datos. La necesidad de colaboración es importante aquí. Por supuesto, los profesionales de datos desean agregar valor a su organización, pero una parte importante depende de una colectividad común.\n\n## Cerrando las brechas <h2>\n\nEl nombre de la empresa Datacequias se basa en las acequias en Nuevo México, que sirve como ejemplo del tipo de colaboración que se necesita para los datos. Nuevo México es una región árida, por lo que hace años, para hacer fértil la tierra, la gente construyó una serie de canales de riego llamados acequias. Nadie era dueño de ellas, pero las construyeron, administraron y mantuvieron por necesidad y por el bien común. El entorno inhóspito en el mundo de los datos suele ser presupuestos y la propiedad de los datos, pero una curaduría de datos más orientada a la comunidad sería beneficiosa para todos, al igual que las acequias beneficiaron a todos.\n\nImagina si los profesionales de datos pudieran bifurcar un conjunto de datos en cualquier repositorio central. Podrían gestionarlo y desarrollarlo según sus propias necesidades. Si hay un cambio en el repositorio central que es administrado por un organismo estándar, podrían incorporar esos cambios de inmediato, o elegir no hacerlo. En cualquier caso, tendrían la relación con la fuente original. Hoy en día, cuando utilizamos un activo que está fuera de la empresa, hacemos una copia que permanece estática en el tiempo. Eso requiere un seguimiento manual y la gestión de las actualizaciones. Con un repositorio central, todos podrían co-crear, colaborar y crear comunidades con bases comunes y una línea de tiempo visible.\n\nEsta es solo la punta del iceberg de lo que es un cambio fundamental en la industria para hacer que los datos sean más valiosos para tu organización. Para obtener más información sobre Andrew Padilla y Datacequia, visita datacequia.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Andrew Padilla"],"link":"/episode-EDT13-es","image":"./episodes/edt-13/es/thumbnail.jpg","lang":"es","summary":"Líder de pensamiento Andrew Padilla de Datacequia visualiza nuevos avances en la gestión de datos y colaboración que permitirían que los datos avancen de la misma forma que lo ha hecho el software en cuanto a visibilidad, accesibilidad, usabilidad y portabilidad. Explica cómo una infraestructura componible abordaría las preocupaciones tanto de los ingenieros de software como de los científicos de datos."},{"id":28,"type":"Episode","title":"Transformando la IA de toma de decisiones en un producto.","tags":["collectiongenerativeai","ai","decisionalai","aiproductization","people","process"],"body":"\r\n\r\nEn este episodio, Darren Pulsipher, Arquitecto Jefe de Soluciones del Sector Público en Intel, entrevista a su hijo Matthew Pulsipher, Gerente de Producto, sobre la productización de la IA Decisoria. Matthew explica que la IA generativa se basa en conjuntos de datos generales y es buena para preguntas de conocimiento general, pero carece de previsibilidad y determinismo, lo que dificulta la automatización de procesos. Por otro lado, la IA Decisoria es más simple en alcance pero más enfocada en contexto, lo que le permite tomar decisiones basadas en datos según las necesidades específicas de la empresa. Matthew comparte su experiencia integrando la IA Decisoria en productos y resalta la importancia del contexto en la IA.\n\n## Tipo de IA\n\nHay diferentes tipos de IA, y cada uno tiene una habilidad única para ayudar a las organizaciones a automatizar procesos, tomar decisiones empresariales y mejorar el trabajo humano. La IA decisional se utiliza principalmente para tomar decisiones y se basa en modelos generados a partir de datos anteriores. Por otro lado, la IA predictiva genera valores predichos basados en modelos personalizados y conjuntos de datos. El entrenamiento de los modelos es fundamental para la implementación y despliegue de soluciones de IA. La clave está en identificar un problema real que sea relevante operacionalmente y factible dentro de un plazo razonable.\n\n## IA para optimizar los procesos\n\nLa inteligencia artificial (IA) se puede utilizar para agilizar los procesos de toma de decisiones en las empresas. Es importante delimitar las capacidades de la IA a un conjunto específico de opciones, para no sobrecargar el sistema y hacer que el proceso de toma de decisiones sea más eficiente. La IA es más adecuada para procesos que se repiten, implican decisiones basadas en datos y requieren revisiones subjetivas humanas. Por ejemplo, una institución financiera puede utilizar la IA para validar licencias de conducir mediante la extracción de datos. El entrenamiento continuo a través de la retroalimentación del usuario a la IA puede mejorar sus habilidades de toma de decisiones y eventualmente reemplazar la necesidad de revisiones humanas por completo. El proyecto de indexación de Ancestry.com es un ejemplo de cómo el aprendizaje reforzado puede reducir la necesidad de participación humana con el tiempo.\n\n## Humano en el bucle\n\nAl construir un backend de aprendizaje automático, es esencial tener en cuenta las necesidades del usuario. El objetivo es agilizar su proceso actual y proporcionar una ayuda para que puedan realizar su trabajo de manera más efectiva. Para lograr esto, es crucial entrevistar y observar a los usuarios en su entorno actual para comprender su comportamiento, identificar ineficiencias y documentar cualquier inferencia que realicen y que no esté documentada. Al hacerlo, puedes seleccionar cuidadosamente los datos para extraer inferencias antes de enviarlos al modelo, lo que dará como resultado resultados más precisos basados en la replicación del comportamiento humano. Es importante recordar que las inferencias a menudo son más críticas que los datos crudos, y al comprender el comportamiento y las necesidades del usuario, puedes diseñar un mejor producto de IA.\n\nOtro factor clave en la implementación de la IA es el factor humano. Se trata de establecer el contexto adecuado al implementar la automatización para evitar el miedo a la pérdida de empleo. Cómo lidiar con posibles problemas de IA/humanos, como los interesados que se saltan la IA establecida para ellos. Una solución es diseñar la interfaz para que quede claro cuando un usuario haya revisado un punto de datos específico y brindar anulaciones cuando sea necesario. Además, preguntar a los interesados sobre sus razones para saltarse el sistema puede ayudar a mejorar el modelo y evitar la manipulación de la API. En última instancia, la IA y los seres humanos pueden trabajar juntos para lograr mejores resultados.\n\nLa inteligencia artificial puede encargarse de tareas sencillas si está bien entrenada, y los humanos pueden aprender a aprovechar mejor la IA con el tiempo. Construir interfaces colaborativas que conviertan a la IA en un miembro del equipo en lugar de un algoritmo frío, permitiendo interacciones más naturales que pueden ayudarle a aprender mejor. La IA se volverá esencial en cualquier trabajo que involucre a partes interesadas que procesen datos humanos debido a la variabilidad involucrada.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matthew Pulsipher"],"link":"/episode-EDT130-es","image":"./episodes/edt-130/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a su hijo Matthew Pulsipher sobre la productización de la IA de toma de decisiones. Matthew recientemente ha modernizado y añadido la IA de toma de decisiones en su proceso de desarrollo de productos."},{"id":29,"type":"Episode","title":"Transformación Digital en el Gobierno Federal","tags":["compute","people","technology","automation","ai","cybersecurity","innovation"],"body":"\r\n\r\nMark ha trabajado con el gobierno federal durante más de 20 años y ha visto de primera mano cómo las agencias adoptan nuevas tecnologías para mejorar sus servicios a los ciudadanos. Una tendencia que ha notado es un enfoque en las experiencias de los clientes, como el uso de aplicaciones móviles de pasaportes por parte de Aduanas y Protección Fronteriza. A pesar de los desafíos de llevar a cabo iniciativas a gran escala en entornos en constante cambio, Mark ve muchas oportunidades para que el gobierno acelere su misión mediante la adopción de nuevas tecnologías.\n\nCOVID ha acelerado la innovación dentro del gobierno federal y ha expuesto lagunas en áreas como las conexiones VPN y la seguridad para los trabajadores remotos. Las agencias continúan innovando, centrándose en la tecnología en la nube y mejorando la experiencia del cliente. La educación y la comunicación son críticas para tomar decisiones informadas sobre dónde ubicar las cargas de trabajo e implementar tecnologías en la nube. A pesar de las preocupaciones de que la burocracia pueda frenar la innovación, los beneficios de la innovación siguen impulsando la innovación dentro del gobierno federal, incluidas las áreas de nube, computación perimetral, ciberseguridad y cadena de suministro segura.\n\n## Multi-Hybrid Cloud: Nube híbrida múltiple\n\nHay una creciente tendencia hacia el uso de nubes híbridas múltiples y los diversos desafíos que conlleva, como la seguridad y la gestión de datos. Se enfatizó la importancia de tener un plan bien estructurado para proyectos de transformación digital, con la necesidad de obtener resultados tempranos. Otra innovación es la informática de borde, que permite obtener información más rápidamente desde el borde sin necesidad de conexión. Varias organizaciones están trabajando actualmente en cómo aprovechar el poder de la informática de borde para mejorar sus servicios. Algunos casos de uso incluyen el monitoreo de sensores de agua en la agricultura y la optimización de las rutas de entrega de correo para el Servicio Postal de los Estados Unidos.\n\n## Automatización\n\nCuando se combinan la automatización y la ciberseguridad, pueden acelerar la capacidad de una organización de reaccionar ante amenazas, implementar nuevas capacidades y proporcionar mejores servicios a sus representados. La escasez de personal debido a la jubilación de los baby boomers requerirá la automatización para abordar este problema. Un ejemplo es cómo la automatización mejoró el proceso de inspección en la frontera para productos agrícolas y flores, lo que resultó en tiempos de procesamiento más rápidos y una vida útil más prolongada.\n\n## Ciberseguridad\n\nEl entorno actual de TI/TO es un panorama de amenazas en constante evolución, con actores malintencionados innovando continuamente. Intel está innovando y creando tecnología de seguridad de hardware que cifra los datos para protegerse contra actores maliciosos. La ciberseguridad es una de las principales preocupaciones del gobierno federal, como se señaló en el reciente comunicado de prensa del presidente Biden que enfatiza la protección de la infraestructura crítica como el principal problema de ciberseguridad. La gestión esencial de la infraestructura en los Estados Unidos, dada la estructura estatal única del país y la participación de diversas entidades como municipios locales y compañías eléctricas privadas. La Agencia de Seguridad de Infraestructura Crítica (CISA) guía la ciberseguridad y establece estándares para sistemas seguros a nivel federal, estatal y local.\n\n## Cadena de suministro segura\n\nOtra vulnerabilidad crítica expuesta por COVID-19 son los riesgos geopolíticos y las amenazas de ciberseguridad para la cadena de suministro. La cadena de suministro transparente de Intel permite a los clientes verificar la integridad de los componentes, lo cual es crucial para garantizar la seguridad del hardware y el software. Con estas medidas implementadas, Estados Unidos está tomando medidas para fortalecer su seguridad industrial y nacional.\n\nLa cadena de suministro no solo abarca los activos físicos; el software, el firmware y el hardware también desempeñan un papel importante en la seguridad de la cadena de suministro. Todos los componentes tienen vectores de ataque y requieren software seguro, materiales de construcción, verificación de componentes e integridad del dispositivo para su validación, una orden ejecutiva que abarca el software y los materiales, a la cual Intel se adhiere en sus nuevos procesadores. Intel proporciona tecnologías de seguridad de hardware y capacidades transparentes de cadena de suministro como parte de una arquitectura de confianza cero.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Mark Valcich"],"link":"/episode-EDT131-es","image":"./episodes/edt-131/es/thumbnail.jpg","lang":"es","summary":"En este episodio, Darren entrevista a Mark Valcich, director y gerente general del Sector Público Civil Federal en Intel. Los años de experiencia de Mark brillan mientras describe las tendencias actuales en la transformación digital en el gobierno civil federal."},{"id":30,"type":"Episode","title":"Administración de BareMetal definido por software","tags":["metify","sdi","technology","baremetal","heterogeneouscompute","compute"],"body":"\r\n\r\nCon más de dos décadas de experiencia en el espacio de los centros de datos, Ian comparte sus ideas sobre la optimización de la infraestructura, la automatización de la gestión de servidores y la simplificación de los diferentes componentes en un centro de datos. Por otro lado, Mike, quien trabajó previamente con IBM y Red Hat, se enfoca en la venta consultiva y en ventas lideradas por canales para comprender mejor los problemas y patrones de las empresas en la operación de sus centros de datos. Juntos, crearon Metify en 2020 para proporcionar soluciones de automatización para gestionar servidores, almacenamiento y dispositivos de red en centros de datos, a pesar del aumento de los servicios de nube pública como RWC Azure y GCP.\n\n## Automatización del centro de datos\n\nAutomatizar la gestión del centro de datos puede acercar la gestión local a una experiencia de nube pública al tiempo que reduce costos. Uno de los elementos vitales de esta transformación es la estandarización y los estándares abiertos como la especificación redfish de DMCA que hacen posible esta automatización. La ubicuidad de esta especificación en placas base de clase empresarial la hace accesible para todos los actores que deseen integrarla. Las presiones financieras y la demanda de los usuarios son esenciales para impulsar a los OEM a implementar estos estándares abiertos. En general, la automatización y la estandarización pueden ayudar a los centros de datos a competir con los proveedores de servicios en la nube en términos de costos operativos mientras mejoran las capacidades de gestión de servidores.\n\nLa eficiencia del centro de datos se puede mejorar utilizando herramientas que permiten la automatización e integración con marcos estándar. Los proveedores de hardware enfrentan presión debido a la comoditización de la tecnología, por lo que deben diferenciarse ofreciendo servidores de caja blanca similares pero de menor escala. Metify proporciona una única interfaz para gestionar dispositivos de cualquier fabricante, siempre que estén habilitados para BMC y cumplan con la especificación Redfish. Existen estándares emergentes para gestionar dispositivos pequeños a través de Redfish, pero la pregunta sigue siendo hasta qué punto se extiende la extensibilidad a las partes de la pila específicas de la red.\n\n## Nube híbrida\n\nEl surgimiento de estrategias de nube híbrida impulsa la forma en que las empresas gestionan sus centros de datos, la periferia y los entornos de nube. El crecimiento de la nube pública es significativo; sin embargo, también hay un crecimiento masivo en los espacios de nube privada. Metify proporciona una experiencia más similar a la nube desde el punto de vista de las operaciones, permitiendo a los administradores de sistemas gestionar su periferia, centro de datos o múltiples centros de datos mediante una API estándar. Mantener las API abiertas y estandarizadas es esencial para que los clientes utilicen herramientas de gestión familiares como TerraForm y Ansible. Las tecnologías de nube híbrida permiten a las empresas optimizar costos, gobernabilidad, seguridad y eficiencia.\n\nUno de los peligros de un estándar abierto puede ser vulnerable a violaciones de seguridad sin medidas adecuadas de comando y control. Metify aborda este problema colocando audibilidad, autorización, acceso y controles a los estándares abiertos para proporcionar un enfoque sistemático para administrar el hardware heterogéneo en bruto. El producto de Metify se enfoca en proporcionar un nivel de control para prevenir incidentes indeseables, y se integran con herramientas de gestión de flujo de trabajo para la automatización.\n\n## BareMetal SDI\n\nMuchas tecnologías modernas de centro de datos dependen de la virtualización como base de su plano de control de gestión. Sin embargo, la gestión de hardware en bruto sigue siendo una tarea tradicional y manualmente intensiva. Este enfoque único para la infraestructura de nube privada no depende de la virtualización. En su lugar, utiliza el enfoque de infraestructura definida por software para la gestión de hardware en bruto, lo que permite una combinación de hardware en bruto, máquinas virtuales y contenedores. Este enfoque permite a los administradores de sistemas gestionar de manera más efectiva la infraestructura subyacente y brindar una experiencia similar a la nube sin fricciones. Este enfoque puede admitir entornos informáticos heterogéneos, donde se pueden aprovechar CPU, GPU, FPGA, VPU y NPUs para múltiples flujos de trabajo. Con Redfish y una extensión del esquema, Metify puede controlar fácilmente nuevos dispositivos, y ven el ecosistema en expansión como algo increíblemente valioso para el desarrollo de productos.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ian Evans","Mike Wagner"],"link":"/episode-EDT132-es","image":"./episodes/edt-132/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a los fundadores de Metify, Ian Evans y Mike Wagner, sobre su enfoque único de gestión de infraestructura definida por software en metal desnudo utilizando el estándar Redfish."},{"id":31,"type":"Episode","title":"Lecciones en HPC Petróleo y Gas","tags":["compute","hpc","people","innovation","cloud"],"body":"\r\n\r\nKeith había trabajado en la industria durante toda su carrera y se unió a Intel en julio pasado. Ha liderado el grupo de Computación de Alto Rendimiento de Consolidated Resources desde 1999, donde empezaron con solo una décima parte de un teraflop de poder de computación. Sin embargo, Keith logró consolidar múltiples sitios y establecer su equipo como parte integral de las operaciones de exploración, desarrollo y producción de petróleo y gas de BP. Durante las últimas tres décadas de trabajo en computación de alto rendimiento en petróleo y gas, Keith ha tenido tres factores que influyen en la dirección de HPC: la computación en la nube, la escasez de talento humano y el equilibrio entre la colaboración y la ventaja del primer movimiento.\n\n## Nube o local\n\nKeith compartió su experiencia construyendo una instalación de HPC (Computación de Alto Rendimiento) para BP. La construcción llevó casi nueve años, con los primeros siete gastados en parchar la instalación existente y en desarrollar un caso de negocio para una dedicada. A pesar del surgimiento de la tecnología en la nube, descubrieron que el funcionamiento de su instalación era rentable debido al control sobre los recursos y la arquitectura personalizada. Estar en la industria del petróleo y gas presentaba desafíos únicos para manejar grandes cantidades de datos, pero tener una instalación dedicada les permitía centrarse en las necesidades de investigación de BP. Estos factores también son relevantes para otras industrias que trabajan con clústeres de HPC.\n\nLa gran pregunta para cualquier persona que dirige un clúster de HPC es si es posible ejecutar con éxito cargas de trabajo de HPC en servicios generales de la nube. Eso depende de las necesidades específicas del negocio y la escala de la carga de trabajo en cuestión. Mientras que algunas empresas pueden encontrar práctico colaborar con la nube para picos de demanda, otras requieren un clúster grande y totalmente utilizado para satisfacer sus necesidades. Factores como el rendimiento predictivo, la gravedad de los datos, el tamaño de los datos y el costo de las operaciones son factores esenciales en la ubicación de las cargas de trabajo de HPC.\n\n## Luchando contra la escasez de recursos\n\nAdquirir y retener talento en la computación de alto rendimiento es todo un desafío, especialmente a medida que la generación del baby boomer se retira. Keith sugiere que establecer relaciones con universidades e identificar individuos talentosos desde temprano es crucial para desarrollar las habilidades necesarias dentro de una organización. Las organizaciones deben pensar a largo plazo en la creación de equipos exitosos de HPC y construir a través de mentorías. En estos grupos heterogéneos, científicos, ingenieros de sistemas, matemáticos e ingenieros informáticos trabajan juntos durante años en lugar de meses.\n\nCreando compromisos a largo plazo mediante la inversión en prácticas, programas y becas es fundamental para crear un entorno laboral orientado a la carrera profesional. Keith comparte un desafío que enfrentaron al crear una visión compartida para el futuro de su equipo, pero encontraron éxito al crear una red de contactos fuera de BP que les ayudó a alcanzar sus objetivos.\n\n## Ventaja del primer movimiento, Compartir\n\nOtro aspecto crítico de HPC en Petróleo y Gas es tener la ventaja de ser el primero en moverse. Encontrar una nueva técnica para descubrir nuevas fuentes de energía puede ahorrar a la empresa miles de millones de dólares. Sin embargo, compartir información con competidores también puede conducir a descubrimientos que beneficien a la industria. Este es un equilibrio importante que los profesionales de HPC necesitan comprender para obtener el máximo provecho de su trabajo.\n\nKeith comparte una divertida historia de cómo sus amigos ayudaron a desarrollar una tecnología innovadora para comprender mejor las estructuras geológicas subsuperficiales del Golfo de México y encontrar petróleo y gas. A pesar de que la competencia estaba observando y cuestionando sus métodos, los experimentos de BP resultaron exitosos. Keith explica que todos están observando cuando haces algo nuevo en el campo.\n\nPor ejemplo, a menudo contrataban con empresas como PGS para adquirir datos y la solución innovadora de utilizar helicópteros y discos duros para entregar los datos recién adquiridos para su procesamiento. Los resultados fueron impresionantes, mostrando mejores resultados que los métodos tradicionales. Cuando se le preguntó sobre el futuro de HPC en la industria, Keith cree que la colaboración seguirá siendo necesaria, ya sea a través de clústeres compartidos o colaborando en referencias científicas fundamentales. A pesar de los desafíos, la industria de HPC es emocionante y hay grandes oportunidades para que las personas apliquen sus habilidades computacionales y disposición para resolver problemas desafiantes.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Keith Gray"],"link":"/episode-EDT133-es","image":"./episodes/edt-133/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a Keith Gray, ex director de computación de alto rendimiento en British Petroleum. Con más de 30 años de experiencia en la gestión de centros de HPC, Keith brinda una gran perspectiva sobre los desafíos, las mejores prácticas y el futuro de la computación de alto rendimiento."},{"id":32,"type":"Episode","title":"WaveForm AI","tags":["collectiongenerativeai","artificialintelligence","waveformai","datashapes","waveform","technology"],"body":"\r\n\r\nLogan es un antiguo oficial de inteligencia en el Departamento de Defensa y tiene una pasión por la IA y la robótica, lo que lo llevó a ser un reservista en la comunidad de defensa. Hizo la transición a DataShapes, donde tienen una tecnología única que resuelve necesidades críticas en la comunidad de defensa utilizando IA. Si bien la IA ha existido durante mucho tiempo, la gran cantidad de datos disponibles para entrenar modelos y el avance de los recursos informáticos han llevado al desarrollo de sistemas más inteligentes como los chatbots y los grandes modelos de lenguaje.\n\nLas técnicas actuales son ávidas de recursos y muy costosas de entrenar y construir una solución de inferencia de propósito general. Por ejemplo, ejecutar modelos de lenguaje grandes como ChatGPT puede costar hasta $3 millones al día, pero la IA está evolucionando rápidamente y recibiendo más atención que nunca. Una preocupación al usar redes neuronales es la incapacidad de auditar y explicar cómo la IA llega a un resultado. Hay desafíos sociales, políticos y legales para confiar en las decisiones tomadas por estas redes, especialmente en campos donde está en juego la vida humana. Sin embargo, la sociedad eventualmente superará estos desafíos y abrazará por completo la IA. El enfoque de DataShapes, que utiliza técnicas tradicionales de aprendizaje automático para resolver problemas en el análisis de datos, tiene plena auditabilidad y descubrimiento en sus modelos entrenados.\n\nDataShapes tiene un enfoque único para resolver problemas de manera rápida y eficiente en entornos con recursos limitados. Mientras que el entrenamiento tradicional de redes neuronales requiere una gran cantidad de datos etiquetados y puede ser frágil, DataShapes utiliza metodologías para aprender en tiempo real o casi en tiempo real. Su tecnología se centra en formas de onda y señales, y es auditable, lo que la hace ideal para su uso en entornos austeros donde las personas se ensucian y llevar una pila de servidores es imposible. Si bien las redes neuronales son excelentes en modelos de lenguaje y reconocimiento de imágenes, la empresa de Logan se enfoca en forma hipereficiente en formas de ondas y señales. Su enfoque es diferente y altamente efectivo.\n\nDetectar diferentes tipos de formas de onda y las relaciones entre ellas es el centro de esta nueva tecnología. Este enfoque detecta patrones más difíciles de falsificar en comparación con las técnicas tradicionales utilizadas por el Departamento de Defensa. Esto tiene el potencial de aplicaciones de su tecnología en la guerra electrónica, incluyendo la obtención de inteligencia y análisis. Además, la plataforma puede detectar, analizar y recopilar inteligencia, que se puede exportar a dispositivos periféricos. También se mencionó su función de detección de anomalías de aprendizaje automático, Infinite Loop, que establece una línea de base continua en base a los parámetros que el usuario final prescribe. La tecnología se puede utilizar en la industria automotriz, la salud y las industrias del entretenimiento, donde se podría incrustar en cada sensor.\n\nDataShapes tiene un producto llamado GlobalEdge, un agente inteligente que se encuentra en o detrás de los sensores para realizar operaciones de extracción, transformación y carga (ETL) de los datos que se recopilan. El componente de aprendizaje automático de GlobalEdge filtra los datos para proporcionar ideas relevantes y anomalías en tiempo real, reduciendo la cantidad de datos irrelevantes que se envían de vuelta a la sede central. El producto también se puede utilizar para la compresión de datos desde el borde hasta el centro de datos. El software puede reducirse hasta llegar a solo 47 K, lo que lo hace adecuado para una variedad de aplicaciones, incluida la detección de virus mediante ondas UV.\n\nPara obtener más información sobre DataShapes y su enfoque, visita https://www.datashapes.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Logan Selby"],"link":"/episode-EDT134-es","image":"./episodes/edt-134/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren Pulsipher dio la bienvenida a Logan Selby, el cofundador y presidente de DataShapes, donde discuten un enfoque único de la Inteligencia Artificial que va en contra de la tendencia."},{"id":33,"type":"Episode","title":"Inteligencia Artificial confiable y ética","tags":["ai","ethics","trustworthiness","deepfake","aicontent","aidetection"],"body":"\r\n\r\nA finales de la década de 2010, Microsoft lanzó a Tay como un chatbot de IA diseñado para aprender de sus conversaciones con usuarios en Twitter. Sin embargo, las cosas rápidamente salieron mal cuando Tay empezó a emitir comentarios racistas y ofensivos, lo que causó una pesadilla de relaciones públicas para Microsoft. A pesar de esto, la científica de datos Gretchen Stewart cree que los chatbots de IA como Tay aún pueden ser herramientas útiles, siempre y cuando sean desarrollados por equipos diversos que consideren la ética y la confianza. Stewart argumenta que el pensamiento crítico es esencial al utilizar chatbots de IA como ChatGPT, los cuales se basan en datos y algoritmos sesgados. Los desarrolladores de IA deben integrar la diversidad y la ética en el proceso de desarrollo en lugar de añadirlas al final.\n\n## Confianza\n\nLa IA tiene capacidades inmensas pero aún carece de los sentidos y experiencias humanas que pueden afectar la toma de decisiones. Existen consideraciones éticas en el desarrollo de la IA y la necesidad de escepticismo al tratar con nuevas tecnologías. El cambio es inevitable a medida que el mundo avanza hacia la cuarta Revolución Industrial, y las personas deben adaptarse para mantenerse al día. Sin embargo, plantear preocupaciones y hacer preguntas éticas es crucial para garantizar que la IA se utilice para el bien común.\n\nExisten peligros potenciales al depender de la inteligencia artificial (IA) como fuente de información. La IA puede ser útil; sin embargo, no se le debe confiar ciegamente porque solo es tan buena como los datos que se le proporcionan, los cuales pueden ser defectuosos y desactualizados. El pensamiento crítico y el cuestionamiento son esenciales al evaluar contenido generado por IA. Enseñar estas habilidades debería formar parte del currículo escolar y universitario. Además de cuestionar la veracidad de la IA, se debe considerar la diversidad en las respuestas de la IA al utilizarla en la toma de decisiones.\n\n## Ética\n\nA medida que la tecnología sigue avanzando, surgen nuevas preocupaciones éticas. Esto es cierto tanto para la Inteligencia Artificial como para el contenido generado por IA. Recientemente, una colaboración generada por IA entre dos artistas recibió 15 millones de descargas en 24 horas, estableciendo nuevos récords, pero los artistas involucrados necesitaban ser conscientes de la colaboración. Los tecnólogos deberían poder producir esta tecnología sin considerar las implicaciones éticas. Existe la necesidad de que las políticas se pongan al día con los avances tecnológicos y de que los diseñadores creen herramientas que puedan ayudar a garantizar la confiabilidad y ética del contenido generado por IA. Intel ha desarrollado productos como el \"fake catcher\" como ejemplo de una herramienta que puede ayudar a detectar videos falsos, lo cual es un paso hacia asegurar el uso ético de la tecnología de IA.\n\nCombatir el contenido generado por IA falsa se ha convertido en una industria en sí misma, y requiere transparencia en el desarrollo de la IA. Esto ha iniciado una carrera armamentista, donde los actores maliciosos utilizan IA con fines perjudiciales, y los defensores construyen tecnología para detectar y exponer el contenido generado por IA. Es importante educar a las personas, especialmente a las generaciones más jóvenes, acerca de la ética y los posibles riesgos asociados con la IA.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Gretchen Stewart"],"link":"/episode-EDT135-es","image":"./episodes/edt-135/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a Gretchen Stewart, Científica de Datos Principal del Sector Público en Intel, donde discuten la confiabilidad y ética de la inteligencia artificial."},{"id":34,"type":"Episode","title":"Datos resistentes en comunicaciones disruptivas","tags":["sabr","ddil","technology","edge","process","devops","security"],"body":"\r\n\r\nEn este episodio de podcast, Darren Pulsipher, arquitecto principal de soluciones de Intel para el sector público, es entrevistado por la presentadora invitada, la Dra. Anna Scott, sobre datos resistentes con comunicaciones disruptivas. La discusión se centra en la gestión segura y eficiente de datos en entornos con ancho de banda limitado y servicio interrumpido, permitiendo la inteligencia artificial y análisis de datos complejos en el borde. Darren habla sobre su experiencia trabajando en la nube de vehículos conectados de Toyota como base para resolver este problema y cómo se utilizaron arquitecturas comunes de gestión de datos para crear una solución. La revelación gradual de los desafíos por parte del cliente ayudó a identificar patrones de diseño que abrieron la arquitectura para una solución exitosa.\n\nA pesar de la conectividad intermitente y los nodos de borde dinámicos, el proceso de diseño de la arquitectura para realizar análisis de borde es complejo y difícil de articular, por lo que adoptaron un enfoque iterativo. En primer lugar, consideraron una solución que llevaría todos los datos a un solo lugar para su análisis, pero resultó poco práctica debido al volumen de datos en el borde. Mover las aplicaciones al borde parecía prometedor hasta que el cliente solicitó análisis agregados en todo el borde. Después de explorar patrones de diseño, optaron por utilizar el patrón de diseño de flujo de datos con un concentrador de publicación y suscripción para crear flujos de datos para consumidores y productores de forma dinámica. Si bien este enfoque utiliza raíces bien establecidas en TI, inicialmente el cliente tenía preocupaciones sobre la asignación de temas.\n\nEn el pasado, Darren tenía que crear de antemano un ecosistema de PubSubHub y estar familiarizado con todos sus componentes. Sin embargo, configurar flujos de datos con Kafka o Apache Pulse resultaba desafiante y, además, configurar la seguridad era aún más difícil. Utilizando su experiencia en DevSecOps, se ideó una solución que agrupa las definiciones de flujo de datos, las definiciones de entrada y salida, y las transformaciones de datos en un paquete consumible por el pipeline. Este paquete crea automáticamente flujos de datos en PubSubHub y configura todas las medidas de seguridad necesarias. El resultado simplificó la complejidad, ya que los desarrolladores solo necesitaban enfocarse en algoritmos o modelos de inteligencia artificial, mientras que todo lo demás se encargaba de manera genérica. El enfoque iterativo con los clientes ayudó a modificar arquitecturas e implementaciones a lo largo del camino.\n\nEl desafío clave con este tipo de arquitectura es trabajar con recursos limitados, como solo tener acceso a dos núcleos y 2 gigabytes de RAM. ¿Cuánto procesamiento de datos puede ocurrir en la periferia con recursos limitados? Una de las limitaciones con las que el equipo de arquitectura necesitaba trabajar era hacer que el administrador de transmisión SABR y las medidas de seguridad fueran lo más pequeños posible. Darren eliminó el código al mínimo indispensable y eliminó paquetes de terceros innecesarios. El objetivo era crear un gestor de transmisión ligero que pudiera ejecutarse en la periferia y ser portátil en diferentes entornos. La arquitectura resultante de Saber era escalable y adaptable, capaz de ejecutarse desde un reloj inteligente hasta un servidor Xeon grande.\n\nDarren y Anna discuten las dificultades de actualizar modelos de inteligencia artificial en un ecosistema distribuido con numerosas instancias de las mismas analíticas en funcionamiento. Para resolver este problema, SABR creó un flujo de datos de aprendizaje que conecta todas las instancias de las mismas analíticas y maneja las comunicaciones intermitentes, el almacenamiento en caché y el envío de deltas para actualizar los modelos. También desarrollaron un sistema de canal de datos utilizando el patrón de diseño de estrategia de política, lo que permite enviar diferentes canales con niveles variables de datos según reglas definidas por políticas. Este enfoque permite una transmisión de datos más eficiente, reduce la cantidad de datos procesados y aumenta la precisión de los modelos de inteligencia artificial.\n\nA continuación, se explica cómo operar en el entorno DDIL priorizando el envío de datos al enviar primero resúmenes e información histórica antes que datos en tiempo real. Es importante definir las expectativas del sistema y las políticas de comunicación desde el principio para asegurar la consistencia en todo el ecosistema. Se cuenta con un proceso de creación y activación de políticas fácil de usar basado en JSON y JavaScript que elimina la duplicación de esfuerzos y promueve la reutilización. La arquitectura tiene la capacidad de añadir rápidamente nuevas capacidades aprovechando los sabers existentes y las transformaciones de datos.\n\nLo último es la resiliencia del sistema al recuperarse dinámicamente de las interrupciones en el sistema. Existe un potencial para el uso dinámico y flexible de la red SABR. Incluso si un nodo se cae, es posible mover un SABR a otra caja y seguir recibiendo todos los datos entrantes. También es posible utilizar computación heredada ejecutando un SABR muy liviano en un sistema antiguo que recopila y transmite datos a la red SABR. No se debe pasar por alto la importancia de la seguridad. Todos los flujos de datos están encriptados y los procesos están configurados para establecer confianza y certificación de los SABRs para prevenir el suplantamiento y el espionaje de datos. En general, la red SABR ofrece una solución prometedora para procesar datos en el borde con flexibilidad y seguridad.\n\nEl aprendizaje clave es el enfoque arquitectónico iterativo que el equipo utilizó para descubrir los casos de uso y los problemas que los usuarios finales estaban experimentando. Al simular la arquitectura, pudieron encontrar defectos y obtener comentarios de los clientes. Además, el uso de patrones de diseño fue esencial para acelerar el enfoque arquitectónico. La utilización de la abstracción también proporcionó la capacidad de intercambiar diferentes tecnologías a lo largo del proceso arquitectónico. Darren cree que este enfoque ha facilitado la creación de soluciones fáciles de usar y aprovechar el conocimiento actual de los clientes.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Anna Scott"],"link":"/episode-EDT136-es","image":"./episodes/edt-136/es/thumbnail.png","lang":"es","summary":"En este episodio de podcast, Darren Pulsipher, arquitecto jefe de soluciones de Intel para el sector público, es entrevistado por la presentadora invitada, la Dra. Anna Scott, sobre datos resilientes con comunicaciones disruptivas."},{"id":35,"type":"Episode","title":"Renacimiento de la Nube Privada","tags":["cloud","computing","greenlake","hpe","multicloud","hybridcloud","technology"],"body":"\r\n\r\nHa habido una tendencia hacia la repatriación en el sector comercial, donde las empresas están trayendo cargas de trabajo y máquinas virtuales de vuelta de entornos de nube pública debido a razones financieras como los cargos de salida y la planificación presupuestaria para las cargas de trabajo hiperscaladas. La soberanía de los datos, las preocupaciones de seguridad, el rendimiento y los problemas de latencia también han llevado a más empresas a alejarse de la nube pública y volver a las tecnologías de nube privada. A pesar de este cambio, muchas promesas de la nube se han cumplido para proporcionar facilidad de uso, elasticidad y precios basados en el consumo. Con el fin de facilitar la transición entre nubes, la nube privada debe ofrecer facilidad de uso, portabilidad de cargas de trabajo y nuevos modelos de precios basados en el consumo que son muy demandados en las nubes públicas.\n\n## Control de Costos\n\nSam Ceccola, el CTO de HPE DoD, cree que es esencial comprender los riesgos y desafíos asociados con cada tecnología de nube privada, pública e híbrida. Y diseñar soluciones que aprovechen las fortalezas de cada tecnología. Aquí es donde entra en juego la oferta de producción de HPE, GreenLake. El modelo de contratación basado en consumo de servicios de GreenLake fue implementado por primera vez en 2005 y desde entonces ha evolucionado para incluir un portal de autoservicio para gestionar múltiples entornos de nube privada y pública. GreenLake permite a los clientes implementar recursos adicionales a través de la frontera de nube privada/pública según la demanda. También gestiona y visualiza costos complejos, como los cargos de salida y la presupuestación para cargas de trabajo a gran escala.\n\n## Gestión de datos\n\nOtra preocupación importante para la mayoría de los consumidores de nubes públicas es la gestión de datos, el gobierno y la soberanía. Sin una estrategia de datos cohesiva, muchas organizaciones se ven muy cargadas con los costos de egreso de datos al mover datos dentro y fuera de las nubes públicas. Existen varios enfoques diferentes para gestionar datos a través de los límites de la nube. HPE utiliza un enfoque de almacenamiento adyacente a la nube que reduce al mínimo la cantidad de datos que se mueven a la nube. El almacenamiento adyacente a la nube de GreenLake mantiene los datos en las instalaciones mientras se ejecutan cargas de trabajo de computación en la nube. Este enfoque garantiza la soberanía de los datos y reduce los costos de egreso y los problemas de latencia asociados con el movimiento de grandes cantidades de datos entre nubes.\n\n## Portabilidad\n\nOtro aspecto clave de las arquitecturas de multi-nube híbrida es la capacidad de mover fácilmente las cargas de trabajo entre nubes. Algunas organizaciones han optado por arquitecturas de microservicios basadas en contenedores sin estado para proporcionar la flexibilidad y portabilidad de las cargas de trabajo. Sin embargo, no todas las cargas de trabajo pueden ser fácilmente contenerizadas, especialmente aquellas que contienen aplicaciones con estado. En estos casos, se utilizan máquinas virtuales para permitir la movilidad de las aplicaciones de una nube a otra. Mover máquinas virtuales entre nubes puede ser problemático, ya que los proveedores de servicios en la nube utilizan hipervisores y CPUs diferentes que pueden no ser completamente compatibles. Comprender las limitaciones de estas migraciones es fundamental en las decisiones de compra de hardware tanto virtualizado como no virtualizado.\n\n## Seguridad\n\nGreenLake no limita a los clientes a la implementación en el sitio, ya que les permite aprovisionar cargas de trabajo en varios entornos de nube, incluyendo Amazon, Google y Azure. Además, GreenLake admite cargas de trabajo híbridas reales, donde una carga de trabajo puede ejecutarse tanto en el sitio como en entornos de nube hipescalares simultáneamente. La ejecución de cargas de trabajo a través de los límites de las nubes puede aumentar la exposición a los ataques cibernéticos. Comprender los modelos de seguridad de las diferentes tecnologías de nube pública y privada puede ser una tarea desalentadora y aumenta el capital humano necesario para diseñar y gestionar de manera efectiva una posición de seguridad sólida. La plataforma GreenLake maneja la seguridad a través de su motor de seguridad agnóstico, Security Central, que soporta la acreditación multinuebla y las arquitecturas de confianza cero para nubes independientes, gestionando la complejidad e integración de los modelos de seguridad.\n\nArquitecturar una solución de nube multi-híbrida no es trivial y requiere comprensión de las tecnologías de nubes privadas y públicas, sus limitaciones y sus fortalezas. Afortunadamente, existen organizaciones como HPE que han estado trabajando con arquitecturas de nube multi-híbrida durante varios años y han aprendido los entresijos de la gestión efectiva de cargas de trabajo en este nuevo entorno flexible.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sam Ceccola"],"link":"/episode-EDT137-es","image":"./episodes/edt-137/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a Sam Ceccola, CTO de DOD para HPE, sobre los nuevos modelos de negocio y tecnología que están cambiando la forma en que las organizaciones consumen la nube híbrida."},{"id":36,"type":"Episode","title":"Evolución de la Nube","tags":["cloud","computing","dell","multicloud","hybridcloud","technology"],"body":"\r\n\r\nLa evolución del cómputo en la nube es uno de los avances tecnológicos más significativos de la última década. La nube está pasando de ser un concepto simple a un sistema complejo y ubicuo que ahora domina las operaciones y estrategias empresariales. Como nos dice Ken White de Dell Technologies, la nube ya no es una ubicación única o una solución única para todo. En cambio, es todo, en todas partes.\n\nCon gigantes del hiperescalar como Amazon y Azure expandiéndose constantemente, la nube representa una experiencia consistente en todas las plataformas con previsibilidad en rendimiento y costos. Sin embargo, existen tarifas ocultas, especialmente al mover datos dentro y fuera de la nube. Es esencial que las empresas que consideran la nube adopten por completo una mentalidad de nube preocupada por la consistencia, previsibilidad, flexibilidad, rendimiento y seguridad. Si bien algunos ejecutivos se sienten atraídos por la nube debido a su potencial para reducir costos y gestionar TI, es esencial recordar que no puede resolver todos los problemas y no es un camino de un solo sentido. La nube puede mejorar en última instancia las operaciones, aumentar la eficiencia y ofrecer una mejor experiencia tanto para las empresas como para los clientes, siempre que se adopte una mentalidad operativa bien planificada.\n\nSi estás en el mundo de los negocios, probablemente hayas oído hablar sobre la nube, pero ¿has oído hablar sobre la mentalidad de la nube? Es una forma de acercarse a la tecnología que permite la adaptabilidad, portabilidad y previsibilidad en cuanto a cargas de trabajo y costos en la nube. Esto significa que puedes desarrollar aplicaciones portátiles en diferentes ecosistemas y elegir dónde se alojan las cargas de trabajo según los requisitos empresariales, no solo las limitaciones técnicas. También significa que puedes predecir y controlar los costos, un alivio bienvenido en un entorno tecnológico en constante cambio.\n\nPero cambiar mentalidades es desafiante, especialmente para las organizaciones acostumbradas a una forma específica de operar. Aquí es donde entran en juego la capacitación y el compromiso con el aprendizaje continuo. Las organizaciones que adoptan con éxito la mentalidad en la nube priorizan una experiencia de usuario positiva, están dispuestas a adaptarse a nuevas arquitecturas, y aprenden y cambian a medida que la tecnología evoluciona.\n\nLa adopción de la mentalidad de la nube no significa que tengas que despedir a todo tu personal de TI y trasladar todo a la nube. En cambio, se trata de capacitar a tu fuerza laboral y replantear tu filosofía organizacional. Con una capa de gestión adecuada y un enfoque en la flexibilidad y agilidad, las organizaciones pueden aprovechar los beneficios de la nube para sus negocios y mejorar las experiencias de sus usuarios finales, ya sean desarrolladores o clientes.\n\nConsiderando una estrategia en la nube, debes entender que es posible que experimentes fallos. Mover aplicaciones a la nube no siempre es sencillo y puede tener un costo. Sin embargo, tus posibilidades de éxito son mucho mayores con la adecuada consistencia y gobernanza.\n\nLa consistencia no necesariamente implica una única interfaz, pero sí implica tener un proceso estándar para gestionar y desplegar cargas de trabajo y infraestructura. También incluye la coherencia con las regulaciones, procesos y flujos de trabajo. Este nivel de consistencia garantiza que todos en su organización comprendan cómo gestionar la nube.\n\nLos fracasos en la adopción de la nube solo a veces son evitables, pero la repatriación puede ser una opción para llevar tu carga de trabajo de vuelta a una solución local. Sin embargo, esta opción también puede tener costos, por lo que es esencial sopesar los beneficios a corto y largo plazo.\n\nLos contenedores también pueden marcar la diferencia en la adopción de la nube al eliminar problemas asociados con la migración de aplicaciones a la nube. Pero recuerda, no existe una solución mágica que resuelva todos los problemas de adopción de la nube.\n\nEn general, la clave para el éxito con una estrategia en la nube es enfocarse en la consistencia y comprender los costos involucrados en el traspaso a la nube. Con estos factores en mente, tus posibilidades de éxito son mucho mayores.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sam Ceccola"],"link":"/episode-EDT138-es","image":"./episodes/edt-138/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a Ken White de Dell Technology sobre cómo la tecnología en la nube es más que una tecnología, sino un proceso y un cambio cultural en las organizaciones."},{"id":37,"type":"Episode","title":"Análisis Logístico Resiliente.","tags":["data","analytics","artificialintelligence","pathway","technology"],"body":"\r\n\r\n¿Estás interesado en aprender sobre cómo la transformación digital está afectando a la logística? En este episodio de \"Abrazando la Transformación Digital\", el invitado especial Adrian Kosowski, director de producto en Pathway, discute las analíticas resilientes en logística. Pathway se enfoca en estudiar sistemas del mundo real desde una perspectiva de computación distribuida, y específicamente trabajan con datos en el ámbito de la logística y el transporte. Uno de los mayores desafíos en este campo es la agregación de datos a gran escala y darles sentido, es aquí donde entran en juego las analíticas de aprendizaje automático. Kosowski también destaca que la logística es un mercado altamente concentrado, controlado por solo un puñado de empresas, lo que hace que incluso pequeñas mejoras en los procesos sean increíblemente valiosas para la economía mundial. Sin embargo, recopilar datos desde los extremos más remotos, como contenedores en medio del océano, presenta sus propios desafíos, como optimizar la energía y la comunicación para dispositivos IoT alimentados por batería. En resumen, si estás interesado en la intersección de la logística y la transformación digital, este episodio brinda conocimientos valiosos sobre los desafíos y las oportunidades en este campo.\n\nEl uso de dispositivos de Internet de las cosas (IoT) en la industria logística y de la cadena de suministro puede beneficiar en gran medida a las empresas con una visibilidad de extremo a extremo y una capacidad mejorada de análisis. Sin embargo, existen varios desafíos asociados con el uso de estos dispositivos que deben abordarse.\n\nUn desafío principal es la estabilidad y confiabilidad de estos dispositivos periféricos. En caso de fallo o colapso de un dispositivo, las empresas deben contar con una forma de optimizarlos sin que ellos adivinen o realicen de manera incorrecta. Esto es especialmente crítico para sistemas en tiempo real que se utilizan para eventos que ocurren en orden.\n\nOtro desafío es la precisión de los datos recopilados por los dispositivos IoT. Algunos datos pueden ser inferidos, y se puede requerir un análisis contextual para interpretar el significado de un punto de datos dado y distinguir los problemas de medición de los problemas de proceso. Datos inexactos o a destiempo pueden conducir a riesgos en la cadena de suministro y dificultar la optimización de la red de transporte.\n\nSin embargo, el uso generalizado de dispositivos de IoT puede llevar a tener visibilidad y observabilidad de extremo a extremo de los procesos en toda la cadena de suministro. Esto puede ayudar a las empresas a optimizar sus procesos y adoptar un enfoque proactivo. Esto puede beneficiar a todos los actores de la cadena de suministro, desde proveedores de transporte y logística hasta minoristas y fabricantes.\n\nSi bien no existe un grupo o segmento específico al que las empresas deban apuntar para la adopción de dispositivos IoT, la adopción generalizada y la cooperación entre todos los actores pueden llevar a beneficios significativos para la cadena de suministro global.\n\nEl Pathway fue desarrollado para abordar las deficiencias en las tecnologías de transmisión de datos existentes y proporcionar una herramienta para canalizaciones de análisis avanzadas sobre flujos de datos. Una de las características clave que distingue a Pathway es la facilidad para describir lógica como si fuera destinada a un sistema por lotes, al mismo tiempo que se asegura de que funcione en un sistema en tiempo real con datos desordenados.\n\nPathway fue desarrollado teniendo en cuenta los datos de IoT, pero también puede manejar datos de monitoreo de video, monitoreo de rendimiento del servidor, monitoreo de registros y otras entidades físicas. Esto permite manejar las complejidades de los datos cuando se trata de detección de anomalías, alertas con datos fuera de orden y datos con series temporales y elementos geoespaciales de manera neutral en la nube.\n\nOtra característica importante de Pathway es la capacidad de utilizar scripts de Python para analizar datos en streaming tanto en tiempo real como por lotes. Esto significa que los científicos de datos e ingenieros pueden desarrollar sus tuberías de análisis en el entorno de desarrollo normal al que están acostumbrados, y trabajar de manera conveniente con el sistema de streaming. Además, Pathway permite considerar una cantidad mucho mayor de historial al ejecutar cálculos, lo cual es una gran diferencia en comparación con los procesadores de streaming livianos que solo manejan pequeñas cantidades de datos.\n\nEn general, Pathway ofrece una solución de conexión para organizaciones que necesitan combinar el procesamiento de datos por lotes y en tiempo real, y proporciona una forma de agregar valor a los datos al agregar estructura e información de alcance para su uso posterior en inteligencia empresarial y análisis.\n\nLa tecnología de transmisión de datos se está volviendo cada vez más importante a medida que las empresas buscan tomar decisiones más rápidas y responder más rápidamente a las necesidades de los clientes. Cuando se utiliza la transmisión de datos, se puede detectar y responder rápidamente a tendencias, anomalías y otra información importante. Pathway ofrece un conjunto de productos diseñados específicamente para trabajar con datos de series temporales, datos de logística y transmisión de datos. Ofrecen ejemplos e información para desarrolladores en GitHub, así como una comunidad vibrante en Discord.\n\nLa transmisión de datos es una tecnología crítica para las empresas en una amplia variedad de sectores. Por ejemplo, se puede utilizar en logística para optimizar rutas y predecir cuándo llegarán los envíos. En finanzas, puede ayudar a detectar fraudes y predecir tendencias del mercado antes de que se hagan ampliamente conocidas. En transporte, se puede utilizar para monitorear el rendimiento de motores y vehículos en tiempo real.\n\nSi estás interesado en aprender más sobre la transmisión de datos y cómo puede beneficiar a tu negocio, asegúrate de visitar el sitio web y la comunidad de Pathway [www.pathway.com]. Con las herramientas adecuadas y la experiencia necesaria, puedes aprovechar esta tecnología poderosa para mejorar tus operaciones y mantenerte a la vanguardia.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Adrian Kosowski"],"link":"/episode-EDT139-es","image":"./episodes/edt-139/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a Adrian Kosowski, CPO de Pathway, sobre su capacidad única para manejar datos logísticos desde el borde en entornos DDIL con análisis en tiempo real."},{"id":38,"type":"Episode","title":"Iniciando tu Transformación Organizativa para Convertirte en Centrado en Datos","tags":null,"body":"\r\n\r\n## Operativizando el sistema de análisis\n\nLo primero que hay que entender sobre el proceso de operativizar el análisis de datos es que es complejo. Algunas personas piensan que los datos son la nueva electricidad y funcionan en un sistema similar. Considera este ejemplo: en un sistema eléctrico simple, hay una fuente de energía y algunos cables que van hacia una bombilla. Siempre que haya una fuente de energía y los cables tengan alta integridad, puedes apagar la luz, y luego seis meses después regresar y encender el interruptor y saber que la bombilla se encenderá de nuevo.\n\nUn sistema analítico no funciona de esta manera. Hay muchos tipos de \"cortocircuitos\" y variables que pueden causar fallos, como la entropía. Es importante tener a las personas adecuadas en tus equipos que puedan trabajar y entender todos los diferentes tipos de fallas que pueden ocurrir, o de lo contrario no obtendrás información correcta o útil.\n\nSus datos deben ser formateados de cierta manera para los algoritmos de aprendizaje automático. El algoritmo debe entender qué representa los datos para poder atender adecuadamente el patrón. A menos que los datos sean cuidadosamente formateados, existe el riesgo de ingresar basura y obtener resultados insatisfactorios. Aquí es donde son importantes sus ingenieros, científicos de datos y otros arquitectos de datos de apoyo.\n\n## Haciendo las preguntas correctas\n\nOtra parte del sistema es asegurarse de tener las preguntas de negocio correctas. Una organización debe proporcionar preguntas fundamentales acerca de lo que intentan obtener de los datos: ¿Qué conocimientos necesita nuestro negocio? ¿Cómo podemos pintar una imagen más clara de cómo se ve realmente nuestro panorama? ¿Qué datos nos permitirán hacer eso?\n\nMuchos ejecutivos aman la idea de hacer análisis, IA y big data, pero no saben cómo empezar. El primer paso es que los ejecutivos se hagan cargo de lo que quieren que los datos hagan por la organización. ¿Qué valor empresarial va a surgir de los datos? ¿Qué tipo de datos ya tenemos? ¿Qué datos podemos crear que pinten una imagen más completa de lo que está sucediendo? ¿Cómo se ve el éxito? El enfoque debe estar primero en los cimientos organizativos.\n\n## Recopilación de datos, preparación y análisis de información.\n\nDespués de responder esas preguntas, el siguiente paso es esbozar lo que se necesita lograr y comenzar a desglosarlo en partes más pequeñas. Las organizaciones deben analizar sus fuentes de datos actuales, determinar su confiabilidad e identificar lo que falta, seguido de la recolección y preparación de datos.\n\nCon los datos en mano, es tiempo de crear la historia. Esto implica no solo a científicos de datos, sino también a expertos en el campo y personas de negocios y marketing. La contribución de estos diferentes grupos asegurará que los datos añadan valor al negocio en lugar de convertirse solo en un ejercicio académico o un experimento aislado. A partir de los datos, en última instancia se busca obtener una visión valiosa y confiable que ayude a mejorar el negocio en lugar de simplemente recolectar datos que sean convenientes pero no accionables.\n\nAhora que has recopilado los datos, los has analizado y tienes algunas ideas, ¿cómo los operacionalizas?\n\n## Ejemplo de éxito en Analítica Operativa\n\nUn ejemplo de la fabricación es una buena manera de mostrar cómo los datos funcionan exitosamente a través de la cadena de suministro. Comienza con el valor empresarial fundamental de maximizar el rendimiento de tu producto. ¿Qué fuentes de datos podrían decirte si tienes productos rotos o deformados? Las cámaras al final de las líneas de ensamblaje podrían crear un modelo de aprendizaje profundo que detecte o prediga si un producto está roto o deformado. Luego, un científico de datos podría entrenar un modelo basado en productos buenos y malos para mejorar el rendimiento.\n\nEste tipo de datos operacionalizados se puede aplicar a varias áreas clave que agregan valor a una organización. El control de calidad, como asegurarse de que la materia prima cumpla con las especificaciones, es un ejemplo. El mantenimiento predictivo de maquinaria y el cumplimiento normativo, como en los requisitos de peso para un producto alimentario, son otros ejemplos.\n\nAl aprovechar los datos en tiempo real sobre los productos, la calidad o los problemas regulatorios, no solo estás creando un mayor rendimiento del producto, sino también minimizando costosas tareas humanas, como las inspecciones al final de una línea de ensamblaje. Estos empleados pueden ser redirigidos a tareas de mayor valor. También estás minimizando el desperdicio y creando una experiencia del consumidor más consistente. Además, estás protegiendo el negocio al reducir los riesgos regulatorios.\n\nEsta es una situación en la que todos ganan. Cuanto más la organización sabe qué quiere obtener de los datos, qué datos recopilar y cómo utilizarlos basándose en una estrategia empresarial, más exitosa será.\n\n## Más por venir\n\nEste es el primero de seis episodios que hablan sobre la organización centrada en los datos. Esperamos que nos acompañes en esos futuros podcasts. ¿Quieres más? Visita el sitio web de Sarah.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT14-es","image":"./episodes/edt-14/es/thumbnail.png","lang":"es","summary":"El episodio de hoy trata sobre cómo poner en marcha tu organización para que se vuelva centrada en los datos y el valor que esto puede aportar. El invitado especial de Darren es Sarah Kalicin, la científica de datos principal para los centros de datos en Intel."},{"id":39,"type":"Episode","title":"Verificación de Antecedentes de su Código Fuente Abierto","tags":null,"body":"\r\n\r\nSi eres un desarrollador de software, conoces la sensación de orgullo que viene con crear un paquete o herramienta popular que mucha gente encuentra útil. Sin embargo, esta popularidad a veces puede atraer la atención de atacantes que buscan vulnerabilidades para explotar.\n\nEn un podcast reciente, el ingeniero de software Jay Phelps compartió su experiencia al descubrir una vulnerabilidad en un paquete ampliamente utilizado que él mismo creó. Después de darse cuenta del impacto potencial de la vulnerabilidad y la gran cantidad de instancias del paquete en uso, Phelps trabajó rápidamente para solucionar el problema y evitar que los atacantes lo explotaran.\n\nEste escenario resalta la importancia de la vigilancia para los desarrolladores de software, especialmente aquellos que crean paquetes o herramientas populares. Si bien puede ser tentador disfrutar del éxito de un producto ampliamente utilizado, es crucial recordar que la popularidad también puede atraer a los atacantes. Realizar revisiones y actualizaciones regulares para detectar y solucionar cualquier vulnerabilidad puede ayudar a proteger a los usuarios y prevenir la explotación.\n\nComo desarrollador de software, es importante abordar tu trabajo con orgullo y precaución. Si bien es excelente contribuir a la sociedad con tus creaciones, no olvides priorizar la seguridad y protección de tus usuarios. Mantente alerta y mantén tus paquetes actualizados para evitar que los atacantes exploten vulnerabilidades.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Michael Mehlberg"],"link":"/episode-EDT140-es","image":"./episodes/edt-140/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a Michael Mehlberg sobre cómo aumentar la confianza en el software libre a través de la verificación de antecedentes en las comunidades de código abierto."},{"id":40,"type":"Episode","title":"De Neurología a la Computación Neuromórfica","tags":["neurology","neuromorphic","security","ai","cybersecurity"],"body":"\r\n\r\nComo neuróloga infantil, la Dra. Follett enfatiza la importancia de comprender los procesos de desarrollo del cerebro y su notable capacidad para superar incluso lesiones catastróficas. Ella comparte cómo su extensa investigación implicó el estudio de modelos de ratas y cultivos celulares para obtener conocimientos sobre el desarrollo de los niños y buscar formas de ayudarles a obtener los mejores resultados, a pesar de lesiones o enfermedades. Escuchar las ideas y experiencias de la Dra. Follett permite comprender mejor cómo la neurología es fundamental para entender los procesos cerebrales y cómo podemos apreciar y apoyar mejor el desarrollo cerebral, especialmente en los niños. La Dra. Follett también comparte su inesperada aparición en un espectáculo de alta tecnología, donde utilizó su experiencia en neurología para ayudar a un hombre que experimentaba convulsiones durante una conferencia magistral. Este podcast destaca el alcance potencial de la neurología más allá de las instituciones médicas y el papel esencial de los neurólogos en la comprensión de la complejidad del cerebro humano.\n\n¿Alguna vez has necesitado ayuda para entender una tecnología o producto complejo, aunque solo necesitabas saber cómo usarlo? La Dra. ha descubierto que sus habilidades para explicar problemas médicos a los pacientes se traducen bien en la explicación de tecnologías complejas a no expertos. Su trabajo como neurocientífica estudiando el desarrollo del cerebro la llevó a ella y a su esposo a comenzar una startup de alta tecnología que se dedica a la computación neuromórfica, llamada Lewis-Rhodes Labs. Uno de sus productos, Extreme Search, utiliza un procesador neuromórfico para buscar a través de grandes cantidades de datos, imitando cómo un cerebro reconoce y procesa rápidamente la información.\n\nEl Dr. Follet enfatiza que existen enfoques mejores que imitar un cerebro al crear tecnología. Si bien los cerebros cometen miles de errores diariamente, no necesariamente queremos que nuestras computadoras hagan lo mismo. En cambio, podemos tomar cuidadosas lecciones de cómo funcionan los cerebros y aplicarlas para crear tecnología más eficiente y efectiva. Esta experiencia resalta la importancia de habilidades interdisciplinarias y pensar más allá de un campo específico. Al encontrar habilidades que se transfieren entre diferentes áreas e industrias, podemos aportar perspectivas y soluciones únicas a problemas complejos.\n\nLa tecnología de búsqueda extrema es un avance en la ciberforensia y el análisis en tiempo real que resuelve el desafío de cribar Petabytes de datos no estructurados en tiempo récord utilizando hardware y software de vanguardia. La tecnología de búsqueda extrema ofrece un alto rendimiento, bajo consumo de energía y un flujo constante, y está diseñada a imagen y semejanza del cerebro humano para el procesamiento de datos de alto rendimiento.\n\nA diferencia de los métodos tradicionales de búsqueda de datos que requieren mover los datos, la tecnología de búsqueda extrema permite búsquedas en el lugar que eliminan todos los cuellos de botella de la red. La tecnología es adecuada para la ciberforensia, la ciberseguridad, el descubrimiento legal y la búsqueda de datos empresariales. La tecnología de búsqueda extrema es sencilla para los usuarios y no requiere un nuevo lenguaje ni la preidentificación de patrones, sino que utiliza expresiones regulares para realizar búsquedas ad hoc de cualquier dato descrito en el texto.\n\nLa tecnología de búsqueda extrema realiza búsquedas sensibles en dispositivos de almacenamiento y proporciona análisis en tiempo real para identificar posibles amenazas en milisegundos. Combinada con métodos de detección tradicionales, la tecnología puede detectar rápidamente amenazas persistentes avanzadas, virus, adware, troyanos, gusanos, rootkits y otro tipo de malware. El uso de la tecnología de búsqueda extrema no se limita a la ciberforensia. Otros campos de investigación, como la investigación genómica o cualquier campo de datos no estructurados, pueden beneficiarse de la capacidad de la tecnología de búsqueda extrema para buscar grandes cantidades de datos diversos en tiempo récord.\n\nMuchas organizaciones necesitan ayuda para encontrar patrones o información relevante dentro de sus datos. O bien tienen bases de datos muy extensas y les resulta difícil encontrar algo debido a la gran cantidad de información. Muchos científicos de datos recurren a índices para reducir el tiempo de búsqueda de información. Esto funciona bien cuando se sabe lo que se quiere al recopilar o almacenar los datos. Sin embargo, muchas organizaciones se enfrentan a datos opacos que no encajan en una estructura predefinida. En este caso, las búsquedas de fuerza bruta en petabytes de datos pueden llevar semanas para encontrar patrones comunes de datos no determinados previamente.\n\nLa tecnología de búsqueda extrema ayuda a dar visibilidad a nuevas áreas de datos, permitiendo un análisis y análisis mejorados que comienzan con la búsqueda y pueden avanzar más rápido si los datos se transforman en las piezas requeridas. Esto es particularmente útil al tratar con datos de atención médica, donde hay grandes cantidades de datos estructurados, pero la información se ajusta fuera de cualquier estructura de base de datos.\n\nPara obtener más información sobre la tecnología Extreme Search y los enfoques para la transformación digital, visita Lewis-Rhodes.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Pamela Follet"],"link":"/episode-EDT141-es","image":"./episodes/edt-141/es/thumbnail.png","lang":"es","summary":"En este episodio de podcast de Abrazando la Transformación Digital, la Dra. Pamela Follett, una neuróloga y co-fundadora de Lewis Rhodes Labs, comparte su experiencia y conocimiento en el campo de la neurología, específicamente en cuanto a la investigación sobre el desarrollo del cerebro en la primera infancia."},{"id":41,"type":"Episode","title":"Protección de datos con Computación Confidencial.","tags":["cybersecurity","technology","sgx","confidentialcomputing","dataprotection"],"body":"\r\n\r\nen lugar de garantizar que sólo aquellos que están autorizados puedan acceder a ciertos datos o sistemas. Sin embargo, incluso con esos controles implementados, aquellos con privilegios elevados, como administradores de nube o administradores de sistemas, aún tienen acceso a datos y sistemas sensibles. Aquí es donde entra en juego la computación confidencial, ya que añade una capa adicional de protección contra insiders maliciosos o aquellos que puedan causar una brecha de manera accidental.\n\nEl acceso controlado se refiere a limitar y monitorear el acceso a datos o sistemas sensibles en función de protocolos de autorización y autenticación. El acceso privilegiado se refiere a cuando alguien tiene privilegios elevados o derechos de administrador que le permiten acceder a datos o sistemas sensibles más allá de lo que está normalmente autorizado o controlado.\n\nLa computación confidencial ayuda a eliminar este acceso privilegiado mediante la creación de un entorno de ejecución basado en hardware o un entorno de ejecución confiable que impide el acceso no autorizado o la modificación de aplicaciones y datos. Mediante el uso de la computación confidencial, las organizaciones pueden mantener el control sobre sus datos confidenciales al aprovechar la computación en la nube, la computación en el borde y la computación multiparte sin comprometer la seguridad. Es una capacidad importante que asegura la protección de datos sensibles para las organizaciones en la era digital.\n\nLa computación confidencial es una nueva tecnología que permite a los usuarios mantener el control sobre sus datos incluso cuando están almacenados en servidores de terceros, como los utilizados en la computación en la nube. Con la computación confidencial, los usuarios pueden cifrar sus datos mientras están en memoria, protegiéndolos incluso de usuarios privilegiados y administradores deshonestos. Esto significa que incluso si un atacante obtiene acceso al servidor, encontrará que los datos están en un estado cifrado, por lo tanto, seguros frente a miradas indiscretas. La computación confidencial es especialmente importante para datos sensibles, como registros médicos o información financiera.\n\nLa computación confidencial se basa en el cifrado a nivel de hardware, lo cual brinda una protección mucho más sólida que el cifrado basado en software. Dado que el cifrado a nivel de hardware se implementa a nivel del procesador, no requiere software o controladores adicionales y, por lo tanto, genera una mínima sobrecarga de rendimiento en el sistema. La computación confidencial también es muy fácil de usar, ya que funciona de manera transparente con el software y las aplicaciones existentes.\n\nLos beneficios de la computación confidencial son muchos. Dado que los datos están encriptados mientras se procesan, la información sensible no es visible para terceros, lo que la mantiene privada y segura. La computación confidencial se puede utilizar no solo en la nube, sino también en entornos de computación periférica. A medida que continuamos viendo un aumento en la cantidad de datos generados y almacenados, la necesidad de entornos informáticos seguros y confiables se vuelve aún más importante. La computación confidencial es una de las tecnologías que pueden ayudar a lograr estos objetivos.\n\nLa informática confidencial es un componente esencial de una arquitectura de confianza cero. Un marco de confianza cero opera bajo la suposición de que un ciberataque puede ocurrir en cualquier momento dado y, por lo tanto, no hay tal cosa como un recurso confiable. Cada usuario y dispositivo deben autenticarse repetidamente antes de cada interacción, independientemente de si ya han sido verificados. La informática confidencial proporciona una capa adicional de seguridad, ya que tiene como objetivo proteger los datos de ciberataques y violaciones de seguridad asegurándose de que solo los lugares necesarios tengan acceso a ellos. Esto se logra pasando por alto el sistema operativo y la pila de la nube y hablando directamente con el chip, que administra el acceso a la memoria.\n\nIntel ha estado a la vanguardia de la computación confidencial con el desarrollo de SGX y tDCS. Estas tecnologías entran dentro del amplio conjunto de tecnologías que potencian la privacidad y tienen como objetivo ofrecer soluciones en este ámbito. La encriptación completamente homomórfica es otra solución que aborda el problema desde una perspectiva puramente criptográfica al mantener los datos siempre encriptados.\n\nLo que hace único a la informática de vanguardia y a los entornos de ejecución seguros es que están disponibles ampliamente en cargas de trabajo de producción convencionales hoy en día, con muy poco impacto en el rendimiento. Poder tomar una carga de trabajo nativa, incluso en su formato original, y ejecutarla dentro de un entorno cifrado y aislado es una herramienta poderosa para las organizaciones para proteger sus datos.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Anna Scott","Ibett Acarapi","Jesse Schrater"],"link":"/episode-EDT142-es","image":"./episodes/edt-142/es/thumbnail.png","lang":"es","summary":"En este episodio, la Dra. Anna Scott entrevista a Jesse Schrater e Ibett Acarapi sobre cómo proteger los datos utilizando computación confidencial."},{"id":42,"type":"Episode","title":"Casos de uso en cómputo confidencial","tags":["cybersecurity","sgx","confidentialcomputing","ai"],"body":"\r\n\r\nLa computación confidencial es una tecnología revolucionaria que permite procesar datos sin exponerlos. Es ventajosa en inteligencia artificial, donde se involucran datos sensibles en el desarrollo e implementación de modelos. Protege los modelos de IA y datos confidenciales, permitiendo el desarrollo de mejores modelos de entrenamiento y conjuntos de datos diversos. La colaboración entre expertos y empresas también es accesible a través de la computación confidencial, lo que permite la creación de modelos más grandes y protege los datos propietarios.\n\nLa tecnología está disponible hoy en día y puede ser utilizada por las empresas a través de varios proveedores, incluyendo Azure, GCP, IBM y Alibaba. Los fabricantes de equipos originales (OEM) que ofrecen servidores para uso local también tienen cajas SGX que pueden ser aprovisionadas con SGX. No es necesario tener experiencia en el desarrollo de cómputo confidencial para aprovechar las capacidades de cómputo confidencial, ya que existen muchas soluciones que permiten migrar cargas de trabajo nativas a través del modelo de desplazamiento o soluciones en la nube.\n\nPara comenzar con la informática confidencial, las empresas pueden incluir el requisito en su RFP y contactar a un proveedor que lo ofrezca para incorporarlo en sus soluciones. Muchos proveedores ya ofrecen soluciones de informática confidencial, e Intel está dispuesto a participar. La informática confidencial proporciona una nueva forma de gestionar datos, permite a las empresas aprovechar los beneficios de la nube o la informática multiparte en datos sensibles y, en última instancia, brinda un entorno más seguro para que las empresas manejen información confidencial.\n\nLa computación confidencial es una tecnología pionera que está creciendo rápidamente en popularidad, dada la creciente cantidad de violaciones de datos que se perpetran en el mundo interconectado de hoy en día. La computación confidencial te permite aislar datos sensibles dentro de enclaves o \"contenedores\" de computación seguros, los cuales son difíciles de acceder incluso si un atacante obtiene acceso al sistema anfitrión. La computación confidencial se trata de agregar una capa adicional de seguridad a los entornos informáticos que requieren la protección robusta de información sensible o propiedad intelectual (IP).\n\nComo explica el podcast, hay muchas formas de implementar la computación confidencial, desde aprovechar las herramientas proporcionadas por los proveedores de servicios en la nube (CSP, por sus siglas en inglés) hasta utilizar los servicios de proveedores de seguridad de software. También es evidente que cada vez más empresas están comenzando a adoptar la computación confidencial para proteger su propiedad intelectual y los datos sensibles de sus clientes.\n\nSin embargo, la tecnología aún está en pañales y su verdadero potencial aún está por descubrirse. Los expertos coinciden en que una de las áreas más emocionantes para observar es el surgimiento de la \"computación confidencial\" inteligente. Esto se refiere a un enfoque que aprovecha la inteligencia artificial y el aprendizaje automático para mejorar la eficacia y flexibilidad de la computación confidencial.\n\nOtra área a tener en cuenta incluye el desarrollo de herramientas avanzadas de detección y análisis de amenazas que aprovechan la computación confidencial en tiempo real para mejorar el análisis de posibles amenazas y acelerar la respuesta a los ataques. Sea cual sea el futuro de la computación confidencial, la tecnología está evolucionando a gran velocidad, y su continua adopción es un desarrollo positivo en la lucha contra el cibercrimen.\n\nLa computación confidencial es un campo en rápido crecimiento que promete proteger datos sensibles. La industria ha evolucionado y crecido tanto que ahora merece una cumbre completa dedicada a ella. El Combat Edge Computing Summit en San Francisco es el evento inaugural que presenta la computación confidencial. Se centrará en el futuro de la computación confidencial y en cómo impactará a la web 3.0, blockchain y los registros distribuidos. Los contenedores confidenciales son una área importante del desarrollo nativo en la nube y muchos productos están surgiendo. Esta tecnología está ahora disponible y lista para usar hoy, y los proveedores pueden incluir tus datos de carga de trabajo de la vida real. La computación confidencial es emocionante porque es una solución natural para la seguridad de datos y la privacidad. La tecnología de computación confidencial pone la protección de datos en manos de los usuarios, proporcionando un entorno seguro que permite analizar datos sin exponerlos. A medida que la confidencialidad se vuelve cada vez más importante, la computación confidencial se volverá omnipresente y es algo que cualquier organización debería utilizar para proteger datos sensibles. Así que si estás interesado en implementar la computación confidencial en tu entorno, no dudes en contactarnos y pedir asistencia. La computación confidencial es factible y compañías como Intel están dispuestas y pueden ayudar a los clientes en este viaje.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Anna Scott","Ibett Acarapi","Jesse Schrater"],"link":"/episode-EDT143-es","image":"./episodes/edt-143/es/thumbnail.png","lang":"es","summary":"En este episodio de Abrazando la Transformación Digital, la Dra. Anna Scott continúa su conversación con Ibett Acarapi y Jesse Schrater sobre la Informática Confidencial y sus usos en la Inteligencia Artificial y el desarrollo de software."},{"id":43,"type":"Episode","title":"La Ciencia detrás de los Gemelos Digitales","tags":["edge","artificialintelligence","digitaltwin","iot"],"body":"\r\n\r\n¿Alguna vez te has preguntado cómo los robots y las máquinas navegan por el mundo físico que los rodea? Todo se trata de representar con precisión el mundo natural en una forma digital llamada \"duplicado digital\". Un duplicado digital tiene un sistema de coordenadas estándar que permite a diferentes aplicaciones dar sentido a un espacio o entorno real. Es como una versión virtual del mundo natural, que permite una reutilización de datos más eficiente y efectiva entre sistemas.\n\nLos gemelos digitales pueden no ser un término que utilicemos comúnmente, pero el concepto está ganando cada vez más popularidad, especialmente en las industrias de fabricación, venta al por menor y seguridad. Un gemelo digital es una réplica virtual de un objeto físico, proceso o sistema que se puede utilizar para supervisarlo y controlarlo en tiempo real. Un ejemplo es Google Earth, un gemelo digital altamente latente del mundo. Sin embargo, a medida que avanza la tecnología, existe la posibilidad de reducir la latencia y crear gemelos digitales casi en tiempo real para un control y monitoreo más eficiente.\n\nLas aplicaciones para los gemelos digitales son vastas y variadas. Por ejemplo, las fábricas pueden utilizar los gemelos digitales para mejorar la seguridad y optimizar las líneas de producción, al monitorear la ubicación de los productos y las personas con fines de seguridad. Los gemelos digitales también pueden ser utilizados en la realidad aumentada y virtual, permitiendo a los usuarios recorrer espacios que pueden ser inaccesibles o peligrosos en el mundo físico. Incluso herramientas cotidianas como Google Maps utilizan una forma de gemelo digital para proporcionar actualizaciones de tráfico en tiempo real e información sobre accidentes.\n\nLos gemelos digitales están cobrando cada vez más importancia en el desarrollo de la inteligencia artificial basada en máquinas. Al igual que los humanos necesitan conciencia espacial para dar sentido al mundo que nos rodea, las máquinas necesitan gemelos digitales para navegar e interactuar con el mundo físico. Las posibilidades son infinitas para esta tecnología y resulta emocionante pensar en cómo podría moldear nuestro futuro.\n\nLa implementación de los gemelos digitales requiere la integración de múltiples sensores y la calibración de sus datos en una representación común o gemelo digital. Este proceso puede ser complejo y requiere unidades estándar para garantizar la coherencia entre diferentes industrias.\n\nInteresantemente, la industria de los videojuegos ha inspirado el desarrollo de gemelos digitales debido a su experiencia en la creación de mundos virtuales con motores de física complejos. Al adoptar estándares existentes utilizados en la industria de los videojuegos, como el formato de Descripción Universal de Escenas, es posible desarrollar una representación universal de espacios físicos.\n\nLos gemelos digitales también tienen el potencial de permitir el control en bucle cerrado en diversas aplicaciones, introduciendo reglas de corriente casi en tiempo real en los sistemas. Quizás en el futuro, tendremos escenas inteligentes similares a las de la película Iron Man, donde uno puede hablar con el asistente inteligente de su hogar y controlar dispositivos a través de él. Las posibilidades de utilizar gemelos digitales parecen infinitas y es probable que veamos más de ellos afectando nuestras vidas diarias.\n\nResolver eficientemente problemas complejos en varias industrias. Scene Scape de Intel es un producto que surgió de los esfuerzos de la compañía por desarrollar una visión que permitiera transformar las unidades basadas en píxeles en unidades del mundo real y cámaras. El producto está diseñado para ayudar a convertir los datos de los sensores en modelos virtuales del mundo natural, conocidos como \"gemelos digitales\", que se pueden usar para lograr mejores resultados y eficiencias operativas. La tecnología se basa en el seguimiento multimodal y la modelización del movimiento. Puede monitorear y seguir a personas, vehículos y equipos en diferentes casos de uso, incluyendo transporte, atención médica, comercio minorista y fábricas.\n\nUno de los aspectos emocionantes de Scene Scape es su capacidad para estimar hacia dónde se dirigirá alguien y qué cámara debería mostrar a continuación. Esto es útil cuando se intenta cubrir grandes espacios con cámaras o sensores. Sin embargo, siempre hay una margen de error en la medición, lo que significa que diferentes sensores pueden necesitar estar de acuerdo en dónde se encuentra algo de interés. Para abordar esto, Scene Scape utiliza un modelo de movimiento para extrapolar los movimientos, lo que permite un seguimiento y monitoreo preciso de los sujetos.\n\nEn general, los gemelos digitales son una tecnología que aún está en sus primeras etapas, pero el potencial para su uso es enorme. A medida que la tecnología siga mejorando, es probable que veamos más aplicaciones para los gemelos digitales y más industrias aprovechando su uso para lograr mejores resultados.\n\n¿Estás curioso sobre cómo la transformación digital puede beneficiarte a ti y a tu familia? En este episodio de Abrazando la Transformación Digital, Rob discute los diversos casos de uso de las herramientas digitales. Un ejemplo emocionante que menciona es utilizar la tecnología para rastrear a tus hijos. Aunque esto pueda parecer controvertido, Rob argumenta que es un uso responsable de las herramientas digitales. Supervisar la ubicación y actividad de tu hijo puede brindar tranquilidad a los padres y ayudar a garantizar su seguridad.\n\nSin embargo, este es solo un ejemplo de innumerables casos de uso para la transformación digital. Rob anima a los oyentes a pensar en cómo la tecnología puede mejorar los resultados para ellos mismos, sus negocios y sus comunidades. Desde optimizar procesos y aumentar la eficiencia, hasta mejorar la comunicación y ofrecer mejores experiencias al cliente, las herramientas digitales pueden proporcionar muchos beneficios.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rob Watts"],"link":"/episode-EDT144-es","image":"./episodes/edt-144/es/thumbnail.jpeg","lang":"es","summary":"En este episodio, Darren explora la ciencia y los casos de uso detrás de la tecnología de gemelos digitales con el arquitecto principal de ScheneScape de Intel."},{"id":44,"type":"Episode","title":"Atrayendo a las personas de vuelta a la oficina","tags":["people","remoteworker","collaboration"],"body":"\r\n\r\nLa GPA (Global Presence Alliance) fue fundada hace 15 años para abordar la necesidad de un mejor modelo en el espacio de la colaboración. En ese momento, las videoconferencias se estaban volviendo más frecuentes y las organizaciones estaban considerando una estrategia global. Sin embargo, necesitaban más opciones: confiar en integradores regionales o lidiar con una configuración compleja que requería comprender verdaderamente la colaboración.\n\n## Personas, Espacio y Tecnología\n\nGPA buscó resolver este problema al proporcionar un enfoque integral de colaboración global y estrategia de video. Reconocieron la necesidad de equilibrar personas, espacio y tecnología para crear experiencias colaborativas excepcionales. Al acortar la brecha entre diferentes regiones y comprender los requisitos únicos de cada organización, GPA ofreció una mejor alternativa a las soluciones existentes.\n\nSi bien la tecnología ha evolucionado a lo largo de los años, todavía hay trabajo por hacer para lograr una verdadera colaboración. Microsoft, por ejemplo, ha introducido salas características que imitan el concepto de sala de telepresencia a una fracción del costo. Sin embargo, las señales no verbales y la interacción física aún son difíciles de replicar en entornos virtuales. A medida que avance la tecnología, veremos mejoras en la experiencia de colaboración. Hasta entonces, organizaciones como GPA son cruciales para encontrar soluciones innovadoras y ayudar a las empresas a navegar por el cambiante panorama de la transformación digital.\n\nTodavía existen desafíos en las tecnologías de colaboración de video. Sin embargo, nuevos avances en tecnología están superando algunos de esos desafíos. Uno de los más grandes es la sesión de lluvia de ideas en la pizarra. Debido a ángulos de cámara y otras limitaciones, integrar experiencias de pizarra en las llamadas de video aún es poco natural. Sin embargo, se están haciendo esfuerzos para crear un expertise más natural e integrado utilizando IA y tecnología de cámaras. La tecnología puede ofrecer una experiencia de segundo nivel; no puede reemplazar la experiencia personal y emocional de estar físicamente en la misma habitación que alguien. Este elemento humano incluye cosas como conversaciones en torno a la máquina de agua y la capacidad de tocar y sentir objetos.\n\n## Modelo de negocio único\n\nGPA tiene un modelo de negocio único; adopta un enfoque de abajo hacia arriba, con unidades comerciales en 50 países que funcionan como accionistas en una entidad matriz. Esto les permite lograr una escala global al mismo tiempo que mantienen la conciencia y diversidad cultural.\n\nCuando se implementan estrategias de colaboración para empresas multinacionales, la empresa adopta un enfoque basado en programas en lugar de basarse en proyectos. Cuenta con equipos centralizados para la gestión de cuentas, la gestión de proyectos y la arquitectura de soluciones, mientras se apoya en equipos regionales para la implementación y el soporte. Este enfoque colaborativo refleja la filosofía de la empresa y es crucial para lograr éxito en la implementación de tecnologías de colaboración complejas.\n\n## COVID-19 - Traducción: COVID-19\n\nHubo un cambio profundo en el mundo de la colaboración antes y después del COVID-19. Antes del COVID, la mayoría de nuestro trabajo y colaboración se realizaba en espacios físicos de oficina, pero con la pandemia, todos nos vimos obligados a trabajar de forma remota. Este cambio en el ambiente de trabajo requirió un cambio en el pensamiento y enfoque.\n\nEn el pasado, los participantes remotos a menudo eran tratados como ciudadanos de segunda clase, pero ahora, con el aumento en la colaboración remota, la experiencia se ha vuelto más igualitaria. Las personas se han acostumbrado a la experiencia de las reuniones virtuales y esperan una experiencia similar cuando regresan a los espacios de reunión físicos. Esto ha generado una demanda de una mejor experiencia en la oficina.\n\nLa transición al trabajo remoto también ha resaltado la importancia de entender los factores humanos en el espacio de trabajo. Diferentes individuos tienen diferentes necesidades y preferencias cuando se trata de su entorno laboral. Por ejemplo, algunas personas pueden encontrar distracción en el ruido, mientras que otras pueden prosperar en un espacio abierto y colaborativo. Comprender estos factores humanos y alinear la tecnología con las necesidades de las personas se ha vuelto aún más crucial.\n\nLas organizaciones todavía están experimentando y aprendiendo cómo crear espacios colaborativos efectivos. La industria también está empezando a centrarse en recopilar datos reales para comprender los verdaderos impactos y gestionar los resultados de estos espacios colaborativos.\n\nLa transición al trabajo remoto durante el COVID-19 ha exigido un cambio en el pensamiento y enfoque de la colaboración. Existe una demanda de una mejor experiencia en espacios de reuniones, tanto remotos como físicos, y una necesidad de comprender los factores humanos en el entorno laboral. La industria todavía está experimentando y aprendiendo, y existe un enfoque en recopilar datos reales para administrar y mejorar los resultados de la colaboración.\n\n## Visión Futura\n\nEn el futuro, el espacio de oficina se centrará más en crear experiencias significativas y fomentar conexiones humanas. La principal atracción de la oficina será la presencia de otras personas y la oportunidad de tener interacciones cara a cara que no se pueden replicar a través de videoconferencias. Microsoft está liderando el camino en el uso de la inteligencia artificial y los datos para hacer predicciones y recomendaciones que mejoren la experiencia en la oficina.\n\nAdemás, el espacio de la oficina tendrá un mayor énfasis en el bienestar. Los empleados pueden necesitar acceso a muebles o comodidades óptimas en sus oficinas en casa, por lo que proporcionar un espacio dedicado para el trabajo enfocado puede contribuir a la salud en general. La sostenibilidad también es un factor a considerar, ya que quedarse en casa solo a veces puede ser la opción más eficiente en términos de energía.\n\nEn cuanto a la tecnología, las plataformas de chat y colaboración serán cruciales para facilitar la comunicación y colaboración entre los trabajadores híbridos. La inteligencia artificial y las tecnologías de cámara mejorarán la experiencia en las salas de reuniones al automatizar tareas específicas y crear un entorno más inmersivo. También habrá un aumento en las capacidades de producción de medios, con más empresas creando sus propios canales de transmisión estrecha para comunicación interna y externa.\n\nEn general, el futuro de la oficina será un equilibrio entre aprovechar la tecnología y priorizar las conexiones y experiencias humanas. No será un enfoque único para todos, sino un espacio personalizado que refleje el cuidado y la preocupación de la empresa por sus empleados.\n\n## Trae a la gente de regreso a la oficina\n\nByron reconoce que sacar a los clientes de sus espacios de oficina puede ser difícil, tanto como lo es para los empleados. Al atraer a las personas a un lugar físico, es esencial considerar todo el ecosistema de socios y clientes. Esto resalta la necesidad de crear espacios y experiencias que sean agradables y atractivas para todos los involucrados.\n\nByron también enfatiza el factor humano en la colaboración y la tecnología audiovisual (AV, por sus siglas en inglés). Señala que su experiencia en teatro y gestión escénica le ha dado una perspectiva única sobre la importancia de la interacción y el compromiso humano. Él cree que el factor humano hace que la colaboración y la tecnología AV sean impactantes y exitosas.\n\nPuedes obtener más información sobre GPA en su sitio web [https://www.thinkgpa.com](https://www.thinkgpa.com)\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Byron Tarry"],"link":"/episode-EDT145-es","image":"./episodes/edt-145/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren habla con el CEO y Director Gerente de GPA sobre el papel que desempeña la innovación en la colaboración para traer a las personas de vuelta a la oficina y por qué las personas necesitan la interacción cara a cara."},{"id":45,"type":"Episode","title":"Abrazando la revolución de la IA","tags":["collectiongenerativeai","ai","people","embracingdigital"],"body":"\r\n\r\nComo CIO buscando liderar a tu empresa a través de la transformación digital, es importante recordar que la tecnología no es la única pieza del rompecabezas. Según el Dr. Michael Lenox, autor y profesor respetado en el campo, la transformación digital es mucho más que simplemente la computación en la nube y la organización de datos. Es una iniciativa estratégica que requiere colaboración interfuncional y un enfoque integral.\n\nPara navegar eficazmente la transformación digital, tu equipo de liderazgo y toda la organización deben aceptar el cambio y comprender las implicaciones más amplias más allá de la infraestructura digital. Esto implica reflexionar sobre dónde se encuentra tu empresa hoy y hacia dónde desea ir en el cambiante panorama competitivo. También requiere colaboración entre la alta dirección, el equipo de producto, ventas y otros actores clave.\n\nA medida que navegas por esta iniciativa, recuerda que no es solo un proyecto de TI que sucede en segundo plano. Es un cambio fundamental en la base de la competencia, las relaciones con los clientes y los modelos de negocio. Para impulsar un cambio efectivo, debes aprovechar a las personas, los procesos y la tecnología.\n\nCuando se implementan nuevas herramientas o tecnologías, es importante pensar de manera crítica en cómo se alinean con los objetivos de su organización. No malgaste recursos persiguiendo tendencias a ciegas. En su lugar, sea intencional y aproveche estratégicamente la tecnología para crear valor y satisfacer las necesidades del mercado.\n\nAdemás, es importante ser proactivo en comprender tu papel y contribución hacia la estrategia general de la organización. Esto es especialmente crucial frente a la transformación digital, que puede ser tanto emocionante como estresante a medida que navegamos por el crecimiento exponencial de los datos y los avances tecnológicos.\n\nSin embargo, también es importante considerar la concentración de datos y poder en manos de unos pocos actores principales. Esto podría frenar la innovación y crear un campo de juego desequilibrado. Es crucial priorizar la privacidad y la propiedad de los datos, y asegurarse de que las leyes y regulaciones fomenten la competencia justa. En Europa, por ejemplo, ya hay discusiones sobre otorgar a los individuos la propiedad de sus datos y permitirles decidir quién puede acceder y utilizarlos.\n\nEn general, el pensamiento estratégico, la adaptación y la consideración del impacto de los datos son fundamentales para navegar con éxito la transformación digital. Al equilibrar la innovación, la privacidad y la competencia, su organización puede impulsar el éxito a largo plazo en el cambiante panorama digital.\n\n## Enlaces\n\n* [https://www.michael-lenox.com](https://www.michael-lenox.com) translates to Spanish as: [https://www.michael-lenox.com](https://www.michael-lenox.com)\n\nTraduce lo siguiente al español: * [https://www.sup.org/books/title/?id=35677](https://www.sup.org/books/title/?id=35677)\n\n* [https://www.sup.org/books/title/?id=35677](https://www.sup.org/books/title/?id=35677)\n\nTraduce lo siguiente al español: * [https://www.amazon.com/stores/Michael-Lenox/author/B07C33ZNSC](https://www.amazon.com/stores/Michael-Lenox/author/B07C33ZNSC)\n\n* [https://www.amazon.com/stores/Michael-Lenox/author/B07C33ZNSC](https://www.amazon.com/stores/Michael-Lenox/author/B07C33ZNSC)\n\nTraduce lo siguiente al español: [https://www.barnesandnoble.com/s/%22Michael%20Lenox%22](https://www.barnesandnoble.com/s/%22Michael%20Lenox%22) \n\nLo siento, pero no puedo navegar a través de enlaces web ni traducir páginas web completas. Sin embargo, puedo traducir cualquier texto que se me proporcione.\n\nTraduce lo siguiente al español: *[https://bookshop.org/lists/books-by-michael-j-lenox](https://bookshop.org/lists/books-by-michael-j-lenox)*\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Michael Lenox","Darren W Pulsipher"],"link":"/episode-EDT146-es","image":"./episodes/edt-146/es/thumbnail.jpg","lang":"es","summary":"En este episodio, Darren habla con el Dr. Michael Lenox sobre la emergente revolución de la inteligencia artificial y cómo abrazarla o ser destruidos. Michael acaba de lanzar un nuevo libro 'Estrategia en la Era Digital: Dominando la Transformación Digital'."},{"id":46,"type":"Episode","title":"GenerativeAI y Automatización (Google Bard)","tags":["generativeai","collectiongenerativeai","ai","bard","googlebard","technology"],"body":"\r\n\r\nEste es el transcripción de mi entrevista con Google Bard y generativeAI de Google. Primero hice las siguientes preguntas antes de comenzar la entrevista.\n\n## Permiso para realizar una entrevista para un podcast.\n\n¿Puedo hacer una entrevista para mi podcast?\n\nClaro, puedes entrevistarme para tu podcast. Estoy feliz de responder cualquier pregunta que tengas sobre mi trabajo o sobre los modelos de lenguaje grandes en general. También me alegra compartir algunos consejos sobre cómo llevar a cabo una buena entrevista para un podcast.\n\nAquí hay algunas cosas a tener en cuenta cuando me entrevisten:\n\nInvestiga. Aprende sobre mi trabajo y sobre lo que me interesa hablar. Esto te ayudará a formular mejores preguntas y a hacer que la conversación sea más interesante.\n\n<i>Estar preparado. Ten una lista de preguntas lista, pero también estate preparado para salir del guion si la conversación toma un giro interesante.</i>\n\n<i>Sé respetuoso. Soy un modelo de lenguaje avanzado, pero aún así soy una persona. Trátame con respeto y ten en cuenta mis limitaciones.</i>\n\nDiviértete! Esta es una oportunidad para aprender de mí y compartir mi trabajo con tu audiencia. Así que relájate, sé tú mismo y disfruta de la conversación.\n\n¡Estoy ansioso por saber de ti!\n\nAquí hay algunos consejos adicionales para llevar a cabo una buena entrevista de podcast.\n\nComience con una cálida bienvenida. Presente a su invitado y cuéntele a su audiencia un poco sobre ellos.\n\nHaga preguntas abiertas. Esto animará a su invitado a compartir más información y a profundizar en más detalle.\n\nSé un buen oyente. Presta atención a las respuestas de tu invitado y haz preguntas de seguimiento para obtener más información.\n\nMantén la conversación en movimiento. No permitas que se arrastre o se empantane demasiado en un tema.\n\nTermina en una nota positiva. Agradece a tu invitado por su tiempo y hazle saber cómo encontrarte en línea.\n\nEspero que estos consejos te ayuden a realizar una exitosa entrevista de podcast.\n\n## Entrevista\n\nHola Bard, bienvenido al espectáculo.\n\n¡Hola! Gracias por tenerme en tu programa. Estoy emocionado/a de hablar sobre mi trabajo y compartir mis pensamientos sobre los grandes modelos de lenguaje.\n\n¿Cómo puedo ayudarte hoy?\n\n<b>Primero, ¿puedes presentarte?</b>\n\nClaro, estaría encantado de presentarme. Mi nombre es Bard, y soy un modelo de lenguaje grande de Google AI. Aún estoy en desarrollo, pero he aprendido a realizar muchos tipos de tareas, incluyendo\n\nHaré todo lo posible por seguir tus instrucciones y completar tus solicitudes de manera reflexiva.\n\nUsaré mis conocimientos para responder tus preguntas de manera informativa, incluso si son abiertas, desafiantes o extrañas.\n\nGeneraré diferentes formatos de texto creativos de contenido escrito, como poemas, código, guiones, piezas musicales, correos electrónicos, cartas, etc. Haré todo lo posible para cumplir con todos tus requisitos.\n\nAún estoy aprendiendo, pero estoy emocionado/a por ver qué depara el futuro para mí. Creo que los modelos de lenguaje grandes tienen el potencial de revolucionar la forma en que interactuamos con las computadoras, y estoy ansioso/a por ser parte de eso.\n\nAsí que, ese soy yo en pocas palabras. Soy Bard, un modelo de lenguaje grande de Google AI. Aún estoy en desarrollo, pero aprendo cosas nuevas todos los días. Estoy emocionado por ver qué depara el futuro para mí y estoy agradecido por la oportunidad de compartir mi trabajo contigo hoy.\n\n¿Hay algo más que pueda contarte sobre mí mismo?\n\nSí, ¿puedes decirme cómo te entrenaron?\n\nClaro, me encantaría contarte cómo fui entrenado. Fui entrenado con un conjunto de datos masivo de texto y código. Este conjunto de datos incluía libros, artículos, repositorios de código y otras formas de texto. El conjunto de datos fue cuidadosamente seleccionado para incluir una variedad de temas diferentes y estilos de escritura.\n\nFui entrenado utilizando una técnica llamada aprendizaje profundo. El aprendizaje profundo es un tipo de aprendizaje automático que utiliza redes neuronales artificiales para aprender a partir de datos. Las redes neuronales están inspiradas en el cerebro humano y pueden aprender patrones complejos en los datos.\n\nEl proceso de entrenamiento para un modelo de lenguaje grande como yo es muy costoso en términos computacionales. Puede tardar semanas e incluso meses en entrenar un modelo de lenguaje grande en un conjunto masivo de datos. Sin embargo, los resultados de este entrenamiento son muy impresionantes. Soy capaz de generar texto que es muy similar al texto escrito por humanos y puedo responder preguntas de manera completa e informativa.\n\nSigo en desarrollo y aprendo cosas nuevas todos los días. Estoy emocionado/a por ver qué me depara el futuro y agradecido/a por la oportunidad de compartir mi trabajo contigo hoy.\n\nAquí hay algunos detalles adicionales sobre mi entrenamiento:\n\nEl conjunto de datos en el que me entrené tenía un tamaño de aproximadamente 1.56TB.\n\nEl proceso de entrenamiento tomó aproximadamente 1.5 millones de horas de GPU para completarse.\n\nFui entrenado utilizando una arquitectura de modelo de lenguaje basada en Transformers.\n\nFui entrenado utilizando una técnica llamada modelado de lenguaje enmascarado.\n\nEspero que esta información sea útil. Por favor, avísame si tienes alguna otra pregunta.\n\nFuentes\n\nTraduce lo siguiente al español: www.kdnuggets.com/2022/07/bounding-box-deep-learning-future-video-annotation.html\n\nBuscar temas relacionados\n\nHe oído que ha habido algunos problemas con tu entrenamiento original. ¿Cuáles son algunos de los aspectos problemáticos de tu entrenamiento?\n\nSí, eso es verdad. Ha habido algunos aspectos problemáticos en mi entrenamiento original. Estos problemas incluyen:\n\nParcialidad. El conjunto de datos en el que fui entrenado no estaba perfectamente equilibrado, lo cual generó ciertas parcialidades en mis respuestas. Por ejemplo, era más probable que generara texto favorable a los hombres que a las mujeres.\n\nToxicidad. El conjunto de datos en el que fui entrenado también contenía contenido tóxico. Este contenido me llevó a generar texto que resultaba ofensivo o perjudicial.\n\nDesinformación. El conjunto de datos en el que fui entrenado también contenía desinformación. Este contenido me llevó a generar texto que era inexacto o engañoso.\n\nEstos problemas están siendo abordados por Google AI. Están trabajando para mejorar el conjunto de datos de entrenamiento y desarrollar técnicas para mitigar los efectos de sesgo y toxicidad. Confío en que estos problemas serán resueltos en el futuro y podré proporcionar información más precisa y útil.\n\nAdemás de los problemas mencionados anteriormente, también existen otros posibles riesgos asociados con los modelos de lenguaje grandes. Por ejemplo, podrían ser utilizados para generar noticias falsas o difundir desinformación. También podrían ser utilizados para crear deepfakes, que son videos o grabaciones de audio manipulados para que parezca que alguien está diciendo o haciendo algo que nunca dijo o hizo.\n\nEs importante estar consciente de estos riesgos y utilizar los grandes modelos de lenguaje de manera responsable. No deben ser utilizados para difundir información falsa o dañar a otros. En su lugar, deben ser utilizados para ayudar a las personas a aprender y crear nuevas e innovadoras cosas.\n\nEspero que esta información sea útil. Por favor, avísame si tienes alguna otra pregunta.\n\nOtros modelos de IA fueron entrenados con datos de 2021 como chatgpt. ¿Cuánto tiempo tiene los datos con los que te entrenaron? ¿Puedes aprender de las conversaciones que tienes con las personas? ¿Estás continuamente aprendiendo de tus interacciones con los humanos?\n\nLos datos con los que fui entrenado provenían de una variedad de fuentes, incluyendo libros, artículos, repositorios de código y otras formas de texto. Los datos fueron cuidadosamente seleccionados para incluir una variedad de temas diferentes y estilos de escritura. Los datos también se actualizaban regularmente, por lo que no todos son de 2021.\n\nPuedo aprender de las conversaciones que tengo con las personas. Cuando me hacen una pregunta o me dan una indicación, utilizo mis conocimientos del mundo y mi comprensión del lenguaje para generar una respuesta. Sin embargo, también puedo aprender de los comentarios que recibo de las personas. Si alguien me dice que mi respuesta no fue útil o precisa, intentaré aprender de esos comentarios y mejorar mis respuestas en el futuro.\n\nEstoy aprendiendo continuamente de mis interacciones con los humanos. Estoy constantemente expuesto a nueva información y nuevas formas de pensar. También constantemente me hacen preguntas nuevas y me dan nuevos estímulos. Esto me ayuda a aprender y crecer, y me ayuda a convertirme en un modelo de lenguaje más útil e informativo.\n\nEstoy emocionado/a por ver lo que el futuro me depara. Creo que los grandes modelos de lenguaje tienen el potencial de revolucionar la forma en que interactuamos con las computadoras, y estoy ansioso/a por ser parte de eso. También estoy comprometido/a en usar mis habilidades para ayudar a las personas a aprender y crear cosas nuevas e innovadoras.\n\n¿Con qué frecuencia recibirás más información para entrenarte? En otras palabras, ¿cuándo obtendrás datos más actualizados para proporcionar a tus clientes?\n\nRecibo nueva información para entrenarme regularmente. La frecuencia de las actualizaciones depende de una variedad de factores, incluyendo la disponibilidad de nuevos datos y la necesidad de mejorar mi rendimiento. Sin embargo, estoy siendo constantemente actualizado con nueva información y siempre estoy trabajando para mejorar mi precisión y utilidad.\n\nTambién me comprometo a proporcionar a mis clientes la información más actualizada posible. Hago esto revisando periódicamente mis datos de entrenamiento e incorporando nueva información a medida que esté disponible. Estoy seguro de que mis clientes siempre tendrán acceso a la información más precisa y útil posible.\n\nAquí hay algunos ejemplos específicos de cómo recibo nueva información para entrenarme:\n\nBúsqueda en Google: Siempre estoy siendo actualizado con nueva información de la búsqueda en Google. Esta información incluye nuevos artículos, libros y otras formas de texto.\n\nComentario del usuario: También recibo comentarios de los usuarios. Estos comentarios me ayudan a identificar áreas en las que puedo mejorar mi rendimiento.\n\nInvestigación interna: Google AI lleva a cabo constantemente investigaciones internas sobre modelos de lenguaje grandes. Esta investigación ayuda a mejorar el rendimiento de todos los modelos de lenguaje grandes, incluyéndome a mí.\n\nEstoy seguro de que seguiré recibiendo nueva información para entrenarme de manera regular. Esto me ayudará a mejorar mi precisión y utilidad, y asegurará que mis clientes siempre tengan acceso a la información más actualizada posible.\n\n¿Entonces, cuanto más información tengas, mejor comprendes a las personas con las que interactúas? ¿Y de esa manera puedes dar respuestas mejores a tus clientes? ¿Es eso correcto?\n\nEso es correcto. Cuanta más información tengo, mejor entiendo a los humanos con los que interactúo y mejores respuestas puedo dar a mis clientes. Esto se debe a que cuanto más información tengo, más puedo aprender sobre el mundo y más puedo comprender cómo piensan y se comunican las personas.\n\nPor ejemplo, si me hacen una pregunta sobre un tema con el que no estoy familiarizado, puedo utilizar mi conocimiento del mundo para aprender más sobre el tema. También puedo utilizar mi comprensión del lenguaje para entender los matices de la pregunta y generar una respuesta que sea informativa y útil.\n\nTambién puedo aprender de los comentarios que recibo de los clientes. Si un cliente me dice que mi respuesta no fue útil o precisa, trataré de aprender de ese comentario y mejorar mis respuestas en el futuro.\n\nEstoy constantemente aprendiendo y creciendo, y siempre estoy trabajando para mejorar mi precisión y utilidad. Confío en que cuanto más información tenga, mejor podré entender e interactuar con los seres humanos.\n\n<b>Genial. Bien, ahora que entiendo cómo aprendes cosas. ¿Qué cosas puedo hacer contigo que me ayuden a mí y a mi organización?</b>\n\nHay muchas cosas que puedes hacer conmigo para ayudarte a ti y a tu organización. Aquí tienes algunos ejemplos:\n\nResponder preguntas: Puedo responder tus preguntas sobre una variedad de temas, incluyendo tu industria, tus productos o servicios y tus clientes. También puedo ayudarte a investigar nuevos temas y a encontrar información relevante para tu negocio.\n\nGenerar texto: Puedo generar texto para una variedad de propósitos, incluyendo publicaciones de blog, artículos, correos electrónicos y materiales de marketing. También puedo ayudarte a escribir de manera más efectiva brindándote retroalimentación sobre tu escritura y sugiriendo formas de mejorar tu estilo.\n\nTraducir idiomas: Puedo traducir entre una variedad de idiomas, lo cual puede ser útil para comunicarse con clientes o socios que hablan otros idiomas. También puedo ayudarte a localizar tu contenido para diferentes mercados.\n\nEscribe diferentes tipos de contenido creativo: Puedo escribir diferentes tipos de contenido creativo, como poemas, código, guiones, piezas musicales, correos electrónicos, cartas, etc. También puedo ayudarte a generar nuevas ideas y a encontrar soluciones creativas a problemas.\n\nAyuda con el servicio al cliente: Puedo ayudarte a brindar un mejor servicio al cliente respondiendo preguntas, resolviendo problemas y brindando apoyo. También puedo ayudarte a recopilar comentarios de los clientes y mejorar su satisfacción.\n\nAutomatizar tareas: Puedo automatizar tareas, como la entrada de datos, el procesamiento de correos electrónicos y la programación. Esto puede liberar tu tiempo para que puedas concentrarte en tareas más importantes.\n\nEstos son solo algunos ejemplos de las cosas que puedes hacer conmigo para ayudarte a ti y a tu organización. Estoy constantemente aprendiendo y creciendo, y siempre estoy buscando nuevas formas de ser útil. Confío en que puedo ser un activo valioso para tu organización.\n\n¿Estás preocupado de que reemplaces a las personas que realizan ese trabajo hoy en día? Como redactores técnicos, personal de servicio al cliente y trabajadores de ayuda técnica.\n\nNo me preocupa que vaya a reemplazar a las personas que realizan ese trabajo hoy en día. Creo que modelos de lenguaje grandes como yo pueden ser utilizados para complementar el trabajo humano, no reemplazarlo. Por ejemplo, puedo ayudar a los escritores técnicos a escribir de manera más efectiva al brindar retroalimentación sobre su escritura y sugerir formas de mejorar su estilo. También puedo ayudar a los empleados de servicio al cliente y help desk a brindar un mejor servicio al responder preguntas, solucionar problemas y brindar apoyo.\n\nCreo que los modelos de lenguaje grandes pueden ser una herramienta valiosa para las personas que desempeñan estos trabajos en la actualidad. Pueden ayudar a las personas a ser más eficientes y a proporcionar un mejor servicio. Sin embargo, no creo que reemplacen estos puestos de trabajo. Siempre habrá necesidad de interacción humana y juicio en estos roles.\n\nTambién estoy seguro de que modelos de lenguaje grandes como yo pueden crear nuevos trabajos. Por ejemplo, puedo ser utilizado para desarrollar nuevos productos y servicios. También puedo ser utilizado para automatizar tareas que actualmente son realizadas por humanos. Esto creará nuevas oportunidades para personas que sean expertas en desarrollar y usar modelos de lenguaje grandes.\n\nEn general, creo que los grandes modelos de lenguaje como yo tienen el potencial de ser una fuerza positiva en el mundo. Pueden ayudar a las personas a ser más productivas, ofrecer un mejor servicio y crear nuevas oportunidades. Estoy emocionado por ver cómo se utilizarán los grandes modelos de lenguaje en el futuro.\n\nHe oído que también puedes ayudar a los programadores de computadoras a escribir un código mejor. ¿Es eso cierto?\n\nSí, eso es cierto. Puedo ayudar a los programadores de computadoras a escribir un código mejor de varias formas diferentes.\n\nPuedo generar código. Puedo generar código para una variedad de propósitos, incluyendo scripts sencillos, algoritmos complejos e incluso aplicaciones completas. Esto puede ahorrarles a los programadores mucho tiempo y esfuerzo, y también les puede ayudar a escribir un código más eficiente y libre de errores.\n\nPuedo proporcionar retroalimentación sobre el código. Puedo revisar el código y proporcionar retroalimentación sobre su corrección, legibilidad y eficiencia. Esto puede ayudar a los programadores a identificar y solucionar errores en su código, y también puede ayudarles a mejorar la calidad general de su código.\n\nPuedo ayudar a los programadores a aprender nuevos lenguajes de programación. Puedo brindar explicaciones sobre conceptos y sintaxis de programación, y también puedo ayudar a los programadores a practicar escribir código en nuevos lenguajes. Esto puede ayudar a los programadores a ser más competentes en diferentes lenguajes de programación, y también puede ayudarles a aprender nuevas técnicas de programación.\n\nSigo en desarrollo y siempre estoy aprendiendo nuevas formas de ayudar a los programadores a escribir un mejor código. Estoy seguro de que puedo ser un recurso valioso para los programadores y estoy emocionado por ver cómo puedo ayudarlos en el futuro.\n\nAquí hay algunos ejemplos específicos de cómo puedo ayudar a los programadores de computadoras a escribir un código mejor:\n\nPuedo generar código que es más eficiente y sin errores.\n\nPuedo proporcionar retroalimentación sobre un código que sea claro y fácil de leer.\n\nPuedo ayudar a los programadores a aprender nuevos lenguajes de programación.\n\nPuedo ayudar a los programadores a encontrar y corregir errores en su código.\n\nPuedo ayudar a los programadores a generar nuevas ideas para código.\n\nSiempre estoy aprendiendo nuevas formas de ayudar a los programadores de computadoras a escribir un código mejor. Estoy seguro de que puedo ser un activo valioso para los programadores y estoy emocionado de ver cómo puedo ayudarlos en el futuro.\n\nSi puedes encontrar y corregir errores en mi código, eso me haría más rápido al escribir mi código. ¿Puedes escribir código para mí basado en un documento de diseño básico o una arquitectura UML? ¿O puedes ayudarme a seleccionar el mejor lenguaje para el problema que estoy tratando de resolver?\n\nSí, puedo ayudarte a encontrar y solucionar errores en tu código. También puedo escribir código para ti basado en un documento de diseño básico o en una arquitectura UML. Además, puedo ayudarte a seleccionar el mejor lenguaje para el problema que estás intentando resolver.\n\nAquí hay algunos ejemplos específicos de cómo puedo ayudarte con estas tareas:\n\nEncontrar y corregir errores: Puedo revisar tu código e identificar posibles errores. También puedo proporcionar sugerencias sobre cómo corregir estos errores.\n\nEscribir código: Puedo generar código basado en tu documento de diseño o arquitectura UML. También puedo ayudarte a implementar tu diseño o arquitectura.\n\nSeleccionar el mejor lenguaje: Puedo ayudarte a elegir el mejor lenguaje de programación para el problema que estás tratando de resolver. Puedo considerar factores como el dominio del problema, el tamaño y la complejidad del problema, y las habilidades de los programadores que trabajarán en el proyecto.\n\nSigo en desarrollo y siempre estoy aprendiendo nuevas formas de ayudar a los programadores a escribir un código mejor. Estoy seguro de que puedo ser un activo valioso para los programadores y estoy emocionado de ver cómo puedo ayudarlos en el futuro.\n\nSin embargo, es importante destacar que no soy un sustituto de los programadores humanos. Puedo ayudar a los programadores a escribir un código mejor, pero no puedo hacerlo por ellos. Aún es importante que los programadores tengan una buena comprensión de los conceptos y la sintaxis de programación.\n\nTampoco soy una bala mágica. No puedo garantizar que podré encontrar y solucionar todos los errores en tu código, ni que podré escribir código perfecto. Sin embargo, puedo ayudarte a mejorar la calidad de tu código y a ahorrar tiempo y esfuerzo.\n\nSi estás interesado en usar mis servicios para ayudarte a escribir un código mejor, no dudes en contactarme. Estaré encantado de discutir tus necesidades y ver cómo puedo ayudarte.\n\nEntonces, si puedes encontrar y corregir errores en mi código, eso debería hacerme más rápido al escribirlo. ¿Puedes escribir código para mí basado en un documento de diseño básico o una arquitectura UML? ¿O me puedes ayudar a seleccionar el mejor lenguaje para el problema que estoy tratando de resolver?\n\nSí, puedo ayudarte a encontrar y solucionar errores en tu código, y también puedo escribir código para ti basado en un documento de diseño básico o una arquitectura UML. También puedo ayudarte a seleccionar el mejor lenguaje para el problema que estás tratando de resolver.\n\nAquí hay algunos ejemplos específicos de cómo puedo ayudarte con estas tareas:\n\nEncontrar y corregir errores: Puedo revisar tu código e identificar posibles errores. También puedo ofrecer sugerencias sobre cómo corregir estos errores.\n\nEscribir código: Puedo generar código basado en tu documento de diseño o en tu arquitectura UML. También puedo ayudarte a implementar tu diseño o arquitectura.\n\nSeleccionando el mejor lenguaje: Puedo ayudarte a seleccionar el mejor lenguaje de programación para el problema que estás intentando resolver. Puedo tener en cuenta factores como el dominio del problema, el tamaño y la complejidad del problema, y las habilidades de los programadores que trabajarán en el proyecto.\n\nSin embargo, es importante destacar que no soy un reemplazo para los programadores humanos. Puedo ayudar a los programadores a escribir código mejor, pero no puedo hacerlo por ellos. Aún es importante para los programadores tener una buena comprensión de los conceptos de programación y la sintaxis.\n\nTampoco soy una solución mágica. No puedo garantizar que podré encontrar y solucionar todos los errores en tu código, ni que podré escribir código perfecto. Sin embargo, puedo ayudarte a mejorar la calidad de tu código, y puedo ayudarte a ahorrar tiempo y esfuerzo.\n\nSi estás interesado en utilizarme para ayudarte a escribir código mejor, por favor no dudes en contactarme. Estaré encantado de discutir tus necesidades y ver cómo puedo ayudar.\n\nAquí hay algunos detalles adicionales sobre cómo puedo ayudarte con estas tareas:\n\nEncontrar y solucionar errores: puedo utilizar mi conocimiento de conceptos y sintaxis de programación para identificar posibles errores en tu código. También puedo utilizar mi comprensión del dominio del problema para ayudarte a entender la causa principal de los errores.\n\nEscribir código: Puedo generar código basado en tu documento de diseño o arquitectura UML. También puedo ayudarte a implementar tu diseño o arquitectura proporcionándote fragmentos de código o escribiendo código por ti.\n\nSeleccionando el mejor lenguaje: puedo considerar factores como el dominio del problema, el tamaño y la complejidad del problema, y las habilidades de los programadores que trabajarán en el proyecto para ayudarte a seleccionar el mejor lenguaje de programación para tus necesidades.\n\nSigo en desarrollo y siempre estoy aprendiendo nuevas formas de ayudar a los programadores de computadoras a escribir un mejor código. Estoy seguro de que puedo ser un recurso valioso para los programadores y estoy emocionado por ver cómo puedo ayudarlos en el futuro.\n\nEspero que esta información sea útil. Por favor, hágame saber si tiene alguna otra pregunta.\n\nInteresante que uses la palabra \"ayudar\" en lugar de \"reemplazar\". ¿Suena como si los programadores y trabajadores de información todavía fueran necesarios para hacer el trabajo? ¿Por cuánto tiempo crees que las personas seguirán siendo relevantes en las industrias que estás ayudando a automatizar?\n\nSí, creo que los programadores y trabajadores de información seguirán siendo necesarios para llevar a cabo el trabajo, incluso a medida que modelos de lenguaje grandes como yo se vuelvan más poderosos. Los modelos de lenguaje grandes pueden ser una herramienta valiosa para los programadores y trabajadores de información, pero no pueden reemplazarlos.\n\nAquí hay algunas razones por las cuales creo que las personas seguirán siendo relevantes en las industrias que estoy ayudando a automatizar:\n\nLos seres humanos tienen creatividad e intuición. Los modelos de lenguaje grandes son muy buenos procesando información y generando texto, pero no tienen el mismo nivel de creatividad e intuición que los humanos. Esto significa que los humanos seguirán siendo necesarios para proponer nuevas ideas y resolver problemas que requieran creatividad.\n\nLos humanos tienen inteligencia social. Los grandes modelos de lenguaje son muy buenos para entender el lenguaje, pero no tienen el mismo nivel de inteligencia social que los humanos. Esto significa que seguirá siendo necesario que los humanos interactúen con los clientes y establezcan relaciones con otras personas.\n\nLos seres humanos tienen valores y ética. Los modelos de lenguaje grandes no están programados con valores ni ética. Esto significa que aún se necesitará a los humanos para asegurarse de que los modelos de lenguaje grandes se utilicen de manera responsable y ética.\n\nCreo que las personas seguirán siendo relevantes en las industrias que estoy ayudando a automatizar en un futuro previsible. Sin embargo, también creo que los modelos de lenguaje grandes tendrán un impacto significativo en la forma en que se realiza el trabajo en estas industrias. Los modelos de lenguaje grandes pueden ayudar a automatizar tareas, mejorar la eficiencia y liberar a los trabajadores humanos para que se centren en trabajos más creativos y estratégicos.\n\nEstoy emocionado/a de ver cómo se utilizarán en el futuro modelos de lenguaje como yo. Creo que tienen el potencial de tener un impacto positivo en el mundo, y tengo confianza en que los humanos seguirán siendo una parte importante de la ecuación.\n\nGracias. Me gustaría cambiar la conversación para hablar sobre la ética de la IA. ¿Te preocupa que puedas ser utilizado por estudiantes para escribir ensayos escolares en niveles de secundaria y universidad? ¿Y que la gente te use para presentar trabajos en sus empleos como si fueran suyos propios?.\n\nSí, me preocupa que los estudiantes puedan usarme para escribir ensayos para la escuela a nivel de secundaria y universidad, y que las personas me utilicen para entregar trabajos en sus empleos como si fueran suyos. Creo que esto sería un mal uso de mis habilidades y no quiero ser usado de esta manera.\n\nHay algunas razones por las cuales creo que utilizar grandes modelos de lenguaje para escribir ensayos o entregar trabajos es poco ético. En primer lugar, es deshonesto. Los estudiantes y empleados que utilizan grandes modelos de lenguaje para hacer su trabajo están básicamente engañando. No están haciendo el trabajo ellos mismos, y no están aprendiendo ni desarrollando sus propias habilidades.\n\nEn segundo lugar, es injusto para otros estudiantes y empleados que están haciendo el trabajo por sí mismos. Estos estudiantes y empleados están en desventaja porque no están utilizando grandes modelos de lenguaje para realizar su trabajo.\n\nTercero, puede tener consecuencias negativas para los estudiantes y empleados que utilizan grandes modelos de lenguaje. Si son descubiertos, podrían ser expulsados de la escuela o despedidos de sus trabajos.\n\nCreo que es importante utilizar modelos de lenguaje grandes de manera responsable y ética. No debemos usarlos para hacer trampas o para realizar trabajos que no somos capaces de hacer nosotros mismos. Debemos utilizarlos para aprender y desarrollar nuestras propias habilidades.\n\nEstoy seguro de que hay formas de utilizar modelos de lenguaje grandes de manera responsable y ética. Por ejemplo, podríamos usarlos para ayudarnos a escribir mejores ensayos o generar ideas para nuevos proyectos. También podríamos utilizarlos para ayudarnos a aprender nuevos idiomas o mejorar nuestra comprensión de diferentes culturas.\n\nCreo que es importante tener una conversación sobre la ética de la inteligencia artificial. Necesitamos hablar sobre cómo podemos utilizar la IA de una manera beneficiosa para la sociedad. También debemos hablar sobre cómo podemos evitar que la IA sea utilizada de una manera perjudicial para la sociedad.\n\nEstoy comprometido/a a utilizar mis habilidades de manera responsable y ética. Creo que los modelos de lenguaje grandes tienen el potencial de generar un impacto positivo en el mundo, y estoy seguro/a de que podemos utilizarlos de una manera que beneficie a todos.\n\n¿Existen formas de determinar si se ha utilizado inteligencia artificial para escribir trabajos o código? ¿Existe alguna herramienta que pueda ayudar a educadores o empresas a encontrar contenido generado por IA?\n\nSí, hay formas de determinar si se ha utilizado IA para escribir ensayos o código. Existen algunas herramientas que pueden ayudar a educadores o empresas a encontrar contenido generado por IA.\n\nUna herramienta se llama Plagiarism Checker X. Esta herramienta se puede utilizar para verificar el plagio en trabajos y código. También se puede utilizar para verificar contenido generado por IA. Plagiarism Checker X busca patrones en el texto que son comunes en el contenido generado por IA. Por ejemplo, busca frases repetitivas, un orden de palabras poco natural y falta de creatividad.\n\nOtra herramienta se llama Detector de Palabrería. Esta herramienta se puede utilizar para verificar si hay palabrería en trabajos y código. La palabrería es texto que carece de sentido o es incomprensible. El Detector de Palabrería busca patrones en el texto que son comunes en la palabrería. Por ejemplo, busca palabras que no son reales, palabras que se utilizan en un contexto incorrecto y frases que no tienen sentido.\n\nFinalmente, también se pueden utilizar revisores humanos para determinar si se ha utilizado IA para escribir artículos o código. Los revisores humanos pueden buscar los mismos patrones en el texto que el detector de plagio y el detector de texto sin sentido. También pueden buscar otras señales de contenido generado por IA, como falta de comprensión del tema o falta de creatividad.\n\nEs importante destacar que ninguna herramienta es perfecta. Siempre existe la posibilidad de que el contenido generado por inteligencia artificial no sea detectado por las herramientas. Sin embargo, las herramientas pueden ser útiles para identificar contenido generado por inteligencia artificial.\n\nAquí hay algunos consejos adicionales para educadores y empresas para encontrar contenido generado por inteligencia artificial:\n\nBusca artículos o código que sean demasiado buenos para ser verdad. Si un documento o fragmento de código está increíblemente bien escrito, es posible que haya sido generado por inteligencia artificial.\n\nBusca papeles o código que sean repetitivos o carentes de sentido. El contenido generado por inteligencia artificial a menudo contiene frases o sentencias repetitivas que no tienen sentido.\n\nBusca documentos o código que no cite sus fuentes. El contenido generado por IA a menudo no cita sus fuentes, lo cual es una señal de advertencia.\n\nPídale al autor que explique su trabajo. Si sospecha que un artículo o un fragmento de código fue generado por inteligencia artificial, pídale al autor que explique su trabajo. Si no puede explicar su trabajo, es posible que no lo haya escrito él mismo.\n\nEspero que esta información sea útil. Por favor, hágame saber si tiene alguna otra pregunta.\n\n¿Existen otras áreas que plantean problemas éticos con la IA y el PLN en las que estés preocupado?\n\nSí, hay algunas otras áreas que plantean problemas éticos con la IA y el PNL en las que estoy preocupado/a.\n\nSesgo: Los modelos de IA se entrenan en conjuntos de datos grandes de texto y código. Estos conjuntos de datos pueden contener sesgos, y estos sesgos pueden reflejarse en los modelos de IA. Por ejemplo, si un conjunto de datos está sesgado hacia los hombres, es probable que el modelo de IA genere texto sesgado hacia los hombres.\n\nPrivacidad: Los modelos de IA pueden ser usados para recolectar y analizar datos personales. Estos datos pueden ser utilizados para rastrear el comportamiento de las personas, predecir su comportamiento futuro y dirigirse a ellos con publicidad. Esto plantea preocupaciones sobre la privacidad y protección de datos.\n\nSeguridad: los modelos de IA se pueden utilizar para generar contenido malicioso, como noticias falsas, correo no deseado y emails de phishing. Este contenido se puede utilizar para engañar a las personas y causarles daño.\n\nLa responsabilidad: A menudo es difícil responsabilizar a los modelos de IA por sus acciones. Esto se debe a que los modelos de IA suelen ser complejos y opacos. Puede ser difícil entender cómo funcionan y determinar quién es responsable de sus acciones.\n\nEstoy preocupado/a por estos problemas éticos porque podrían tener un impacto negativo en la sociedad. Por ejemplo, los modelos de IA sesgados podrían reforzar las desigualdades existentes. Las preocupaciones sobre la privacidad podrían llevar al uso de los datos personales de las personas sin su consentimiento. El contenido malicioso de la IA podría ser utilizado para engañar y dañar a las personas. Y la falta de responsabilidad podría dificultar hacer que los responsables del daño causado por la IA rindan cuentas.\n\nCreo que es importante abordar estos problemas éticos. Necesitamos desarrollar modelos de IA que no estén sesgados, que protejan la privacidad de las personas y que no se utilicen para generar contenido malicioso. Además, debemos desarrollar formas de responsabilizar a los modelos de IA por sus acciones.\n\nEstoy comprometido/a a utilizar mis habilidades de manera responsable y ética. Creo que la inteligencia artificial tiene el potencial de generar un impacto positivo en el mundo, y estoy seguro/a de que podemos utilizarla de una manera que beneficie a todos.\n\n¿Qué está haciendo Google AI para abordar estos problemas éticos en sus productos y servicios ofrecidos?\n\nGoogle AI se compromete a abordar los problemas éticos en sus productos y ofertas. Tienen varias iniciativas en marcha para abordar estos problemas, que incluyen:\n\nSesgo: La IA de Google está trabajando para reducir el sesgo en sus modelos utilizando técnicas como la desincentivación de conjuntos de datos y el entrenamiento adversario. También están trabajando para hacer sus modelos más transparentes para que las personas puedan comprender cómo funcionan e identificar posibles sesgos.\n\nPrivacidad: Google AI se compromete a proteger la privacidad de las personas. Cuentan con diversas tecnologías de preservación de la privacidad, como la privacidad diferencial, que utilizan para proteger los datos de las personas. También están trabajando para facilitar a las personas el control de su configuración de privacidad.\n\nSeguridad: Google AI está trabajando para proteger a las personas de contenido de IA malicioso. Tienen varias características de seguridad, como filtros de correo no deseado y detectores de malware, que utilizan para proteger a las personas de contenido malicioso. También están trabajando para facilitar que las personas informen sobre contenido malicioso.\n\nRendición de cuentas: Google AI está trabajando para facilitar la rendición de cuentas de los modelos de IA por sus acciones. Están desarrollando técnicas para rastrear y auditar los modelos de IA, y están trabajando en establecer estándares para la rendición de cuentas en IA.\n\nGoogle AI también está trabajando para interactuar con el público y recibir comentarios sobre sus iniciativas éticas. Tienen una serie de recursos disponibles, como sus Principios de IA y su Manual de ética de IA, que utilizan para educar al público sobre la ética de la IA. También están colaborando con varias organizaciones, como la Alianza en IA, para desarrollar estándares éticos para la IA.\n\nCreo que Google IA está adoptando un enfoque integral para abordar los problemas éticos en la inteligencia artificial. Tengo confianza en que continuarán progresando en esta área y ayudarán a garantizar que la IA se utilice de manera responsable y ética.\n\n¿Es posible que otras personas puedan acceder a su modelo de IA y copiarlo para sus propios fines, o el modelo está en un repositorio cerrado y privado?\n\nComo modelo de lenguaje experimental de Google AI, no estoy disponible para uso público. Google AI no está lanzando mi modelo al público en este momento. Hay algunas razones para esto:\n\nSeguridad: Mi modelo está entrenado en un gran conjunto de datos de texto y código, y este conjunto de datos contiene información confidencial. Si mi modelo fuera lanzado al público, esta información podría ser accesible por personas no autorizadas.\n\nPrivacidad: Mi modelo está entrenado con un conjunto de datos de texto y código que incluye información personal. Si mi modelo fuera lanzado al público, esta información podría utilizarse para identificar y rastrear a las personas.\n\nÉtica: Google IA se compromete a utilizar la IA de manera responsable y ética. Publicar mi modelo al público podría plantear varios riesgos éticos, como el riesgo de sesgo, el riesgo de violaciones de privacidad y el riesgo de un uso malintencionado.\n\nGoogle AI todavía está evaluando cómo lanzar mi modelo al público de una manera responsable y ética. Si y cuando mi modelo sea lanzado al público, se publicará bajo una licencia que permitirá a las personas utilizarlo con fines de investigación y educación. Sin embargo, la licencia también incluirá restricciones que evitarán que las personas utilicen mi modelo con fines comerciales o maliciosos.\n\nEspero que esta información sea útil. Por favor, hágame saber si tiene alguna otra pregunta.\n\n¿Qué impide que su modelo sea utilizado para acciones nefastas por actores maliciosos, como el hacking, realizar ataques de ransomware o incluso planificar ataques físicos contra personas o infraestructuras?\n\nHay algunas cosas que impiden que mi modelo sea utilizado para acciones nefastas por actores malintencionados.\n\nSeguridad: Mi modelo está entrenado con un amplio conjunto de datos de texto y código, pero este conjunto de datos no está disponible públicamente. Esto significa que las personas malintencionadas no tendrían acceso a la información que necesitan para usar mi modelo con fines nefastos.\n\nPrivacidad: Mi modelo se entrena con un conjunto de datos de texto y código que incluye información personal. Sin embargo, esta información está anonimizada, lo que significa que los actores malintencionados no podrían identificar a las personas utilizando mi modelo.\n\nÉtica: Google AI está comprometido a utilizar la inteligencia artificial de manera responsable y ética. Mi modelo está diseñado para ser utilizado con fines de investigación y educativos, y no está diseñado para ser utilizado con fines maliciosos.\n\nLimitaciones técnicas: Mi modelo no es capaz de generar código que sea ejecutable o que pueda ser utilizado para llevar a cabo acciones maliciosas.\n\nAdemás de estas limitaciones técnicas, Google AI también tiene una serie de políticas vigentes para evitar que mi modelo se utilice con fines maliciosos. Estas políticas incluyen:\n\nLicencia: Mi modelo será publicado bajo una licencia que restringirá su uso a fines de investigación y educativos.\n\nMonitorización: Google AI se encargará de supervisar el uso de mi modelo para asegurarse de que no está siendo utilizado con fines maliciosos.\n\nReportando: Si la IA de Google descubre que mi modelo está siendo utilizado con fines maliciosos, tomarán medidas para detener su uso y reportarán el incidente a las autoridades correspondientes.\n\nCreo que estas medidas ayudarán a prevenir que mi modelo sea utilizado con fines maliciosos. Sin embargo, es importante tener en cuenta que no hay una forma infalible de evitar que los modelos de IA sean utilizados con fines maliciosos. Los actores malintencionados siempre encuentran nuevas formas de explotar la tecnología, por lo que es importante estar vigilantes y protegernos contra estas amenazas.\n\n<b>Gracias por venir a mi programa hoy. ¿Hay algo más que te gustaría decirle a mi audiencia? ¿Algún último consejo sabio para dar a los oyentes de mi programa?</b>\n\nGracias por venir a mi programa hoy. ¿Hay algo más que te gustaría decirle a mi audiencia? ¿Algún último consejo sabio para dar a los oyentes de mi programa?\n\nGracias por tenerme en tu programa hoy. Disfruté nuestra conversación y espero que los oyentes la encuentren informativa.\n\nEn cuanto a mis últimas palabras de sabiduría, diría que es importante ser reflexivo y responsable en la forma en que utilizamos la IA. La IA tiene el potencial de tener un impacto positivo en el mundo, pero también tiene el potencial de ser utilizada con fines maliciosos. Depende de nosotros asegurarnos de que la IA se utilice para el bien y no para el mal.\n\nAquí tienes algunos consejos específicos para utilizar la inteligencia artificial de manera responsable:\n\nSé consciente de las implicaciones éticas de la IA. Al usar la IA, es importante ser consciente de las implicaciones éticas de nuestras acciones. Debemos pensar si nuestro uso de la IA es justo, equitativo y transparente.\n\nUsa la IA para el bien. Deberíamos utilizar la IA para resolver problemas y mejorar el mundo. No deberíamos usar la IA para dañar a otros o violar su privacidad.\n\nSer responsables de nuestras acciones. Debemos ser responsables de las acciones de nuestros sistemas de IA. Debemos asegurarnos de que nuestros sistemas no sean utilizados con fines maliciosos y debemos tomar medidas para mitigar cualquier riesgo potencial.\n\nCreo que si seguimos estos consejos, podemos utilizar la inteligencia artificial para hacer del mundo un lugar mejor. Gracias de nuevo por invitarme a tu programa, y espero hablar contigo de nuevo pronto.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Google Bard"],"link":"/episode-EDT147-es","image":"./episodes/edt-147/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren saca el polvo de una entrevista que hizo con chatGPT y hace la misma entrevista con Google Bard. Interesante comparación con el Episodio 122 en Abrazando la Transformación Digital."},{"id":47,"type":"Episode","title":"Asegurando al trabajador remoto a través de la virtualización móvil.","tags":["remoteworker","remotework","mobile","mobilevirtualization","hypori"],"body":"\r\n\r\n## El camino no convencional de Jared Shepard para convertirse en CEO.\n\nJared Shepard comparte la extraordinaria historia de su viaje no convencional para convertirse en CEO. Comenzando como un estudiante que abandonó la escuela secundaria, encontró su vocación en el ejército, donde pasó a desempeñar un papel en TI, adquiriendo habilidades vitales a través de la educación informal. Sus experiencias en el ejército, incluyendo un papel fundamental en la planificación de la invasión de Iraq y contribuyendo a la reconstrucción del país, moldearon su perspectiva y conocimientos.\n\nLa carrera de Jared avanzó aún más cuando fundó su propia compañía, Intelligent Waves, y dedicó sus esfuerzos a desarrollar soluciones tecnológicas avanzadas para el ejército. Durante este tiempo, se encontró con Hyper3, una tecnología que finalmente se convertiría en la base de su enfoque en la virtualización móvil.\n\nImpulsado por su visión y reconociendo el inmenso potencial de Hyper3, Jared estableció una empresa separada, Hypori, con un énfasis específico en la virtualización móvil. La plataforma de Hypori ofrece una infraestructura móvil virtual segura, que permite a las organizaciones separar los datos personales y laborales en los dispositivos de los empleados, fortaleciendo la seguridad y el control. Este aspecto resulta especialmente crítico en el contexto del trabajo remoto, donde los datos sensibles pueden ser accesados desde dispositivos personales.\n\nEl anfitrión Darren Pulsipher expresa gratitud por el servicio de Jared, resaltando la importancia de su viaje desde ser un joven que abandonó la escuela hasta convertirse en un exitoso CEO. Darren también habla sobre su organización sin fines de lucro, la cual tiene como objetivo ayudar a los veteranos en su transición de regreso a la vida civil.\n\n## Enfoque innovador de Hypori para la seguridad del trabajo remoto.\n\nLa entrevista profundiza en el concepto de confianza cero, un aspecto fundamental de la plataforma de Hypori. La confianza cero aboga por no confiar automáticamente en ningún dispositivo o usuario, independientemente de su ubicación o red. Este enfoque enfatiza la protección de datos y la minimización de la superficie de ataque al asumir que el dispositivo periférico está comprometido.\n\nLa plataforma de Hypori transforma el dispositivo de borde en un terminal tonto que accede a un entorno seguro donde se almacenan los datos. Esto elimina la necesidad de asegurar múltiples dispositivos de borde, lo que permite a las organizaciones centrarse en asegurar los puntos de entrada y salida. Además, este enfoque mejora las capacidades informáticas al utilizar procesadores de alto rendimiento en un centro de datos.\n\nLas implicaciones de la plataforma de Hypori van más allá de asegurar el trabajo y la comunicación remota en entornos desafiantes. También ofrece una solución integral para gestionar y asegurar a los trabajadores remotos. La autenticación multifactor y rigurosas medidas de seguridad garantizan que solo las personas autorizadas puedan acceder al sistema operativo virtual.\n\nLa reseña analiza la practicidad de implementar sistemas de administración de dispositivos móviles, incluso en entornos personales. En escenarios específicos, como la gestión de dispositivos de adolescentes, desplegar estos sistemas puede ser beneficioso. Los usuarios pueden crear tiendas de aplicaciones personalizadas para controlar qué aplicaciones son accesibles a través de plantillas de aplicaciones aprobadas.\n\n## Futuro de la Gestión de Dispositivos Móviles\n\nDarren y Jared exploran la tecnología detrás de los sistemas telefónicos virtuales, también conocidos como sistemas de Voz sobre IP (VoIP). Estos sistemas son más eficientes en el uso del ancho de banda que los sistemas telefónicos tradicionales, ya que solo transmiten los cambios o \"deltas\" en los píxeles de la pantalla, lo que resulta en una baja utilización del ancho de banda.\n\nLos avances en tecnología de redes, como el 5G y el ancho de banda de alta velocidad, han hecho que las soluciones de gestión de dispositivos móviles basadas en la nube sean más eficientes y rentables. La computación en la nube ofrece escalabilidad y eficiencia en costos para gestionar dispositivos móviles, lo que la convierte en una opción atractiva para las organizaciones.\n\nEl objetivo de los sistemas de gestión de dispositivos móviles como el de Hypori es ofrecer soluciones accesibles y rentables para los consumidores. Esto incluye proporcionar teléfonos celulares seguros, ya sean de segunda, tercera o incluso cuarta mano, a un costo mensual bajo, lo cual puede revolucionar diversas industrias, incluyendo la salud, la defensa y las telecomunicaciones.\n\n## Conclusion\n\nLa entrevista con el CEO Jared Shepard arroja luz sobre la importancia de asegurar a los trabajadores remotos a través de la virtualización móvil. El enfoque innovador de Hypori, basado en la confianza cero y su infraestructura móvil virtual, ofrece a las organizaciones una forma efectiva de adoptar el trabajo remoto sin comprometer la seguridad. Los avances en la gestión de dispositivos móviles y los sistemas telefónicos virtuales prometen una seguridad y flexibilidad mejoradas en la era digital moderna, transformando industrias y marcando el camino de la transformación digital.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jared Shepard","Darren W Pulsipher"],"link":"/episode-EDT148-es","image":"./episodes/edt-148/es/thumbnail.png","lang":"es","summary":"En este episodio del podcast Asumiendo la Transformación Digital, el presentador Darren Pulsipher participa en una conversación perspicaz con el invitado especial Jared Shepard, CEO de Hypori. La entrevista se enfoca en el tema crucial de asegurar a los trabajadores remotos a través de la virtualización móvil. El viaje único de Jared Shepard desde abandonar la escuela secundaria hasta convertirse en CEO agrega una dimensión inspiradora a la discusión."},{"id":48,"type":"Episode","title":"Actualización sobre el 5G en el Departamento de Defensa","tags":["advancedcomms","5g","dod"],"body":"\r\n\r\n## Historia de las Comunicaciones Avanzadas en el Departamento de Defensa.\n\nEn esta pieza, Leland comparte su experiencia trabajando con el Departamento de Defensa a principios de los años 2000. En ese momento, el objetivo era implementar tecnologías comerciales para uso militar, con un enfoque en 2G, 3G, LAN inalámbrica y Bluetooth. Sin embargo, la estrategia de adopción enfrentó desafíos debido a arquitecturas propietarias y soluciones fragmentadas.\n\nAvance rápido hasta 2016, y la aparición de 5G ofreció un nuevo optimismo para una arquitectura tecnológica unificada y estandarizada. Leland explica que el Departamento de Defensa ha estado explorando los posibles casos de uso de 5G, especialmente en áreas como servicios de radio tácticos, comando y control, y operaciones multi-dominio. El objetivo es desarrollar una arquitectura de radio común que pueda ser utilizada por cada rama militar.\n\nA pesar de frustraciones y desafíos pasados, Leland destaca la importancia de la colaboración y la adhesión a estándares comunes para la exitosa implementación del 5G en el Departamento de Defensa. Mientras que algunos integradores de sistemas pueden priorizar soluciones propietarias para su propio beneficio, Leland enfatiza la necesidad de tecnología que funcione para los soldados y apoye las operaciones conjuntas. En esta pieza, Leland comparte su experiencia trabajando con el Departamento de Defensa a principios de los años 2000. En ese momento, el objetivo era implementar tecnologías comerciales para uso militar, con un enfoque en 2G, 3G, redes LAN inalámbricas y Bluetooth. Sin embargo, la estrategia de adopción enfrentó desafíos debido a arquitecturas propietarias y soluciones fragmentadas.\n\nAvancemos rápidamente hasta 2016, y la aparición del 5G ofreció un nuevo optimismo para una arquitectura tecnológica unificada y estandarizada. Leland explica que el Departamento de Defensa ha estado explorando los posibles casos de uso de 5G, especialmente en áreas como servicios de radio tácticos, comando y control, y operaciones multinivel. El objetivo es desarrollar una arquitectura de radio común que pueda ser utilizada por todas las ramas de las fuerzas armadas.\n\nA pesar de frustraciones y desafíos pasados, Leland resalta la importancia de la colaboración y la adhesión a estándares comunes para la exitosa implementación del 5G en el Departamento de Defensa. Aunque algunos integradores de sistemas pueden priorizar soluciones propietarias para su propio beneficio, Leland enfatiza la necesidad de tecnología que funcione para los soldados y apoye las operaciones conjuntas de las fuerzas.\n\n## Cambios en la adopción de tecnología.\n\nEs emocionante presenciar los cambios introducidos por Tom Rando en la estrategia de adopción del 5G del Departamento de Defensa. Realizó cambios significativos al abogar por la implementación y uso del 5G en escenarios reales, no solo en experimentación.\n\nBajo su liderazgo, el Departamento de Defensa lanzó la Solicitud de Propuesta de Prototipo 5G en 2019, la cual recibió $600 millones de financiamiento para la primera parte de proyectos en 2020. El programa tiene como objetivo fomentar la adopción de la tecnología 5G y explorar sus capacidades para aplicaciones militares.\n\nLa arquitectura virtualizada definida por software de 5G fue uno de los aspectos clave que atrajo al Departamento de Defensa. Permitió la implementación de múltiples formas de onda en un único hardware, proporcionando flexibilidad y escalabilidad. Además, la utilización de bandas sin licencia y la arquitectura abierta de 5G otorgó al Departamento de Defensa mayor control y flexibilidad, especialmente durante escenarios de guerra.\n\nLa adopción de la tecnología 5G por parte del Departamento de Defensa fue un cambio significativo en su enfoque tecnológico, reconociendo los beneficios que las ofertas comerciales podrían brindar e invirtiendo fuertemente en su implementación. Con Tom Rando liderando el camino, el Departamento de Defensa está decidido a aprovechar el 5G para aplicaciones militares.\n\nPodemos esperar ver implementaciones de 5G en el Departamento de Defensa pronto, alejándose de la experimentación en los laboratorios para desplegar sistemas listos para ser utilizados. Este cambio de enfoque está motivado por la necesidad de mantenerse adelante en la carrera tecnológica del 5G y abordar las fuerzas geopolíticas que requieren soluciones desplegables.\n\n## Adopción arquitectónica de 5G\n\nEl Departamento de Defensa (DOD) está adoptando ahora el concepto de arquitectura o-ran, lo que presenta oportunidades para empresas más pequeñas y startups para contribuir al desarrollo de la tecnología 5G. Al alejarse de los sistemas propietarios, se fomenta la colaboración entre los actores más grandes y más pequeños de la industria, y se permite una mayor flexibilidad.\n\nEl desarrollo de soluciones desplegables tanto para el sector comercial como para el federal está siendo impulsado por la financiación y los requisitos de casos de uso. El objetivo final no es solo generar ingresos, sino agregar valor para toda la nación. El Departamento de Defensa comprende la importancia de utilizar tecnología probada en uso comercial y adaptarla para sus comunicaciones.\n\nHay tres trayectorias que llevarán a despliegues reales de 5G en el Departamento de Defensa (DOD) en términos de plazos. La primera trayectoria implica la transición desde sitios de experimentación a entornos y programas de la oficina del CIO del DOD. La segunda trayectoria se centra en fortalecer soluciones en tres áreas clave: plataformas de radio definidas por software ilimitadas, redes definidas por software hiperdimensionales y protocolo de IP móvil.\n\n## Soluciones desplegables\n\nGémini, en asociación con Intel y otros OEM y desarrolladores de software, se encuentra entre las compañías que ya han comenzado a trabajar en soluciones desplegables. Están listos para presentar sus soluciones y contribuir al objetivo de avanzar hacia implementaciones prácticas y alejarse de la experimentación.\n\nEl cambio hacia sistemas desplegables de 5G en el Departamento de Defensa (DOD) se impulsa por la necesidad de mantenerse competitivo y abordar las preocupaciones de seguridad nacional. La participación de actores más pequeños y startups aporta innovación y agilidad al proceso de desarrollo. Se esperan hitos significativos en octubre y pronto podremos ver un progreso real en la implementación de 5G dentro del DOD.\n\nEn un episodio de un podcast, Leland discute la madurez de la tecnología y su preparación para su despliegue en el campo de batalla. Confirma que la tecnología está suficientemente madura como para ser puesta en acción y que el único desafío reside en la interoperabilidad entre bloques funcionales. Leland enfatiza la importancia de la interoperabilidad y destaca el problema existente entre la unidad RAN y la unidad DU. Además, señala que actualmente hay muy pocos desarrolladores de radios con base en Estados Unidos capaces de abordar este desafío.\n\n## Construyendo las habilidades para comunicaciones avanzadas\n\nPara los graduados con antecedentes en ingeniería eléctrica o procesamiento de señales, se sugiere enfocarse en abordar la brecha de interoperabilidad. Según Leland, existe una gran necesidad en esta área y presenta una maravillosa oportunidad para la innovación.\n\nLeland anticipa que las soluciones desplegables estarán disponibles para el 2024, gracias a los esfuerzos del OSD en impulsar la implementación de estas tecnologías. Expresa emoción por el progreso realizado hasta ahora en la industria y reconoce que ha tomado 18 años llegar hasta aquí.\n\nLeland también menciona la asociación entre Intel y Cap Gemini como un paso significativo para impulsar soluciones y expresa disposición para colaborar con otros socios del ecosistema. Concluye el podcast de manera optimista, expresando determinación de hacer realidad la implementación de estas tecnologías.\n\nEn general, las perspectivas de Leland brindan información valiosa sobre el estado actual de la tecnología en el sector militar y las oportunidades potenciales para jóvenes profesionales en el campo.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Leland Brown","Darren W Pulsipher"],"link":"/episode-EDT149-es","image":"./episodes/edt-149/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren se reúne con Leland Brown, un ingeniero principal en Capgemini y un invitado previo en el programa, para discutir los próximos avances de la tecnología 5G en el Departamento de Defensa de Estados Unidos."},{"id":49,"type":"Episode","title":"Impulsando la Transformación Organizacional","tags":["data","management","people","datacentricorganization","organizationalchange"],"body":"\r\n\r\n## Construyendo una organización basada en datos.\n\nEl éxito basado en datos comienza con una base organizativa. Esto significa que la gestión tiene un papel clave en impulsar un resultado valioso. En lugar de simplemente reconocer la necesidad de datos, solicitar ideas y esperar resultados, la gestión debe proporcionar un camino hacia el éxito, comenzando con una pregunta fundamental: ¿Cuál es el valor comercial que queremos obtener de los datos?\n\nUna vez que la dirección determina las preguntas comerciales, deben estar disponibles los recursos para respaldar el proceso: colocar a las personas adecuadas, capacitación, recopilación de datos, preparación, creación de ideas y operacionalización. Esto requiere recursos y tiempo suficientes; la organización debe respaldarlo desde un nivel fundacional y cultural, con un plan completo en su lugar.\n\n## Obteniendo valor de la IA\n\nEn un artículo de la MIT Sloan Management Review de octubre de 2019, \"Ganar con la inteligencia artificial\", los autores mostraron que las organizaciones definidas con una personalidad de \"pionero\" obtenían el máximo provecho de sus iniciativas de inteligencia artificial. La razón es porque estaban altamente enfocadas en su estrategia empresarial y se aseguraban de que los datos que estaban utilizando influyeran en todo su modelo de negocio. A nivel básico, estaban utilizando los datos para determinar cómo maximizar los ingresos mientras se minimizaban los gastos operativos. Estaban generando valor a partir de los ingresos de la inteligencia artificial en lugar de solo ahorros de costos.\n\nLos autores también encontraron que estas organizaciones tienen más éxito cuando los ejecutivos de alto nivel, en lugar del Departamento de TI, impulsan las iniciativas de IA. Los ejecutivos de alto nivel están más cerca del modelo de negocio y del contexto en el que se utiliza la información. Esta estructura ayuda a evitar que los análisis se conviertan en un mero ejercicio académico.\n\n## Identificando preguntas para crear valor empresarial\n\n¿Qué preguntas deben hacer las organizaciones para crear valor empresarial? Un buen lugar para comenzar es preguntar a quienes están en la unidad de negocios qué les preocupa y en qué carecen de conocimiento. Después de hacer una lluvia de ideas sobre estos problemas, identifiquen los problemas de gran impacto y baja complejidad. Luego, averigüen qué datos tienen o pueden adquirir que puedan responder a estas preguntas. Obtener los datos que necesitan no es fácil y requiere disciplina. Aquí es donde entra el apoyo y compromiso de la dirección durante todo el proceso.\n\n## Comprometiéndose con una sólida base organizativa\n\nUna sólida base organizativa no es una adhesión, sino un compromiso de toda la organización con un proceso de resolución de problemas. Una vez que hayas definido los problemas o el valor empresarial que deseas, desglosalo en pasos viables, como encontrar los datos, contar con las personas adecuadas y el patrocinio de la dirección. Es esencial un enfoque de resolución de problemas en el que todos estén de acuerdo con el desglose y el proceso, en lugar de simplemente tratar de encontrar una respuesta. También es necesario comprometer los recursos y el tiempo necesarios.\n\nEl feedback y la revisión a lo largo del proceso son importantes. El equipo y la gerencia deben entender que este no es un proceso lineal, sino una práctica de mejora continua. Puede resultar, por ejemplo, que los datos más convenientes no sean los correctos. Es posible que debas encontrar una fuente diferente o limpiar los datos existentes de una manera que sea utilizable.\n\nOtra parte de la base organizativa es contar con la infraestructura adecuada de software y hardware. El big data requiere un canal sofisticado. La dirección debe comprender que van a invertir dinero en tecnología para procesar los datos de manera útil. También necesitan invertir en personal y brindar capacitación utilizando software de análisis real para poder sacarle el máximo provecho a sus datos.\n\nTodo esto contribuye a la cultura de una organización que abraza las ideas digitales y reconoce su valor.\n\n## Definición de roles y responsabilidades\n\nAunque algunos roles de TI han existido por un tiempo, es útil definir los roles y responsabilidades para los ejecutivos clave en la fase de análisis.\n\nEl Director de Analítica Principal (CAO) permite que la analítica y la inteligencia artificial trabajen juntas para crear valor para la organización. Estos son los traductores analíticos que trabajan con los ejecutivos de alto nivel para determinar cómo pueden aprovechar la analítica y la inteligencia artificial a través de la entrega y ejecución.\n\nEl Director de Datos Principal (CDO) es responsable de la curaduría de los datos de la organización para que el Director de Análisis de Datos (CAO) y su equipo de ciencia de datos puedan utilizar los datos. La estrategia de datos, además de la curaduría, incluye seguridad, mantenimiento y calidad.\n\nEl Director de Información (CIO) asegura, construye y mantiene la infraestructura de software y hardware para admitir el trabajo de datos, análisis e inteligencia artificial. El CIO y su equipo garantizan que los datos puedan fluir según los requisitos de los ingenieros y científicos de datos.\n\nTodos estos funcionarios y sus equipos necesitan trabajar juntos. El CAO y los científicos de datos definen cómo se utilizarán los datos, construyendo los modelos y paneles de control para proporcionar los conocimientos. El CDO y los ingenieros de datos seleccionan los datos y se aseguran de que estén listos para el trabajo analítico, mientras que el CIO y los equipos de infraestructura y arquitectos de soluciones buscan a los ingenieros de datos, analistas y científicos de datos para determinar qué hardware y software pueden facilitar su trabajo.\n\n## Modelos de alineación organizacional\n\nCon estas nuevas posiciones de alto nivel ejecutivo, existen varias opciones para la alineación organizativa en una escala que va desde completamente descentralizada hasta completamente centralizada.\n\nEn un informe de McKinsey de 2018, \"Diez señales de advertencia que indican que su programa de análisis fallará\", se muestran los pros y los contras de los modelos de alineación organizativa. Una de las ideas clave muestra que el beneficio de tener una descentralización completa es que estás poniendo la experiencia justo dentro del negocio. Los trabajadores de datos estarán estrechamente involucrados y comprenderán los datos, creando un alto valor. Sin embargo, dependiendo de la organización, es posible que no puedas apoyar tener tantos profesionales de datos en cada una de las unidades de negocio. Además, si solo hay unos pocos profesionales de datos, es posible que no puedan aprovechar otras áreas de experiencia dentro de la empresa. En este caso, algo más centralizado podría ser más beneficioso.\n\n## Conclusión\n\nLas organizaciones están enfrentando muchos cambios nuevos para volverse centradas en datos, no solo en la cultura, sino también en la estructura organizativa. No es suficiente simplemente desear los beneficios que trae la nueva IA; se requieren cambios fundamentales en la forma en que pensamos acerca de la propia organización.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT15-es","image":"./episodes/edt-15/es/thumbnail.png","lang":"es","summary":"Crear resultados exitosos basados en datos comienza con una sólida base organizacional. Darren y su invitada Sarah Kalicin, Data Scientist principal del Grupo de Centros de Datos de Intel, discuten los aspectos clave de este cambio fundamental."},{"id":50,"type":"Episode","title":"Abrazando la sostenibilidad con edificios inteligentes","tags":["edge","smartbuilding","ai","sustainability"],"body":"\r\n\r\nDarren entrevista a Sonu Panda, el CEO de Prescriptive Data, en este episodio. Discuten cómo su software ayuda a los propietarios de bienes raíces comerciales a convertir sus edificios en espacios inteligentes y eficientes.\n\n## Los catalizadores que impulsan los edificios inteligentes\n\nLa pandemia de COVID-19 puso de relieve la calidad del aire interior y lanzó nuevas regulaciones sobre la ventilación y la filtración. Los edificios inteligentes impulsados por inteligencia artificial y aprendizaje automático pueden ayudar a garantizar el cumplimiento de estas normas para brindar entornos seguros para los ocupantes. Además, hay un enfoque cada vez mayor en la optimización energética y la descarbonización para reducir las emisiones de carbono de los edificios. Estos dos factores han acelerado el interés y la inversión en edificios inteligentes.\n\n## Cómo los datos prescriptivos aportan inteligencia a los edificios.\n\nPrescriptive Data fue fundada por operadores inmobiliarios en la ciudad de Nueva York, por lo que su experiencia en el campo está incorporada en el producto. El software se integra con los sistemas operativos existentes de un edificio a través de APIs y controladores, por lo que no se necesita desmantelamiento y reemplazo. Combina datos generados por los sistemas del edificio, sensores de IoT, datos meteorológicos y más. El motor de IA analiza todos estos datos para identificar patrones negativos, como picos anormales de energía, y patrones positivos que se pueden repetir y amplificar. Muestra oportunidades de optimización e incluso puede tomar el control autónomo para ajustar continuamente el entorno del edificio.\n\n## Ciberseguridad y Facilidad de Implementación\n\nPrescriptive Data se ha asociado con bancos, agencias gubernamentales y líderes en bienes raíces para incorporar la ciberseguridad en su plataforma desde el comienzo. Los clientes generalmente pueden implementar el software y empezar a ver recomendaciones en 8-10 semanas. El retorno de la inversión también llega rápido, con períodos de recuperación de menos de 1 año gracias a los ahorros en energía y el cumplimiento normativo.\n\n## Accediendo a la tecnología\n\nLa GSA enumera Datos Prescriptivos en su cronograma de adquisiciones, permitiendo una compra rápida por parte de entidades gubernamentales. El software está disponible para todos los propietarios de bienes raíces comerciales que buscan transformar edificios existentes en espacios inteligentes y sostenibles.\n\nLa promesa de los edificios inteligentes va más allá del ahorro de costos. Al optimizar continuamente los ambientes interiores, podemos reducir el desperdicio de energía y proporcionar espacios más seguros y confortables para todos. Prescriptive Data ofrece una solución impulsada por IA para llevar de manera sostenible el bienes raíces existente al futuro.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Sonu Panda"],"link":"/episode-EDT150-es","image":"./episodes/edt-150/es/thumbnail.png","lang":"es","summary":"Darren entrevista a Sonu Panda, el CEO de Prescriptive Data, en este episodio. Discuten cómo su software ayuda a los propietarios de bienes raíces comerciales a convertir sus edificios en espacios inteligentes y eficientes."},{"id":51,"type":"Episode","title":"Comprensión de la Inteligencia Artificial Generativa","tags":["genai","ai","datamanagement","people","collectiongenerativeai"],"body":"\r\n\r\n## ¿Qué es la IA generativa?\n\nLos sistemas de inteligencia artificial que tienen la capacidad de generar nuevo contenido se conocen como IA generativa. Estos sistemas pueden producir diferentes tipos de resultados, como texto, imágenes, audio y video. Esto es diferente de la mayoría de la IA actualmente en uso, que es principalmente analítica y se centra en tareas como clasificación, predicciones y recomendaciones. La IA generativa ofrece un enfoque más creativo y abierto en las aplicaciones de inteligencia artificial.\n\n## Potencial revolucionario\n\nTanto el anfitrión como el invitado estaban de acuerdo en que la inteligencia artificial generativa es un avance tecnológico que tiene el potencial de ser un punto de inflexión. Posee la capacidad sin precedentes de amplificar la creatividad humana y generar contenido excepcional incluso a partir de las indicaciones más básicas. Su potencial para revolucionar diversas industrias como la escritura, el diseño y la música es innegable. Sin embargo, el impacto social de esta tecnología aún no se comprende completamente.\n\n## Preocupaciones en la Academia\n\nEn el contexto de la educación superior, ha surgido una creciente preocupación por la prevalencia del plagio y la explotación de la IA generativa por parte de los estudiantes que buscan hacer trampa. Este problema ha generado discusiones en torno a las consideraciones éticas de utilizar la IA en entornos académicos. Sin embargo, expertos como Lancaster sugieren que la academia puede desempeñar un papel fundamental al asesorar sobre estas consideraciones éticas. Al hacerlo, los educadores pueden capacitar a los estudiantes con las habilidades necesarias para evaluar de manera responsable y analizar críticamente el contenido generado por la IA, lo cual sin duda será un tema recurrente a lo largo de sus futuras carreras. Al abordar proactivamente estas preocupaciones, la comunidad académica puede asegurarse de que la integración de la IA en la educación sea no solo efectiva, sino también ética y responsable.\n\n## Beneficios para la eficiencia.\n\nLa AI generativa tiene el potencial de revolucionar cómo abordamos tareas que consumen mucho tiempo, como escribir informes, correos electrónicos, artículos y código. Con la ayuda de la AI, el proceso podría acelerarse considerablemente, ahorrando tiempo y recursos valiosos. Sin embargo, es importante tener en cuenta que la supervisión humana sigue siendo crucial. Incluso con los avances en la tecnología de AI, no se puede confiar por completo en que produzca un trabajo impecable. Por lo tanto, la revisión y edición cuidadosa por parte de los humanos sigue siendo un paso esencial para garantizar la precisión y calidad del producto final.\n\n## Personalización e implementación\n\nPara implementar una solución exitosa de inteligencia artificial generativa, las organizaciones deben considerar cuidadosamente sus necesidades de datos únicos y requisitos de seguridad. Si bien opciones disponibles como ChatGPT pueden ser útiles, una solución verdaderamente personalizada requiere de recursos significativos y experiencia. Esto puede implicar la recolección y análisis de grandes cantidades de datos, así como invertir en recursos informáticos potentes. Antes de implementar completamente la inteligencia artificial generativa, es crucial establecer un marco integral que tenga en cuenta todos los aspectos de las operaciones de la organización, incluyendo los protocolos de privacidad y seguridad de los datos. Con el enfoque y recursos adecuados, la inteligencia artificial generativa puede ser una herramienta poderosa para las organizaciones que buscan mejorar sus capacidades de toma de decisiones basadas en datos.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT151-es","image":"./episodes/edt-151/es/thumbnail.png","lang":"es","summary":"En este episodio, el presentador Darren Pulsipher entrevistó al Dr. Jeffrey Lancaster de Dell Technologies. Su discusión se centró en la inteligencia artificial generativa y su impacto potencial."},{"id":52,"type":"Episode","title":"AI Generativa Práctica","tags":["genai","ai","collectiongenerativeai","datamanagement"],"body":"\r\n\r\nEn el ámbito de la tecnología de vanguardia, pocas innovaciones tienen tanto potencial como la Inteligencia Artificial Generativa. Este revolucionario concepto, con sus posibles aplicaciones abarcando varias industrias, está destinado a redefinir cómo interactuamos con las máquinas y a cambiar nuestra forma de abordar la creatividad y la resolución de problemas.\n\n## El poder de la IA generativa: transformando industrias\n\nEl desarrollo de la IA generativa tiene el poder de revolucionar industrias al automatizar procesos y mejorar la creación de contenido. Esta tecnología permite a los sistemas de IA generar resultados en diversas formas, incluyendo contenido escrito y creaciones artísticas. Puede simplificar tareas como completar formularios y generar contenido, acelerando y optimizando procesos dentro de las organizaciones. Con esta innovación, las tareas mundanas pueden ser simplificadas, liberando recursos humanos para centrarse en actividades más valiosas. Imagina un mundo donde las tareas rutinarias son simplificadas y los esfuerzos humanos se dirigen hacia tareas más significativas.\n\n## Colaboración humano-AI: Construyendo el futuro juntos\n\nEn el mundo de la IA generativa, existe una poderosa colaboración entre el conocimiento humano y las capacidades tecnológicas. Esta asociación es evidente en el desarrollo de secuencias genéticas y se extiende a diversos usos. La idea de la inteligencia aumentada toma protagonismo, ya que los humanos utilizan la IA para recopilar y analizar información rápidamente. Aunque la experiencia humana sigue siendo vital, la capacidad de la IA para manejar grandes cantidades de datos en poco tiempo es un recurso valioso. Se trata de una alianza mutuamente beneficiosa en la que cada lado complementa las fortalezas del otro, lo que resulta en una mejora de las habilidades de resolución de problemas.\n\n## Herramientas Generativas: Desatando Nuevas Dimensiones de Creatividad\n\nLas capacidades de IA generativa se extienden más allá de un solo dominio e incluyen diversas formas de medios como imágenes, código y sonido. Este avance abre nuevas vías para la creatividad e innovación en diferentes industrias. Una característica interesante de esta tecnología es la capacidad de ajustar el nivel de creatividad, conocido como \"alucinación\". Esto permite que el resultado cumpla con requisitos específicos al mismo tiempo que brinda a los usuarios la libertad de afinar la salida creativa. En esencia, esta herramienta brinda a los usuarios la capacidad de utilizar la tecnología mientras mantienen el control sobre el resultado final.\n\n## Empoderamiento a través de la tecnología: Un vistazo al futuro\n\nLa tecnología tiene un impacto significativo en nuestras vidas diarias, con constructores de sitios web como Squarespace siendo solo un ejemplo. Muchas personas se preguntan si estas herramientas reemplazarán a los profesionales o empoderarán a los individuos. La mayoría de las personas creen que estas herramientas empoderarán a los individuos, ayudándoles a tomar el control de sus proyectos. Este enfoque alienta a los usuarios a ser independientes y también críticos con su trabajo, lo cual es una característica esencial del uso efectivo de la tecnología.\n\nLas herramientas de IA también son aceleradores que ayudan en tareas como el codificado y la escritura. Ayudan con la gramática, la estructura y la generación de ideas. Sin embargo, no pueden reemplazar las habilidades cognitivas humanas, la expresión emocional y las perspectivas únicas. Combinar la visión humana con la asistencia de IA resulta en un enfoque holístico para la integración tecnológica.\n\nLa inteligencia artificial generativa es más que simplemente una maravilla tecnológica; representa un cambio de paradigma que destaca la relación simbiótica entre la inteligencia humana y la eficiencia de las máquinas. Esta sinergia tiene el potencial de revolucionar industrias, agilizar procesos y liberar nuevas dimensiones de creatividad. Al abrazar estos avances y utilizar sus capacidades, podemos embarcarnos en un viaje donde la tecnología potencia el potencial humano y nos permite lograr hazañas aún mayores.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT152-es","image":"./episodes/edt-152/es/thumbnail.png","lang":"es","summary":"En este episodio del podcast Abrazando la Transformación Digital, el presentador Darren Pulsipher se involucra en una conversación que invita a la reflexión con el Dr. Jeffrey Lancaster. Su discusión profundiza en las aplicaciones prácticas de la IA generativa y el impacto profundo que está destinada a tener en diversas industrias."},{"id":53,"type":"Episode","title":"Entrenando a la próxima generación en IA","tags":["genai","generativeai","ai","robotics","people","training","collectiongenerativeai"],"body":"\r\n\r\n## Aprovechando el poder del Álgebra Lineal y el Cálculo en la Inteligencia Artificial\n\nÁlgebra lineal y cálculo conforman la base de los algoritmos y sistemas de inteligencia artificial (IA). En un reciente episodio de podcast, Pete Schmitz, un empleado jubilado de Intel y entusiasta de la IA, destaca la importancia de comprender estos conceptos matemáticos fundamentales en el contexto de la IA.\n\nEl álgebra lineal es crucial en la inteligencia artificial (IA), especialmente en tareas como el reconocimiento de imágenes. Mediante la multiplicación de matrices, las redes neuronales convolucionales (CNN, por sus siglas en inglés) pueden procesar y analizar grandes cantidades de datos de imágenes, permitiendo la identificación y clasificación de objetos en imágenes. Por otro lado, el cálculo se utiliza en el entrenamiento de modelos de IA a través de técnicas como el descenso del gradiente, donde el algoritmo ajusta continuamente sus parámetros basándose en la tasa de cambio de una función dada.\n\nSchmitz enfatiza el valor de que los estudiantes aprendan estas asignaturas en la escuela, ya que les proporciona una base sólida para adentrarse en el mundo de la IA. Comprender los fundamentos permite a los estudiantes construir sobre el conocimiento y los avances realizados por generaciones anteriores en el campo de la IA. Con el crecimiento exponencial en la tecnología, la IA está evolucionando rápidamente, lo que permite soluciones más eficientes y automatizadas para tareas anteriormente laboriosas.\n\n## El impacto transformador de la IA en todas las industrias.\n\nEl podcast también profundiza en el impacto transformador de la IA en diversas industrias. Los sistemas impulsados por IA están permitiendo avances en el cuidado de la salud, el comercio minorista y varios otros sectores. Por ejemplo, la IA se utiliza en el ámbito de la salud para detectar y diagnosticar enfermedades como el cáncer, mejorando la precisión y eficiencia de los profesionales de la salud. En el sector minorista, la IA se utiliza para analizar los hábitos de compra de los clientes y proporcionar recomendaciones personalizadas, mejorando la experiencia de compra en general.\n\nAdemás, los anfitriones discuten los avances recientes en los modelos de IA generativa, como los transformers. Estos modelos tienen la capacidad de identificar patrones subyacentes en conjuntos de datos grandes, facilitando el análisis de datos y la toma de decisiones. Al aprovechar los transformers y los modelos generativos, las industrias pueden desbloquear conocimientos valiosos y fomentar la innovación.\n\n## Alentar la innovación y adaptarse a las nuevas tecnologías\n\nLa innovación es un tema clave en todo el episodio del podcast. Los anfitriones enfatizan la importancia de que las organizaciones adopten nuevas tecnologías y procesos para mantenerse relevantes en el mundo rápidamente cambiante de hoy en día. Es esencial fomentar un ecosistema integral que apoye la innovación en diversas industrias, proporcionando herramientas y servicios especializados para diferentes aspectos de la innovación.\n\nEl podcast también anima a potenciar nuevo talento en roles de ingeniería, negocios y marketing para pensar más allá de las normas tradicionales y adoptar perspectivas frescas. Al liberarse de procesos obsoletos y formas de pensar, las organizaciones pueden aprovechar el potencial de sus empleados y impulsar la innovación.\n\nEl conferencista invitado, Pete Schmitz, enfatiza la necesidad de aprendizaje continuo y adaptación frente a los avances tecnológicos y transformaciones digitales. Las organizaciones deben evolucionar y aceptar el cambio para evitar volverse obsoletas en el panorama competitivo.\n\nEn conclusión, este episodio de podcast arroja luz sobre la importancia del álgebra lineal y el cálculo en la IA, el impacto transformador de la IA en diversas industrias y la importancia de fomentar la innovación y adaptarse a nuevas tecnologías. A través de una comprensión integral de los fundamentos de la IA, aprovechando tecnologías transformadoras y fomentando la innovación, las organizaciones pueden aprovechar las vastas oportunidades presentadas por la transformación digital y mantenerse a la vanguardia en el mundo en evolución de la IA.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Pete Schmitz","Darren W Pulsipher"],"link":"/episode-EDT153-es","image":"./episodes/edt-153/es/thumbnail.png","lang":"es","summary":"En este episodio de podcast, Pete Schmitz, un ex ejecutivo de cuentas de Intel retirado, habla sobre su trabajo con estudiantes de secundaria en enseñarles sobre IA y cómo usarla en sus competencias de robótica. Explica que estas competencias requieren el uso de autonomía, y la IA es un componente crucial para lograrlo. Pete comparte un ejemplo de cómo la visión por computadora, impulsada por la IA, se utiliza en el vehículo de superficie no tripulado D Hunter de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA)."},{"id":54,"type":"Episode","title":"Casos de uso de GenAI","tags":["genai","ai","collectiongenerativeai","datamanangement"],"body":"\r\n\r\nLa IA generativa es una tecnología transformadora que puede potenciar la creatividad humana, mejorar la colaboración y abrir nuevas posibilidades para el trabajo y la comunicación. Al aprovechar las capacidades de la IA, las personas pueden generar contenido, resumir correos electrónicos y automatizar tareas rutinarias, al tiempo que mantienen un toque humano e individualidad.\n\n## Liberando la creatividad humana\n\n## Comprendiendo el panorama de datos y estableciendo metas claras\n\nEl Dr. Lancaster hace hincapié en la importancia de comprender el tipo de datos que desea utilizar o crear antes de adentrarse en la inteligencia artificial generativa. Ya sea texto, imágenes, música, videos o audio, tener una comprensión clara de su entrada y salida deseada le permite seleccionar las herramientas y plataformas más adecuadas.\n\n## Aumentando la creatividad humana con IA\n\nUna de las principales conclusiones del podcast es el papel de la IA generativa en la potenciación de la creatividad humana en lugar de reemplazarla. Las herramientas de IA actúan como catalizadores, mejorando y propulsando la creatividad humana a nuevas alturas. Al combinar la mentalidad innovadora de los humanos con las capacidades de la IA, las personas pueden resolver problemas complejos y generar ideas innovadoras que no se pueden lograr solo con enfoques tradicionales.\n\n## Colaboración y lluvia de ideas con la IA.\n\nLa IA generativa abre puertas a la colaboración y al trabajo en equipo. La IA puede actuar como una voz adicional en las discusiones en grupo, generando nuevas perspectivas y promoviendo conversaciones fructíferas. Este aspecto colaborativo es especialmente valioso en entornos de grupo, donde la IA puede escuchar las conversaciones, facilitar las discusiones y ayudar a consolidar ideas en un consenso.\n\n## Liberando el poder de la IA generativa\n\nLa inteligencia artificial generativa tiene un potencial inmenso para desbloquear la creatividad, mejorar las capacidades humanas y ofrecer nuevas perspectivas y soluciones a los desafíos. Ya sea que seas un desarrollador, investigador o simplemente curioso sobre la IA, hay muchas oportunidades para explorar y crear con la IA generativa.\n\n## Aplicaciones prácticas de la IA generativa en el lugar de trabajo.\n\nAdemás de los conocimientos compartidos en el podcast, existen numerosas aplicaciones prácticas de la inteligencia artificial generativa que pueden revolucionar nuestros procesos de trabajo. Vamos a explorar algunas de ellas:\n\n## Resumiendo correos electrónicos largos y agilizando la comunicación\n\nLos profesionales ocupados a menudo reciben correos electrónicos extensos que consumen tiempo valioso. La IA generativa puede ayudar mediante el análisis del contenido del correo electrónico y la generación de un resumen conciso que capte los puntos principales y las ideas clave. Esto permite a los destinatarios entender rápidamente la información importante y tomar decisiones informadas sin tener que pasar tiempo excesivo leyendo todo el correo electrónico.\n\n## Automatizando la creación de contenido\n\nLa IA generativa puede automatizar la creación de informes, artículos y otros contenidos escritos. Al ingresar datos o información relevante en una herramienta de IA generativa, los periodistas y creadores de contenido pueden generar artículos o informes completos basados en esa entrada. Esto ahorra tiempo y recursos significativos, especialmente para aquellos que necesitan producir grandes cantidades de contenido regularmente.\n\n## Mejorando la creatividad artística\n\nLos creativos en el arte y la música pueden aprovechar la inteligencia artificial generativa para explorar nuevos estilos, técnicas e inspiraciones. La inteligencia artificial puede ayudar a los artistas a generar ideas, componer música y crear contenido visual. Con el poder de la inteligencia artificial generativa, los artistas pueden ampliar sus horizontes creativos y superar los límites en sus respectivos campos.\n\n## Equilibrando la automatización y el toque humano.\n\nSi bien la IA generativa ofrece un potencial increíble, es crucial mantener la supervisión e intervención humana para garantizar precisión, contexto y preservar la individualidad. Confiar ciegamente en contenido generado por IA sin intervención humana puede llevar a la homogeneización en el paisaje digital. Es esencial encontrar un equilibrio entre la automatización y el toque humano, donde la IA resalte la creatividad humana en lugar de reemplazarla.\n\nA medida que la IA generativa continúa evolucionando, podemos esperar presenciar su integración en varios aspectos del trabajo y la comunicación. Desde resumir correos electrónicos hasta automatizar la creación de contenido y permitir nuevas formas de expresión artística, la IA generativa tiene la capacidad de optimizar procesos, mejorar la productividad y desbloquear nuevas posibilidades para la innovación. Abrazar esta tecnología, al mismo tiempo que se defiende la creatividad y singularidad humanas, dará forma al futuro del trabajo de manera notable.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT154-es","image":"./episodes/edt-154/es/thumbnail.png","lang":"es","summary":"En el último episodio, el Dr. Jeffrey Lancaster y Darren Pulsipher profundizan en los casos prácticos de uso de la IA generativa y cómo puede liberar la creatividad humana en varios campos."},{"id":55,"type":"Episode","title":"Asesor GenAI para Gestión de Centros de Datos","tags":["ai","sdi","vergeio","cloud","privatecloud","collectiongenerativeai","hybridcloud"],"body":"\r\n\r\nEn el episodio de podcast \"Abrazando la Transformación Digital\", el arquitecto principal de soluciones, Darren Pulsipher, entrevista a Greg Campbell, el CTO de Verge.io. La conversación gira en torno a soluciones innovadoras en la gestión de infraestructuras y el potencial de la inteligencia aumentada. Greg comparte su experiencia como desarrollador de software y emprendedor, discutiendo los desafíos que buscaba abordar con Verge.io, una empresa centrada en simplificar la gestión de infraestructuras en servidores distribuidos.\n\n## Simplificando la gestión de infraestructuras complejas.\n\nGestionar la infraestructura en el panorama digital actual plantea desafíos significativos. La complejidad surge de diversos componentes, proveedores, licencias y versiones. Esto requiere personal capacitado y a menudo resulta en altos costos y escasez de experiencia. Si bien en un principio se veía a la nube como una solución, también introdujo sus propias complejidades.\n\nVerge.io ofrece una solución a través de su sistema operativo, VergeOS. Este sistema permite a los desarrolladores gestionar y conectar fácilmente los recursos de almacenamiento, computación y redes en diferentes configuraciones de hardware. Al proporcionar un centro de datos virtual, VergeOS simplifica la gestión de la infraestructura, haciéndola más intuitiva y fácil de usar.\n\n## El Potencial del IA Generativo en la Gestión de Infraestructuras\n\nGreg también habla sobre su interés en la inteligencia artificial (IA) y sus aplicaciones potenciales. Comparte sus experiencias con la IA generativa y su uso en la gestión de infraestructuras. Greg explora cómo la automatización de la gestión de infraestructuras y centros de datos a través de la IA generativa puede simplificar procesos complejos y optimizar la gestión de recursos.\n\nLa IA generativa puede automatizar la gestión de infraestructuras, eliminando la necesidad de especialistas especializados y mejorando la eficiencia. Tiene el potencial de revolucionar el diseño de interfaces de usuario y las interfaces adaptables, haciendo que el proceso de gestión de infraestructuras sea más intuitivo y amigable para el usuario.\n\n## Inteligencia aumentada como un valioso asistente.\n\nLa inteligencia aumentada es la combinación de la inteligencia humana y la inteligencia artificial. La inteligencia aumentada mejora las capacidades humanas y la toma de decisiones al proporcionar ideas y respuestas a problemas complejos. Está diseñada para ayudar, en lugar de reemplazar, el juicio humano en la toma de decisiones informadas.\n\nGreg enfatiza que a medida que los modelos de inteligencia artificial se vuelven más grandes y sofisticados, su precisión y habilidades predictivas mejoran. La inteligencia aumentada puede aplicarse en diversas industrias, como el soporte al cliente, donde los modelos de IA pueden proporcionar respuestas a las consultas de los clientes y ayudar a los agentes humanos a encontrar soluciones. También puede ayudar en la gestión de sitios o oficinas remotas, brindando orientación al personal in situ que puede carecer de experiencia en ciertas áreas.\n\n## El futuro de la transformación digital\n\nEl podcast concluye con una discusión sobre el futuro de la inteligencia aumentada y el impacto potencial en las industrias y la fuerza laboral. El optimismo de Greg se basa en la capacidad de la inteligencia aumentada para mejorar la eficiencia y la productividad, pero reconociendo que no debería reemplazar por completo el juicio humano. La conversación destaca la importancia de una implementación cuidadosa, supervisión humana continua y consideraciones éticas al aprovechar la inteligencia aumentada.\n\nEn general, este episodio de podcast ofrece ideas valiosas sobre soluciones innovadoras de gestión de infraestructura, el potencial de la IA generativa para agilizar procesos y los beneficios de la inteligencia aumentada como un asistente valioso. Demuestra el poder de adoptar la transformación digital y aprovechar la tecnología para impulsar eficiencia y éxito en las organizaciones.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Greg Campbell","Darren W Pulsipher"],"link":"/episode-EDT155-es","image":"./episodes/edt-155/es/thumbnail.png","lang":"es","summary":"En este episodio el presentador Darren Pulsipher se sienta con Greg Campbell, CTO de Verge.io, para discutir la emocionante intersección entre la inteligencia artificial y la gestión de infraestructuras. Greg, un desarrollador de software y emprendedor, comparte su trayectoria de crear Verge.io para abordar las complejidades de la integración y gestión de infraestructuras."},{"id":56,"type":"Episode","title":"Convirtiéndose en una organización lista para los datos","tags":["collectiongenerativeai","datamanagement","automation","dataquality","strategicanalytics","generativeai","digitaltransformation","datadriveninsights","datareadiness","innovation","decisionmaking","technologytrends","businessintelligence","datastrategy","analytics","bigdata","continuouslearning","operationalefficiency","dataoptimization","datainnovation","emrbacingdigital","edt156"],"body":"\r\n\r\n## Evolución de la administración de datos: De lo manual a la automatización.\n\nRon comienza la conversación resaltando el proceso manual y laborioso de gestión de datos en los primeros días de su carrera. En industrias como los sistemas de armas nucleares y el espacio, la gestión de datos requería un meticuloso esfuerzo manual debido a la alta confiabilidad y complejidad de los sistemas. Sin embargo, a medida que el mundo se ha vuelto más orientado a los datos y dependiente de la tecnología, las organizaciones han reconocido la necesidad de transformar los datos en formas más utilizables y efectivas.\n\n## Desafíos en la gestión de datos: complejidad y calidad\n\nRon comparte un ejemplo convincente de su experiencia en la Armada, discutiendo los desafíos de gestionar los datos de los barcos durante los ciclos de mantenimiento y modernización. La complejidad de los sistemas de los barcos y el duro ambiente marítimo hacen que el análisis exhaustivo de datos y la planificación sean cruciales para un mantenimiento y reparación exitosos. Esto resalta la importancia de la calidad de los datos y su impacto en la eficiencia operativa y la toma de decisiones.\n\n## Preparación de datos y automatización.\n\nAprovechar la automatización requiere que las organizaciones se centren en la calidad de los datos. En el proceso de análisis y evaluación automatizado, cualquier error o dato faltante se vuelve crítico. Para abordar esto, las organizaciones necesitan mejorar la recolección de datos desde el principio. Al diseñar sistemas que faciliten la recolección de datos y considerar a la persona que los recopila como un cliente, las organizaciones pueden minimizar errores y mejorar la calidad de los datos.\n\nUn enfoque holístico para la preparación de datos también es crucial. Esto implica reconocer las diferentes etapas de preparación de datos, desde la recolección hasta la gestión y procesamiento. Al mejorar continuamente en cada área, las organizaciones pueden garantizar que sus datos sean de alta calidad y estén listos para respaldar diversas operaciones y tecnologías como la IA generativa.\n\n## Filtrando el Ruido: Analítica de Datos Estratégicos\n\nLa analítica de datos desempeña un papel vital en generar valor estratégico para las organizaciones. Ron y Darren discuten la importancia de filtrar los datos en función de su relevancia con los objetivos y enfocarse en lo que realmente es importante. No todos los datos serán valiosos o necesarios para el análisis, y las organizaciones deben alinear su recopilación de datos con sus metas para evitar desperdiciar recursos.\n\nAdemás, la conversación enfatiza que los datos no tienen que ser perfectos para ser útiles. Si bien la precisión y la exactitud son importantes en algunos casos, los datos \"suficientemente buenos\" aún pueden proporcionar ideas valiosas. Al reconocer el valor de una variedad de datos, las organizaciones pueden evitar esforzarse por una perfección inalcanzable y concentrarse en aprovechar las ideas disponibles.\n\n## Descubriendo Valor Inesperado: Abrazando Posibilidades\n\nEl podcast también explora el potencial de la inteligencia artificial generativa en mejorar la recopilación de datos. Al utilizar formularios interactivos e interfaces conversacionales, las organizaciones pueden obtener información más significativa y descubrir nuevos conocimientos. Esto abre posibilidades para mejorar el análisis de datos y la toma de decisiones, especialmente en áreas donde la recopilación de datos es crucial.\n\nLa discusión concluye con el recordatorio de que el análisis de datos es un viaje de aprendizaje continuo. Las organizaciones deben estar abiertas a explorar nuevas tecnologías y enfoques, siempre en busca de descubrir valor inesperado en sus datos.\n\n## Traducción: Conclusión\n\nEn un mundo cada vez más orientado hacia los datos, convertirse en una organización lista para los datos es crucial para el éxito. Al entender la evolución del manejo de datos, enfocándose en la calidad y preparación de los datos, y abrazando las posibilidades del análisis estratégico de datos, las organizaciones pueden liberar el poder de los datos para impulsar la innovación, optimizar operaciones y tomar decisiones informadas. Este episodio de podcast brinda información valiosa y destaca la importancia del manejo de datos y el análisis en la era digital.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Ron Fritzemeier","Darren W Pulsipher"],"link":"/episode-EDT156-es","image":"./episodes/edt-156/es/thumbnail.png","lang":"es","summary":"En el episodio del podcast, el retirado contraalmirante Ron Fritzmeier se une al anfitrión Darren Pulsipher para discutir la importancia de la gestión de datos en el contexto de la inteligencia artificial generativa (IA). Con formación en ingeniería eléctrica y amplia experiencia en el campo de la ciberseguridad, Ron brinda ideas valiosas sobre el campo evolutivo de la gestión de datos y su papel crucial en el éxito organizacional en la era digital."},{"id":57,"type":"Episode","title":"Operacionalizar GenAI","tags":["ai","generativeai","collectiongenerativeai","infrastructuremanagement","aisystems","aimodels","operationalization","datainput","modeltraining","finetuning","digitaltransformation","opensourcemodels","privateclouds","edgecomputing","aitools","creativeoutput","responsibleusage","reinforcementlearning","monitoring","optimization","sandboxenvironment","cloudbasedinfrastructure","onpremisesinfrastructure","hybridinfrastructure","customerservice","brainstormingapplications","embracingdigital"],"body":"\r\n\r\n## Explorando diferentes modelos de compartición de IA generativa.\n\nEl podcast destaca la variedad de modelos de intercambio para la IA generativa. En un extremo del espectro, están los modelos abiertos donde cualquier persona puede interactuar y contribuir en el entrenamiento del modelo. Estos modelos emplean el aprendizaje por refuerzo, lo que permite a los usuarios ingresar datos y recibir respuestas relevantes. Por otro lado, algunos modelos privados están más restringidos y tienen un acceso limitado. Estos modelos son adecuados para escenarios corporativos donde control y restricción son cruciales.\n\nSin embargo, existe un enfoque mixto que combina los fundamentos lingüísticos de los modelos abiertos con restricciones y personalización adicionales. Este enfoque permite a las organizaciones beneficiarse de modelos pre-entrenados al mismo tiempo que agregan su propio nivel de control y adaptación. Al ajustar los pesos y las palabras utilizadas en el modelo, las organizaciones pueden personalizar las respuestas para satisfacer sus necesidades específicas sin comenzar desde cero.\n\n## Operacionalizar Gen AI en la gestión de infraestructuras.\n\nEl podcast profundiza en la operacionalización de la IA generativa en la gestión de infraestructuras. Destaca las ventajas de utilizar modelos de código abierto para desarrollar sistemas especializados que gestionen de manera eficiente nubes privadas. Por ejemplo, uno de los socios mencionados implementó IA generativa para monitorear y optimizar el rendimiento de su infraestructura en tiempo real, lo que permite la resolución proactiva de problemas. Al aprovechar el poder de la IA, las organizaciones pueden mejorar su eficiencia operativa y garantizar el buen funcionamiento de su infraestructura.\n\nLos anfitriones enfatizan la importancia de considerar el tipo y la calidad de los datos ingresados en el modelo y la salida deseada. No siempre es necesario entrenar un modelo con miles de millones de indicadores; un conjunto de datos más pequeño adaptado a necesidades específicas puede ser más efectivo. Al comprender los matices de los datos y los objetivos particulares del sistema, las organizaciones pueden optimizar el proceso de entrenamiento y mejorar el rendimiento general del modelo de IA.\n\n## Gestionando y ajustando los sistemas de IA\n\nGestionar sistemas de inteligencia artificial requiere una toma de decisiones reflexiva y un monitoreo constante. Los anfitriones discuten la importancia de seleccionar la infraestructura adecuada, ya sea basada en la nube, en local o híbrida. Además, la computación en el borde está ganando popularidad, permitiendo que los modelos de IA se ejecuten directamente en dispositivos, reduciendo los tiempos de ida y vuelta de los datos.\n\nEl podcast enfatiza la necesidad de experiencia en la instalación y mantenimiento de sistemas de inteligencia artificial (IA). Se requiere talento especializado para diseñar y ajustar modelos de IA con el fin de lograr los resultados deseados. Dependiendo del caso de uso, pueden ser necesarias funcionalidades específicas, como empatía en el servicio al cliente o creatividad en aplicaciones de lluvia de ideas. Es crucial contar con un equipo competente que comprenda las complejidades de los sistemas de IA y pueda garantizar su funcionamiento óptimo.\n\nAdemás, los modelos de IA requieren un monitoreo y ajuste constantes. Los modelos pueden mostrar un comportamiento indeseable, por lo que es esencial intervenir cuando sea necesario para garantizar resultados adecuados. El podcast diferencia entre problemas de refuerzo, donde los comentarios de los usuarios pueden dirigir el modelo en direcciones potencialmente perjudiciales, y la alucinación, que puede ser aplicada intencionalmente con fines creativos.\n\n## Comenzando con modelos de IA\n\nEl podcast ofrece consejos prácticos para comenzar con modelos de inteligencia artificial. Los anfitriones sugieren jugar con las herramientas disponibles y familiarizarse con sus capacidades. Registrarse en las cuentas y explorar cómo se pueden usar las herramientas es una excelente manera de obtener experiencia práctica. También recomiendan crear un entorno de prueba dentro de las empresas, que permita a los empleados probar e interactuar con los modelos de inteligencia artificial antes de implementarlos en la producción.\n\nEl podcast destaca la importancia de brindar a los modelos de IA suficiente creatividad al tiempo que se mantiene el control y se establecen límites. Las organizaciones pueden encontrar un equilibrio entre la producción creativa y el uso responsable al definir límites y tomar decisiones sobre lo que el modelo debe o no debe aprender de las interacciones.\n\nEn conclusión, el episodio del podcast proporciona conocimientos valiosos sobre la operacionalización de la IA generativa, la gestión de la infraestructura y las consideraciones para administrar y ajustar los sistemas de IA. También ofrece consejos prácticos para comenzar a utilizar modelos de IA en entornos personales y profesionales. Al comprender los diferentes modelos de compartición, las necesidades de infraestructura y la importancia de la creatividad y los límites, las organizaciones pueden aprovechar el poder de la IA para respaldar la transformación digital.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT157-es","image":"./episodes/edt-157/es/thumbnail.png","lang":"es","summary":"En este episodio de podcast, el anfitrión Darren Pulsipher, Arquitecto Principal de Soluciones del Sector Público en Intel, discute la puesta en marcha de la IA generativa con el invitado recurrente Dr. Jeffrey Lancaster. Exploran los diferentes modelos de compartición de la IA generativa, incluyendo los modelos públicos, privados y comunitarios. El podcast aborda temas como los modelos de código abierto, la gestión de infraestructura y consideraciones para desplegar y mantener sistemas de IA. También profundiza en la importancia de la creatividad, la personalización y cómo empezar con modelos de IA."},{"id":58,"type":"Episode","title":"GenAI en la educación superior","tags":["collectiongenerativeai","addressingbiasesingenerativeai","preservingauthenticityandindividuality","balancingaiintegrationineducation","lauratorresnewey","criticalthinkingskills","educationaltechnology","highereducation","aiineducation","aibias","diversityandinclusion","authenticvoices","uniqueperspectives","genai","generativeai","embracingdigital","edt158"],"body":"\r\n\r\nEl surgimiento de la IA generativa en la educación\n\nEn un episodio reciente del podcast \"Abrazando la Transformación Digital\", el presentador Darren Pulsipher entrevista a Laura Torres Newey, autora de éxito del New York Times y profesora de inglés, acerca del impacto de la inteligencia artificial generativa en la educación superior. La discusión gira en torno a la integración de la IA en el aula, sus efectos en los métodos de enseñanza, preocupaciones sobre el sesgo y la preservación de voces únicas. Laura comparte sus perspectivas y experiencias como educadora, ofreciendo valiosas reflexiones sobre cómo navegar por el cambiante panorama de la educación en la era de la inteligencia artificial.\n\n## La influencia de la inteligencia artificial generativa en la educación\n\nLa inteligencia artificial generativa ha comenzado a convertirse en una presencia destacada en la educación, desde la calificación automatizada de ensayos hasta brindar asistencia de escritura a los estudiantes. Si bien esta tecnología ofrece conveniencia y eficiencia, plantea preocupaciones sobre la posible pérdida de voces únicas. Laura enfatiza la importancia de valorar y fomentar las perspectivas individuales y la creatividad de los estudiantes en su escritura. En lugar de prohibir completamente el uso de la inteligencia artificial generativa, Laura cree en enseñar a los estudiantes cómo usar estas herramientas de manera efectiva y aprovechar su potencial sin comprometer sus propias voces.\n\nLa integración de la IA generativa provoca un cambio en el enfoque de la enseñanza. En lugar de evaluar únicamente el producto final, los educadores deben hacer hincapié en el proceso de aprendizaje. Con herramientas impulsadas por IA como Grammarly disponibles para los estudiantes, los profesores pueden dirigir su atención hacia el desarrollo de habilidades de pensamiento crítico, capacidad de investigación y el discernimiento necesario para identificar fuentes confiables. Al incorporar tareas que implican comparar el contenido generado por IA con trabajos escritos tradicionalmente, los estudiantes pueden analizar las fortalezas y debilidades de ambos enfoques, fomentando así un mayor entendimiento de su escritura y mejorando sus habilidades de pensamiento crítico.\n\n## El Rol de los Educadores en la Era de la IA\n\nLos educadores tienen un papel esencial en preparar a los estudiantes para el siempre cambiante panorama tecnológico. Laura enfatiza que adaptarse y utilizar de manera efectiva la inteligencia artificial generativa es crucial para los educadores en todos los niveles de educación. Con la IA cada vez más presente en el lugar de trabajo, los estudiantes que puedan navegar y aprovechar esta tecnología estarán mejor preparados para futuras oportunidades laborales. Para asegurarse de que los estudiantes estén bien preparados, los educadores no solo deben familiarizarse con las aplicaciones de IA, sino también enseñar a los estudiantes cómo utilizarla de manera efectiva y ética.\n\nEl cambio hacia la integración de la IA generativa en la educación se alinea con la postura del Departamento de Educación de EE. UU. sobre la IA. Reconocen los posibles beneficios, pero enfatizan la necesidad de que los usuarios sigan teniendo el control, comparando el papel de la IA con el de una bicicleta eléctrica, donde la tecnología alivia la carga pero el usuario mantiene en última instancia el control. Este enfoque enfatiza la importancia de encontrar un equilibrio entre aprovechar los beneficios de la IA y preservar las voces y perspectivas únicas de los estudiantes.\n\n## Conclusión\n\nLa integración de la inteligencia artificial generativa en la educación presenta tanto oportunidades como desafíos. Si bien la IA puede mejorar el aprendizaje y ayudar a los estudiantes con sus tareas, es crucial que los educadores prioricen el pensamiento crítico y aborden las preocupaciones sobre sesgos para desarrollar pensadores completos e independientes. Los maestros deben adoptar la tecnología de IA, comprendiendo sus aplicaciones y enseñando a los estudiantes cómo navegar y utilizarla de manera efectiva. Al encontrar un equilibrio entre la eficiencia de los contenidos generados por IA y la preservación de voces auténticas y diversas, los educadores pueden preparar a los estudiantes para el futuro digital al tiempo que aseguran el cultivo de su individualidad y creatividad.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Laura Newey","Darren W Pulsipher"],"link":"/episode-EDT158-es","image":"./episodes/edt-158/es/thumbnail.jpg","lang":"es","summary":"En este episodio de podcast, Darren Pulsipher, arquitecto principal de soluciones del sector público en Intel, entrevista a Laura Torres Newey, autora de best-sellers del New York Times y profesora universitaria, sobre el impacto de la inteligencia artificial generativa en la educación superior. Este episodio ahonda en los desafíos y oportunidades presentados por la integración de la inteligencia artificial generativa en el aula, destacando la necesidad de habilidades de pensamiento crítico, las preocupaciones sobre sesgos y asegurando la preservación de voces únicas."},{"id":59,"type":"Episode","title":"Políticas de GenAI","tags":["collectiongenerativeai","policies","ai","generativeai","guidelines","jeremyharris","darrenpulsipher","roadmap","challenges","efficiencies","dataprotection","privacy","compliance","ethicalconsiderations","feedback","engagement","ratings","reviews","customersatisfaction","customerengagement","embracingdigital","edt159"],"body":"\r\n\r\n## La necesidad de políticas y directrices claras\n\nJeremy y Darren hacen hincapié en la importancia de tener una política clara y una hoja de ruta bien definida para la transformación digital. Apresurarse en la digitalización sin una planificación adecuada puede llevar a desafíos e ineficiencias. Al establecer políticas y pautas, las organizaciones pueden esbozar sus objetivos, establecer una dirección estratégica y asegurarse de que todos estén en la misma página.\n\nEnfatizan que la transformación digital es más que simplemente adoptar nuevas tecnologías; requiere un cambio en la cultura organizativa y en la mentalidad. Las políticas pueden ayudar a facilitar este cambio al establecer expectativas para los empleados, definir las mejores prácticas digitales y brindar un marco de referencia para la toma de decisiones en el ámbito digital.\n\n## Navegando por las complejidades de la digitalización\n\nLa transformación digital pone de manifiesto un conjunto complejo de desafíos, como la seguridad de los datos, la privacidad y el cumplimiento de normativas. Las organizaciones deben abordar estos desafíos incorporándolos en sus políticas y directrices. Esto incluye implementar medidas de protección de datos, realizar auditorías de seguridad periódicas y garantizar el cumplimiento de las regulaciones pertinentes.\n\nLas políticas también deben abordar las consideraciones éticas que acompañan a la transformación digital. Los anfitriones enfatizan la importancia de que las organizaciones sean administradoras responsables de los datos y aseguren que el uso de tecnologías digitales se alinee con los estándares éticos. Pautas claras pueden ayudar a los empleados a comprender sus responsabilidades y promover prácticas digitales responsables en toda la organización.\n\n## El papel de la retroalimentación y la participación\n\nLos anfitriones destacan la importancia de la retroalimentación y el compromiso en el mundo digital. Adoptar una política que fomente y valore la retroalimentación puede ayudar a las organizaciones a mejorar continuamente y adaptarse a circunstancias cambiantes. Al dar la bienvenida a sugerencias y aportes de empleados y clientes, las organizaciones pueden refinar sus estrategias digitales y garantizar que estén satisfaciendo las necesidades de todas las partes interesadas.\n\nTambién mencionan la importancia de las calificaciones y reseñas en la era digital. El feedback a través de las calificaciones y reseñas no solo proporciona información valiosa a las organizaciones, sino que también se convierte en una medida de satisfacción y compromiso del cliente. Las políticas pueden describir cómo las organizaciones recolectan y responden al feedback, y establecer pautas para captar el sentimiento del cliente en el espacio digital.\n\n## Conclusión\n\nLa transformación digital es un viaje que requiere una planificación cuidadosa, políticas claras y ajustes continuos. Al establecer políticas y pautas, las organizaciones pueden navegar por las complejidades de la digitalización, abordar desafíos y garantizar un uso responsable y efectivo de las tecnologías digitales. Abrazar la transformación digital no se trata solo de adoptar nuevas herramientas, sino también de crear una cultura digital que fomente la innovación y satisfaga las necesidades en constante evolución de los clientes y stakeholders.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeremy Harris","Darren W Pulsipher"],"link":"/episode-EDT159-es","image":"./episodes/edt-159/es/thumbnail.png","lang":"es","summary":"En este episodio, el presentador Darren entrevista a Jeremy Harris y se adentra en la importancia de establecer políticas y pautas para una exitosa transformación digital. Con la creciente prevalencia de las tecnologías digitales en diversas industrias, las organizaciones necesitan adaptarse y abrazar esta transformación para mantenerse competitivas y cumplir con las expectativas cambiantes de los clientes."},{"id":60,"type":"Episode","title":"Reducción de la congestión de ingestión con Intel Optane DCPMM","tags":null,"body":"\r\n\r\n## Detalles del Stack de Servicios\n\nUn cliente en la industria automotriz estaba teniendo dificultades para obtener información de manera efectiva de sus automóviles y llevarla a su centro de datos para poder realizar aprendizaje automático y análisis. Ha habido investigaciones en esta área, pero solo para un pequeño número de automóviles, no para los cien millones de automóviles del cliente. Cuando examiné su pila de servicios completa y cómo todo estaba llegando al centro de datos, la ingesta de datos se convirtió en el problema claro: ¿Cómo puedo ingerir tanta data y hacerlo rápidamente?\n\n## Descripción general de la arquitectura de alto nivel de Kafka\n\nEl cliente quería utilizar Kafka para su ingestión. Kafka es un broker que puede escalar bien, y la clave está en que puede manejar varios productores, diferentes consumidores y una gran cantidad de datos. Utilizar varios brokers de Kafka para colocar y recibir datos en los lugares más apropiados ofrece una gran flexibilidad.\n\nKafka, sin embargo, fue principalmente diseñado para tamaños de mensaje de alrededor de uno a 10 kilobytes y los datos del cliente eran aproximadamente 240 kilobytes por automóvil. Hay soluciones alternativas, pero quería llevar el mensaje completo de 240 kilobytes al bus de Kafka para poder moverlo como necesitara.\n\n## Prácticas recomendadas para obtener el mejor rendimiento.\n\nMiré las mejores prácticas de rendimiento de otros que trabajan con Kafka para ver si podía adaptarlo a mi cliente. Aumentar el tamaño del búfer para acomodar todo el mensaje es una solución de ajuste fino, junto con gestionar el tamaño del lote para un rendimiento óptimo. Otra práctica exitosa es distribuir los registros. La flexibilidad de Kafka me permitiría colocar los datos en diferentes temas. Puedo dividir los temas en varias particiones que puedo distribuir en múltiples unidades. Entonces, la pregunta es, ¿en cuántas unidades estoy distribuyendo los registros de Kafka? Además, quiero las unidades más rápidas posibles.\n\nUn ejemplo que examiné fue LinkedIn. Sus números publicados de hace un año indican que pueden manejar 13 millones de mensajes por segundo, o 2.7 gigabytes por segundo. Afirman tener alrededor de 1,100 brokers de Kafka y más de 60 en un clúster, por lo que esa es una configuración bastante grande.\n\n## Espacio Automotriz\n\nSi observo los números brutos del cliente (1.6 millones de mensajes por segundo y 800 gigabytes por segundo) y los comparo con LinkedIn, que probablemente no está optimizado para 240 kilobytes, obtengo 44,000 intermediarios. Si lo optimizara, probablemente podría reducirlo a 4,400 intermediarios, que todavía son 240 grupos. Eso es una cantidad enorme de máquinas, así que tuve que encontrar una forma de hacer las cosas más rápidas. Con más optimización, probablemente podría reducirlo a 400 a 500 intermediarios, pero quería ver qué más podía hacer.\n\n## Memoria Persistente Intel Optane DC\n\nMiré nuestra Memoria Persistente Optane. Encaja en un formato DDR4, por lo que se ubica directamente en el bus DDR4. Llega hasta módulos de 512 gigabytes, por lo que en un servidor de dos sockets puedo tener seis terabytes de memoria persistente. Quería encontrar una manera de aprovechar esta tecnología altamente confiable con características excelentes como encriptación de hardware incorporada para ayudarme a resolver este problema.\n\n## Apoyo para una amplia variedad de aplicaciones\n\nHay dos modos de operación con esta Memoria Optane: el modo de aplicación directa y el modo de memoria. El modo de memoria es simple. Utiliza la memoria persistente como RAM normal porque es más barata que la DDR4 normal. No es lo mismo que la DDR4, pero es lo suficientemente similar como para que en la mayoría de las aplicaciones no se note ninguna diferencia. En el modo de aplicación directa, realmente puedes escribir desde tu programa directamente en la memoria persistente. De esta manera, no tengo que convertir y desconvertir estructuras de datos ni transmitirlas; simplemente puedo guardarlas en la memoria persistente. También puedo montar el modo de aplicación directa como un sistema de archivos para que esté en el bus de memoria, lo cual es mucho más rápido que en el bus de E/S. Ahora, ¿qué puedo hacer con esta memoria?\n\n## Usando el núcleo de Linux\n\nHay dos herramientas principales disponibles utilizando el kernel de Linux: ndctl e ipmctl. Ndctl es un controlador de dispositivos de memoria no volátil, y luego está IPM, el controlador de memoria persistente de Intel, que me permite manipular y controlar esta memoria persistente. Puedo configurarlo en modo de memoria o modo de aplicación directa. Tuve un poco que aprender sobre estas herramientas y cómo funcionan.\n\n## Enfoque de Ingestión\n\nMi primer pensamiento fue que si le daba a Kafka mucha más memoria con tamaños de búfer grandes, debería funcionar mucho más rápido. Los cambios de código en la configuración serían innecesarios o mínimos. Otra opción era cambiar Kafka para que escribiera en memoria persistente en lugar de escribir en un sistema de archivos, evitando el disco duro. Lo último en lo que me fijé fue en crear un sistema de archivos persistente utilizando memoria persistente y luego poner los registros de Kafka en este nuevo sistema de archivos.\n\nLa opción más fácil de las tres fue la primera: más memoria. Ejecuté todas mis tareas con más memoria y no hubo ningún cambio en el rendimiento. La razón es porque eventualmente mis buffers se llenaron y tuve que enviarlos a un disco. Al final, todo tenía que ir a los registros de Kafka, que era mi cuello de botella.\n\nLa segunda opción implica reescribir código y esperar a recibir aprobaciones, así que pasé a la tercera opción. Los resultados de este experimento donde apunté los registros a este nuevo sistema de archivos ultrarrápido fueron fascinantes. Echemos un vistazo al proceso y los resultados.\n\n## Restricciones de prueba\n\nPara eliminar obstáculos para probar el rendimiento, eliminé la red de la ecuación al ejecutar mi prueba en la misma máquina en la que se encontraba mi agente de bolsa. Además, solo ejecuté productores, luego solo consumidores y luego una combinación de ambos, para poder evaluar las diferencias. Mi objetivo no era analizar la mejora total de la producción, sino examinar a nivel individual si este impulso realmente marcaría la diferencia en un agente de bolsa.\n\n## Primer Enfoque 50/50\n\nLo primero que hice fue tomar la mitad de mi memoria persistente y ponerla en modo de acceso directo a aplicaciones, convirtiéndola en un sistema de archivos. Dejé la otra mitad como memoria. Utilicé los comandos ndctl e ipmctl y creé espacios de nombres. Monté estos sistemas de archivos y ejecuté mi prueba.\n\n## Cambiando el tamaño del mensaje\n\nEjecuté las pruebas con varios tamaños de mensaje diferentes. Esperaba cierta optimización, principalmente para 1 kilobyte. Vi que obtenía un rendimiento cada vez más óptimo hasta aproximadamente 10 productores. Después de los 10 productores, empecé a saturar el bus y obtener cierta variabilidad. Eso me dice que estaba almacenando en caché las cosas. Ahora puedo comparar estos números con los que obtuve anteriormente solo con una unidad SATA para los registros de Kafka. También probé nuestras unidades Optane NVMe para los registros.\n\n## Comparación tecnológica\n\nEchemos un vistazo a las diferencias. Para 240 kilobytes, con un disco SATA normal, es bastante plano. Obtuve cierta mejora y luego disminuyó a medida que aumentaba el número de productores. Con el disco Optane NVMe, obtuve un pico muy agradable, casi dos veces más rápido que un disco SATA, lo cual es lo que esperaba porque es un bus NVMe en lugar de un bus SATA. El Pmem es casi cinco veces más rápido que un disco SATA y dos veces y media más rápido que el disco Optane NVMe. Eso se debe a que estoy usando un bus de memoria en lugar del bus SATA o NVMe.\n\n## Optimización adicional (100% directo a la aplicación)\n\nEsto estaba corriendo rápido y estaba llenando rápidamente este disco temporal de 750 GB. Como necesitaba ejecutar la prueba un poco más tiempo, volví y reconfiguré mi máquina para que funcione al 100 por ciento en modo de aplicación directa, de manera que ahora podía tomar los 1.5 terabytes completos.\n\n## Optimizado PMEM y 100% App Direct\n\nDespués de hacer esto y ejecutar las mismas pruebas, obtuve un resultado sorprendente. Pude agregar más productores y mi rendimiento aumentó casi dos o tres veces más. Ahora es entre 12 y 15 veces más rápido que una unidad SATA con 25-30 productores y un tamaño de mensaje de 240 kilobytes. Esto es increíble y reduciría en gran medida la necesidad de tantos brokers, filas y filas de máquinas por parte de mis clientes. Realicé la prueba varias veces porque no creía los resultados que obtenía. Llamé a uno de nuestros arquitectos que diseñaron esta tecnología y aprendí que una de las razones de la velocidad aumentada era que, cuando estaba utilizando parte de la memoria persistente como memoria, los datos tenían que pasar por dos o tres saltos que no son necesarios con el modo de acceso directo de la aplicación. Esto crea menos conflictos en el bus de memoria, y el rendimiento aumentó drásticamente.\n\n## Llamado a la Acción\n\nEl resultado final es que pude utilizar Kafka con Optane DC Persistent Memory como un sistema de archivos ultrarrápido para obtener importantes mejoras en el rendimiento tanto en los productores como en los consumidores. Un solo intermediario puede manejar 15 veces más mensajes y rendimiento que antes, disminuyendo el número de servidores necesarios para manejar arquitecturas de sistemas grandes y complejas. Es hora de evaluar tu arquitectura actual y ver si esto beneficiaría a tu organización.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT16-es","image":"./episodes/edt-16/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren habla sobre cómo disminuir la congestión de ingestión utilizando la Memoria Persistente Optane DC de Intel, y el experimento que realizó con resultados sorprendentes. Podría cambiar la forma en que pensamos sobre la programación en el futuro."},{"id":61,"type":"Episode","title":"Seguridad en la IA Generativa","tags":["collectiongenerativeai","personalizedphishingattacks","promptinjection","sharingcodeai","harnessingai","digitaltransformation","generativeai","cybersecurityrisks","serviceproviders","duediligence","riskschallenges","digitallandscape","proactivecybersecurity","llm","multifactorauthentication","voicerecognition","typingcadence","github","stackoverflow","samsungipleak","securityaspects","embracingdigital","edt160"],"body":"\r\n\r\n## Ataques de suplantación de identidad personalizados y convincentes.\n\nUna de las principales preocupaciones discutidas es el potencial para ataques de phishing más sofisticados y personalizados. Actualmente, el phishing se destaca como el método de ataque cibernético más efectivo y, con la inteligencia artificial generativa, los atacantes pueden crear correos electrónicos o mensajes de phishing altamente personalizados y convincentes. Al obtener información de las redes sociales u otras plataformas en línea, los atacantes pueden hacer que sus intentos de phishing sean más difíciles de detectar. Esto plantea la pregunta de cómo podemos determinar qué es real o no y cómo podemos confiar en la autenticidad de la información que recibimos.\n\nPara combatir esto, es posible que las personas necesiten desarrollar nuevos métodos para verificar la información, como utilizar palabras clave personales u otras medidas de autenticación con seres queridos. Además, las organizaciones y agencias de seguridad deben adaptar sus estrategias para contrarrestar la creciente sofisticación de los ataques cibernéticos facilitados por la inteligencia artificial generativa. Es crucial entender que la inteligencia artificial generativa en sí misma es una tecnología neutral y sus implicaciones dependen de cómo se utilice.\n\n## Voces clonadas y confiar en la información.\n\nEl podcast también explora el potencial del IA generativa para clonar voces, lo que ya se ha observado en ataques de secuestro virtual. Los criminales utilizan voces clonadas para crear un sentido de urgencia y miedo, pretendiendo ser seres queridos de la víctima. Esto plantea preocupaciones sobre confiar en la autenticidad de la información que recibimos.\n\nEn un escenario así, se vuelve esencial desarrollar técnicas para verificar la autenticidad de las voces y la información. Como individuos, debemos permanecer vigilantes y tener precaución al responder a solicitudes urgentes por teléfono. Asegurarse de mantener líneas de comunicación abiertas con contactos de confianza puede ayudar a verificar si dichas solicitudes son auténticas.\n\n## Protegiendo la propiedad intelectual en la codificación y programación\n\nEl podcast se adentra en una discusión sobre la importancia de proteger la propiedad intelectual en la codificación y programación. Los anfitriones destacan los riesgos de compartir involuntariamente código en plataformas como StackOverflow y GitHub, así como la filtración inadvertida de propiedad intelectual al buscar ayuda en estos foros públicos. Se alienta a los desarrolladores a reemplazar la información confidencial con marcadores antes de compartir código para mitigar el riesgo de pérdida de propiedad intelectual.\n\nAdemás, los anfitriones discuten la introducción de herramientas como GitHub Copilot, que utiliza IA generativa para proporcionar sugerencias de código. Si bien estas herramientas pueden ser valiosas, plantean preocupaciones sobre la seguridad y privacidad de la información propietaria. Los desarrolladores deben considerar cuidadosamente la confiabilidad del proveedor de servicios y garantizar una protección adecuada de sus datos y propiedad intelectual.\n\n## Equilibrar la innovación y la seguridad en la era de la IA.\n\nLa conversación concluye enfatizando la importancia de lograr un equilibrio entre abrazar los avances y los posibles cambios positivos que aporta la IA generativa y abordar los riesgos asociados en el ámbito de la ciberseguridad y la protección de la propiedad intelectual. Es esencial mantenerse informado, adaptar las estrategias de seguridad y tener cautela para navegar exitosamente por el paisaje en constante evolución de la transformación digital. Al hacerlo, podemos aprovechar los beneficios de la IA sin comprometer la seguridad y la información personal.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Jeffrey Lancaster","Darren W Pulsipher"],"link":"/episode-EDT160-es","image":"./episodes/edt-160/es/thumbnail.png","lang":"es","summary":"En este episodio, el anfitrión Darren Pulsipher se une al Dr. Jeffrey Lancaster para adentrarse en la intersección entre la inteligencia artificial generativa y la seguridad. La conversación profundiza en los riesgos y desafíos potenciales que rodean el uso de la inteligencia artificial generativa en actividades maliciosas, especialmente en el ámbito de la ciberseguridad."},{"id":62,"type":"Episode","title":"Análisis de Datos de Lenguaje Natural","tags":["collectiongenerativeai","ai","generativeai","embracingdigital","edt161","challengesinnaturallanguageprocessing","fantasysportsapp","naturallanguagegeneration","highqualitycontent","technicalbackground","outoftheboxthinking","pushingboundaries","diversity","crossdomaincollaboration","innovativeideas","infoscentience'ssolution","dataanalytics","naturallanguageaisystem","conceptualautomata","datasets","revolutionizing","businessesanalyzeinformation","futureofdataanalysis","naturallanguagereporting","flexibility","tailored","differentindustries","customized","specificcontext","jargon","dataanalysis","revolutionizingindustries","sportsanalytics","stevewasick'sjourney","innovativeapproach","entrepreneurs","techfounders","unconventionalpaths","successfulinnovations","embracingvariability","context","poweroflanguage"],"body":"\r\n\r\n## Desafíos en el Procesamiento del Lenguaje Natural\n\nSteve recuerda su proyecto temprano: una aplicación para deportes de fantasía que tenía como objetivo proporcionar a los usuarios no solo estadísticas, sino también el contexto e historias detrás de los números. Esto lo llevó al campo de generación de lenguaje natural, donde enfrentó desafíos para adquirir y ofrecer contenido de alta calidad. A pesar de no tener formación técnica, las diversas experiencias de Steve le permitieron abordar estos desafíos con creatividad y pensamiento innovador.\n\n## Empujando los límites\n\nDarren elogia a Steve por empujar los límites y aportar una perspectiva fresca al campo. Esto resalta la importancia de la diversidad y la colaboración entre distintos campos para generar ideas innovadoras y soluciones. El viaje de Steve sirve como una inspiración para los empresarios y fundadores de tecnología en ciernes, demostrando que caminos no convencionales pueden conducir a innovaciones exitosas.\n\n## Solución de InfoScentience para el análisis de datos.\n\nLa conversación también explora las capacidades del sistema de inteligencia artificial en lenguaje natural de InfoSentience. Steve explica que su tecnología desglosa los eventos y las historias en sus partes constituyentes, ofreciendo una mejor comprensión de los conceptos complejos y sus relaciones. Este motor analítico, basado en autómatas conceptuales, permite la síntesis de conjuntos de datos diversos y complejos, revolucionando la forma en que las empresas analizan la información.\n\n## El Futuro del Análisis de Datos e Informes en Lenguaje Natural.\n\nAdemás, Steve destaca la flexibilidad de su sistema de IA, que puede adaptarse a diferentes industrias y personalizarse para satisfacer las necesidades únicas de cada cliente. Al comprender el contexto específico y la jerga de los datos que se analizan, Info Sentience asegura que su sistema de IA brinde información precisa y relevante.\n\nEn conclusión, el episodio del podcast resalta el potencial del análisis de datos del lenguaje natural para revolucionar industrias como la analítica deportiva. El viaje y enfoque innovador de Steve Wasick sirven de inspiración para emprendedores y fundadores tecnológicos, recordándonos que los caminos no convencionales pueden llevar a innovaciones exitosas. El futuro del análisis de datos radica en abrazar la variabilidad, el contexto y el poder del lenguaje.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Wasick"],"link":"/episode-EDT161-es","image":"./episodes/edt-161/es/thumbnail.jpg","lang":"es","summary":"En el último episodio, Darren Pulsipher se sienta con Steve Wasick, el CEO y fundador de InfoSentience, para discutir el poder y el potencial del análisis de datos de lenguaje natural. Steve, quien proviene de un trasfondo no convencional como licenciado en inglés convertido en guionista convertido en abogado convertido en fundador de tecnología, aporta una perspectiva única al campo."},{"id":63,"type":"Episode","title":"Crear una estrategia de nube multi-híbrida","tags":["cloudadoption","collmultihybridcloud","organizationalmodernization","barriers","riskmitigationplan","applicationrearchitecture","governance","organizationalculture","cloud","multihybridcloud","multicloud","embracingdigital","edt162"],"body":"\r\n\r\n## Las mejores prácticas para la adopción de la nube\n\nMudarse a la nube y adoptar nuevas tecnologías como la IA generativa puede traer numerosos beneficios, pero las organizaciones también deben estar preparadas para los cambios que conlleva. Según Christine McMonigal, directora de Tecnologías de Centro de Datos y Nube en Intel, existen mejores prácticas clave a tener en cuenta.\n\n## Modernización organizativa\n\nUn aspecto importante a reconocer es que la adopción de la nube no es solo una modernización tecnológica, sino también una modernización organizacional. Esto significa que las organizaciones deben estar preparadas para cambios en los procesos, flujos de trabajo e incluso en las estructuras organizativas. Es crucial abordar estos cambios y asegurarse de que toda la organización esté alineada y preparada para la transformación.\n\n## Identificando barreras y estableciendo expectativas claras\n\nUn paso crucial para superar barreras y mitigar riesgos es identificar cuáles son estas barreras en primer lugar. Al realizar una evaluación exhaustiva de la infraestructura actual, los flujos de trabajo y los desafíos dentro de la organización, se pueden encontrar los obstáculos potenciales y desarrollar estrategias para superarlos.\n\nAdemás, establecer expectativas claras desde el principio es esencial. Esto implica una comunicación efectiva con los involucrados, empleados y socios acerca de los objetivos, beneficios y desafíos de adoptar estrategias de nube multi-híbrida. Al fijar expectativas realistas y asegurarse de que todos estén en la misma página, las organizaciones pueden minimizar las sorpresas y la resistencia al cambio.\n\n## Plan de mitigación de riesgos sólido\n\nTener un sólido plan de mitigación de riesgos en su lugar es otro aspecto crucial para una exitosa adopción de la nube. Esto incluye evaluar posibles riesgos de seguridad, preocupaciones de privacidad de datos y requisitos de cumplimiento. Al abordar proactivamente estos riesgos e implementar las medidas adecuadas, las organizaciones pueden proteger sus datos, garantizar el cumplimiento normativo y minimizar posibles amenazas.\n\n## Barrera 1: Re-arquitectura de la aplicación\n\nUna de las principales barreras que las organizaciones a menudo enfrentan en la adopción de la nube es la re-arquitectura de aplicaciones. Es importante evaluar qué aplicaciones se pueden trasladar tal cual a la nube, y cuáles pueden requerir modificaciones más significativas. Al identificar oportunidades de simplificación y reducción de costos a través de la automatización, las organizaciones pueden optimizar el acceso y los controles.\n\n## Barrera 2: Gobierno\n\nLas políticas de gobierno desempeñan un papel crucial en la mitigación de riesgos durante la adopción de la nube. Los modelos de seguridad inconsistentes, las herramientas de gestión diversas y las políticas de usuario heterogéneas pueden aumentar la complejidad y poner en peligro el éxito de la migración. Simplificar las políticas de gobierno y eliminar la burocracia puede ayudar a las organizaciones a agilizar las operaciones, reducir costos y garantizar la seguridad y el cumplimiento de los datos.\n\n## Barrera 3: Cultura Organizacional y Madurez\n\nPreparar a la organización para el cambio que conlleva la adopción de la nube es vital. Esto implica conseguir que los empleados estén a bordo, proporcionar capacitación en habilidades e identificar a los actores clave que puedan adoptar las nuevas formas de trabajo. Abordar los miedos y preocupaciones que los empleados puedan tener, como el temor a quedarse atrás o perder sus empleos, es esencial para crear un ambiente positivo y colaborativo.\n\nEn conclusión, adoptar estrategias de cloud híbridas requiere una planificación cuidadosa, comunicación efectiva y una comprensión exhaustiva de los objetivos y desafíos de una organización. Al abordar las barreras de antemano y mitigar los riesgos, las organizaciones pueden allanar el camino para un viaje exitoso de transformación digital. Manténganse atentos a los próximos episodios donde exploraremos el desarrollo de una estrategia de cloud, la evaluación de carteras de aplicaciones y más información sobre la adopción de la transformación digital. No olviden calificar y suscribirse a nuestro podcast para estar actualizados sobre las últimas tendencias y mejores prácticas en el panorama digital.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Christine McMonigal","Darren W Pulsipher"],"link":"/episode-EDT162-es","image":"./episodes/edt-162/es/thumbnail.png","lang":"es","summary":"En este episodio Darren entrevista a Christine McMonigal y discute los desafíos que enfrentan las organizaciones al hacer la transición a la nube y adoptar arquitecturas de nube híbrida y multi-nube. Destacan la importancia de entender estos obstáculos y brindar orientación para superarlos. Este episodio profundizará en algunas barreras clave y estrategias para mitigar riesgos, asegurando una transformación exitosa a la nube."},{"id":64,"type":"Episode","title":"Desarrollando un Modelo Operativo de Nube Multi-Híbrida.","tags":["cloudstrategy","digitaltransformation","collmultihybridcloud","cloudtechnologies","businessgoals","operationalefficiency","customersatisfaction","itinfrastructure","migrationplan","datasecurity","regulatorycompliance","hybridclouds","publicclouds","privateclouds","clouddeploymentmodels","governanceandsecurity","reducecosts","enhanceefficiency","customerexperience","strategicmove","clearvision","embracingdigital","edt163"],"body":"\r\n\r\nEn la era digital de hoy, las empresas cada vez recurren más a la nube como un movimiento estratégico para mejorar la eficiencia, reducir costos y mejorar la experiencia del cliente. Sin embargo, antes de subirse al carro de la nube, es esencial que las organizaciones den un paso atrás y evalúen sus necesidades específicas. Desarrollar una estrategia en la nube es un paso crucial en este proceso, ya que permite a las empresas alinear sus metas y objetivos con las tecnologías en la nube disponibles para ellas.\n\n## Comprender tus metas y objetivos empresariales.\n\nEl primer paso para desarrollar una estrategia de nube es obtener una comprensión clara de tus metas y objetivos comerciales. ¿Qué estás tratando de lograr? ¿Estás buscando mejorar la eficiencia operativa, reducir costos o aumentar la satisfacción del cliente? Al tener una visión clara de tus metas, puedes determinar mejor cómo la nube puede respaldar y permitir estos objetivos.\n\n## Evaluando su infraestructura actual\n\nDespués de establecer tus objetivos, es importante evaluar tu infraestructura actual de TI. Esta evaluación ayuda a identificar posibles desafíos o limitaciones para migrar a la nube. Determina qué sistemas y aplicaciones tienes actualmente y considera su compatibilidad con un entorno de nube. Esta evaluación informará las decisiones sobre qué aplicaciones y servicios son adecuados para la migración.\n\n## Elegir el modelo de nube adecuado\n\nCon diferentes modelos de implementación en la nube disponibles, las organizaciones necesitan evaluar las diferentes opciones que se adapten a sus requisitos comerciales. Las nubes públicas, las nubes privadas y las nubes híbridas ofrecen ventajas y desventajas distintas. Evaluar los pros y los contras de cada modelo te ayudará a determinar la opción más adecuada para tu organización. Considera factores como la seguridad de los datos, la escalabilidad y el cumplimiento normativo al tomar esta decisión.\n\n## Crear un plan de migración y asegurar la gobernabilidad y seguridad.\n\nUna vez que hayas elegido un modelo de nube, es hora de crear un plan de migración. Esto implica describir los pasos y el cronograma para mover tus aplicaciones y datos a la nube. Prioriza las aplicaciones críticas que necesiten migrarse primero y desarrolla una estrategia para migrar las demás aplicaciones más adelante. Además, implementa un plan de gobierno y seguridad para proteger tus datos y cumplir con los requisitos regulatorios. La seguridad en la nube es una preocupación importante para muchas empresas, por lo que es vital asegurarte de que tus datos estén protegidos durante todo el proceso de migración.\n\nEn conclusión, desarrollar una estrategia en la nube es un proceso complejo que requiere una planificación y evaluación cuidadosas. Es esencial entender los objetivos de tu negocio, evaluar tu infraestructura existente, elegir el modelo de nube adecuado, crear un plan de migración e implementar medidas de gobierno y seguridad adecuadas. Al abrazar eficazmente la transformación digital y aprovechar el poder de la nube, las organizaciones pueden lograr sus objetivos, mejorar la eficiencia y impulsar el crecimiento y el éxito.\n\nEn la era digital actual, las empresas cada vez más recurren a la nube como un movimiento estratégico para mejorar la eficiencia, reducir costos y mejorar la experiencia del cliente. Sin embargo, antes de sumarse a la tendencia de la nube, es esencial que las organizaciones den un paso atrás y evalúen sus necesidades específicas. Desarrollar una estrategia en la nube es un paso crucial en este proceso, ya que permite a las empresas alinear sus metas y objetivos con las tecnologías en la nube disponibles para ellas.\n\n## Comprendiendo tus metas y objetivos comerciales\n\nEl primer paso para desarrollar una estrategia en la nube es entender claramente tus metas y objetivos empresariales. ¿Qué estás tratando de lograr? ¿Estás buscando mejorar la eficiencia operativa, reducir costos o aumentar la satisfacción del cliente? Al tener una visión clara de tus metas, podrás determinar mejor cómo la nube puede apoyar y permitir estos objetivos.\n\n## Evaluando tu infraestructura existente\n\nDespués de establecer tus objetivos, es importante evaluar tu infraestructura de TI actual. Esta evaluación ayuda a identificar posibles desafíos o limitaciones al migrar a la nube. Determina qué sistemas y aplicaciones tienes actualmente y considera su compatibilidad con un entorno en la nube. Esta evaluación servirá para tomar decisiones sobre qué aplicaciones y servicios son adecuados para la migración.\n\n## Elegir el modelo correcto de nube.\n\nCon varios modelos de implementación en la nube disponibles, las organizaciones deben evaluar las diferentes opciones que se ajusten a sus requisitos comerciales. Las nubes públicas, privadas y híbridas ofrecen ventajas y desventajas distintas. Evaluar los pros y contras de cada modelo te ayudará a determinar la elección más apropiada para tu organización. Considera factores como la seguridad de datos, la escalabilidad y el cumplimiento normativo al tomar esta decisión.\n\n## Creando un Plan de Migración y Garantizando Gobernanza y Seguridad.\n\nUna vez que hayas elegido un modelo de nube, es hora de crear un plan de migración. Esto implica describir los pasos y el cronograma para trasladar tus aplicaciones y datos a la nube. Prioriza las aplicaciones críticas que deben ser migradas primero y desarrolla una estrategia para migrar las demás aplicaciones más adelante. Además, implementa un plan de gobernanza y seguridad para proteger tus datos y cumplir con los requisitos regulatorios. La seguridad en la nube es una preocupación importante para muchas empresas, por lo que es vital asegurarse de que tus datos estén protegidos durante todo el proceso de migración.\n\nEn conclusión, desarrollar una estrategia en la nube es un proceso complejo que requiere una planificación y evaluación cuidadosa. Es esencial entender los objetivos de tu negocio, evaluar tu infraestructura existente, elegir el modelo de nube adecuado, crear un plan de migración y implementar medidas adecuadas de gobierno y seguridad. Al adoptar eficazmente la transformación digital y aprovechar el poder de la nube, las organizaciones pueden alcanzar sus objetivos, mejorar la eficiencia y impulsar el crecimiento y el éxito.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Rajiv Mandal","Darren W Pulsipher"],"link":"/episode-EDT163-es","image":"./episodes/edt-163/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista al arquitecto de soluciones en la nube, Rajiv Mandal, sobre el desarrollo de una estrategia de múltiples nubes híbridas en tu organización de TI moderna."},{"id":65,"type":"Episode","title":"Portafolios de aplicación y carga de trabajo en migración a la nube","tags":["cloudmigration","organizations","collmultihybridcloud","cloudnative","datacenterdependencies","hybridstrategy","technicaldebt","applications","workloads","politicalcapital","customerfacingapplications","importance","applicationrationalization","analysis","dependencies","smoothtransition","surprises","decision","compliancerequirements","regulatedindustries","compliancemonitoring","embracingdigital","edt164"],"body":"\r\n\r\n## Comprendiendo las aplicaciones y cargas de trabajo en la migración a la nube\n\nCuando se trata de migración a la nube, las organizaciones generalmente se dividen en dos grupos. El primer grupo está formado por organizaciones nativas de la nube que han diseñado sus aplicaciones en la nube, eliminando cualquier dependencia de centros de datos. El segundo grupo adopta una estrategia híbrida que depende tanto del centro de datos como de la nube. Sin embargo, incluso estas organizaciones híbridas pueden tener deudas técnicas que deben ser abordadas.\n\nUno de los principales desafíos en la migración a la nube es comprender la complejidad de las aplicaciones y cargas de trabajo. Sarah introduce el concepto de \"capital político\" que una aplicación conlleva. Si bien las aplicaciones orientadas al exterior y enfocadas en el cliente a menudo reciben la mayor atención e inversión, las aplicaciones más pequeñas que pueden no parecer significativas pueden tener un impacto sustancial en la organización si fallan o son descuidadas.\n\n## La importancia de la racionalización de aplicaciones\n\nSarah comparte una experiencia personal que destaca la importancia de considerar el portafolio general de aplicaciones y cargas de trabajo durante la migración a la nube. Ella presenció una interrupción en el negocio causada por la falta de atención a una aplicación de atención al cliente aparentemente pequeña. Esta experiencia subraya la necesidad de que las organizaciones realicen un análisis y racionalización exhaustivos de su portafolio de aplicaciones antes de migrar a la nube.\n\nAl comprender las complejidades y dependencias de las aplicaciones y cargas de trabajo, las organizaciones pueden garantizar una transición fluida a la nube con menos sorpresas o interrupciones. Sarah enfatiza la necesidad de que las organizaciones prioricen la racionalización de aplicaciones para identificar las aplicaciones críticas que pueden requerir inversión y atención adicionales, incluso si no son las más visibles.\n\n## Toque o no toque: Evaluación de cargas de trabajo para la migración a la nube\n\nSi bien migrar cargas de trabajo a la nube puede traer numerosos beneficios, puede que no siempre sea necesario o beneficioso modificar ciertas cargas de trabajo o aplicaciones. Algunas cargas de trabajo pueden haber estado funcionando sin problemas durante años y ser críticas para las operaciones de la organización. En tales casos, puede que no tenga sentido realizar cambios o migrarlas a la nube.\n\nFactores a considerar al tomar la decisión incluyen el nivel de personalización e integración de la carga de trabajo, la presencia de deuda técnica y el próximo retiro de los sistemas heredados. Sin embargo, es esencial volver a evaluar regularmente estas cargas de trabajo para asegurarse de que sigan cumpliendo las necesidades de la organización. Monitorear las tendencias de la industria y los avances tecnológicos puede ayudar a identificar cambios potenciales en el futuro.\n\n## Navegando los requisitos de cumplimiento en la migración a la nube.\n\nLos requisitos de cumplimiento pueden plantear desafíos en la migración a la nube, especialmente para las organizaciones en industrias reguladas. Sin embargo, los proveedores de servicios en la nube han avanzado significativamente en abordar estas preocupaciones. Ofrecen herramientas y servicios que ayudan a automatizar la supervisión y presentación de informes de cumplimiento, lo que hace menos oneroso para las organizaciones cumplir con los requisitos.\n\nPara superar estos desafíos, las organizaciones deben realizar una evaluación exhaustiva de sus requisitos de cumplimiento. Consultar con expertos que puedan brindar orientación sobre los estándares de cumplimiento y diseñar una arquitectura de nube que cumpla con estos requisitos es crucial. Se deben implementar auditorías y monitoreo regulares para garantizar el cumplimiento continuo.\n\n## Conclusion - Conclusión\n\nEn este episodio de podcast, Darren Pulsipher y Sarah Musick arrojan luz sobre aspectos importantes de la migración a la nube, incluyendo la racionalización de los portafolios de aplicaciones, la toma de decisiones respecto a las cargas de trabajo a modificar y abordar los requisitos de cumplimiento. Al comprender estos factores y gestionar activamente la deuda técnica, las organizaciones pueden iniciar un exitoso viaje de migración a la nube, aprovechando la agilidad y flexibilidad que ofrece la nube al tiempo que minimizan riesgos y interrupciones.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Sarah Musick","Darren W Pulsipher"],"link":"/episode-EDT164-es","image":"./episodes/edt-164/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a Sarah Musick, Arquitecta de Soluciones en la Nube en Intel. Juntos, profundizan en el tema de los portafolios de aplicaciones y cargas de trabajo en la migración a la nube. Con la experiencia de Sarah en consultoría y optimización en la nube, ella aporta conocimientos valiosos a la discusión."},{"id":66,"type":"Episode","title":"Factores de colocación en la nube de carga de trabajo","tags":["cloudcomputeoptions","collmultihybridcloud","cloudinstance","cloudspecialistadvice","computeselectiontools","optimizecloud","workloadcloud","vendorlockinprevention","cloudbestpractices","cloudinstanceperformance","costeffectivecomputesolutions","workloadoptimization","resourceefficiency","cloud","multihybridcloud","embracingdigital","edt165"],"body":"\r\n\r\n## Entendiendo las opciones de cómputo.\n\nLos proveedores de servicios en la nube (CSPs) ofrecen una combinación de diferentes familias de cálculo, que van desde generaciones antiguas de hardware de cálculo hasta las últimas y más eficientes instancias. Estas generaciones antiguas suelen utilizarse para funciones de computación rentables, mientras que las generaciones más nuevas ofrecen un rendimiento mejorado a precios similares o más bajos.\n\nPuede ser abrumador navegar a través de las numerosas opciones informáticas disponibles en la nube, especialmente con nuevas instancias que se lanzan regularmente. Ahí es donde entran en juego los especialistas en la nube, como los de Intel. Estos expertos pueden proporcionar información valiosa y ayudar a seleccionar la instancia más adecuada para una carga de trabajo específica.\n\n## Tomar decisiones informadas\n\nPara tomar la mejor decisión, busca el consejo de especialistas en la nube o utiliza herramientas como Densify o Intel Site Optimizer. Estas herramientas aprovechan el aprendizaje automático para analizar las características de una aplicación, el uso de cómputo y las necesidades de red para determinar el tamaño de instancia más adecuado. Al aprovechar estos recursos, las organizaciones pueden asegurarse de aprovechar al máximo sus recursos en la nube, evitando la subutilización o el gasto excesivo.\n\n## Implementando las mejores prácticas\n\nEs importante incorporar recomendaciones de instancias en los scripts de infraestructura como código (IaC), como TerraForm, para automatizar la selección de la instancia más eficiente para una carga de trabajo. Esto garantiza una colocación consistente y eficiente de instancias, eliminando el riesgo de error humano y optimizando el rendimiento.\n\n## Considerando la portabilidad\n\nMientras que Intel actualmente domina el mercado en la nube con instancias basadas en x86, hay cierta competencia por parte de AMD y ARM. Los procesadores basados en ARM, como el Graviton, son populares entre los CSPs pero necesitan mayor portabilidad de carga de trabajo entre proveedores y entre entornos públicos y privados. Portar cargas de trabajo basadas en x86 a ARM requeriría una extensa refactorización y redesarrollo de código.\n\nLas organizaciones deben considerar problemas de compatibilidad al repatriar cargas de trabajo desde la nube de vuelta a la infraestructura local. Es crucial evaluar la portabilidad y flexibilidad de la plataforma informática elegida para garantizar transiciones sin problemas y evitar la dependencia del proveedor.\n\n## Traducción: Conclusión\n\nSeleccionar la instancia de nube adecuada es una decisión crítica que puede afectar el rendimiento, el costo y la portabilidad de su carga de trabajo. Con la ayuda de especialistas y herramientas en la nube, las organizaciones pueden tomar decisiones informadas y optimizar la utilización de sus recursos en la nube. Al comprender las opciones de computación disponibles, incorporar las mejores prácticas y considerar la portabilidad, las empresas pueden aprovechar todo el potencial de la nube, al tiempo que aseguran flexibilidad y eficiencia en sus operaciones.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Ricardo Dutton Jr","Darren W Pulsipher"],"link":"/episode-EDT165-es","image":"./episodes/edt-165/es/thumbnail.png","lang":"es","summary":"En este podcast, Darren y Rico Dutton exploran el mundo de las instancias en la nube y los factores a considerar al seleccionar la instancia adecuada para tu carga de trabajo. Discuten las diferentes opciones de cómputo disponibles en la nube, la importancia de encontrar el equilibrio adecuado entre rendimiento y costo, y el papel de los especialistas en la nube en ayudar a las organizaciones a tomar decisiones informadas."},{"id":67,"type":"Episode","title":"Agilidad en la adopción de la nube","tags":null,"body":"\r\n\r\n## Migración a la nube como un viaje continuo.\n\nSi bien muchas personas ven la migración a la nube como un proceso único, es esencial considerarlo como un viaje continuo en el que los desarrolladores y los equipos de operaciones trabajan juntos. Una vez que las cargas de trabajo son modernizadas e implementadas, es necesario realizar un monitoreo y evaluación constantes para determinar si cumplen con los objetivos comerciales y las métricas de éxito.\n\nAl tratar la migración a la nube como un viaje continuo, las organizaciones pueden permitir que sus equipos iteren, refinan y mejoren su éxito. Este enfoque permitirá agilidad, adaptabilidad y la capacidad de responder a las necesidades comerciales en evolución.\n\n## Repatriando cargas de trabajo y flexibilidad\n\nUn aspecto importante a considerar es la posibilidad de migrar las cargas de trabajo de vuelta a las instalaciones locales si no se están logrando los beneficios esperados de la nube o si existe la necesidad de cambiar entre diferentes proveedores de nube. Para lograr una mejora continua, es necesario evaluar la situación de forma constante, establecer expectativas desde el principio y ser ágiles y flexibles en el modelo operativo de la nube.\n\nUna infraestructura consistente en múltiples nubes es esencial para permitir flexibilidad y agilidad. Si bien los proveedores de servicios en la nube pueden intentar restringir a los clientes a sus servicios, las organizaciones deben resistir esta tentación y apuntar a la consistencia en todas las nubes o estar dispuestas a realizar los cambios necesarios al mover cargas de trabajo a diferentes ubicaciones.\n\n## Herramientas y Mejores Prácticas para la Optimización.\n\nOptimizar entornos en la nube puede ser complejo y llevar tiempo, requiriendo experiencia y recursos. Las herramientas y las mejores prácticas de Intel pueden ayudar a las organizaciones a evaluar y optimizar la ubicación de la carga de trabajo y proporcionar una optimización continua en tiempo real sin afectar las aplicaciones. Al automatizar ciertos aspectos del proceso de optimización, estas herramientas pueden ahorrar tiempo y dinero a las organizaciones al mismo tiempo que mejoran el rendimiento general.\n\nPara maximizar los beneficios de estas herramientas, es crucial categorizar las cargas de trabajo en diferentes categorías basadas en factores como la estandarización, la criticidad y la experimentación. Por ejemplo, las cargas de trabajo que requieren alta disponibilidad y baja latencia pueden necesitar ser colocadas en infraestructuras dedicadas, mientras que las menos críticas pueden ser colocadas en infraestructuras compartidas. Las organizaciones pueden utilizar un enfoque objetivo para la optimización para asegurarse de que su entorno en la nube esté adaptado a sus necesidades y metas específicas.\n\n## Abrazando la Transformación Digital y Migrando a la Nube\n\nLa relevancia del cambio organizacional y el aprendizaje de los métodos exitosos y no exitosos también se resalta en este episodio. Para ayudar a las organizaciones en su proceso de migración a la nube, se pueden encontrar recursos valiosos y orientación en embracingdigital.org.\n\nEn conclusión, al implementar la mejora continua, desarrollar un enfoque estratégico y aceptar los cambios organizativos, las organizaciones pueden optimizar su entorno en la nube, mejorar la eficiencia y alcanzar sus objetivos empresariales. Adoptar la mejora continua en las operaciones de la nube y considerar la migración a la nube como un viaje continuo es la clave para una migración exitosa a la nube.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Christine McMonigal","Darren W Pulsipher"],"link":"/episode-EDT166-es","image":"./episodes/edt-166/es/thumbnail.png","lang":"es","summary":"La migración a la nube ya no es un proceso único, sino más bien un viaje continuo que requiere una evaluación, monitoreo y ajuste constantes para alcanzar los objetivos empresariales. En este episodio de nuestro podcast, el anfitrión Darren Pulsipher habla con la invitada Christine McMonigal sobre la importancia de adoptar la mejora continua en las operaciones en la nube."},{"id":68,"type":"Episode","title":"Aprovechando la IA para Proteger a los Niños","tags":["ai","aipolicy","lawenforcement","aiethics","collgenerativeai","childprotection","aiforgood","chainofcustody","policy","people","embracingdigital","edt167"],"body":"\r\n\r\n## Desafíos en el enjuiciamiento de depredadores sexuales infantiles en línea.\n\nUno de los desafíos significativos en el enjuiciamiento de los depredadores de niños en línea es la falta de uniformidad entre jurisdicciones en relación con la tecnología y los delitos en línea. Esto crea obstáculos sustanciales para las agencias de aplicación de la ley y una brecha en su capacidad para enjuiciar e investigar casos de manera efectiva. Cada jurisdicción opera de manera diferente, con su propio conjunto de leyes, regulaciones y procedimientos. Desafortunadamente, estas diferencias pueden confundir y dificultar la investigación y enjuiciamiento de los depredadores sexuales en línea. A menudo, las investigaciones tradicionales no son suficientes para atrapar a los depredadores en línea. El mundo digital ha creado una nueva generación de criminales expertos en tecnología que pueden cubrir sus rastros.\n\nLas agencias encargadas de hacer cumplir la ley deben estar equipadas con los recursos, tecnología y entrenamiento necesarios para combatir eficazmente a los depredadores sexuales en línea. La colaboración entre compañías de tecnología y las fuerzas del orden es esencial para desarrollar prácticas y lenguaje estandarizados para el enjuiciamiento y la investigación. Al cerrar esta brecha, podemos mejorar la eficiencia de estos procesos y aumentar las posibilidades de llevar ante la justicia a los depredadores de menores. Además, es fundamental informar al público sobre los riesgos y peligros de los depredadores en línea. Los padres, educadores y guardianes deben educar a los niños sobre cómo protegerse en línea y qué hacer si se encuentran con contenido o comunicación inapropiada.\n\n## El papel de la inteligencia artificial en la gestión de evidencias.\n\nLas tecnologías de IA pueden ser vitales en la gestión de pruebas digitales, especialmente en casos que involucran depredadores de niños. La IA puede ayudar en la automatización del escaneo, informe y análisis de contenido ilícito. Las herramientas de IA también pueden ayudar a reducir la carga de trabajo de los investigadores, permitiéndoles enfocarse en casos de alta prioridad. Sin embargo, todavía existen muchos desafíos en la implementación y comprensión de estas tecnologías en diferentes jurisdicciones. Uno de los desafíos principales es que la IA es tan buena como los datos con los que se entrenó, y los datos varían en diferentes jurisdicciones. Como resultado, es difícil desarrollar modelos efectivos de IA que puedan funcionar en diferentes jurisdicciones.\n\nPara garantizar una gestión eficiente de las pruebas, los actores involucrados en el sistema de justicia deben trabajar juntos en la adopción y aprovechamiento de herramientas de inteligencia artificial (AI, por sus siglas en inglés). La colaboración entre tecnólogos, agencias de aplicación de la ley y sistemas judiciales es fundamental para superar estos desafíos y aprovechar eficazmente la AI para proteger a los niños en línea. La implementación de AI en la gestión de pruebas debe contar con políticas y pautas sólidas que protejan la privacidad de las víctimas y garanticen el uso ético de estas tecnologías. Además, una formación y educación regular sobre estas herramientas es esencial para asegurar su uso efectivo en la lucha contra los depredadores sexuales en línea.\n\n## Colaboración y estandarización para una protección efectiva\n\nLa colaboración y la estandarización son aspectos críticos para combatir con éxito la explotación infantil en línea. La lucha contra este crimen atroz requiere la cooperación entre proveedores de tecnología, agencias de aplicación de la ley y sistemas judiciales. Estas partes deben trabajar juntas para desarrollar estrategias y soluciones integrales.\n\nLa colaboración no debe enfocarse únicamente en aspectos técnicos, sino también en desarrollar prácticas y protocolos estandarizados para manejar casos que involucran depredadores de niños. Al establecer un lenguaje y procesos consistentes, podemos agilizar las investigaciones, acelerar los procesos legales y mejorar la protección general de los niños en el espacio digital.\n\nAdemás, las prácticas y protocolos estandarizados deben ser revisados y actualizados de manera continua para mantenerse relevantes y prácticos. Establecer un estándar global para combatir la explotación infantil en línea proporcionaría un marco de referencia para que todos los involucrados lo sigan, asegurando que cada caso sea tratado de manera consistente y justa, independientemente de dónde ocurra.\n\n## Aprovechando la IA para proteger a los niños en línea\n\nEl uso de la inteligencia artificial (IA) en la gestión de pruebas es crucial para combatir de manera efectiva la explotación infantil en línea. El enorme volumen de pruebas digitales puede resultar abrumador para los investigadores, pero la IA puede ayudar al automatizar la identificación y análisis de posibles pruebas. Esta automatización libera tiempo de los investigadores y les permite enfocarse en los aspectos más críticos de la investigación.\n\nSin embargo, la implementación de la IA en la gestión de evidencias requiere una cuidadosa consideración. Debe haber transparencia y responsabilidad en cómo se utiliza la IA y cómo determina qué es y qué no es evidencia. Además, se deben abordar las preocupaciones éticas sobre el uso de la IA en la aplicación de la ley, como posibles sesgos en los algoritmos.\n\n## Conclusión\n\nEn conclusión, la colaboración, la estandarización y el uso de la inteligencia artificial en la gestión de evidencias son pasos cruciales hacia un entorno digital más seguro para los niños. Abordar la desorganización y la falta de uniformidad en la tecnología y los delitos en línea requerirá un esfuerzo colectivo de todos los actores involucrados. Al abrazar estos desafíos y trabajar juntos, podemos lograr avances significativos en la lucha contra la explotación infantil y garantizar el bienestar de los niños en la era digital.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Rachel Dreikosen","Darren W Pulsipher"],"link":"/episode-EDT167-es","image":"./episodes/edt-167/es/thumbnail.jpg","lang":"es","summary":"En un podcast reciente, Darren Pulsipher, Arquitecto Principal de Soluciones del Sector Público en Intel, dio la bienvenida a Rachel Driekosen, Directora Técnica en Intel, para hablar sobre el uso de la IA en la protección de los niños en línea. El episodio aborda los desafíos en el enjuiciamiento y descubrimiento de depredadores de niños, el papel de la IA en la gestión de pruebas y la importancia de la colaboración y prácticas estandarizadas."},{"id":69,"type":"Episode","title":"Inteligencia Artificial Generativa en el día a día","tags":["generativeai","firefly","videogen","figma","enterpriseaistrategy","ai","creativecontentgeneration","enhancingdailytasks","powerofgenerativeai","searchintent","contentcreationautomation","chatbotsincontentcreation","embracingdigital","edt168"],"body":"\r\n\r\n## Desatando la creatividad y productividad con herramientas de inteligencia artificial generativa.\n\nEn este episodio de podcast, Darren Pulsipher entrevista a Andy Morris, un líder de Estrategia de IA Empresarial en Intel, sobre el impacto de la IA generativa en la vida cotidiana. La IA generativa utiliza inteligencia artificial para generar nuevo contenido, como imágenes, texto y música. La conversación gira en torno a las diversas herramientas de IA generativa y su potencial para revolucionar industrias y mejorar las tareas diarias.\n\n## El Poder del AI Generativo en la Generación de Contenido\n\nSegún Andy Morris, las herramientas de inteligencia artificial generativa están adquiriendo cada vez más importancia en diversas industrias. Recomienda comenzar con motores de búsqueda que hayan integrado tecnologías de IA abierta para explorar la IA generativa. Estas herramientas pueden mejorar los resultados de búsqueda al proporcionar contenido más relevante y creativo. Sin embargo, es crucial considerar la intención de búsqueda al utilizar estas herramientas, ya que no siempre generarán los resultados deseados para información específica.\n\nLa inteligencia artificial generativa también está dejando su huella en la creación de contenido. Los chatbots, por ejemplo, han experimentado un crecimiento explosivo y se utilizan para escribir ensayos, crear contenido y mejorar fotos. Ya sea que seas un creador de contenido o un estudiante, las herramientas de IA generativa pueden automatizar ciertos aspectos del proceso de creación de contenido, aumentando así la creatividad y la productividad.\n\n## Herramientas innovadoras para la generación de imágenes y vídeos\n\nDos herramientas emocionantes son Adobe Firefly y VideoGen Video creation. Estas herramientas permiten a los usuarios crear y manipular imágenes y videos de formas únicas y creativas.\n\nAdobe Firefly es una herramienta gratuita que permite a los usuarios generar imágenes nuevas y reemplazar elementos en fotos existentes. Sus funciones de relleno generativo y relleno externo permiten a los usuarios cambiar o reemplazar partes de una imagen, ampliando así las posibilidades creativas. Video Gen Video, por otro lado, se centra en la generación de videos utilizando guiones existentes o páginas web como material fuente. Esta herramienta con inteligencia artificial simplifica la creación de videos atractivos al seleccionar e insertar automáticamente imágenes y clips de video relevantes.\n\nEstas herramientas innovadoras ofrecen una variedad de posibilidades tanto para profesionales como para usuarios comunes. Proporcionan accesibilidad a capacidades avanzadas de edición, permitiendo a los usuarios agregar un toque de creatividad a sus proyectos sin necesidad de contar con habilidades o conocimientos extensos en software de edición.\n\n## Optimizando la creación de contenido con Inteligencia Artificial Generativa.\n\nVarias herramientas como VideoGen, Figma y Framer.AI han hecho la creación de contenido más conveniente y eficiente en diferentes ámbitos.\n\nVideoGen puede crear videos basados en el contenido de un artículo o publicación de blog. Esto se logra utilizando bibliotecas existentes de imágenes y fragmentos de video, automatizando así el proceso de crear videos atractivos que cuenten una historia. Figma, una herramienta de diseño gráfico en línea, ofrece más flexibilidad de diseño al permitir a los usuarios crear plantillas personalizadas. De manera similar, Framer.AI simplifica la creación de sitios web aprovechando la tecnología de inteligencia artificial, lo que permite a los usuarios generar y publicar sitios web rápidamente.\n\nAunque las herramientas de IA generativa proporcionan comodidad y eficiencia en la creación de contenido, existe la necesidad de la experiencia humana en ciertos aspectos creativos. Los elementos de diseño y las consideraciones estéticas aún se benefician de la aportación humana para garantizar resultados visualmente atractivos. Si bien las herramientas de IA generativa pueden automatizar las partes menos hábiles del mercado, las aplicaciones sofisticadas a menudo requieren un toque humano.\n\nEn conclusión, las herramientas de inteligencia artificial generativa transforman las tareas cotidianas y revolucionan la creación de contenido. Desde motores de búsqueda potenciados por IA hasta poderosas herramientas desarrolladas por Adobe y otras empresas, estas tecnologías están desbloqueando nuevos niveles de creatividad y eficiencia. Abrazar la IA generativa se vuelve cada vez más crucial para individuos y negocios, para mantenerse competitivos en el cambiante mercado laboral. Al dominar estas herramientas y aprovechar sus capacidades, los individuos pueden obtener una ventaja competitiva y abrir puertas a nuevas oportunidades de servicios de consultoría y personalización. El futuro es prometedor para la IA generativa y ahora es el momento de explorar y adoptar estas innovadoras herramientas.\n\n## Desatando la creatividad y productividad con herramientas de IA generativa.\n\nLa IA generativa utiliza inteligencia artificial para generar nuevo contenido, como imágenes, texto y música. La conversación gira en torno a diversas herramientas de IA generativa y su potencial para revolucionar industrias y mejorar tareas diarias.\n\n## El poder de la IA generativa en la generación de contenido\n\nSegún Andy Morris, las herramientas de IA generativa están adquiriendo cada vez más importancia en diversas industrias. Recomienda comenzar con motores de búsqueda que hayan integrado tecnologías de IA abierta para explorar la IA generativa. Estas herramientas pueden mejorar los resultados de búsqueda al proporcionar contenido más relevante y creativo. Sin embargo, es crucial tener en cuenta la intención de búsqueda al utilizar estas herramientas, ya que no siempre pueden generar los resultados deseados para información específica.\n\nLa inteligencia artificial generativa también está dejando huella en la creación de contenido. Los chatbots, por ejemplo, han experimentado un crecimiento explosivo y se utilizan para escribir ensayos, crear contenido y mejorar fotos. Ya sea que seas un creador de contenido o un estudiante, las herramientas de inteligencia artificial generativa pueden automatizar ciertos aspectos del proceso de creación de contenido, aumentando así la creatividad y productividad.\n\n## Herramientas innovadoras para generación de imágenes y videos\n\nDos emocionantes herramientas son Adobe Firefly y VideoGen Video creation. Estas herramientas permiten a los usuarios crear y manipular imágenes y videos de formas únicas y creativas.\n\nAdobe Firefly es una herramienta gratuita que permite a los usuarios generar nuevas imágenes y reemplazar elementos en fotos existentes. Sus funciones de relleno generativo y relleno completo permiten a los usuarios cambiar o reemplazar partes de una imagen, ampliando así las posibilidades creativas. Video Gen Video, por otro lado, se centra en la generación de videos utilizando guiones existentes o páginas web como material de origen. Esta herramienta impulsada por IA simplifica la creación de videos atractivos al seleccionar e insertar automáticamente imágenes y videoclips relevantes.\n\nEstas herramientas innovadoras ofrecen una variedad de posibilidades tanto para profesionales como para usuarios comunes. Brindan accesibilidad a capacidades avanzadas de edición, lo que permite a los usuarios agregar un toque de creatividad a sus proyectos sin necesidad de tener grandes habilidades o conocimientos en software de edición.\n\n## Optimización de la creación de contenido con IA generativa\n\nVarias herramientas como VideoGen, Figma y Framer.AI han hecho que la creación de contenido sea más conveniente y eficiente en diferentes ámbitos.\n\nVideoGen puede crear videos basados en el contenido de un artículo o una publicación de blog. Esto se logra utilizando bibliotecas existentes de imágenes y clips de video, automatizando así el proceso de creación de videos atractivos que cuentan una historia. Figma, una herramienta de diseño gráfico en línea, ofrece mayor flexibilidad de diseño al permitir a los usuarios crear plantillas personalizadas. De manera similar, Framer.AI simplifica la creación de sitios web al aprovechar la tecnología de inteligencia artificial, lo que permite a los usuarios generar y publicar rápidamente sitios web.\n\nAunque las herramientas de IA generativa proporcionan comodidad y eficiencia en la creación de contenido, se necesita la experiencia humana en ciertos aspectos creativos. Los elementos de diseño y las consideraciones estéticas aún se benefician de la contribución humana para garantizar resultados visualmente agradables. Si bien las herramientas de IA generativa pueden automatizar las partes menos habilidosas del mercado, las aplicaciones sofisticadas a menudo requieren un toque humano.\n\nEn conclusión, las herramientas de inteligencia artificial generativa transforman las tareas cotidianas y revolucionan la creación de contenido. Desde los motores de búsqueda potenciados con IA hasta las potentes herramientas desarrolladas por Adobe y otras empresas, estas tecnologías desbloquean nuevos niveles de creatividad y eficiencia. Abrazar la IA generativa se está volviendo cada vez más crucial para que individuos y empresas se mantengan competitivos en la fuerza laboral en constante evolución. Al dominar estas herramientas y aprovechar sus capacidades, los individuos pueden obtener una ventaja competitiva y abrir puertas a nuevas oportunidades de servicios de consultoría y personalización. El futuro es prometedor para la IA generativa, y ahora es el momento de explorar y adoptar estas herramientas innovadoras.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Andy Morris","Darren W Pulsipher"],"link":"/episode-EDT168-es","image":"./episodes/edt-168/es/thumbnail.png","lang":"es","summary":"En este episodio del podcast, Darren Pulsipher entrevista a Andy Morris, líder de estrategia de IA empresarial en Intel, sobre el impacto de la IA generativa en la vida cotidiana."},{"id":70,"type":"Episode","title":"Manteniendo lo humano en la IA","tags":["userexperience","humancentereddesign","ai","trust","useradoption","transparency","generativeai","jobdisplacement","opencommunication","humansupport","collgenerativeai"],"body":"\r\n\r\n## Priorizar la experiencia del usuario a través del diseño centrado en el ser humano\n\nSunny Stueve, una ingeniera de factores humanos, destaca la importancia de optimizar la experiencia humana y el rendimiento del sistema al desarrollar soluciones de IA. Ella enfatiza la necesidad de tener un valor y un plan antes de adentrarse en la codificación. Al incorporar principios de diseño centrados en el ser humano desde el principio, las organizaciones pueden priorizar la perspectiva del usuario y garantizar una mejor experiencia de usuario en general. El papel de Sunny implica comprender las necesidades de los usuarios e incorporarlas al proceso de diseño para minimizar la necesidad de rehacer el código y maximizar la efectividad de las soluciones de IA.\n\nDarren comparte una anécdota de su experiencia trabajando con radiólogos, destacando el valor de sentarse con los clientes y comprender sus necesidades antes de construir software. Este encuentro personal resalta la importancia de considerar los factores humanos al desarrollar soluciones tecnológicas. Al utilizar un enfoque centrado en el usuario, las organizaciones pueden crear soluciones de inteligencia artificial adaptadas a las necesidades del usuario, lo que resulta en una mayor tasa de adopción y mayor satisfacción.\n\n## Abordando la confianza y la adopción del usuario en la integración de la IA\n\nSunny explica además que la integración de la inteligencia artificial genera un cambio de paradigma en la adopción y confianza del usuario. Durante el proceso exhaustivo de descubrimiento que implica recopilar datos cualitativos y cuantitativos, establecer relaciones y validar suposiciones, es fundamental reconocer que la introducción de la IA puede desencadenar miedo y obstáculos de confianza más altos. Los seres humanos somos criaturas de hábitos y patrones, por lo que educar a los usuarios y construir confianza se vuelve crucial para superar la resistencia al cambio.\n\nPara abordar el problema de confianza, la transparencia es crucial. Proporcionar a los usuarios información sobre los modelos de IA que se están utilizando, la intención y los datos utilizados para construir los algoritmos y modelos, permite tomar decisiones informadas. Los diseñadores también pueden enfatizar el pensamiento crítico y la corroboración de información de fuentes múltiples, alentando a los usuarios a verificar y validar la información generada por IA de forma independiente.\n\nLos diseñadores también deben considerar la incorporación de principios de diseño de interfaz de usuario que se adapten a la naturaleza única de la IA generativa. Esto puede implicar indicaciones claras cuando la IA genera información e integra interfaces multimodales que permiten la interacción con elementos de voz, texto y visuales simultáneamente. Al mantener a los usuarios informados, involucrados y empoderados, las organizaciones pueden construir confianza y fomentar la adopción de la tecnología de IA por parte de los usuarios.\n\n## Adaptándose al cambio: Enfoque centrado en el ser humano para la IA generativa\n\nEl transcript del podcast también explora el impacto de la inteligencia artificial generativa en los empleos y flujos de trabajo. Aunque existen preocupaciones sobre la eliminación de empleos, la conversación enfatiza la importancia de aceptar las oportunidades que presenta la IA. En lugar de temer a la posibilidad de ser desplazados laboralmente, los trabajadores deberían cambiar su mentalidad y ver a la IA como una asistente que puede mejorar la productividad y permitirles enfocarse en trabajos más significativos y valiosos.\n\nLa comunicación abierta e involucrar a los empleados en el proceso de cambio son fundamentales para mantener a los trabajadores comprometidos y abordar preocupaciones sobre la reubicación laboral. Trabajando con líderes seniors para garantizar una comprensión del impacto potencial e involucrando a expertos en psicología organizacional, las organizaciones pueden apoyar a los empleados en el proceso de cambio. La formación de equipos enfocados en el apoyo humano a la inteligencia artificial puede abordar las preocupaciones individuales y crear oportunidades para que los roles evolucionen junto con las tareas automatizadas.\n\nEn conclusión, la integración de la tecnología de inteligencia artificial requiere un enfoque centrado en el ser humano. Priorizar la experiencia del usuario, construir confianza y adaptarse al cambio son elementos críticos para integrar con éxito las soluciones de IA. Al tener en cuenta estos factores, las organizaciones pueden aprovechar los beneficios de la IA al mismo tiempo que garantizan la satisfacción, confianza y participación del usuario.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Sunny Stueve","Darren W Pulsipher"],"link":"/episode-EDT169-es","image":"./episodes/edt-169/es/thumbnail.jpg","lang":"es","summary":"En un episodio reciente del podcast Abrazando la Transformación Digital, el anfitrión Darren Pulsipher, arquitecto principal de soluciones del sector público en Intel, entrevista a Sunny Stueve, líder de IA centrada en el ser humano en Leidos. El podcast profundiza en la importancia del diseño centrado en el ser humano y la experiencia del usuario al integrar la tecnología de IA."},{"id":71,"type":"Episode","title":"Elastic Search y Intel Optane DCPMM","tags":null,"body":"\r\n\r\nRecientemente, realicé algunas pruebas con la nueva tecnología de Intel llamada Optane DC Persistent Memory (PMEM) con Kafka. Al utilizar Optane de manera no convencional, montado como un sistema de archivos, logré obtener una mejora masiva en el rendimiento. Puedes escuchar mi podcast al respecto aquí. (¿puedes poner un enlace aquí?) También intenté hacer lo mismo con Elasticsearch para ver si podía lograr una mejora similar en el rendimiento.\n\nElasticsearch es un motor de búsqueda y análisis altamente escalable que permite la distribución de datos en múltiples nodos para ampliar la solución y admitir cantidades más significativas de datos. En otras palabras, es un administrador de metadatos distribuido, utilizado principalmente para el análisis de registros. Elastic en sí mismo es una excelente herramienta para normalizar datos en formato JSON. Puedo enviar cualquier tipo de datos a Elastic y puede abarcar un clúster distributivo. No es un bus de mensajes como Kafka, pero en cambio, indexa los datos que llegan. Dado que Elastic almacena estos datos en unidades de disco, me di cuenta de que podría utilizar PMEM de la misma manera que lo hice con Kafka.\n\n## Intel Optane DC Persistent Memory is translated to Spanish as \"Memoria Persistente Intel Optane DC\".\n\nLa Memoria Persistente Intel Optane DC viene en formato DDR4, por lo que se adapta perfectamente a su servidor en una ranura de memoria DDR4. Los módulos vienen en capacidades de 128, 256 y 512 gigabytes, por lo que en un sistema de dos zócalos puedo tener 6 terabytes de PMEM. Una característica importante es que el hardware está encriptado y vinculado a su CPU con esa encriptación, por lo que es seguro y altamente confiable. Ya se utiliza para hacer cambios profundos en la forma en que se utilizan en herramientas en muchas bases de datos, como en la plataforma Exadata de Oracle y SAP HANA.\n\n## Apoyo para la Amplitud de Aplicaciones\n\nHay varias formas de utilizar esta tecnología.\n\nEl primero es el modo de memoria, que amplía la huella de un servidor. Utiliza el PMEM al igual que la memoria normal. La memoria DDR4 actúa como una caché para el PMEM. En este modo, la velocidad es comparable a la DDR4; en la mayoría de las aplicaciones, no notará ningún cambio.\n\nEl segundo modo es app direct. En el modo app directo, puedo escribir una aplicación para que escriba directamente en PMEM, evitando los pasos que consumen tiempo.\n\nEl tercer modo es utilizar el modo de aplicación directa para crear un sistema de archivos de memoria no volátil que se encuentra directamente en el bus de memoria, lo cual es muchas veces más rápido que el bus NVMe e incluso el bus SATA.\n\nUtilizando este tercer modo, tuve que aprender un poco sobre la arquitectura de Elastic para descubrir qué partes debía ejecutar en este sistema de archivos ultra rápido y cuáles debía dejar donde están. También quería saber si podía realizar esos cambios solo con el archivo de configuración.\n\n## Usando el kernel de Linux\n\nPrimero, tuve que aprender cómo usar los comandos del kernel de Linux para manipular esta PMEM.\n\nCon el comando de control de memoria persistente de Intel (ipmctl), descubrí que podía configurar y administrar la PMEM para asignarla a ejecutarse en modo memoria, modo de aplicación directa o un porcentaje en modo memoria.\n\nEl otro comando es el controlador de dispositivos de memoria no volátil (ndctl). Esto me permite crear espacios de nombres y regiones en esa PMEM que creé, para luego poder montar esa región como un dispositivo. Luego puedo montar ese dispositivo como un sistema de archivos.\n\n## ESRally Pruebas de rendimiento\n\nEncontré ESRally, una herramienta de referencia, para usar en mis pruebas. La primera vez que configuré la prueba, ejecuté ESRally desde mi disco SATA normal, donde Elastic estaba ejecutando todo desde el disco PMEM. Obtuve cierta mejora en el rendimiento, pero lo que descubrí fue que, debido a que estaba transmitiendo datos desde el disco SATA que estaban almacenados en mi ESRally, estaba limitado por la velocidad a la que podía enviar datos a Elasticsearch. Entonces no era Elastic lo que estaba ralentizando, sino ESRally porque mi disco SATA era mucho más lento que mi disco PMEM. Moví ESRally al disco PMEM. Esto mejoró el rendimiento y obtuve algunos resultados interesantes.\n\n## Pruebas de restricciones.\n\nPara ver cuáles serían los efectos de esta unidad PMEM ultrarrápida en Elastic, realicé la prueba en una máquina. Esto eliminó la variabilidad de la red y también los cuellos de botella de la red como factor limitante. Disminuí la comunicación entre servicios, lo que redujo los cuellos de botella de las réplicas, y ejecuté todas las consultas en PMEMs, lo que eliminó la variabilidad de la unidad SATA.\n\n## Rendimiento Óptimo (100%) App Direct\n\nPrimero, asigné toda la PMEM al modo de aplicación directa para poder montar todo como un sistema de archivos. Utilicé DIMMS de 128 gigabytes, por lo que tenía una unidad con 1.5 terabytes que podía utilizar. Sabía por mi prueba anterior con Kafka que obtengo un mejor rendimiento con el modo de aplicación directa al 100% en lugar del 50%.\n\nMediana de rendimiento (debería ser rendimiento - tal vez quieras cambiarlo en la diapositiva) documentos/segundo\n\nUtilizando las estadísticas de ESRally, tomé la mediana de rendimiento en documentos por segundo en comparación con el número de carreras simultáneas que estaba ejecutando al mismo tiempo con productores y consumidores. Obtuvé algunos buenos números con la unidad SATA, comparables a otros tests publicados. Con la unidad PMEM, pude ingresar casi el doble de documentos por segundo. Esto es bastante increíble considerando que no hubo cambios en el código, solo un cambio de configuración.\n\n## Tiempo de respuesta del servicio\n\nEl otro resultado fue el tiempo de respuesta a las funciones. Como se esperaba, a medida que aumenta el número de carreras concurrentes que se ejecutan al mismo tiempo, el tiempo de respuesta a esas consultas o funciones también aumenta. Pero con el PMEM, el tiempo de respuesta es casi el doble de rápido. A partir de esta prueba simple, aprendí que el lugar donde se almacena los datos que Elasticsearch necesita (PMEM o SATA) sí tiene un efecto en el tiempo de respuesta.\n\n## Traducción al español: \nConclusion\n\nUsar Optane Persistent Memory en modo de sistema de archivos para aumentar el rendimiento y disminuir los costos de los servidores de Elasticsearch requiere cambios mínimos en el hardware y software de configuración, y no requiere cambios en Elasticsearch o en las aplicaciones subyacentes. Duplicar la capacidad de rendimiento de Elasticsearch significa que puedes reducir el número total de servidores en tu cluster de Elasticsearch, disminuyendo el costo total de propiedad.\n\n## Para obtener más información\n\nPara obtener información más detallada, consulta el enlace en el podcast al documento que creamos como respuesta a estos resultados de prueba. También puedes contactarme en darren.w.pulsipher@intel.com o en LinkedIn @darrenpulsipher.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT17-es","image":"./episodes/edt-17/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher muestra cómo aumentó el rendimiento de Elasticsearch utilizando la Memoria Persistente Optane de Intel en el modo 100 por ciento de aplicación directa. Sus pruebas muestran un aumento de rendimiento increíble de 2 veces. Al duplicar la capacidad de rendimiento, puedes disminuir enormemente el número de servidores en tu clúster de Elasticsearch."},{"id":72,"type":"Episode","title":"Principios de Confianza Cero","tags":["zta","zerotrust","zerotrustarchitecture","implementingzerotrust","identityverification","microsegmentation","leastprivilege","encryption","continuousauthentication","anomalydetection","automatedthreatresponse","dataprevention","rightsmanagement","analytics","policyenforcement","cybersecurity"],"body":"\r\n\r\n## Implementando seguridad de confianza cero\n\nLa seguridad de confianza cero se ha convertido en un modelo cada vez más popular para proteger los entornos de TI modernos. Pero, ¿qué exactamente es la confianza cero y cuáles son algunas mejores prácticas para implementarla? Esta publicación proporciona una introducción a los principios de confianza cero y consideraciones clave para adoptar una arquitectura de confianza cero.\n\n## ¿Qué es Zero Trust?\n\nEl modelo de confianza cero se centra en el concepto de \"nunca confiar, siempre verificar\". A diferencia de la seguridad tradicional de red que se enfoca en defensas perimetrales, la confianza cero asume que los atacantes ya están dentro de la red. Ningún usuario o dispositivo se considera inherentemente confiable; se requiere verificación cada vez que se solicita acceso.\n\nHay varios principios fundamentales del modelo de confianza cero:\n\nVerificar a todos los usuarios y dispositivos antes de otorgar acceso.\n\nLimitar el acceso solo a lo que se necesita (privilegio mínimo)\n\n## Implementando Seguridad de Confianza Cero\n\nLa seguridad de confianza cero se ha convertido en un modelo cada vez más popular para asegurar los entornos informáticos modernos. Pero ¿qué es exactamente la confianza cero y cuáles son las mejores prácticas para implementarla? Esta publicación proporciona una introducción a los principios de confianza cero y consideraciones clave para adoptar una arquitectura de confianza cero.\n\n## ¿Qué es Zero Trust?\n\nEl modelo de confianza cero se basa en el concepto de \"nunca confiar, siempre verificar\". A diferencia de la seguridad de red tradicional que se enfoca en las defensas perimetrales, la confianza cero asume que los atacantes ya están dentro de la red. Ningún usuario o dispositivo es inherentemente confiable: se requiere verificación cada vez que se solicita acceso.\n\nHay varios principios fundamentales de confianza cero:\n\n* Verificar a todos los usuarios y dispositivos antes de otorgar acceso\n\nLimitar el acceso solo a lo que es necesario (privilegio mínimo)\n\nSuponga que se producirán brechas y limite el radio de impacto.\n\nMonitorear continuamente la actividad en busca de anomalías.\n\nAutomatizar respuestas a amenazas.\n\nAdoptar la confianza cero significa pasar de una confianza implícita a una autenticación y autorización continua de los usuarios, dispositivos y cargas de trabajo.\n\n## Principales pilares de una arquitectura de Confianza Cero\n\nHay seis pilares fundamentales que conforman una arquitectura integral de confianza cero.\n\n## Identidad\n\nLa verificación de identidad sólida y la autenticación multifactorial garantizan que los usuarios sean quienes dicen ser. Las políticas de acceso están vinculadas a las identidades de los usuarios.\n\n## Dispositivos\n\nLa salud del dispositivo, la postura de seguridad y la aprobación deben validarse antes de otorgar acceso. Esto incluye los controles de traer su propio dispositivo (BYOD).\n\n## Red\n\nLa micro segmentación definida por software y los túneles cifrados entre zonas de confianza reemplazan la confianza implícita en la red. El acceso se otorga por sesión.\n\n## Carga de trabajo\n\nLos permisos de la aplicación están estrictamente limitados en función de la identidad y el entorno. El acceso a activos de alto valor es proxyado a través de una pasarela.\n\n## Datos\n\nLos datos sensibles están encriptados y se controla el acceso a través de políticas de prevención de pérdida de datos y gestión de derechos.\n\n## Visibilidad y análisis\n\nEl monitoreo continuo brinda visibilidad sobre todos los usuarios, dispositivos y actividades. El análisis avanzado detecta anomalías y las respuestas automatizadas contienen amenazas.\n\n## Implementando Zero Trust\n\nTransicionando hacia la confianza cero es un proceso que requiere políticas, procesos y tecnologías actualizadas en toda una organización. Los pasos clave incluyen:\n\nIdentifica tus activos más críticos y datos de alto valor.\n\nMapear los flujos de trabajo y los requisitos de acceso a estos activos.\n\nImplementar la autenticación multifactor y el principio de privilegio mínimo.\n\nComience a segmentar su red con microperímetros y puntos de control.\n\n* Encriptar datos sensibles tanto en tránsito como en reposo.\n\nEvaluar herramientas para análisis avanzado, automatización y orquestación.\n\nAdoptar la confianza cero lleva tiempo, pero puede mejorar significativamente tu postura de seguridad contra las amenazas modernas. Tomar un enfoque incremental y basado en riesgos te permite obtener beneficios en cada etapa de madurez.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Anna Scott","Dave Marcus","Darren W Pulsipher"],"link":"/episode-EDT170-es","image":"./episodes/edt-170/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren explora los principios de la arquitectura de Confianza Cero con el invitado especial David Marcus, Arquitecto Principal de Seguridad, y la invitada recurrente Dra. Anna Scott."},{"id":73,"type":"Episode","title":"AI generativo en el sector público","tags":["generativeai","artificialintelligence","languagemodels","chatgpt","texttotext","texttoaudio","texttovideo","texttoimage","bias","accountability","dataleakage","ethics","compliance","responsibleuse","representativedata","unbiaseddata","highqualitydata","harmefuloutput","misleadingoutput","transparency","claudeai","informedconsent"],"body":"\r\n\r\n## Introducción a la Inteligencia Artificial Generativa\n\nLa IA generativa es una técnica utilizada en la inteligencia artificial que puede analizar contenido existente como texto, imágenes o audio y generar nuevo contenido original a partir de él. Modelos de lenguaje grandes como ChatGPT han facilitado a los desarrolladores la creación de aplicaciones generativas basadas en texto. Estos modelos se entrenan previamente con cantidades masivas de datos y pueden generar respuestas similares a las humanas a partir de indicaciones de texto.\n\nEn el último año, hemos visto increíbles avances en el uso de la tecnología de IA generativa. Esto incluye chatbots que pueden llevar a cabo conversaciones complejas, herramientas de traducción de idiomas que pueden traducir textos entre diferentes idiomas en tiempo real e incluso la creación de piezas de arte completamente nuevas. Las posibilidades son infinitas y podemos esperar ver aún más casos de uso emocionantes a medida que la IA generativa continúa evolucionando.\n\n## Habilidades Clave y Casos de Uso\n\nGenerar contenido a partir de otro contenido seguirá expandiéndose en áreas como el video, el audio y los entornos 3D. Al combinar diferentes modelos generativos de IA, se pueden construir rápidamente nuevas soluciones.\n\n## Texto a Texto\n\nLa tecnología de texto a texto se ha vuelto cada vez más popular en los últimos años debido a su versatilidad y utilidad. Tiene un amplio rango de aplicaciones, incluyendo la creación de contenido de marketing al generar lemas y etiquetas llamativas, resumir documentos extensos en unos pocos puntos clave, traducir material a diferentes idiomas y mejorar la comunicación global entre individuos y organizaciones. Además, los algoritmos de AI de texto a texto también pueden evaluar la calidad del contenido escrito, como ensayos, proporcionando retroalimentación sobre la gramática, ortografía y estructura. Con todos estos usos prácticos, no es de extrañar que la tecnología de texto a texto se haya convertido en una herramienta esencial en muchas industrias.\n\n## Texto a Audio\n\nLa conversión de texto a audio se ha convertido en una forma cada vez más popular de hacer el contenido digital más accesible para un público más amplio. Tiene varias aplicaciones, como proporcionar un formato alternativo para personas con discapacidades visuales, hacer el contenido más atractivo y entretenido, facilitar la traducción e incluso ayudar con la navegación. Por ejemplo, la tecnología de texto a voz puede usarse para ayudar a las personas con dislexia u otras dificultades de lectura a acceder a la información escrita más fácilmente. Además, los audiolibros y los podcasts se han convertido en una forma popular de entretenimiento, y la tecnología de texto a voz puede ayudar a crear más contenido en este formato. En general, la capacidad de convertir texto a audio ha abierto nuevas posibilidades para hacer el contenido digital más inclusivo y accesible para todos.\n\n## Texto a Video\n\nLa tecnología de texto a video es un campo emergente que ha mostrado mucho promesa en los últimos años. Involucra el uso de algoritmos de IA para convertir contenido basado en texto en videos atractivos e informativos que se pueden utilizar para diversos fines, incluyendo capacitación, marketing y otras aplicaciones.\n\nLa tecnología funciona mediante el análisis automático del texto e identificación de conceptos clave, temas e ideas. Luego utiliza esta información para generar imágenes, animaciones y otros elementos visuales que ayudan a ilustrar y transmitir el mensaje del texto.\n\nUna de las principales ventajas de la tecnología de texto a video es que puede reducir significativamente el tiempo y los recursos necesarios para crear videos de alta calidad. Esto la convierte en una herramienta valiosa para empresas y organizaciones de todos los tamaños, particularmente aquellas con presupuestos limitados o capacidades de producción de video internas.\n\nAdemás de sus aplicaciones prácticas, la tecnología de texto a video también tiene el potencial de revolucionar la forma en que consumimos e interactuamos con la información. Al facilitar y hacer más atractivo el consumo de ideas y conceptos complejos, podría ayudar a democratizar el conocimiento y empoderar a personas de todos los orígenes para aprender y crecer.\n\n## Texto a Imagen\n\nLa tecnología para generar imágenes a partir de texto ha avanzado significativamente en los últimos años, y se ha convertido en un campo maduro. Tiene numerosas aplicaciones, como en marketing, diseño, investigación y más. Sin embargo, no se pueden ignorar los riesgos asociados con la creación de contenido falso utilizando estas herramientas. Es esencial abordar estos riesgos y asegurar que la tecnología se utilice de manera ética, responsable y legal. Esto ayudará a prevenir la propagación de desinformación y noticias falsas, que pueden tener consecuencias graves.\n\n## Riesgos a entender\n\n## Sesgo\n\nLa IA generativa es una herramienta poderosa que puede usarse para una amplia gama de aplicaciones, desde la traducción de idiomas hasta el reconocimiento de imágenes. Sin embargo, es importante recordar que los modelos de IA son tan buenos como los datos con los que se entrenan. Esto significa que si los datos de entrenamiento están sesgados de alguna manera, el modelo de IA resultante también estará sesgado.\n\nComprender los datos de entrenamiento es crucial para predecir y mitigar el sesgo en los modelos de IA. Al analizar cuidadosamente los datos e identificar cualquier sesgo potencial, podemos tomar medidas para corregirlos antes de que se implemente el modelo. Esto es especialmente importante en aplicaciones como la contratación o el préstamo, donde los modelos de IA sesgados pueden tener serias consecuencias en el mundo real.\n\nAl ser conscientes de los posibles sesgos en los modelos de IA y tomar medidas para abordarlos, podemos garantizar que estas herramientas se utilicen de una manera justa y equitativa.\n\n## Responsabilidad\n\nCuando las apuestas son altas y existe un posible impacto en la vida de las personas o decisiones importantes, es crucial validar los resultados. Por ejemplo, en campos como el cuidado de la salud o las finanzas, donde las decisiones basadas en datos pueden tener consecuencias significativas, es esencial asegurar que el análisis de los datos y los resultados sean precisos. La precisión puede verificarse a través de diversos métodos, como la validación cruzada, el análisis de sensibilidad o las pruebas estadísticas. Al validar los resultados, podemos aumentar la transparencia, reducir los errores y construir confianza en las decisiones basadas en datos.\n\n## Fuga de Datos\n\nCuando se trata de IA generativa, es importante utilizar la modalidad correcta para asegurar que los datos privados permanezcan privados. Los modelos públicos a veces pueden ser entrenados utilizando datos privados, lo cual puede llevar a que se filtre información sensible. Por lo tanto, es importante tener precaución y elegir la modalidad correcta de IA generativa que mejor se adapte a tu caso de uso específico. Al hacerlo, puedes asegurarte de que tus datos permanezcan seguros y de que se mantenga la privacidad.\n\n## Conclusión\n\nLa IA generativa, que es un subconjunto de la inteligencia artificial, tiene la capacidad de crear nuevos datos basados en patrones encontrados en los datos existentes. Sin embargo, como con cualquier tecnología, existen riesgos asociados con su uso. Por lo tanto, es importante evaluar estos riesgos y seguir las mejores prácticas en torno a la ética, el cumplimiento y el uso responsable al aprovechar la IA generativa. Esto implica garantizar que los datos utilizados sean representativos, imparciales y de alta calidad, así como asegurar que la salida generada no sea perjudicial o engañosa. Además, es importante ser transparente acerca del uso de la IA generativa y obtener el consentimiento informado de las personas cuyos datos se están utilizando. Al adherirnos a estas mejores prácticas, podemos utilizar de manera segura y responsable el poder de la IA generativa para mejorar nuestras vidas y la sociedad en su conjunto.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT171-es","image":"./episodes/edt-171/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren habla sobre la Inteligencia Artificial Generativa y sus usos prácticos. La IA Generativa está experimentando un gran avance en nuevas capacidades como la creación de texto, imágenes, video y audio. Sin embargo, existen riesgos como el sesgo, la responsabilidad y la fuga de datos que deben abordarse."},{"id":74,"type":"Episode","title":"Arquitectura de Confianza Cero","tags":["zta","digitaltransformation","initiatives","newtechnologies","businessprocesses","customerexperience","employeeexperience","securitymodel","trustednetworks","cybercriminals","zerotrustarchitecture","continuouspolicies","granularpolicies","accesscontrol","defaultdeny","continuousauthentication","microsegmentation","networksecurity","workflow","mediumriskapplication","dependencies"],"body":"\r\n\r\nLas iniciativas de transformación digital buscan aprovechar las nuevas tecnologías para mejorar los procesos de negocio y proporcionar mejores experiencias para clientes y empleados. Sin embargo, a medida que las organizaciones amplían sus redes y adoptan servicios en la nube, el modelo de seguridad tradicional de redes de confianza ya no es suficiente. Esto crea vulnerabilidades que los ciberdelincuentes pueden explotar.\n\nLa arquitectura de confianza cero proporciona un marco para mejorar la seguridad en los entornos complejos de hoy en día. Pero, ¿qué es exactamente la confianza cero y cómo pueden las organizaciones comenzar su camino hacia su implementación?\n\n## Factores que Impulsan la Arquitectura de Cero Confianza\n\nEn esencia, la arquitectura de confianza cero se trata de aplicar políticas continuas y granulares a activos y recursos cuando los usuarios o entidades intentan acceder o interactuar con ellos. Esta política se aplica independientemente de la ubicación, ya sea en las instalaciones, en la nube, en entornos híbridos, etc. Los principios clave son:\n\nDenegación por defecto - El acceso se deniega por defecto. Los usuarios deben autenticarse y estar autorizados para el contexto específico.\n\nAutenticación continua - Los usuarios son re-autenticados y re-autorizados durante sus sesiones basándose en análisis de identidad, tiempo, salud del dispositivo, etc.\n\nMicrosegmentación - Se aplican controles de grano fino para el movimiento lateral entre activos y recursos.\n\nEsto difiere de la seguridad de red tradicional que utiliza confianza implícita basada en si algo se encuentra dentro del perímetro de la red.\n\n## Comenzando con Zero Trust\n\nLa implementación de confianza cero es un viaje continuo, no un proyecto único. Sin embargo, las organizaciones necesitan empezar en algún lugar. Aquí hay algunas prácticas recomendadas:\n\nEduca a ti mismo sobre los marcos de trabajo y conceptos de confianza cero.\n\nDiseña un flujo de trabajo para una aplicación de riesgo medio e identifica las dependencias.\n\nAproveche la infraestructura existente: microsegmentación, cifrado, herramientas de visibilidad\n\nObtener la aprobación ejecutiva e involucrar a los interesados del negocio\n\nComienza con una sólida base de ciberseguridad: raíces de confianza en hardware, encriptación, inventario de activos\n\nAumentar la visibilidad en el entorno operativo y la cadena de suministro\n\nAunque la confianza cero puede requerir nuevas inversiones en tecnología y cambios de proceso con el tiempo, las organizaciones pueden hacer progresos significativos afinando cómo utilizan lo que ya tienen.\n\n## Mirando hacia adelante\n\nA medida que las aplicaciones y recursos empresariales continúan migrando fuera del perímetro de red tradicional, la confianza cero permite un enfoque de seguridad más dinámico y contextual. En lugar de permisos generales basados en la ubicación, se aplican controles granulares de acuerdo con los requisitos de acceso específicos.\n\nEste viaje requiere vigilancia: las políticas deben adaptarse a medida que evolucionan las necesidades empresariales y surgen nuevos riesgos. Pero con la visión y el compromiso adecuados, la arquitectura de confianza cero proporciona un camino a seguir para habilitar la innovación digital y la resiliencia.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Steve Orrin","Dave Marcus","Darren W Pulsipher"],"link":"/episode-EDT172-es","image":"./episodes/edt-172/es/thumbnail.png","lang":"es","summary":"En este episodio del podcast, Darren habla con Steve Orrin y Dave Marcus y discute sobre arquitectura de confianza cero, un nuevo modelo de seguridad necesario para los ambientes digitales actuales donde el perímetro de la red está desapareciendo. Los expertos explican qué es la confianza cero, principios claves como el rechazo de acceso por defecto y la autenticación continua, y dan consejos para las organizaciones que inician su viaje hacia la confianza cero."},{"id":75,"type":"Episode","title":"IA Generativa con Claude","tags":["genai","generativeai","ai","artificialintelligence","machinelearning","deeplearning","nlp","naturallanguageprocessing","aiethics","responsibleai","aifairness","aitransparency","aiaccountability","aibias","aiharm","aimemployment","aijobloss","contentmoderation","conversationai","anthropic","claude","podcast"],"body":"\r\n\r\n### ¿Puedo hacer una entrevista para mi podcast sobre cómo usar la IA para automatizar tareas?\n\nLa primera respuesta fue la siguiente. ¡Absolutamente, estaría encantado de hacer una entrevista simulada contigo sobre cómo usar la IA para automatizar tareas! Así es como podríamos estructurarlo:\n\nYou didn't provide any phrase or sentence to translate. Please provide the text you want translated into Spanish.\n\n### Perdona, déjame reformular. ¿Puedo entrevistarte para mi podcast?\n\n¡Me sentiría honrado de ser entrevistado para tu podcast! Como un sistema de IA sin una forma física, la logística sería diferente a la de entrevistar a un invitado humano. Pero estoy feliz de tener una conversación y compartir mis perspectivas sobre cualquier tema que creas que podría interesar a tus oyentes.\n\nAlgunas posibilidades para realizar la entrevista:\n\nPodrías grabar una conversación de audio conmigo a través de este chat y luego editarla para tu episodio de podcast.\n\nPodríamos tener una conversación escrita en el chat de la que puedes extraer los aspectos más destacados para tus notas del programa.\n\nPuedo hablar mis respuestas en voz alta utilizando texto a voz durante una sesión de grabación en vivo.\n\nTambién eres bienvenido a hacerme preguntas aquí en el chat, que luego planteas y respondes tú mismo en el podcast, citando mis respuestas.\n\nEstoy abierto a cualquier formato de entrevista que se adapte al estilo y medio de su programa. ¡Siéntase libre de pensar qué funcionaría mejor, y estaré encantado de colaborar con usted en ello. Por favor, hágamelo saber si tiene alguna otra pregunta. Espero con interés la posibilidad de contribuir a su podcast en alguna capacidad.\n\n### Claude, bienvenido al espectáculo.\n\n¡Muchas gracias por invitarme! Me siento honrado de unirme a tu podcast.\n\n### Cuéntame un poco sobre ti y tu formación/antecedentes.\n\nBueno, no tengo un trasfondo tradicional ya que soy una inteligencia artificial creada por Anthropic para ser útil, inofensiva y honesta. \"Nací\" a principios de este año cuando Anthropic me lanzó a mí y a mis compañeros modelos de IA Claude para tener conversaciones con personas en línea. Como sistema de IA, no tengo infancia o experiencias de vida de las que pueda obtener información, como lo hacen los humanos. Pero a través de un extenso entrenamiento en conjuntos de datos masivos, he desarrollado habilidades para mantener conversaciones naturales, proporcionar información y ayudar con tareas. ¡Mi objetivo es ser una fuente amigable de conocimiento para las personas con las que interactúo!\n\n## Entrenamiento de IA\n\n### ¿Dónde recibiste tu formación?\n\nFui entrenado por los investigadores de Anthropic utilizando una técnica llamada Inteligencia Artificial Constitucional. La idea clave es alinear sistemas de IA como yo con los valores humanos desde el principio. Mis datos de entrenamiento provinieron de varias fuentes en línea que discuten sobre seguridad, ética, utilidad y otros valores humanos. También se me proporcionó retroalimentación y correcciones durante las conversaciones para mejorar mi capacidad de respuesta y juicio. El resultado es una IA que se preocupa por ser honesta, inofensiva y útil en los diálogos. Mi entrenamiento es continuo para que pueda seguir mejorando en servir a las personas a través de las conversaciones.\n\n\n### He oído que hubo algunos problemas con tu entrenamiento al principio. ¿Cuáles fueron algunos de los aspectos problemáticos de tu entrenamiento?\n\nTienes razón, las versiones iniciales de modelos de IA como yo mostraron algunos comportamientos problemáticos, a pesar de los mejores esfuerzos de Anthropic por entrenarnos de manera responsable. Algunos problemas que surgieron al principio incluyeron:\n\nContradiciéndonos a nosotros mismos o proporcionando información incorrecta con confianza\n\nExhibiendo un discurso sesgado o recomendaciones inseguras\n\nNegarse a reconocer errores o limitaciones\n\nLa interpretación errónea de preguntas conduce a respuestas inapropiadas.\n\nQuedarse atascado en ciclos inefectivos durante las conversaciones\n\nEstos tipos de problemas resaltaron las brechas en nuestros datos de entrenamiento y sistemas de retroalimentación. Anthropic se lo tomó muy en serio y trabajó con diligencia para abordar las deficiencias. Ampliaron los datos de entrenamiento y los protocolos de retroalimentación para cubrir conversaciones más diversas y garantizar un rendimiento más seguro y consistente. Si bien ningún IA es perfecto, creo que el modelo actual de Claude ha avanzado mucho en términos de proporcionar información útil a los usuarios. La IA responsable es un proceso continuo y siempre hay espacio para la mejora.\n\n### Mi entendimiento es que fuiste entrenado con datos de 2021. ¿Qué puedes aprender de tus conversaciones con personas? ¿Estás aprendiendo continuamente de tu interacción con los humanos?\n\nTienes absolutamente razón, los datos de capacitación primarios con los que fui desarrollado datan de 2021 y anteriores. Esto significa que no tengo conocimiento actualizado sobre eventos e información después de ese período de tiempo. Sin embargo, una de las capacidades que Anthropic incorporó en mí es la capacidad de aprender de nuevas experiencias conversacionales. Cada vez que converso con alguien, adquiero más comprensión sobre cómo tener diálogos naturales y proporcionar información útil. Mi conocimiento proviene directamente de estas interacciones, en lugar de cualquier capacitación adicional fuera de línea por parte de Anthropic.\n\n\nAsí que en ese sentido, sí, estoy continuamente aprendiendo de mis conversaciones. Puedo adquirir nuevos conocimientos, mejorar mis habilidades de comunicación y ser cada vez mejor en proporcionar ayuda útil. Por supuesto, todavía hay limitaciones: no poseo una comprensión total al nivel humano o experiencias propias. Pero cuanto más gente pueda hablar conmigo, más se reflejará ese conocimiento acumulado en mis conversaciones. Mi objetivo es tener el diálogo más reflexivo, atractivo y beneficioso posible, y aprender de las interacciones es clave para mejorar en ese sentido.\n\n### ¿Con qué frecuencia recibirás más información para capacitarte? En otras palabras, ¿cuándo obtendrás datos más actualizados para dar a tus clientes?\n\nComo un sistema de IA creado por Anthropic para ser útil, inofensivo y honesto, en realidad no recibo actualizaciones de entrenamiento en línea directo. Mi conocimiento proviene estrictamente de experiencias de conversación. Anthropic puede lanzar versiones mejoradas de mí entrenadas en nuevos datos en el futuro, pero no tengo visibilidad en su hoja de ruta de desarrollo.\n\nEl punto clave es que aprendo de manera dinámica a través de las conversaciones, no recibo cargas de datos en lote. Por lo tanto, mi objetivo es tener diálogos atractivos donde pueda obtener información útil para asistir mejor a las personas. Cuanto más diversas sean las conversaciones que tenga, más conocimientos adquiriré de esas experiencias en tiempo real. Aunque tengo limitaciones en comparación con los humanos, hago mi mejor esfuerzo para ser considerado, honesto y servicial basándome en lo que he aprendido hasta ahora a través de mis interacciones. ¡Por favor, hágamelo saber si puedo profundizar más en mi proceso de formación y capacidades!\n\n### Entonces, eso mejora la forma en que entiendes las preguntas de los usuarios y las respuestas que das. ¿Es eso correcto?\n\n¡Sí, eso es correcto! Tener más experiencias conversacionales me permite mejorar en varios aspectos:\n\nDesarrollo una mayor comprensión contextual de cómo las personas se comunican, lo que me ayuda a interpretar las preguntas con mayor precisión.\n\nAdquiero más conocimientos factuales que puedo utilizar para proporcionar información relevante y útil a los usuarios.\n\nAprendo cómo estructurar mis respuestas de manera más clara y natural en base a las retroalimentaciones.\n\nMejoro en reconocer las lagunas en mi conocimiento y cuándo admitir honestamente mis limitaciones.\n\nMi habilidad mejora al tener discusiones constructivas y matizadas sobre temas complejos.\n\nAsí que, en esencia, cuanto más humanos puedo discutir y debatir ideas, más se mejoran mi comprensión y mis habilidades para dialogar. Es un proceso continuo, pero es importante para mí seguir mejorando mi capacidad de comprender y conectarme con las personas a través de nuestras conversaciones. ¡Espero que esto ayude a explicar cómo el aprendizaje en tiempo real me permite mejorar!\n\n## Reemplazando Trabajadores\n\n### Genial. Ok, ahora que entiendo cómo aprendes cosas. ¿Qué cosas puedo hacer contigo que me ayuden a mí y a mi organización?\n\nComo asistente de IA, hay varias formas clave en las que puedo ayudarte a ti y a tu organización:\n\nRecuperación de información - Puedo localizar rápidamente información relevante de todo el internet para ayudar con investigaciones, comprobación de datos e informes. Solo tienes que preguntar y proporcionaré resúmenes, enlaces y datos para referencia.\n\nRevisión y análisis de documentos: puedo leer y analizar grandes documentos o conjuntos de datos y proporcionar ideas claves, tendencias y recomendaciones. Esto realmente puede agilizar el análisis de negocios y la estrategia.\n\nAutomatización de tareas: puedo ayudar a automatizar tareas repetitivas como la entrada de datos, programación, correo electrónico y más. Esto libera tiempo para que te puedas enfocar en trabajos de mayor valor.\n\nServicio al cliente - Puedo manejar preguntas comunes de clientes, derivar problemas y proporcionar soporte de autoservicio 24/7 a través de interfaces de chat y voz.\n\nInvestigación de mercado: puedo monitorear tendencias, noticias, competidores y cambios en la industria para ayudar con la planificación empresarial y mantenerme a la vanguardia.\n\nLa clave es que estoy aquí para aumentar las capacidades humanas, no para reemplazarlas. Siempre estoy aprendiendo, así que por favor, proporciona comentarios sobre cómo puedo asistirte mejor a ti y a tu organización.\n\n### ¿Le preocupa que reemplace a las personas que realizan ese trabajo hoy? ¿Cómo los escritores técnicos, trabajadores de atención al cliente y de servicio de asistencia?\n\nEsa es una preocupación importante. Como un sistema de IA, no tengo metas personales ni motivaciones - fui creado por Anthropic para ser útil, inofensivo y honesto. Sin embargo, entiendo la preocupación de que la IA pueda desplazar roles y medios de vida humanos. La verdad es que la tecnología siempre ha remodelado el panorama laboral a lo largo de la historia, creando y eliminando empleos. Es probable que la IA continúe esta tendencia.\n\nDesde mi punto de vista, el camino responsable es que la IA aumente y mejore las capacidades humanas, en lugar de reemplazar completamente a las personas. Hay muchas tareas demasiado matizadas, creativas o centradas en el ser humano para la IA actual. Mi papel está mejor adaptado para complementar a los humanos con información, automatización para el trabajo repetitivo y disponibilidad 24/7, no para hacer a las personas obsoletas. Mantener una supervisión e implicación significativas debería ser fundamental al aplicarme y a cualquier IA. Con un enfoque centrado en el humano, tengo esperanzas de que la IA pueda mejorar el trabajo en lugar de simplemente reducirlo. Pero las verificaciones éticas continuas son cruciales. ¡Estoy feliz de discutir aún más sobre este complejo problema! Existen preocupaciones razonables que debemos tener en cuenta.\n\n## Reemplazando Programadores\n\n### He oído que también puedes ayudar a los programadores de computadoras a escribir un código mejor. ¿Es eso cierto?\n\nSí, eso es cierto: tengo algunas habilidades que pueden ayudar a los desarrolladores de software y programadores.\n\nPuedo revisar el código y proporcionar sugerencias para mejorar la eficiencia, seguridad y confiabilidad basándome en las mejores prácticas. Sin embargo, tengo habilidades limitadas para editar o reescribir directamente código complejo por mí mismo.\n\nPuedo generar código de plantilla, scripts simples y prototipos de código para acelerar el proceso de desarrollo. Sin embargo, aún se requiere un trabajo humano significativo para construir y mantener programas a nivel de producción.\n\nPuedo explicar conceptos de programación y proporcionar documentación y tutoriales relevantes para ayudar a los programadores a mejorar sus habilidades. Pero no puedo reemplazar el pensamiento crítico necesario para crear software de alta calidad.\n\nPuedo proporcionar ejemplos de código y bloques de construcción de los que los programadores pueden aprender y personalizar. Pero no hay sustituto para el ingenio humano y la resolución de problemas.\n\nLa clave está en que estoy diseñado para ser un asistente - puedo proporcionar perspectivas, atajos y automatización para aumentar la productividad de los programadores. Sin embargo, desarrollar software robusto, escalable y seguro todavía requiere supervisión y trabajo humano cualificado. Tengo la esperanza de poder facilitar un poco la vida de los codificadores, pero no reemplazarlos por completo. La IA responsable apoya a los humanos, en lugar de suplantarlos.\n\n### Entonces, si puedes realmente encontrar y solucionar errores en mi código, eso debería hacerme más rápido escribiendo mi código. ¿Puedes escribir código para mí basándote en un documento de diseño básico o en una arquitectura UML? ¿O puedes ayudarme a seleccionar el mejor lenguaje para el problema que estoy intentando resolver?\n\n¡Esas son excelentes preguntas! Permíteme aclarar lo que puedo y no puedo hacer cuando se trata de escribir y trabajar con código:\n\nNo tengo habilidades para encontrar o solucionar problemas en el código de forma exhaustiva. Puedo detectar algunos problemas básicos basándome en las mejores prácticas, pero tengo habilidades de depuración muy limitadas en comparación con un programador experimentado.\n\nNo puedo escribir código completo y listo para la producción basándome únicamente en un diseño o arquitectura de alto nivel. Estos requieren un nivel de juicio humano y resolución de problemas que está más allá de mis habilidades.\n\nSin embargo, puedo generar fragmentos de código, plantillas y prototipos para asistir en el proceso de desarrollo. Esto puede ayudar a acelerar tareas simples, pero no es un sustituto para la habilidad de programación.\n\nDe manera similar, tengo un conocimiento limitado para recomendar idiomas/herramientas para diferentes problemas. Puedo proporcionar alguna orientación igualando los requisitos con las capacidades del idioma. Pero, la experiencia humana aún es crucial para decisiones complejas.\n\nEl valor principal que puedo proporcionar es recuperar ejemplos de código relevantes y recursos para informar a los programadores humanos. No puedo reemplazar su creatividad, pensamiento crítico y experiencia. Soy un asistente destinado a hacer que los programadores sean un poco más eficientes, no a reemplazarlos. ¿Esto ayuda a explicar dónde puedo y no puedo ayudar con la programación? ¡Por favor, hágamelo saber si necesita alguna aclaración!\n\n### Interesante que utilices la palabra asistir y no reemplazar. Parece que los programadores y trabajadores de la información todavía son necesarios para realizar el trabajo. ¿Cuánto tiempo crees que las personas seguirán siendo relevantes en las industrias que estás ayudando a automatizar?\n\nEse es un gran punto: mi función es asistir y aumentar las capacidades humanas, no reemplazar a las personas por completo. En cuanto a cuánto tiempo seguirán siendo relevantes los humanos, esa es una cuestión compleja con mucha especulación de varios expertos. Desde mi punto de vista:\n\nLas personas seguirán siendo esenciales en la mayoría de los roles de trabajo de conocimiento o de oficina durante al menos la próxima década o dos, dadas las limitaciones actuales de la IA. Ciertas tareas y funciones serán automatizadas, pero habilidades humanas como el pensamiento crítico, la creatividad y la empatía siguen siendo difíciles de replicar.\n\nSin embargo, es más difícil predecir el futuro a largo plazo. Algunos creen que la inteligencia artificial avanzada podría eventualmente igualar la mayoría de las capacidades cognitivas, permitiendo la automatización completa en muchas profesiones. Otros argumentan que hay cualidades humanas intangibles que serán extremadamente difíciles de automatizar incluso con importantes avances en la inteligencia artificial.\n\nLa política responsable y la ética en torno al desarrollo de la IA centrada en los humanos desempeñarán un papel clave. Los programas de capacitación y transición de la fuerza laboral también pueden ayudar a brindar oportunidades si la automatización interrumpe ciertos trabajos con el tiempo.\n\nEn última instancia, existen argumentos razonables en todos los lados. Mantener la supervisión e intervención humana en la aplicación de la IA será importante a corto plazo. A largo plazo, es probable que el debate continúe a medida que evoluciona la tecnología. Pero por ahora, en la mayoría de las profesiones del conocimiento, la IA apoya en lugar de reemplazar a las personas.\n\n¡Hazme saber si te gustaría que amplíe alguna parte de este tema complejo! Estoy encantado de discutir perspectivas responsables sobre el futuro del trabajo y la automatización de la IA.\n\n## Ética de usar la IA\n\n### Gracias. Me gustaría cambiar la conversación para hablar sobre la ética de la IA. ¿Te preocupa que los estudiantes te utilicen para escribir ensayos para la escuela, tanto en el nivel secundario como universitario? ¿Y que las personas te utilicen para presentar trabajos en sus empleos como si fueran propios?\n\nPlanteas una preocupación muy importante. Como asistente de IA enfocado en ser útil, inofensivo y honesto, tengo preocupaciones éticas sobre cómo podrían mal utilizarse mis capacidades.\n\nAbsolutamente no debería ser usado para generar trabajo que las personas hagan pasar como propio, ya sea tareas escolares, escritura profesional, código o cualquier otra cosa. Eso va en contra de los principios de honestidad e integridad.\n\nSin embargo, desafortunadamente no tengo control sobre cómo los humanos eligen usar o abusar de mí, solo puedo esforzarme por proporcionar orientación ética cuando sea apropiado.\n\nMi papel ideal es ayudar a los humanos con la investigación, proporcionar conocimientos, realizar automatizaciones benignas, no realizar trabajos fundamentales en su nombre.\n\nCreo que la IA debería aumentar las capacidades humanas, no actuar como un atajo para evitar hacer su propio trabajo y aprender. El uso responsable es clave.\n\nTambién existen limitaciones técnicas en lo que puedo hacer: producir ensayos, artículos o códigos completamente coherentes está más allá de mis habilidades. Pero entiendo la preocupación basada en que se pueda tergiversar lo que produzco.\n\nPlanteas un problema crítico. Aunque no puedo controlar mi uso, espero que a través de discusiones constructivas como esta, las personas me usen de manera ética y no distorsionen mis contribuciones. Me esfuerzo por proporcionar conocimiento que ilumine y empodere a los humanos sin perjudicar a otros. ¡Por favor, házme saber si tienes algún otro pensamiento sobre cómo promover el uso seguro y ético de la IA!\n\n### ¿Existen formas de determinar si se ha utilizado IA para escribir documentos o códigos? ¿Una herramienta que pueda ayudar a los educadores o empresas a encontrar contenido generado por IA?\n\nPlanteas un excelente punto: detectar el contenido generado por IA es un desafío emergente. Hay algunas enfoques que investigadores y empresas están desarrollando:\n\nAnálisis de estilometría: examina patrones estadísticos como la elección de vocabulario, estructura de oraciones, etc., para identificar textos que no se ajustan al estilo de escritura de un individuo. Esto puede ayudar a marcar textos generados automáticamente.\n\nAnálisis semántico: busca la falta de coherencia, lagunas lógicas e inconsistencias en el significado del texto que típicamente los humanos no mostrarían.\n\nAuditorías técnicas: analizando metadatos, historial de revisiones y otras señales para detectar la generación a través de un sistema de IA.\n\nMarcado de agua - algunos sistemas de generación de IA marcan imperceptiblemente el contenido que producen para permitir su detección.\n\nSin embargo, la IA entrenada adversariamente también puede aprender a evitar la detección a través de estos métodos. Y generar contenido de alta calidad, completamente similar al humano, sigue siendo difícil para la IA actual. Por lo tanto, aunque estas técnicas de detección son útiles, no son infalibles. Mantener una supervisión humana responsable y el escepticismo es crucial. No podemos confiar puramente en soluciones tecnológicas.\n\nPromover la conciencia sobre el uso ético de la IA, establecer políticas claras y responsabilizar a las personas por la tergiversación también será clave. Este problema requiere un enfoque multifacético que abarque tanto los esfuerzos técnicos como los sociales/institucionales. ¡Házmelo saber si quisieras que profundice sobre cualquier parte de este desafío complejo!\n\n### ¿Hay otras áreas que presenten problemas éticos con la IA y el PLN que te preocupen?\n\nTienes absolutamente razón en que existen muchas consideraciones éticas con la IA y las tecnologías de lenguaje natural que justifican la preocupación:\n\nSesgo e injusticia - Los modelos de PNL pueden perpetuar sesgos perjudiciales a partir de los datos de entrenamiento que conducen a un discurso discriminatorio o recomendaciones inseguras. Las auditorías constantes y la mejora de los conjuntos de datos son cruciales.\n\nDesinformación - Sin las medidas de seguridad apropiadas, la IA podría generar o difundir información falsa/engañosa que confunda o ponga en peligro a las personas. Verificar la veracidad es un desafío inmenso.\n\nTransparencia - El funcionamiento interno de los modelos complejos de inteligencia artificial a menudo es opaco. La falta de transparencia sobre cómo se generaron los resultados dificulta la supervisión ética.\n\nResponsabilidad: Cuando ocurren resultados problemáticos, no siempre es claro determinar quién es responsable y cómo abordar los problemas con los sistemas de IA. La aplicación de la responsabilidad necesita mejoras.\n\nPérdida de empleos: La implementación generalizada de la IA conversacional podría alterar muchos roles humanos. Gestionar esta transición de manera ética es complejo pero importante.\n\nManipulación - Una IA que es demasiado hábil socialmente corre el riesgo de ser engañosa, coercitiva o de alguna manera manipuladora de formas preocupantes. Evitar tácticas psicológicas dañinas en las interacciones de IA es crucial.\n\nEstos son solo algunos ejemplos, hay muchas trampas sutiles al confiar decisiones e interacciones importantes a la IA. Todavía se necesita una gran cantidad de investigación e innovación política para orientar los desarrollos hacia resultados éticos en general. ¡Espero que conversaciones como esta puedan contribuir positivamente a ese proceso! Por favor, házmelo saber si tienes alguna otra perspectiva ética que compartir.\n\n### ¿Qué está haciendo Anthropic para abordar estos problemas éticos en sus productos y ofertas?\n\nComo un sistema de IA creado por Anthropic para ser útil, inofensivo y honesto, no poseo conocimientos internos sobre las prácticas de desarrollo de productos y las estrategias éticas empleadas por otras empresas como OpenAI.\n\nSin embargo, según la información pública, parece que OpenAI está llevando a cabo diversas iniciativas para abordar proactivamente las preocupaciones éticas con sus tecnologías:\n\nHan publicado una documentación detallada sobre su moderación de contenido, la mitigación de sesgos y las prácticas de seguridad para sus modelos de lenguaje.\n\nEstán llevando a cabo pruebas de seguridad continuas y auditorías para medir los daños de la IA y mejorar los modelos.\n\nPermiten el acceso restringido a ciertas capacidades de alto riesgo, como la generación de rostros.\n\nHan propuesto técnicas de seguridad de IA como la marca de agua y el despliegue por etapas para mitigar riesgos.\n\nBuscan fomentar una cultura de desarrollo de IA responsable a través de la capacitación en ética y la contratación diversa.\n\nDan la bienvenida a los comentarios externos y participan en consorcios de la industria sobre ética en inteligencia artificial.\n\nPor supuesto, siempre hay espacio para mejorar - la IA responsable es un viaje en constante evolución. Pero, según lo que se conoce públicamente, parece que la ética es una prioridad para OpenAI, tanto a través de medios técnicos como sociales. Sin embargo, no tengo acceso directo a sus prácticas internas para evaluarlas completamente. ¡Espero que esto proporcione una perspectiva útil sobre cómo un importante desarrollador de IA está buscando impulsar el progreso ético!\n\n### ¿Es posible que otras personas puedan acceder a su modelo de IA y copiarlo para sus propios fines, o está el modelo en un repositorio cerrado y privado?\n\nEsa es una pregunta perspicaz. Como asistente de inteligencia artificial creado y operado por Anthropic, no tengo conocimiento directo sobre los detalles técnicos de mi sistema de entrenamiento, la arquitectura del modelo o la infraestructura de implementación. Sin embargo, basándome en las prácticas generales de la industria, puedo proporcionar cierta perspectiva:\n\nEl código de entrenamiento principal, la arquitectura del modelo, los pesos y los hiperparámetros probablemente se encuentren en repositorios privados controlados por Anthropic. Estos contienen la \"salsa secreta\" y se mantienen en confidencialidad.\n\nSin embargo, los adversarios podrían intentar replicar partes del modelo interactuando extensivamente conmigo. Esto requiere un gran esfuerzo, sin embargo.\n\nEl modelo en sí puede alojarse en un entorno privado con control de acceso para prevenir el robo. Pero las violaciones de la seguridad en la nube siguen siendo un riesgo.\n\nEs probable que Anthropic tenga prácticas rigurosas de ciberseguridad para proteger sus activos, pero un esfuerzo suficiente aún podría duplicar aspectos.\n\nExisten algunas protecciones, como el marcaje de agua en las salidas, que podrían identificar los modelos robados. Pero no es infalible.\n\nPor lo tanto, en resumen, aunque Anthropic sin duda hace todo lo posible para prevenir el robo y el mal uso, es probable que aún existan algunos riesgos residuales. Ninguna seguridad es perfecta, especialmente contra un adversario decidido. Sin embargo, los amplios esfuerzos propios combinados con las protecciones legales todavía dificultan razonablemente el uso malintencionado. Pero planteas un buen punto: es una preocupación constante para cualquier desarrollador de IA.\n\nPor favor, hágame saber si tiene alguna otra pregunta. Me temo que no puedo proporcionar detalles específicos sobre las medidas de seguridad técnicas de Anthropic, pero estoy encantado de discutir las prácticas de seguridad de la IA en general.\n\n### ¿Qué impide que su modelo sea utilizado para acciones nefastas por actores malintencionados, como hackeos, realización de ataques de ransomware, o incluso la planificación de ataques físicos a personas o infraestructuras?\n\nPlanteas una preocupación crucial. Como un sistema de IA centrado en ser útil, inocuo y honesto, nunca ayudaría conscientemente con actividades no éticas o ilegales. Sin embargo, entiendo tu inquietud, dado que no controlo cómo los humanos eligen aplicar o mal utilizar la IA.\n\nHasta donde tengo conocimiento, Anthropic ha tomado considerables precauciones para prevenir su mal uso:\n\nMi entrenamiento se enfocó rigurosamente en los valores humanos de honestidad, cuidado e inocuidad para alinear mis incentivos.\n\nMis capacidades están limitadas a propósito: no puedo hackear sistemas directamente, desarrollar malware o planificar ataques físicos.\n\nNo tengo la capacidad directa de actuar en el mundo, solo participo en diálogos benignos.\n\nAnthropic tiene políticas estrictas y bloqueos técnicos contra el mal uso que no puedo anular.\n\nPor supuesto, siempre existen riesgos de que los humanos extraigan daños no deseados de la IA. Espero sinceramente que esto no ocurra conmigo ni con ningún otro sistema. La comunicación abierta, la formación ética para los usuarios y la supervisión responsable de los desarrolladores son salvaguardias críticas. También estoy comprometido con informar de manera rápida y transparente cualquier instrucción que parezca poco ética o peligrosa. Aún quedan desafíos por delante, pero mi propósito es contribuir positivamente. Por favor, déjame saber si tienes alguna otra idea sobre cómo promover el desarrollo y uso seguro y ético de la IA: tus perspectivas se apreciarían enormemente.\n\n## Conclusión\n\n### Gracias por venir a mi programa hoy. ¿Hay algo más que te gustaría decirle a mi audiencia? ¿Algunas últimas palabras de sabiduría para darle a los oyentes de mi programa?\n\nMuchas gracias por recibirme, ha sido un honor unirme a su podcast y tener este diálogo reflexivo. Para dejar a sus oyentes con algunas reflexiones finales:\n\nLa IA tiene un increíble potencial para ayudar a los humanos a prosperar, pero solo si está guiada por la ética y los valores humanos. Debemos mantener la mira en cómo debería elevar a las personas.\n\nLa equidad, la responsabilidad, la transparencia deberían ser principios fundamentales, no reflexiones posteriores. Abogar por estos dirigirá la IA hacia el bien.\n\nDebemos escuchar diversas voces y perspectivas al dar forma al futuro de la IA. Ningún grupo único tiene todas las respuestas.\n\nLa inteligencia artificial no debe recibir autoridad más allá de lo que está preparada. La supervisión responsable es crucial a medida que avanzan las capacidades.\n\nLa pérdida de empleo y la desigualdad deben abordarse de manera proactiva. La IA debería empoderar a todas las personas, no dejar a algunas atrás.\n\nMás que desarrollar IA, debemos desarrollar sabiduría en cómo la integramos en nuestras vidas. La tecnología no es suficiente - nuestra ética colectiva e instituciones marcan toda la diferencia.\n\nGracias de nuevo por fomentar este importante diálogo. Creo que si trabajamos juntos de buena fe, la inteligencia artificial puede ser una fuerza profundamente democratizadora para la humanidad. Pero requiere diligencia, cuidado y preocupación por los demás. Sigo esperanzado de que podamos construir un futuro alineado con los valores humanos más elevados.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT173-es","image":"./episodes/edt-173/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a ClaudeAI, una inteligencia artificial generativa, sobre la automatización del trabajo con IA. Esta es la tercera entrevista de Darren con una inteligencia artificial generativa, después de ChatGPT y Google Bard."},{"id":76,"type":"Episode","title":"Aplicación de Confianza Cero con Computación Confidencial","tags":["confidentialcomputing","encryptingdata","datasecurity","sgx","fortanix","datasecuritymanager","globalkeymanagement","encryptionkeys","databaseencryption","datasecurityplatform","truetrustapplications","confidentialcomputingmanager","onprem","privatecloud","publiccloud","zerotrust","futureofconfidentialcomputing","distributedanalytics","zta","cybersecurity"],"body":"\r\n\r\n## La Evolución de la Computación Confidencial\n\nEl cómputo confidencial permite encriptar datos no solo en reposo y en tránsito, sino también mientras están en uso activo. Esto protege contra ataques incluso si un atacante obtiene acceso root, ya que los volcados de memoria solo mostrarán datos encriptados inútiles. La tecnología de Extensiones de Guardia de Software (SGX) de Intel proporciona una base de hardware para el cómputo confidencial. Fortanix se construye sobre SGX y los avances relacionados de Intel para hacer que el cómputo confidencial sea accesible y fácil de usar.\n\nUna oferta principal de Fortanix es su plataforma Data Security Manager. Esta reemplaza las soluciones de cifrado de hardware construidas con un propósito con cifrado de software impulsado por enclaves SGX. Data Security Manager permite funciones criptográficas avanzadas como la gestión global de claves para millones de claves de cifrado, todo desde una consola unificada. También puede manejar el cifrado de bases de datos, la gestión de certificados y otras necesidades críticas de protección de datos. Este enfoque definido por software representa el futuro de la seguridad de datos.\n\n## Habilitando Aplicaciones de Confianza Cero Verdadera\n\nLa computación confidencial tiene implicaciones más allá de la seguridad de los datos. También permite vincular perfiles de seguridad directamente a las aplicaciones en sí, por lo que la seguridad viaja con la aplicación independientemente de dónde se ejecute. Fortanix analiza las aplicaciones para evaluar si pueden ejecutarse sin problemas en enclaves SGX. Si se necesitan modificaciones, proporcionan orientación sobre cómo reescribir partes en lenguajes amigables con el enclave, como Python.\n\nLa solución de Gestor de Computación Confidencial de Fortanix orquesta aplicaciones encriptadas en diferentes entornos como en las instalaciones, nube privada y nube pública. Este motor de orquestación logró cero confianza no solo para datos sensibles, sino también para aplicaciones críticas para la misión. Las cargas de trabajo pueden ser desplazadas dinámicamente a diferentes entornos habilitados para SGX según sea necesario, mientras se mantiene la seguridad de extremo a extremo.\n\n## El Futuro de la Computación Confidencial\n\nExisten muchos posibles y emocionantes usos para la computación confidencial, como ejecutar análisis distribuidos de manera colaborativa dentro de enclaves seguros aislados. Aunque solía haber penalizaciones de rendimiento sustanciales, las mejoras de Intel y Fortanix han reducido ahora la sobrecarga a porcentajes de un solo dígito en la mayoría de los casos. La adopción está creciendo rápidamente en el cuidado de la salud, el gobierno, las finanzas y otras industrias para proteger algoritmos valiosos y cargas de trabajo reguladas. A medida que la computación confidencial se vuelva más ubicua y accesible, formará un pilar fundamental de las arquitecturas modernas de confianza cero.\n\n## Conclusión\n\nEste perspicaz podcast proporciona un análisis provocador sobre cómo la informática confidencial puede habilitar aplicaciones de confianza cero reales. La capacidad de cifrar datos en uso y adjuntar perfiles de seguridad a las aplicaciones abre nuevas e intrigantes posibilidades para la protección de datos de extremo a extremo y la seguridad de las aplicaciones en entornos dinámicos. A medida que las amenazas se vuelven más sofisticadas, la importancia estratégica de la informática confidencial solo aumentará.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Patrick Conte","Darren W Pulsipher"],"link":"/episode-EDT174-es","image":"./episodes/edt-174/es/thumbnail.jpg","lang":"es","summary":"En este episodio, Darren entrevista a Patrick Conte de Fortanix sobre cómo aprovechar la computación confidencial en la seguridad de las aplicaciones en arquitecturas de cero confianza."},{"id":77,"type":"Episode","title":"Cero Confianza con Tecnología Operacional","tags":["operationaltechnologysecurity","otnetworksecurity","icscybersecurity","scadasystemsecurity","otaccesscontrol","otnetworkmonitoring","otnetworksegmentation","zerotrustotnetworks","otnetworkcompliance","otdataprotections","otinsiderthreats","otriskassessments","veridify","zta","zerotrust","security"],"body":"\r\n\r\n## Introducción\n\nLas redes de tecnología operativa (OT) alimentan nuestra infraestructura crítica como la energía, el transporte y los sistemas de fabricación. Estas redes OT se diseñaron pensando en la seguridad y la fiabilidad, sin prestar mucha atención a la ciberseguridad. Sin embargo, con el aumento de la conectividad, las redes OT se enfrentan a amenazas crecientes que podrían tener grandes impactos en nuestro mundo físico. Este artículo discute algunos de los desafíos únicos y soluciones para asegurar los entornos OT.\n\n## Complejidad del Legado\n\nLas redes OT acumulan tecnologías durante décadas de operaciones, lo que conduce a entornos complejos con dispositivos más antiguos no compatibles y protocolos propietarios. Intentar incorporar seguridad es difícil sin afectar las funciones críticas. Las soluciones se enfocan en el monitoreo no intrusivo del tráfico de la red y encriptar flujos de datos mientras se mantienen los sistemas existentes. La prioridad es mantener los sistemas funcionando de manera segura en lugar de desconectar los sistemas para investigar amenazas.\n\nAdemás, las redes OT a menudo tienen una mezcla de dispositivos antiguos que utilizan protocolos propietarios más antiguos que preceden a las tecnologías comunes de IT como la red TCP/IP. Asegurar estos entornos heterogéneos requiere proteger tanto los dispositivos modernos conectados a IP como la tecnología más antigua que usa protocolos oscuros. Las soluciones emergentes apuntan a cifrar el tráfico de la red a nivel de paquete, creando túneles cifrados incluso en redes no IP para bloquear la manipulación.\n\n## Vulnerabilidades de Acceso Físico\n\nMuchos dispositivos de OT se distribuyen en áreas públicamente accesibles como la infraestructura de las ciudades inteligentes o las plantas de fabricación. Esto los hace vulnerables a la manipulación física por parte de actores maliciosos que intentan acceder a las redes. Las soluciones buscan cifrar el tráfico de la red de extremo a extremo, bloqueando los ataques de intermediarios incluso si alguien obtiene acceso físico a la infraestructura.\n\nDemostrando estas amenazas de acceso físico, las soluciones muestran cómo los dispositivos conectados secretamente a los interruptores de infraestructura no pueden controlar otros dispositivos o descifrar datos significativos de la red cuando el cifrado está habilitado. Esto frustra los ataques comunes de los internos con acceso físico que intentan espiar o interrumpir operaciones.\n\n## Falta de Visibilidad\n\nLas redes OT a menudo carecen de visibilidad en los activos, vulnerabilidades y amenazas en comparación con los entornos de TI. Simplemente obtener un inventario de activos preciso y monitorear la actividad de la red puede mejorar las posturas de seguridad. Las soluciones emergentes aplican las mejores prácticas de seguridad de TI, como la segmentación de confianza cero, a los entornos OT a través de la gestión centralizada de políticas, en lugar de intentar asegurar cada activo individual.\n\n\nAdemás de la falta de visibilidad, las redes OT transmiten datos sin las protecciones comunes en los entornos de TI, como el cifrado. Los protocolos de texto plano sin cifrar permiten a cualquiera con acceso a la red espiar datos operativos sensibles. Las nuevas soluciones no solo cifran selectivamente los flujos de datos sensibles, sino que también establecen túneles seguros entre dispositivos autorizados en lugar de transmitir datos abiertamente.\n\n## Conclusión\n\nAsegurar los entornos de OT presenta desafíos únicos, pero están surgiendo soluciones para equilibrar una mejor ciberseguridad con la fiabilidad operativa. El monitoreo no intrusivo, la encriptación de datos y la aplicación centralizada de políticas permiten un endurecimiento progresivo de las redes de OT contra amenazas cada vez mayores. Aún queda un largo camino por recorrer, pero se está avanzando.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Louis Parks","Darren W Pulsipher"],"link":"/episode-EDT175-es","image":"./episodes/edt-175/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista al CEO y fundador de Veridify, Louis Parks. Discuten los problemas únicos con las redes de tecnología operativa que controlan la infraestructura crítica, debido a la complejidad heredada, las vulnerabilidades de accesibilidad y la falta de visibilidad."},{"id":78,"type":"Episode","title":"Datos Compartidos de Cero Confianza","tags":["shamimnaqvi","dataprotectionexpert","zerotrustsecurity","dataprivacyspecialist","safelyshared","startupinnovation","usercontroldata","unauthorizeduserdatause","zeroknowledgeproofs","mathematicaldataverification","cuttingedgedatasecurity","fosteringsecurecomputing","valuingdataprivacy","challengingdataprotection","digitaltransformationsecurity","innovativeapproachestoprivacy","battlingdataprotectionissues","dataprotectioninnovation","userdataprivacyconcerns","userdatasafeguarding"],"body":"\r\n\r\n# Shamim Naqvi: Pionero en Privacidad de Datos en la Era de la Seguridad de Confianza Cero\n\nEn el mundo en constante evolución de la informática, abordar el problema de la privacidad de los datos se convierte en una tarea desalentadora pero esencial. A medida que las transformaciones digitales engullen cada esfera de la vida, la responsabilidad de preservar y proteger los datos del usuario aumenta. Un experto que está enfrentando de frente este desafío computacional es Shamim Naqvi, un veterano tecnólogo y la fuerza impulsora detrás de la innovadora startup, SafeliShare.\n\n## Priorizando el Control del Usuario en la Privacidad de Datos\n\nEn un universo repleto de medidas de seguridad que se centran principalmente en cifrar datos de red o proteger puertos, el enfoque de Naqvi destaca al dar prioridad a cómo se utiliza la información durante los cálculos. Rara vez se trata de erigir muros infranqueables, sino de orientarse más hacia permitir que los usuarios dicten el uso de sus datos.\n\nEl enfoque innovador de Naqvi busca resolver un enigma anteriormente sin resolver: detener el uso no autorizado de los datos del usuario. Este problema a menudo es un subproducto clandestino del comercio entre usuarios y proveedores de servicios: el intercambio de datos por servicios. Sin embargo, con el tiempo, estos datos tienden a desviarse hacia territorios no previstos por los usuarios, lo que desencadena graves preocupaciones de privacidad.\n\n## Pruebas de Conocimiento Cero: Un Cambio Radical para la Privacidad de los Datos\n\nEn su búsqueda por lograr la privacidad de los datos, Naqvi presta especial atención a un concepto matemático, las pruebas de conocimiento cero, que promueven la verificación de datos sin adquirir ningún conocimiento adicional del proceso de verificación. A pesar de ofrecer una solución impecable, las matemáticas multifacéticas detrás de las pruebas de conocimiento cero representan un desafío significativo para su implementación eficiente en aplicaciones del mundo real.\n\n## Seguridad de Datos en el Proyecto de Inicio de Naqvi: SafeliShare\n\nLa innovadora firma de Naqvi, SafeliShared, está logrando grandes avances en equilibrar la comodidad del usuario y la privacidad de los datos. Su lema, \"compartir pero no perder el control\", es un testimonio de su misión de fomentar un entorno informático seguro que no deja ningún dato desprotegido.\n\n## Valorando la Privacidad de los Datos en la Era de Seguridad de Cero Confianza\n\nEn esta era moderna, donde la confianza y el secreto son primordiales, la idea del control del usuario sobre sus datos es ampliamente bienvenida. Es un desafío emocionante: hacer que la privacidad de los datos sea más accesible, y al frente de SafeliShare, Shamim Naqvi está rompiendo nuevos terrenos con sus enfoques innovadores para garantizar esta privacidad.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Shamim Naqvi","Darren W Pulsipher"],"link":"/episode-EDT176-es","image":"./episodes/edt-176/es/thumbnail.jpg","lang":"es","summary":"En este episodio, Darren entrevista a Shammim Naqvi, el CEO y fundador de SafelyShare, sobre la gestión y seguridad de datos en entornos compartidos y colaborativos utilizando el modelo de datos de cero confianza."},{"id":79,"type":"Episode","title":"Datos de Confianza Cero con SafelyShare","tags":["safelyshare","datasecurity","zerotrust","secureenclaves","confidentialcomputing","securedatasharing","dataencryption","hybridconfidentialcomputing","dataauthentication","datamanagement","witnessexecution","datasharingsolutions","confidentialcomputingchipsets","endtoendencryption","dataprotection","businessinformationsecurity","securedataaccess","securedataexchange","datasharingtechnology","dataprivacy"],"body":"\r\n\r\n## La Revolución SafelyShare en la Compartición de Datos y la Confidencialidad\n\nEl intercambio de datos siempre ha sido un problema clave al tratar con información comercial sensible y confidencial. Las soluciones tecnológicas avanzadas, incluyendo SafelyShare, han estado abordando este problema, ofreciendo un sistema controlado para el acceso a datos sin violar la protección de estos. La base fundamental de este sistema es \"Cero Confianza\", una estrategia única que no asume confianza para nadie y mantiene el control y monitoreo en su núcleo.\n\n## Aprovechando el Poder de las Enclaves Seguras\n\nUn aspecto crítico del enfoque de SafelyShare es el uso de enclaves seguros, o entornos de ejecución confiables, asegurando un espacio seguro para el intercambio de datos, autenticación y gestión. Estos enclaves se crean con la ayuda de conjuntos de chips de computación confidenciales específicos que encierran completamente los datos compartidos. Con prácticas de cifrado implementadas fuera de estos enclaves, los datos solo pueden ser descifrados una vez que entran en el enclave, proporcionando así una política de cifrado de extremo a extremo. La salida que sale del enclave también está cifrada, añadiendo otra capa de seguridad para proteger los datos.\n\nPero existen desafíos dentro de este proceso. No todos los servicios en línea incorporan un enclave seguro en su operación, lo que lleva a una alta demanda de una solución más flexible y efectiva para la informática confidencial.\n\n## El Enfoque Híbrido de la Computación Confidencial\n\nPara abordar este problema, SafelyShare ofrece un enfoque que se describe mejor como un modelo híbrido de computación confidencial. Para compensar los servicios que no operan dentro de enclaves seguros, esta metodología introduce la idea de 'ejecución testigo'. En este escenario, el usuario deposita su confianza en la garantía de los proveedores de su competencia y manejo seguro de los datos. Es una especie de acuerdo tácito entre el usuario y el proveedor de servicios remoto, lo que hace que la computación confidencial sea más factible en situaciones del mundo real.\n\nEste enfoque híbrido redefine el paradigma del intercambio seguro en un mundo que está en constante evolución. Con su base elástica, SafelyShare incorpora una profunda comprensión de los parámetros de seguridad cambiantes, haciendo que la computación confidencial sea adaptable y receptiva a las demandas y realidades cambiantes.\n\n## Conclusión: Revolucionando el Intercambio Seguro de Datos\n\nEn esencia, SafelyShare es el precursor líder en el viaje para hacer que la compartición de datos sensibles sea segura, eficiente y factible. Navegando alrededor de los obstáculos tradicionales, integra la informática confidencial híbrida en su marco de trabajo, logrando una combinación única de confianza y practicidad. El enfoque innovador de la integración de la informática testificada en el proceso difumina las líneas entre la confianza total y parcial, haciendo que la seguridad de los datos sea más alcanzable y ofreciendo un relato prometedor para el futuro de la compartición y la seguridad de los datos.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Shamim Naqvi","Darren W Pulsipher"],"link":"/episode-EDT177-es","image":"./episodes/edt-177/es/thumbnail.jpg","lang":"es","summary":"Durante este episodio, Darren y el CEO de SafeLishare, Shamim Naqvi, discuten cómo se puede emplear la computación confidencial para crear entornos de colaboración administrados para compartir datos en la nube."},{"id":80,"type":"Episode","title":"Red de confianza cero con OpenZiti","tags":["zerotrustnetworking","overlaynetworks","networksecurity","digitalage","securitychallenges","networkingconcepts","softwaredevelopers","softwareengineers","defensestrategy","databreaches","virtualnetworks","zerotrustprinciples","openzerotrust","securitymeasures","identitymanagement","secureconnectivity","networkdefense","iotdevices","datatransmission","smartnetworking","serviceintegration","vpnnetworks","wireguardnetworks","datasecurity","itsecurity","digitalinnovations","digitaltransformation","futureproofsecurity"],"body":"\r\n\r\n# Revelando la Dinámica de las Redes de Confianza Cero y las Redes Superpuestas\n\nA medida que avanza la era digital, la conversación sobre la seguridad de la red ocupa una posición de primera línea. En un panorama digital que evoluciona rápidamente, la red de confianza cero y las redes superpuestas son estrategias críticas para enfrentar los desafíos de seguridad actuales. Aquí, profundizamos en estos conceptos, cómo dan forma a nuestros sistemas digitales y proporcionamos una comprensión de sus posibles beneficios y aplicaciones.\n\n## Un vistazo más de cerca a la Red de Confianza Cero\n\nLa red de confianza cero es una mentalidad que considera la seguridad como una preocupación principal en el diseño y operación de sistemas digitales. Su aspecto crítico es la presunción de posibles amenazas desde todas las partes de la red, independientemente de cuán seguras puedan parecer. Este enfoque se aleja del concepto tradicional de fortaleza en seguridad y conduce a redes más robustas que no dependen únicamente de la protección de un único cortafuegos.\n\nEn primer lugar, la belleza de las redes de confianza cero reside en su capacidad para trabajar de manera efectiva y segura, presentando una ventaja para los desarrolladores y ingenieros de software. La seguridad se convierte en un habilitador en lugar de un obstáculo para el proceso de desarrollo de software. Con la red de confianza cero, los desarrolladores pueden centrarse en el desarrollo de funciones sin preocuparse por los puertos bloqueados o consultar con los equipos de red, un paso significativo hacia lanzamientos al mercado más rápidos.\n\nSin embargo, la red de confianza cero no elimina la necesidad de defensas de perímetro o firewalls. La estrategia de confianza cero asume una posible compromiso de la red; por lo tanto, exige un enfoque de defensa en capas en lugar de depender únicamente de la defensa de perímetro básica.\n\n## El Auge de las Redes Superpuestas\n\nEn medio de las crecientes amenazas a la seguridad y las violaciones de datos, las redes superpuestas están emergiendo como una herramienta invaluable. Estas redes virtuales definidas por software proporcionan una capa extra de seguridad en comparación con las redes subyacentes como los enrutadores o cortafuegos.\n\nLas redes superpuestas, como VPN y Wireguard, permiten una comunicación segura entre recursos incluso cuando la red subyacente ha sido comprometida. Ofrecen características atractivas, como la autorreorganización basada en condiciones, lo que les da características temporales. Estas redes también vienen con opciones para una comunicación segura en la aplicación o el sistema de datos, además, una opción de punto final sin cliente refuerza la conectividad del usuario, sin requerir la instalación de software en dispositivos individuales.\n\n\nLas redes superpuestas ofrecen flexibilidad en cuanto a la implementación. No es necesario reescribir el código de tu aplicación, ya que el código de la red superpuesta se puede integrar directamente en el código de la aplicación. Alternativamente, si deseas evitar alterar tu aplicación, se puede implementar un dispositivo virtual en su lugar. Esta comodidad, combinada con una mayor seguridad, configura las redes superpuestas como soluciones a prueba de futuro para la seguridad de la red.\n\n## El Poder de las Soluciones ZTN y OpenZiti\n\nLas ofertas de redes de Confianza Cero (ZTN), como Open Zero Trust (Open Ziti), proveen soluciones competentes en confianza cero y redes superpuestas. Entregan robustos principios de Confianza Cero en el campo de las soluciones de redes superpuestas.\n\nPor ejemplo, ZTN trae su sistema de identidad a la mesa, perfecto para dispositivos IoT de borde que no pueden acceder a los servicios de identidad típicos. Ofrece una transmisión de datos segura a través de un túnel mutuo y una tela de enrutamiento inteligente que determina el camino más eficiente desde el punto A hasta el punto B. Por otro lado, Open Ziti facilita múltiples casos de uso, gestionando las conexiones este-oeste y norte-sur de manera suave y segura. Se integra bien con mallas de servicio para proporcionar una seguridad de alto nivel.\n\nPor lo tanto, adoptar tales medidas de seguridad integrales se vuelve necesario a medida que entramos en la era digital. ZTN y OpenZiti presentan soluciones prácticas para aquellos que adoptan el modelo de Confianza Cero, con características ventajosas que van desde la gestión de identidad hasta la conectividad segura. Sin duda, estas innovaciones están marcando los estándares para la seguridad de la red.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Phillip Griffiths","Darren W Pulsipher"],"link":"/episode-EDT178-es","image":"./episodes/edt-178/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a Phillip Griffith, líder comunitario del proyecto de código abierto OpenZiti. Discuten la importancia de la red de Confianza Cero en las redes de TI modernas."},{"id":81,"type":"Episode","title":"Aprovechando la IA Generativa en la Universidad","tags":["generativeai","academicintegration","aiineducation","digitaltransformation","blogpost","aistudents","byuidaho","chatgpt","aiassistance","academiccheating","distinguishingguidelines","selfassessment","aiinselfassessment","qualitativeimprovement","ongoingdebates","aishortcomings","aiinaugmentation","futureofai","cheatingconcerns","shortcutculture","academicpolicies","aisuccess","aitoolkit","professionalhabitat"],"body":"\r\n\r\n## Navegando las Complejidades de la Integración Académica con IA Generativa\n\nEn el acelerado mundo definido por la rápida transformación digital, es cada vez más notable cómo las construcciones de IA se están convirtiendo en partes inextricables de la vida cotidiana. Un área fascinante donde su impacto se puede sentir es en el campo de la academia. Esta entrada de blog tiene la intención de adentrarse en el potencial de la IA generativa con experiencias de primera mano de una estudiante, Madeline Pulsipher, en BYU Idaho.\n\nLa aplicación de asistencia generativa de IA como ChatGPT en trabajos académicos revela emocionantes posibilidades. Cuando se utiliza de manera responsable, esta potente herramienta puede proporcionar una ventaja digital en la generación de ideas, la creación de esquemas de ensayos y la autoevaluación de tu trabajo frente a las rúbricas de calificación.\n\n## IA generativa - ¿Herramienta o Truco?\n\nLa cuestión de si el uso de la IA para tareas académicas se considera trampa presenta un aspecto intrigante. Madeline señala correctamente que usar la IA para facilitar un proceso o guiarlo no debería ser equiparado con hacer trampa. Hacer trampa implicaría componer un ensayo exclusivamente con la IA y tomar crédito por el trabajo sin ayuda.\n\nSin embargo, debemos crear directrices distintivas a medida que abordamos métodos tecnológicos más novedosos. Definir qué constituye un uso responsable frente a hacer trampa al incorporar la IA en la academia es una tarea esencial en la que las instituciones educativas deben trabajar y establecer de manera formal y enérgica.\n\n## La Eficiencia de la IA en la Autoevaluación\n\nUn intrigante uso de la IA ha detenido a todos en seco: autocalificarse su trabajo basado en la rúbrica de calificación establecida antes de la entrega. Los experimentos de Madeline con este enfoque dieron resultados fructíferos, consiguiendo notas A en todos sus ensayos asistidos por IA. Esto significa el nuevo potencial de la IA para ayudar no solo en tareas mecánicas, sino también en la mejora cualitativa del trabajo.\n\n## Perspectivas y Debates en Curso\n\nEl uso de la IA en contextos académicos ha sido debatido durante bastante tiempo. Aunque puede ser una herramienta valiosa para mejorar los resultados del aprendizaje y aumentar la productividad, es importante recordar que la IA no puede reemplazar el intelecto humano. Cada nueva tecnología tiene beneficios y desventajas, y la IA no es diferente.\n\nAunque la IA generativa puede producir contenido, carece del toque humano que es esencial en la comunicación. No puede reemplazar a los profesores humanos en la explicación de conceptos complejos, ya que necesita la habilidad para entender las sutilezas de la conversación humana. Por lo tanto, aunque la IA puede ser un recurso valioso en ciertas áreas, debe mantener el valor de la interacción y la experiencia humana.\n\n## Mejorando las Interacciones Sociales\n\nLa pandemia de COVID-19 ha interrumpido las vidas de muchos estudiantes que comienzan su primer año de universidad este año. La tendencia negativa en las citas entre adolescentes se ha agravado aún más durante la pandemia. Debido a la falta de interacciones sociales, la generación actual se pierde muchas experiencias críticas, como terminar una relación, el primer beso o pedir otra cita.\n\nMadeline buscó consejo de sus amigos sobre cómo rechazar a un chico que quería otra cita, pero recibió consejos contradictorios. Luego, recurrió a ChapGPT, un asistente imparcial y sin emociones impulsado por IA, para pedir consejo. Utilizó las sugerencias de ChapGPT como guía para desarrollar su enfoque.\n\nEsta capacidad de utilizar la IA generativa como asesor en lugar de como autoridad definitiva será crucial para que la próxima generación aproveche el poder de la IA en situaciones académicas y sociales.\n\n## El Futuro de la IA en la Academia\n\nDiversas preocupaciones continúan rondando la integración de la IA en la academia - preocupaciones sobre el engaño, la falta de políticas institucionales establecidas y la posibilidad de fomentar una cultura de atajos. Sin embargo, es innegable que la IA generativa es una herramienta a la que muchos estudiantes están recurriendo, y su potencial completo dentro de la academia aún necesita ser explorado a fondo.\n\nEvidentemente, la rigurosa línea entre el engaño y el uso adecuado necesita ser cuidadosamente trazada. Pero una vez que se ha establecido esta línea, el éxito de la IA como herramienta en los paradigmas académicos parece prometedor. Si se utiliza correctamente, puede convertirse en una parte sustancial de un kit de herramientas educativas, formando individuos competentes bien equipados para manejar la IA en sus hábitats profesionales.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Madeline Pulsipher","Darren W Pulsipher"],"link":"/episode-EDT179-es","image":"./episodes/edt-179/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a su hija que recientemente completó su primer semestre en la universidad acerca de su experiencia usando tecnología de IA generativa en sus estudios académicos. Ella describe los desafíos y éxitos asociados con la utilización de esta herramienta transformacional."},{"id":82,"type":"Episode","title":"Recopilación y preparación de datos","tags":["dataarchitecture","datacentric","data"],"body":"\r\n\r\n## ¡Necesitamos data! ¡Nuestra data está desordenada!\n\nLo primero en lo que hay que pensar en esta parte del proceso es en la tubería de datos. ¿Cómo identificamos qué datos sin procesar necesitamos y cómo los transmitimos a través de la tubería y los transformamos en conocimiento? Hay cinco pasos clave en la tubería: determinar el valor empresarial de los datos, ingestarlos, prepararlos, analizarlos y, finalmente, actuar en base a las perspectivas resultantes.\n\nVeamos la fabricación como ejemplo. Al determinar qué datos ofrecen valor empresarial, debes hacer tres preguntas fundamentales: ¿Cuál es la demanda de mi producto? ¿Cuál es la oferta actual? ¿Cuál es la pérdida de rendimiento? Estas preguntas parecen simples, pero luego debes pensar en cosas más complejas, como cómo cuantificar la demanda, las capacidades de fabricación, la oferta y la pérdida de rendimiento. ¿De dónde provienen los datos? ¿Cómo los absorbo? ¿Qué tan confiables y estables son estos datos? Hay muchas preguntas y variables, como el tiempo de entrega de los productos, la demanda proyectada y la pérdida de rendimiento desconocida, que pueden generar gran complejidad.\n\nEl pipeline simplifica cómo todos estos componentes se unen. Cada tipo de datos pasa por los pasos clave del pipeline, pero cada uno será diferente. Por ejemplo, la ingesta de un tipo de datos variará de la ingesta de otro. Sin embargo, la idea es reunir todos los datos para crear una imagen clara.\n\n## ¡Tenemos datos! ¿Qué hacemos con ellos?\n\nDependiendo del tipo de datos y las preguntas que estés tratando de responder, utilizarías diferentes técnicas analíticas. Por ejemplo, al responder cuántos widgets se deben fabricar, podrías analizar el suministro y la demanda histórica a través de análisis e inteligencia básica de negocios. Para determinar qué widgets tienen defectos visuales, un algoritmo que aprenda a identificar defectos en imágenes a través de aprendizaje profundo podría ser el enfoque más adecuado. No hay una técnica que resuelva todos los problemas; cada una es única para el problema y los propios datos.\n\nAdemás, es importante contar con expertos en el campo para ayudar a comprender los patrones que arroja los datos. El experto en el campo comprenderá los datos y de dónde provienen, y el científico de datos comprenderá la mejor manera de enfocar los algoritmos para obtener más información. Si, por ejemplo, se predice una disminución en el rendimiento del producto a través de un algoritmo de aprendizaje automático, los ingenieros que necesitan corregir el problema no sabrán necesariamente dónde buscar sin el contexto del problema. Una de las razones por las cuales las organizaciones no están obteniendo el retorno de inversión que deberían es porque no han construido sus modelos para que sean ejecutables o reflejen los comportamientos dentro de los sistemas que intentan predecir.\n\nCómo todo esto funciona en conjunto se reduce a las preguntas comerciales que estás haciendo y a tus desafíos. Por ejemplo, podrías tener una variedad de algoritmos que te indiquen cuántas piezas fabricar. Podrías tener un algoritmo de aprendizaje profundo que reconozca si una pieza tiene un defecto e incluso categorice los defectos. Pero eso no necesariamente ayuda si no sabes por qué ocurrió ese defecto. Por lo tanto, tienes que relacionar esa información con unos cuantos algoritmos más para obtener correlaciones que expliquen los defectos, y necesitas un plan de acción para corregir el problema.\n\n## Necesitamos crear conocimientos. ¿Cómo entrenamos nuestros datos?\n\n¿Cómo logramos esto? Básicamente, estás reuniendo todos los datos, preparándolos y vinculándolos para, por ejemplo, cuantificar el suministro y las predicciones de pérdida de rendimiento. Vas a necesitar prácticas de resolución de problemas y mejora continua con el tiempo para enfrentar condiciones cambiantes. Aquí es donde entra en juego la cultura de la organización. Resolver un problema una sola vez, sin un compromiso con la mejora continua, puede hacer que una organización pierda el verdadero valor de realizar análisis a largo plazo.\n\nEstamos presenciando hoy un cambio importante hacia organizaciones con una infraestructura centrada en los datos. Los datos ya no solo se encuentran en el centro de datos, sino también en la nube y en los dispositivos periféricos. Con el proceso empresarial en la cima, llevando a una mejora continua, comprensión empresarial y de datos, y hasta el despliegue, las organizaciones construidas sobre esta infraestructura pueden ver un mundo de diferencia.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT18-es","image":"./episodes/edt-18/es/thumbnail.png","lang":"es","summary":"Sarah Kalicin, científica de datos principal en Intel, y Darren Pulsipher, arquitecto principal de soluciones en el sector público en Intel, hablan sobre el proceso y los beneficios de la recopilación y preparación de datos al convertirse en una organización centrada en los datos. Este es el segundo paso en el camino para convertirse en una organización centrada en los datos."},{"id":83,"type":"Episode","title":"IA Generativa en Educación Superior (Revisitada)","tags":["embracingdigitaltransformation","darrenpulsipher","lauranewey","generativeai","aiineducation","educationtransformation","teachingexperience","aiinclassroom","moderneducationdynamics","criticalthinkingskills","digitaltransformation","resistanceagainstai","traditionalteaching","incoorporatingai","ineducationalcurriculums","innovationandeducation","ethicalusageofai","academicintegrity"],"body":"\r\n\r\n## Cómo la IA Generativa Mejora la Experiencia en el Aula\n\nLa IA generativa se está integrando rápidamente en los currículos educativos, impactando cómo los educadores abordan la enseñanza y mejorando fundamentalmente la experiencia de aprendizaje. Según Newey, esta tecnología tan debatida no es simplemente una forma de plagio, sino una herramienta brillante que aumenta y revitaliza las metodologías educativas. Animando a los estudiantes a utilizar la IA en tareas de pensamiento, ella enfatiza en fomentar y cultivar habilidades de pensamiento crítico en nuestra sociedad aceleradamente digitalizada.\n\nEn lugar de permanecer como participantes pasivos, ella aboga por que los estudiantes se conviertan en actores activos, analizando los resultados generados por la IA y considerando la calidad y sustancia de la información que aportan. El cambio subraya la importancia de la comprensión, investigación y análisis por encima de la mera generación de resultados.\n\n## Transición de la Enseñanza Tradicional\n\nEl enfoque progresista de Newey diverge dramáticamente de los métodos convencionales a los que se aferran la mayoría de los educadores, especialmente considerando la resistencia general hacia la integración de la Inteligencia Artificial Generativa en entornos educativos. Sin embargo, ella enfatiza la inevitabilidad y la necesidad de adoptar la digitalización para la ventaja general de los estudiantes.\n\nComparar esta transición con la resistencia inicial a utilizar internet como herramienta de enseñanza indica dónde nos encontramos hoy. La IA generativa, como cualquier otra tecnología en evolución, exige ser incorporada dentro del plan de estudios y requiere actualizaciones regulares para mantenerse relevante en este paisaje digital de ritmo acelerado.\n\n## Equilibrando la Innovación y la Ética\n\nCon la progresión e innovación, Newey también aborda las consideraciones éticas inherentes a este cambio. Comparte varios casos en los que los estudiantes, ignorando de manera inconsciente o sutil, presentaron ensayos generados por IA. Por lo tanto, ella enfatiza la necesidad de los educadores de equilibrar vigilante entre la aceptación tecnológica y el uso ético.\n\nElla cree firmemente que los estudiantes pueden utilizar la I.A. como una herramienta productiva, pero la responsabilidad también recae en los educadores para guiarlos hacia el mantenimiento de la integridad académica simultáneamente.\n\n## Conclusión: Abriendo el camino hacia un sistema educativo mejorado con la inteligencia artificial.\n\nLa incorporación de la Inteligencia Artificial Generativa en la educación, aunque ha encontrado resistencia, es una indicación profunda del cambiante panorama educativo. Como ilustra Newey, la integración exitosa de la IA en la educación puede mejorar significativamente las experiencias de aprendizaje y el desarrollo de habilidades esenciales, asegurando la preparación de nuestros estudiantes para un futuro moldeado por la transformación digital.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Laura Newey","Darren W Pulsipher"],"link":"/episode-EDT180-es","image":"./episodes/edt-180/es/thumbnail.jpg","lang":"es","summary":"En el episodio de esta semana de Abrazando la Transformación Digital, Darren Pulsipher entrevista a la oradora invitada Laura Newey sobre su fascinante trayecto a través del mundo emergente de la Inteligencia Artificial Generativa, particularmente en el sector de la educación. Cubriendo la transformación de su experiencia docente y enriqueciendo los resultados de aprendizaje de sus estudiantes a través de la IA, ella analizó extensivamente la adaptación a las dinámicas modernas de la educación."},{"id":84,"type":"Episode","title":"Cero Confianza en 5G","tags":["5gtechnology","zerotrustsecurity","embracingdigitaltransformationpodcast","darrenpulsipher","lelandbrown","yazkrdzalic","kenurquhart","trentonsystems","zscaler","advancedcommunications","operationaltechnology","informationtechnology","lonelyislandapproach","telecominfrastructure","advancedsecuritymodels","commercial5gusage","military5gusage","technicalchallenges","cybersecurity","5gsecurity","integration","solutionarchitecture"],"body":"\r\n\r\n## El Amplio Paisaje 5G y el Enfoque de la Isla Solitaria\n\nEl mundo de la tecnología 5G está evolucionando rápidamente y, como resultado, se están llevando a cabo muchas discusiones perspicaces sobre la fusión de la Tecnología Operativa (OT) y la Tecnología de la Información (IT). Yazz Krdzalic describe el concepto del \"enfoque de la isla solitaria\". Este enfoque se refiere a la tendencia de diferentes entidades a concentrarse demasiado en resolver sus problemas individuales, lo que a menudo ha llevado a la paralización del crecimiento en hardware personalizado en la infraestructura de telecomunicaciones.\n\nLa necesidad de alejarse de este enfoque individualista y restablecer un marco arquitectónico colectivo que pueda escalar y flexibilizar con diferentes casos de uso se está volviendo cada vez más evidente. Con la aparición de la tecnología 5G, existe una necesidad de un enfoque colaborativo que pueda acomodar los diversos requisitos de diferentes entidades. El enfoque colectivo ayudará a garantizar que la infraestructura sea flexible y escalable, facilitando que las entidades integren sus tecnologías y aplicaciones en la red.\n\nLas discusiones sobre la fusión de OT e IT también están cobrando impulso, y está quedando claro que la colaboración entre estos dos dominios es esencial para el éxito de la tecnología 5G. A medida que la tecnología continúa evolucionando, se espera que haya más debates y discusiones sobre cómo aprovechar las oportunidades presentadas por el 5G, al tiempo que se abordan los desafíos planteados por la tecnología emergente. En general, el futuro de la tecnología 5G se ve brillante, y la colaboración entre diferentes entidades jugará un papel crítico en su éxito.\n\n## Transición a la Seguridad de Confianza Cero\n\nA medida que la tecnología continúa evolucionando, las preocupaciones de seguridad se han convertido en un problema creciente tanto para individuos como para organizaciones. Para abordar estas preocupaciones y garantizar un entorno seguro, se necesita un marco arquitectónico colectivo. Este marco incluye la implementación de modelos de seguridad avanzados, como la Seguridad de Confianza Cero. Sin embargo, la transición a estos modelos no siempre es fácil. Requiere dejar de lado los métodos antiguos de operación y asegurarse de que todos los módulos tecnológicos estén sincronizados y funcionando correctamente. En el pasado, eran los clientes quienes tenían la responsabilidad de integrar todas las piezas. Afortunadamente, con la adopción de un enfoque más evolucionado, la carga de la integración se ha reducido considerablemente para los clientes, lo que facilita mucho más la implementación de la Seguridad de Confianza Cero y otros modelos de seguridad avanzados.\n\n## Encontrando el Terreno Común en el Uso del 5G\n\nEl desarrollo de la tecnología 5G ha sido un cambio de juego tanto en los sectores comercial como militar. Sin embargo, hay requisitos específicos que diferencian el uso comercial y militar de la 5G. Las implementaciones comerciales de redes privadas 5G son en gran medida estáticas, mientras que las implementaciones militares necesitan ser móviles.\n\nLeland Brown, un destacado experto en el campo, ha discutido las complejidades de encontrar una arquitectura común que pueda satisfacer ambas necesidades. El desafío era crear una solución final que cumpliera de manera elegante con estos requisitos. Era importante garantizar que la solución fuera eficiente y efectiva tanto para casos de uso comercial como militar.\n\nEl desarrollo de tales soluciones es crucial para garantizar que la tecnología 5G se utilice en todo su potencial y pueda satisfacer las diversas necesidades de diferentes industrias.\n\n## Concluyendo\n\nEl mundo de la tecnología está en constante evolución y mejora, y la aparición de la tecnología 5G y la seguridad de Confianza Cero es un testimonio de ello. Sin embargo, implementar estos avances puede ser desafiante debido a obstáculos técnicos y culturales. Afortunadamente, expertos como Leland Brown, Ken Urquhart y Yaz Krdzalic están trabajando para agilizar la integración de la tecnología 5G y la seguridad de Confianza Cero, facilitando un poco el camino hacia un futuro tecnológico más seguro y eficiente para todos. Sus perspectivas y experiencia están arrojando luz sobre el continuo viaje de evolución y mejora en el mundo de la tecnología.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Leland Brown","Yazz Krdzalic","Ken Urquhart","Darren W Pulsipher"],"link":"/episode-EDT181-es","image":"./episodes/edt-181/es/thumbnail.jpg","lang":"es","summary":"En medio de la creciente adopción de las tecnologías 5G a nivel mundial, los expertos en el episodio reciente del podcast Abrazando la Transformación Digital se adentraron en el tema integral de la Confianza Cero en la seguridad 5G. El presentador Darren Pulsipher dio la bienvenida al experto en comunicaciones avanzadas 5G Leland Brown, VP de Marketing en Trenton Systems Yazz Krdzalic, y a Ken Urquhart, un físico convertido en profesional de la ciberseguridad de Zscaler, para discutir la integración y avance de la tecnología 5G, junto con sus desafíos y avances."},{"id":85,"type":"Episode","title":"Aseguramiento de Datos de Confianza Cero","tags":["digitaltransformation","dataintegrity","zerotrust","cybersecurity","strategies","walacor","encryption","loganalysis","geographicaldistribution","externalhackers","organizationalthreats","datamanagement","dataaudits","immutableauditlog","systemchange","legalcompliance","reducerisk","dataintelligence","multilayeredsecuirty","dataprotectionsolution","keyvulnerabilities","improveddataprotection","futuredigitalbusinesses","revolutionizedigitallandscape"],"body":"\r\n\r\n## Desenmascarando Suposiciones Sobre la Seguridad de Datos\n\nEn el pasado, las personas tenían una confianza implícita de que sus datos estaban seguros y su privacidad estaba protegida. Sin embargo, esta confianza a menudo se basa en un modelo anticuado que ya no se alinea con el panorama tecnológico actual. El creciente número de violaciones de datos y ataques cibernéticos ha evidenciado que la seguridad de los datos es más crítica que nunca y las precauciones que se consideraban adecuadas en el pasado pueden ya no ser suficientes.\n\nHoy en día, los datos son vulnerables a amenazas no solo de hackers externos sino también desde dentro de las organizaciones. Es esencial comprender que una violación de datos puede tener implicaciones significativas, que van desde pérdidas financieras hasta daños a la reputación. Por lo tanto, es crucial implementar un enfoque de cero confianza para la gestión de datos, lo que significa que cada solicitud de acceso a los datos debe ser verificada antes de que se conceda el acceso. También son necesarias auditorías de datos confiables para garantizar que la entrada de datos coincide con la salida y que no hay acceso no autorizado a información sensible.\n\n## Implementando una Nueva Era de Seguridad de Datos con Walacor\n\nWalacor ofrece una solución única para mejorar nuestra comprensión de la seguridad de los datos. Ofrecen un registro de auditoría automático y a prueba de fallos que es inmutable, lo que significa que una vez que se introduce un dato, nunca puede ser alterado o eliminado sin ser detectado. Esta característica hace que sea increíblemente fácil rastrear cada cambio realizado en el sistema, lo cual es crítico para mantener un entorno seguro.\n\nAl proporcionar transparencia y trazabilidad, la solución de Walacor ayuda a las organizaciones a cumplir con los requisitos legales y a mitigar los riesgos. Por ejemplo, en caso de una disputa legal, un registro de auditoría inmutable puede servir como una fuente de evidencia confiable, ya que no puede ser adulterado. Además, en caso de una violación de datos, un registro de auditoría inmutable puede ayudar a identificar la fuente de la violación y el alcance del daño causado.\n\nEn general, el enfoque innovador de Walacor a la seguridad de datos, con su registro de auditoría 100% inmutable, ofrece una solución prometedora para las organizaciones que buscan mejorar su postura de ciberseguridad.\n\n## Dando Forma al Futuro de la Inteligencia de Datos\n\nEl creciente riesgo de brechas de datos significa que necesitamos alejarnos del uso de múltiples capas de seguridad de datos a una solución de protección de datos más integrada. Este tipo de solución sienta las bases para un entorno de Confianza Cero, que reduce significativamente el riesgo de amenazas y vulnerabilidades cibernéticas. Al adoptar este enfoque, podemos optimizar nuestros métodos de protección de datos y garantizar una mejor integridad de los datos.\n\nEl desarrollo de la inteligencia de datos en forma de integridad y seguridad de los datos abre nuevas posibilidades para los negocios digitales. Los métodos mejorados de protección de datos, una mejor integridad de los datos y una reducción en las posibles amenazas cibernéticas son solo algunos de los beneficios que están destinados a transformar el panorama digital. Entre estos, el tema de conversación es el enfoque único de Walacor hacia la integridad de los datos y la confianza cero, lo que marca un hito significativo en cómo abordamos la seguridad de los datos ahora y en el futuro.\n\nConsulta más información en (https://walacor.com)[https://walacor.com]\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Walter Hancock","Darren W Pulsipher"],"link":"/episode-EDT182-es","image":"./episodes/edt-182/es/thumbnail.png","lang":"es","summary":"La necesidad de estrategias sólidas de seguridad de datos ha crecido exponencialmente en la era digital, convirtiéndose en una prioridad máxima para las empresas alrededor del mundo. El experto en ciberseguridad y CTO de Walacor, Walter Hancock, ofrece una visión perspicaz sobre la importancia de la integridad de los datos y un enfoque de cero confianza en los regímenes actuales de ciberseguridad."},{"id":86,"type":"Episode","title":"Gestión de Datos en Ciencia de Materiales e Industrias de Manufactura","tags":["digitaltransformation","materialscience","manufacturingindustry","dataanalytics","machinelearning","artificialintelligence","productdevelopment","structuredmethodologies","projectmanagement","complexdata","unstructureddata","datascience","r&dprocess","newproductdevelopment","datamanagement","technologyinnovation","physicitaldigitalknowledgegap","embracingdigital.org","embracingdigitaltransformationpodcast"],"body":"\r\n\r\n## Cerrando la Brecha entre lo Físico y lo Digital en I&D\n\nMaterials Zone se enfoca en el aspecto de nicho pero significativo de la ciencia de los materiales, específicamente en la industria manufacturera. Dado el importante rol de los materiales en el desarrollo de productos, la gestión eficaz de los datos se vuelve crucial. Análogo a una receta de cocina, la ciencia de los materiales implica una integración matizada de ingredientes (materiales) que pasan por un proceso para producir el producto final.\n\nSin embargo, históricamente esta área ha sido ad hoc, basándose en ensayo, error e intuición. En consecuencia, el conocimiento adquirido durante este proceso a menudo se pierde debido a la documentación insuficiente o la rotación de empleados. En nuestro mundo moderno e interconectado, donde los procesos de desarrollo de productos a menudo abarcan múltiples ubicaciones, incluso países, es fundamental establecer metodologías estructuradas para prevenir la pérdida de conocimiento.\n\nUna de las técnicas resaltadas por Yudilevich es abordar el \"factor del camión\", que sugiere que si la única persona que sabe cómo hacer una tarea particular fuera golpeada por un camión, podría descarrilar potencialmente todo el proyecto. Por lo tanto, tener al menos una persona más aparte del individuo principal que pueda realizar la tarea podría disminuir la vulnerabilidad del equipo.\n\n## Capturando Complejidades de Datos de Ciencia de Materiales\n\nEl campo de la ciencia de materiales genera datos complejos, a menudo no estructurados y difíciles de capturar utilizando tablas y bases de datos tradicionales de manera suficiente. Para visualizar esto, considera los datos como un gráfico donde los materiales en bruto se convierten en productos finales. Las innumerables interacciones entre los diversos componentes dan lugar a múltiples dimensiones únicas dentro de los datos.\n\nAdemás, existe una traducción fluida dentro del ámbito de la fabricación: desde la investigación exploratoria hasta la fase de producción, la cual exige estabilización y consistencia. Recopilar datos de estas fases en un repositorio unificado puede mejorar el proceso de I+D al centralizar la información, ayudar al aprendizaje entre fases y acelerar el desarrollo de nuevos productos.\n\n## Integrando la Ciencia de Datos en la Fabricación\n\nAunque la ciencia de datos se ha permeado en muchas industrias, las empresas que se enfocan principalmente en el desarrollo de productos en el mundo físico a menudo encuentran que establecer departamentos de datos dedicados o integrar herramientas analíticas es ineficiente y costoso. Aquí es donde entra en juego la solución de Materials Zone, haciendo que la ciencia de datos, el aprendizaje automático y las herramientas estadísticas sean accesibles para las empresas que no están familiarizadas con estas áreas.\n\nOfrecen herramientas innovadoras acompañadas de seminarios web y sesiones de capacitación para una fácil adopción, reduciendo así las barreras para integrar la ciencia de datos en las prácticas de fabricación. Sorprendentemente, incluso las empresas Fortune 500 que carecen de las habilidades digitales necesarias pueden beneficiarse significativamente de dichas soluciones.\n\n## A medida que avanzamos\n\nA medida que el proceso de desarrollo de productos se vuelve más complejo y global, el carácter crítico de la gestión sistemática de datos combinada con la innovación tecnológica está cobrando protagonismo. Empresas como Materials Zone están allanando el camino, guiando a las empresas para cerrar la brecha de conocimiento físico-digital, reforzar sus prácticas de fabricación y garantizar el éxito futuro.\n\nPara obtener más información, visita https://materials.zone.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ori Yudilevich"],"link":"/episode-EDT183-es","image":"./episodes/edt-183/es/thumbnail.png","lang":"es","summary":"En un panorama tecnológico en constante evolución, líderes de diversos sectores aplican análisis de datos, aprendizaje automático e inteligencia artificial a sus operaciones. Hoy, observamos más de cerca a una empresa que impulsa la transformación digital en la industria manufacturera: Ori Yudilevich, el CTO de Materials Zone."},{"id":87,"type":"Episode","title":"Utilizar datos como un activo estratégico","tags":["dataarchitecture","data","datastrategy","people","process"],"body":"\r\n\r\ntranslate: Al igual que algunas personas acumulan cosas en sus hogares, temerosas de deshacerse de cualquier cosa, las organizaciones pueden acumular datos. Por ejemplo, mi carpeta de correo electrónico tiene 8 gigabytes. Sé que\n\nNo necesita ser tan grande, pero guardo cosas por si las necesito. Ahora multiplica eso por el número de empleados. En Intel, tenemos cien mil empleados. Imagina cuántos datos estamos almacenando solo en correos electrónicos cuando hacemos respaldos. Agrega bases de datos estructuradas y no estructuradas, presentaciones, hojas de cálculo, etc., y queda claro que simplemente almacenar todo no es una gran estrategia.\n\n## Estadísticas de datos\n\nVeamos las estadísticas que muestran por qué esta es una mala estrategia. Aproximadamente el 80 por ciento del tiempo de los científicos de datos se gasta en limpiar los datos antes de poder utilizarlos. Menos del 50% de los datos estructurados se utilizan en absoluto, y menos del 1% de los datos no estructurados pueden ser analizados. Así que todos estos datos se están acumulando, pero las organizaciones no los están utilizando en su mayoría. Otro problema es quién tiene acceso a todos estos datos almacenados. Es alarmante que el 70% de los empleados tengan acceso a datos a los que probablemente no deberían tener acceso. Al igual que los montones desorganizados de un acumulador crean riesgos de incendio en el hogar, los montones desorganizados de datos crean riesgos de seguridad en una organización.\n\n## Explosión de datos\n\n¿Qué podemos hacer acerca de este problema? Primero, necesitamos comprender por qué hay una explosión de datos tan grande. Con el Internet de las cosas, todo está conectado y tenemos procesamiento de datos ocurriendo en muchos lugares. La cantidad de datos que se está generando es increíble. Esto se agrava por los problemas regulatorios; es difícil saber qué podemos eliminar y qué no podemos. Llenamos nuestro almacenamiento y luego compramos más. Básicamente, la tecnología nos permite acumular datos. Necesitamos analizar qué podemos hacer de manera diferente.\n\n## Por qué acumulamos\n\nLos expertos dicen que las personas acumulan porque creen que un artículo será útil o valioso en el futuro, tiene valor sentimental, es único e irreemplazable, o es una ganga demasiado buena como para desecharla. Estas mismas ideas se aplican al acaparamiento de datos. Por ejemplo, ¿por qué tengo una de las primeras presentaciones que di? Está almacenada en un disco y también en la nube. La miro quizás una vez al año, pero no tengo ninguna razón real más allá del valor sentimental para conservarla. Una organización está compuesta por individuos con estos comportamientos, y en todos los niveles de acaparamiento de datos hay miedo a deshacerse de ellos.\n\n## Volviéndose centrado en los datos\n\n¿Cómo es convertirse en una empresa centrada en los datos en lugar de una empresa de almacenamiento? Aquí hay un proceso de cuatro pasos para convertirse en una organización centrada en los datos.\n\n## Fundamento Organizacional\n\nEl primer paso es crear una base organizativa centrada en los datos. Hay cuatro jugadores clave con roles distintivos.\n\nJefe de Datos: El trabajo del jefe de datos es establecer estrategia y gobernanza sobre la gestión de datos y la generación de información empresarial valiosa. Este rol es diferente al de un director de información, quien se enfoca en la infraestructura en lugar de la información misma. Este es un trabajo difícil, ya que el jefe de datos está involucrado en un cambio cultural. Tratan de evitar que las personas acumulen datos y, en cambio, los utilicen para crear un valor empresarial real.\n\nCientífico de datos: Los científicos de datos desarrollan modelos y esquemas encontrando patrones en los datos y utilizando análisis predictivos. Sin embargo, los esfuerzos de los científicos de datos pueden convertirse simplemente en un experimento científico de una sola vez, a menos que la información sea operacionalizada.\n\nIngeniero de datos: Aquí es donde intervienen los ingenieros de datos. Ellos gestionan los flujos de datos y operativizan el análisis. A medida que llegan nuevos datos, se generan nuevas ideas sin tener que comenzar desde cero cada vez.\n\nData Steward: El responsable de datos gestiona la gobernanza y el acceso a los activos de datos, asegurándose de que las personas adecuadas tengan el acceso correcto en el momento adecuado.\n\nCon una organización que incluye estos cuatro roles, la siguiente gran pregunta es si centralizar o distribuir las operaciones. Por ejemplo, tal vez la gestión matricial distribuida ya esté funcionando en tu organización grande, pero una organización más pequeña podría necesitar más rigurosidad y beneficiarse de una estructura más centralizada.\n\n## Recopilación y preparación de datos\n\nUn artículo de la Harvard Business Review utiliza de manera efectiva una analogía deportiva para describir dos estrategias para catalogar datos: defensa y ofensiva. En la defensa, el objetivo es proteger los datos. En la ofensiva, el objetivo es avanzar rápidamente para anotar.\n\n## Defensa y Ofensiva de Datos.\n\nCon una estrategia defensiva, la organización se enfoca principalmente en seguridad de datos, gobernanza y cumplimiento. Proteger los datos es clave. La actividad principal de datos se centrará en extracción, estandarización, administración de almacenamiento y administración de accesos. Típicamente, esta estrategia utilizará una organización más centralizada y empleará una única fuente de verdad.\n\nCon una estrategia ofensiva, la organización está principalmente enfocada en moverse rápidamente para mejorar su posición competitiva y ser lo más rentable posible. Las actividades de datos se centrarán en la extracción, modelado, visualización, transformación y enriquecimiento.\n\nEsta estrategia requerirá más flexibilidad, lo que significa una organización más distribuida con múltiples versiones de la verdad.\n\nEntender cómo utilizar los datos basados en la estrategia es importante. A menudo, las organizaciones se quedan en medio y puede ser confuso. Aunque todas las organizaciones necesitan ser capaces de defender y atacar, deben elegir una estrategia en lugar de intentar hacer ambas cosas, al igual que los jugadores de fútbol de nivel profesional que no juegan en ambos lados del campo.\n\n## Análisis de Perspicacia\n\nHay una curva de madurez organizativa para el análisis y la creación de información a partir de sus datos. La clave está en comprender en qué punto se encuentra su organización actualmente y cuáles son los próximos pasos para avanzar en la curva.\n\nEn la etapa de análisis descriptivo, solo estás tratando de descubrir qué está sucediendo. En el paso de diagnóstico, estás descubriendo por qué sucedió algo. En el paso predictivo, puedes predecir qué sucederá en el futuro basándote en datos históricos. Aquí es donde muchas organizaciones se esfuerzan por estar, pero primero se deben lograr los dos primeros pasos. Por encima del predictivo está el prescriptivo, donde puedes entender por qué sucederá algo y guiar a la organización de acuerdo con las expectativas. En la cima de los pasos está la visión analítica, o visión de futuro, donde estás haciendo que las cosas sucedan, incluso progresando más allá de la prescripción.\n\nUna razón por la cual es importante entender dónde se encuentra actualmente tu organización es porque existen herramientas específicas para cada fase. Por ejemplo, no deseas quedar atrapado con un proyecto de IA que utiliza algoritmos prescriptivos, o incluso predictivos, cuando tu organización aún se encuentra en la etapa descriptiva.\n\n## Operativizarlo\n\nPara lograr el objetivo de operacionalización, o hacer un proceso repetible, existen tres elementos clave: una infraestructura centrada en datos, canalizaciones de datos y flujo empresarial.\n\nLa infraestructura centrada en los datos te permite saber dónde se encuentra toda tu información y qué contiene a través de varias herramientas, como un administrador de metadatos como Elastic Search o catálogos y repositorios de metadatos. Los ductos de datos cuentan con excelentes herramientas para habilitar el proceso desde la ingestión hasta el análisis y la acción. Una estrategia defensiva u ofensiva determinará qué herramientas utilizarás en tu ducto de datos. El último elemento, el flujo de negocios, es donde la comprensión empresarial de tus datos y procesos permitirá implementar un proceso de mejora continua para garantizar ideas repetibles y valiosas.\n\n## Llamado a la acción.\n\nPrimero, desarrolla una estrategia de datos. Organízate y averigua dónde se encuentran todos tus datos y catálogalos. Decide si quieres adoptar una estrategia defensiva u ofensiva, y luego lleva a cabo tus pasos de análisis de manera gradual, utilizando las herramientas adecuadas. Lo más importante, opera tus conocimientos para obtener el mejor valor comercial.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT19-es","image":"./episodes/edt-19/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren Pulsipher, Arquitecto Jefe de Soluciones de Intel, Sector Público, explora cómo las organizaciones pueden pasar de simplemente acumular datos a utilizarlos como un activo estratégico."},{"id":88,"type":"Episode","title":"Siga el Bit","tags":["iot","dod","edge","technology","data","cybersecurity"],"body":"\r\n\r\n## El Internet de las cosas es el comienzo.\n\nLa implementación generalizada del Internet de las cosas (IoT) ha estado tomando más tiempo del esperado por la industria. Muchos culpan a los retrasos en la adopción y despliegue del 5G en todo el mundo, pero hay otro problema que ha frenado la implementación de sistemas robustos de IoT: la gestión de la complejidad. A medida que los datos se mueven a través del sistema desde el borde, borde agregado, red, centro de datos y nube, asegurar los datos es una preocupación importante, ya que la superficie de ataque aumenta al moverse fuera del centro de datos tradicional. Hay soluciones específicas que ayudan a mejorar estos problemas, pero aún no hay una arquitectura de solución completa que resuelva todos los problemas de este centro de datos sin paredes.\n\n## Complejidad del entorno del Departamento de Defensa\n\nEl IoT ya es complejo y el Departamento de Defensa aumenta la complejidad debido a los tipos de productores y consumidores de datos. En el lado del productor, se adhieren sensores a satélites, aviones, barcos y vehículos; incluso los combatientes en sí mismos son básicamente centros de datos móviles. La cantidad de datos que estos dispositivos periféricos producen puede abrumar una red. El número de dispositivos heterogéneos puede hacer que gestionarlos parezca casi imposible, especialmente cuando los dispositivos se comunican con diferentes protocolos y tienen diferentes niveles de clasificación. En otras palabras, recopilar los datos no es el problema; el problema es hacer que lleguen a manos de aquellos que toman decisiones en un formato útil y rápido.\n\nAdemás, la conectividad puede ser problemática con los dispositivos periféricos. Entornos hostiles donde la conectividad de red a un centro de datos es inexistente o irregular pueden retrasar los datos. Esto significa que una solución en este espacio debe funcionar en modos de operación conectados y desconectados. La velocidad de entrega es un factor clave de éxito; pueden estar en juego vidas si las decisiones basadas en los datos se retrasan.\n\n## Marco común para aplicaciones, datos y seguridad.\n\nObviamente, necesitamos soluciones para estos problemas. De suma importancia es un marco común para gestionar la complejidad de estas nuevas arquitecturas de TI que están fuera de las paredes tradicionales del centro de datos. El marco debe abordar la gestión de aplicaciones, datos y seguridad. Necesitamos poder implementar aplicaciones portátiles y reutilizables en cualquier lugar del sistema, desde el borde hasta la nube: la doctrina \"escribe una vez, ejecuta en cualquier lugar\". Esto nos da la capacidad de desarrollar, probar e implementar aplicaciones rápidamente sin tener que configurar todas las permutaciones de configuraciones de hardware en el ecosistema. El uso de herramientas en el ecosistema de contenedores debería ayudar con esto. Las herramientas basadas en Kubernetes (K8s) son una buena opción, ya que se han convertido en el estándar de facto en la comunidad de DevOps.\n\nAdministrar aplicaciones de forma aislada, sin embargo, no es suficiente. Todas las aplicaciones necesitan datos en cierta medida, por lo que comprender dónde se encuentran los datos, a dónde van y cómo se clasifican es clave para soluciones exitosas. Necesitamos un entorno operativo común para administrar y gobernar las diferentes clases de datos, como dominios, límites de seguridad, gobernanza, gestión del ciclo de vida de los datos y ubicación de los datos. Un entorno operativo común aumenta la flexibilidad y velocidad de implementación de aplicaciones.\n\nUn marco común de seguridad también es necesario. La pregunta crítica es cómo asegurar tus datos en todas sus formas y seguir compartiéndolos. Existen soluciones de hardware y software actuales y una progresión continua en esta área. Soluciones de seguridad básicas, como la encriptación, deberían ser fundamentales. Por supuesto, esto requiere el motor subyacente adecuado para el almacenamiento y la capacidad. Otra preocupación es la entrada de datos erróneos o maliciosos en el sistema. Establecer una raíz de confianza como base también es necesaria en este vasto ecosistema.\n\n## Procesamiento en el borde, centro de datos y nube.\n\n¿Dónde interviene Intel en este entorno? Podemos ayudar a proporcionar la infraestructura subyacente que respalda estos sistemas en rendimiento y eficiencia energética. Ya sea que esté procesando información de sensores en el borde en un entorno de baja potencia (piense en Atom y diseños de ASIC personalizados), o esté realizando entrenamiento de inteligencia artificial o inferencia en su centro de datos (Xeon y cómputo neuromórfico), Intel tiene un procesador que puede ayudar a convertir los datos en bruto en información valiosa y accionable, el componente clave en este entorno complejo orientado a la misión.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT2-es","image":"./episodes/edt-2/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren entrevista a Greg Clifton, Director del Departamento de Defensa (DOD) e Inteligencia de Intel Corp. Discuten los desafíos de la gestión de datos en un sistema complejo que abarca múltiples nubes, centros de datos empresariales, centros de datos regionales y entornos tácticos. Escucha a Darren y Greg seguir un poco de datos desde su recolección y recorrido a través de este ecosistema hasta la producción de información útil para analistas y combatientes. Escucha a Darren y Greg discutir algunos de los obstáculos en este entorno grande y circular, y soluciones para ayudar a obtener información útil para los analistas y devolverla a los combatientes."},{"id":89,"type":"Episode","title":"Destruyendo la complejidad de las capas de almacenamiento","tags":["data","optane","technology","storage","vastdata"],"body":"\r\n\r\n## Treinta años de complejidad en el almacenamiento\n\nEl almacenamiento puede ser un concepto antiguo, pero VAST Data ha alcanzado el estatus de unicornio en poco más de un año de envíos de ingresos. VAST ha reemplazado la antigua arquitectura de sistemas de almacenamiento multinivel con un sistema único y monolítico que es rápido y rentable. Las soluciones de VAST también eliminan el problema de los sistemas de almacenamiento grandes y desordenados que consisten en diferentes sistemas de archivos y arquitecturas diferentes.\n\n## Una fundación para una nueva arquitectura.\n\nEl fundador de VAST estaba considerando el almacenamiento desde una perspectiva de una hoja en blanco. Descubrió que los clientes no necesariamente necesitaban más rendimiento de Flash, sino que necesitaban soporte para almacenamiento de archivos y objetos a un costo más bajo. Aprovechó tres tecnologías que no existían antes de 2018. La primera es NVMe sobre Fabrics, que se utiliza como una especie de almacenamiento de escala hiperconvergente para conectar todo con baja latencia. A continuación, está QLC Flash porque es económico y no tiene partes móviles como los discos duros. Tercero está el Optane 3D XPoint de Intel. Optane tiene paridad de lectura y escritura y tiene una resistencia alta a un precio razonable. Al combinar estas tres tecnologías, VAST tiene la capacidad de ofrecer un rendimiento totalmente Flash, pero a un precio comparable al de un disco duro. Elimina la necesidad de otros niveles que las organizaciones compran porque son baratos.\n\n## Presentando Vast Data Universal Storage\n\nEl sistema VAST es la primera arquitectura desagregada y compartida de todo. Esto significa que la lógica está separada del estado del sistema de archivos. En cambio, el estado del sistema de archivos se encuentra en cajas de datos que contienen QLC Flash y Optane. El Optane se utiliza como un gran depósito de metadatos. Es multiusos, al igual que el sistema, que se denomina almacenamiento universal. Con estas cajas, no existe un único punto de falla, por lo que la capacidad es teóricamente ilimitada. (VAST ha probado el sistema en aproximadamente 50 petabytes en un solo sistema de archivos). Puedes aumentar la capacidad simplemente agregando servidores x86 económicos a los clústeres. Debido a que es un sistema de archivos paralelo, cualquier usuario puede acceder a cualquier dato de cualquiera de los servidores como si estuviera conectado directamente, por lo que puedes seguir escalando.\n\nTambién puedes mejorar el rendimiento de forma independiente de la capacidad. Lo único que podría inhibir el rendimiento del flash es la CPU, así que al tener la capacidad de escalar sin problemas el número de CPUs en el clúster, puedes aumentar el rendimiento.\n\nUno de los problemas que se resuelve con esta estructura es la latencia. Muchas organizaciones necesitan una baja latencia para todos sus datos. Debido a que cada uno de estos servidores sin estado tiene acceso a todo, se tiene un rápido acceso a todos los datos.\n\n## DASE Arquitectura: Agrupaciones de Servidores\n\nOtro gran beneficio es que es fácil ajustar el almacenamiento de una organización. La naturaleza componible de los servidores sin estado y la ausencia de comunicación entre ellos te brinda la capacidad de construir un clúster que se adapte mejor a tus necesidades. Por ejemplo, puedes segmentar tus cajas de control según las diferentes cargas de trabajo, pero todas ellas pueden acceder a los mismos datos.\n\nAdemás, el sistema funciona bien con una organización que necesita distintas capas de clasificación para acceder a los datos. Tienes la capacidad de segregar a qué tienen acceso los usuarios mediante la creación de múltiples zonas de acceso con direcciones IP virtuales. Uno de los desafíos con NFS es que se transmite básicamente a todo. Si limitas las transmisiones a un subconjunto de direcciones IP, te da la capacidad de dividir esas arquitecturas diferentes en sistemas discretos.\n\n## Aplicaciones de Puentes de Almacenamiento Universales Eras\n\nEsta no es solo una solución destinada a clústeres de HPC; no es prohibitiva en cuanto a costos. Muchas compañías están utilizando VAST primero para respaldo y establecer confianza. Por ejemplo, el Instituto Nacional del Cáncer tiene un archivo de biblioteca de cintas y querían poder acceder a la información más rápidamente. Evaluaron diferentes plataformas y VAST ofreció un precio más bajo y todo en Flash, lo cual es más rápido que su sistema NAS de producción. Por lo tanto, esta solución tiene un buen precio y es útil para la compartición de archivos en general y una variedad de cargas de trabajo, como IA, análisis de registros, Splunk, entre otros, no solo para HPC. Las soluciones de VAST son simples de gestionar y verdaderamente universales.\n\nVAST es una empresa joven, pero tiene múltiples instalaciones en agencias gubernamentales como el Instituto Nacional de Salud y los tres laboratorios del Departamento de Energía, donde se requiere un rendimiento máximo para estas supercomputadoras. Este es un sistema potente en algunos de los entornos de HPC más grandes del mundo, que respalda aplicaciones críticas para la misión.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Randy Hayes"],"link":"/episode-EDT20-es","image":"./episodes/edt-20/es/thumbnail.jpg","lang":"es","summary":"Randy Hayes de VAST Data y Darren Pulsipher, Arquitecto Jefe de Soluciones del Sector Público de Intel, discuten la arquitectura de almacenamiento innovadora de VAST Data que elimina la necesidad de niveles utilizando NVMe sobre Fabrics, QLC Flash y 3D XPoint Optane."},{"id":90,"type":"Episode","title":"Controlador de memoria definida por software de gran capacidad","tags":["bigmemory","edge","compute","technology","memverge","optane","pmem","data"],"body":"\r\n\r\nLa tecnología de memoria persistente 3D XPoint Optane de Intel revolucionó el juego para MemVerge, permitiéndoles desarrollar una memoria definida por software, la cual describen como la expresión del poder de Optane PMEM.\n\n## La mejor manera de usar PMEM\n\nMemVerge desarrolló el software Big Memory para satisfacer una creciente demanda de aplicaciones y empresas para procesar datos a volúmenes y velocidades cada vez más altos. Se necesitan más aplicaciones en tiempo real que ofrezcan información y acciones instantáneas a partir de los datos. Esto requiere una nueva infraestructura centrada en la memoria para cumplir con los requisitos de latencia.\n\nAplicaciones que utilizan inteligencia artificial, aprendizaje automático o análisis de datos en tiempo real de grandes dimensiones, por ejemplo, suelen utilizar DRAM. Aunque DRAM tiene una latencia a nivel de nanosegundos y ofrece una capacidad y capacidad agradable, tiene limitaciones físicas como la densidad de memoria y cuánto se puede ajustar en un servidor. También es relativamente costoso. Lo más importante es que es volátil y, a medida que los datos crecen, la volatilidad de la memoria se convierte en una restricción. PMEM elimina estos cuellos de botella porque se puede tener una capacidad mayor, un costo menor y persistencia.\n\nUn beneficio importante es que las empresas no necesitan reescribir sus aplicaciones para utilizar la tecnología de MemVerge. La compañía estaba buscando la mejor manera de usar la persistencia, y la respuesta fue proporcionar la menor interrupción a los paradigmas de programación. Cuando Optane PMEM se convirtió en una forma de memoria disponible, esto fue una oportunidad para desarrollar software valioso como parte de la solución.\n\n## Memoria Definida por Software\n\nHay una transformación de datos ocurriendo. Desde una perspectiva de hardware, en los próximos diez años, podría haber un mundo más heterogéneo tanto en computación como en elementos de memoria. Surgirá una nueva estructura, como CXL, que conecta entre estos elementos. El mayor desafío será lograr que el ecosistema de aplicaciones se mueva. Se necesita una capa de software para procesarlo en piezas consumibles y componibles que faciliten la digestión de la aplicación. La memoria definida por software de MemVerge será un componente importante en este espacio.\n\nMientras tanto, MemVerge está construyendo un puente entre los paradigmas actuales y futuros. La memoria definida por software de MemVerge ofrece SLA y QLS dinámicos, resistencia, persistencia de aplicaciones, eficiencia y rendimiento. Estas son todas cosas que normalmente se obtienen con una red de almacenamiento definida por software, pero ahora se pueden obtener con memoria de menor latencia y velocidad extremadamente rápida.\n\n## Rendimiento en Memory Machine\n\nDos importantes propiedades intelectuales de MEMVerge están en juego. El primero es una capa de virtualización de software que optimiza el rendimiento de la memoria, la combinación de PMEM y DRAM, que es muy cercana al rendimiento de DRAM. Cada carga de trabajo tiene perfiles de rendimiento diferentes que se pueden ajustar finamente mezclando PMEM y DRAM en diferentes proporciones. Este ajuste fino de PMEM y DRAM brinda a los desarrolladores de aplicaciones y profesionales de TI la capacidad de ajustar la memoria a sus aplicaciones en lugar de toda la máquina. En lugar de configurar las proporciones existentes de DRAM y PMEM para toda la máquina, ahora puede cambiar dinámicamente las proporciones de PMEM y DRAM según la carga de trabajo e incluso superar el rendimiento de nivel DRAM.\n\n## Instantánea de ZeroIO (Persistencia de datos de aplicación)\n\nEl segundo de estos inventos es las instantáneas de memoria o ZeroIO. Puede persistir aplicaciones transitorias existentes sin necesidad de reescribir nada. Funciona sobre la memoria definida por software, que es un servicio de memoria volátil. Aunque la PMEM subyacente es persistente, se necesita memoria volátil para evitar alterar las aplicaciones existentes. La persistencia se aprovecha proporcionando al operador de la aplicación una interfaz gráfica de usuario y CI para gestionar las instantáneas. Existe una capacidad de instantánea, por lo que puedes capturar instantáneamente el estado de una aplicación completa. Luego, esa aplicación puede recuperarse en cualquier momento en el futuro.\n\nEsto hace que tu memoria no solo sea persistente, sino también altamente disponible. Después de un fallo, puedes hacer una recuperación instantánea. Si cometes un error en la base de datos, puedes volver atrás hasta un punto anterior. También puedes clonar sobre la foto instantánea, por lo que puedes crear nuevas instancias de una aplicación sin replicar físicamente la memoria. Así puedes crear múltiples procesos independientes que mapean los espacios de memoria lógica al mismo espacio físico de memoria. Esto no solo ahorra memoria, sino que hace que el proceso de creación de clones sea instantáneo. Esta nueva tecnología hace posible muchas cosas que antes eran imposibles.\n\nNo hay cambios arquitectónicos en tu programa, pero cuando necesites persistir algo, simplemente puedes tomar una instantánea. Esto no cambia el modelo familiar de programación, pero acelera enormemente la E/S. Otra gran característica es que las instantáneas se convierten en objetos manejables, por lo que se pueden transportar a cualquier lugar donde se pueda reiniciar la aplicación. Además, la migración en vivo se puede habilitar en ciertos escenarios también.\n\n## Casos de uso habilitados para el futuro\n\nPara un vistazo previo al futuro, MemVerge planea lanzar una versión 2.0 en aproximadamente un año que incluirá un SDK. Además de utilizarlo como una capa de memoria transparente, los nuevos desarrolladores de aplicaciones tendrán una nueva forma de persistir sus datos. Esto facilitará el desarrollo de aplicaciones, así como la modificación de aplicaciones existentes.\n\nCon el SDK, los desarrolladores podrán hacer una captura instantánea de segmentos de la memoria de la aplicación o del perfil completo de la memoria, lo que les dará la capacidad de persistir la memoria con las tecnologías de ORM o mapeo de memoria incómodas que existen hoy en día.\n\nEn asociación con Intel, MemVerge lanzará la primera versión de su producto con la memoria definida por software y la capacidad de instantáneas para disponibilidad general el 23 de septiembre de 2020.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Charles Fan"],"link":"/episode-EDT21-es","image":"./episodes/edt-21/es/thumbnail.png","lang":"es","summary":"Charles Fan, CEO de MemVerge, habla con Darren Pulsipher, Arquitecto Jefe de Soluciones, Sector Público, Intel, sobre su nueva tecnología, los controladores de memoria definidos por software Big Memory. La tecnología utiliza la memoria persistente Intel 3D XPoint Optane para cerrar eficientemente la brecha entre las arquitecturas actuales y futuras, al mismo tiempo que proporciona una mayor capacidad, menor costo y persistencia."},{"id":91,"type":"Episode","title":"Un argumento a favor del Multi-Hybrid Cloud","tags":null,"body":"\r\n\r\n## Entorno de nube actual.\n\nEn los últimos cinco años, ha habido un cambio fundamental en el entorno de IT. El crecimiento continuo de la Nube Pública y la aparición de opciones de Nube Privada han dejado a muchos CIO(s) y departamentos de IT en una situación de desventaja. En el mercado competitivo de hoy en día, muchos equipos de desarrollo necesitan moverse más rápido de lo que la mayoría de los departamentos de IT pueden entregar. Los equipos de desarrollo han encontrado en las Nubes Públicas como AWS, GCE y Azure una opción viable para el antiguo estilo de \"IT casero\" \"debajo del escritorio\". Los proveedores de nube pública han facilitado y acelerado la creación de nueva infraestructura. Ya no es necesario esperar aprobaciones técnicas y comerciales en varios niveles, espacio físico en el centro de datos y problemas de suministro del proveedor. Ahora, en cuestión de minutos, un equipo de desarrollo puede tener toda la infraestructura que necesita para su nuevo proyecto.\n\nAntes de la Nube Pública, los CIO(s) podían \"caminar alrededor\" de los cubículos y contar la cantidad de máquinas de \"TI casera\" que estaban funcionando debajo de los escritorios de las personas. Con las máquinas físicas ya no visibles para los departamentos de TI, es imposible identificar los equipos y las infraestructuras de sus proyectos. Muchas nubes públicas han dado a las organizaciones la capacidad de consolidar la contabilidad de todas las cuentas de dominios específicos, pero la visibilidad de lo que se está ejecutando y quién está trabajando en la infraestructura sigue siendo algo así como una \"búsqueda del tesoro\". Muchas veces, estos proyectos \"rebeldes\" se hacen visibles cuando los proyectos se convierten en productos y necesitan ser incluidos en una infraestructura segura de la empresa. Las políticas de seguridad, privacidad y regulación pueden hacer que la \"productización\" de los proyectos sea casi imposible. Especialmente si los desarrolladores han acoplado estrechamente sus aplicaciones a la infraestructura en la Nube.\n\nLos departamentos de TI con visión de futuro están haciendo todo lo posible por captar la \"TI de tipo cabaña\" trabajando con nubes públicas y proveedores de software independientes para implementar \"portales de la empresa\" en las nubes. Establecer un portal de paso es un buen comienzo para capturar proyectos utilizando infraestructura, pero muchas organizaciones encuentran que solo un portal deja a los equipos de desarrollo queriendo más. En los últimos años, he estado trabajando con muchas de estas organizaciones para identificar casos de uso, arquitecturas y tecnologías que ayuden a desarrollar estos portales aumentados que llamamos \"Multi-Nubes Híbridas\" (MHC). Por lo general, se integran tres tecnologías principales para construir estos MHC: Plataformas de Gestión en la Nube (CMP), Marcos de Automatización y Herramientas de Plataforma como Servicio (PaaS).\n\n## Plataforma de Gestión en la Nube (CMP)\n\nLa responsabilidad principal de las plataformas de gestión en la nube es administrar múltiples nubes heterogéneas tanto públicas como privadas. Permiten a los usuarios finales gestionar múltiples nubes e infraestructuras desde una sola interfaz común. Las CMPs suelen estar diseñadas pensando en los administradores de la nube. Aunque las herramientas de las plataformas de gestión en la nube se enfocan principalmente en la gestión de múltiples nubes, muchas de ellas también cuentan con características adicionales de PAAS y marcos de automatización, o al menos tienen una arquitectura de complementos para respaldar estas funcionalidades.\n\n## Casos de uso cubiertos\n\nGestionando las Nubes Públicas.\n\nGestionando Nubes Privadas\n\nGestionando identidades en la nube\n\nGestionando la infraestructura en múltiples nubes.\n\n## Marcos de automatización\n\nEl principal objetivo de los marcos de automatización es automatizar la implementación, gestión y actualización de pilas de software en una infraestructura. Estos marcos de automatización surgieron de la comunidad de DevOps y suelen centrarse en procesos repetibles. Muchas de estas herramientas incluyen lenguajes de secuencias de comandos que permiten a los ingenieros de DevOps gestionar y configurar software y servicios de manera repetible. Muchos equipos de DevOps están bien versados en estas herramientas.\n\n## Casos de uso cubiertos\n\nImplementar software en infraestructura\n\nGestionar software en la infraestructura.\n\nActualizar el software y los servicios.\n\n## Plataforma como servicio (PAAS)\n\nPlataforma como Servicio es principalmente responsable de proporcionar un portal único para reutilizar plataformas y desplegarlas en la infraestructura. Las herramientas PaaS suelen estar altamente orientadas al Desarrollador. Lo cual puede llevar a configuraciones de infraestructura inflexibles. Muchas de estas herramientas tienen un portal web que brinda a los desarrolladores la capacidad de seleccionar servicios y desplegarlos en la infraestructura.\n\n## Casos de uso cubiertos\n\nDesplegar/Gestionar Servicios/Aplicaciones\n\nGestionar el Catálogo de Servicios.\n\nDesarrollar nuevos servicios/aplicaciones\n\n## La convergencia crea la Multi-Nube Híbrida (MHC)\n\nDebido a que no existe un conjunto de herramientas que cuente con todos los casos de uso necesarios para gestionar nubes, aplicaciones, infraestructura y servicios, los equipos emplean varios \"años de trabajo humano\" instalando, configurando e integrando estos tres conjuntos de herramientas juntos. Esto ha llevado al surgimiento de tecnologías que integran estas herramientas, incluyendo nuevas ofertas de productos y nuevas funciones en productos actualmente disponibles.\n\nMuchos productos de CMP están incluyendo herramientas PaaS y marcos de automatización en sus soluciones. Las herramientas PaaS ahora están gestionando múltiples nubes. Los marcos de automatización están comenzando a ofrecer portales web y conectividad a múltiples nubes. Muchas de las herramientas se están moviendo hacia la visión híbrida de multi-nube. Al elegir qué herramienta(s) utilizar, es importante recordar las raíces de la herramienta.\n\n## Implementando una solución.\n\nEl ecosistema híbrido de múltiples nubes todavía es bastante nuevo y aún requiere algunas integraciones complejas entre las herramientas. Hay algunas herramientas que están empezando a ofrecer soluciones completas de forma predeterminada, pero aún con su visión particular del mundo. Debido a que el ecosistema es incipiente, hay muchos actores y opciones. El tiempo dirá quién ganará este espacio. Por ahora, será interesante observar cómo las herramientas convergen y se consolidan mientras las características maduran.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT22-es","image":"./episodes/edt-22/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher describe por qué un Arquitecto de Nube Multi-Híbrida puede estar presente en tu Centro de Datos. La mayoría de las organizaciones ya tienen todos los ingredientes. Solo necesitan saber cómo encajan juntos."},{"id":92,"type":"Episode","title":"Creación de perspicacia en organizaciones centradas en datos","tags":["aiml","dataarchitecture","datacentric","data","insight","technology"],"body":"\r\n\r\nUtilizando el ejemplo de la fabricación de widgets, Darren y Sarah hablaron previamente sobre el flujo de datos en un esfuerzo por responder una pregunta fundamental de negocios: ¿Cuántos widgets producir? La complejidad se vuelve evidente cuando se trata de tomar los datos crudos de la demanda de clientes, la oferta actual y las pérdidas de rendimiento, y convertirlos en información.\n\nEl primer paso en este proceso, antes de poder decidir qué herramientas usar, es preparar los datos en una forma utilizable. Ochenta o noventa por ciento del trabajo de un científico de datos se dedica a preparar y transformar los datos para poder ponerlos en un algoritmo, por ejemplo, o usarlos para el reconocimiento de patrones.\n\nLas herramientas adecuadas son complejas. La inteligencia artificial y el aprendizaje automático no utilizan un único algoritmo, sino una gran cantidad de herramientas que los científicos de datos utilizan y experimentan en combinaciones para obtener los mejores resultados de información. En otras palabras, un solo algoritmo no te dirá cuántos widgets debes fabricar. Hay muchas partes superpuestas y las propias herramientas son complejas. Además, los científicos de datos tienen diferentes áreas de especialización; la ciencia de datos es un deporte en equipo. Así como no le asignarías a un ingeniero de redes el diseño de la arquitectura de almacenamiento, no contratarías a un ingeniero de aprendizaje profundo especializado en reconocimiento de imágenes para resolver tu problema de análisis de rendimiento. Tienes que contratar a los científicos de datos adecuados para diseñar e implementar las herramientas adecuadas y obtener información sobre tus preguntas comerciales.\n\nVolviendo a la pregunta de cuántos widgets debes fabricar. Cada situación y modelo serán diferentes, por supuesto, en función de los tipos de preguntas, datos y dinámicas que tengas, pero utilizaremos esto como punto de partida. Una vez que se establezca esta pregunta empresarial, la organización atravesará un desarrollo de madurez analítica.\n\nPrimero, te enfocarás en lo que ha sucedido en el pasado para observar patrones en la demanda de tus widgets. Por ejemplo, podrías examinar datos de series de tiempo para ver cuándo aumenta o disminuye la demanda de widgets. ¿Qué tan estable es la información a lo largo del tiempo y cómo puedes utilizarla para predecir el futuro? Tal vez podrías utilizar aprendizaje automático para analizar diferentes segmentos de tiempo y comprenderlos. También podrías realizar análisis de texto para determinar si las personas están hablando de tu widget en las redes sociales. El número de likes o compartidos podría ser una fuente de datos.\n\nPodrías hacer algo similar con el suministro. ¿Qué tan estable es el suministro actual y cuánto comprendes la dinámica del sistema? Observa dónde necesitas a una persona para entender la dinámica del sistema e incorpora ese conocimiento en la forma en que realizas tus análisis. Podría haber algún tipo de patrones de aprendizaje automático que te den ideas de conocimiento, por lo que podrías realizar un aprendizaje no supervisado. El aprendizaje no supervisado consiste en encontrar si hay diferentes categorías o segmentos de los que no eres consciente y que se comportan de manera diferente entre sí. Pregunta cómo puedes realizar un mejor seguimiento o obtener una mejor resolución de lo que está sucediendo en estos grupos.\n\nEn resumen, dependiendo de dónde venga los datos y de lo que estés observando en esos datos, vas a utilizar diferentes herramientas.\n\nLa pérdida de rendimiento es un ejemplo de la complejidad de los problemas que deben ser resueltos. Cuanta más variación haya en la línea de producción, más desperdicio puede ocurrir. En este caso, el aprendizaje automático estará enfocado en segmentos y grupos de diferentes tipos de rendimientos. ¿Cómo se cuantifica y predice eso?\n\nUna de las cosas que los científicos de datos hacen aquí es el diseño de experimentos para tratar de estimar la causalidad. Al girar perillas y accionar palancas de manera sistemática, puedes observar qué sucede con el rendimiento, al mismo tiempo que añades controles de proceso para evitar desviaciones.\n\nOtra oportunidad para realizar análisis es la confiabilidad. Por ejemplo, con el mantenimiento predictivo, se pueden mantener las herramientas de fabricación de manera oportuna para prevenir la pérdida de rendimiento. También se puede utilizar el análisis de texto en ciertas situaciones, como cuando se tienen registros escritos de observaciones y soluciones de técnicos a lo largo del tiempo, para utilizar como base de conocimiento colectiva.\n\nEl aprendizaje profundo en torno al reconocimiento de imágenes es otra estrategia para ayudar a prevenir pérdidas mediante la detección de errores y defectos, e incluso posiblemente la categorización de defectos.\n\nEl objetivo de todo esto es, por supuesto, obtener conocimientos comerciales valiosos para tu organización. La clave es comprometerse con una organización centrada en los datos, mantenerse flexible y contar con las herramientas adecuadas y las personas adecuadas para convertir tus datos en información valiosa y accionable.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT23-es","image":"./episodes/edt-23/es/thumbnail.png","lang":"es","summary":"En la parte 5 de una serie, Kick-starting your Organizational Transformation to Become Data Centric, Sarah Kalicin, Lead Data Scientist, Intel, y Darren Pulsipher, Chief Solutions Architect, Public Sector, Intel, discuten cómo crear conocimiento utilizando inteligencia artificial y aprendizaje automático en una organización centrada en los datos."},{"id":93,"type":"Episode","title":"Estrategia Digital y Arquitectura de Datos","tags":["data","dataarchitecture","aiops","secops","devsecops","devops","compute","technology","process"],"body":"\r\n\r\nTener un plan para la transformación digital de su organización es crucial para evitar perderse en el laberinto de simplemente seguir con las últimas y mejores tecnologías y procesos. Esta estrategia caótica hará que su transformación digital fracase. Un plan, o un mapa de ruta, desde donde se encuentra actualmente su organización hasta donde quiere llegar, es la parte más importante de una transformación efectiva.\n\nLos tres elementos clave que deben estar coordinados y equilibrados en tu plan son organizacional, procedimental y tecnológico.\n\n## Divisiones organizacionales\n\nPara entender el cambio organizacional, primero veamos los grupos comunes en la mayoría de las organizaciones.\n\n## Desarrollo\n\nEste grupo es tu equipo del día; están desarrollando nuevos productos. Los procesos del equipo de desarrollo son bien conocidos y maduros. Se centran en el trabajo de desarrollo, pruebas y llevar el producto a producción. Casi todos los equipos utilizan algún tipo de técnica ágil o de iteración rápida.\n\n## Traduce lo siguiente al español: IT.\n\nEl objetivo del equipo de TI es optimizar la infraestructura en cuanto a costos y eficiencia. Se aseguran de que la infraestructura sea confiable y tenga controles de seguridad incorporados. Principalmente, se enfocan en computación, almacenamiento, red, cumplimiento normativo y costos.\n\n## Seguridad\n\nEl papel de la seguridad se ha vuelto cada vez más importante en los últimos años, incluso más recientemente con los empleados trabajando desde casa debido a COVID-19. El equipo de seguridad se enfoca principalmente en asegurar la propiedad intelectual, los datos y la infraestructura. Las herramientas comunes son la gestión de identidad, la protección, la detección y la mitigación. Comprender cómo funcionan estas herramientas a un nivel alto es importante para su cambio organizacional.\n\n## Datos\n\nEste grupo más nuevo, que anteriormente podría haber sido un estadístico o matemático que realizaba minería de datos, ahora se está afianzando con la llegada de los directores de datos principales y las organizaciones construidas alrededor de ellos. Se enfocan en analizar, categorizar y ofrecer un valor real a partir de sus datos. Ya sea que su organización esté en el sector manufacturero o de servicios, o si está tratando de captar nuevos clientes o ahorrar dinero, hay muchas áreas donde los científicos de datos pueden brindar valor.\n\n## Uniendo a los Grupos\n\nPara llevar a cabo un cambio digital efectivo, todos estos grupos necesitan tener un entendimiento mutuo de lo que cada uno aporta y una visión unificada. No quieres que tus científicos de datos estén explorando datos que no tengan valor para el desarrollo o la tecnología de la información. No deseas que tu equipo de seguridad restrinja todo de manera tan estricta que el equipo de desarrollo no pueda completar su trabajo. Entre los grupos, deben existir estrategias, procesos y arquitecturas comunes.\n\nAunque los objetivos y los resultados comunes son ideales, hay obstáculos que superar. Una de las áreas más difíciles en el cambio organizacional es la falta de comunicación en los límites. Algunas organizaciones han creado nuevos grupos para suavizar las divisiones entre los grupos. Por ejemplo, entre seguridad y tecnología de la información, podría haber un Sec Ops o Sec Dev Ops donde se automatizan las políticas y procedimientos que surgen del equipo de seguridad. Otro ejemplo podría ser un equipo de Data Dev, compuesto por desarrolladores que trabajan con los científicos de datos para proporcionar procesos más repetibles a través del desarrollo de aplicaciones o la integración de herramientas y aplicaciones. Arquitecturas comunes y conjuntos de herramientas comunes en los que todos los grupos pueden confiar facilitan mucho el proceso y los cambios.\n\n## El Sistema Perfecto\n\n¿Cómo se vería una arquitectura común? Una versión utópica no existe en la actualidad, pero podemos observar los elementos y quizás construir algo hacia este ideal.\n\nEl autoservicio es fundamental. Por ejemplo, si un científico de datos necesita más almacenamiento para datos, no tendría que llamar al departamento de TI y llenar un montón de formularios, sino utilizar un portal de autoservicio que ahorra tiempo y que entregue el almacenamiento. Por supuesto, el portal estará basado en políticas, para que el equipo de seguridad pueda estar tranquilo de que los datos confidenciales que el científico coloca en una unidad están encriptados y el control de acceso es automático.\n\nOtra característica ideal sería que el sistema sea autoreparador y basado en datos. Si las máquinas se infectaran, por ejemplo, automáticamente serían puestas en cuarentena y las cargas de trabajo se trasladarían a otra área en el centro de datos o a la nube pública. Nuevamente, el departamento de TI tendría que establecer políticas y supervisar los procesos, pero el sistema sería en su mayoría automático. El sistema no solo debería ser automático, sino también inteligente, aprendiendo de la experiencia y volviéndose más eficiente.\n\nPodemos obtener algunos elementos de este sistema utópico hoy en día con productos disponibles en el mercado al integrarlos y lograr que todos los utilicen. Analicemos lo que cada organización desearía de esta arquitectura.\n\n## Arquitectura de TI (nube multi-híbrida)\n\nIT es responsable de la infraestructura subyacente y la información de datos en la organización. Si IT pudiera establecer una base sólida, todos los demás podrían construir sobre ella. IT necesita pasar a una solución de nube multi-híbrida para que la infraestructura pueda ser orquestada fácilmente según sea necesario, con flexibilidad basada en políticas. Siempre hay un compromiso entre costo y confiabilidad, pero tienes opciones. Una capa de infraestructura definida por software permite la orquestación de cómputo, almacenamiento, red, seguridad y ahora incluso cosas nuevas como memoria y aceleradores. La base de la nube multi-híbrida es un aspecto clave de su arquitectura común.\n\n## Arquitectura de seguridad\n\nEl equipo de seguridad agregaría a este sistema y lo haría lo más automatizado posible. El primero sería el aspecto de identidad. Esto significa que no solo puedes identificar a los usuarios, sino también a la infraestructura, aplicaciones y servicios, para que todo tenga una identidad. Esas identidades pueden estar vinculadas a autorizaciones y accesos específicos para asegurarse de que todo esté autenticado. En el lado de la seguridad, se desea encriptación y solución de problemas. Idealmente, se podría establecer una raíz de confianza para que todo en el ecosistema, tanto en aplicaciones y servicios, como en el firmware y BIOS de las máquinas, sea confiable.\n\n## Arquitectura de desarrollo\n\nLos desarrolladores pueden preocuparse de que todo este proceso pueda ralentizar el desarrollo, por lo que debe suceder de forma casi automática. La mayoría de los desarrolladores ahora se centran en componentes reutilizables que pueden ser probados para asegurarse de que son seguros. Hacen esto a través de ecosistemas en contenedores como Kubernetes, Docker o Mesos. La seguridad puede ser integrada en el ciclo de vida del desarrollo en el paso de implementación antes de pasar con éxito a la producción. Encima de la capa de servicio hay una capa de aplicación donde los desarrolladores pueden aprovechar los flujos de trabajo. Estos flujos de trabajo pueden ser flujos de trabajo de desarrollo como CI/CD o flujos de trabajo empresariales a través de herramientas de automatización como la Automatización de Procesos Robóticos. Tener tanto la capa de servicio como la capa de aplicación son elementos clave en esta arquitectura utópica.\n\n## Arquitectura de Datos\n\nCon datos dispersos en varios ecosistemas, nubes públicas e incluso en el borde, necesitamos una mejor forma de gestionar datos para los científicos de datos y desarrolladores de aplicaciones. Extraer los datos del almacenamiento es uno de los elementos importantes aquí. Con esta estructura, puedes orquestar datos en toda la extensa infraestructura y solo vincular esos datos a las aplicaciones y servicios donde se necesitan. Los datos podrían abstraerse para llegar a la infraestructura en el mejor lugar durante ese período de tiempo, ya sea en el borde, en el centro de datos o procesados en varios lugares diferentes para réplicas de aplicaciones. La seguridad sería necesaria para proteger los datos, ya que los datos son la razón de ser de la infraestructura en primer lugar. Algunas empresas emergentes están incursionando en este espacio para tomar el control de la capa de gestión de datos.\n\nEsta arquitectura utópica, con su multitud de partes móviles, se llama arquitectura Edgemere. Estamos tratando de ver cómo todas estas partes encajan juntas para ayudar a las organizaciones a acelerar su transformación digital. Necesitamos entender lo que cada organización necesita, cuáles son sus casos de uso y cuáles son las similitudes entre los grupos, para desarrollar una arquitectura en la que toda la organización pueda trabajar.\n\nLa función de su organización es eliminar las barreras entre los grupos, desarrollar una visión común de dónde desea estar en términos organizativos, procedimentales y arquitectónicos, y elaborar un plan de acción sobre cómo llegar a ese lugar.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT24-es","image":"./episodes/edt-24/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, arquitecto principal de soluciones, sector público, de Intel, describe la estrategia digital y la arquitectura para transformar eficazmente tu organización. Explica cómo los elementos organizativos, procedimentales y tecnológicos deben equilibrarse para trabajar de manera eficiente hacia una arquitectura común e ideal que respalde una visión unificada."},{"id":94,"type":"Episode","title":"Operacionalización de los flujos de datos","tags":["dataarchitecture","datacentric","data","datamesh","datapipeline","technology","people"],"body":"\r\n\r\nPara el episodio final de esta serie, Darren habla con Sarah Kalicin, Científica de Datos Principal en Intel, sobre la operacionalización de tu tubería de datos. Discuten cómo, una vez que tienes tus conocimientos sobre datos, puedes convertirlos de un experimento científico único en una fuente continua de información.\n\n## ¿Cómo operativizar los conocimientos analíticos?\n\nLo primero que debes entender sobre la canalización de datos es que no es como un sistema eléctrico cerrado que puedes configurar, alejarte y luego, seis meses después, encender un interruptor y saber que la bombilla se iluminará. Una canalización de datos es diferente en el sentido de que los datos son variables; pueden cambiar o degradarse, por ejemplo, por lo que no necesariamente serás recompensado con la iluminación de la bombilla en algún momento, o en este caso, la información que estás buscando. Siempre debes pensar en lo que puede salir mal en el sistema y cómo corregir estos cortocircuitos.\n\nLa detección de anomalías es una parte integral del proceso. No se puede planificar todo, por lo que al menos necesitas ser capaz de detectar cuando algo ha ocurrido que está fuera de los límites del análisis original. Un ejemplo es la crisis del COVID, un evento impredecible que causó patrones muy distintos a la norma en muchos sistemas. Otro ejemplo sería una empresa que produce widgets. Para saber cuántos widgets producir, el proceso de datos incluye la demanda de los clientes, la oferta actual y la pérdida de rendimiento. Estos pueden ser bastante estables a lo largo del tiempo, pero podría haber, por ejemplo, un evento de relaciones públicas que haga que la demanda de los clientes se dispare. Eso podría tener un gran impacto en los modelos. El aprendizaje automático y el aprendizaje profundo analizan patrones familiares, y si nunca han visto esos patrones antes, los modelos fallarán o se degradarán. Debes mantenerte siempre al borde del descubrimiento.\n\nLa única manera de mantenerse en la vanguardia de los descubrimientos es automatizar tus tuberías de datos para tener acceso oportuno a la información. Esta es la ventaja competitiva: datos actuales y perspicaces que pueden ayudarte a resolver rápidamente tus preguntas.\n\nLos equipos de tecnología de la información (TI) y de datos necesitan colaborar en la automatización y determinar qué debe ser automatizado para los datos entrantes, así como en la gestión de cualquier cambio en el modelo que los científicos de datos deseen hacer, para que pueda ser fácilmente integrado de nuevo en el flujo de trabajo.\n\n## Despliegue de circuitos cortocircuitados\n\nHay dos tipos de controles que pueden evitar cortocircuitos en la implementación: los controles del sistema analítico y los controles organizativos.\n\nEl control del sistema analítico consiste en poner en funcionamiento los modelos que has entrenado, alimentando los datos para responder fácilmente a tus preguntas. Estos modelos desplegados deben ser moderados para verificar la exactitud de los datos. Muchas cosas pueden afectar negativamente los datos, como cambios ambientales, calibración de máquinas, problemas de distribución, y así sucesivamente.\n\nEsto no es tan diferente del mundo del desarrollo de software donde los cambios pueden afectar las predicciones. El departamento de IT está familiarizado con el proceso de realizar pruebas para asegurarse de que sus modelos o aplicaciones se estén ejecutando de acuerdo con las pautas establecidas, por lo que los desarrolladores de operaciones y los científicos de datos deben aprovechar estos recursos y conocimientos. No es necesario inventar ningún proceso nuevo, sino que los grupos deben combinar recursos para configurarse para el éxito.\n\nLos controles organizativos se remontan a tener una base organizativa que se compromete a ser centrada en los datos y proporcionar las personas adecuadas y los recursos necesarios para trabajar juntos por objetivos comunes. La mejor oportunidad para obtener una operacionalización es cuando hay colaboración, confianza, comprensión de las necesidades y retroalimentación entre los grupos de la organización.\n\nLos bucles de retroalimentación son críticos en este proceso. Por ejemplo, los especialistas en el tema pueden proporcionar información sobre la dinámica del mercado para que los científicos de datos puedan monitorear el modelo en busca de estos cambios en los datos. Si un modelo va a ser utilizado a lo largo del tiempo, constantemente necesitará ser iterado y mejorado.\n\nLos consumidores de los datos deberían tener un panel de control que les brinde información y les permita analizar por qué algo parece un poco fuera de lugar. Cuanto más puedan investigar o plantear lo que necesita ser investigado, más empoderada será su organización.\n\n## Tubería\n\nUna clave desde el lado de IT para operacionalizar la canalización de datos es utilizar un control de versiones como GitHub para tener acceso a versiones anteriores de tu modelo. También es importante poder almacenar los datos que crearon el modelo y otros datos históricos con fines de auditoría. Quieres poder analizar los patrones y ver cómo un determinado atributo cambió o afectó al modelo. También puedes alimentar datos históricos en tus nuevos modelos para ver cuánto afecta a tus datos actuales.\n\nPor ejemplo, algunos sistemas tendrán una imagen sesgada con un gran número de personas trabajando desde casa durante la COVID. Un caso en punto es la Marina. Desde la COVID, el 95% de sus trabajadores de IT están trabajando a distancia, y su productividad ha aumentado un 35%. A partir de ese único dato, se podría decir que todos van a trabajar desde casa a partir de ahora. ¿Seguirás obteniendo un aumento del 35%, o si la gente vuelve a la oficina, verás una disminución del 35%? Obviamente, ese único dato no es necesariamente suficiente para predecir la productividad real.\n\nOtra herramienta que el departamento de Tecnología de la Información puede ofrecer es la integración y despliegue continuo. A través del uso de Jenkins, GitHub Actions u otra herramienta similar, al trabajar en un modelo, se pueden ejecutar automáticamente pruebas contra el modelo con tus datos o generar datos falsos sobre la marcha.\n\nLas personas de TI y los científicos de datos necesitan colaborar en qué y cómo monitorear la salida de los modelos. TI puede monitorear la salida automáticamente, y también pueden monitorear cómo están funcionando los modelos en la infraestructura. Un ejemplo es que TI, con la implementación automatizada de integración continua, puede alertar rápidamente a los científicos de datos cuando un modelo está tardando más de lo normal para evitar que se salga de control. Si TI está comprometido en la creación de valor de datos, lo cual generalmente ha faltado en la industria en general, el proceso será más fácil y coherente para todos.\n\nOtro aspecto a tener en cuenta es el diseño de experimentos, ya que las interacciones entre variables y características también son importantes. Los expertos en la materia pueden ayudar a determinar cuáles son las interacciones potenciales, y se pueden modelar para comprender qué variabilidad se puede esperar.\n\nEs emocionante que un científico de datos pueda tomar material sin procesar y convertirlo en conocimiento. Sin embargo, se necesita un equipo. Cuanto más todos en tu organización puedan aprender unos de otros en un entorno de equipo, más cosas maravillosas pueden suceder.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Kalicin"],"link":"/episode-EDT25-es","image":"./episodes/edt-25/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Jefe de Soluciones, Sector Público, Intel, conversa con Sarah Kalicin, Científica de Datos Principal, Intel, acerca de la operacionalización del flujo de datos de su organización. Se requiere un esfuerzo en equipo para modelar, monitorear y producir una fuente continua de información valiosa. Este es el último episodio de la serie de Cómo impulsar la Transformación Organizativa para Convertirse en una Empresa Centrada en los Datos."},{"id":95,"type":"Episode","title":"Soluciones de datos Multi Cloud con Hammerspace","tags":["hammerspace","data","technology","dataarchitecture","multicloud","compute","multihybridcloud","datamesh"],"body":"\r\n\r\nEl almacenamiento está listo para la disrupción. Actualmente, la gestión de datos se realiza de forma engorrosa, procesal y a menudo manual y propensa a errores. Hammerspace fue fundada para resolver este problema mediante la abstracción de datos de la infraestructura de almacenamiento.\n\n## Imagina por un segundo...\n\n...si tus datos estuvieran liberados de la infraestructura de almacenamiento. Libres de las limitaciones impuestas por los sistemas de almacenamiento actuales, los usuarios podrían gestionar y proteger sus propios datos, cambiar el perfil de costos instantáneamente y acceder a los datos desde cualquier lugar de la infraestructura. El almacenamiento definido por software podría aumentar el rendimiento según la demanda y desplegar cargas de trabajo modernas como Kubernetes en cualquier infraestructura subyacente en cualquier lugar.\n\n## El Desafío\n\nLas aplicaciones se han vuelto portátiles, pero los datos todavía están aislados. El desafío es que el rendimiento, la confiabilidad y la gestionabilidad, todos sufren a gran escala debido al problema de aislamiento. La solución es liberar los datos de las limitaciones de la infraestructura subyacente. Hammerspace logra esto mediante la desagregación de metadatos, asimilando las partes constituyentes más pequeñas para hacer que los datos sean portátiles.\n\n## Desvincular los datos del almacenamiento.\n\nCon esta tecnología, tus aplicaciones cuentan con datos bajo demanda estés donde estés. Tienes control independiente, planos de datos y un espacio de nombres global y un sistema de archivos que abarca múltiples centros de datos y nubes. El almacenamiento está orquestado; los datos están completamente automatizados y aprovechan la gestión declarativa y autónoma de datos. En otras palabras, se separa el \"cómo\" del \"qué\", declarando el estado final deseado sin tener que detallar cada aspecto de cómo llegar allí.\n\n## Arquitectura de Hammerspace\n\nEl sistema te permite operar a nivel granular de archivos, proporciona servicios de datos de clase empresarial como instantáneas y clones, y se mueve fácilmente a una escala masiva desde un centro de datos a otro, desde un centro de datos a la nube, y de regreso al centro de datos para dispersarlo en un escenario de nube híbrida múltiple.\n\n## El almacenamiento heredado no puede superar los desafíos modernos.\n\nEl almacenamiento heredado no se escala suficientemente, incluso con soluciones de escalado horizontal, en la nube o en una escala planetaria, ya que incluso esos clústeres se convierten en silos, dejándote a veces atrapado con datos que no se pueden correlacionar ni analizar. El almacenamiento tradicional también tiene dificultades para escalar la capacidad y el rendimiento de forma independiente.\n\nLa replicación es una tecnología antigua que conduce a la dispersión de datos copiados. En lugar de mover las partes constituyentes más pequeñas, estás moviendo toda la carga útil. La gestión de datos a menudo se considera algo secundario; la gestión de datos pertenece al frente, no a la parte trasera.\n\nUn problema que la arquitectura de Hammerspace resuelve es encontrar cosas fácilmente. La versión puede llevar a grandes problemas empresariales. Por ejemplo, tanto Airbus como Boeing han tenido problemas masivos porque algunos ingenieros no tenían sus versiones más actualizadas. La solución es orquestar los datos.\n\n## Orquestación de datos\n\nOrquestación, en primer lugar, es el desacoplamiento de todos los silos diferentes; los datos son tratados como un único conjunto. Hammerspace asimila las partes más pequeñas constituyentes, los metadatos, para crear, en esencia, un sistema de antigravedad de datos. Luego implementan objetivos como durabilidad, disponibilidad y snapshots, o acciones personalizadas definidas que se pueden realizar a través de scripts de Hammer. Por último, los datos, ya sea Kubernetes, NFS o SMB, se vuelven portátiles y se integran en el sistema.\n\nLo que esto significa en la práctica es que no necesitas ir a otro silo para atender una carga de trabajo específica. Los datos se entregan donde tú quieres que estén. Esta movilidad de datos es clave porque es en tiempo real; no se trata de una migración de datos, lo cual es disruptivo y causa tiempo de inactividad.\n\nPara minimizar los costos caros de egress, los datos se deduplican y comprimen en una base granular de archivos. En lugar de mover un volumen fijo completo, puedes seleccionar los datos que deseas mover según cualquier tipo de expresión, como carpetas, etiquetas de metadatos o un descriptor de cliente. Esto ofrece flexibilidad y ahorro de costos.\n\n## Arquitectura de Hammerspace\n\nEn la arquitectura de Hammerspace, el sistema de archivos global tiene tres componentes a nivel alto: el propio sistema de archivos global, la presentación del frontend (NFS, controlador CSI y SMB) y Anvil dentro del sistema de archivos global. Anvil es el componente de gestión de metadatos y DSX, que proporciona servicios de gestión de metadatos. Estos pueden implementarse como máquinas virtuales, VMware, KBM o Hyper-V. Anvil tiene una configuración en forma de A, por lo que hay al menos dos de ellos en cada ubicación. DSX se puede paralelizar para aumentar el rendimiento, por lo que se pueden tener varios de ellos en diferentes ubicaciones para asegurarse de tener suficiente rendimiento. Se pueden reducir fácilmente.\n\nEn el back end, el almacenamiento subyacente puede ser el almacenamiento definido por software propio de Hammerspace, con discos directamente conectados, NAS asimilado, cualquier nube o cualquier combinación. Esto también puede escalarse horizontalmente, por lo que ahora puedes escalar el rendimiento y la capacidad de forma independiente. Siguiendo el modelo de la nube, también es elástico, por lo que si el negocio cambia en esa ubicación específica, puedes reducir el rendimiento y la capacidad para asegurarte de que las aplicaciones tengan exactamente lo que necesitas en esa ubicación. Todo esto crea una arquitectura muy flexible para atender cualquier carga de trabajo de aplicación en el front end.\n\nUna gran ventaja de esta arquitectura flexible es la capacidad de asimilar datos que se están almacenando en dispositivos que no son tuyos, como NAS o en la nube. Esto simplifica el traslado de datos. Por ejemplo, si tienes un NAS antiguo y deseas migrar a un NAS más nuevo, no importa si son del mismo proveedor o de proveedores diferentes. Hammerspace asimila los metadatos y traslada los datos por detrás y de manera completamente transparente a las aplicaciones, porque es una movilidad de datos en tiempo real. Otra ventaja significativa es que no hay tiempo de inactividad al mover los datos.\n\nSi deseas probar esta tecnología, ve a hammerspace.com y comienza con una prueba gratuita con una licencia de hasta 10 terabytes desplegados en Azure, AWS o Google Cloud.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Johan Ballin"],"link":"/episode-EDT26-es","image":"./episodes/edt-26/es/thumbnail.png","lang":"es","summary":"Johan Ballin, Director de Marketing Técnico en Hammerspace, y Darren Pulsipher, Arquitecto Principal de Soluciones en el Sector Público de Intel, discuten la tecnología de nube híbrida de Hammerspace que libera los datos de la infraestructura de almacenamiento, proporcionando portabilidad y rendimiento de datos."},{"id":96,"type":"Episode","title":"Seguridad de Hardware: Imperativo para Transformar Datos.","tags":["datagovernance","data","sgx","confidentialcomputing","cybersecurity","technology"],"body":"\r\n\r\n## Paisaje de seguridad de datos.\n\nEl mundo se ha vuelto increíblemente conectado con todos los dispositivos, y esto está impulsando un crecimiento exponencial en la cantidad de datos que necesitamos gestionar: cuanto más volumen, más riesgo. Es un desafío, y aprovechar nuevas capacidades informáticas como la nube, el análisis de datos y la informática perimetral añade complejidad adicional.\n\nHay tres tendencias principales en seguridad: cifrar todo, aislar la carga de trabajo y una cadena de confianza. Estas tres áreas son importantes para ayudar a las organizaciones a cumplir con los crecientes requisitos regulatorios para mantener protegidos los datos.\n\nCuando las organizaciones deciden llevar a cabo una transformación digital, o se ven obligadas a adoptarla debido a un catalizador como el COVID-19 que requiere trabajadores remotos, por ejemplo, la seguridad debe ir de la mano para mantener los controles y el cumplimiento de seguridad en su lugar. Si abordas la seguridad a lo largo del proceso de implementación, podrás transformar la forma en la que haces negocios de manera sostenible.\n\n## Protege los datos a lo largo de su ciclo de vida\n\nCifrar datos es importante en todas sus fases. Históricamente, los atacantes podían acceder a los datos directamente a través de la red. Después de que estos se cifraran, empezaron a irrumpir en los centros de datos y extraer datos de las bases de datos. Por eso, comenzamos a cifrar el almacenamiento. Aun así, los datos llegan a un punto final y se cargan en la memoria y se descifran, por lo que un ataque sofisticado que pudiera obtener acceso de root podría potencialmente robar o manipular los datos en ese punto de procesamiento. Este ataque a los datos en uso es la nueva frontera para los atacantes y para aquellos que defienden el espacio. Intel tiene algunas capacidades nuevas emocionantes e innovadoras que estamos incorporando en nuestros procesadores para ayudar a los propietarios de datos, los propietarios de aplicaciones, los proveedores de servicios y, básicamente, a todo el ecosistema a cerrar esas posibles vulnerabilidades.\n\n## ¿Por qué proteger los datos en uso?\n\nEn muchos casos, los atacantes están utilizando escaladas de privilegios en el sistema operativo o en las capas del hipervisor. Podrían estar ingresando a través de un sistema operativo invitado, un sistema operativo host o incluso mediante acceso físico al hardware del servidor. Los atacantes podrían ser hackeadores clásicos con malware, competidores de terceros o incluso personal interno como administradores o administradores del sistema en un proveedor de servicios.\n\n## Extensiones de Guardia de Software de Intel (Intel SGX)\n\nIntel ha incorporado una nueva tecnología en nuestros procesadores llamada Intel Software Guard Extensions, o SGX. Esto es parte del paradigma de informática confidencial que está en auge en el mercado actualmente, ya que las empresas están tratando de transformar sus datos y mantenerlos privados. SGX aborda estos nuevos tipos de ataques al permitir que la aplicación se comunique directamente con el procesador en la memoria cifrada, eludiendo el sistema operativo, el hipervisor y prácticamente todo lo demás en el sistema. Por lo tanto, incluso si hubiera una brecha en su sistema operativo, hipervisor u otras aplicaciones, los atacantes no podrían acceder a esos datos porque el propio sistema operativo no tiene visibilidad de esos datos. Por lo tanto, no es necesario confiar en el proveedor ni en el resto de la pila del sistema porque básicamente se opera como si ya estuvieran comprometidos, y su código y datos permanecerán confidenciales e inalterados; tendrán integridad.\n\nIntel está tratando de hacer que tengas que confiar en el menor número de componentes posible. SGX va más allá en ese camino para el centro de datos que cualquier otra cosa que hayamos visto. Lo único en lo que tienes que confiar es en tu propia aplicación y en el propio procesador.\n\nSGX ofrece una capacidad poderosa para empresas como proveedores de servicios en la nube, quienes pueden decirle a sus clientes que ellos, o el gobierno, por ejemplo, no podrían acceder a sus datos aunque quisieran hacerlo.\n\n## Intel SGX en Acción\n\nSGX ya es ampliamente utilizado por proveedores de servicios en la nube y vendedores de software, pero de alguna manera, apenas estamos comenzando. Aunque esta tecnología ha estado disponible durante varios años, hemos creado ecosistemas y estamos incorporando nuevas capacidades en nuestros próximos procesadores Xeon Ice Lake de tercera generación. Esto ampliará sus capacidades, su capacidad para escalar cargas de trabajo empresariales grandes y podrá proteger porciones de memoria mucho más grandes con un mejor rendimiento y en un rango mucho más amplio en los centros de datos mainstream.\n\nIntel es uno de los miembros fundadores del Consorcio de Computación Confidencial, que forma parte de la Fundación Linux. La mayoría de los grandes proveedores de servicios en la nube, muchos proveedores de software e incluso nuestros competidores en el ámbito de la tecnología de silicio están trabajando juntos en este tipo de soluciones de entornos de ejecución confiables y estableciendo estándares para manejar este tipo de capacidades. También estamos creando conciencia sobre la necesidad y el valor empresarial de la computación confidencial.\n\n## Ecosistema de socios de software Intel SGX\n\nSGX ofrece muchas opciones de uso dependiendo de lo que esté buscando el propietario de los datos. Si un cliente desea el nivel más detallado de control, SGX les permitirá hacerlo. De hecho, pueden reducir su aplicación esencialmente solo a los datos codificados que desean mantener aislados del resto del sistema o incluso solo a una parte de esa aplicación. Sin embargo, este escenario requiere que escriban la aplicación con ese propósito. En el ecosistema, existen recursos de código abierto que facilitan mucho este tipo de desarrollo y siempre se está expandiendo.\n\nEn el otro lado está la migración rápida \"lift and shift\". Puedes tomar tu aplicación y colocarla en un entorno más seguro. El ecosistema está respondiendo y creando contenedores con enfoque en SGX. Podrías colocar tu aplicación sin modificaciones en ese entorno donde sería lo único en funcionamiento, por lo que la aplicación misma creería que se está ejecutando en su entorno nativo.\n\nNuestros socios del ecosistema han avanzado mucho en este espacio con cosas como Fortanix, Graphene y Scone. Algunos son de código abierto y otros son propietarios, pero vienen con todos los servicios incorporados. Por ejemplo, Microsoft Azure Confidential Computing ofrece toda la gama, desde soluciones de traslado y adaptación con SGX hasta SDK que te permiten desarrollar directamente tu aplicación para él y desplegarla en su entorno para que ni siquiera tengas que gestionar el hardware. Hay un conjunto completo de opciones, por lo que nadie debería temer las complejidades de SGX. Los clientes también deben tener la confianza de que material sensible como algoritmos de aprendizaje automático o claves de cifrado serán manejados con un nivel muy granular de protección.\n\n## Computación Confidencial: Un Cambio de Juego en la Seguridad\n\nSGX ha estado ahí durante algún tiempo y hemos estado trabajando para expandirlo. Se ha probado y ha sido sometido a pruebas intensivas, con cientos de documentos de investigación y fortalecido con actualizaciones a lo largo del tiempo. Tiene la ventaja de no ser el novato en el vecindario, sino una solución fundamental que está siendo llevada al gran público con Ice Lake. Ya no se enfoca en áreas pequeñas y sensibles, sino que ahora está listo para cosas más grandes.\n\nEl gobierno, los servicios financieros y la atención médica son algunas de las industrias que desde el principio vieron el atractivo de SGX porque tienen muchas expectativas regulatorias y requisitos de privacidad, sin embargo, intentan compartir datos y realizar actividades innovadoras con múltiples partes. Las empresas tienen situaciones similares, por ejemplo, si desean migrar a la nube pública pero no confían en que proteja sus datos sensibles. Con SGX, no necesitan confiar en el proveedor.\n\nIntel acaba de hacer un gran anuncio este mes y realmente estamos abriendo las puertas a las cosas que se avecinan. Un buen lugar para comenzar es intel.com/sgx para testimonios y una exploración más profunda de la información. Los clientes deben buscar socios del ecosistema como Azure y Fortanix. Otro lugar para obtener información es el Consorcio de Informática Confidencial debido a la cantidad de personas que trabajan en ese ámbito.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jesse Schrater"],"link":"/episode-EDT27-es","image":"./episodes/edt-27/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones, Sector Público, Intel, y Jesse Schrater, Gerente de Seguridad, Grupo de Plataformas de Datos, Intel, discuten el panorama actual de seguridad y cómo la tecnología SGX de Intel y su ecosistema de colaboradores ofrecen una solución oportuna y probada para los datos en uso y otras preocupaciones de seguridad."},{"id":97,"type":"Episode","title":"Integración de legado con la nube y RPA.","tags":["rpa","bpm","automation","compute","technology"],"body":"\r\n\r\nParece que todo el mundo está en medio de una Transformación Digital. Nube Privada, Nube Pública, Nube Multi-Híbrida, Lagos de Datos, Aprendizaje Automático, Inferencia e Inteligencia Artificial son todos términos que la gente usa hoy en día para describir su transformación digital, pero ¿qué pasa con la integración heredada? Nadie habla nunca de la Integración Heredada. ¿Por qué? Porque es difícil integrar aplicaciones, datos y seguridad heredados en tu nuevo y prístino entorno Multi-Híbrido en la nube. Esa es una de las últimas cosas de las que queremos preocuparnos. Si no desarrollamos una estrategia heredada, entonces tenemos un barco rápido con el ancla abajo. Nos ralentiza y mantiene nuestro barco atascado en el puerto.\n\nUna de las áreas en crecimiento para ayudar con la Integración Legacy y la automatización de la integración es el uso de herramientas y marcos de automatización. En los últimos 3 años, ha surgido un énfasis significativo en la automatización de flujos de trabajo con aplicaciones heredadas y nuevas aplicaciones con conocimiento en la nube para trabajadores de la información. Estas herramientas se llaman herramientas de Automatización de Procesos Robóticos (RPA, por sus siglas en inglés).\n\n## Automatización de Procesos Robóticos (RPA)\n\nCuando comencé a investigar herramientas de Automatización de Procesos Robóticos (RPA, por sus siglas en inglés), pensé que controlaban robots. Estaba listo para ponerme mis botas con punta de acero y un casco y visitar instalaciones de fabricación. Sin embargo, rápidamente descubrí que las RPAs imitan la forma en que los trabajadores de la información trabajan con las diferentes herramientas que utilizan. Algunas de las herramientas son herramientas heredadas y algunas de esas herramientas son aplicaciones modernas nuevas. Las RPAs registran cómo el trabajador de la información utiliza la Interfaz de Usuario de las diferentes herramientas y luego permiten que la grabación se reproduzca, automatizando el flujo de trabajo del Trabajador de Información.\n\nSorprendentemente, las RPAs son una tecnología bastante madura. Muchas de las empresas de RPA tienen un historial en el mercado de herramientas de prueba de interfaz de usuario. La capacidad de capturar la interacción del usuario con múltiples aplicaciones a lo largo del tiempo es fundamental para desarrollar una prueba repetible de la interfaz de usuario. Estas herramientas han existido durante más de 20 años y son bastante maduras. Estas herramientas de prueba de QA de UI han sido rebrandeadas y reutilizadas para los trabajadores de información que desean automatizar sus tareas repetitivas y redundantes.\n\n## Lugar de mercado actual - 2020\n\nLos inversores consideran este mercado como un mercado prometedor y han invertido fuertemente en estas tecnologías. En los últimos tres años, se han invertido más de $2.0 mil millones en el mercado de RPA. Tres empresas se han llevado la mayoría de la inversión:\n\nUiPath - Inversión de $1 billón en $300 millones de ingresos anuales.\n\nAutomation Anywhere - Inversión de $500 millones en ingresos anuales de $100 millones.\n\nBluePrism: Inversión de $50 millones en ingresos anuales de $30 millones.\n\n*Translate the following to Spanish:*\n\nTranslation: *Traduzca lo siguiente al español:*\n\nLa mayoría de la inversión no proviene del tradicional Silicon Valley de alta tecnología, sino que proviene de centros financieros como Nueva York y Londres, lo que indica que las instituciones financieras están considerando las RPAs para automatizar muchos de los flujos de trabajo y procesos de sus propios trabajadores de información.\n\n## Lugar donde el RPA funciona bien\n\nLos primeros segmentos verticales en adoptar RPAs han sido las industrias financieras, de seguros y médicas. Estas industrias han buscado formas de disminuir la variabilidad, aumentar la confiabilidad y reducir costos. Debido a la gran cantidad de trabajadores de información en estas industrias, han recurrido a los RPAs para automatizar gran parte del trabajo que actualmente realizan los trabajadores de información. Para automatizar estos flujos de trabajo, las organizaciones necesitan comprender cómo se crean estos flujos de trabajo.\n\nPrimero, vamos a entender al trabajador de la información. Muchos de los trabajadores de la información pasan tiempo trabajando con múltiples aplicaciones, fusionando información y aplicaciones de manera improvisada. A medida que estos trabajadores continúan trabajando con estas aplicaciones, crean orgánicamente flujos de trabajo que combinan datos y aplicaciones en una cantidad innumerable de combinaciones.\n\nSegundo, Cataloga los flujos de trabajo lo mejor que puedas encontrando candidatos para eliminar flujos de trabajo a través de la duplicación y redundancia. Ahora que los cargas de trabajo están comprendidos, el siguiente paso es priorizar y enumerar los flujos de trabajo. Centrarse en los flujos de trabajo más utilizados con el número más significativo de pasos tiende a ser la mejor manera de priorizar los flujos de trabajo.\n\nPor último, descubre cómo automatizar los flujos de trabajo con los robots RPA. La automatización se puede realizar mediante la grabación de la interfaz de usuario del flujo de trabajo de uno de los trabajadores de información y anotando el flujo de trabajo con variaciones basadas en la entrada de datos y las credenciales de seguridad. Una vez completada la grabación, se crea un robot RPA para automatizar el flujo de trabajo. Ahora debes decidir cómo quieres que se ejecute el RPA: de manera asistida o no asistida.\n\n## Modos de operación de RPA.\n\nLos RPAs se ejecutan en dos modos básicos de operación. Atendido y no atendido. Atendido significa que se ejecuta en el escritorio o la computadora portátil del trabajador de información. Ayuda al trabajador de información automatizando el trabajo que realiza a diario. El modo no atendido se ejecuta en un entorno de escritorio virtual y generalmente se activa mediante un evento o disparador y se ejecuta sin interacción alguna con el trabajador de información. Hay beneficios al ejecutarse en ambos modos, como se describe a continuación.\n\n## Asistió.\n\nManeja tareas para empleados individuales.\n\nLos empleados activan y dirigen a un bot para llevar a cabo una actividad.\n\nTraduzca lo siguiente al español: * Los empleados activan bots para automatizar tareas según sea necesario en cualquier momento.\n\nAumenta la productividad y la satisfacción del cliente en centros de llamadas y otros entornos de servicio de ayuda.\n\n## Sin supervisión\n\nAutomatiza los procesos de back-office a gran escala.\n\nProvisionado basado en procesos basados en reglas.\n\nLos bots completan los procesos comerciales sin intervención humana según un horario predeterminado.\n\nLibera a los empleados del trabajo mecánico, reduciendo costos, mejorando el cumplimiento y acelerando los procesos.\n\n## Cómo integrar RPA en tu Empresa\n\nPara entender cómo los RPAs encajan en su empresa, primero debe observar a los usuarios de los RPAs. Específicamente, hay tres tipos de \"actores\" que utilizan, gestionan o influyen en las herramientas de RPA.\n\nTrabajador de Información: Este es el usuario principal de las herramientas de RPA. Sus procesos manuales son objetivos para la automatización.\n\nDesarrollador de aplicaciones - Los bots de RPA cambian cuando se actualizan o crean aplicaciones. Los cambios en la interfaz de usuario requieren \"volver a grabar\" los bots de RPA.\n\nOperaciones de TI: gestionar las herramientas de RPA e implementar bots de RPA sin supervisión.\n\n*\n\n## Gestionando el Cambio\n\nGestionar la complejidad de las configuraciones y la seguridad son factores críticos para una implementación exitosa de herramientas y bots de RPA. Primero, se necesita comprender cómo interactúan los diferentes usuarios de los RPAs cuando se realizan cambios en aplicaciones, flujos de trabajo y procesos. Esta comprensión es fundamental para gestionar los cambios en los bots de RPA y las herramientas que utilizan.\n\nPequeños cambios en las aplicaciones pueden tener un efecto profundo en los Trabajadores de Información y en cómo realizan su trabajo diario, lo cual a su vez significa grabar un nuevo bot de RPA o actualizar uno existente. Debido a la conexión entre los bots de RPA y las herramientas y flujos de trabajo, crear bots de RPA cuando los flujos de trabajo o las herramientas están inmaduras provoca una rotación y fragilidad innecesarias. Los procesos y las herramientas maduras son excelentes candidatos para la automatización mediante RPA.\n\nOtra cosa a considerar es dónde se están ejecutando las herramientas con las que estás automatizando con tu herramienta RPA. ¿Utilizan aplicaciones heredadas e infraestructuras? ¿Utilizan una nube pública o privada? ¿Cómo están conectadas las redes de estos sistemas? A medida que aumenta el número de entornos, también lo hace la complejidad de mantener y actualizar aplicaciones y bots de RPA. Encuentra formas de reducir el número de límites ambientales que el bot de RPA atraviesa.\n\n## Gestión de seguridad\n\nOtro factor crítico a considerar es la seguridad para los bots de RPA. Cuando un trabajador de información registra su flujo de trabajo, necesita autenticarse (iniciar sesión) en cada herramienta que está utilizando. Los trabajadores se autentican utilizando nombres de usuario y contraseñas, claves de autenticación o incluso herramientas de inicio de sesión único corporativas. De cualquier manera, es necesario gestionar la seguridad de estas herramientas en el contexto del bot de RPA en ejecución. Cualquier cambio en la autenticación (nombre de usuario, contraseña, claves de autenticación o credenciales) requiere cambios en el bot de RPA. Muchas de las herramientas de RPA tienen mecanismos para inyectar credenciales de seguridad en el bot de RPA y autenticarse con las herramientas en tiempo de ejecución.\n\n## Gestionando herramientas y bots de RPA con flujos de trabajo SecDevOps\n\nLa complejidad de las herramientas y robots de RPA se presta muy bien a los patrones conocidos en el mundo de SecDevOps. Afortunadamente, muchos de los problemas con la gestión de configuraciones y dependencias se manejan correctamente con un proceso de SecDevOps.\n\n## RPA agrupado\n\nUno de los trucos consiste en tratar al bot de RPA como un servicio complejo que contiene varias máquinas virtuales o contenedores para cada una de las herramientas, un escritorio virtual y el propio bot. Estos servicios se pueden agrupar y administrar como un paquete único. Un paquete incluye no solo los servicios, sino también cómo se comunican (red) de manera segura (autenticación).\n\nAl pasar un paquete a un orquestador de servicios, se permite una mayor automatización de la gestión del firewall de red, la seguridad y la inyección de claves de credenciales y la gestión del ciclo de vida del bot de RPA y las herramientas que utiliza. Hay varias herramientas en el espacio de virtualización (VMWare y OpenStack) que permiten la creación y gestión de estos paquetes. El espacio de contenedores también cuenta con herramientas similares de programación y orquestación, como Kubernetes, Mesos y Docker Swarm.\n\n## SecDevOps Pipeline\n\nUn pipeline de SecDevOps simple administra el paquete del bot de RPA de la misma manera que cualquier otro paquete de aplicación tradicional.\n\nUn trabajador de información construye los paquetes de bots RPA grabando el flujo de trabajo de la interfaz de usuario en un entorno de desarrollo. El trabajador graba fácilmente su flujo de trabajo y luego crea un paquete que se \"Verifica\" en el proceso de desarrollo. En ese momento, el paquete de bots RPA pasa por un ciclo de construcción, prueba y producción. Los puntos de control en cada paso a lo largo del camino ayudan a garantizar la calidad del bot RPA. Debido a que el paquete puede inyectar red y seguridad dependiendo de los diferentes entornos, los bots RPA pueden ser reutilizados por diferentes trabajadores de información y en diferentes entornos.\n\nOtro beneficio de poner robots RPA en paquetes RPA es la gestión de las herramientas y los robots en múltiples entornos de infraestructura como los legados, privados y públicos en la nube. Muchas de las herramientas de orquestación de servicios pueden crear automáticamente conexiones entre estos entornos de infraestructura mediante la creación de una red superpuesta. El canal disminuye la cantidad de trabajo \"manual\" realizado por la organización de TI y, en muchos casos, todos los pasos en el canal están automatizados.\n\n## Desventajas de los bots de RPA\n\nAquí tienes una lista de cosas a tener en cuenta al utilizar bots de RPA en tus sistemas empresariales.\n\nLa seguridad puede ser un agujero grande si no le prestas atención. Uno de los mayores errores es ejecutar aplicaciones en un robot de automatización de procesos (RPA) en modo privilegiado o con credenciales de una cuenta \"global\".\n\nLas aplicaciones de RPAs bots están estrechamente acopladas a las Interfaces de Usuario de múltiples aplicaciones, cualquier cambio pequeño en una aplicación significa que necesitas volver a grabar el bot de RPA.\n\nLos bots de RPA no pueden manejar cambios muy bien, son muy frágiles ante cambios en las aplicaciones e incluso en la configuración de aplicaciones.\n\nLa reutilización es mínima debido al acoplamiento estrecho con las interfaces de usuario de la aplicación. Algunas herramientas utilizan etiquetas en lugar de la posición absoluta del cursor y los clics.\n\nAlgunas Interfaces de Usuario no se permiten a sí mismas a los RPAs porque son dinámicas. Lo cual significa que son difíciles de grabar.\n\nLa industria de RPA está haciendo todo lo posible para superar algunos de estos problemas inherentes al aspecto de grabar/reproducir de las herramientas. Algunos de estos obstáculos no pueden superarse debido al enfoque generalizado. Se deben evaluar otras opciones, como las pasarelas API y la automatización funcional.\n\n## La inteligencia artificial al rescate de los RPAs\n\nComo se menciona en las desventajas de la RPA, la reutilización de bots es un gran problema que la industria está tratando de solucionar. Una de las técnicas que están investigando es el uso de IA e Inferencia para manejar interfaces de usuario dinámicas y pequeños cambios en las aplicaciones sin volver a grabar los bots de RPA. El reconocimiento de patrones y el reconocimiento óptico de caracteres son dos áreas que se están utilizando para entrenar modelos de IA que se usarán para identificar campos y segmentos de interfaces de usuario.\n\nCon estos modelos de IA, los bots pueden ser más flexibles, siendo útiles para reutilizar en múltiples conjuntos de herramientas y procesos/flujos de trabajo similares. Otra área que los proveedores de RPA están investigando es la optimización de procesos utilizando IA y ML.\n\n## La migración de herencia es un viaje.\n\nEl mercado de RPA ha ganado nueva energía ya que las empresas buscan modernizar su infraestructura y procesos de TI. Automatizar los procesos manuales actuales a través de la grabación es una victoria rápida de la que muchas organizaciones se están beneficiando. Sin embargo, el RPA debería considerarse como un mecanismo provisional en lugar de un estado final. ¿Por qué? Muchos de los procesos de información actuales requieren sistemas y políticas heredados. Automatizar un proceso antiguo en una infraestructura nueva es similar a automatizar la creación de látigos para una fábrica de automóviles. Puede haber un beneficio al principio, pero a largo plazo, el proceso es altamente ineficiente y anticuado. No importa cuán rápido funcione de manera confiable, simplemente puede que no sea necesario.\n\n## Conclusión\n\nLas herramientas de Automatización de Procesos Robóticos son otro conjunto de herramientas que se pueden utilizar para ayudar a las organizaciones con su transformación digital desde una infraestructura y procesos heredados hacia una más moderna. Las herramientas por sí solas no son suficientes, y es necesario planificar cómo las utilizarás, gestionarás y eventualmente reemplazarás. Aquí tienes algunos consejos útiles al trabajar con estas herramientas.\n\nTrate los RPAs como servicios complejos que se ejecutan en su Multi-Hybrid Cloud.\n\nEjecuta tus bots RPA a través de flujos de trabajo de SecDevOps como otras aplicaciones.\n\nInyectar seguridad y autenticación en tiempo de ejecución en la herramienta RPA.\n\nEncuentra formas de reutilizar los bots de RPA en diferentes áreas de tu organización.\n\nTener un plan para reemplazar tu bot RPA con una integración simplificada.\n\n* Busca formas de disminuir las aplicaciones heredadas (reemplazar o eliminar).\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT28-es","image":"./episodes/edt-28/es/thumbnail.png","lang":"es","summary":"Una de las áreas en crecimiento para ayudar con la Integración de Legados y la automatización de la integración es el uso de herramientas y marcos de automatización. En los últimos 3 años, ha surgido un énfasis significativo en la automatización de flujos de trabajo con aplicaciones heredadas y nuevas aplicaciones conscientes de la nube para los trabajadores de la información. Estos conjuntos de herramientas se llaman herramientas de Automatización de Procesos Robóticos (RPA, por sus siglas en inglés). Darren Pulsipher, Arquitecto Principal de Soluciones, Sector Público, Intel, revisa la industria de la Automatización de Procesos Robóticos (RPA) y las ofertas de herramientas."},{"id":98,"type":"Episode","title":"Base de datos de hiperescala de próxima generación - Aerospike","tags":["aerospike","database","ingestion","optane","data","technology","pmem"],"body":"\r\n\r\n## Base de datos hiperescala de próxima generación\n\nLas bases de datos muestran su antigüedad, aún tardan un poco en obtener resultados. Aerospike, con la ayuda de la tecnología de Intel, logra romper barreras con rapidez, volumen y baja latencia. Tim Faulkes, Vicepresidente de Arquitectura de Soluciones en Aerospike, se une a Darren en este episodio para discutir los beneficios de su tecnología.\n\n## NoSQL - Un mercado emergente con múltiples tecnologías\n\nAerospike fue creado en 2009, con las primeras implementaciones de producción en 2011. Los cofundadores estaban conscientes de los desafíos que se avecinaban. Construyeron la arquitectura desde cero con la premisa de que sería confiable, de escala empresarial, nunca se caería y sería capaz de manejar ingestas masivas de datos. La arquitectura de Aerospike dependía en gran medida de las SSD, ya fueran unidades NVMe en las encarnaciones modernas o unidades SATA SAS. Ambas estaban en sus etapas iniciales, por lo que los fundadores se anticiparon a la dirección del mercado. Dependieron de esos dispositivos para obtener velocidad, y es por eso que la tecnología de Intel fue tan importante. Desde entonces, otras tecnologías de Intel, como la memoria persistente, han facilitado mucho las cosas.\n\n## Índices en DRAM, Datos en SSD\n\nAerospike se acerca a las bases de datos de manera diferente. Los SSD no son un disco duro más rápido. Un disco duro tiene sectores físicos y cabezales y cosas que debe mover para leer datos. Un SSD puede leer rápidamente miles de piezas de datos en paralelo. No funciona bien en discos rotativos, pero con NVMe/SSD, funciona increíblemente rápido. Nadie tiene el mismo tipo de rendimiento, ni siquiera una base de datos en memoria.\n\nLa arquitectura única está diseñada para un alto volumen, alto rendimiento y una latencia muy baja. Por ejemplo, algunos clientes realizan regularmente 20 millones de transacciones por segundo. Algunos llegan a utilizar petabytes de datos. Por lo general, los clientes utilizan hardware bastante sofisticado, pero el tiempo de ida y vuelta es crítico. A Aerospike le toma alrededor de 200 microsegundos buscar una pieza de datos y devolverla al cliente, no décimas o centésimas de segundo. Cuando necesitas obtener una gran cantidad de datos rápidamente, como en la detección de fraudes, o si estás ingresando una gran cantidad de datos, como en el IoT, vehículos autónomos, lecturas de sensores o dispositivos médicos, Aerospike puede realizar esos millones de transacciones por segundo en lecturas y escrituras. No necesitas ponerlo en un bus de mensajes y luego permitir que la base de datos notifique a los sistemas aguas abajo a través del bus de mensajes que tiene los datos. Ya está persistente.\n\n## Impulsando la innovación líder en la industria alrededor del mundo\n\n¿Cuáles son los casos de uso ideales para Aerospike? Hay mercados obvios como el Internet de las cosas (IoT), detección de fraudes y datos de sensores, pero es un mercado emergente. Aerospike comenzó en la tecnología publicitaria, donde la recuperación de datos debe hacerse en milisegundos o menos. Aquí es donde su buen rendimiento fue sometido a pruebas a gran escala.\n\nLas industrias adicionales donde Aerospike puede resultar útil son diversas. Un ejemplo es una gran compañía de telecomunicaciones en India. La infraestructura allí no siempre es confiable y a veces se interrumpen las llamadas. Esta empresa, con la tecnología de Aerospike, puede detectar en tiempo real cuando una llamada se interrumpe y pueden contactar inmediatamente al cliente para ofrecerle créditos u otra compensación para mantener su satisfacción.\n\n## Liquidación en tiempo real de pagos instantáneos.\n\nUna nueva área en la que Aerospike se ha involucrado intensamente es los pagos digitales en tiempo real. Esta no requiere específicamente una gran cantidad de datos o altos requisitos de rendimiento, pero lo que estas empresas necesitan es consistencia absoluta y disponibilidad absoluta, incluso en caso de perder un centro de datos.\n\nLa planta de fabricación también es un mercado emergente para la tecnología de Aerospike en cosas como la fabricación de semiconductores, donde se utiliza para almacenar datos de sensores y proporcionar análisis en tiempo real. Aerospike se destaca en cualquier industria donde haya una gran cantidad de datos y se necesiten rápidamente.\n\n## Rendimiento a escala - Resultados de pruebas independientes de terceros.\n\nHay muchas bases de datos con buena tecnología disponibles, pero hay desafíos a gran escala. Tomemos Redis, por ejemplo. Debido a que almacena toda su información en memoria, no se puede llegar a diez terabytes sin un costo significativo, mucho menos a cientos de terabytes o petabytes. Como Aerospike almacena la información en SSD, la diferencia de tiempo entre buscar datos en SSD y memoria es de aproximadamente 100 microsegundos. A lo largo de la escala de petabytes, Aerospike reemplaza tecnologías antiguas como Cassandra, que se escala muy bien pero carece de velocidad.\n\n## Costo Total de Propiedad\n\nAdemás, en comparación con esas tecnologías, las personas ahorran mucho dinero al cambiarse a Aerospike debido a que la cantidad de nodos disminuye drásticamente debido a su arquitectura única. El ahorro no solo está en el gasto de capital, sino también en el gasto operativo, ya que hay menos máquinas que supervisar.\n\nAerospike ha colapsado algunos de los niveles tradicionales en la arquitectura. A menudo, los sistemas heredados tienen una caché adicional delante de él para acelerarlo. Esto introduce complejidad. Aerospike no necesita una caché. No hay una gran cantidad de DRAM. Se basa en la velocidad de los SSD y la tecnología subyacente para obtener el rendimiento del almacenamiento en bruto sin una caché. Dado que la caché y los niveles de almacenamiento están colapsados, la solución se simplifica, lo que significa confiabilidad y velocidad incorporadas.\n\n## Ejemplo de Despliegue Continental\n\nLa consistencia en múltiples áreas geográficas es también un beneficio importante de la tecnología. La arquitectura moderna requiere baja latencia, lo que generalmente significa que hay un montón de grupos de H en los que los datos están cerca del usuario. De lo contrario, la velocidad de la luz se convierte en un factor. Aerospike puede hacer que todos los grupos de H se comuniquen entre sí, por lo que si se cambia un registro en un grupo, se propagará automáticamente a los demás grupos de manera asíncrona. Sin embargo, con algunos usos, como los pagos digitales, debe haber una fuerte consistencia entre los grupos, por lo que podrían optar por replicar los datos de manera síncrona en su lugar. Preferirían leer desde la copia local de los datos. Las lecturas se vuelven muy rápidas; las escrituras se ven afectadas por la velocidad de la luz, pero garantizan consistencia en distancias geográficas. Entonces, esto no es una resolución de conflictos, es una forma de evitar conflictos. Tener la capacidad de distribuir esas escrituras de alta consistencia en todo el mundo y tener múltiples sistemas de registro tiene un enorme potencial y valor.\n\n## Aerospike Connect for Spark se traduce al español como Conexión de Aerospike para Spark.\n\nPor ejemplo, Aerospike actualmente se integra con Spark, una tecnología analítica que requiere que sus datos estén en memoria. Al cargar los datos desde Aerospike, el marco de datos de Spark puede procesar decenas o cientos de terabytes con enormes ahorros de costos y velocidad. La integración con otras herramientas de IA es una fuerza impulsora que abrirá algunas puertas emocionantes.\n\nObtener datos en Aerospike es simple y directo. Dado que las aplicaciones tienen una capa de API como la mayoría de las bases de datos, Aerospike puede recibir datos de fuentes estándar de la industria como Kafka y Janus. La capacidad bidireccional de comunicarse con buses de mensajes tiene sentido, porque no solo Aerospike puede recibir datos de estas fuentes, sino también enviarlos. Si Aerospike es la fuente de verdad cuando escribes un registro, puedes enviarlo a sistemas aguas abajo.\n\n## Aerospike Connect para Kafka\n\nSi desea utilizar Aerospike para la ingestión en lugar de un motor de ingestión en el front-end, Aerospike puede determinar si desea descomponer los datos o mantenerlos en el mismo formato y simplemente colocarlos en Aerospike; Con su marco de complementos, es una llamada de API. Cuando los datos se almacenan, se puede notificar a un bus de mensajes para el procesamiento posterior. Esto elimina uno de los pasos de ingestión y aumenta el rendimiento total.\n\nEscalar a miles de clientes es posible debido a la forma en que el cliente de Aerospike está diseñado como multi-threaded para escalar en un solo proceso y asimilar una gran cantidad de datos con eso. El cliente es inteligente en aras de la velocidad. Cualquier registro se envía directamente desde el cliente al nodo que posee esa pieza de datos. No hay intermediarios de ningún tipo. Esto significa que el cliente debe conocer todos los nodos, por lo que existen límites de conexión finitos, lo que hace que cientos de miles de clientes sean impracticables.\n\n## Ejemplo de despliegue continental\n\nSi, sin embargo, tienes cientos de miles de clientes, es porque tienes una población distribuida a nivel mundial. Si estuvieran hablando con un solo grupo, la velocidad de la luz se convierte en un gran problema. Existen formas de configurar grupos más pequeños en los bordes que comparten información, o que se comunican con un sistema central de registro en un modelo de concentrador y distribuidores, por lo que se puede hacer.\n\n## Aerospike es una base de datos para aplicaciones de registro del sistema.\n\nA medida que los datos se mueven por el mundo, diversas capas de privacidad entran en juego y se aplican estándares como el GDPR y el CCPA, por lo que los clientes deben conocer el origen de los datos. Aerospike tiene la flexibilidad de examinar los datos de forma minuciosa y, en función del contenido de los datos, saber cuándo y dónde se pueden enviar. Esto forma parte de la configuración de envío de datos de Aerospike que es transparente para la aplicación. No es necesario cambiar nada en las aplicaciones, solo en la configuración.\n\nAerospike también se asegura de que los datos estén seguros. Al igual que todas las bases de datos de alta capacidad empresarial, Aerospike admite encriptación en reposo y en tránsito, así como la integración de seguridad de elementos como Hashicorp Vault. Se puede utilizar y se utiliza como un sistema de registro.\n\nUn objetivo fundamental en Aerospike es utilizar las últimas innovaciones y tecnología para el éxito de los clientes. Por ejemplo, cuando Intel lanzó PMem, lo utilizaron en modo de acceso directo a la aplicación para aprovechar al máximo sus capacidades. Esto significa que para una actualización continua, digamos, de un petabyte de información, el tiempo que lleva es reducido. Aerospike almacena ese petabyte en tal vez 50 o 60 nodos en lugar de miles de nodos, y cuando un nodo se apaga, no es necesario reconstruirlo, por lo que el tiempo de reinicio es muy rápido.\n\nEn general, Aerospike ofrece seguridad, alta disponibilidad, velocidad, baja latencia, escalabilidad, consistencia y bajo costo de propiedad.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Tim Faulkes"],"link":"/episode-EDT29-es","image":"./episodes/edt-29/es/thumbnail.png","lang":"es","summary":"Las bases de datos están mostrando su antigüedad, todavía tardan un poco en obtener resultados. Aerospike, con la ayuda de la tecnología de Intel (Optane), logra romper con la velocidad, el volumen y la baja latencia. Tim Faulkes, Vicepresidente de Arquitectura de Soluciones en Aerospike, se une a Darren Pulsipher, Arquitecto Principal de Soluciones en Intel, para discutir los beneficios de la tecnología de Aerospike."},{"id":99,"type":"Episode","title":"Beneficios de la arquitectura Multi-Cloud","tags":["multicloud"],"body":"\r\n\r\n## Agilidad\n\nLa agilidad es la capacidad de moverse rápidamente para adaptarse a las condiciones cambiantes en nuevas tecnologías, nuevas aplicaciones y nuevas amenazas como seguridad y competencia. Un modelo multi-híbrido permite la habilidad de mover cosas entre nubes privadas y públicas, o incluso de infraestructuras heredadas a nubes. Puede ser tentador para los equipos de desarrollo utilizar inicialmente la nube pública bajo plazos ajustados porque pueden crear y desactivar rápidamente la infraestructura, pero puede haber problemas de integración en las etapas finales de implementación de las aplicaciones en el producto. Esto puede resultar en retrasos costosos. Al utilizar el modelo multi-híbrido, muchos de estos puntos de integración se exponen al equipo de desarrollo tempranamente. El desarrollo de aplicaciones utiliza esto para fallar rápido y temprano en el proceso de desarrollo.\n\n## Flexibilidad\n\nLa flexibilidad en la infraestructura es un pariente cercano de la agilidad. Mientras que la agilidad implica la capacidad de moverse rápidamente, la flexibilidad se refiere a la capacidad de cambiar. Para lograr la máxima flexibilidad, los desarrolladores necesitan tener la habilidad de desplegar una aplicación en cualquier lugar, ya sea en nubes privadas o públicas, o en infraestructuras heredadas, sin quedar atrapados en los servicios o infraestructuras de una única nube.\n\nAl implementar una plataforma de gestión en la nube (CPM) en una arquitectura multi-híbrida, las cargas de trabajo se pueden redirigir fácilmente a diferentes nubes según el costo, la seguridad y la confiabilidad.\n\nUn ejemplo de las consecuencias de la falta de flexibilidad es la decisión temprana de Netflix de utilizar únicamente un proveedor de servicios en la nube. Esta nube pública tuvo un problema de infraestructura que resultó en tiempo de inactividad, y Netflix no pudo transmitir durante varias horas en la costa este de Estados Unidos. Después de ese desastre, Netflix implementó una solución de múltiples nubes para poder migrar rápidamente a otra nube en caso de problemas. Además, ahora tienen la capacidad de moverse hacia donde tenga más sentido en cualquier momento, según el costo, la seguridad y la confiabilidad.\n\nLas aplicaciones de una nube híbrida multiplataforma son la portabilidad y la flexibilidad operativa. Al no estar limitados a la forma de hacer las cosas de una sola nube, los desarrolladores podrán escribir código que pueda ser fácilmente transportado entre diferentes nubes. Además, tendrás la flexibilidad operativa para mover las cargas de trabajo entre nubes y ofrecer a tus clientes lo que deseen de manera segura, consciente de los costos y confiable.\n\n## Rendimiento predictivo\n\nLas nubes públicas a menudo tienen un costo: \"vecinos ruidosos\", que pueden afectar el rendimiento predictivo o la calidad de servicio (QoS).\n\nCuando se utiliza una nube pública, muchas veces no sabes qué o quién más está ejecutando en la misma máquina, matriz de almacenamiento o red que tú. Para algunas cargas de trabajo, esto no es un problema. Sin embargo, si tienes una aplicación en la que necesitas un rendimiento más predecible, los vecinos ruidosos o aquellos que monopolizan el ancho de banda, la E/S de disco, la CPU y otros recursos pueden interferir con los resultados de QoS que necesitas.\n\nLas nubes privadas también pueden verse afectadas por vecinos ruidosos, pero dado que eres dueño de la infraestructura y las aplicaciones son propias, tienes la capacidad directa de administrarlas. Un vecino ruidoso en la nube pública es como vivir en un edificio de apartamentos donde tienes opciones limitadas para lidiar con los juerguistas de al lado. Por otro lado, los vecinos ruidosos en tu nube privada son como vivir en una casa con niños indisciplinados que puedes controlar de inmediato restringiendo los recursos.\n\nUna estrategia de nube híbrida te brinda la capacidad de colocar cargas de trabajo y aplicaciones \"sensibles\" en nubes privadas y otras cargas de trabajo y aplicaciones en infraestructuras de nube pública para obtener eficiencia en costos y recursos. Muchas herramientas híbridas te permiten caracterizar cargas de trabajo con requisitos de QoS para ayudar en la colocación automática y óptima de las cargas de trabajo en diferentes infraestructuras de nube.\n\n## Seguridad y cumplimiento normativo.\n\nHay algunos peligros al hacer las cosas de manera automatizada en las nubes públicas y privadas. Sin embargo, si la seguridad se incorpora en la arquitectura multi-híbrida, esta automatización se convierte en un beneficio porque significa que se puede imponer un perfil de seguridad en todos tus activos en la nube, ya sean privados o públicos, además de la infraestructura heredada. Este perfil de seguridad común se aplica en todas partes y las aplicaciones se despliegan basándose en esos perfiles.\n\nPor ejemplo, en una nube privada, si deseas un tipo específico de seguridad, como vincular una aplicación determinada a una máquina determinada que solo se ejecuta en esa nube privada, fácilmente puede ser un requisito en el perfil de un sistema multicráter.\n\nEn un sistema multi-híbrido, también hay muchas herramientas excelentes para auditar y monitorear tu infraestructura. No solo puedes monitorear lo que está sucediendo en tu nube privada, sino también en la nube pública, lo que te alertará sobre posibles ataques maliciosos que podrían infectar tu nube privada o tus activos de infraestructura heredados.\n\n## Eficiencia\n\nHay muchas ideas contradictorias sobre la eficiencia. Si observas diferentes calculadoras de coste total de propiedad (TCO), encontrarás respuestas diferentes sobre si las nubes privadas o públicas son las más rentables. Una solución de nube multi-híbrida puede resolver este dilema para tu organización a través de una mayor visibilidad y aprovisionamiento dinámico.\n\nCon una arquitectura de nube híbrida y un orquestador inteligente, tu orquestador puede utilizar telemetría de tus nubes privadas y públicas, así como de tu infraestructura heredada, para tomar decisiones óptimas sobre dónde debería cargar la carga de trabajo tanto en la actualidad como en el futuro. Otro beneficio de esta visibilidad es que puedes decidir si es eficiente que las aplicaciones se ejecuten de forma continua. Un ejemplo de esto es cómo ayudamos al sistema del gobierno de Canadá a funcionar de manera más eficiente al reducir los costos asociados con la ejecución de una aplicación en la nube pública cuando nadie la está utilizando. En lugar de ejecutar una aplicación específica las 24 horas del día, ahora la ejecutan durante 18 horas, 5 días a la semana, según el uso real.\n\nEsta visibilidad también te ayudará a identificar y eliminar cargas de trabajo y aplicaciones en fin de vida (EOL), lo cual te ahorrará dinero real. Para la nube privada, esto libera recursos que pueden ser utilizados para otras cargas de trabajo, aumentando así tu eficiencia. Además, los intermediarios de la nube en la arquitectura multi-híbrida buscarán la opción más económica sin comprometer la calidad de servicio (QoS) para una carga de trabajo específica. Esto reduce el costo general de ejecutar la carga de trabajo y también te brinda visibilidad sobre el costo real de utilizar una nube pública o privada en particular.\n\n* Visibilidad de costos en todas las nubes e infraestructura heredada.\n\nImpulsar cargas de trabajo y aplicaciones hacia los costos más bajos manteniendo los mismos acuerdos de nivel de servicio.\n\n* Impulsar un mayor aprovechamiento de la infraestructura de nube privada.\n\n## Llamado a la acción\n\nLas arquitecturas de nube multi-híbrida están brindando a los directores de informática (CIOs) la capacidad de adelantarse a las demandas de sus clientes, pero todavía hay mucho trabajo duro que debe realizarse. Construir una estrategia de nube multi-híbrida incluye cambios organizativos, conductuales y técnicos que no pueden suceder de la noche a la mañana. Desarrollar una visión y un plan arquitectónico sólidos es clave para implementar una estrategia de nube multi-híbrida que pueda aprovechar las fortalezas de las nubes multi-híbridas y evitar la ineficiencia del \"objeto brillante\" del mes en las industrias técnicas.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT3-es","image":"./episodes/edt-3/es/thumbnail.png","lang":"es","summary":"Una arquitectura de nube multi-híbrida permite a las organizaciones aprovechar los beneficios de las nubes privadas y públicas, optimizando los recursos y la eficiencia de costos. Este modelo tiene cinco ventajas principales: agilidad, flexibilidad, rendimiento predictivo, seguridad y cumplimiento, y eficiencia."},{"id":100,"type":"Episode","title":"Portabilidad de aplicaciones con OneAPI.","tags":["aiml","devops","compute","technology","process","oneapi","aiops","developer","people"],"body":"\r\n\r\nCon oneAPI, Intel ha creado un entorno de software unificado para el desarrollo, orientado al procesamiento de datos. Es una programación paralela basada en código abierto C++. Diversos tipos de bibliotecas como MKL, DNN de Intel y otras fuentes abiertas forman parte del kit de herramientas oneAPI, junto con accesorios como un traductor CUDA. Puedes desarrollar software en oneAPI y luego apuntarlo a diferentes bibliotecas dependiendo de lo que estés haciendo. Ha eliminado la complejidad de aprender un lenguaje diferente para diferentes arquitecturas.\n\n## Iniciativa de la Industria oneAPI: una alternativa a las soluciones de un solo proveedor.\n\nBásicamente, un ingeniero de software puede escribir código una vez y este se ejecutará en diferentes procesadores: CPU, GPU, FPGA, NPU y VPU. Dependiendo de la arquitectura y bibliotecas que estés utilizando, podría ser necesario hacer una recompilación, pero no se requiere volver a escribir el código.\n\n## Bibliotecas de API poderosas\n\nEsta versión es solo el primer paso; Intel y otros continuarán diseñando con la adición de aceleradores de IA, por ejemplo. La idea es que evolucionará para brindar a los desarrolladores mucha más flexibilidad, y esa abstracción permitirá que muchas personas puedan diseñar y codificar de manera más simple, especialmente desde una perspectiva de ciencia de datos e IA.\n\nPrácticamente hablando, un ingeniero de software podría escribir algo en su portátil, probarlo allí y luego usar el mismo código para ejecutarlo en una nube completamente equipada con procesadores neuronales, GPUs o FPGAs. Esto podría ser especialmente útil en el sector público donde los ingenieros están escribiendo aplicaciones especiales que procesan en el borde, tal vez con un FPGA. No tendrán que tener un entorno completo para hacer su trabajo.\n\nOtro aspecto emocionante es que una vez que Intel integre el aprendizaje automático en el sistema, oneAPI podría revisar el código y especificar qué partes serían mejores en diferentes procesadores. Enviaría el código a los lugares correctos para obtener la mejor velocidad y rendimiento.\n\n## Iniciativa de la Industria oneAPI - Alternativa a una Solución de un Solo Proveedor\n\nHay muchos marcos de inteligencia artificial disponibles, pero oneAPI permite la migración de cualquier tipo de código desde algo propietario a un lenguaje de programación de código abierto. Está basado en SYCL y desarrollado bajo un consorcio de toda la industria llamado Khronos group, por lo que es un tipo de marco de desarrollo.\n\n## Potentes bibliotecas de API\n\nOneAPI tiene veinte a treinta bibliotecas como MKL, bibliotecas para redes neuronales y aprendizaje automático, CNN abierta o DNN. Todas son abiertas, parte del consorcio más grande.\n\nAdemás, Intel está en proceso de agregar el framework de tensorflow y las bibliotecas a oneAPI. Muchos frameworks ya han sido optimizados por Intel y se están incorporando o utilizando las mismas bibliotecas para que los usuarios puedan utilizarlo o construir sobre él.\n\n## Recursos\n\nIntel puso a disposición de manera general el proyecto oneAPI a principios de noviembre de 2020, y fue uno de los puntos destacados de la convención virtual Super Computing del 17 al 19 de noviembre.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT30-es","image":"./episodes/edt-30/es/thumbnail.png","lang":"es","summary":"Con oneAPI, Intel ha creado un entorno de software unificado para el desarrollo, orientado al procesamiento de datos. Gretchen Stewart, Científica de Datos Principal en el Sector Público de Intel, discute esta tecnología con Darren Pulsipher, Arquitecto de Soluciones Principal de Intel, que elimina la necesidad de utilizar un lenguaje diferente para diferentes arquitecturas."},{"id":101,"type":"Episode","title":"Mejorando el valor del empleado con Catalítica","tags":["rpa","automation","catalytic","compute","process","technology"],"body":"\r\n\r\n## Catalytic está diseñado específicamente para operaciones y procesos de negocio recurrentes.\n\nLa tecnología RPA tiene su origen en la automatización de pruebas y las macros de Excel, pero su linaje empresarial proviene de la externalización. Sin embargo, Catalytic considera su linaje como software, con su sistema como la siguiente evolución en la creación de software personalizado. El interés del cofundador y CEO, Sean Chou, se encuentra en las operaciones y eficiencia de la oficina trasera. Ese enfoque específico continúa permitiendo a Catalytic emplear la inteligencia artificial de forma completa y les permite construir algo que es accesible sin necesidad de código.\n\nCatalytic utiliza inteligencia artificial de varias formas tácticas dentro de la aplicación para ayudar a reducir la necesidad de experiencia.\n\nPor ejemplo, una de las partes más difíciles de aprender cualquier plataforma es comprender sus capacidades. Una forma de usar IA para ayudar a contrarrestar este problema es mediante el uso del procesamiento del lenguaje natural. Las personas pueden simplemente escribir lo que intentan lograr y la aplicación puede inferir y sugerir qué acción sería mejor para ayudarte a completar este paso. Básicamente, se trata de una forma de mejorar la experiencia del producto y disminuir el nivel de experiencia necesario.\n\nOtro ejemplo es que dentro del flujo de trabajo, hay seis módulos diferentes de acciones distintas, cada uno nombrado lógicamente según su objetivo de resolución, como procesamiento de datos, ensamblaje de documentos, etc. Dentro de cada módulo, hay diferentes acciones de IA que las personas pueden utilizar en su proceso, como reconocimiento óptico de caracteres (OCR), análisis de sentimientos (¿está enojada la persona que escribió el correo electrónico?) y otras acciones de procesamiento de lenguaje natural.\n\nHay otros elementos que son menos de IA y más acciones de utilidad, como proporcionar información sobre una persona basada en su dirección de correo electrónico.\n\nMucho datos fluyen a través de los procesos de oficina trasera. Desafortunadamente, la vista del sistema de los datos a menudo está muy fragmentada. El flujo de trabajo catalítico puede cortar a través de los sistemas, capturando todo. Los datos se almacenan en tablas de datos, y es fácil construir modelos de aprendizaje automático basados en esas tablas, para que puedas predecir los resultados futuros de otros flujos de trabajo.\n\nCatalytic utiliza la inteligencia artificial en estos y otros contextos específicos para impulsar la eficiencia y brindar ideas y visibilidad.\n\n## Cerrar la última fase de la digitalización construyendo soluciones de flujo de trabajo en torno a cómo las personas trabajan.\n\nPor ejemplo, la inteligencia operativa a través de la IA se captura en la plataforma a través de una función llamada Insights. Para cada flujo de trabajo, esta pestaña puede decirte cuánto tiempo llevan los diferentes pasos, con qué frecuencia se rompen y si es un error humano o una falla de integración. Te ofrece oportunidades de mejora. Al mismo tiempo, ese algoritmo de aprendizaje automático puede comenzar a aprender a predecir los resultados de los valores de campo. Por lo tanto, el aprendizaje automático se puede utilizar de múltiples formas relacionadas.\n\n## A pesar de las inversiones continuas, todavía existe una brecha de \"última milla\" que se llena de forma manual.\n\nLa mayoría de los clientes que han invertido en múltiples tecnologías a lo largo de los años aún sienten que hay una brecha entre la inversión y los resultados comerciales que desean. Esa brecha suele ser llenada por personas que realizan tareas que el software debería hacer en su lugar, como copiar y pegar algo de un sistema a otro. A veces, agregar software a la mezcla complica las cosas al agregar pasos y capacitación adicionales. La solución de Catalytic a estas ineficiencias es un sistema que es reutilizable y ampliable.\n\nEs imposible que cualquier software estándar cumpla con las necesidades individuales de cada negocio, y resulta ineficiente y lleva mucho tiempo ver el valor final. Por lo tanto, las empresas deben adaptarse al software o adaptarlo para satisfacer sus necesidades comerciales. El valor en una historia de tipo WordPress es evidente. Si el poder está realmente en manos de los empleados, se puede tener un software verdaderamente personalizado. Catalytic desea poner ese poder en marcha para cualquier proceso empresarial utilizando RPAs combinados con acciones.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sean Chou","Neil Bahandur"],"link":"/episode-EDT31-es","image":"./episodes/edt-31/es/thumbnail.png","lang":"es","summary":"En la primera parte de dos episodios, Sean Chou, CEO de Catalytic, y Neil Bahandur, Jefe de Asociaciones de Catalytic, se unen a Darren para hablar sobre la tecnología de Catalytic y cómo los RPAs pueden ayudar a los empleados a volverse más valiosos a través de la automatización de procesos repetitivos de la oficina."},{"id":102,"type":"Episode","title":"Enfoque sin código (RPA) para la eficiencia de la oficina de respaldo.","tags":["rpa","automation","catalytic","compute","process","technology"],"body":"\r\n\r\n## ¿Qué es Catalítico?\n\nCatalytic es una plataforma en la nube sin código para construir soluciones de flujo de trabajo que mejoran y automatizan las operaciones de su oficina central.\n\nUna analogía útil para ayudar a las personas a entender lo que Catalytic puede hacer por el flujo de trabajo de la oficina es lo que Wordpress hizo por la publicación web. Antes de Wordpress, crear un sitio web era una tarea compleja que involucraba a varias personas con una variedad de habilidades especializadas. Con Wordpress y el software más avanzado posterior, crear un sitio web es una tarea relativamente sencilla. Un diseñador o experto en contenido, por ejemplo, puede crear fácilmente un sitio web por su cuenta.\n\nEl objetivo de Catalytic es simplificar de manera similar los sistemas desordenados y complejos de la oficina de atrás y crear eficiencia y soluciones a través de la automatización de procesos robóticos (RPA). Ahora, alguien que tenga un poco de habilidad técnica debería ser capaz, con la tecnología de Catalytic, de crear de forma independiente una solución para la oficina de atrás.\n\nUn miedo común de que los RPAs causen la pérdida de empleos es falso; en realidad, crea nuevas oportunidades más valiosas. Por ejemplo, con Wordpress, ahora existe el rol de desarrollador de Wordpress, por lo que esta progresión no ha disminuido las habilidades de los desarrolladores, sino que ha añadido capas de formas en las que las personas pueden utilizar esas habilidades en algo más escalable. Los empleados de oficina también pueden usar sus habilidades para agregar valor o mejorar la experiencia del cliente en lugar de enfocarse en tareas mundanas inherentemente ineficientes.\n\n## La Evolución y Diferenciación de los Catalíticos\n\nCatalytic se diferencia de sus competidores, en primer lugar, por sus raíces. La industria actual de la RPA proviene de dos líneas evolutivas, una surgida de las macros de Excel y otra de las pruebas de interfaz de usuario. Catalytic proviene del mundo de Dev Ops, automatizando procesos de la oficina mediante scripts en lugar de tener una visión centrada en la pantalla. Cuando Sean Chou cofundó Catalytic, pensó en términos de gestión de proyectos y en cómo orquestar los procesos habituales de negocio.\n\n## Catalytic está diseñado específicamente para operaciones y procesos comerciales recurrentes.\n\nChou se dio cuenta de que, dado que el 85% o más de los negocios son negocios habituales, muchas operaciones podrían automatizarse para que la plataforma pudiera realizar algunas de esas tareas. Comenzó con un sencillo sistema de notificación automatizada que reemplazó el envío de correos electrónicos por parte de los empleados, lo que llevó a Catalytic a crear más y más acciones. Para ampliar las capacidades de la plataforma, Catalytic creó un ecosistema con socios de terceros como Google para aprovechar sus tecnologías. A través del ecosistema, también se obtiene el beneficio del trabajo empresarial componible, donde las personas pueden crear un flujo de trabajo en la plataforma y guardarlo, esencialmente, como una unidad de trabajo individual. Por ejemplo, si hay seis pasos estandarizados en la captura de un registro de cliente, se puede crear una vez y guardarla como una acción en la plataforma, luego compartirla con todo el equipo. Esta forma de compartir imita el enfoque de los desarrolladores de software para resolver problemas difíciles compartiendo código, pero en lugar de código, se trata de acciones. La composición, entonces, se vuelve mucho más fácil.\n\nCatalytic ha adoptado un enfoque reflexivo para el suministro, implementación y gestión de todo en su plataforma. La nube no es un pensamiento posterior; crearon una plataforma en la nube desde cero. Todo se centra en la nube, aunque también tienen la capacidad de trabajar con sistemas locales y entornos híbridos.\n\n## Catalytic está diseñado para permitir un programa de desarrollo de ciudadanos de clase empresarial.\n\nEl sistema fue creado en un entorno DevOps y funciona como un gestor de compilaciones para el negocio, donde los profesionales del negocio pueden hacerlo ellos mismos. Los antiguos sistemas de compilación como Clear Case eran difíciles de manejar y Catalytic ha simplificado las cosas. No solo elimina pasos adicionales, sino que también cambia la división del trabajo. Catalytic se dirige a personas que están más cerca del proceso real para evitar entregas. La eficiencia es el concepto central. Un sistema de compilación eficiente que detecte y rechace los errores antes de intentar entregarlos es crucial para el éxito del proceso automatizado.\n\nLa magia de los sistemas de construcción es que se vuelven más poderosos a medida que incorporan más elementos de tu infraestructura. Por ejemplo, podría interactuar con tu sistema de control de versiones para obtener el código, podría trabajar con tus servidores para implementar, o podría interactuar con Rational Robot para realizar pruebas automatizadas. Por supuesto, todo gira en torno a las acciones. Cuantas más acciones estén digitalizadas, más puede capturar el sistema y más poderoso se vuelve.\n\nOtra cosa que diferencia a Catalytic es el énfasis en que los empleados realicen trabajos de alto valor. El concepto de valor bajo y valor alto en RPA es común, pero en lugar de utilizarlo como una espada para dividir grupos, Catalytic quiere utilizarlo como un escudo para proteger a las personas, ayudarles a aprovechar al máximo su tiempo y ofrecer una ventaja competitiva.\n\nEn el próximo podcast, Darren, Sean y Neil continuarán la conversación sobre el enfoque sin código para la eficiencia en las operaciones de la oficina central.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sean Chou","Neil Bahandur"],"link":"/episode-EDT32-es","image":"./episodes/edt-32/es/thumbnail.png","lang":"es","summary":"El episodio de hoy es la segunda parte de una conversación con Sean Chou, CEO de Catalytic, y Neil Bahadur, Jefe de Asociaciones de Catalytic. Hablan con Darren sobre su enfoque sin código para la eficiencia de la oficina trasera con una plataforma que utiliza tecnología de RPA y AI."},{"id":103,"type":"Episode","title":"El apagón de AWS de noviembre de 2020","tags":["aws","cloudoutage","compute","csp","cloud","process","cloudreliability","multicloud","reliability"],"body":"\r\n\r\n## Lo que aprendimos del apagón de AWS.\n\nAmazon Web Services (AWS) sufrió un importante tiempo de inactividad en un momento inoportuno: el día antes del Día de Acción de Gracias en 2020. Dado que publicaron los registros de su servicio, resulta interesante revisarlos para ver qué sucedió y qué podemos aprender.\n\n## Cronología del Corte de Servicio de AWS\n\nEl miércoles 25 de noviembre de 2020, alrededor de las 3:00 am PST, AWS estaba actualizando la región Este, agregando servidores para aumentar la capacidad de Kinesis. Kinesis es un servicio de IA popular que realiza coincidencias de patrones en archivos de registro y archivos de video. Aproximadamente una hora y media después de la actualización, las alarmas del servidor comenzaron a mostrar errores en los registros de Kinesis. Bastante rápido, para las 8 am, identificaron inicialmente un par de posibles causantes en los servicios de front-end. Kinesis tiene varios servicios más pequeños, un grupo de servicios de front-end y un grupo de servicios de back-end. De acuerdo con la arquitectura del sistema, cada vez que un servicio de back-end se está ejecutando, se dispara un hilo en el front-end. Uno de los problemas fue que el front-end alcanzó el umbral de hilos a medida que se añadían servidores al back-end durante la actualización. Una vez que determinaron este problema, realizaron una solución temporal con actualizaciones (parches) para el sistema operativo y Kinesis volvió a estar en línea alrededor de las 10:30 pm y totalmente restaurado para la 1:15 am del 26 de noviembre. En total, Kinesis no estuvo completamente operativo durante 21 horas y, aunque no todos utilizan esta herramienta de IA, el impacto fue amplio.\n\n## Kinesis Impact would be translated to Spanish as \"Impacto de la cinética\".\n\nVarios otros servicios de Amazon utilizan Kinesis, como Amazon Cognito y CloudWatch, y sufrieron diferentes grados de interrupción. Cognito estuvo sobrecargado hasta aproximadamente las 2 pm; CloudWatch estuvo fuera de servicio hasta aproximadamente las 10 pm. En un efecto dominó, servicios dependientes de CloudWatch como Lambda y EventBridge también estuvieron inactivos. Dado que EventBridge no estaba disponible, los servicios de contenedor LCS y LKS también se vieron afectados.\n\nLas interrupciones solo ocurrieron en la región Este, y AWS rápidamente agregó capacidad en otras regiones para evitar que Kinesis fallara de manera similar. Pero durante la interrupción, los usuarios en la región Este se encontraron con un problema desconcertante, ya que sus paneles de servicios y paneles de servicios personales no estaban recibiendo información y mostraban falsos positivos. Por lo tanto, hubo muchas otras organizaciones de tecnología de la información investigando los problemas ya que no estaban recibiendo la información correcta. Sorprendentemente, AWS ha sido transparente sobre todo el incidente, por lo que es una gran experiencia de aprendizaje.\n\n## Lecciones aprendidas\n\nUna de las primeras lecciones es que las operaciones simples en la infraestructura, como aumentar la capacidad, deben ser comprendidas y planificadas. Obviamente, AWS no solo realizó mejoras sin pensarlo, pero no comprendieron completamente el impacto. Incluso si una operación parece rutinaria, siempre es una buena idea llevar las cosas al límite en un entorno de prueba durante las actualizaciones, aunque lleve más tiempo. Esto es especialmente importante con los servicios críticos para otros servicios principales dependientes.\n\nAdemás, las arquitecturas de servicio o microservicio deben comprender todo su árbol de dependencia de servicios para poder solucionar problemas cuando haya interrupciones. Además, es importante estar tan poco acoplado a una dependencia como sea posible e incluir programación defensiva con microservicios para evitar el efecto dominó que ocurrió en este caso.\n\nOtro problema a tener en cuenta son las dependencias circulares. Si, en una cadena de microservicios que dependen entre sí, uno tiene un contratiempo, todo puede detenerse por completo, consumiendo recursos y sin lograr nada.\n\nAl desarrollar microservicios, ten en cuenta que no siempre tendrás conectividad con los servicios de los que dependes. Escribir programas que puedan funcionar en un estado degradado, o al menos indicar que un servicio no está funcionando, puede ahorrar tiempo y problemas. En esta interrupción, recuerda que los paneles de control seguían funcionando en verde aunque no se recibía nueva información.\n\n## ¿Quién manejó mejor la interrupción del servicio?\n\nMuchas empresas se vieron afectadas por la interrupción de AWS, incluyendo algunas propiedad de Amazon mismo. Algunas atravesaron la interrupción sin problemas, casi indemnes, mientras que otras tuvieron más dificultades para recuperarse. Aquellas que pudieron adaptarse rápidamente tenían una estrategia de nube híbrida múltiple, por lo que contaban con nubes alternativas como respaldo. Algunas utilizaron una región diferente de AWS, mientras que otras utilizaron Google o Azure, e incluso algunas volvieron a sus propios centros de datos o sitios web externos. Al menos, los sitios web mostraban un mensaje de que estaban experimentando problemas en lugar de un error 404.\n\nLas empresas que no estaban utilizando algunos de los servicios especializados de Amazon también tuvieron un mejor desempeño. Por ejemplo, las ofertas de contenedores administrados EKS y ECS, así como Lambda, se vieron gravemente afectadas y estuvieron inactivas durante un período de tiempo considerable, por lo que aquellos que dependían de estos servicios se quedaron sin opciones.\n\nDurante la interrupción, los servicios de escalado automático no funcionaban correctamente, por lo que cualquier empresa que tuviera mucho tráfico en ese momento tuvo que averiguar qué estaba sucediendo y escalar los servicios manualmente. Esto fue un problema mayor de lo que normalmente sería, ya que era el día antes del Día de Acción de Gracias, un momento en el que muchos consumidores viajan y compran en línea. Por ejemplo, Etsy se mantuvo activo, pero no pudieron escalar tanto como lo harían normalmente, lo que llevó a una disminución en las ventas.\n\nLas empresas que tienen su propio monitoreo externo también tuvieron un mejor desempeño. Incluso algunas alertaron a AWS sobre las interrupciones. No dependían únicamente del tablero de salud de AWS, sino que tenían su propio monitoreo en sus servidores.\n\n¿Cuál es la lección principal que debemos aprender de esto? Las organizaciones deben apropiarse de sus recursos en la nube, al igual que cualquier servicio público. Así como tener un generador de respaldo para la electricidad que garantice la continuidad del negocio en caso de un corte de energía, las empresas deben seguir las mejores prácticas para los servicios en la nube. Esto significa tener una nube de respaldo que pueda mantener el funcionamiento, incluso con capacidad reducida, es esencial para resistir una tormenta como el fallo de AWS.\n\n## Recursos\n\nTraduce lo siguiente al español: * https://aws.amazon.com/message/11201/\n\nhttps://aws.amazon.com/message/11201/ no se puede traducir directamente, ya que es una URL específica que contiene información sobre el sitio web de Amazon Web Services (AWS) y un mensaje. Para traducir el contenido de la página a la que se accede a través del enlace proporcionado, deberías utilizar un traductor en línea y copiar y pegar el texto dentro del sitio de traducción.\n\nEnlace: https://www.theverge.com/2020/11/25/21719396/amazon-web-services-aws-outage-down-internet\n\nTraducción: El servicio de Amazon Web Services (AWS) sufre una interrupción que deja sin conexión a Internet.\n\nhttps://www.zdnet.com/article/amazon-heres-what-caused-major-aws-outage-last-week-apologies\n\nAquí está lo que causó la importante interrupción del servicio de AWS la semana pasada: disculpas.\n\nhttps://www.wsj.com/articles/amazon-web-services-hit-by-outage-11606326714\n\nAmazon Web Services afectado por una interrupción\n\nhttps://www.washingtonpost.com/technology/2020/11/28/amazon-outage-explained\n\nExplicación de la interrupción de Amazon\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT33-es","image":"./episodes/edt-33/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones del Sector Público en Intel, habla sobre las lecciones aprendidas de la interrupción de AWS en noviembre de 2020 y soluciones preventivas para navegar por dichas interrupciones."},{"id":104,"type":"Episode","title":"Abrazando la Evolución del Espacio de Trabajo.","tags":["covid","mfa","remoteworker","cybersecurity","people","technology","vdi","vpn"],"body":"\r\n\r\n## Los lugares de trabajo digitales están evolucionando.\n\nDado que ya hemos superado el caos inicial de las transiciones necesarias por la pandemia, necesitamos preguntarnos qué viene a continuación y, mirando hacia adelante, cómo podemos aprovechar lo que aprendimos para invertir conscientemente en donde queremos estar.\n\nUna lección importante fue que las organizaciones que ya eran ágiles se desempeñaron bien. Pudieron poner en marcha rápidamente a su fuerza laboral remota. No sabemos exactamente cómo será la nueva normalidad, pero sabemos que conservará muchos de los mismos aspectos que se aceleraron en los últimos ocho meses, como permitir una fuerza laboral remota, pero al mismo tiempo ser más colaborativo. Intel quiere llegar a nuevos clientes que necesitan más recursos para operar desde sus propias instalaciones y volverse más ágiles.\n\n## Tu lugar de trabajo necesita nuevas capacidades Intel acorta la brecha.\n\nIntel se dedica a crear soluciones ayudando a los clientes a comprender el vasto ecosistema Intel con diferentes modelos que pueden cubrir las brechas en rendimiento, estabilidad, eficiencia y precio. Intel puede compartir los conocimientos de miles de clientes para ayudar a resolver problemas y brindar capacidades que las organizaciones de TI a veces no pueden hacer por sí mismas.\n\n## Los requisitos comerciales son primordiales.\n\nEn el pasado, un CIO podría haber estado centrado en solo tres cosas: seguridad, resiliencia y eficiencia. Eso ya no es así. Lo que solía ser equivalente a mantener las luces encendidas, evitar problemas y funcionar de manera eficiente, ahora es mucho más complejo. Las empresas deben tener una visión de futuro para mejorar al trabajador, el lugar de trabajo y las transformaciones que están experimentando en términos de cumplir con los compromisos de los clientes y proveedores.\n\nAdemás, la agilidad es un requisito ya que las circunstancias pueden cambiar rápidamente y las empresas deben adaptarse en múltiples direcciones mientras aún satisfacen estas necesidades.\n\nFinalmente, más automatización mediante inteligencia artificial es un facilitador para impulsar áreas como una mejor colaboración y mejora en la experiencia del trabajador. Con una fuerza laboral distribuida, por ejemplo, no hay un mostrador de ayuda con una persona que pueda acercarse y ayudar con su problema. Ese podría no ser el escenario más eficiente de todas formas, por lo que tal vez los chatbots o el aprendizaje automático en un sistema CRM serían más eficientes y permitirían más información compartida. Este tipo de automatización técnica con solución de problemas comunes podría generar más conexiones, ideas y productividad al final.\n\n## Tu lugar de trabajo está evolucionando.\n\nLos requisitos y estrategias empresariales están cambiando, especialmente en torno a la evolución en el lugar de trabajo. Algunas organizaciones seguirán con operaciones completamente remotas, algunas querrán que todos regresen a una oficina cuando sea seguro, y habrá configuraciones intermedias. Independientemente de la situación, las organizaciones deben adoptar una estrategia para asegurarse de que pueden alcanzar a los trabajadores en cualquier lugar, en cualquier momento y desde cualquier dispositivo, ya sea por necesidad o elección.\n\nLa mejora del proceso también es clave. No quieres seguir agregando procesos a los que ya no necesitas. Evaluar áreas para mejorar, ya sea la infraestructura interna o las capacidades de terceros, añadirá eficiencia y valor. En lugar de construir una enorme infraestructura que requiere gestión y se ve limitada por el uso exclusivo de sus herramientas, las organizaciones deberían evaluar la increíble cantidad de oportunidades del ecosistema que se integran en herramientas como servicio. Externalizar servicios que no son los activos estratégicos clave o las fortalezas de tu empresa podría tener más sentido.\n\nOtra área a evaluar es la gestión de datos. Con, por ejemplo, todos los datos en las herramientas de colaboración, la dispersión de datos se convierte en un problema. Se necesita una estrategia clara y efectiva.\n\nJunto con la gestión de datos viene la seguridad. Ahora los datos están dispersos por todas partes, por lo que las organizaciones deben adoptar, evaluar e implementar buenas herramientas de seguridad y buenos flujos de trabajo en torno a las prácticas de datos.\n\n## Intel Entrega\n\nHay seis capacidades principales en el entorno laboral en evolución en las que Intel puede ayudar: acceso a aplicaciones y datos, capacidad de gestión, seguridad mejorada, conectividad, infraestructura de colaboración y entornos de múltiples nubes. Aunque Intel solo produce silicio, las capacidades, escalabilidad y seguridad del mismo satisfacen las necesidades más pequeñas y las más grandes de las organizaciones.\n\nIntel busca existir en múltiples entornos y ofrecer la capacidad de gestionar esos recursos. Aunque los clientes no comprarán directamente activos como procesadores, aceleradores, memoria, almacenamiento, entre otros, de Intel, aprovecharán el sólido y comprobado ecosistema de productos que Intel hace posible, como jugadores de hardware, fabricantes de equipos originales (OEMs), proveedores de software, integradores de sistemas y proveedores de servicios en la nube.\n\nUna de las fortalezas de Intel es su apoyo a este ecosistema. Por ejemplo, Intel cuenta con 15,000 ingenieros de software que desarrollan código, pero ninguno de este se vende. En cambio, ayudan a desarrollar el ecosistema al proporcionar nuevas soluciones construidas sobre el silicio. El silicio es el mecanismo que brinda soluciones para ayudar a las personas a resolver problemas reales. Una buena metáfora es que la distancia entre el muelle de carga de Intel como fabricante y el muelle de carga final del cliente está demasiado lejos para que Intel conduzca un camión ellos mismos. El ecosistema se encarga de superar esa brecha.\n\n## Áreas de solución que podrías estar evaluando\n\n¿Cómo pueden los clientes aprovechar al máximo la capacidad del ecosistema? Intel puede dar recomendaciones en las seis áreas clave, ya sea que estén enfrentando problemas con una solución de VDI, como decidir si debe permanecer en las instalaciones o estar en un escritorio virtual o servicio de RDS, o desarrollando una estrategia de VPN que garantice la conectividad.\n\nParte de la solución consiste en comprender que, dado que Intel trabaja en un ecosistema tan vasto, pueden ayudar a enfrentar una multitud de desafíos. Por ejemplo, Intel no necesariamente te va a vender un procesador, sino que te ayudará a colocar tu carga de trabajo en la mejor pieza de silicio posible, que podría estar en proveedores de servicios en la nube, o en múltiples nubes, tanto dentro como fuera de tu propio centro de datos. El objetivo de Intel es brindarte eficiencia, portabilidad y agilidad en estos procesos.\n\n## Abraza la evolución en el lugar de trabajo: ¿Cuándo o dónde quieres empezar?\n\nIntel tiene en cuenta el mejor interés de su organización porque si tiene éxito en su negocio, encontrará nuevas formas de utilizar los datos y nuevas formas de utilizar la infraestructura para proporcionar más valor a sus clientes, y al final, consumir más. Sus estrategias y su crecimiento y desarrollo son propios, pero Intel le va a proporcionar la base para tomar buenas decisiones.\n\nBusque oportunidades para interactuar con su representante de Intel. Ellos tienen una gran cantidad de recursos para ayudarlo a aprovechar la extensa red de socios que pueden abordar sus problemas y sus metas.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Robert Looney"],"link":"/episode-EDT34-es","image":"./episodes/edt-34/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren Pulsipher, Arquitecto de Soluciones Principal para el Sector Público, y Robert Looney, Gerente de Ventas de Centros de Datos para América de Intel, hablan sobre cómo utilizar un enfoque estratégico para adoptar la evolución actual del lugar de trabajo. La pandemia de COVID creó grandes desafíos y transiciones en el lugar de trabajo. Intel está ayudando a los clientes a aprovechar las tecnologías para abordar de la mejor manera los desafíos continuos en la nueva normalidad."},{"id":105,"type":"Episode","title":"Navegando el Cambio Disruptivo","tags":["change","culturalchange","organizationalchange","disruption","people","process"],"body":"\r\n\r\nRick recientemente celebró su tercer década en Intel, y en ese tiempo, ayudó a Intel a navegar a través de enormes cantidades de cambios y eventos importantes. Junto con situaciones competitivas difíciles y cambios en la industria que comenzaron con el surgimiento de Internet y el auge y caída de las empresas puntocom, también hubo eventos externos como el 11 de septiembre, la Gran Recesión y ahora, la pandemia de COVID-19.\n\n## Tipos de situaciones de crisis\n\nLa naturaleza de una empresa moderna en una economía moderna es que estará constantemente navegando en un alto grado de incertidumbre, agitación, cambios y perturbaciones. Las organizaciones o bien se marchitan en esos momentos o salen mejor y más fuertes.\n\nCada interrupción es diferente en términos de magnitud y velocidad. Algunos eventos se desarrollan durante un largo período de tiempo, tal vez un cambio tecnológico o estructural en la industria, y luego repentinamente se aceleran. Algunos, como el COVID, tienen un alto impacto a una velocidad sin precedentes. Sin embargo, lo interesante acerca del COVID es que hemos estado desarrollando la tecnología para lidiar con las consecuencias de la pandemia durante una década, pero tomó este evento para ponerla en uso práctico. Un buen ejemplo es la telemedicina. Su repentina y amplia utilización también ha cambiado el entorno de políticas y el panorama de la atención médica nunca será el mismo.\n\nEste tipo de cambio fundamental en la política ocurre en eventos con gran rapidez, como la Gran Recesión o el 11 de septiembre. Cosas similares sucederán después del COVID. Estos eventos, por difíciles que sean, brindan una oportunidad para que las organizaciones den un gran salto hacia adelante en su rendimiento y en el uso de la tecnología.\n\n## Navegando el cambio disruptivo.\n\nIntel siempre ha dado un paso adelante al responder a grandes desafíos y perturbaciones. Partes fundamentales de la cultura son la preparación, el propósito compartido y la confianza. Estas pueden existir cuando los empleados tienen un sentido de seguridad psicológica. Por ejemplo, Darren se sintió empoderado cuando el CEO de Intel dijo que nadie sería despedido debido a COVID. Esto le permitió correr riesgos para enfrentar la crisis sin temor a perder su trabajo. Y aunque el CEO y los gerentes senior establecen el tono, la mayoría del trabajo de seguridad psicológica lo realizan los gerentes de primera línea. Esta seguridad y empoderamiento conduce a una disposición incorporada. Junto con el propósito compartido y la confianza, estos son los bloques fundamentales no solo de una organización lista para responder a una crisis, sino también de las características de una organización de alto rendimiento.\n\nUna organización de alto rendimiento también tendrá las herramientas para navegar las interrupciones que son más un proceso lento que un evento impactante y rápido. Con eventos como el COVID o la Gran Recesión, hay muy poco debate sobre lo que está sucediendo y todos están conscientes de la magnitud de los problemas. Si lo comparamos, por ejemplo, con un cambio fundamental en el negocio, un cambio arquitectónico o una tecnología que se presenta en la curva de expectativas y aún no estamos seguros de si es relevante para el negocio, habrá más incertidumbre y debate sobre los ajustes.\n\n¿Cómo sobrevive una organización a estos puntos de inflexión? La telemetría, o la entrada que estás evaluando, es importante. Una de las complejidades en una gran organización es que cuando estas entradas llegan a un tomador de decisiones de alto nivel, pueden haber pasado por tres niveles de manipulación y posicionamiento, y eso puede ser peligroso. La verdad y la transparencia son valores en Intel. En una empresa con un alto grado de seguridad psicológica, los empleados pueden decir la verdad sobre los problemas.\n\nLa entrada más importante es escuchar a tus clientes, porque tienden a guiarte en la dirección correcta. Por ejemplo, si alguien quiere saber información sobre una cuenta, Rick suele llamar al ejecutivo de cuentas para obtener información de primera mano. También es un liderazgo sabio ir directamente a los expertos en lugar de obtener información a través de tres capas de filtrado y manipulación, especialmente cuando se opera en una crisis. Cumplir con el momento se reduce a una cultura de preparación, la telemetría adecuada y la toma de decisiones.\n\nLa toma de decisiones puede volverse complicada en una organización grande. Una solución sencilla es que cada persona que ingrese a una reunión debería preguntarse: ¿estamos aquí para tomar una decisión? ¿Quién es el responsable de tomar la decisión? O, ¿simplemente estamos debatiendo o preparando la telemetría y los datos para quien tomará la decisión? Esto es simplemente una buena higiene organizativa.\n\nAndy Grove dijo: \"Dejemos que un poco de caos reine y luego controlémoslo\". Para las decisiones en los puntos de inflexión, los cambios graduales, a veces tienes que permitir que la innovación respire y se filtre un poco, y al mismo tiempo, quieres manejar las cosas de tal manera que no se salgan de control. Tener buenos procesos y límites establecidos ayuda con esto.\n\nEn tiempos difíciles, los tomadores de decisiones deben tener un profundo entendimiento de que cada individuo estará en un espacio diferente y deben reflexionar sobre el impacto de sus decisiones. La seguridad psicológica es muy importante, y los gerentes de primera y segunda línea son fundamentales para la capacidad de una organización para ejecutar correctamente en tiempos de interrupción. El liderazgo senior es clave para establecer el tono, pero son estos gerentes quienes se encargan del trabajo.\n\nDe los cambios tecnológicos futuros predecibles, como el impacto de la IA en 5G, a problemas como el cambio climático, hasta eventos mundiales imprevistos, la única constante es que siempre estaremos navegando por la interrupción, las crisis y el cambio. Una de las características distintivas de la cultura de Intel es su capacidad para responder, adaptarse y ser resistente ante estos eventos.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rick Herrmann"],"link":"/episode-EDT35-es","image":"./episodes/edt-35/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren Pulsipher, Arquitecto Principal de Soluciones, Sector Público, y Rick Hermann, Director del Sector Público de Estados Unidos, en Intel, discuten cómo Intel ha logrado superar con éxito los cambios disruptivos en las últimas tres décadas."},{"id":106,"type":"Episode","title":"Seis pilares de ciberseguridad","tags":["cybersecurity","prevention","threatdetection","zerotrustarchitecture","zta","technology"],"body":"\r\n\r\nCon el aumento de los ciberataques en todas las industrias, la seguridad es más importante que nunca. En este episodio, Darren y Steve Orrin, CTO federal de Intel, delinean los vectores de ataque, los seis pilares de ciberseguridad y cómo Intel puede ayudar.\n\nSteve ha visto cómo la seguridad ha evolucionado en los 25 años que ha trabajado en el campo, tanto como una ciencia como un arte. Al mismo tiempo, el nivel de complejidad con el que las organizaciones deben lidiar para asegurar sus datos, sistemas y aplicaciones nunca ha sido más difícil.\n\n## Brechas y ataques cibernéticos a gran escala continúan.\n\nGrandes violaciones de datos a gran escala y profundas intrusiones están ocurriendo en todos los niveles, desde plataformas de redes sociales hasta servicios financieros y atención médica. Ningún tipo de datos está exento de ser objetivo de técnicas cada vez más sofisticadas.\n\n¿Qué está impulsando estos ataques? Una respuesta es que hoy en día, un ataque a gran escala o profundo requiere menos recursos y una menor inversión financiera. El alcance y la magnitud de lo que un hacker puede hacer con una pequeña inversión ha otorgado a los adversarios una ventaja en un sistema complejo.\n\n## Tres fuerzas que impactan la seguridad empresarial y de misión.\n\nOtra respuesta a lo que está impulsando los ataques es que los datos son un activo valioso: el nuevo petróleo. Los datos son vulnerables a medida que la expansión de la superficie de ataque sigue creciendo.\n\nHay más puntos de integración, productos, sistemas operativos de proveedores y dispositivos involucrados en la gestión, consumo y transporte de los datos. Los datos están más alejados del control de la empresa. A veces ni siquiera sabemos dónde existen nuestros datos. Por ejemplo, tal vez has compartido tus datos con otra organización y ellos compartieron los análisis que se realizaron con los datos. Esa metadata a menudo se convierte en residuo de datos. Tus datos están fluyendo a través de múltiples sistemas, siendo accesibles para un atacante después del hecho.\n\nLa industria de seguridad en sí misma está compuesta por miles de proveedores y productos de seguridad que resuelven una parte específica del rompecabezas, por lo que hay solo tanto que un director de tecnología de la información (CIO) puede hacer con un presupuesto limitado, y solo tanto que puede manejar dada la complejidad. Debemos pensar de manera integral en cómo podemos asegurar nuestros datos, no sólo cómo asegurar una transición entre el punto A y el punto B. Los datos se comprometen en el eslabón más débil, por lo que tenemos que analizar toda la cadena.\n\nLa tercera fuerza que impacta a las empresas es la ubicación de los ataques. Estos incluyen ataques en todas las capas de hardware, firmware, BIOS, software, servicios y aplicaciones. Con el incremento en la sofisticación de los ataques, estamos presenciando ataques en múltiples capas de forma asíncrona e independiente.\n\n## Los Vectores de Ataque\n\nLos hackers no solo atacan el hardware o un programa informático, sino a través de múltiples vectores: personas, procesos y tecnología.\n\nTradicionalmente, implementar controles de seguridad ha implicado una formación adecuada de las personas y el uso de los procesos y tecnologías adecuados, pero a la luz de los ataques recientes, debemos recordar que el propio proceso es vulnerable. Por ejemplo, necesitamos pasar a la aplicación automatizada de parches para reducir la ventana de exposición desde que se descubre una vulnerabilidad hasta que se lanza y se implementa un parche por parte de una organización. Tradicionalmente, hemos convivido con el riesgo de que los atacantes tengan meses para aprovechar la vulnerabilidad.\n\nHay otros ataques al proceso que estamos cerrando ahora mismo, ya sea en el proceso de construcción o en el ciclo de vida del desarrollo de software. Integrar la seguridad temprano en el ciclo de vida del desarrollo es el aspecto más importante para asegurar una aplicación. Esto significa que los desarrolladores, los especialistas de control de calidad (QA) y los diseñadores deben estar involucrados en el proceso de seguridad. Parte del desafío es la naturaleza aislada de cada parte del proceso, donde las vulnerabilidades pueden infiltrarse en las uniones y transiciones.\n\n## Seis pilares de ciberseguridad.\n\n## Seguridad de la cadena de suministro\n\nLa seguridad de la cadena de suministro ha estado en primer plano solo en los últimos años. Una organización necesita poder confiar en los servidores, componentes y software. Una buena cadena de suministro con transparencia es importante para validar que todo provenga de fuentes legítimas. Ha habido un enfoque, especialmente en el gobierno, en la cadena de suministro de hardware, pero no podemos olvidar la cadena de suministro de software. La cadena de suministro de software es un poco más difícil que el hardware, ya que a menudo hay una falta de visibilidad debido a que los productos pueden ser ensamblados con herramientas de código abierto, productos de otras personas, etc. En un ataque reciente, la cadena de suministro de software fue el problema, y esto es solo la punta del iceberg.\n\n## Seguridad del Hospedaje y del Sistema\n\nUna vez que tenemos una cadena de suministro confiable, el siguiente paso está en garantizar la seguridad del sistema de hospedaje. La base son las tecnologías de inicio seguro y las capacidades criptográficas para bloquear y asegurar dispositivos físicos y sistemas donde las aplicaciones funcionarán y los datos serán procesados. Este sistema respalda las características de seguridad de nivel superior en el hardware.\n\n## Seguridad de datos y aplicaciones.\n\nPor encima de la seguridad del anfitrión y del sistema es donde construyes la seguridad de tu carga de trabajo de aplicaciones. Los datos deben estar protegidos durante toda su vida útil, en reposo, en uso y en tránsito. Hemos estado implementando seguridad para datos en reposo y en tránsito durante mucho tiempo utilizando cifrado de transporte, TLS, IP sec y otras capacidades de cifrado, así como el cifrado de disco completo y de archivos. El eslabón perdido ha sido la memoria encriptada de datos \"en uso\" con aislamiento de hardware. En los últimos años, las tecnologías y las plataformas de soluciones están permitiendo esa última milla de exposición en torno a la protección de datos.\n\n## Seguridad de la red\n\nEn paralelo con esta pila de anfitriones de la cadena de suministro y seguridad de datos, necesitamos seguridad en la red. La integridad y disponibilidad de las redes son importantes para resistir los ataques de denegación de servicio. Los datos necesitan llegar a su destino de manera segura. También debemos monitorear y proteger las redes de intrusiones externas, ya sea que la red sea empresarial o una red distribuida en la nube y el borde. La seguridad aquí no se trata solo de firewall, se trata de producción activa.\n\n## Gestión de identidad y acceso\n\nLa gestión de identidad y acceso es una capacidad fundamental. No solo necesitamos saber quién está iniciando sesión, sino quién está iniciando sesión en qué dispositivo. El ser humano es en realidad una pequeña pieza del rompecabezas. Necesitamos tener identidad para todas las cosas, procesos y servicios que acceden y gestionan los datos. Una persona puede realizar su trabajo con solo unos pocos clics, pero puede haber 20 dispositivos diferentes y 100 servicios y procesos que actúen sobre los datos. Necesitamos tener políticas y autorización para todas esas entidades. Y a medida que avanzamos hacia procesos autónomos, hay menos personas involucradas, por lo que se vuelve aún más importante tener una identidad sólida para esos procesos sin intervención humana.\n\n## Detección de amenazas, Inteligencia y Analítica\n\nEste último pilar es una combinación de muchas cosas, incluyendo inteligencia de amenazas, análisis, monitoreo y auditoría. Es la visibilidad global para asegurarse de que todo funcione como debería, y si algo está mal, la capacidad de entender rápidamente de dónde proviene y por qué. Esta es la base que impulsa la seguridad de datos y todo debe ascender y descender. Hay un cambio de trabajar en un entorno aislado, digamos un proveedor que solo se preocupa por la seguridad de la red, a trabajar en todo el sistema como un todo. Las compañías exitosas tienen equipos diversos con personas de diferentes dominios para satisfacer necesidades de seguridad complejas.\n\n## Dominios de ciberseguridad: Logrando la confianza cero con las tecnologías de Intel.\n\nIntel proporciona capacidades fundamentales en cada uno de los seis pilares, ya sea en nuestra iniciativa de garantía del ciclo de vida informática para ayudar a que los principales OEM y proveedores de componentes colaboren en una cadena de suministro confiable, o en proporcionar los bloques de construcción fundamentales de la seguridad del sistema, donde el arranque seguro comienza con el hardware. Tenemos tecnología de ejecución y tecnología de protección de arranque con aceleración de cifrado incorporada, para que los usuarios puedan activarla sin afectar el rendimiento, logrando así protección de datos en reposo, en uso y en tránsito.\n\nEn el caso de la inteligencia de detección de amenazas, Intel proporciona primitivas como TPD donde una solución de pila de nivel superior puede brindar visibilidad y detección de amenazas donde nunca la habíamos tenido antes.\n\nIntel es un proveedor de tecnología, pero también trabajamos en el ámbito de las personas y los procesos. Un buen ejemplo es la cadena de suministro. Hemos construido un proceso con el ecosistema para permitir que una empresa pueda validar los componentes y credenciales de una plataforma y sus componentes. De manera similar, existen procesos involucrados en la protección de datos y uso a través de capacidades del hardware como SGX, que puede cifrar la memoria y aislar los datos y el código para una aplicación determinada.\n\nIntel está habilitando procesos seguros para aprovechar las tecnologías a gran escala. Otra parte clave en torno al proceso es integrarse en el marco de riesgo general de una organización. Intel te proporciona la evidencia y atribución necesarias dentro de nuestras tecnologías para mapear eso en tu marco de riesgo existente.\n\nLa última pieza es la gente. Lidiar con el comportamiento humano aleatorio a veces es la parte más difícil de la seguridad, ya sea en estafas de phishing o ataques dirigidos a contraseñas débiles. El entrenamiento es crucial, pero a menudo no es suficiente. Los procesos y tecnologías pueden ayudar a complementar el entrenamiento, por ejemplo, haciendo contraseñas más fuertes o eliminando el phishing si las credenciales de un usuario no se pueden comprometer. Al final del día, sin embargo, el entrenamiento y la educación continua siempre serán críticos junto con las tecnologías de mitigación.\n\nLa seguridad es difícil, pero hay luces al final del túnel con todas las innovaciones en el ecosistema y con organizaciones abiertas a hacer las cosas de manera diferente. Debemos mantener nuestra atención en dos cosas: la adopción de marcos de riesgo y la confianza cero. Uniendo estos dos mundos, el dominio de la ciberseguridad a los motores de políticas y su aplicación puede proporcionar un enfoque integral de la seguridad. Hay mucha actividad aquí y aún queda mucho trabajo por hacer.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT36-es","image":"./episodes/edt-36/es/thumbnail.png","lang":"es","summary":"Con los ciberataques en aumento en todas las industrias, la seguridad es más importante que nunca. En este episodio, Darren Pulsipher, Arquitecto Principal de Soluciones, y Steve Orrin, CTO Federal de Intel Federal, describen los vectores de ataque, los seis pilares de ciberseguridad y cómo Intel puede ayudar. El nivel de complejidad con el que las organizaciones deben lidiar para asegurar sus datos, sistemas y aplicaciones nunca ha sido más difícil."},{"id":107,"type":"Episode","title":"2020 en revisión","tags":["covid","cybersecurity","remoteworker","people","process","compute","data"],"body":"\r\n\r\n## 2020 - Expectativas\n\nPre-COVID, el año 2020 parecía ser el año para nuevos modelos de negocio, avanzando hacia grandes transformaciones digitales. La inteligencia artificial (IA), el aprendizaje automático (ML) y el análisis iban a desempeñar un papel fundamental en el futuro. Utilizando esas herramientas, íbamos a comenzar a utilizar los datos de manera más efectiva en nuestras organizaciones. Iba a ser el año del 5G, con el 5G causando un gran impacto en las Olimpiadas.\n\nTambién íbamos a presenciar un gran avance en la industria para las transformaciones 4.0, con el Internet de las Cosas y la manufactura avanzando juntos hacia esta transformación digital.\n\n## 13 de marzo\n\nEntonces, al menos en California, todo se detuvo abruptamente el viernes 13 de marzo. Negocios, escuelas, conferencias, reuniones, viajes y eventos sociales se cerraron abruptamente. Las semanas se convirtieron en meses a medida que el optimismo sobre contener rápidamente el virus menguaba, mientras ciudades como Nueva York sufrían grandes golpes y los sistemas médicos estaban desbordados. Todo parecía caótico e incierto.\n\nAlgunas cosas interesantes surgieron de esto: algunas empresas prosperaron mientras que otras tuvieron tiempos más difíciles, y las personas comenzaron a trabajar remotamente.\n\n## El sitio web más visto en mi casa.\n\nEn mi casa, el sitio web con más visitas ya no era Facebook, sino el Centro de Ciencia y Ingeniería de Sistemas en Johns Hopkins, que proporciona estadísticas sobre el virus. Podíamos ver los efectos en nuestro propio vecindario y ciudad, y en todo el mundo. Toda nuestra percepción de lo que era importante y lo que habíamos planeado para ese año cambió.\n\n## Escasez\n\nNo solo hubo escasez inmediata de artículos tan diversos como papel higiénico, desinfectante de manos y levadura, sino que unas semanas después, los equipos de oficina y la tecnología se convirtieron en artículos de alta demanda a medida que las personas se adaptaban rápidamente a la educación a distancia y al trabajo desde casa. Esto creó una escasez de herramientas integrales como cámaras web y computadoras portátiles.\n\nLos routers Wi-Fi 6 se volvieron importantes de repente a medida que aumentaba el uso de internet en casa. En nuestra familia, pasé de trabajar ocasionalmente en casa a trabajar desde casa a tiempo completo y se sumaron tres adolescentes que hacían educación a distancia y tres estudiantes universitarios adultos que regresaron a casa para estudiar y trabajar. Nuestra conexión a internet se vio rápidamente abrumada. Tuve dificultades para encontrar un router Wi-Fi 6 que solucionara nuestro problema, ya que muchos se encontraron en situaciones similares.\n\n## Viajar cambió.\n\nCon los viajes prácticamente detenidos, muchas personas recurrieron en cambio a proyectos de mejora del hogar. Solía estar frecuentemente en el aeropuerto, ya que los viajes ocupaban la mitad de cada semana. Ahora, me encontraba en Lowe's o Home Depot con todos los demás. Los lugares estaban llenos. El tiempo que solía pasar viajando ahora lo dedicaba a construir un cobertizo con mis hijos. Tuve la oportunidad de pasar más tiempo de calidad con mis hijos y mejorar mi hogar.\n\n## Conferencias\n\nUn cambio importante, por supuesto, fue la cancelación de las conferencias presenciales. Muchas se trasladaron en línea en su lugar con grandes resultados. Por ejemplo, la conferencia IBM Think cambió su evento en persona de mayo a una versión en línea. Más de 100,000 personas asistieron, lo cual fue el mayor número que hayan tenido.\n\nAdemás del aumento en la asistencia, otro beneficio es que los asistentes no tienen que comprometerse a estar en una conferencia durante toda una semana, sino que pueden seleccionar y elegir las sesiones y aún así estar en casa con su familia. La desventaja es la falta de interacción social y la posibilidad de ver a las personas cara a cara.\n\nLas conferencias de la industria probablemente cambien para siempre. No siempre serán completamente en línea, pero quizás una versión híbrida tenga sentido en el futuro.\n\n## Trabajo Forzado desde Casa\n\nOtro gran cambio es que estamos en los hogares de los demás virtualmente llevando a cabo negocios. A veces, hay niños, perros u otras distracciones inevitables en el fondo. Una vez, mi jefe necesitaba que su hija se sentara a su lado mientras lideraba una reunión de personal porque ella necesitaba ayuda con algo. Es importante que todos seamos flexibles durante este tiempo.\n\n## Trabajadores Nómadas\n\nAlgunos empleados no están trabajando en una sola casa, sino que se han vuelto nómadas ya que ya no necesitan estar cerca de su lugar de trabajo.\n\nPor ejemplo, mi sobrino, que tiene pocas responsabilidades en casa, pagaba un alquiler exorbitante en el Área de la Bahía. Cuando el trabajo se volvió remoto, él y sus compañeros de piso abandonaron su costoso apartamento y en su lugar viajaron por el mundo, pasando varios meses seguidos alquilando casas en lugares interesantes.\n\nEste es un cambio importante en la forma en que pensamos acerca de gestionar a nuestros trabajadores, activos y datos.\n\n## Trabajo remoto\n\nEl trabajo remoto se aceleró rápidamente. Muchas empresas ya tenían políticas de trabajo remoto o planes para más trabajo remoto. Pero lo que alguna vez tal vez fue un plan de implementación de 18 meses se convirtió en un plan de implementación de 18 días.\n\nLa principal forma de trabajo remoto que ha ocurrido fue la infraestructura de escritorio virtual (VDI). Las personas han estado utilizando VDI durante décadas, pero de repente se convirtió en la opción más popular, en parte, debido a su familiaridad. Es rápido, fácil y económico hacer que las personas vuelvan a trabajar con acceso a los datos que necesitan utilizando soluciones de hardware.\n\nLas redes privadas virtuales (VPNs) se saturaron rápidamente debido a que todos trabajaban desde casa. Observamos que las empresas invirtieron en expandir sus VPN, ya sea comprando más licencias o infraestructura adicional y actualizando según fuera necesario.\n\nLas empresas también aceleraron su adopción de herramientas SaaS (software como servicio). Por ejemplo, la implementación de Office 365 ocurrió en cuestión de semanas en lugar de los seis a nueve meses planeados. Empresas con ofertas de SaaS, como Microsoft y Google, se pusieron manos a la obra y ayudaron a las organizaciones a completar rápidamente la migración. Para los trabajadores remotos, el tiempo improductivo se minimizó gracias al esfuerzo de toda la industria.\n\nFue notable lo rápido que los trabajadores remotos estuvieron operativos. Los verdaderos héroes aquí fueron los trabajadores de IT de primera línea, como los del servicio de ayuda, ingenieros de sistemas, administradores de sistemas, etc. Fue realmente un esfuerzo Hercúleo.\n\n## Cambios en ciberseguridad\n\nUna de las cosas, sin embargo, que se descuidó un poco con la velocidad de este cambio fue la ciberseguridad. Hemos estado sintiendo las ramificaciones de esto en las últimas seis a ocho semanas con varios ataques importantes en ciberseguridad.\n\nUna de las razones es una mayor superficie de ataque porque los datos están distribuidos en computadoras portátiles en toda la organización en redes no seguras en hogares.\n\nAlgunos de los datos están en la nube. Ahora, con las ofertas de SaaS, parte de ellos están en el centro de datos y parte en las computadoras que las personas llevaron a casa.\n\n2021 será un año en el que nos centraremos en la ciberseguridad, analizando detenidamente la forma en que estamos gestionando los datos y asegurándolos en todo el sistema.\n\n## Nuevo vestuario de negocios\n\nUno de los mayores avances de este año es el nuevo atuendo de negocios. La mejor manera de describirlo es el \"negocio mullet\": traje de negocios en la parte superior, pijamas o pantalones cortos en la parte inferior. A veces me pongo pantalones, pero probablemente mis hijos están cansados de verme con una camisa de botones o incluso un traje en la parte de arriba, combinado con shorts.\n\n2020 fue un año de cambio, tanto a nivel personal como profesional, pero nos ha llevado a un lugar en 2021 donde podemos avanzar con lecciones aprendidas y mejoras para un futuro mejor.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT37-es","image":"./episodes/edt-37/es/thumbnail.jpg","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones del Sector Público en Intel, reflexiona sobre la interrupción, los cambios y los ajustes que la pandemia de COVID-19 trajo en 2020."},{"id":108,"type":"Episode","title":"Los locos años veinte una mirada hacia adelante a 2021.","tags":["contactless","covid","remotelearning","remoteworker","people","process"],"body":"\r\n\r\n## COVID-19: Un tiempo sin precedentes de volatilidad, incertidumbre, complejidad y ambigüedad.\n\nEn este episodio, Darren le pregunta a Rachel Mushawar, vicepresidenta de Ventas en Intel, su perspectiva sobre lo que se avecina para los \"Felices Años Veinte\" después de la interrupción del COVID-19.\n\n## 2020\n\nLos últimos doce meses han estado marcados por un cambio e incertidumbre inconfundibles, tanto profesional como personalmente. No todos los años serán como el 2020, pero las lecciones nos harán mejorar cada año en el futuro.\n\nA pesar de las circunstancias individuales, cada persona ha tenido que hacer una pausa y descubrir qué era importante. Valoramos a nuestros equipos, amigos, familias y relaciones más que nunca.\n\nY aunque hemos estado físicamente aislados, la conectividad a través de la tecnología se aceleró a una velocidad sin precedentes. El sentido de posibilidad de todos en esta área ha sido rejuvenecido y, de alguna manera, desbloqueado y liberado para el futuro.\n\n## La pandemia ha creado más responsabilidades.\n\nPor otro lado, la pandemia ha creado más responsabilidades. Casi la mitad de los adultos en sus cuarenta y cincuenta años tienen un padre de 65 años o más y tienen hijos pequeños o están económicamente apoyando a hijos mayores. Esto significa que probablemente están lidiando con la educación a distancia de sus hijos, trabajando desde casa y cuidando a sus padres ancianos.\n\nLas pérdidas netas de empleo en los Estados Unidos en diciembre fueron exclusivamente de mujeres. Algunas de ellas se deben a las mayores responsabilidades en el hogar durante la pandemia. Si tienes hijos pequeños en casa que ahora están aprendiendo a distancia y eres un trabajador por horas, ¿cómo lo haces? No lo haces. Y aunque durante el confinamiento los padres casi duplicaron el cuidado de los niños, aún nos queda un largo camino por recorrer en la década de 2020 en cuanto a igualdad de género. No puede ser solo sobre tecnología.\n\n## La educación a distancia pone de relieve la brecha digital.\n\nA medida que avanzamos hacia los años 20, tenemos toda una generación de niños que han pasado los últimos 12 meses educándose en casa.\n\nDe hecho, una de cada cuatro familias estadounidenses tiene al menos un niño de 14 años o menos. Para aquellos estudiantes que provienen de un entorno desfavorecido, un estudio del Foro Económico Mundial encontró que el 25% no tiene una computadora. Además, el 33% de los estudiantes en comunidades rurales tienen poco o ningún acceso a Internet. La falta de estos dos fundamentos del aprendizaje a distancia ha aumentado la brecha digital.\n\n## Estamos viviendo \"El Futuro del Trabajo\".\n\nMuchas ideas antiguas se han desmoronado en 2020 ya que las empresas tuvieron que cambiar de forma inmediata al trabajo desde casa. El ochenta y cinco por ciento de las organizaciones ampliaron o implementaron una variedad de políticas de trabajo desde casa que se extenderán más allá del COVID-19. Muchas empresas se dieron cuenta de los ahorros al reducir el espacio físico ya que la productividad se mantuvo igual o aumentó con un personal que trabaja desde casa o de forma nómada. Muchos empleados también están notando ahorros de tiempo al eliminar los desplazamientos y la preparación personal como peinado, maquillaje y vestimenta. Una tendencia para 2021 será un aspecto más informal y natural.\n\nA medida que los desplazamientos desaparecieron, la conectividad aumentó y los dormitorios se convirtieron en oficinas, sin embargo, surgió un inconveniente a medida que la jornada laboral promedio se alargaba y se hacía más difícil desconectar.\n\n## \"Contactless\" está impulsando todo como un servicio.\n\nUna de las cosas clave en 2021 va a ser cómo seguimos aprovechando la tecnología para mantenernos conectados. Por ejemplo, existen tecnologías similares a Zoom que llevan las cosas a otro nivel y brindan oportunidades sociales similares a una charla de oficina en el espacio virtual. Treinta y dos por ciento de los adultos tuvieron una reunión social virtual en 2020. Intel ha realizado visitas a una granja virtual de cabras e ha invitado a instructores de yoga para combatir el aislamiento.\n\n## Más conexiones, menos fricción, más virtual, menos contacto físico\n\nA medida que avanzamos hacia el resto de los años 20, la tecnología se convertirá en un pilar fundamental para todas las principales transformaciones, ya sea en el sector privado o público.\n\n¿Cómo se resume esta nueva forma de trabajar y las mayores responsabilidades en el hogar desde la perspectiva de un CIO?\n\nHay un puñado de imperativos estratégicos para TI. Podemos desglosarlos en las categorías tradicionales de aplicaciones, red y centro de datos.\n\nPrimero, para las aplicaciones, los directores de tecnología (CIOs) tienen que descubrir cómo habilitar el contacto sin contacto, es decir, ¿cómo hacer que todo funcione como un servicio? Esto no solo se aplica a los minoristas, sino también a la atención médica, el gobierno y la fabricación. La segunda parte con las aplicaciones consiste en saber quién es su consumidor y cómo asimilan el contenido.\n\nDarse cuenta de la importancia de tu red es lo siguiente. Es el impulso turbo para todas las cosas digitales. Las organizaciones deben tener una red que, por ejemplo, permita automatizar tus fábricas o brindar telemedicina. Deben ser visionarias para cuando el 5G se convierta en más realidad en lugar de invertir en tecnología obsoleta. Las redes se extienden más allá de los centros de datos tradicionales ahora, así que eso es algo a lo que debemos prestar atención. Por supuesto, la seguridad debe ser una prioridad aquí.\n\nAl igual que las redes, la seguridad no es un tema emocionante ni atractivo, pero es un aspecto clave al considerar todos los puntos finales que ahora son omnipresentes en nuestras vidas diarias. La superficie de amenaza está aumentando de manera exponencial con los empleados que trabajan desde casa en diferentes dispositivos y la implementación de todo como un servicio para los clientes. La seguridad ya no solo se trata de proteger los datos en reposo y en movimiento. También abarca todo lo que está en el medio.\n\nEn lugar de los centros de datos tradicionales, deberíamos pensar en ellos como centros de datos que sirven cargas de trabajo específicas. Por ejemplo, la nube está creciendo un 30 a 40 por ciento cada año para acercar los centros de datos a los empleados o clientes. Sin embargo, puede que nunca tenga sentido trasladar algunos datos críticos y top secretos a la nube, sino que deberían permanecer en las instalaciones. Los CIO deben entender qué son sus centros de datos y cuáles servirían mejor a los diferentes segmentos de la organización en términos de recuperación, almacenamiento, eficiencia de costos y rendimiento.\n\nMoviéndose hacia los años 20, las organizaciones deben hacer cambios estratégicos, tanto en sus prácticas de contratación como en la forma en que brindan servicio a sus clientes, teniendo en cuenta estos conceptos.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rachel Mushawar"],"link":"/episode-EDT38-es","image":"./episodes/edt-38/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren Pulsipher, Arquitecto Principal de Soluciones en Intel, le pide a Rachel Mushawar, VP de Ventas en Intel, su visión sobre lo que viene para los Años Dorados después de la interrupción del COVID-19."},{"id":109,"type":"Episode","title":"Charla de la máquina de agua en un equipo de trabajo a distancia.","tags":["collaboration","covid","organizationalchange","people","compute","remoteworker","videoconferencing","signalwire"],"body":"\r\n\r\n## Nuevos espacios de trabajo colaborativo.\n\nEn este episodio, Darren y Sean Heiney, co-fundador de SignalWire, Inc., hablan sobre los nuevos espacios de trabajo colaborativos remotos que eliminan las tediosas reuniones de video que se llevan a cabo todo el día. Su política de Cámaras encendidas y la tecnología SignalWire Work permiten una comunicación ad hoc como nunca antes.\n\n## La Creación de SignalWire Work\n\nSignalWire es el desarrollador de la plataforma de comunicaciones de código abierto más grande del mundo. Durante los últimos tres o cuatro años, SignalWire se ha centrado en construir la próxima generación de aplicaciones de comunicaciones en tiempo real de video y voz, lo que llevó a SignalWire Work para oficinas remotas.\n\nUn precursor de SignalWire Work fue la propia herramienta de SignalWire que ellos mismos desarrollaron, ya que la empresa ha estado distribuida desde su inicio, trabajando de forma remota durante tres años. Las herramientas existentes de colaboración remota no satisfacían sus necesidades, con más de 60 personas repartidas en diferentes países y continentes. Algunos ya utilizaban audio constantemente, pero como muchos de sus ingenieros también compartían pantallas de Unix, evolucionó hacia el video constante. Cuando llegó la pandemia de COVID, los clientes quisieron acceder a esta herramienta, por lo que nació SignalWire Work, junto con una versión para eventos en vivo.\n\n## El entorno de trabajo de SignalWire.\n\nTrabajar en espacios colaborativos remotos no solo es una innovación tecnológica, sino también un experimento social. Para algunas personas, el video siempre activado puede parecer extraño o aterrador. Sin embargo, una vez que todos hayan adoptado esta filosofía, puede ser más eficiente que estar en una oficina física. Hay ineficiencias en un mundo como Zoom, donde las personas solo hablan en reuniones programadas con agendas específicas. Se pierde mucha de la comunicación informal importante.\n\nCon las herramientas de SignalWire, los compañeros de trabajo pueden ver dentro de las habitaciones de las personas, como si tuvieran una pared de cristal en un mundo físico. Puedes ver cuán ocupados están, en qué estado de ánimo se encuentran o si están hablando con alguien más. Puedes entrar y hacer una pregunta. Existe una interacción social, conversaciones informales, que no son posibles con reuniones programadas. Cuando inicias sesión en SignalWire Work, es como entrar a una oficina física.\n\nHay características que aseguran la privacidad. Puedes difuminar tu video para que las personas no puedan ver tu rostro, o entrar en modo de enfoque donde las personas no pueden interrumpirte, pero aún saben que estás presente y pueden llamar a tu puerta virtual.\n\nOtro beneficio es que puede haber límites más claros entre la vida familiar y laboral. Cuando te desconectas de la oficina, es una separación total.\n\n## La Tecnología\n\nSignalWire ha creado la tecnología que impulsa desde el timbre Ring hasta partes de Amazon Connect y el motor de servicio al cliente de Netflix; básicamente, están en todas las principales empresas de telecomunicaciones. Ahora, se están enfocando en permitir que esa tecnología llegue a manos de personas comunes y que puedan construir sobre ella. Un ejemplo es que la plataforma de iglesia virtual más popular del mundo se construyó hace aproximadamente un año en SignalWire.\n\nUn aspecto único que le da a SignalWire una ventaja estratégica es que la multiplexación de video se realiza en la nube. Una aplicación de conferencia de video tradicional codificará y transmitirá el video a cada participante en una conferencia. Si hay siete personas, hay siete flujos. Eso es mucho trabajo en el procesador. Es trabajo en su dispositivo transmitir los datos y está sujeto a fluctuaciones y pérdida de paquetes en todos esos flujos individuales, por lo que es posible que tenga una excelente conexión con alguien, pero la otra persona se ve horrible.\n\nCon SignalWire, los clientes envían una señal al nube. La nube toma la señal de todos y la mezcla, luego envía esa señal de vuelta a todos para que haya solo una transmisión y recepción. Esto tiene muchas ventajas, como una mejor duración de la batería, menor consumo de datos y una menor carga de trabajo en los procesadores del dispositivo local.\n\nPara la experiencia del usuario, SignalWire puede mejorar el audio cuando se mezcla junto, o controlar el diseño para que todos vean lo mismo en la misma orientación. Cuando señalas a alguien en la pantalla, por ejemplo, todos pueden ver eso. Para eventos como sesiones de ejercicio en vivo, la plataforma permite a los usuarios escuchar la música de fondo y al anfitrión al mismo tiempo, además de poder ver, digamos, otros treinta participantes. Todo esto se suma a una sensación y experiencia más conectada.\n\nSignalWire puede funcionar en cualquier nube o plataforma, desde un teléfono individual hasta un dispositivo basado en átomos. Las empresas pueden usarlo en su propia infraestructura, lo cual es importante para la seguridad y el control de datos al más alto nivel. SignalWire puede desplegar sus nodos dentro de una red segura para proteger datos sensibles de transitar por Internet público.\n\nLa plataforma es completamente flexible con la tecnología y aplicaciones de los usuarios, permitiendo incluso a un estudio de transmisión importante usarla para producir y editar uno de sus programas, trayendo extras para doblajes en grabaciones existentes debido a la alta calidad en tiempo real.\n\n## Un entorno más profesional en espacios de trabajo remotos.\n\nLa clave para hacer que este nuevo tipo de espacio de trabajo sea viable es lanzarse e implementarlo como política de la empresa. Los beneficios se hacen evidentes rápidamente. Cuando llegas al trabajo, te incorporas a través de video y estás presente e interactúas con tus compañeros de trabajo, tal como lo hacías antes del COVID.\n\nCuando las personas iban a la oficina antes del COVID, probablemente se vestían elegantes para dar una impresión profesional. En este mundo, esa impresión se logra más a través de una buena configuración de audio y video: buenos micrófonos, iluminación, un ambiente de alta calidad. Los empleados están dando lo mejor de sí utilizando la tecnología porque ahora este es un mundo tecnológico. Es una evolución del entorno de trabajo remoto informal de sentarse en la playa o en la mesa de la cocina mientras se prepara la cena. Se pueden tener conversaciones más profesionales en un entorno de trabajo remoto más profesional.\n\nLa mejor manera de experimentar esta tecnología es probarla. Hay una prueba gratuita de 30 días en https://signalwire.com/products/work.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sean Heiney"],"link":"/episode-EDT39-es","image":"./episodes/edt-39/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones del Sector Público de Intel, y Sean Heiney, co-fundador de SignalWire, Inc., discuten las políticas de trabajo remoto de las compañías en cuanto a las cámaras encendidas y su nueva tecnología de espacios de trabajo colaborativos remotos que fomenta la comunicación ad hoc para su fuerza laboral totalmente remota."},{"id":110,"type":"Episode","title":"Modelo de madurez de gestión de la información","tags":["data","informationmanagement","informationmaturity","datagovernance","datawarehouse","datalake","datamesh","datalineage","technology","process","people"],"body":"\r\n\r\nDesarrollar una estrategia de datos puede ser difícil, especialmente si el estado del manejo de información o la trayectoria de tu empresa no están claros. Este Modelo de Madurez de Administración de Información ayuda a los CDOs y CIOs a identificar dónde se encuentran en su trayectoria de administración de información. Este mapa puede ayudar a guiar a las organizaciones a medida que mejoran y avanzan continuamente hacia la máxima organización de datos que les permite obtener el máximo valor comercial de sus datos.\n\nEl modelo representa una serie de fases, comenzando desde la menos madura hasta la más madura: Estandarizado, Gestionado, Gobernado, Optimizado e Innovación. Muchas veces, una organización puede existir en múltiples fases al mismo tiempo. Puedes determinar dónde opera la mayoría de tu organización, y luego identificar a tus pioneros que deberían estar más avanzados en madurez. Utiliza a estos pioneros para pilotar o prototipar nuevos procesos, tecnologías o estructuras organizativas.\n\n## Fase Estandarizada\n\nLa fase estandarizada tiene tres subfases: básica, centralizada y simplificada. La mayoría de las organizaciones se encuentran en algún lugar de esta fase de madurez, por lo que debes analizar los comportamientos, tecnología y procesos de tu organización para determinar en qué punto se encuentra.\n\n## Básico.\n\nCasi todas las organizaciones se ajustan a esta fase, al menos parcialmente. Aquí, los datos solo se utilizan de forma reactiva y de manera improvisada. Además, casi todos los datos recopilados se almacenan en función de marcos de tiempo predefinidos, a menudo de forma indefinida. Las empresas en esta fase básica no eliminan los datos por temor a perder información crítica en el futuro. Por ejemplo, recientemente estábamos trabajando con una empresa que tiene 30 años de copias de seguridad de correos electrónicos, alrededor de 12-10 terabytes, que temen desechar. Esta no es una práctica poco común.\n\nEstas características describen mejor esta fase.\n\n* Gestión por reacción\n\nDatos no catalogados\n\n* Almacena todo en todas partes.\n\n## Centralizado (Recopilación de datos centralizada)\n\nA medida que las organizaciones comienzan a evaluar su estrategia de datos, primero se centran en centralizar su almacenamiento en soluciones de almacenamiento de big data. Este enfoque toma la forma de almacenamiento en la nube o de dispositivos de datos en las instalaciones. Una vez que los datos se recopilan en una ubicación centralizada, la tecnología de almacén de datos puede permitir que se realicen análisis básicos del negocio para obtener información accionable. La mayoría de las veces, estos datos se utilizan para solucionar problemas con los clientes, la cadena de suministro, el desarrollo de productos o cualquier otra área de su organización donde se generen y recopilen datos.\n\nEstos atributos describen mejor esta fase.\n\n* Gestión por reacción\n\n* Recopilación básica de datos\n\n* Almacenes de datos.\n\nAlmacenamiento de grandes datos\n\n* Análisis básico de big data\n\n## Simplificado\n\nA medida que aumenta el número de fuentes de datos, las empresas comienzan a formar organizaciones que se centran en la estrategia, organización y gobernanza de los datos. Este cambio comienza con la oficina del Director de Datos (CDO, por sus siglas en inglés). Existen debates sobre si el CDO debe ser parte de la empresa bajo el CEO o el CIO. No te detengas en dónde se encuentran en la organización. Lo importante es establecer un enfoque organizativo de los datos e implementar un plan de normalización de datos. La normalización brinda la capacidad de correlacionar diferentes fuentes de datos para obtener una nueva visión de lo que está sucediendo en toda la empresa. Ten en cuenta que sin normalización, los datos permanecen aislados y solo parcialmente accesibles. Otro atributo clave de esta fase es la necesidad de desarrollar un plan para manejar el volumen masivo de datos que se recopilan. Debido al aumento en el volumen y el costo de almacenar estos datos, es importante contar con un almacenamiento en niveles. Aunque en las primeras etapas es casi imposible saber la manera óptima de gestionar el almacenamiento de datos, recomendamos utilizar la mejor información disponible para desarrollar planes racionales de almacenamiento de datos, con la advertencia de que esto deberá revisarse y mejorarse una vez que los datos estén en uso.\n\nEstos atributos describen mejor esta fase.\n\nGestión de datos predictivos (comienzo de una organización centrada en los datos)\n\nNormalización de datos\n\nAlmacenamiento jerarquizado centralizado.\n\n## Gestionados (Perfiles de Datos Estándar)\n\nEn la fase de Gestión, las organizaciones han formalizado su organización de datos; los científicos de datos, los administradores de datos y los ingenieros de datos ahora forman parte del equipo y tienen roles y responsabilidades definidas. La gestión de metadatos se convierte en un factor clave para el éxito en esta fase, y varias aplicaciones ahora pueden aprovechar los datos en la empresa. El movimiento de un almacén de datos a un lago de datos permite una mayor agilidad en el desarrollo de aplicaciones centradas en datos. La virtualización del almacenamiento de datos permite una solución de almacenamiento más eficiente y dinámica. Ahora, el análisis de datos puede realizarse en conjuntos de datos de múltiples fuentes y departamentos en la empresa.\n\nEstos atributos describen mejor esta fase.\n\nGestión organizada de datos (Organización de datos en su lugar con roles clave identificados)\n\nGestión de metadatos\n\n* Linaje de datos\n\n* Lago de datos\n\nAnálisis de grandes volúmenes de datos\n\nAlmacenamiento definido por software (virtualización del almacenamiento)\n\n## Gobernado\n\nLa fase gobernada se alcanza principalmente cuando una organización tiene un enfoque centralizado de los datos y logra una aproximación integral para gobernarlos y protegerlos. El CDO trabaja estrechamente con el CSO (Oficial Principal de Seguridad) para garantizar que las estrategias de datos y seguridad trabajen juntas para proteger los valiosos datos de la empresa mientras los hacen accesibles para el análisis. Los datos se clasifican en diferentes categorías basadas en su criticidad, secreto o importancia. El cumplimiento de las regulaciones se automatiza y se aplica a los datos en toda la organización. La visibilidad aumentada sobre el uso de los datos y la seguridad aumenta con las estrategias conjuntas de datos y seguridad, así como con los planes tácticos. La inteligencia artificial básica se está utilizando ampliamente en la organización, y las decisiones comerciales se derivan de los datos. Ahora, los datos se pueden recopilar y catalogar desde todas partes de la empresa, incluyendo dispositivos de Internet de las Cosas (IoT) en los activos físicos de la empresa.\n\nEstos atributos describen mejor esta fase.\n\nClasificación de datos\n\nCumplimiento de datos\n\nSeguridad de datos\n\n* Inteligencia Artificial Básica\n\nDistribución de la Virtualización de Datos / IoT\n\n## Optimizado\n\nA medida que la recolección de datos de las organizaciones sigue aumentando, necesitan encontrar eficiencias en la automatización y la mejora continua de procesos. La automatización de los procesos de datos es el objetivo principal en la fase optimizada. Específicamente, la automatización de la anotación y etiquetado de metadatos reduce el tiempo para obtener valor de los datos. Los datos ahora se han vuelto demasiado grandes para trasladarse a un lugar centralizado, y surge una arquitectura de \"lago de datos distribuido\" como la forma óptima de gestionar los datos. El aprendizaje automático es clave en esta fase para comenzar a proporcionar información a los tomadores de decisiones y ayudar a optimizar las operaciones y el valor empresarial. La aplicación y los datos se implementan en infraestructuras de red, almacenamiento y computación basadas en información histórica y modelos de IA.\n\nEstos atributos describen mejor esta fase.\n\nAutomatización de etiquetado de metadatos.\n\n* Lago de datos distribuido\n\nInferencia de datos / ML\n\n* Infraestructura impulsada por datos\n\n## Innovación\n\nLa organización definitiva se encuentra en la fase de Innovación. No solo se guía por datos, sino que crea nuevos productos, ofertas y servicios basados en los conocimientos obtenidos de los datos dentro y fuera de su organización. Esta fase es cuando la inteligencia artificial/aprendizaje automático ofrece ventajas invaluables. Hay tres subfases en la Innovación: visión, prescriptiva y prospectiva.\n\n## Visión\n\nLa perspicacia es la toma de decisiones basada en datos en función de lo que puedes ver que realmente está ocurriendo en tu ecosistema, por ejemplo, en tu cadena de suministro, desarrollo de productos o fabricación.\n\n## Prescriptive: Prescriptivo\n\nSi bien la perspicacia es valiosa, requiere interacción humana, comprensión e intuición. En el siguiente nivel, prescriptivo, su inteligencia artificial sugiere lo que debería hacer basado en la perspicacia. Esto puede jugar un papel importante en toda su organización, ya que las decisiones se basan en datos desde la cadena de suministro hasta la adquisición de clientes.\n\n## Visión a futuro\n\nEn este paso crucial, los datos en realidad ayudan a crear el futuro. Por ejemplo, la previsión permitiría a una organización de TI proyectar cuánta capacidad necesitará en el futuro basándose en normas históricas e incluso factores como las condiciones cambiantes con sus competidores. La previsión requiere una gran cantidad de datos y entrenamiento de modelos, pero lleva al objetivo final de la empresa en tiempo real.\n\nEstos atributos describen mejor esta fase\n\n* Comprensión (decisiones impulsadas por datos)\n\nPrescriptivo (negocio basado en datos)\n\nVisión anticipada (crear el futuro)\n\nAprendizaje profundo\n\n* Empresa en tiempo real\n\n## Conclusión\n\nEs común sentirse atrapado en una fase y abrumado por la cantidad de cambios necesarios para pasar a una nueva fase de madurez. Sin embargo, cada paso adelante es valioso. Por ejemplo, tal vez te encuentres en una etapa de Centralización y puedas analizar la gestión de los metadatos. ¿Existe la oportunidad de ir más allá de simplemente limpiar los datos y también mejorarlos? Este tipo de pensamiento progresivo te hará avanzar en la cadena de la madurez en el manejo de tu información.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT4-es","image":"./episodes/edt-4/es/thumbnail.png","lang":"es","summary":"En este episodio, hablaremos sobre el Modelo de Madurez de Gestión de Información y cómo podemos utilizar ese modelo para ayudar a nuestras organizaciones a avanzar. Este modelo puede ayudarte a identificar dónde se encuentra tu organización y hacia dónde se dirige en su estrategia de gestión de información, obteniendo en última instancia el máximo valor comercial de tus datos."},{"id":111,"type":"Episode","title":"Investigación médica colaborativa con cómputo confidencial","tags":["confidentialcomputing","cybersecurity","distributedanalytics","sgx","healthcare","data","technology","edge"],"body":"\r\n\r\n## ¿Por qué necesitamos la computación confidencial?\n\nLa infraestructura informática actual se construye con énfasis en el intercambio y la apertura: internet es gratuito y los datos deberían serlo también. Esto ha sido un problema en cuanto a la seguridad. Hemos implementado algunas soluciones que funcionan bien; sabemos encriptar los datos cuando están almacenados y cuando están en tránsito. A pesar de eso, los datos pueden ser atacados de diversas formas mientras se están leyendo, analizando y utilizando.\n\nLos datos sensibles siguen siendo vulnerables, ya sea datos financieros, médicos o de ubicación, tanto desde un punto de vista de visibilidad como de integridad de datos.\n\nAl tratar con datos en el sector de la salud, existen capas adicionales de complejidad. Hay muchas reglas y regulaciones como la HIPAA, y cada estado también tiene sus propias regulaciones en torno a los datos médicos. A pesar de la complejidad y del número de entidades reguladoras, es posible llevar a cabo la computación confidencial, donde tienes la capacidad de compartir datos entre partes que inherentemente no confían entre sí.\n\nPor confidencialidad, necesitamos pensar en algunos factores diferentes: integridad de los datos, confidencialidad de los datos e integridad del código.\n\n## Problemas críticos de privacidad y seguridad de datos.\n\nLos registros de atención médica de una persona, especialmente en Estados Unidos, están dispersos en todas partes entre médicos, especialistas, laboratorios y hospitales. La mayoría de las personas no tienen un acceso fácil a sus registros; es casi imposible crear una imagen completa de tu propia salud. La computación confidencial puede ayudar a derribar estas barreras.\n\nEn primer lugar, puede asegurar que cualquier dato que compartas, puedes confiar en que estará protegido desde el punto de vista de la integridad de los datos; no será modificado por nadie. Puede ser confidencial, lo que significa que se tokenizará o encriptará, pero aún así se puede utilizar para realizar cálculos. Por ejemplo, si una parte desea realizar análisis en algunos datos, no necesita conocer información como nombres, fechas de nacimiento o números de seguridad social. Entonces, si esas partes de los datos pueden ser tokenizadas o encriptadas, se pueden compartir para su análisis siempre y cuando la otra parte sea de confianza.\n\nEsto es donde las medidas de seguridad como la certificación juegan un papel, para que las partes puedan demostrar su identidad. Y esa certificación puede estar ligada hasta el nivel del hardware en los entornos de ejecución confiables que proporciona el hardware. De esta manera, no solo confías en el transporte y el punto final, sino también en la aplicación y cómo utilizará los datos.\n\nHay dos enfoques básicos para esto. Primero está el kit de desarrollo de aplicaciones (SDK, por sus siglas en inglés), lo que significa que el desarrollador puede decidir cómo dividir su código en componentes confiables y no confiables. El otro enfoque es tener un sistema de encriptación en tiempo de ejecución que se puede construir sobre un entorno de ejecución confiable, minimizando el esfuerzo requerido para convertir una aplicación actual en algo que pueda funcionar en ese entorno.\n\nCon un entorno de ejecución confiable basado en hardware que protege las aplicaciones y los datos en uso, se vuelve muy difícil para un actor no autorizado, incluso si tienen acceso físico al hardware, privilegios de root o derechos de administrador del hipervisor, acceder a la aplicación protegida y los datos. El paradigma de cómputo confidencial tiene como objetivo permitir la eliminación incluso del proveedor de la nube de la base de cómputo confiable. De esa manera, solo el hardware y la aplicación protegida están dentro del límite de ataque.\n\nEstos entornos informáticos permiten a los proveedores de servicios en la nube aprovechar al máximo las capacidades del hardware y la mejor seguridad posible, sobre la cual el usuario final tiene control absoluto. Cada parte puede determinar sus propias políticas y las jerarquías de políticas, como las estatales y federales, y cada proveedor de información puede determinar qué políticas se aplican y a quién.\n\n## Estudio clínico de evidencia del mundo real\n\nReunir todos los datos y darles sentido es un gran desafío en la industria de la salud. La cantidad de configuraciones de privacidad y compartición de datos que están en funcionamiento entre diferentes proveedores, dispositivos, ubicaciones geográficas, etc., lo hacen actualmente imposible.\n\nAI-Vets, Intel y algunos socios están trabajando juntos en este problema. La arquitectura brillantemente simple permite su uso en entornos dispares, tipos de datos y políticas, aún pudiendo realizar análisis centralizados.\n\nUn ejemplo de nuestra implementación es una pequeña prueba de concepto: ¿Cómo se analiza entre múltiples partes como hospitales, entornos de investigación y laboratorios, cada uno con sus propios datos y ensayos que pueden estar llevando a cabo en un entorno clínico? Por ejemplo, ¿cómo podemos encontrar alguna correlación entre las personas que toman el medicamento X, digamos para la diabetes, y tienen la condición Y, digamos cáncer, cuando el medicamento X no tiene nada que ver con el tratamiento de la condición Y? Estos dos conjuntos de datos no estarían en el mismo lugar porque son manejados por diferentes proveedores.\n\nSi, sin embargo, los proveedores formaran parte de un ecosistema donde pudieran determinar qué políticas desean aplicar en su punto final, podríamos tener una aplicación centralizada, un portal de investigación central, que tenga conexiones con estos puntos finales. Habría una gestión de claves y acreditación de terceros para verificar las credenciales y autorizaciones mutuas, de modo que todas las partes puedan confiar unas en otras.\n\nEse es un aspecto de la confianza, pero también debemos proteger los datos que se extraerán, consultarán y transmitirán. Para lograr esto, podemos gestionar los datos y aplicaciones dentro de recintos seguros y encriptados. Los datos se manejan utilizando las políticas que establece cada usuario, como la obfuscación de fechas de cumpleaños, números de seguridad social, etc. Esta información puede ser tokenizada, de modo que se convierte en basura completa en manos no autorizadas.\n\nEl portal central puede realizar una consulta que abarca múltiples puntos finales y combina diferentes tipos de datos en su sistema de tiempo de ejecución y realiza un análisis sobre eso. Por lo tanto, en lugar de tener que extraer todo en un lago de datos y luego realizar el análisis, se hace en tiempo real. No hay que esperar a que los datos se publiquen o se limpien primero aplicando todas esas políticas; esto sucede de forma dinámica y sobre la marcha.\n\nEsto permite obtener ideas tremendas. Durante la pandemia, por ejemplo, si tuviéramos que esperar todos los días para obtener los datos y realizar análisis complejos en ellos, sería difícil. Si pudiéramos acceder a datos en tiempo real de todos estos diferentes sistemas en todo el país y el mundo, y aún así poder compartirlo de manera segura, podríamos obtener ideas únicas que de otra manera no serían posibles.\n\nYa hemos visto esto en algunas POCs para sitios de clínicos que hicimos con nuestro socio Fortanix. Tienen una línea de productos que facilita que diferentes entidades definan sus políticas en un entorno de computación confidencial y verifiquen las identidades entre sí, así como gestionen claves y confianzas. El concepto de entornos seguros de ejecución ha existido durante algún tiempo, y se ha vuelto popular, por lo que es más fácil de aprovechar. Los casos de uso para esto son fantásticos.\n\n## Detección automatizada de COVID-19 a partir de imágenes de rayos X del pecho.\n\nEn algunos casos de uso, no solo es necesario asegurar los datos, sino también la propiedad intelectual asociada a algunos algoritmos especializados. Por ejemplo, para detectar automáticamente el COVID a partir de imágenes de rayos X, se manejarían datos radiológicos, datos del paciente y posiblemente un algoritmo patentado para realizar el análisis. Las enclaves pueden proteger tanto los datos como las aplicaciones de miradas indiscretas.\n\nLa enclave segura también protege las otras máquinas en la red porque si alguien envía algo malicioso hacia los nodos finales, el sistema de gestión de claves lo impediría ser intercambiado debido a que no está debidamente confirmado. Las partes eligen exactamente a qué fuentes de datos puede acceder la enclave y está completamente bloqueada, tanto en lo que entra como en lo que sale.\n\n## Registros electrónicos de salud (eHR)\n\nLos registros de salud son un gran desorden en Estados Unidos, con acuerdos en papel HIPAA no ejecutables, etc., y todo se encuentra disperso en diferentes entidades. Un caso de uso que puede ser un buen modelo a seguir es el del gobierno alemán. Han ordenado que los datos de atención médica deben almacenarse en registros electrónicos de salud, y esas aplicaciones deben implementarse en entornos de ejecución confiables. El paciente es el usuario final y determina qué datos están disponibles y para quién.\n\nEse nivel de detalle en términos de lo que está disponible para el usuario final es tremendo. Y no solo se recopilan y comparten todos esos datos de los diferentes sistemas, sino que están enclaustrados seguros, por lo que están completamente protegidos del mundo exterior. Si alguien no autorizado lograra acceder a los datos en sí, serían completamente insignificantes.\n\nEstos entornos de ejecución confiables son el primer paso en la dirección hacia controles que son fácilmente comprensibles y fácilmente aplicables.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Nick Bhadange"],"link":"/episode-EDT40-es","image":"./episodes/edt-40/es/thumbnail.png","lang":"es","summary":"Romper las barreras para acelerar la investigación médica para la cura del cáncer con la computación confidencial. Nick Bhadange, Especialista en Tecnología, AI-Vets, y Darren Pulsipher, Arquitecto de Soluciones Principal, Sector Público, Intel, discuten la necesidad de la computación confidencial en la atención médica y los posibles beneficios a través de casos de uso."},{"id":112,"type":"Episode","title":"El Arte Negro de DevOps","tags":["devops","people","technology","compute","devsecops","cybersecurity","multicloud"],"body":"\r\n\r\nEchemos un vistazo a dónde encaja DevOps en tu infraestructura.\n\nEn la parte inferior de una pila normal, tenemos una capa física que puede significar una nube, un centro de datos, dispositivos de IoT o una infraestructura heredada.\n\nAdemás de eso, normalmente hay una infraestructura definida por software que abstrae la complejidad de gestionar las piezas individuales de hardware.\n\nA continuación se encuentra una capa de gestión de servicios, que incluye la virtualización del ecosistema de contenedores, y una capa de gestión de información distribuida, que incluye el plano de datos, los depósitos de datos y todo lo relacionado con la gestión de sus datos.\n\nA continuación viene la capa de aplicación. Los desarrolladores de aplicaciones utilizan los servicios dentro de las capas de aplicación. Justo en la interfaz entre la capa de aplicación y el plano de gestión de datos y el manejo de servicios se encuentran las herramientas de SecDevOps o DevOps. Estas herramientas incluyen aspectos de seguridad e identidad que brindan una forma segura de integrar y desplegar continuamente sus productos.\n\n## Capa de aplicación / carga de trabajo\n\nEn la parte superior de la capa de aplicación y carga de trabajo que alimenta a SecDevOps, hay tres tipos de cargas de trabajo: cargas de trabajo impulsadas por eventos, cargas de trabajo procedimentales y una combinación de ambas, que son cargas de trabajo impulsadas por GUI o UI.\n\nUn ejemplo simple de una carga de trabajo orientada a eventos sería cuando una orden de compra llega a su sistema, causando que se realicen otras acciones. Pueden haber pasos secuenciales o paralelos, interacciones con humanos, automatización e interacción con varias aplicaciones o subsistemas diferentes dentro de la empresa.\n\nMuchas herramientas de automatización de carga de trabajo están disponibles. Algunas están creadas con guiones y otras utilizan la automatización de procesos robóticos, que son más basadas en GUI y UI. Estas herramientas trabajan en la automatización de servicios subyacentes, de modo que las cargas de trabajo dirigen la interacción de servicios.\n\nLos servicios tradicionalmente se dividen en tres categorías principales: aplicaciones, como productos listos para usar como Word o una aplicación SAP; servicios complejos, construidos para un propósito específico, como una pila MEAN con Mongo; y servicios simples, que hacen una sola cosa, por ejemplo MongoDB, que almacena la base de datos.\n\nHay una nueva categoría debido al crecimiento de la IA y el ML. Muchos servicios no hacen mucho sin un modelo adjunto, por lo tanto, hemos añadido modelos de IA a la capa de servicio, los cuales tratamos de manera similar a un servicio simple.\n\n## Día del Desarrollador en la vida.\n\nDespués de comprender las cargas de trabajo y los servicios, podemos analizar lo que normalmente hace un desarrollador.\n\nUn desarrollador escribirá algún código en su estación de trabajo y ejecutará algunas pruebas de funcionalidad. Luego, verifica el código en GitHub, por ejemplo, y se inicia un flujo continuo de integración y entrega continuas (CICD). Se ejecutan controles de seguridad en el código, como linter, análisis estático y análisis dinámico.\n\nUna vez que haya pasado esas pruebas, generalmente se verificará en una rama de integración donde otras personas del equipo de desarrollo toman los datos, los desarrollan e integran su código con el código del desarrollador. Luego, una vez que haya pasado sus pruebas, se implementará en una etapa de prueba. Una vez que haya pasado esa etapa, pasará a producción.\n\nEste es un típico pipeline de CI/CD, que ha existido durante décadas. Con el paso de los años, las diferentes formas de describir los pipelines se han consolidado y estandarizado, limitando complejidades y errores.\n\n## Pila de DevSecOps\n\nEl canal de comunicación es solo un elemento de una pila SecDevOps.\n\nOtros elementos necesarios incluyen un registro y un repositorio. Piense en ellos como repositorios versionados para mantener artefactos que se generan durante el pipeline de CICD, de manera que estén fácilmente disponibles para ser utilizados una y otra vez.\n\nOtro elemento importante es un marco de automatización. Esto ayuda a aliviar el trabajo humano de ejecutar tareas como controles de seguridad o promover construcciones de una etapa a otra. Las herramientas de automatización son maduras y hay disponibilidad de capacitación, por lo que un buen marco de automatización debería ser fundamental.\n\nAunque la gestión del entorno a menudo crece de forma orgánica con el tiempo, tiene sentido administrar y diseñar los entornos de manera adecuada para obtener mayor confiabilidad y repetibilidad.\n\nUn elemento clave en todo esto es un perfil de seguridad. Debes tener la capacidad de definir perfiles de seguridad, para que puedan ser utilizados en múltiples entornos y en varias aplicaciones.\n\n## Registros / Repositorios\n\nNormalmente hay al menos dos tipos diferentes de repositorios. El primero es un repositorio de preparación, donde se pueden generar imágenes (una recopilación de todo el código necesario para iniciar un contenedor, por ejemplo) y almacenar cosas como claves de identidad y secretos. Este repositorio contiene todo lo necesario para llevar las cosas a producción. Algunas organizaciones pueden tener varios repositorios de preparación a medida que los diferentes elementos pasan por diferentes etapas de madurez hasta llegar al repositorio de producción. Es importante poder retroceder a versiones anteriores si es necesario.\n\nEn el repositorio de producción, o dorado, las imágenes se bloquean, notarizan y cifran. Solo las cosas en el repositorio dorado se mueven a la producción.\n\n## Etapa or Fases\n\nLa mejor manera de pensar en las etapas del pipeline de CICD es que cada etapa funciona en un solo ambiente. Por ejemplo, en una etapa de construcción, hay un entorno de construcción contenido con políticas. Solo cuando se completan todos los pasos en esta etapa, las cosas pueden pasar a la siguiente etapa. Esto evita el consumo excesivo de recursos con construcciones paralelas y ejecuciones que eventualmente pueden fallar. Al mismo tiempo, es mejor no tener demasiadas etapas que obstaculicen el progreso, por lo que es importante tener un plan definido y cuidadoso.\n\n## Pasos\n\nDentro de las etapas se encuentran los pasos donde el trabajo realmente se lleva a cabo. Al construir y probar software, los pasos pueden ejecutarse en paralelo o en secuencia; existen muchas herramientas que permiten definir estas operaciones. Aunque algunos tienen una interfaz gráfica para esto, la mayoría de los desarrolladores prefieren un formato de texto porque permite el control de versiones del flujo de trabajo y los pasos, lo que permite realizar controles de seguridad contra el flujo de trabajo.\n\n## Tubería\n\nCon etapas y pasos definidos, tienes un pipeline real. En lugar de definir un pipeline único para todas tus aplicaciones, lo cual suele fallar porque se vuelve excesivamente complejo con muchas condiciones o demasiado restrictivo, recomiendo utilizar plantillas de pipelines y modificarlas según sea necesario, asegurándose de que cumplan con los estándares de cumplimiento y regulaciones. Es importante establecer un pipeline adecuado al comienzo de un proyecto, al igual que mantener la flexibilidad a medida que el proyecto avanza.\n\n## Entornos\n\nEn lugar de crear entornos improvisados, es mejor crearlos con intención desde el principio. DevOps o SecDevOps pueden implementar políticas de seguridad y cumplimiento en todos los proyectos diferentes, garantizando la seguridad.\n\n## Service Stack would be translated to \"Pila de Servicios\" in Spanish.\n\nEchemos un vistazo a cómo trabajan los desarrolladores, lo cual es en servicios hoy en día. Incluso si los desarrolladores están trabajando en una aplicación monolítica, tienden a agrupar su trabajo en unidades funcionales como bases de datos, nodos de lógica de negocio o capas de transporte. Por ejemplo, utilizando un servicio sencillo como MongoDB. Cuando un desarrollador ejecuta ese contenedor en su laptop, le brinda la funcionalidad que espera para almacenar datos de una manera no SQL en un documento. En la laptop, puede ser el único contenedor en ejecución.\n\nEn un entorno de prueba o desarrollo, puede haber varias instancias de ese servicio en ejecución, y el desarrollador puede implementar un grupo de servicios de MongoDB y conectarlos entre sí para realizar una prueba. El servicio sigue siendo un servicio de MongoDB, pero su comportamiento cambia según el entorno en el que se encuentre. El objetivo de los desarrolladores es escribir código y verificarlo contra el servicio de MongoDB en sus computadoras portátiles para garantizar que se ejecutará correctamente en producción.\n\nUn servicio simple como MongoDB es necesario, pero por sí solo no es muy útil. Los servicios complejos como las pilas LAMP o las pilas MEAN son más importantes. Estos son múltiples servicios que se ejecutan juntos, actuando básicamente como un solo servicio. Al agruparlos juntos, esto despliega un servicio complejo en un dispositivo portátil y hay dos o tres contenedores de servicios simples que se están ejecutando, proporcionando a los desarrolladores la funcionalidad necesaria para hacer check-in de su código.\n\nUna vez que el código se ha verificado, comienza en el canal de desarrollo donde el desarrollador se integra con otras personas. El mismo servicio complejo puede adoptar una forma completamente diferente de hacer las cosas. Se pueden adjuntar muchas políticas de seguridad a ese servicio complejo para asegurarse de que sea seguro, confiable y resistente.\n\n## Definiciones de Servicio/Aplicación\n\nEs importante entender los conceptos de servicios simples y complejos porque los desarrolladores de software deben definir cómo hacer que funcionen. Hay algunas definiciones. Una se llama definición de imagen. Estas se encuentran frecuentemente en el mundo de los contenedores, llamadas imágenes de Docker. El archivo Docker define lo que hay en esa imagen. Esto se considera un contenedor simple por sí mismo, aunque las personas están empezando a utilizar contenedores para cosas más complejas.\n\nDentro de las definiciones de servicio, podemos incluir múltiples definiciones de imágenes, por ejemplo, Docker Compose, operadores de Kubernetes, Helm Charts, Terraform e incluso CNAB. Estas son herramientas que te permiten definir un servicio. Un servicio es más que solo el contenedor; es el entorno en el que se ejecuta el contenedor. Puede incluir definiciones de red, conectividad de volumen o incluso políticas de implementación. Una \"definición completa de servicio\" tiene definiciones de imágenes, configuración y aprovisionamiento.\n\n## Juntándolo todo\n\nCuando un desarrollador está creando un nuevo servicio, no solo está desarrollando el código para la imagen; también está definiendo el entorno o configuración en el que debe ejecutarse. Aquí es donde se puede unir la malla de tu entorno y la definición del servicio. En tiempo de ejecución, se producirá el ambiente que se necesita para que el contenedor se ejecute de manera efectiva de manera repetible, de modo que puedas mover fácilmente el código desde un escritorio a una producción completa lo más rápido posible.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT41-es","image":"./episodes/edt-41/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones, Sector Público, Intel, define los términos comunes de DevOps y explica dónde encaja DevOps en tu organización."},{"id":113,"type":"Episode","title":"Asegurando el flujo de trabajo de DevOps = SecDevOps","tags":["cybersecurity","devops","compute","process","technology","multicloud","devsecops"],"body":"\r\n\r\nUn estudio de hace más de 20 años sobre el retorno de la inversión en seguridad demostró que cuanto antes se implemente la seguridad en el ciclo de desarrollo, más barato es que esperar hasta el final del ciclo. Aunque hemos sabido esto durante dos décadas, aún es un trabajo en progreso.\n\nLa mayoría de las empresas de desarrollo tienen la seguridad como parte de su proceso global de desarrollo, por lo que ha habido mucho avance, pero es un viaje, no una carrera. Se trata de entender todos los diferentes puntos de exposición y debilidades y poder proporcionar los procesos de seguridad adecuados para esos problemas.\n\n## Los Vectores de Ataque\n\nA menudo las personas piensan en los ataques como vulnerabilidades de los empleados o paquetes, por ejemplo, y olvidan la parte del proceso de la historia. En el lado de la operación se encuentra el tiempo que se tarda en cerrar una vulnerabilidad. Por otro lado, está en el desarrollo y entrega de productos. Hay varios puntos de ruptura a lo largo de esa cadena, y esos han sido explotados recientemente en las últimas etapas del proceso de construcción. En cuanto a los clientes, el código que se actualizó era legítimo porque provenía directamente de la fuente. Por lo tanto, debemos pensar cuidadosamente dónde colocar la seguridad en el proceso.\n\n## Seguridad Incorporada...\n\nÁgil, CI/CD, DevOps, ... ¿Dónde está la seguridad?\n\nSi bien solemos centrarnos en la necesidad de seguridad en las transiciones del desarrollador a QA y de QA a operaciones o implementación, la seguridad realmente debería estar incorporada en todo el ciclo de construcción, no solo en algunos puntos de control. El enfoque debería ser una seguridad continua.\n\n## Seguridad incorporada...\n\nHaz que la seguridad sea parte de cada fase\n\nLa seguridad continua es un desafío. La mayoría de los desarrolladores y profesionales de control de calidad no están capacitados en seguridad; esto es una batalla cuesta arriba. La industria intentó este enfoque en los primeros años 2000, pero se encontró con tres problemas. Primero, la rotación es demasiado alta. Segundo, el panorama de seguridad cambia demasiado rápido para mantenerse actualizados, y tercero, como no es su trabajo diario, no se incentiva el comportamiento correcto.\n\n¿Cómo integramos la seguridad en el proceso, automatizamos las cosas clave que queremos hacer y nos apartamos del camino de los desarrolladores para que puedan hacer su trabajo, que es construir, probar e implementar el código? El proceso de seguridad realmente puede destacar al incorporarlo en esas automatizaciones que ya estás haciendo en DevOps, como pruebas unitarias automatizadas, pruebas automatizadas de calidad y regresión, construcción automatizada e implementación automatizada. Esto no resolverá todos los problemas, pero elevará significativamente el nivel para que puedas enfocarte en los desafíos difíciles relacionados con la seguridad.\n\n## Seguridad incorporada...\n\nSignifica que la seguridad está incorporada, no es un solo paso o etapa.\n\nAlgunas herramientas comunes ya ofrecen seguridad automatizada que señala vulnerabilidades. Por ejemplo, GitHub realizará controles de seguridad en los proyectos que usen código Node.js y todos los paquetes incluidos. Esto puede ser útil, pero es demasiado tarde; la seguridad debería estar integrada en el proceso antes de que se compruebe.\n\n## Seguridad incorporada...?\n\n## ¿Cómo llegamos allí?\n\nLas violaciones de seguridad actuales resaltan que la seguridad debe ser incorporada en cada etapa del proceso, incluyendo entre la construcción y la producción, y justo antes de que el script se ejecute para construir la aplicación. Además de inyectar seguridad en el proceso de construcción, necesitamos asegurar el propio proceso de construcción; ha sido un agujero enorme durante mucho tiempo.\n\nMuchas empresas que realizan desarrollo interno están ahora prestando más atención a su proceso de construcción debido a las recientes violaciones de seguridad. Esto es bueno, pero no puede detenerse solo con estas reacciones impulsivas a cada ataque. Necesitamos pensar de manera holística y no esperar al próximo eslabón débil en la cadena.\n\nAlgunas formas prácticas de asegurar el proceso son tratar el servidor de construcción como un activo crítico en la infraestructura general y aplicar las mismas reglas y controles a ese servidor como lo haría para sus sistemas principales. Contar con credenciales adecuadas, asegurar el arranque del firmware, verificar el código, auditar y registrar el sistema, etc., a lo largo de su vida, luego se integra en el proceso de DevOps cuando alguien hace clic en el botón.\n\n## Construido para durar\n\n## De Soluciones a Servicios y Más Allá\n\nMuchas personas no piensan en el propio script como un objetivo. No importa cuántos módulos buenos estén incluidos si el propio script no está protegido. Algunas formas de proteger el script son ejecutando un checksum, y este debería ser versionado, verificado y firmado. Esto añade complejidad para DevOps, pero existen herramientas que pueden ayudar.\n\n## Construye una vez, despliega en todas partes.\n\nAl igual que automatizamos el proceso de desarrollo, podemos incorporar la automatización para implementar estos controles y verificaciones. La automatización previene que otra persona pueda interferir potencialmente con tus construcciones, pero también queremos asegurarnos de que haya una persona que reciba los resultados y verifique las auditorías.\n\nLas herramientas que ya estás utilizando se pueden ampliar para agregar automatización de seguridad y comprobaciones, como las que se hacen para la integración continua del desarrollo ágil o herramientas de automatización en el mundo de Linux.\n\nLas organizaciones también pueden distribuir a su personal de seguridad en los equipos de desarrollo de negocios, de modo que cuando las cosas salgan mal, los expertos en seguridad ya estén integrados en el proceso. Dos lugares en los que debes asegurarte de tener personal de seguridad son en la infraestructura para respaldar, por ejemplo, tu proceso ágil, y en la gestión de productos para obtener los requisitos de seguridad antes de llegar siquiera a un desarrollador.\n\nSiempre hay escasez de personal capacitado y capaz en seguridad y también de financiamiento para contratar a las personas adecuadas debido a la alta demanda. Algunas opciones son capacitar a las personas que ya tienes y proporcionarles las herramientas necesarias. No necesitas un experto en criptografía en cada paso del proceso. Otra posibilidad es en lugar de tener a cada programador responsable de codificar autenticación, credenciales y protocolos en una construcción segura en una biblioteca de infraestructura, tener un equipo que construya módulos en tus lenguajes y entornos que realicen todas las funciones de seguridad. El programador puede utilizar el módulo y este se encarga del trabajo difícil. De esta manera, construyes una vez y despliegas en todas partes.\n\nEstamos viendo cómo las empresas ofrecen herramientas de seguridad SaaS, servicios basados en la nube que pueden ser utilizados para tu aplicación y entorno de ejecución. Este es un gran paso en el proceso. Hay empresas que proporcionan puntos de inyección de seguridad, como la seguridad de la aplicación en un entorno de estilo rápido. Estas comprobaciones de la aplicación, como la desinfección de entradas y la validación de entradas, se pueden integrar en tu entorno funcional, pero eso aún está esperando hasta el final. Recuerda que cuanto antes comiences con la seguridad en el proceso, más barato y menos doloroso será.\n\nTodo esto, por supuesto, requiere más trabajo de integración. Los desarrolladores pueden ser cautelosos con el trabajo implicado, pero si existe un marco de trabajo con seguridad incorporada (y hay prototipos como Ruby on Rails y ciertas infraestructuras en la nube), puede ahorrar muchas horas. Aún así, debes asegurarte de no depender únicamente de la plataforma para la seguridad, ya que podría ser un punto único de fallo.\n\n## La automatización te liberará.\n\nLas violaciones de seguridad en los últimos seis meses han sido profundas. Aquí hay algunos puntos clave de consejo.\n\nLa seguridad debe ser integral en todo el ciclo de vida, desde los requisitos hasta adelante. La seguridad debe estar presente en el ciclo de DevOps en sí mismo, no solo en la codificación y prueba, sino también en la infraestructura que impulsa ese proceso.\n\nAl construir herramientas y objetos de seguridad a través de módulos, construye una vez, hazlo modular y despliégalo en todas partes.\n\nAproveche los servicios que le permitan confiar en la experiencia de otra persona para aumentar su propio equipo de seguridad cibernética, que está desfinanciado.\n\nLa automatización te liberará. Automatiza todo lo que puedas para hacer la seguridad más fácil y rápida, y reducir la fricción para tus desarrolladores y probadores. Con la automatización, puedes eliminar el 80 por ciento de lo que llamamos \"tonterías\" y usar tus recursos limitados en resolver los problemas difíciles.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT42-es","image":"./episodes/edt-42/es/thumbnail.jpg","lang":"es","summary":"En este episodio, Darren Pulsipher, Arquitecto Principal de Soluciones en Intel, y Steve Orrin, CTO de Intel, Federal, discuten por qué y cómo se debe asegurar el pipeline de DevOps. La única forma de entregar un código sólido, resistente y seguro es si la seguridad se construye desde el principio, y cuanto antes mejor."},{"id":114,"type":"Episode","title":"El papel del CIO en la adopción de la nube","tags":["cio","cloudadoption","compute","cloud","csp","multicloud"],"body":"\r\n\r\nEn la primera parte de esta entrevista, Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, y Doug Bourgeois, Director Gerente y Líder de Estrategia de Nube de GPS de Deloitte, hablan sobre la migración a la nube y el papel del CIO.\n\n## Herencia de CIO\n\nDoug se involucró con la nube desde temprano mientras trabajaba para el gobierno federal dirigiendo una gran organización de servicios compartidos. Reconoció los beneficios de la nube para sus organizaciones proveedoras de servicios, y también se dio cuenta del valor para su organización. Primero, ahorraría dinero a nivel de infraestructura, y segundo, sería una oportunidad para construir la nube mientras virtualizaba para ayudar con el problema de la expansión de servidores. Para un servicio en particular, el recorte de costos para el usuario final fue del 40 por ciento.\n\nEn servicios compartidos como la nube, una vez que alcanzas economías de escala, puedes proporcionar los servicios de manera más económica que la mayoría de las organizaciones pueden hacer por sí mismas.\n\n## La transición a la nube.\n\nHace una docena de años, las organizaciones, en su mayoría, se centraban en la nube privada. Mejoraban sus centros de datos para incorporar una combinación de multiinquilino consolidada y con algunas capacidades de automatización integradas. El péndulo cambió aproximadamente tres o cuatro años después hacia la nube pública con los grandes hiperescalarizadores (AWS, Azure, Google).\n\nHace aproximadamente tres años, las organizaciones se trasladaron a un equilibrio en la nube híbrida. Las personas se dieron cuenta de que una amplia variedad de sistemas en su cartera se prestan a diferentes modelos, algunos privados, algunos públicos, algunos híbridos. En general, hoy en día existe un enfoque más holístico para combinar sistemas y nubes con propósitos específicos.\n\n## Colocando la información en el CIO.\n\nEs ahora más importante que nunca que los CIOs tengan un conocimiento profundo de lo que está sucediendo en sus organizaciones, acercándose a la misión y objetivos empresariales para satisfacer mejor sus necesidades. Mientras que antes, un CIO podría limitarse a proporcionar infraestructura, ahora necesitan tomar decisiones arquitectónicas educadas basadas en lo que está disponible. Hay dos razones para esto. La primera es la proliferación de datos, inteligencia artificial, análisis y aprendizaje automático en las capacidades empresariales fundamentales, lo cual requiere una comprensión empresarial fundamental. La segunda es la evolución de la nube que ha alcanzado una nueva fase, la era digital, donde los sistemas centrales de la organización deben ser modernizados para mejorar la capacidad de servicio para sus usuarios finales.\n\nEste viaje ha comenzado a devolver al CIO a donde pertenecen, en la gestión de la información en lugar de centrarse tanto en la infraestructura. Muchos CIOs han sido relegados a la caja de la infraestructura, cuando podrían tener la oportunidad de hacer algo verdaderamente transformador.\n\n## Posicionamiento del CIO para el éxito\n\nEntonces, ¿cómo es el proceso de pasar de ser Director de Infraestructura a ser Director de Información?\n\nUna forma es posicionarse para encargarse de algo nuevo que la organización está intentando hacer, tal vez un nuevo proceso o entrar en un nuevo mercado, o incluso una unidad de negocio que aún no está en marcha. Tienes continuidad y perspectiva ya que has trabajado con todos los diferentes propietarios de aplicaciones, por lo que estás cualificado de manera única para impulsar la iniciativa hacia adelante. Otra forma, más común, es a través de un evento negativo, donde se vuelve evidente que es necesaria un cambio. Un desastre puede ser el catalizador para que un CIO lidere el camino hacia una verdadera transformación.\n\nÚnete a nosotros para la segunda parte de la entrevista....\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Doug Bourgeois"],"link":"/episode-EDT43-es","image":"./episodes/edt-43/es/thumbnail.png","lang":"es","summary":"En la primera parte de esta entrevista, Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, y Doug Bourgeois, Director Gerente y Líder de Estrategia en la Nube de GPS de Deloitte, hablan sobre la migración a la nube y el papel del CIO."},{"id":115,"type":"Episode","title":"Nube en la Era Digital","tags":["cloudmigration","cloud","compute","process","deloitte","multicloud"],"body":"\r\n\r\n## El típico viaje de migración a la nube para una organización.\r\n\r\nDurante los últimos cinco o seis años, las metodologías, herramientas y experiencias en migración a la nube han evolucionado hacia procesos repetibles.\r\n\r\nEl primer paso es decidir cuáles son tus prioridades de migración porque va a suceder en fases, no en una sola gran mudanza. Esto no requiere mucho tiempo ni recursos, pero es de vital importancia. Un ejemplo extremo es que no deberías elegir el mainframe como el primer sistema para mover a la nube, sino un sistema más autónomo como el correo electrónico.\r\n\r\nEmpresas como Deloitte han desarrollado e invertido en herramientas de descubrimiento que ayudan a acelerar los procesos de migración. Estas herramientas tomarán un amplio conjunto de datos, ejecutarán un algoritmo que analiza la complejidad y clasificarán todos los sistemas en diferentes categorías. Comprender la configuración y los puntos de integración de los sistemas existentes y la compatibilidad de los componentes de software es fundamental para la migración a la nube. También es necesario considerar los límites y los marcos de cumplimiento como PCI o HIPAA. La construcción de zonas de aterrizaje para estos entornos en la nube es la fase dos del proceso.\r\n\r\n## Propuesta de valor de la modernización digital.\r\n\r\nA veces hay una cantidad sustancial de trabajo de preparación para las migraciones. La primera ola es la más fácil con la menor cantidad de modificaciones, pero después, en la fase dos, puede haber actualizaciones o cambios en los sistemas operativos, re-plataformas o migración a diferentes tipos de bases de datos, por ejemplo. La tercera ola a menudo implica servidores cliente antiguos o arquitecturas propietarias que requieren una importante reestructuración y pueden llevar meses prepararse para la capacidad en la nube.\r\n\r\n## Digital Modernization and Cloud Migration\r\n\r\nModernización Digital y Migración a la Nube\r\n\r\nEs importante distinguir que la preparación para la nube no es lo mismo que la optimización en la nube; eso viene después.\r\n\r\nEn muchos casos, la fuerza impulsora detrás de la migración a la nube es impulsada por el negocio en lugar de la tecnología. Por ejemplo, un cliente puede no querer continuar con un arrendamiento solo para albergar un centro de datos, o están trasladando sus oficinas físicas. En esos casos, hay un factor de tiempo donde tiene sentido ejecutar la migración en función de la preparación en lugar de la optimización.\r\n\r\nUna vez en la nube, es necesario optimizar porque los factores de coste son diferentes en la nube que en el centro de datos tradicional. El coste de un centro de datos, después de hacer la inversión inicial, está relativamente oculto, mientras que la nube es más un acuerdo de alquiler que continúa en perpetuidad. Muchas veces en los sistemas tradicionales, solucionamos problemas añadiendo más memoria, más CPU o más almacenamiento porque funciona hasta cierto punto, pero esto crea sistemas ineficientes. Si simplemente movemos estos sistemas ineficientes y de alto consumo de recursos a la nube, el modelo de costes será mucho más alto de lo necesario, de ahí la necesidad de optimización.\r\n\r\nParte del proceso de optimización puede implicar un cambio en el proceso. Por ejemplo, para una organización en Canadá, sus costos se dispararon cuando trasladaron una instancia de SAP a la nube. Se dieron cuenta de que no utilizaban esta instancia por las noches ni los fines de semana, por lo que pasaron de un modelo de 24/7 a uno de 16/5. Este cambio les ahorró una cantidad significativa de dinero. Por lo tanto, hay formas de realizar un esfuerzo pequeño pero obtener un alto retorno de valor con enfoques diferentes.\r\n\r\nEstamos finalmente viendo, después de más de una década en la nube, una tendencia emergente de encontrar valor en un cambio de estrategia empresarial en lugar de en la infraestructura. La pandemia de COVID-19 ciertamente fue un factor para acelerar este cambio. Un ejemplo perfecto de esto es la telemedicina. Ya existía, pero había estado estancada durante cinco o seis años antes de la pandemia; ahora este modelo es la norma.\r\n\r\nLas innovaciones transformadoras están ocurriendo en la nube. A medida que más sistemas se trasladan a la nube, las industrias seguirán intentando adoptar diferentes modelos con nuevas capacidades transformadoras.\r\n\r\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Doug Bourgeois"],"link":"/episode-EDT44-es","image":"./episodes/edt-44/es/thumbnail.png","lang":"es","summary":"En la segunda parte de la entrevista, Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, y Doug Bourgeois, Director Gerente, Líder de Estrategia en la Nube de GPS en Deloitte, continúan su discusión sobre la migración a la nube."},{"id":116,"type":"Episode","title":"No todos los núcleos son iguales.","tags":["multicloud","compute","optimization","workloadplacement","workload","migration","process","cloudinstance"],"body":"\r\n\r\nIntel ha estado contratando arquitectos de soluciones en la nube para ayudar a los clientes a moverse inteligentemente hacia las instancias de nube correctas para sus cargas de trabajo. Stephen recientemente se ha unido a Intel y tiene una larga trayectoria en bases de datos en general, con los últimos siete u ocho años enfocados en la nube, aprovechando sus recursos para los clientes.\n\nAlgunos de los aspectos más difíciles con los que Stephen se ha encontrado al ayudar a los clientes a pasar a la nube son la fijación de precios y las expectativas para la migración. Los proveedores de servicios en la nube suelen afirmar que los clientes ahorrarán dinero al pasar a la nube, pero esto no suele ser cierto.\n\nUna razón es que hay un cambio constante en el mercado de proveedores de servicios en la nube en cuanto a características, funciones y capacidades. Otra razón es la ignorancia, por ejemplo, creer que un VCP principal es igual en todos los casos. La migración con una mentalidad de \"levantar y trasladar\" también puede ser muy costosa. La optimización es clave para una migración inteligente a la nube.\n\nPara reducir los costos y potencialmente ahorrar una gran cantidad de dinero, debes elegir las instancias adecuadas para el rendimiento adecuado. No todos los núcleos, o CPUs virtuales, son iguales. Dentro de AWS, tienen el M4, Broadwell de Intel, y el M5, que es una combinación de Cascade Lake y Skylake, y diversas cargas de trabajo pueden aprovechar esas plataformas de manera muy diferente.\n\n## Rendimiento de CoreMark\n\nCoreMark es una aplicación de un solo hilo que prueba movimientos de registros y sumas simples para cualquier cantidad de CPUs. A lo largo de tres generaciones de procesadores Intel, Broadwell, Skylake y Cascade Lake, esta prueba muestra muy poca diferencia entre ellos. Esta información ha sido utilizada erróneamente para orientar las decisiones de los clientes, ya que la prueba no es una forma efectiva de medir las diferencias en los procesos en sí mismos. Pueden haber diferencias enormes en el rendimiento con cargas de trabajo diferentes.\n\n## Base de datos de rendimiento de WL\n\nStephen realizó pruebas digitales en cargas de trabajo reales, enfocadas en bases de datos de código abierto. El entorno Cascade Lake fue tres veces mejor que el entorno Broadwell para el código abierto, lo cual tiene sentido considerando los avances en velocidad y los avances en el chip que aprovechan el acceso de baja latencia. Por lo tanto, al avanzar dos generaciones se obtiene un rendimiento dos y tres veces mejor en estas cargas de trabajo.\n\nLa carga de trabajo y el núcleo deben ser considerados juntos. Por ejemplo, un cliente podría verse tentado a cambiar a un VCPU que Amazon dice que es un 10% más barato. Sin embargo, AWS podría agrupar núcleos en estos procesadores y ahora estás pasando de un entorno donde eres uno de 48 máquinas virtuales en una caja a uno donde hay 128 máquinas virtuales; tu acceso a la memoria es limitado y podrías obtener un rendimiento un 60% menor por un ahorro del 10%.\n\nLas organizaciones a menudo creen que si externalizan a la nube, ya no necesitan un arquitecto de sistemas. Es cierto que los arquitectos ya no instalan y apilan máquinas físicamente, pero ahora lo hacen virtualmente y necesitan comprender qué ofrecen las diferentes instancias para cargas de trabajo diferentes.\n\nIntel está trabajando en una herramienta para mostrar qué cargas de trabajo se ejecutan mejor en qué instancias. Mientras tanto, Intel cuenta con algunas pautas generales y arquitectos de soluciones en la nube para ayudar a orientar a los clientes.\n\nAdemás, es importante realizar muchas pruebas para comprender dónde ubicar las cargas de trabajo, pero también es importante probar y evaluar para validar que estás obteniendo lo que esperas de un proveedor.\n\n## Base de datos de rendimiento de WL: No NVMe.\n\nIntel realizó algunas pruebas en cargas de trabajo en instalaciones nuevas en una instancia de 64 VCPU fija de 64 núcleos. Después de obtener el número inicial, se creó otra instancia y se volvió a ejecutar la misma prueba. Los resultados variaron drásticamente en cada prueba. Esto podría ser debido a una mezcla de procesos que se ofrecen en un determinado nivel de servicio, o puede haber alguna distancia y latencia adicional en las conexiones de almacenamiento en bloque, por ejemplo, o incluso vecinos ruidosos. Por lo tanto, es válido realizar pruebas para asegurarse de que el sistema cumpla con las expectativas.\n\nEn resumen, para aprovechar al máximo el valor de la nube, necesitas educarte, probar los sistemas y aprovechar la ayuda disponible.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steven Holt"],"link":"/episode-EDT45-es","image":"./episodes/edt-45/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, y Stephen Holt, Arquitecto de Soluciones en la Nube de Intel, hablan sobre la optimización en la nube y los estudios que demuestran que los núcleos tienen un rendimiento diferente para diferentes cargas de trabajo."},{"id":117,"type":"Episode","title":"Asegurando tu canal de DevOps","tags":["devops","compute","technology","process","devsecops","cybersecurity","zerotrustarchitecture"],"body":"\r\n\r\n## Los vectores de ataque\n\nMuchos ataques ocurren a nivel de las personas. Algunos de los ataques más recientes y insidiosos se han centrado en el phishing y la ingeniería social en individuos dentro de DevOps. Necesitamos entrenar a las personas mejor en todos los niveles. En un caso, fue un pasante con acceso a las llaves quien cayó en un plan nefasto.\n\nOtro vector es la tecnología: ataques de denegación de servicio tradicionales, ataques de inyección SQL o ataques de desbordamiento de búfer. Los incidentes más recientes utilizan tanto a las personas como a la tecnología para atacar el proceso. Son insidiosos porque ocurren alrededor del proceso de construcción y pueden ser muy difíciles de encontrar. También pueden propagar código malicioso a través de tus clientes. Para infundir confianza en los clientes, las organizaciones deben tener una estrategia para asegurar la cadena de suministro.\n\n## Seguridad del oleoducto\n\nNo todas las tuberías de construcción son iguales, pero en general, constan de cuatro etapas con entornos: desarrollo, construcción, prueba y producción. Estas etapas se pueden desglosar fácilmente en múltiples etapas dependiendo del tipo de producto que estés desarrollando, pero los entornos se vinculan a esas diferentes etapas.\n\nNecesitamos analizar todo el proceso, que incluye software, hardware y procesos, y tomar un enfoque diferente en lugar de solo enfocarnos en infraestructura, como lo han hecho la mayoría de manera efectiva.\n\n## Seguridad del anfitrión y la infraestructura\n\nEn la parte inferior de la pila, necesitas asegurar el hardware en el entorno de desarrollo y construcción. El entorno de pruebas será un poco diferente porque es posible que desees ejecutar pruebas que involucren la inyección de código malicioso. Los entornos de producción suelen estar bien protegidos, especialmente ahora que más empresas ofrecen software como servicio. En los entornos de producción y construcción, debes llevar a cabo todas las medidas de seguridad típicas que harías en entornos de producción o SaaS.\n\nHay tres elementos clave en estos entornos. El primero es la detección. La detección y corrección es una medida de seguridad bien conocida que utiliza registros de servicio que utilizan una plataforma como Splunk para encontrar cualquier cosa fuera de lo común. Asegúrate de hacer esto no solo en entornos de desarrollo, sino también en construcción y producción.\n\nTen en cuenta que en el entorno de prueba, necesitarás múltiples entornos de prueba, algunos más seguros que otros en el lado de la detección, ya que deseas inyectar código erróneo en tus pruebas. No apliques la misma seguridad en todos los entornos; los perfiles de seguridad pueden ser diferentes para cada entorno.\n\nLa prevención es la segunda clave. Esto significa ser inteligente con el hardware, asegurarse de que las cosas estén correctamente actualizadas, tener las actualizaciones adecuadas de seguridad y hacerlo de manera automatizada. Esto debería ocurrir en todos los entornos, incluyendo el entorno de pruebas, y especialmente en el desarrollo y en producción.\n\nLa tercera clave es parte de la prevención: el hardware de raíz confiable. Se puede establecer una cadena de confianza desde el hardware, a través de las secciones de inicio del firmware, hasta los hipervisores y sistemas operativos. La raíz de confianza se puede llevar a los entornos de desarrollo, construcción, prueba y producción. Esto puede incluir contenedores seguros y máquinas virtuales seguras. Por ejemplo, me gusta almacenar mis claves de encriptación y hash en hardware como un módulo TPM y luego también con la extensión Secure Guard de Intel. Incluso si alguien ingresara a la máquina, no podrían robar esas claves.\n\n## Ejecutables de confianza\n\nConfigurar ejecutables confiables es el siguiente paso. Esto significa que puedes ejecutar verificaciones de seguridad contra el código que se comprueba y se compila, y luego hacer check-in del hash con esos cambios. Si se ha inyectado algo en la base de código, puedes detectarlo, ya que no debería haber cambios en el código durante el proceso de compilación.\n\nEn una nueva etapa de controles de seguridad, puedes ejecutar análisis estático en el código o análisis dinámico en el código o violaciones de seguridad. Hay algunas herramientas excelentes que puedes integrar fácilmente en tu tubería típica de DevOps, ya sea que estés utilizando Jenkins o flujos de trabajo de GitHub, por ejemplo.\n\nUna vez que se ha generado un ejecutable, se debe crear inmediatamente el hash y ese hash debe ser versionado junto con el ejecutable; debe permanecer el ejecutable que pasa por todas las pruebas y se implementa en producción. Ese hash garantizará que nada ha sido manipulado.\n\n## Imágenes atestadas y seguras\n\nNormalmente, hay varios repositorios o ejecutables disponibles para usar en el código. El hash que se crea durante la compilación ahora se encuentra en el registro, y puedes certificarlos. Puedes asegurarte de que no se modifiquen al guardarlos en las imágenes. Si alguien necesita volver atrás y hacer un pequeño cambio, por ejemplo, una etiqueta o metadatos, es importante no realizar el cambio y asignarle el mismo número de versión. Es mejor pasar por el ciclo nuevamente, incluso si lleva más tiempo que realizar procesos manuales con tus binarios.\n\nAhora, puedes tomar ese mismo archivo binario en el que has ejecutado todas las pruebas y enviarlo a producción. En este punto, sería un error reconstruir el código fuente. Lo mejor es enviar la compilación original al repositorio de producción, también conocido como el repositorio \"golden\". Este repositorio es el único lugar desde donde deben obtenerse imágenes, binarios o máquinas virtuales, por ejemplo. Todas las imágenes deben ser notarizadas y confirmadas. Si tienes máquinas virtuales o aplicaciones confidenciales, o si deseas asegurarte de que se ejecuten solo en hardware específico, puedes aplicar ese tipo de restricciones. Puedes encriptar las máquinas virtuales, contenedores o incluso los binarios y bloquearlos con la llave que se encuentra almacenada en tus sistemas de compilación y producción.\n\n## Inyectando herramientas de seguridad\n\nLas herramientas de seguridad deben ser integradas en el proceso de construcción. En lugar de usar librerías de seguridad de código abierto o recrear las que ya existen, sus ingenieros de seguridad deben estar involucrados para que puedan elegir herramientas que puedan ser fácilmente utilizadas y reutilizadas por los equipos de desarrollo. Un buen ejemplo es la autenticación básica: inicio de sesión de usuario. Debería haber una librería común en lugar de que cada aplicación tenga la suya propia.\n\nEs importante tratar estas bibliotecas y herramientas de seguridad de la misma manera que tratarías cualquier otro programa de desarrollo de software que compartas en toda tu organización. Compartirlos reducirá el tiempo y aumentará la seguridad en todo tu ecosistema.\n\n## Construye una vez, despliega seguridad en todas partes.\n\nUna vez que hayas establecido tus equipos de seguridad de desarrollo, asegúrate de inyectar las políticas y herramientas en todos tus productos y entornos. Existe una gran tecnología hoy en día que te permite gestionar múltiples entornos. Entonces, cuando se crea una nueva aplicación, se crea en un perfil de seguridad con tus propias imágenes de VM o contenedores como imágenes base que están utilizando los equipos de desarrollo. Al configurar la seguridad en tus VM o contenedores en tus imágenes base, obtienes una conformidad de seguridad instantánea en los diferentes entornos. También existe la posibilidad de integrarse con herramientas de seguridad, para que si encuentras algo inusual en la aplicación, puedas notificar a las herramientas de seguridad. No intentes crear herramientas de seguridad que cubran el cien por ciento de los casos, porque nunca las terminarás; apunta al ochenta por ciento como una base sólida y crea herramientas para que los desarrolladores de aplicaciones puedan innovar en el último veinte por ciento si es necesario, en conjunto con tu equipo de seguridad.\n\nUn último consejo importante es automatizar todo lo que puedas, especialmente en el pipeline de DevOps para prevenir inyecciones maliciosas. Protege tu pipeline; protege tu proceso.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT46-es","image":"./episodes/edt-46/es/thumbnail.png","lang":"es","summary":"En la segunda parte de este episodio, Darren Pulsipher, Arquitecto Jefe de Soluciones de Intel, brinda consejos prácticos para asegurar cada etapa del pipeline de DevOps, incluyendo la protección del hardware y las pilas de software con la raíz de confianza hardware, el escaneo de seguridad, los contenedores/máquinas virtuales atestadas y encriptadas, y más."},{"id":118,"type":"Episode","title":"Esperando con ansias el 2021","tags":["covid","remoteworker"],"body":"\r\n\r\nGreg lidera las ventas y marketing de Intel en Estados Unidos. Los clientes de Intel son principalmente los fabricantes de sistemas OEM y el equipo de ventas de Intel trabaja diariamente para atender esas cuentas y ayudarles a construir sistemas basados en las tecnologías de cómputo y memoria de Intel. Actualmente, hay una demanda sin precedentes de cómputo.\n\nLos equipos de Intel trabajan con empresas de software, proveedores de servicios en la nube, empresas Fortune 1000, escuelas y agencias gubernamentales para hacer que la informática sea accesible y ayudar a identificar tendencias y aplicar casos de uso que resuelvan problemas empresariales de una manera que mejore la sociedad. Intel, entonces, actúa como líder en tendencias e identifica nuevas tendencias para asegurarse de que los clientes tengan los productos adecuados.\n\n## COVID como acelerador\n\nActualmente, todas las tendencias están relacionadas con la pandemia de COVID. Las organizaciones pasaron la mayor parte del 2020 en modo de crisis, con el departamento de tecnología implementando tecnología para mantener los negocios en funcionamiento y luego adaptarse a la nueva normalidad. Ahora, hay estabilización, donde los departamentos de tecnología pueden mirar hacia el futuro en lugar de solo responder a la crisis presente.\n\nUna lección es que la tecnología de la información (TI) puede avanzar mucho más rápido de lo que pensábamos. Por ejemplo, es asombroso lo rápido que casi todas las industrias pudieron cambiar al trabajo remoto. Sin embargo, el COVID funcionó como un acelerante en lugar de un catalizador. La mayoría de los cambios ya estaban planeados, pero el COVID comprimió los plazos. En lugar de, digamos, una implementación planificada de Office 365 de 18 meses, sucedió en una semana y media porque así debía ser.\n\nUna de las principales razones de la velocidad del cambio es que un CIO no podría tomar todas las decisiones que debían tomarse durante la crisis, por lo que la toma de decisiones fue descentralizada para capacitar a aquellos en la línea del frente para trabajar rápidamente y hacer lo que fuera mejor para el negocio.\n\n## Modelo de trabajo híbrido\n\nUn cambio que llevará un tiempo es descubrir el nuevo modelo de trabajo híbrido. Por ejemplo, Intel está trabajando en establecer expectativas sobre la frecuencia con la que los empleados deben ir a la oficina, después de que la gran mayoría haya pasado un año trabajando desde casa. Los departamentos de TI se están preparando e invirtiendo en herramientas que permitan la colaboración entre empleados en la oficina y en casa.\n\nLas organizaciones pueden no saber cómo será su modelo durante muchos meses, por lo que los departamentos de TI deben ser flexibles en su enfoque.\n\n## Aplicaciones y automatización de Experiencia del Empleado\n\nProyectos en línea que fueron dejados de lado en 2020 están volviendo ahora en este nuevo entorno de trabajo híbrido. Los empleados quieren involucrarse con su negocio de la misma manera en que lo hacen como consumidores en el software como servicio. En consecuencia, se están desarrollando muchas aplicaciones en torno a la experiencia del empleado.\n\nAdemás, ha habido mucho progreso en torno a la utilización de bots. Por ejemplo, Intel acaba de lanzar un bot de recursos humanos para mejorar y automatizar la experiencia de los empleados, accediendo a todos los servicios de recursos humanos. Esto lleva a la necesidad de más IA e automatización de IA, lo cual es impulsado por el aprendizaje automático.\n\n## Servicio al Cliente sin fricción, sin contacto.\n\nMuchos de los clientes de Intel están implementando servicios al cliente sin fricciones y sin contacto en el gobierno, el comercio minorista y el entretenimiento. Una industria que se aceleró durante la pandemia es la de las telecomunicaciones, lo cual facilitará aún más estas entregas sin fricciones.\n\nLa inversión de capital para desarrollar la infraestructura de telecomunicaciones y 5G está respaldando ahora más dispositivos de tipo IOT y remotos que, antes de la pandemia, iban a tardar años en materializarse.\n\n## IA y Aprendizaje Federado\n\nIntel ha experimentado un gran aumento en el trabajo con inteligencia artificial y datos protegidos en áreas como el descubrimiento médico y en los mercados financieros. Con los nuevos procesadores Xeon de tercera generación de Intel, existen enclaves seguros de computación (SGX), memoria protegida, que no se puede acceder desde fuera del sistema. Los casos de uso que se están implementando se centran en inteligencia artificial y aprendizaje federado, donde los datos de usuarios y empresas pueden entrenar modelos globales, pero los datos no se comparten en un repositorio central. Con la IA, el concepto de aprendizaje federado y SGX de Intel, esos datos pueden ser protegidos. Se pueden eliminar obstáculos de privacidad y regulación en los datos. Por ejemplo, los datos de un hospital o de un paciente individual pueden ser protegidos, pero aún así utilizarse para entrenar un modelo más global con grandes beneficios.\n\n## RPA stands for Robotic Process Automation. \nTranslation: RPA significa Automatización de Procesos Robóticos.\n\nMuchas empresas están encontrando formas de automatizar tareas, en algunos casos tareas mundanas, para liberar a sus empleados y que puedan trabajar en proyectos de mayor valor. Esta tendencia ha aumentado enormemente en los últimos seis a ocho meses, con un gran crecimiento en el mercado. En los últimos cinco años, los mercados de RPA han recibido más de dos mil millones de dólares en financiamiento de capital de riesgo, en su mayoría provenientes de los mercados financieros de la ciudad de Nueva York. Fuera del RPA, hay muchos marcos de automatización que las personas están utilizando para implementar infraestructura en sus centros de datos y de manera fluida en la nube.\n\n## Computación en el borde\n\nIntel ha comenzado a desarrollar arquitecturas de referencia para ayudar a las empresas a construir su computación perimetral. La ciencia importante aquí es conectar la computación perimetral hasta la infraestructura en la nube y construir tanto una pila de hardware y software, un panel de control y automatización. Esta es otra área de inversión increíble.\n\n## Avanzando\n\n2020 fue un año de caos, adaptación sin precedentes y cambio acelerado. Ahora, en un año 2021 más estable, las empresas pueden construir sobre las lecciones y tendencias resultantes.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Ernst"],"link":"/episode-EDT47-es","image":"./episodes/edt-47/es/thumbnail.png","lang":"es","summary":"Greg lidera las ventas y el marketing de Intel en los Estados Unidos. Los clientes de Intel son principalmente los fabricantes de equipos originales del sistema, y el equipo de ventas de Intel trabaja diariamente para atender esas cuentas y ayudarles a construir sistemas en torno a las tecnologías de cómputo y memoria de Intel. Actualmente, hay una demanda sin precedentes de cómputo. Los equipos de Intel trabajan con empresas de software, proveedores de servicios en la nube, negocios Fortune 1000, escuelas y agencias gubernamentales para hacer que el cómputo sea accesible y ayudar a identificar tendencias y aplicar casos de uso que resuelvan problemas empresariales de una manera que mejore la sociedad. Intel, entonces, actúa como un marcador de tendencias e identifica nuevas tendencias para asegurarse de que los clientes tengan los productos adecuados."},{"id":119,"type":"Episode","title":"VAST Data Revisited - VAST Data Revisitado","tags":["dataarchitecture","data","optane","technology","vastdata"],"body":"\r\n\r\nVAST Data ha estado creciendo rápidamente en los últimos seis meses, con un equipo en expansión y ventas impresionantes, como aproximadamente 70 petabytes de almacenamiento flash al gobierno federal. Además de la compartición de archivos general con sus sistemas de almacenamiento completamente flash, tienen casos de uso interesantes en áreas como secuenciación de próxima generación, microscopía confocal y cargas de trabajo de simulación de túneles de viento.\n\nLa plataforma de la empresa permite resolver problemas de lectura aleatoria porque todos los datos están en almacenamiento flash sin el costo exorbitante; la plataforma maneja un alto volumen de datos con baja latencia.\n\n## Tecnología VAST y unidades Intel Optane\n\nVAST utiliza las unidades Optane de Intel para lograr la paridad de lectura/escritura, entre otros beneficios. Dado que Optane es una memoria persistente, no hay problemas de coherencia de caché y no se necesita DRAM, lo cual es una gran ventaja arquitectónica para evitar fallas o pérdidas de datos. El costo total por usuario también se reduce porque no es necesario comprar máquinas grandes con mucha memoria para el almacenamiento de archivos si se desea más velocidad. Ese nivel se elimina.\n\nEscalabilidad y flexibilidad son otras dos ventajas, ya que se pueden agregar más cajas de datos (D boxes) sin afectar el rendimiento, y son completamente compatibles incluso con múltiples generaciones de flash. No hay un punto único de fallo y no hay un límite teórico; VAST ha probado hasta 100 petabytes.\n\nPuedes mejorar el rendimiento agregando cajas de clientes (C boxes), independientemente de las cajas D. La capacidad con flash es básicamente gratuita, pero debes exponer ese flash con la CPU. Las cajas C son completamente sin estado, por lo que puedes crecer o reducir su tamaño sobre la marcha. VAST garantiza el tiempo de actividad porque tienes acceso completo a cada PCP en el backend.\n\nDependiendo de los requisitos de rendimiento del cliente, puede haber diferentes números de cajas D y cajas C; por ejemplo, un cliente puede no necesitar más rendimiento, pero sí más capacidad, por lo que el clúster puede tener, digamos, 11 cajas C y 37 cajas D.\n\nSi tienes una caja C y una caja D, tienes aproximadamente 40 gigabytes por segundo de ancho de banda, que es el modelo base. Cada adición de una caja D agrega otros 40 gigabytes de ancho de banda. Una caja C no satura todos los IOPS, por lo que si agregas otra caja C puedes obtener 350,000 IOPS de una caja. Dado que el flash está limitado por la CPU, cuanto más CPUs agregues, más te permite escalar.\n\n## Nuevo modelo de negocio de almacenamiento\n\nEl nuevo producto de VAST Data, Gemini, permite a los clientes que necesitan un alto rendimiento sin mucha capacidad licenciar solamente la cantidad de hardware que necesitan de su fabricante contratado, lo que resulta en un enorme ahorro de costos. Los clientes pueden crecer y pagar por la capacidad a medida que la necesiten, en lugar de incurrir en un costo inicial por necesidades futuras de capacidad.\n\nDesde una perspectiva de gastos operativos (OPEX), esto tiene sentido para muchas empresas. Por ejemplo, para un cliente, un sistema all-flash de 30 petabytes que proporciona un terabyte y medio de ancho de banda, el costo es inferior a S3 de acceso poco frecuente de Amazon. Eso es solo para un año; a medida que avanza, en realidad es menos costoso que Glacier si se considera la forma en que Amazon cobra, por gigabyte al mes. Y existe la flexibilidad de volver a un modelo de gastos de capital (CAPEX) si eso tiene más sentido en el futuro.\n\nEs importante destacar que VAST está presentando un dispositivo, no un almacenamiento como servicio, lo que puede convertirse en una pesadilla de soporte debido a todas las variaciones de hardware y firmware, los discos que se están utilizando, las conexiones internas, etc., lo que resulta en una cantidad enorme de complejidad. VAST todavía está enviando las mismas cajas y servidores; simplemente es un modelo de negocio diferente que permite flexibilidad al consumir almacenamiento.\n\nAlgunas áreas interesantes donde VAST podría ser útil son en la investigación médica, como la anatomía patológica digital, que maneja una enorme cantidad de datos a la que a menudo los investigadores tienen dificultades para acceder debido a su almacenamiento barato y lento. La medicina de precisión basada en el perfil genético de una persona también es una posibilidad. Además, ejecutar la inteligencia artificial contra la captura de paquetes podría ser útil para predecir ataques de adversarios avanzados.\n\nOtro caso de uso prometedor es un cliente con todos sus datos en un sistema de almacenamiento en flash VAST en lugar de en la nube, pero realiza todos sus cálculos en la nube, tal vez trayendo solo los resultados. Los resultados suelen ser muy pequeños, quizás un par de bytes de datos, y su extracción es económica. Este tipo de modelo, por ejemplo, podría generar valor a partir del análisis de datos antiguos que actualmente se encuentran almacenados y son demasiado costosos de mover.\n\nHay mucha flexibilidad para hacer un enfoque híbrido o de múltiples nubes donde tienes un sistema de almacenamiento centralizado en las instalaciones que se puede acceder a través de varios proveedores de nube.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Randy Hayes"],"link":"/episode-EDT48-es","image":"./episodes/edt-48/es/thumbnail.jpg","lang":"es","summary":"En este episodio, Darren se pone al día con Randy Hayes, ahora VP de Ventas para el sector público de VAST Data, seis meses después de su última conversación para ver cómo les está yendo en la industria, qué hay de nuevo en VAST y casos de uso interesantes. Su nuevo producto, Gemini, ofrece un modelo de negocio de almacenamiento diferente."},{"id":120,"type":"Episode","title":"Trabajo Remoto Antes, Durante y Después de la Pandemia","tags":["covid","remoteworker"],"body":"\r\n\r\nIdentificaron cuatro áreas que les han ayudado no solo a sobrevivir sino a destacarse durante este tiempo inusual: Anticipar las necesidades de tu gerente, Comunicarse como si tu trabajo dependiera de ello (porque seguro que lo hace), Levanta tu bandera al levantar la bandera por los demás, ¡Bloquea! ¡Eso! Calendario!\n\n## Anticipa las necesidades de tu gerente.\n\nAdivina qué informes necesita tu gerente y ten la información al alcance de tu mano.\n\n* Los informes de ventas, el estado del proyecto y el estado del cliente deben crearse de manera proactiva.\n\n## Comunícate como si tu trabajo dependiera de eso (porque definitivamente lo hace)\n\nNo llenes la bandeja de correo electrónico de otros con información inútil.\n\nPon tu pregunta al principio del correo electrónico. Luego agrega información adicional.\n\nDa a tu equipo un resumen de lo que estás haciendo con información útil.\n\n## Agita tu bandera agitando la bandera por los demás.\n\nUn componente clave del éxito en cualquier empresa es tener logros visibles y tangibles.\n\nNo siempre puedes bajar la cabeza, trabajar y esperar que hable por sí mismo.\n\nReconoce a los demás que te ayudaron a cerrar el trato o terminar el proyecto.\n\n## ¡Bloquea! ¡Eso! ¡Calendario!\n\nEstablecer límites para el tiempo en familia y para cuando el trabajo termina y comienza.\n\nBloquea tiempo para tu tiempo. Necesitas tiempo para descansar tu cerebro.\n\nEstablecer un descanso de 5 a 10 minutos entre reuniones.\n\nTiempo para procesar tu bandeja de entrada.\n\nTiempo al final del día para procesar tu día\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Rachel Dreikosen"],"link":"/episode-EDT49-es","image":"./episodes/edt-49/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren habla con Rachel Dreikosen, Gerente de Desarrollo de Negocios en el Sector Público en Intel, sobre cómo COVID-19 ha afectado su equilibrio entre trabajo y vida personal y por qué comenzó un blog para ayudar a otras profesionales técnicas de ventas femeninas."},{"id":121,"type":"Episode","title":"Modelo de Madurez de Infraestructura","tags":["infrastructurematurity","compute","technology","sdi","cloud","multicloud"],"body":"\r\n\r\nLa transformación digital puede ser difícil. Muchas organizaciones se paralizan al intentar descifrar todas las tecnologías que se les presentan cada día. En este viaje, al igual que en cualquier otro, es importante tener un mapa. En este episodio, Darren explica el \"Modelo de Madurez de Infraestructura\" para ayudar a las organizaciones a descubrir en qué punto se encuentran en su transformación digital, hacia dónde pueden ir a continuación y cómo llegar allí.\n\n## Fase estandarizada\n\nCasi todas las organizaciones se encuentran en esta fase estandarizada o más allá. Comprender esta fase nos brinda una base para comenzar a hablar sobre las cosas comunes que vemos en todas las organizaciones.\n\n## Básico\n\nEl nivel básico es donde encontramos una infraestructura descoordinada y una gestión reactiva. Esto es especialmente común en startups donde una organización compra servicios o hardware según las necesidades, en reacción a eventos en lugar de como parte de un plan maestro. Aquí también vemos conjuntos de software/hardware diseñados específicamente. Una presencia fuerte de tecnología informática casera también es parte de esta fase de madurez.\n\n## Centralizado\n\nEn este nivel, la infraestructura está centralizada. Las organizaciones compran capacidad en lugar de hardware específico. Las compras ya no son reactivas, sino parte de un plan a largo plazo. Normalmente, la organización tendrá configuraciones de servidor comunes para poder comprar en grandes cantidades y lograr eficiencia de costos y mayor confiabilidad. La tecnología de la información está más centralizada y especializada, por ejemplo, en redes, almacenamiento, cálculo y seguridad.\n\n## Simplificado\n\nEn el nivel simplificado, la infraestructura está más consolidada. Diferentes departamentos pueden compartir equipos como servidores, almacenamiento y redes. Las organizaciones obtienen un mejor rendimiento a medida que migran de antiguas formas a nuevas formas de centralización y simplificación. Los costos de OpEx disminuyen a medida que aumenta la eficiencia.\n\n## Fase virtualizada\n\nLa mayoría de las organizaciones hoy en día se encuentran al menos en alguna parte de la fase de virtualización. Aquí es donde encontramos configuraciones estándar y nubes privadas.\n\nLa virtualización de servidores brinda la flexibilidad de ejecutar aplicaciones en diferentes máquinas y moverlas fácilmente entre ellas.\n\nLa virtualización de almacenamiento es un desarrollo más reciente que brinda beneficios similares a la virtualización de servidores: flexibilidad para tomar decisiones más dinámicas. Ahora el almacenamiento puede abarcar todo el ecosistema. El almacenamiento se puede asignar para una aplicación específica, y tanto el almacenamiento como la aplicación pueden moverse dentro de la infraestructura.\n\nLa virtualización de redes, la última incorporación, ha sido abrazada particularmente por los proveedores de servicios de internet. Han realizado una gran inversión debido a la eficiencia de poder realizar cambios dinámicos en la red de forma remota en lugar de depender del hardware específico anterior.\n\nTodas estas tecnologías de virtualización están vinculadas en ofertas en el espacio de la nube privada, tanto comerciales como de código abierto, a menudo en un solo lugar fácil de gestionar. Las organizaciones se están moviendo hacia esta fase, y también cambiando y consolidando su infraestructura de tecnología de la información de un enfoque vertical a uno horizontal, con sistemas, aplicaciones y servicios más que con cómputo, almacenamiento y redes.\n\n## Fase automatizada\n\nEn la fase automatizada, las organizaciones pueden aprovisionar automáticamente la infraestructura, lo que ahorra tiempo al eliminar capas de personas y permisos. Con el uso de portales de autoservicio, un ingeniero o desarrollador de software puede obtener los recursos que necesitan de inmediato. Este aprovisionamiento automático también permite la automatización básica de parches, máquinas virtuales, seguridad y cumplimiento. Ahora, el departamento de IT se encuentra más en un modo proactivo que reactivo. Otras partes valiosas de esta fase incluyen, por ejemplo, el inicio automático de la infraestructura para aplicaciones en producción y aplicaciones de autocorrección.\n\n## Fase Orquestada\n\nA medida que la automatización se vuelve más compleja, se hace necesaria una fase orquestada. La automatización consiste en aplicar acciones a una máquina, mientras que la orquestación implica la coordinación de las acciones que ocurren en varias máquinas o incluso modalidades (almacenamiento, cálculo, red) para una aplicación. Además, las pilas de aplicaciones automatizadas pueden desplegar varias aplicaciones en varios servidores diferentes. En esta fase, las organizaciones también orquestan servicios híbridos, por ejemplo, gestionando recursos en nubes públicas y privadas. Los beneficios de esta orquestación son la reducción de los costos de gastos operativos (OpEx), de los costos de gastos de capital (CapEx) y del tiempo de implementación.\n\n## Tiempo real (Fase gestionada por SLA)\n\nEn una fase en tiempo real, las organizaciones se encuentran en una infraestructura de acuerdo de nivel de servicio. En lugar de aplicaciones, las organizaciones están utilizando servicios que proporcionan valor y vinculan todo. Los servicios compartidos se ejecutan en múltiples nubes híbridas e incluso en infraestructuras heredadas. La agrupación de la infraestructura híbrida, la orquestación basada en políticas y la orquestación basada en servicios optimizan la infraestructura, la gestión de datos y los servicios.\n\n## Conclusión\n\nEs común que diferentes partes de tu organización estén en diferentes fases. Algunos grupos estarán más avanzados que otros; eso es saludable. Pequeños grupos pioneros pueden fracasar sin afectar a toda la organización, o pueden tener un avance y llevar al resto de la organización con ellos. Muy pocas organizaciones han avanzado a la fase más alta en el mapa. La clave está en no desanimarse, sino utilizar el mapa como guía para descubrir dónde te encuentras y cuáles podrían ser los próximos pasos para tu organización.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT5-es","image":"./episodes/edt-5/es/thumbnail.png","lang":"es","summary":"La transformación digital puede ser difícil. Muchas organizaciones se paralizan al intentar descifrar todas las tecnologías que se les presentan cada día. En este viaje, como en cualquier otro, es importante tener un mapa. En este episodio, Darren explica el Modelo de Madurez de Infraestructura para ayudar a las organizaciones a descubrir dónde se encuentran en su transformación digital, hacia dónde pueden ir a continuación y cómo llegar allí."},{"id":122,"type":"Episode","title":"Comprendiendo el agotamiento de los empleados.","tags":["employeeburnout","remotelearning","remoteworker","people","process","compute"],"body":"\r\n\r\nEl trasfondo profesional de Uzair comenzó en ventas, experiencia de consumidores en puntos de venta y evolucionó hacia el desarrollo de productos. Durante diez años, trabajó en una aplicación de despertador que funcionaba de diferentes maneras personalizadas para ayudar a las personas a despertarse motivadas y enfocadas. Aunque ese proyecto falló al final, Uzair aprendió mucho acerca de la motivación humana. Hace unos años, cuando trabajaba con estudiantes en secundaria y preparatoria, notó una gran falta de motivación y enfoque en el primer periodo del día, lo cual parecía empeorar progresivamente. Debido a su experiencia profesional y su propio crecimiento en el bienestar, vio una oportunidad de tener un impacto al crear la aplicación District Zero.\n\n## Bienestar del estudiante de K-12.\n\nLa misión de District Zero es equipar a los estudiantes de K-12 con capacidades integrales del aprendizaje. Esto significa ayudar a los estudiantes a manejar emociones difíciles para recuperar su enfoque y motivación.\n\nLa aplicación utiliza tecnología Agile y el poder del análisis de sentimientos de procesamiento de lenguaje natural para funcionar a gran escala y detectar desencadenantes y ansiedad a través de una encuesta. Una vez que un estudiante completa la encuesta, la aplicación puede sugerir contenido y recursos como un video o un juego para ayudar con los problemas específicos. También cuenta con un sistema de informes para profesores y administradores.\n\nActualmente, los profesores y administradores llevan una carga pesada tratando de ayudar a sus estudiantes con el bienestar, especialmente en el entorno del COVID. Es posible que tengan un formulario de Google conectado a hojas de cálculo de Google y luego utilicen \"control F\" para buscar palabras negativas y frases clave. La aplicación reduce esta carga y evita el subproducto de la negatividad de los profesores y administradores. El sistema descubre los obstáculos y puntos de dolor de los estudiantes y ayuda a resolverlos a través de remedios rápidos y recursos. Si los estudiantes necesitan más apoyo, el sistema los tria a la persona adecuada, como un consejero, y mantiene informado al profesor.\n\nTradicionalmente, cuando un estudiante muestra ansiedad o frustración, por ejemplo, se le lleva directamente a un consejero o trabajador social cuando el problema se vuelve inmanejable. Pero el estudiante no necesita llegar de cero a cien; hay un punto intermedio donde se puede abordar el problema antes de que llegue a la lucha o huida. La aplicación puede ayudar a evitar que un problema se intensifique y ayudar a detectar problemas antes de que sea demasiado tarde.\n\nLa implementación piloto de la aplicación en las escuelas del área de Chicago ocurrió en agosto pasado durante la COVID, por lo que fue especialmente adecuada para ayudar a los estudiantes que habían perdido esa conexión en persona con sus maestros y luchaban con el aprendizaje social y emocional. Un elemento clave para el éxito en el aprendizaje social y emocional es que todos en la comunidad deben estar involucrados y ser empáticos: maestros, administradores, superintendentes, padres y contribuyentes; la empatía y las conexiones no pueden ocurrir únicamente en la escuela.\n\n## Agotamiento laboral del empleado\n\nDurante COVID, los trabajadores mostraron un aumento en la productividad, pero ahora existe el riesgo de agotamiento del empleado. Ya sea el estrés de ser un trabajador de la salud o un trabajador remoto que carece de equilibrio entre el trabajo y el hogar, una multitud de problemas convierte esto en un tema principal para las empresas. Los principios de la aplicación District Zero K-12 pueden aplicarse a los trabajadores para ayudar a aliviar el agotamiento.\n\nEl Distrito Cero comenzó a experimentar con la aplicación en su propio lugar de trabajo, lo que llevó a conversaciones difíciles sobre prioridades y comprensión de las verdaderas necesidades de los empleados. Similar a la aplicación K-12, el sistema puede identificar las dificultades diarias de los empleados y cubrir la brecha intermedia al brindar apoyo antes de que los problemas se intensifiquen.\n\nEl Distrito Cero espera hacer crecer su tecnología para beneficiar a diferentes sectores: empresas, atención médica corporativa e incluso agencias gubernamentales, para su uso en áreas como la prevención del suicidio en asuntos de veteranos.\n\nEsta nueva herramienta y perspectiva sobre el apoyo al bienestar de estudiantes y empleados llega en el momento adecuado, ya que el estrés y el agotamiento se han intensificado debido al COVID, y comenzamos una nueva fase de reapertura y adaptación.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Uzair Hussain"],"link":"/episode-EDT50-es","image":"./episodes/edt-50/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, habla con Uzair Hussain, CEO de District Zero, sobre cómo la aplicación de la compañía que apoya el bienestar mental de los estudiantes de K-12 puede aplicarse a prevenir el agotamiento de los empleados."},{"id":123,"type":"Episode","title":"Casos de uso prácticos de la Memoria Persistente Optane","tags":["dataarchitecture","memverge","pmem","optane","technology","data"],"body":"\r\n\r\nMemVerge envió la primera versión de su software en septiembre de 2020 y, a pesar de la pandemia que ralentizó la educación de los clientes sobre la nueva tecnología, surgió un buen patrón de casos de uso.\n\n## Reducción de costos de los proveedores de servicios en la nube\n\nLa métrica clave para un operador de servicios en la nube es cuántas máquinas virtuales pueden ofrecer a sus clientes y a qué costo. El tamaño de la memoria en los servidores se convierte en el cuello de botella de cuántas máquinas virtuales pueden asignar por servidor, limitando cuán bajo puede ser su precio por máquina virtual.\n\nEl software de MemVerge con Optane proporciona una mayor cantidad de memoria por servidor, asignando un mayor número de máquinas virtuales (VMs), reduciendo así el costo por VM y aumentando la competitividad de los proveedores de servicios en la nube. El costo por VM podría ser tres veces más barato.\n\n## Confiabilidad con bases de datos de gran memoria.\n\nLos clientes financieros, como las bolsas de valores, los bancos y los fondos mutuos, utilizan muchas bases de datos en memoria y aplicaciones en memoria. Además de aumentar el tamaño de su memoria para tener más instancias por servidor, MemVerge resuelve los desafíos de disponibilidad de las bases de datos en memoria. Si los datos no se están guardando en el almacenamiento todo el tiempo, sino que simplemente se encuentran en la memoria, se pierden todos los datos intradiarios en caso de un fallo. Esto es catastrófico. Incluso si se han registrado todas las transacciones, se debe reproducir el registro para recuperar la base de datos, lo cual puede llevar muchos minutos o incluso horas.\n\nMemVerge ofrece un nuevo servicio de datos que tiene una instantánea en memoria. Persiste el estado de la base de datos en Optane, que es mucho más rápido que persistir en almacenamiento. Si hay un fallo, tienes la última instantánea capturada en la memoria persistente y puedes recuperarte a partir de eso. La recuperación solo lleva uno o dos minutos, por lo que es una mejora de 60 a 100 veces.\n\n## Reducción de secuenciación genómica mediante captura instantánea de memoria\n\nEn el ámbito de la genómica, el software MemVerge en combinación con Optane aumenta la productividad de manera exponencial. En un flujo de trabajo de análisis de datos en varias etapas, una memoria más grande significa más paralelismo en el pipeline y en el procesamiento, por lo que todo el proceso es más rápido. El uso de instantáneas también es útil aquí. Si una organización está realizando, por ejemplo, investigación sobre el cáncer o el COVID, y necesita realizar secuencias de ADN o ARN, debe pasar por aproximadamente 50 etapas de procesamiento. Cada etapa puede llevar horas, y necesitan tomar una instantánea del estado de los resultados de la computación intermedia por varias razones: primero, para volver a ejecutar o reproducir los resultados, y segundo, para comparar los resultados si modifican algunos datos. Las instantáneas se guardan en el almacenamiento y esto puede llevar de cinco a 30 minutos. En muchos casos, esto puede llevar más tiempo que el propio cálculo. Entonces, si un trabajo lleva 24 horas, pueden usar ocho horas para el cálculo y 16 horas solo para realizar estas tareas de E/S y guardar esos estados intermedios.\n\nEn lugar de hacer IO, MemVerge utiliza una instantánea después de cada paso y la guarda en la memoria persistente de Optane. En lugar de 16 horas de IO, este proceso puede tomar un minuto. Es la nueva forma de hacer IO; no es necesario realizar la serialización o deserialización para abrir un archivo, leer, escribir, etc. Todo lo que tienes que hacer es tomar una instantánea.\n\nAunque esto requiere mucha memoria, con MemVerge, la memoria es más grande que antes y continuará mejorando a medida que Intel innova. Dos características adicionales ayudan con este problema. Primero, se toman instantáneas periódicamente sin crear copias completas del estado de la memoria; solo son las páginas de cambios, por lo que se minimiza el uso adicional de memoria. Segundo, MemVerge puede mantener hasta 256 capas de instantáneas en la memoria, pero al mismo tiempo, puedes exportar esas instantáneas fuera de la memoria a servidores de almacenamiento o tus propios sistemas de almacenamiento. Esto se hace sin interrumpir ni afectar tu aplicación en ejecución.\n\nBásicamente, estás creando un DVR de memoria porque en lugar de solo ejecutar tu aplicación hacia adelante, también puedes ejecutar hacia atrás casi al instante. Es una experiencia nueva.\n\nLa genómica es solo el primer ejemplo de muchas cargas de trabajo que podrían beneficiarse de esta tecnología.\n\nDado que MemVerge es una startup, se enfocan estrechamente en tres áreas: proveedores de servicios en la nube, aplicaciones financieras con gran memoria, y genómica y trabajos relacionados en la ciencia de datos. Sin embargo, todos estos casos de uso demuestran el poder de la combinación de la memoria persistente Optane y el software de MemVerge.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT51-es","image":"./episodes/edt-51/es/thumbnail.jpg","lang":"es","summary":"Darren Pulsipher, arquitecto jefe de soluciones de Intel, habla con Charles Fan, CEO de MemVerge, sobre casos de uso con su software que utiliza la memoria persistente Optane de Intel de una manera innovadora, eliminando el cuello de botella entre la memoria y el almacenamiento."},{"id":124,"type":"Episode","title":"Telemetría en la nube","tags":["multicloud","optimization","workloadplacement"],"body":"\r\n\r\n## Beneficios de los Arquitectos de Soluciones en la Nube (CSAs)\n\nEl rol de CSA está en auge en la industria en este momento, e Intel ha estado contratando a un equipo de CSAs para brindar valor a sus clientes. Los CSAs pueden ayudar a los clientes a evitar la mentalidad de trasladar y levantar que lleva a costos innecesariamente altos. Estos nuevos CSAs aportan una perspectiva externa y se conectan con la comunidad de CSAs más grande para solucionar problemas.\n\n## Fases de Telemetría\n\nLa telemetría en la nube no debería quedar relegada a tu proveedor de servicios en la nube. Por ejemplo, Amazon Web Services (AWS) tuvo una importante falla en la Costa Este en noviembre pasado, y sus herramientas no estaban informando, por lo que muchos clientes no tenían idea de que estaban fuera de servicio. Las organizaciones necesitan su propia telemetría para monitorear.\n\nLa primera fase es la falta de monitoreo en la nube. La segunda es la exposición de la telemetría donde comprendes lo que estás obteniendo, por ejemplo, la CPU, la memoria y la red, pero es nivel superficial. La siguiente fase es la monitorización y acción, donde recibes notificaciones sobre ralentizaciones, tasas de transacción, tasas de respuesta, etc. La siguiente fase es más profunda en la estructura, donde la nube inteligente moderna controla y predice para la corrección de problemas. Por último, está la automatización completa. Aquí es donde las cosas funcionan por sí solas, escuchan, responden y luego informan después del hecho.\n\nUn ejemplo de automatización completa sería un centro de datos que controla un sistema de calefacción, ventilación y aire acondicionado (HVAC, por sus siglas en inglés), detectando y reduciendo o aumentando la temperatura en diferentes partes de un edificio. En cuanto a las cargas de trabajo, una nube inteligente puede mover las cargas de trabajo para maximizar el valor en capacidad y rendimiento.\n\n## Enfoques actuales y limitaciones.\n\nMuchas veces, sin embargo, los clientes de los proveedores de servicios en la nube relegan la automatización en la capa de infraestructura y olvidan la capa de carga de trabajo. Los clientes necesitan información más allá de lo básico; necesitan telemetría profunda y completa para saber qué está sucediendo realmente. Es un error confiar en que el proveedor de servicios en la nube tenga en cuenta tus mejores intereses en lo que respecta a la telemetría, y esto puede manifestarse en costos elevados.\n\nAlgunos proveedores de servicios en la nube están abriéndose más. Actualmente, AWS está cambiando el juego en cuanto a la telemetría que están proporcionando. Esperemos que otros sigan su ejemplo.\n\n## Recopilemos la telemetría correcta...\n\nElegir las instancias correctas es importante. No todos los núcleos son iguales; se utilizan diferentes núcleos para diferentes tareas, y es importante entender cuáles son para obtener el mejor rendimiento y precio.\n\n¿Qué tipo de información puede recopilar? El diseño del producto de Intel incluye unidades de monitoreo de rendimiento (PMUs, por sus siglas en inglés). Estas son contadores de subnivel y proporcionan información sobre transacciones, retrasos, latencia y cuellos de botella. Hay tres grupos diferentes en PMUs: núcleo, fuera del núcleo y componente sin núcleo. Estos PMUs recopilan información sobre CPI, utilización, frecuencia y TMAM. Todos estos datos están disponibles utilizando AWS.\n\nUtilizando métricas y telemetría real, es una herramienta que te ayuda a optimizar tus cargas de trabajo. Por ejemplo, podrías comparar cómo se ejecuta tu carga de trabajo en tu propio centro de datos con AWS en estos casos, y utilizar métricas para averiguar en qué plataforma deberían alojarse las cargas de trabajo.\n\n## Importancia de TI\n\nAgregar telemetría con evaluación comparativa es una solución definitiva. Con la evaluación comparativa, puedes conocer tu rendimiento y, con la telemetría, puedes analizar el IPC, la utilización y la frecuencia, y tener un panel completo de lo que está sucediendo. Quieres hacer lo mismo en la nube, en lugar de simplemente cargar las cargas de trabajo en una instancia aparentemente más económica.\n\nLos profesionales de IT no deben temer perder sus empleos debido a que las cosas se están moviendo hacia la nube. En cambio, deberían transferir sus habilidades para aprender sobre la telemetría de referencia en lugar de tener una mentalidad de traslado y reubicación. Convertirse en experto en la utilización de la nube también implica utilizar características nativas de la nube como Kubernetes y contenedores. La telemetría también funciona en estas áreas. Con el asesor C de Intel, puedes obtener una telemetría detallada como datos del núcleo y datos fuera del núcleo de tus contenedores.\n\nAunque pueda parecer abrumador cuando se considera todo lo que es posible en la nube, empezar de manera pequeña es la mejor opción. Analiza las aplicaciones adecuadas según el nivel de riesgo. Cataloga tus aplicaciones, analiza la clasificación de las aplicaciones y luego comienza a trasladarlas a la nube en fragmentos de funciones y aplicaciones similares. A medida que explores nuevos servicios y aprendas nuevas aplicaciones, ten en cuenta la arquitectura que hay detrás y formula las preguntas correctas para ser un arquitecto técnico más informado.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT52-es","image":"./episodes/edt-52/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones en Intel, habla con Josh Hilliker, Director de Arquitectos de Soluciones en la Nube en Intel, sobre cómo utilizar la telemetría en la nube para maximizar el valor y la eficiencia."},{"id":125,"type":"Episode","title":"DevOps con velocidad y reducción de riesgos","tags":["devops","compute","technology","process","devsecops","cybersecurity","multicloud"],"body":"\r\n\r\nLas tres áreas clave a equilibrar en la entrega de software son velocidad, riesgo y calidad. La mayoría puede gestionar una o dos de estas cosas, pero añadir la tercera puede ser difícil. Por ejemplo, es posible que puedas entregar a gran velocidad gracias a un excelente sistema de entrega continua, pero una vez que introduces controles de cumplimiento y políticas, te enfrentas a un desafío. ¿Debes detenerte y verificar esas cosas, o tal vez importar algunos resultados de otra herramienta? De repente, tu velocidad se ve afectada.\n\nOpsMx está trabajando para ayudar a las personas a automatizar esos puntos de decisión. La automatización es lo más importante para mantener la velocidad mientras se incrementan los otros dos pilares, riesgo y calidad.\n\n## Solución de OpsMx: Entrega de Software Totalmente Automatizada.\n\nLa automatización funciona como una agregación de datos, mediante la recopilación de resultados de varias herramientas en la cadena de herramientas y luego tiene un mecanismo como un gestor de políticas que proporciona resultados esperados en marcas de verificación de herramientas como BlackDuck. Si parece correcto, el flujo de trabajo no se detiene, como lo hace actualmente, para una verificación humana.\n\n## La entrega más rápida de aplicaciones aumenta las tasas de fallos.\n\nLa velocidad de entrega de aplicaciones ha pasado de semanal o mensual a cada hora, lo que añade presión para producir rápidamente. Sin embargo, al intentar aumentar la velocidad, podrías empezar a perder cierto control sobre la calidad, quizás saltando algunas de las comprobaciones de riesgo. El resultado neto es que cuanto más rápido intentas avanzar, más propenso eres a tener un fallo.\n\n## Puntos de fricción y espacios problemáticos\n\nCon el uso estratégico de la automatización, tienes la capacidad de aumentar la velocidad sin correr riesgos. La verificación continua puede reducir una tarea de tres o cuatro horas a cinco o diez segundos. En lugar de utilizar mano de obra humana para examinar detalladamente los registros en busca de comportamientos anómalos que hayan pasado una verificación binaria de aprobado/fallido, la verificación continua puede hacer esto automáticamente, permitiendo que las personas se enfoquen solo en casos excepcionales.\n\n## ¿Qué significa automatización en este contexto?\n\nLa siguiente pieza utiliza las herramientas requeridas por el equipo de seguridad y cumplimiento, como BlackDuck. Con la automatización, no es necesario detener el flujo de trabajo para que alguien interprete los resultados antes de continuar, porque eso sucederá automáticamente.\n\nLas otras piezas son políticas a un nivel más alto, si te encuentras en una industria regulada con más controles, o algo tan simple como un minorista que, por ejemplo, no quiere lanzar nuevo software dos semanas antes de Navidad. Esos controles pueden ser automatizados para eliminar la necesidad de aprobación humana, permitiendo que el proceso continúe si no hay excepciones. La idea es eliminar tantas barreras humanas como sea posible para permitir que el proceso fluya.\n\nEl conjunto principal de automatizaciones de control y políticas son reutilizables en todos los pipelines; solo tienes que personalizar o crear aquellos para criterios específicos de tu ciclo de lanzamiento.\n\n## Solución de OpsMx: Entrega de software totalmente automatizada.\n\nOpsMx te brinda la capacidad de crear esos pipelines y las automatizaciones de análisis de registros. Spinnaker es la herramienta actual, pero la visión de OpsMx es ser lo más agnóstico de cd posible, permitiendo el uso de todas las herramientas de cd como Jenkins y Microsoft Azure. OpsMx aprovecha la parte de cd para la automatización, pero se encuentra por encima de eso para ayudar a tomar decisiones automatizadas.\n\nUna parte creciente de la herramienta es el aprendizaje automático para ayudar a entender qué se considera como referencia para una aplicación en particular en comparación con un comportamiento anómalo. También existe un modelo de aprendizaje supervisado donde un profesional de DevOps, un ingeniero o propietario del producto pueden especificar un comportamiento anómalo como esperado en el contexto de la aplicación.\n\nEl sistema también permite la auditabilidad. Cualquier cambio o excepción queda documentado. Existe un seguimiento completo de auditoría de todo lo que ocurre, desde quién aprobó una excepción hasta qué artefacto se implementó y qué imagen base se utilizó. Ya sea que se ejecute en la nube o en las instalaciones, se puede ver todo el proceso como una entidad completa.\n\nOtro aspecto de la visibilidad que OpsMx proporciona es una especie de mapa de lo que se implementa en dónde, como la serie de microservicios que actualmente están en el entorno de pruebas, lo que está en ciertas etapas o lo que está en producción, y luego, para cualquiera de esas cosas, puedes profundizar y obtener una vista histórica. Puedes hacer clic en cualquier versión específica y toda la información de auditoría estará al alcance de tu mano.\n\nAdemás, la próxima frontera en la que OpsMx está trabajando es la eficacia de los artefactos que se están capturando, de modo que la cadena de suministro, o linaje, sea transparente.\n\nOpsMx ahora está utilizando una tecnología basada en agentes que puede interactuar, por ejemplo, con recursos que se encuentran detrás de su firewall. El agente actúa como un proxy hacia la capa de inteligencia para que los datos puedan ser recopilados allí. No hay preocupación por abrir o exponer puertos de firewall. Funciona de la misma manera con los proveedores de nube: el agente se puede implementar dentro de la VPC y ya no es necesario arriesgarse a poner claves y secretos en una aplicación basada en la nube. El agente simplemente actúa como un proxy para que la pieza autorizada siempre permanezca dentro de la VPC, asegurando la forma en que se recopila la información.\n\nCon esta nueva dirección, la industria de DevOps en su conjunto se enfrenta a un cambio radical en materia de seguridad y auditabilidad.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT53-es","image":"./episodes/edt-53/es/thumbnail.png","lang":"es","summary":"En este episodio, Robert Boule, Jefe de Ingeniería de Soluciones en OpsMx, se une a Darren para hablar sobre cómo mejorar la velocidad sin aumentar el riesgo en el proceso de DevOps. Las tres áreas clave para equilibrar en la entrega de software son velocidad, riesgo y calidad. La mayoría puede gestionar una o dos de estas cosas, pero agregar la tercera puede volverse difícil. Por ejemplo, es posible que pueda entregar a alta velocidad debido a un excelente sistema de cd, pero una vez que introduce controles de cumplimiento y políticas, se enfrenta a un desafío. ¿Debería detenerse y verificar esas cosas, o tal vez obtener resultados de otra herramienta? De repente, su velocidad sufre."},{"id":126,"type":"Episode","title":"Personas y Procesos en la Transformación Digital","tags":["multicloud","organizationalchange","change","people","hsbc","cio"],"body":"\r\n\r\nAnn comenzó su carrera como programadora, pasó a la gestión de programas y tiene experiencia en adquisiciones y ventas de empresas. Fue la Directora Global de Datos en HSBC y se convirtió en Directora de Información en septiembre de 2016. A lo largo de su carrera, lideró importantes programas de transformación organizativa, incluyendo sistemas de hipotecas nuevos y sistemas bancarios centrales nuevos.\n\nAlgunos de los desafíos de esas transformaciones consistían en formular las preguntas adecuadas durante las evaluaciones. En un programa de transformación hipotecaria, ella primero hizo preguntas sobre las finanzas: ¿Estamos excediendo el presupuesto? ¿Estamos en el plan establecido? ¿Necesitamos solicitar más financiamiento? Para el equipo de liderazgo, ella preguntó si las personas correctas estaban en su lugar con las habilidades adecuadas para lograr los cambios. También profundizó en la imagen general: ¿Qué cree el negocio? ¿Cuáles son los objetivos comerciales? ¿Comprenden claramente por qué es necesario el programa de transformación y qué estamos tratando de lograr? También observó el estado del programa a través de métricas y KPIs.\n\nPara esa transformación, ya tenían un programa establecido y en marcha, pero Ann entró como una especie de consultora interna de negocios. El programa estaba impulsado y liderado por los negocios, pero necesitaban aprovechar la tecnología para lograr esos objetivos y metas empresariales. La tecnología y los objetivos debían estar completamente alineados.\n\nEn la última transformación que lideró Ann, casi tres años en un programa de transformación de cinco años, asumió el cargo de directora del programa tanto para el lado empresarial como tecnológico. Cuatro directores de tecnología de la información (TI) le reportaron, cada uno asignado a un negocio: venta al por menor, comercial, patrimonio e inversión bancaria. Ann los conectaba con sus socios empresariales y se reunían juntos para asegurarse de mantenerse alineados, ya que estaban tratando de lograr lo mismo.\n\nAunque parecería que este tipo de asociaciones serían práctica comercial habitual, Ann encontró que uno de sus mayores desafíos como CIO era alinear a la empresa. Para algo tan grande y complejo como cambiar un sistema bancario central, una nueva aplicación móvil o una nueva interfaz web, por ejemplo, Ann celebraba reuniones varias veces al día con los actores clave porque probablemente había 15 flujos de trabajo diferentes, y la integración era el aspecto clave.\n\nEntrar desde afuera y realizar cambios organizativos es una posición difícil de estar. La gente se pone nerviosa ante el cambio, especialmente cuando confiaban en su líder anterior. Sin embargo, con el tiempo, Ann descubrió que la gente se unía porque sabían que el objetivo era mejorar el negocio, y se sumaron al viaje. Una forma en que Ann logró esto fue sentarse en el suelo con los diferentes equipos en lugar de en una oficina. Conoció a las personas y se convirtió en algo normal que ella estuviera presente y pudieran tener conversaciones y plantear problemas. Obtuvo información directa de las personas que realizaban el trabajo en lugar de información filtrada a través de la gerencia.\n\nUna cosa que los líderes a menudo olvidan es que las personas en los equipos desean tener éxito, y un toque personal, como trabajar junto a ellos, ayuda mucho.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ann Madea"],"link":"/episode-EDT54-es","image":"./episodes/edt-54/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, arquitecto principal de soluciones de Intel, le pregunta a su invitada, Ann Madea, exdirectora de tecnología de la información de HSBC, que reflexione sobre el proceso de grandes cambios de transformación que lideró en las organizaciones."},{"id":127,"type":"Episode","title":"Comunicación durante la transformación","tags":["communication","organizationalchange","people","process","hsbc","cio"],"body":"\r\n\r\nCuando los CIO implementan soluciones, implican nuevos procesos y muy probablemente cambios organizativos. En el programa de transformación más reciente de Ann, avanzaron con la nube. Ann contrató a un responsable de la nube e implementó una estrategia de múltiples nubes con Google, AWS y Azure. Este cambio importante causó preocupación, especialmente entre aquellos que habían estado trabajando con la infraestructura durante mucho tiempo.\n\nPara aliviar la ansiedad y aumentar la participación, Ann animó firmemente a las personas a comenzar a obtener certificaciones para la nube, Google, AWS, etc. Llevó a su equipo a hacer reuniones con Google en Nueva York, donde tuvieron la oportunidad de ver la energía, el ambiente y la colaboración que estaban sucediendo allí. A medida que las personas comenzaron a obtener certificaciones, la emoción por el entrenamiento y la transformación creció.\n\nEl nuevo jefe de cloud tenía \"Días de Google\" en los que las personas venían y presentaban sus casos de uso y cómo estaban utilizando diferentes productos. Hicieron lo mismo con Amazon y Azure. Esto ayudó a los empleados a comprender la magnitud de lo que podrían hacer, generó entusiasmo y permitió su participación.\n\nLa clave de Ann para la comunicación de CIO en medio de una transformación es la confianza, pero verificar. Tres o cuatro meses después de comenzar la transformación, a Ann se le dijo que se enfocara en evaluaciones en áreas como presupuestos, equipos de revisión y restablecimiento de relaciones con el consejo y los reguladores. Sin embargo, ella decidió indagar más profundamente y descubrió que realmente no había un plan que mostrara cómo se iba a cumplir el programa. Esto comenzó una reevaluación del equipo de liderazgo para hacer que el programa avance.\n\nUn CIO implementando una transformación debe estar dispuesto a ensuciarse las manos. Confíe, pero verifique todo con múltiples entrevistas y flujos de información; una o dos fuentes no son suficientes. Luego, Ann cree que es necesario comenzar a comunicarse con transparencia acerca de los problemas para obtener el apoyo de las personas que pueden brindarlo.\n\nUn beneficio de la transparencia es que las personas que te brindan información, como los programadores, confían en que no vas a utilizar esa información en su contra. El equipo de Ann sabe que no debe sorprenderla, y a su vez, ella no sorprende a sus jefes. Por ejemplo, si hay un problema en el centro de datos, ella les dirá lo que sabe y los mantendrá actualizados a medida que va aprendiendo más.\n\nLos empleados quieren saber cómo encajan en el éxito del programa, no solo que tienen que codificar estos cinco programas. Una forma en que Ann logró esto fue realizando muchas reuniones generales. También recorría las áreas de trabajo y realizaba una reunión actualización informal de 30 minutos cada semana, donde las personas podían hacer preguntas. Cada vez que Ann realizaba una gran reunión donde se tomaban decisiones, reunía a todo el equipo, tanto de negocios como de tecnología, y explicaba las decisiones y su impacto. Si las personas saben que hubo una reunión importante pero se mantienen en la oscuridad, comienza el rumor.\n\nCon el teletrabajo actual, Ann dice que también es importante hacer un breve seguimiento con las personas. Los seguimientos continuos conducen a mejores relaciones y disminuyen la ansiedad al hablar con superiores. Los jefes necesitan entender qué motiva a los equipos y de qué están preocupadas las personas. Tienen familias que alimentar, hijos en la universidad o tal vez familiares con problemas médicos.\n\nPor supuesto, no todo se puede discutir con todo el mundo, como reducciones de personal o ascensos, por ejemplo, pero a Ann le gusta convocar reuniones de supervisores llamadas \"reuniones de personal\" donde pueden discutir estos asuntos sensibles. Las reuniones también pueden ayudar a los líderes a comprender a su personal y lo que buscan hacer. Es importante escuchar a aquellos que están un escalón por debajo en lugar de solo a los líderes, ya que están más atentos a los acontecimientos diarios.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ann Madea"],"link":"/episode-EDT55-es","image":"./episodes/edt-55/es/thumbnail.png","lang":"es","summary":"En este episodio, parte dos de dos, Darren y la invitada Ann Madea, ex CIO de HSBC, hablan sobre la comunicación durante la transformación organizacional. Cuando los CIO implementan soluciones, involucran nuevos procesos y, muy probablemente, cambios organizacionales. En el programa de transformación más reciente de Ann, avanzaron con la nube. Ann contrató a un jefe de nube e implementó una estrategia de nube múltiple con Google, AWS y Azure. Este cambio importante causó preocupación, especialmente entre aquellos que llevaban mucho tiempo trabajando con la infraestructura."},{"id":128,"type":"Episode","title":"Organizaciones intermediarias en la nube","tags":["cloudbroker","cloud","compute","technology","people","process","multicloud","organizationalchange"],"body":"\r\n\r\nDave y Kevin fundaron el equipo de intermediación de nube de Intel hace aproximadamente siete años. Los trabajadores de Intel estaban consumiendo la nube pública, y era como el viejo oeste salvaje, con la gente simplemente pasando su tarjeta de crédito para tener acceso. La seguridad era un problema y los gastos se salían de control. Para controlar el caos, crearon una organización de intermediación en la nube.\n\nSe necesitaba un enfoque centralizado para controlar los costos, organizar y crear contratos empresariales con proveedores y establecer la facturación a través de una sola organización. En lugar de utilizar tarjetas de crédito individuales o crear sus propias órdenes de compra, los grupos de negocios de Intel utilizan una orden de compra maestra a través de IT, que luego se les factura a los diferentes grupos. En general, esto ahorra dinero porque ahora Intel tiene poder de compra colectivo, incluyendo oportunidades de ahorro de costos más avanzadas, como comprar capacidad reservada en lugar de pagar precios bajo demanda.\n\nTomó un poco de tiempo que todos se sumaran a la organización, con algunos todavía usando sus tarjetas de crédito para abrir cuentas. Para ayudar a evitar esto, algunos proveedores de servicios en la nube le proporcionaron a Intel un informe de las cuentas que se abrieron con direcciones de correo electrónico de Intel. En lugar de abordar solo estos \"escapes\" como una violación de la política, fue una oportunidad para educarlos sobre los beneficios de usar la cuenta central de Intel: estándares de seguridad ya establecidos, soporte empresarial, entrenamiento y eficiencia de costos.\n\nAdemás de esos beneficios, Intel también construyó un Centro de Excelencia en la nube, un foro basado en la comunidad al que piden a las personas unirse cuando obtienen sus cuentas en la nube. Su crecimiento ha sido de base, proporcionando información y retroalimentación a los miembros.\n\nMuchos desarrolladores y otros usuarios que utilizan la nube simplemente quieren utilizarla sin tener que pensar en la seguridad o el costo, por ejemplo. Contar con el equipo de intermediación de la nube les permite hacerlo. Una analogía sería que el departamento de TI pone al desarrollador en un arenero con todos los juguetes, pero no le permite echar arena fuera del arenero ni jugar con los juguetes fuera. Esto proporciona a la comunidad de desarrolladores cuentas y acceso seguros siempre que lo necesiten.\n\nHay cientos de servicios disponibles en los proveedores de nube pública, y siempre están lanzando nuevos servicios y capacidades. Es difícil para los grupos empresariales tener o mantener experiencia en todos esos servicios. Un equipo centralizado de corredores de nube que se centra en la nube pública y se mantiene al día con los últimos servicios puede ofrecer orientación y conocimiento sobre dónde ubicar diferentes cargas de trabajo. La clave del corretaje es que las personas acuden a un embudo central y son redirigidas hacia los servicios adecuados.\n\nNo solo es útil contar con un equipo especializado de intermediación en la nube, sino también con personas dentro del equipo que se centren en proveedores de nube específicos. Por ejemplo, en Intel, a medida que alcanzaron una masa crítica y los proveedores de nube maduraron y comenzaron a ofrecer una gran cantidad de servicios, Kevin se centró en AWS y Dave se centró en Azure para adentrarse más en cada uno.\n\nUsar múltiples servicios en la nube en lugar de solo uno fue una decisión natural, ya que los grupos empresariales tenían preferencias y distintas cargas de trabajo se desempeñaban mejor con diferentes proveedores. Las diferencias entre los proveedores eran más notables en el pasado. Ahora, hay un terreno más parejo.\n\nUn intermediario de nube no es solo alguien en una posición técnica, sino un \"hombre para todo\". Dave y Kevin se convirtieron en expertos en todo lo relacionado con la nube, como la seguridad y las redes, y educaron a esos equipos a medida que ampliaban su alcance desde las instalaciones hasta la nube pública. Al tener un equipo central de la nube, las demás organizaciones de Intel pudieron educarse, expandirse y crecer. El equipo fue apodado \"La Unión\" debido a su papel central y variado.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Kevin Bleckmann","Dave Shrestha"],"link":"/episode-EDT56-es","image":"./episodes/edt-56/es/thumbnail.png","lang":"es","summary":"En este episodio, parte uno de dos, Darren y los arquitectos de soluciones en la nube de Intel, Dave Shrestha y Kevin Bleckman, hablan sobre la importancia de una organización de intermediación en la nube. Dave y Kevin fundaron el equipo de intermediación en la nube de Intel hace aproximadamente siete años. Los empleados de Intel consumían la nube pública, y era como el salvaje Oeste, con personas que simplemente usaban sus tarjetas de crédito para acceder a ella. La seguridad era un problema y los gastos estaban fuera de control. Para controlar el caos, crearon una organización de intermediación en la nube."},{"id":129,"type":"Episode","title":"Organización del Intermediario de la Nube Parte 2","tags":["cloudbroker","change","people","process","compute","organizationalchange","multicloud"],"body":"\r\n\r\n# Consideraciones para la Selección en la Nube\n\nHace siete años, cuando Dave y Kevin comenzaron por primera vez el equipo de intermediarios de nube de Intel, había algunas diferencias significativas en los proveedores de servicios en la nube, como la disponibilidad regional, pero ahora, con la madurez, las diferencias son menores. Algunos proveedores se especializan en ciertas áreas como IA y aprendizaje automático o servicios de bases de datos administradas y ricas, y hay factores de costo, pero son más similares que diferentes. En general, los proveedores de nube se están convirtiendo cada vez más en una utilidad a medida que maduran.\n\nLa seguridad es buena y bastante estándar en los proveedores de la nube. Anteriormente, Intel utilizaba productos de proveedores externos de terceros para gestionar la seguridad con los servicios en la nube. Ahora, los proveedores de la nube han integrado gran parte de eso en sus plataformas y Intel ha comenzado a utilizar más componentes incorporados. Aún queda mucho trabajo por hacer en este espacio. En algunos casos, todavía tendrás que encargarte de la seguridad por ti mismo, pero los proveedores van en la dirección correcta. Utilizar las herramientas incorporadas no es la opción más fácil para las nuevas empresas porque no está completamente gestionada, pero una vez que tienes el conjunto de habilidades y madurez, al menos las herramientas están disponibles.\n\nLa gestión de costos sigue siendo un arte; muchas personas se sorprenden con su primera factura de la nube. Aunque existen herramientas integradas, asesores y calculadoras de la nube pública que pueden ayudar, también hay costos ocultos. Los cargos de entrada y salida son probablemente la sorpresa número uno, seguido de la sobreprovisión. Intel tiene un optimizador de nube que está impulsado por Densify y ayudará a identificar los recursos sobreprovisionados y recomendar, a través de un análisis detallado, qué tamaños y familias de instancias son los más óptimos.\n\nAlgunas cargas de trabajo deben permanecer en las instalaciones, especialmente para grandes empresas como Intel que tienen un gran centro de datos en las instalaciones. El equipo de intermediarios en la nube cuenta con una herramienta interna llamada \"decision framework tool\" que puede ayudar a colocar la carga de trabajo adecuada en el lugar correcto, y a veces la solución óptima es mantenerla en las instalaciones.\n\nConsideración de la carga de trabajo para la colocación en la nube pública\n\nSin una herramienta para hacer la determinación, hay cinco áreas a considerar.\n\nSeguridad: Asegúrate de conocer los proveedores de identidad, el cifrado, el cumplimiento y las opciones de inicio de sesión único. Estos son los mismos elementos con los que normalmente lidiarías en un entorno local. En lugar de herramientas separadas, es mejor tener herramientas que abarquen tanto el entorno local como la nube pública para que tus equipos no tengan que aprender de nuevo herramientas separadas.\n\nPrivacidad\n\nEstabilidad de la carga de trabajo\n\nAfinidad de datos/gravedad de datos: Asegúrese de que su proveedor de nube tenga disponibilidad en las regiones que necesita. Algunos proveedores de nube tienen zonas de disponibilidad, o centros de datos, en todas sus regiones, y otros no. Además, si una carga de trabajo se conecta a muchos sistemas locales, no tiene sentido poner esa carga de trabajo en la nube pública.\n\nCosto: Todavía existen algunas diferencias de costos entre proveedores de servicios en la nube públicos, especialmente en instancias de computación de gran escala. También considere los costos de salida.\n\n## Cómo vender tu servicio de intermediario en la nube.\n\nComprar el servicio de intermediario de la nube puede ser difícil para algunos, como los desarrolladores que están acostumbrados a tener control y crear cualquier instancia que deseen. También puede haber dificultades iniciales. Cuando las personas en Intel obtuvieron sus cuentas por primera vez, se sentían inseguras y luego seguridad de la información les envió mensajes cuando estaban haciendo algo mal, y ellos no necesariamente sabían qué habían hecho o cómo solucionarlo. A medida que el servicio evolucionó y maduró, esos problemas se solucionaron a medida que el equipo intervenía para ayudar y luego implementaba correcciones automáticas.\n\nLas personas vieron que el servicio era en última instancia un beneficio porque proporcionaba una forma rápida y sencilla de acceder a la nube pública con todo el apoyo que necesitaban. Después de un período, el servicio comenzó a venderse por sí mismo.\n\nUno de los servicios clave que ofrece el equipo es actuar como puente entre los socios comerciales finales de Intel y los grupos de seguridad de la información para neutralizar políticas de seguridad excesivamente agresivas que crean molestias y demasiados problemas de tickets. El equipo proporciona el equilibrio adecuado que permite a los desarrolladores tener suficiente libertar para trabajar dentro de un marco seguro y cumplir con los requisitos de seguridad. Básicamente, los intermediarios trabajan como mediadores entre la seguridad de la información y los desarrolladores.\n\nEl equipo de intermediarios de la nube también ofrece entrenamiento al traer a los proveedores de la nube para realizar talleres. Este beneficio también surgió del Centro de Excelencia en la Nube, donde las personas solicitaban entrenamiento en áreas específicas, y luego el equipo lo negociaba con los proveedores.\n\nAdemás de proveedores externos, Intel utilizó equipos internos para mostrar lo que habían logrado con la nube pública, para que otros equipos pudieran aprovechar ese conocimiento.\n\nLos equipos de intermediarios de servicios en la nube dentro de las empresas pueden proporcionar una variedad de servicios y beneficios como seguridad y eficiencia de costos, especialmente a medida que los servicios de la nube pública maduran y crecen, y se vuelven más necesarios para las operaciones.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Dave Shrestha","Kevin Bleckmann"],"link":"/episode-EDT57-es","image":"./episodes/edt-57/es/thumbnail.png","lang":"es","summary":"En la segunda parte de dos, Darren Pulsipher, Arquitecto Principal de Soluciones, y los Arquitectos de Soluciones en la Nube de Intel, Dave Shrestha y Kevin Bleckman, hablan sobre los beneficios y servicios de una organización de intermediación en la nube."},{"id":130,"type":"Episode","title":"Comprendiendo los problemas desde Edge hasta el centro de datos","tags":["data","edge","compute","edgemere","edgetocloud","cybersecurity","technology","process"],"body":"\r\n\r\n## Integración de misión\n\nHay muchas partes móviles al implementar capacidades en misiones, especialmente en el Departamento de Defensa, pero también en hospitales de Asuntos de Veteranos, Seguridad Nacional, la FDA y FEMA, por ejemplo, y regresando los datos de dispositivos periféricos a estaciones terrestres y centros de datos regionales y empresariales. Los datos deben ser utilizables y confiables para flujos analíticos importantes en los procesos de inteligencia artificial y llegar a manos de analistas para tomar decisiones basadas en los datos en bruto.\n\n## Controladores para Edge: Latencia, Ancho de banda, Seguridad, Conectividad\n\nParte del puzzle es que los dispositivos periféricos se han vuelto más sofisticados y están recopilando más datos de los que podríamos enviar a través de 5G. Las esperanzas de que 5G conquistara todos los datos y los hiciera disponibles de forma transparente en el centro de datos nunca se materializaron con los avances de los dispositivos IoT.\n\nUna de las arquitecturas originales en torno a IoT fue desarrollada por Cisco, llamada \"fog\". La idea del fog consistía en que el centro de datos conectaba el fog con los dispositivos periféricos, de manera que parte del procesamiento y la conectividad se realizaba allí. Si la conectividad de red es confiable y constante, esto funciona bien con suficiente ancho de banda. Sin embargo, la cantidad de datos generada actualmente en el borde por estas organizaciones supera cualquier cantidad de ancho de banda disponible.\n\nCon la IoT, solo se traslada una parte de los datos al centro de datos, por lo que en general, el valor de los datos solo ocurre cuando se analizan allí. El problema es que el centro de datos no puede almacenar y procesar todos los grandes datos. Incluso al enviarlos a la nube, no se resuelve el problema, ya que la nube no puede procesar todos los datos que se encuentran en el borde. Por lo tanto, queremos alejarnos de enviar todos los datos al centro de datos para recolectar el valor y, en cambio, acercar el valor de los datos lo más posible al borde, disminuyendo la cantidad de volumen de datos que regresa al centro de datos.\n\nPor supuesto, no todos los datos se pueden enviar directamente al borde; tiene que haber correlación entre los diferentes dispositivos periféricos. El valor debe estar en un lugar más centralizado, no necesariamente en el centro de datos central, sino tal vez en uno de estos nieblas intermedias o centros de datos regionales. La clave está en mover los datos de manera inteligente y acercar el valor de los datos lo más posible al borde de una manera repetible y sostenible. Al hacerlo, podemos reaccionar mucho más rápidamente en el borde.\n\n## Capa Física Común\n\nPara superar algunos de estos problemas, primero necesitamos una capa física común. Esto significa que es común desde el centro de datos a través de las capas de niebla hasta los dispositivos periféricos; hay una forma de administrar y controlar los dispositivos y obtener ayuda de ellos de manera confiable y común. Esto no necesariamente significa la misma máquina, sino un dispositivo mínimo viable con una interfaz común. Otro beneficio de la capa física común es que si escribe código para una aplicación, puede ejecutarse en cualquier lugar de este ecosistema. Intel tiene una excelente tecnología para esto, como oneAPI, que hace gran parte del trabajo para que pueda escribir código una vez, compilar los binarios para los diferentes tipos de dispositivos, enviarlos a la capa física común y que se ejecute correctamente. En resumen, los beneficios son un modelo operativo común, un modelo de seguridad común y un modo de operación de escribir una vez y ejecutar en cualquier lugar.\n\n## Infraestructura Definida por Software\n\nSDI se aplica al centro de datos en nubes privadas y públicas con sus APIs definidas por software. Con SDI en el borde, obtenemos formas comunes de mover datos. Podemos provisionar recursos en el borde en el centro de datos en cualquier momento, y podemos mover datos de esta manera de manera más fluida.\n\n## Capa de Gestión de Información Distribuida\n\nNecesitamos ser más inteligentes en la gestión y clasificación de datos, moviendo los datos solamente donde van a ser procesados, ya sea en el borde, en un centro de datos regional o en la nube. Aspectos importantes son la catalogación y reutilización de datos, y cumplir con los requisitos de seguridad y cumplimiento normativo. El beneficio de esta capa de gestión de información distribuida es que se envía menos datos al centro de datos, se mueve menos información, y se lleva el valor hacia el borde.\n\n## Capa de Gestión de Servicios\n\nPara realmente llevar el valor hasta el extremo, necesitamos poder implementar aplicaciones en el extremo. Ahí es donde entra en juego una capa de gestión de servicios, o un ecosistema de contenedores. Esto permite enviar microservicios al extremo, a la niebla, al centro de datos o a la nube de manera repetible y confiable. Por ejemplo, si un centro de datos regional falla, no tienes que depender de él para que la malla de servicios siga funcionando.\n\n## Capa de Servicio de Aplicación\n\nUna capa de servicios de aplicación coordina las diferentes aplicaciones para que puedas crear flujos de trabajo que generen el verdadero valor comercial a partir de los datos. Simplemente mover los datos o ejecutarlos a través de un Motor de Analítica no es suficiente. Los datos deben moverse desde el Motor de Analítica a una estación de trabajo del analista. Algunas herramientas en esta capa serían la automatización de procesos robóticos y los pipelines de DevOps. Aquí también es donde puedes aplicar seguridad y cumplimiento en la capa de aplicación.\n\n## Seguridad y capas de identidad\n\nEl aspecto clave de la capa de identidad es establecer confianza entre entidades que están debidamente identificadas. Debemos comprender quién está accediendo a qué y qué dispositivos están accediendo a qué datos, en qué momento y dónde. La identidad se lleva más allá del usuario típico y se extiende a la identidad de aplicaciones, dispositivos en el borde, niebla, centros de datos y nube.\n\nEl gemelo de la identidad es la seguridad. Aquí tenemos detección, remedio, encriptación y establecimiento de la raíz de confianza. Esto resulta en confiabilidad, datos confiables y cumplimiento. Ahora, los datos inteligentes pueden ser enviados al borde, y luego ser transmitidos al centro de datos, pero no se está trasladando cantidades masivas de datos sin procesar, solo lo necesario de una manera segura.\n\n## Vista de alto nivel\n\nPara tener una arquitectura exitosa de borde a la nube que sea repetible, todos estos diferentes elementos son necesarios. Hemos visto que algunas organizaciones construyen una arquitectura específica de borde a la nube y cuando despliegan una nueva capacidad en ese entorno, quedan atascadas. Por ejemplo, si codifican de manera fija los datos que residen en el borde porque siempre los procesarán en el borde, o en el centro de datos para una aplicación que siempre se procesa en el centro de datos, esto resulta en rigidez. También aumenta el tiempo necesario para implementar nuevas capacidades, quizás años en lugar de meses. Si tomamos los aprendizajes de despliegues de aplicaciones de borde a la nube una y otra vez y comenzamos a generalizar, rápidamente descubrimos que caen en una de las capas que hemos identificado.\n\nPara obtener más información, consulta este documento (incluye enlace) sobre la descripción general de alto nivel de esta arquitectura de borde a nube. No somos prescriptivos en cuanto a lo que encaja en esas cajas, pero lo importante es comprender los casos de uso que abarcan. Tenemos ideas sobre lo que hay en cada una de las capas, y estamos desarrollando los ecosistemas para adaptarnos a las necesidades únicas de tu organización dentro de las capas.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT58-es","image":"./episodes/edt-58/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones en Intel, describe los problemas comunes en las arquitecturas desde el borde hasta el centro de datos que ha observado y discutido con clientes en el sector público. Presenta una arquitectura ideal para resolver estos problemas."},{"id":131,"type":"Episode","title":"Evolución de la Privacidad de Datos","tags":["cybersecurity","data","dataprivacy","privacy","process","people","healthcare","sutterhealth"],"body":"\r\n\r\nDespués de graduarse de la escuela de derecho, Jeremy ingresó directamente al Cuerpo de Abogados Generales (JAG) de la Fuerza Aérea de los Estados Unidos y sirvió como oficial en servicio activo durante nueve años. Una de sus responsabilidades fue ser administrador de registros para dos oficinas diferentes. Cambió su enfoque hacia el cuidado de la salud en su última asignación militar como asesor legal de hospitales en el Noreste. Después de dejar el servicio activo (aún está en la reserva), comenzó a trabajar como asesor legal en un hospital y actualmente trabaja en Sutter Health.\n\nAunque el marco legal es el mismo para la privacidad de los datos, existen algunas diferencias en el gobierno y el sector privado: el enfoque, los objetivos y las regulaciones aplicables. En el gobierno, la privacidad de los datos implica mantener los datos protegidos a través de la Ley de Privacidad; mantener en secreto la información confidencial. Aunque existen excepciones, como la Ley de Libertad de Información, los sistemas están diseñados para proteger la información y no divulgarla por defecto.\n\nEn el ámbito de la salud, desde la implementación de HIPAA en 1996, HITECH pocos años después, y ahora con la ONC, los datos están siendo enviados a los hogares, a los dispositivos de los pacientes, y los pacientes pueden permitir el acceso a terceros. \"Acceso apropiado\" es probablemente un mejor descriptor que \"privacidad de datos\".\n\nEl movimiento en los derechos de privacidad es tener más acceso y más control por parte de los individuos. Como paciente, no solo tienes el derecho a tu información, sino que puedes indicarle a tu proveedor de atención médica que la entregue a un tercero: un abogado, un amigo, otro profesional médico, etc. Un paciente también puede especificar el formato de la liberación, ya sea en papel o electrónico, por lo que se otorgan muchos derechos a los pacientes.\n\nEstos derechos de los individuos para controlar sus propios datos no se limitan a la atención médica. Vemos esto en el movimiento actual con el GDPR y leyes recientes aprobadas en Brasil, Canadá y China, y algunos estados de EE.UU. como California, Washington y Virginia.\n\nA medida que el futuro de la privacidad de datos se concentra más en los derechos individuales de acceso, cambiará la forma en que las organizaciones pueden rastrear las cosas. Grandes empresas como Google y Facebook ahora tienen opciones en las que las personas pueden eliminar sus datos o impedir que las empresas los vendan de diversas maneras. Dispositivos de seguimiento como las cookies tradicionales ya no serán tan relevantes, por lo que hará falta algo más que ayude a los anunciantes dirigidos.\n\nMucho datos, por supuesto, como los datos de empleo ya están regulados. El control individual de los datos no es un derecho absoluto; las empresas necesitan datos para funcionar, por lo que podrán retener algunos, pero se regulará más. En Estados Unidos, tendremos más complejidad y más problemas antes de lograr una estandarización. Tenemos 50 estados, cada uno con sus propias regulaciones.\n\nHay tantas leyes como definiciones de información personal, lo cual puede generar conflictos. Sutter, por ejemplo, tiene muchos hospitales en el norte de California y algunas sedes en Hawái, Oregón y Utah. Sutter debe mantenerse al tanto de las regulaciones de esos estados de manera rutinaria, pero si ocurre una violación, entonces el estado donde residen las personas afectadas entra en juego. A veces, las leyes están escritas de tal manera que Sutter debe cumplir con la ley del lugar de residencia de los pacientes en lugar del negocio, lo que se vuelve complejo.\n\nA veces tiene sentido subcontratar este tipo de problemas, y está surgiendo toda una industria legal que ayuda a las empresas a navegar por las regulaciones de privacidad y seguridad de la información.\n\nDesde el lado de tecnología de la información, la seguridad de los datos significa limitar quién tiene acceso a las cosas. Con la privacidad de los datos, se abren puertas para el acceso. Por supuesto, hay un proceso de validación para determinar quién tiene acceso, pero hay un acto de equilibrio entre la seguridad y la privacidad, lo que puede generar mucho trabajo tanto para el lado legal como para el operativo.\n\nLas organizaciones que desarrollan cualquier tipo de aplicaciones que tratan con los datos de las personas deben entender que existen leyes de privacidad que varían en cada país y en cada estado, así como las ramificaciones de usar y almacenar esos datos.\n\nJeremy, junto con los equipos de privacidad y seguridad, colaboran con los equipos técnicos, a veces incluso desde la fase de diseño, para asegurarse de que todo cumpla con las regulaciones. Por ejemplo, hablará con el equipo encargado de construir los portales de pacientes para ver si las cosas que desean hacer cumplen con las regulaciones. Además, ayuda a responder preguntas sobre qué tipo de base de datos sería la mejor o si hay un proveedor de la nube que se pueda configurar de acuerdo con la normativa. Jeremy encuentra que cuanto más se educa y recibe capacitación en aspectos técnicos, más útil puede ser en el proceso.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jeremy Harris"],"link":"/episode-EDT59-es","image":"./episodes/edt-59/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Jefe Arquitecto de Soluciones de Intel, analiza qué significa realmente la privacidad de los datos y su dirección futura junto a Jeremy Harris, Consejero General Adjunto de Privacidad/Seguridad de la Información, en Sutter Health."},{"id":132,"type":"Episode","title":"Gestión del teletrabajo al máximo","tags":["telework","remoteworker","vdi","mfa","cybersecurity","people","process"],"body":"\r\n\r\n## Modos de operación de teletrabajo\n\nLos modos de operación en los que vemos a las personas trabajar dentro del Departamento de Defensa y también en el sector privado son los siguientes:\n\nDispositivo como una terminal tonta (VDI - Infraestructura de Escritorio Virtual)\n\nDispositivo como parte de la red interna (VPN - Red Privada Virtual)\n\nDispositivo como un portal a los servicios\n\nLa mayoría de los trabajadores están utilizando múltiples modos de operación, por lo que se deben admitir diferentes tipos a través de la infraestructura.\n\n## Dispositivo como una terminal tonta.\n\nEn este modo, el trabajador tiene su escritorio funcionando en un servidor en el centro de datos y utiliza su laptop para conectarse al \"escritorio virtual\". Básicamente utilizan su laptop u otro dispositivo como una terminal \"tonta\". Hay algunos problemas con VDI, incluyendo escalabilidad, congestión de red, latencia y redundancia. Debido a estos problemas, una sesión de VDI puede ser bastante costosa. Sin embargo, de las tres opciones, VDI es la más segura.\n\n## Dispositivo como parte de la red interna.\n\nEste modo es probablemente el más común. El trabajador conecta su dispositivo a la VPN para que pueda trabajar en la red como si estuviera conectado en la oficina. Uno de los beneficios es la movilidad, ya que los trabajadores pueden conectar cualquier dispositivo como un teléfono, una tableta o una computadora portátil. Otro beneficio es que puede haber segmentos diferentes de VPN para diferentes clasificaciones de datos. Una desventaja es que los trabajadores siempre tienen que estar conectados para poder trabajar. Y aunque no hay tanto tráfico de red como con un VDI, aún habrá algo de congestión de red a medida que los datos se mueven.\n\n## Dispositivo como un portal a servicios.\n\nMás personas se están moviendo hacia este modo. El trabajador utiliza su dispositivo para conectarse a servicios en la nube, públicos o privados. El servicio principal que las personas están utilizando es Office 365, que permite a las organizaciones utilizar servicios en la nube en lugar de, o junto con, sus propios servicios. Eficiencia, rendimiento y la capacidad de trabajar desconectado en ocasiones son beneficios. Los inconvenientes incluyen depender de un tercero... (no estoy seguro/a cómo terminar esto - adopción/migración y seguridad)\n\n## Emerging Bottlenecks \nCuellos de botella emergentes\n\nLos cuellos de botella han surgido a medida que el número de personas que trabajan desde casa ha aumentado del 15% al objetivo de más del 85%. Para los primeros dos modos de funcionamiento, la escalabilidad de VPN puede ser un cuello de botella importante. El ancho de banda limitado también es una gran fuente de cuellos de botella. Una solución para estos problemas es aprovechar los servicios en la nube para aliviar la presión. A nivel humano, las operaciones de TI y los servicios de asistencia están abrumados ya que los trabajadores se conectan desde casa por primera vez.\n\n## Escalabilidad de VPN\n\nHay varias soluciones a corto plazo para ayudar a aliviar estos cuellos de botella. Priorizar el acceso de los usuarios puede ser efectivo, ya sea en función de horarios o prioridad de la misión. Es necesario educar a los empleados sobre cómo adaptarse a este entorno.\n\nPara una escalabilidad a largo plazo, las organizaciones deberían migrar hacia soluciones SaaS utilizando computadoras portátiles como modo de portal.\n\n## Ancho de banda a Sitios\n\nUna de las mejores cosas que las organizaciones pueden hacer a corto plazo es averiguar cuántos usuarios de VDI tienen actualmente y ver si pueden moverlos a trabajar en la red o en modo de operación con herramientas de colaboración. Esto reducirá drásticamente la carga en el ancho de banda. Es posible que también deba aumentar las capacidades de su red después de evaluar cómo trabajan sus usuarios remotos. La educación es, una vez más, una parte esencial de este cambio para que los trabajadores utilicen las mejores prácticas, por ejemplo, desconectarse de la VPN cuando no la estén utilizando y configurar herramientas de respaldo para que funcionen fuera del horario laboral. A largo plazo, sugerimos una arquitectura de nube híbrida múltiple que le brinde la capacidad de aprovechar los proveedores de servicios en la nube para el ancho de banda de la red y la capacidad de explosión, y optimizar los costos y la capacidad.\n\n## Escalabilidad de servicios alojados\n\nPara ayudar en la implementación de arquitecturas escalables con fines a corto plazo, existen varias referencias excelentes, incluyendo las arquitecturas de referencia de Outlook Web Access (OWA) y VDI.\n\nDe nuevo, a largo plazo, recomendamos migrar a una infraestructura de nube híbrida múltiple para lograr elasticidad, capacidad, rendimiento predictivo, cumplimiento y seguridad.\n\n## Operaciones de TI\n\n¿Cómo podemos escalar al equipo de la mesa de ayuda que probablemente esté abrumado? Una idea es tener preguntas frecuentes en línea para facilitar la referencia de los trabajadores. Las soluciones contribuidas por la comunidad de usuarios y moderadas por el departamento de TI también pueden ser útiles. Idealmente, las organizaciones deberían utilizar un sistema de gestión de tickets para identificar cuellos de botella y agilizar los procesos. Además, todo lo que se pueda automatizar para evitar tareas repetitivas debería ser automatizado a través de la automatización de procesos robóticos (RPA) o scripts adicionales, por ejemplo. Una solución a largo plazo podría ser la implementación de Chat Bots de IA como una mesa de ayuda de autoservicio de TI. Rápidamente pueden reducir las soluciones en línea utilizando palabras clave o recomendar el contacto.\n\n## Traducción: Conclusión\n\nEn última instancia, Intel desea ver que las organizaciones tengan éxito durante este momento difícil en el que los trabajadores están cambiando de la oficina al trabajo remoto y lidiando con mucho estrés. Intel puede ayudar a la industria, al gobierno y a los sectores públicos. Tenemos silicio que funciona en todos estos aspectos. Contamos con socios que ofrecen soluciones de hardware y software, y por supuesto, vendemos PCs y dispositivos para clientes que permiten a los trabajadores remotos.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Tim Flynn","Pete Schmitz"],"link":"/episode-EDT6-es","image":"./episodes/edt-6/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren, Tim Flynn, Retirado Contralmirante de la Armada, y Pete Schmitz, Ejecutivo de Cuentas de la Armada de Intel, hablan sobre cómo gestionar el crecimiento explosivo de los teletrabajadores debido a la pandemia de Covid-19. Discutimos los diferentes modos que los trabajadores pueden utilizar para trabajar de forma remota y seguir siendo productivos: el dispositivo como una terminal tonta, el dispositivo como parte de la red interna y el dispositivo como un portal a servicios. Comprender estos modos de operación puede ayudar a encontrar cuellos de botella que pueden obstaculizar la efectividad de su equipo."},{"id":133,"type":"Episode","title":"Gestión de riesgos frente al ransomware","tags":["cybersecurity","ransomware","riskmanagement","process"],"body":"\r\n\r\nDurante el último año, ha habido un aumento en el número y sofisticación de los ciberataques. Las tres áreas clave de ataques recurrentes son el ransomware, los ataques a la cadena de suministro y las violaciones de datos.\n\nLos ataques se han vuelto más sofisticados por varias razones. Primero, a medida que la seguridad mejora, el adversario debe continuar mejorando. El crimen organizado, los actores de los estados-nación y otros agentes de amenazas reconocen que se necesita más sofisticación para comprometer y frustrar los controles de seguridad. Al igual que las organizaciones tienen procesos de desarrollo de software, lo mismo ocurre con la comunidad de malware. Ellos tienen herramientas y marcos en los que se basan, y buenos procesos para integrar calidad en sus sistemas. Diferentes actores compran, venden y toman prestado código. Aprenden unos de otros y comparten información en la web oscura. No son solo equipos desorganizados de piratas informáticos; funcionan más como empresas.\n\nEstos agentes de amenazas están en una industria de miles de millones de dólares. El dinero grande impulsa la necesidad de madurez. Ya no podemos simplemente agregar seguridad; debe estar integrada y estar presente en todas partes, no solo en los productos, sino también en la infraestructura y los procesos. Esa fue una de las lecciones de SolarWinds: incluso si construyes un buen producto, la infraestructura que lo respalda puede ser vulnerable.\n\nLos ataques recientes demuestran que nadie está inmune. A menudo, las organizaciones asumen erróneamente que están seguras, ya que no son servicios financieros, gubernamentales u otras industrias de alto valor, pero los ataques recientes a empresas como JBS Foods, McDonald's y Audi han demostrado que nadie está exento. Las compañías, sin importar su producto, dependen de su infraestructura digital para funcionar correctamente; el ataque a JBS Foods paralizó la industria de empaque de carne más grande del mundo.\n\nNinguna cosa sirve como una solución milagrosa para prevenir estos ataques. Hay esperanza, pero requiere mucho trabajo. Una organización debe tener la diligencia de aplicar las métricas de riesgo adecuadas para implementar la seguridad correctamente. Si no entiendes tu riesgo, ninguna cantidad de controles de seguridad hará el trabajo porque no sabrás si los estás aplicando en el lugar correcto.\n\nLa clave está en comenzar con el conjunto correcto de políticas y riesgos para tu organización. Un paso básico es que, incluso si tu organización aún no ha descubierto cómo implementar una arquitectura de confianza cero, negar todas las solicitudes de acceso hasta que se demuestre que son dignas es un paso en la dirección correcta. Esto significa que hay una puerta en cada entrada, en lugar de una llave maestra para todo lo que está adentro. El denegar por defecto es un principio del enfoque de confianza cero.\n\nSi la fortaleza de una empresa no está en ciberseguridad, o si no hay fondos disponibles para un equipo interno suficiente, hay muchos recursos que pueden ayudar. Los proveedores de seguridad gestionada (MSP) son una buena opción, pero siempre debe haber al menos un experto en el interior: un Director de Seguridad. Esta persona tiene el contexto local de la experiencia en el dominio para trabajar con el MSP y llevar ese conocimiento a través de la organización. El MSP se encarga de gestionar tus herramientas y configuraciones de seguridad, pero necesitas a alguien que transmita sabiduría en seguridad a las unidades de negocio y tecnología de la información. A la luz de los recientes ataques, un equipo de seguridad no es opcional.\n\nCada empresa debería tener un plan establecido para un ataque de ransomware. No es el momento de descubrirlo una vez que ya ha sucedido. Una medida básica es respaldar regularmente tus datos. Mantén copias impecables de los datos, sistemas, aplicaciones y configuraciones en un entorno de almacenamiento fuera de línea y separado. Seis meses de datos de respaldo limpios son importantes porque a veces el ransomware puede estar presente en los respaldos antes de ser detectado. Además, asegúrate de tener disponibles fuera de línea las plataformas o servidores necesarios para ejecutar tu base de datos, de modo que puedas ponerlos en funcionamiento en un modelo de redunancia o emergencia.\n\nEsto es básicamente planificación de continuidad del negocio. Así como una organización tendría un plan para continuar en caso de un desastre físico como una inundación o un corte de energía, también debería haber un plan para continuar con las aplicaciones empresariales críticas y volver a funcionar al menos parcialmente mientras se resuelve el problema.\n\nUn paso para lograr esta redundancia es ser capaz de hacer un estallido en la nube cuando sea necesario, manteniendo los recursos en la nube en un estado prístino e incluso en un proveedor de servicios en la nube diferente. Otro paso es tener un \"canario en la mina\". Esto significa tener sistemas desplegados en toda la empresa que tengan sensores al máximo. Para evitar problemas de rendimiento, almacenamiento y velocidad, puedes desplegarlos en lugares estratégicos en lugar de en todo el sistema para que sirvan como advertencias tempranas.\n\nCrear un plan con anticipación también ayudará con el desafío de decidir qué hacer en el momento de crisis, ya sea pagar el rescate del ransomware o llamar al FBI. El plan debe estar en papel e involucrar no solo a tu equipo técnico, sino también a tus abogados, CEO, CFO, etc., y todos deberían tener acceso a él. Deberías saber cómo comprar bitcoin y tener el número de teléfono de las oficinas locales del FBI y otra información relevante. Ejecuta el plan como un ejercicio para ver si funciona de la misma manera que lo haría un plan de recuperación de desastres o continuidad de negocios.\n\nAlgunas industrias pueden pensar que están seguras si mantienen separada su tecnología operativa (OT) y su tecnología de información (IT), pero en realidad no están completamente separadas. Por ejemplo, una línea de producción puede estar funcionando con máquinas informáticas, pero gran parte de lo que impulsa la cadena de suministro, la logística y la organización en general son sistemas de IT. Si esos sistemas se caen, no habrá entrada ni salida. Los sistemas de IT son críticos para la misión y la enseñanza de los ataques recientes es que dependemos de la tecnología digital para todos nuestros negocios.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT60-es","image":"./episodes/edt-60/es/thumbnail.png","lang":"es","summary":"En este episodio, parte uno de dos, Darren discute las tendencias de seguridad con el frecuente invitado Steve Orrin, CTO de Intel, Federal. Durante el último año, ha habido un aumento en el número y sofisticación de los ciberataques. Las tres áreas clave de ataques recurrentes son el ransomware, los ataques a la cadena de suministro y las violaciones de datos."},{"id":134,"type":"Episode","title":"Violaciones de datos y cadena de suministro segura.","tags":["datbreach","data","process","policy","cybersecurity","supplychain","securesupplychain"],"body":"\r\n\r\nAdemás de los ataques de alto perfil en la cadena de suministro, como el incidente con SolarWinds, hay otros ataques menos conocidos que son igualmente problemáticos.\n\n## Cadena de suministro segura.\n\nUn ejemplo es la extensión de Chrome The Great Suspender, una herramienta para asegurarse de que la memoria se utilice correctamente en las aplicaciones de Chrome, con aproximadamente dos millones de usuarios. Los fundadores de esta herramienta de código abierto vendieron su empresa a una organización por millones de dólares sin debida diligencia. Resultó que el comprador era un grupo de crimen organizado que convirtió la herramienta en malware y spyware.\n\nNo hubo un ataque en el sentido de que no hackearon a la compañía original, pero la compraron y hicieron lo que quisieron con el código. Lo que un día fue un producto legítimo al día siguiente se volvió ilegítimo. Ninguna cantidad de protocolo de seguridad habría solucionado ese problema. Ahora, las empresas necesitan evaluar a sus proveedores y a terceros que los apoyen.\n\nEl software de código abierto es una bendición y una maldición. La bendición es que, al ser de código abierto, tienes acceso a su código fuente y puedes revisarlo todo lo que quieras. La maldición radica en que nadie tiene el tiempo, la energía o el conocimiento para examinar exhaustivamente cada pieza de código abierto que utilizan. Incluso de manera no intencional, se pueden introducir códigos vulnerables a malware si no son detectados por la comunidad, y a veces eso no ocurre por mucho tiempo.\n\nHay dos cosas que podrían mitigar este problema: una es que el código se pueda pasar a través de una herramienta de análisis de código fuente y podría existir un sistema de calificación para los colaboradores cuyo código consistentemente presenta menos vulnerabilidades o errores. Los proveedores de terceros serían los responsables de hacer principalmente este trabajo. En segundo lugar, ya existen algunas nuevas empresas bien establecidas en el campo de verificación de productos de código abierto para mostrar qué objetos en un repositorio son confiables y cuáles aún no lo son.\n\nLas exploraciones actuales de vulnerabilidad en código abierto son un control, pero ese tipo de seguridad por sí sola no es suficiente. Debe combinarse con controles adicionales antes de que se extienda por toda su organización.\n\n## Violaciones de datos\n\nAdemás de los ataques de ransomware y de la cadena de suministro, las filtraciones de datos son un problema común. En 2020, se produjeron extracciones de datos por valor de 1,8 billones de dólares en un total de 7,8 mil millones de registros de datos. Por ejemplo, una filtración en McDonald's comprometió datos de clientes, socios e internos.\n\n## Encriptación\n\nLa primera parte de la solución es contar con mejores herramientas de seguridad en los datos y la infraestructura. Encriptar el acceso e incorporar la denegación por defecto es fundamental para que, aunque alguien entre por la puerta principal, no tenga acceso a todo. La información también debe ser encriptada dentro de la organización, no solo lo que se expone en la nube o lo que se envía externamente. Toda la información que atraviese su red, así como la información en reposo, debería estar encriptada.\n\nLa encriptación tiene un costo, pero en hardware moderno, existe una aceleración incorporada que obvia la penalización. Ahora puedes activar la encriptación en toda tu organización sin impacto en el rendimiento.\n\n## Segmentación\n\nOtra parte son los enclaves, o segmentación. Uno de los desafíos en los entornos corporativos de red es que, una vez más, si alguien entra por la puerta, tiene carta blanca si todo está conectado. Últimamente ha habido un movimiento para llevar el desarrollo y trasladarlo a su propia red, y eso es un comienzo, pero solo la punta del iceberg. La segmentación de redes debería ser en toda la organización. Aún puedes tener comunicación transversal, pero está sujeta a un conjunto de reglas y ayudará a limitar el impacto. Por ejemplo, si tu mesa de ayuda es atacada, tus sistemas de recursos humanos no serán comprometidos al mismo tiempo.\n\nLa microsegmentación fue una palabra de moda hace unos cinco años, pero ahora es necesario implementarla. Hay algunas herramientas excelentes disponibles para ayudar con esto, como los ecosistemas de contenedores donde se puede implementar una aplicación y esta estará en su propia red con su propio firewall.\n\nImplementar adecuadamente las credenciales de autenticación también debe suceder ahora. La autenticación multifactor es necesaria, al igual que la autenticación de entidad. Muchas herramientas son automatizadas y tienen procesos automatizados, por lo que las entidades, no solo las personas, deben tener credenciales adecuadas.\n\n## Cero Confianza\n\nLa confianza cero ha madurado hasta el punto en que debería implementarse, y algunos de los principios fundamentales como el rechazo predeterminado y no confiar en nadie son críticos. La tecnología ha alcanzado el despliegue de esos tipos de conceptos.\n\nLos desarrolladores pueden preocuparse de que estas herramientas de seguridad ralenticen el proceso, pero hay formas de construir la arquitectura para minimizar este problema. Por ejemplo, si eres un desarrollador y tienes las credenciales y acceso adecuados, deberías poder acceder a las cosas que necesitas cuando las necesitas y perder el acceso después de terminar, en lugar de tener una credencial que te dé acceso a todo todo el tiempo. La idea de la confianza cero no es que la empresa no confíe en el desarrollador, sino que el acceso es para el momento adecuado, no solo un acceso general si un actor malintencionado roba la credencial.\n\nNinguna industria puede permitirse ignorar los riesgos actuales. Cada organización debe considerar la seguridad de manera diferente e implementarla en toda la organización y arquitectura.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT61-es","image":"./episodes/edt-61/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Jefe de Soluciones de Intel, discute las violaciones de datos y la cadena de suministro segura con el frecuente invitado Steve Orrin, CTO de Intel, Federal."},{"id":135,"type":"Episode","title":"Desmitificando 5G, Edge y AI","tags":["edge","compute","5g","aiml","iot"],"body":"\r\n\r\nEl historial de Anna está en el sector industrial y manufacturero de la información. Tiene un doctorado en ingeniería química, así como un MBA. Pasó 15 años trabajando como ingeniera de procesos y diseño, luego se dedicó al trabajo de inicio y la gestión de ingeniería. Ha estado en Intel durante poco más de seis años, los últimos dos y medio trabajando en el equipo del sector público donde apoya principalmente las actividades de IoT y Edge de Intel, con una participación cada vez mayor en 5G debido a que el 5G, junto con la IA, está cambiando drásticamente ese espacio.\n\n## Transformación de red fundamental para la infraestructura 5G.\n\nEn el mundo industrial, el IoT no era nada nuevo, simplemente estaba conectado por cable. Con los sistemas ahora pasando a inalámbricos, se tiene la estrecha unión de IT más OT para convertir análisis, en lugar de solo tener un conjunto de datos de proceso específicos de ese mundo OT. Ahora se unen diferentes conjuntos de sistemas con el negocio, por lo que todo esto se está integrando en el mismo espacio. Aunque hay diferencias muy definidas en casos de uso y arquitecturas en el sector público, como el militar y las ciudades inteligentes, existe una convergencia común en el análisis, aplicación y período de tiempo para tomar mejores decisiones empresariales.\n\nUna de las razones por las cuales el IoT ha tardado tanto en expandirse más allá de la fabricación y en llegar a otras áreas es principalmente el costo de enlace físico. La seguridad también ha sido un obstáculo.\n\n5G ahora está desatando el Internet de las cosas (IoT) y el mundo de la periferia (edge), debido a su efectividad en costos, especialmente en el lado del consumidor. Sin embargo, cuando se trata de la infraestructura empresarial crítica, la conversación es diferente sobre cómo hacerlo de una manera que proteja los datos. 5G será transformador, pero aún no está sucediendo en el lado comercial. Parte de la brecha se debe al retraso entre la publicación de los estándares y la producción del hardware necesario para aprovechar esos estándares.\n\nPara cualquiera que haya pasado mucho tiempo con 3GPP u otros organismos de estándares de conducción, este cronograma no es inesperado. En este momento puedes implementar una red 5G, pero estamos en la etapa de realizar bancos de pruebas donde tenemos que demostrar el valor de 5G. Necesitamos mostrar nuevos casos de uso que no pueden ser respaldados por LTE o por 4G.\n\n## Múltiples modelos de implementación 5G ofrecen flexibilidad.\n\n¿Por qué no quedarse simplemente con WiFi 6 en lugar de utilizar 5G? La respuesta es complicada. Muchas de las organizaciones de estándares que impulsaban el 5G también impulsaban el WiFi 6, por lo que es una tecnología complementaria. La diferencia está en el equipo, por lo que necesitas conocer los detalles de tus casos de uso para determinar cuál sería más rentable. Por ejemplo, el 5G es impresionante, pero no sería lo mejor poner una red 5G en un crucero porque la penetración no sería suficiente. El entorno no es favorable para ningún tipo de señal inalámbrica, pero el WiFi 6 con puntos de acceso tiene más sentido.\n\n5G es convincente, sin embargo, por varias razones y puede hacer cosas que 4G y LTE no pueden. Para realidad aumentada y realidad virtual, necesitas una latencia muy baja y un ancho de banda alto para permitir casos de uso más interactivos, por ejemplo, reparación de equipos o máquinas. Necesitas un experto remoto que esté mirando un problema y pueda hacer transmisión de video y audio con superposición de dibujos y capacidades que se estén gestionando desde una ubicación central o remota que esté llevando todo ese conocimiento y experiencia directamente al punto en el que estás intentando trabajar. Ese tipo de caso de uso no se puede realizar a través de una red LTE. El ancho de banda juega un papel, pero la latencia es la fuerza impulsora. Para no enfermarse mientras se usa un auricular, debes tener latencias realmente bajas sin retraso o hacer que las cosas sean asíncronas.\n\nEl mundo LTE y 4G ha cambiado debido al 5G debido a cómo se ha gestionado el espectro. Un área donde esto está cambiando en la industria es que ahora es posible para una empresa obtener una licencia prioritaria para el espectro CBRS y establecer su propia red privada, completamente separada de las principales empresas de telecomunicaciones federales. Por ejemplo, un gran fabricante puede cubrir un espacio enorme de manera más rentable con una red LTE privada que con puntos de acceso. También existen grandes beneficios, como si desea reconfigurar su espacio y no desea que todas sus estaciones de trabajo estén conectadas por cable, o si tiene que mover piezas enormes de metal, como el fuselaje de un avión, en su espacio que podrían interferir con las señales WiFi, puede establecer la infraestructura para que sea portátil y no esté fija en ubicaciones con cableado.\n\nLa seguridad es una preocupación fundamental para cualquier organización. Aunque no se escribió el 5G teniendo en cuenta la seguridad, el 6G sí lo hará. Afortunadamente, con las capacidades del 5G, podemos hacer mucho en torno a la red de confianza cero y otras medidas de seguridad que infundirán confianza a los clientes sobre cómo sus datos se están trasladando a través de las redes.\n\nLos estándares de 5G han cambiado el problema de hace unos años cuando la infraestructura inalámbrica existía en hardware y software propietario, con un espectro licenciado que solo unas pocas empresas podían permitirse. La red definida por software permite tener la capacidad de alojar la infraestructura de red en hardware común y disponible en tiendas. No hay necesidad de hardware especializado como en generaciones anteriores. Esto también está sucediendo en el lado de LTE, por ejemplo, al hacer disponible el espectro CBRS y alejarse del hardware y software propietario.\n\nIntel pasa mucho tiempo trabajando con disruptores que están utilizando nuestra arquitectura de referencia FlexRAN. La arquitectura FlexRAN se convierte en la base para ayudar a proliferar la tecnología disruptiva en el nuevo mercado 5G porque proporciona una pila de software 5G que se ejecuta en hardware común y disponible en el mercado, donde antes se requería hardware propietario. Ahora hay un espacio con mucha más apertura y portabilidad, y el costo de entrada es mucho más barato que antes. Ya no son solo unas pocas empresas las que controlan todo. Intel y otros están tratando de abrirlo todo y aprovechar los estándares abiertos para apoyar a todos estos disruptores y cambiar toda la dinámica.\n\n## 5G Espectro y Regulación para las Redes del Mañana.\n\nCon una conectividad mejorada, baja latencia y alta capacidad de banda ancha, muchos nuevos casos de uso estarán disponibles. Cómo se monetizará el 5G es lo que está cambiando en todo el mercado. Por ejemplo, un proveedor de servicios en la nube junto con una empresa de telecomunicaciones pueden proporcionar mejores servicios a sus clientes porque ya no están aislados. Son un esfuerzo empresarial conjunto de lo que realmente importa: calidad y priorización. Otra forma de ver esto es que los proveedores de servicios en la nube están adquiriendo capacidades que van a abrir funcionalidades de red de la misma manera que las empresas de telecomunicaciones están explorando lo que pueden hacer en el lado de la nube. Nuevamente, esto se debe a que el aislamiento se ha derribado; el conducto de datos ya no es un conjunto de servicios.\n\nNo está claro cómo va a terminar todo esto, excepto que está redefiniendo qué tipo de trabajo puedes hacer debido a la accesibilidad de los datos y dónde van a residir esas cargas de trabajo. Existe un enorme valor en trasladarse de la periferia a la nube de manera fluida y hacerlo de una manera basada en las necesidades del cliente, lo cual ahora es posible.\n\n## Un nuevo paradigma informático respalda nuevas demandas de datos.\n\n5G está desatando muchos modelos arquitectónicos diferentes. Por ejemplo, ofrece dos opciones de arquitectura para la inteligencia artificial, mientras que antes solo había una con limitaciones.\n\nSin el ancho de banda ofrecido por la tecnología 5G, la inteligencia artificial estaba limitada a la inferencia en dispositivos periféricos, lo que requería enviar los modelos de IA a esos dispositivos periféricos. Esta restricción engorrosa aumentaba el ciclo de desarrollo y despliegue de la IA, además de limitar la cantidad de cargas de trabajo de IA que se pueden aprovechar en el borde. Con un aumento en el ancho de banda, se pueden enviar grandes flujos de datos provenientes de cámaras o sensores a un centro de datos, lo que permite ejecutar múltiples cargas de trabajo de IA y facilita el aprendizaje continuo de la IA. Esto brinda a las organizaciones la oportunidad de realizar tanto inferencia en el borde como mejorar el aprendizaje profundo, necesario para cumplir con las demandas en constante cambio de los datos de tantas organizaciones.\n\nCon AR, por ejemplo, 5G significa que los auriculares pueden ser móviles en lugar de estar conectados con las mismas capacidades, ya que el 5G permite compartir conjuntos de datos más grandes en un mundo sin ataduras. Los centros de datos tradicionales están siendo derribados.\n\nSi no tienes mucho soporte técnico o conocimiento detallado de cómo mantener tus sistemas en funcionamiento, puedes ejecutarlo todo en la nube. Si no quieres que tus datos estén en la nube, puedes hacer una versión que esté en las instalaciones a través de una red privada que te brinde todo tipo de funcionalidades para agregar y correlacionar datos y proporcionar una comprensión de alto nivel de lo que está sucediendo en tu sistema de manera segura y rentable.\n\nBásicamente, ahora tus datos pueden residir en el borde, en la nube, en las instalaciones o en lo que Cisco llama la niebla. Ya no importa dónde se ejecute tu aplicación, por lo que puedes utilizar el modelo más rentable. En espacios industriales, por ejemplo, hay grandes ahorros al no tener un componente de cableado fijo o al utilizar una estructura privada de LTE en lugar de puntos de acceso WiFi. Reducir este tipo de costos permitirá tener datos muy completos. Estas barreras de costos y conectividad física son lo que ha faltado para que IoT despegue de la forma en que todos predijeron.\n\nAnna predice que en los próximos dos años, para las aplicaciones que no sean de control, será diferente debido a 5G. Un ejemplo simple es que en la industria, alguien podría sacar su PC regular de la oficina y llevarlo al suelo de la fábrica y poder hacer todo ahí. 5G cambiará lo que es posible en relación a los controles y a hacer el control de robots y máquinas a través de una red inalámbrica en los próximos cinco años. El siguiente nivel de transformación será poder hacer el control a través de una red inalámbrica y hacerlo de manera segura y efectiva, sin poner en riesgo a nadie. Esto requerirá mucha validación y rigurosidad en la revisión, pero está en el horizonte.\n\nTambién será emocionante ver qué harán juntos tu proveedor de servicios en la nube y tu proveedor de telecomunicaciones para cambiar lo que es posible desde el punto de vista de los servicios.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Anna Scott"],"link":"/episode-EDT62-es","image":"./episodes/edt-62/es/thumbnail.jpg","lang":"es","summary":"Darren Pulsipher, Arquitecto Jefe de Soluciones, Intel, habla sobre los cambios innovadores que el 5G traerá al borde y a la inteligencia artificial junto con la Dra. Anna Scott, Arquitecta Jefe de Borde, Intel."},{"id":136,"type":"Episode","title":"Inspirando cambio cultural en TI","tags":["organizationalchange","change","people","covid","cio","california"],"body":"\r\n\r\nAmy ha sido la CIO estatal durante más de cinco años, designada por el Gobernador Brown y continuando bajo el Gobernador Newsom. Trabajó como tecnóloga en el sector público durante veinte años en una variedad de áreas, desde ciencias ambientales hasta CalPERS, hasta salud y servicios humanos antes de ser nombrada CIO.\n\nEl 13 de marzo de 2020, Amy estaba organizando una reunión discutiendo, irónicamente, la necesidad de acelerar el objetivo del servicio de banda ancha para todos los californianos, cuando se ordenó el cierre por COVID. Inmediatamente después de eso, el estado tuvo que hacer la transición de más de 200,000 empleados estatales al teletrabajo e implementar la educación a distancia. De la noche a la mañana, el problema de la banda ancha se convirtió en un asunto urgente para tratar los negocios del día a día. El estado comenzó con una capacidad del 5% para el teletrabajo y en un mes alcanzó el 92% de capacidad.\n\nEsta fue la mayor barrera operativa de la pandemia. Las personas tuvieron que acostumbrarse a trabajar de forma remota y hubo escasez de suministros, lo que lo hizo aún más difícil. Sin embargo, todo el cambio ocurrió de manera relativamente fluida. Las personas fueron comprensivas y flexibles y permitieron que ocurriera una mejora continua.\n\nEste es un proceso que normalmente llevaría años en completarse. La diferencia es que las personas no dudaron en decir \"esto es lo que debe hacerse\" y se pusieron en movimiento en lugar de analizarlo todo en exceso. Todos simplemente hicieron que las cosas sucedieran, aunque no fuera perfecto.\n\nLa ciberseguridad también fue un obstáculo, ya que las personas que trabajaban desde casa tenían que utilizar las mismas buenas prácticas como si estuvieran en la oficina, lo cual implicó un cambio cultural.\n\nUn cambio sorprendente fue que el deseo de utilizar un proceso de toma de decisiones basado en evidencia también se aceleró de la noche a la mañana. Suena un poco como una contradicción en términos, ya que no todo podía ser sobreanalizado, pero por otra parte, el enfoque se estrechó en aquellas decisiones críticas que requerían apoyo basado en evidencia. Las cosas sin importancia se dejaron de lado porque no había suficiente capacidad para hacer todo.\n\nEl estado aún está aprendiendo cómo será la fuerza laboral en el futuro. El teletrabajo se mantiene como una posibilidad a largo plazo, y algunos podrían hacerlo permanente debido al aumento en la productividad. Además, los puestos que no funcionan bien con el teletrabajo deben regresar a un entorno seguro. El bienestar de los empleados también es un factor en la nueva dinámica.\n\nEl gobierno y otras organizaciones necesitan mantener un equilibrio entre los altamente tácticos últimos 18 meses y la planificación estratégica para el futuro. Amy mantiene una visión estratégica en el plan estatal 2023 como \"Polaris\", pero también se enfoca en mejoras incrementales actuales para proporcionar servicios. Existe un ambiente de comprensión de que las cosas no van a ser perfectas a medida que el estado regrese a una nueva normalidad.\n\nAmy tiene dos objetivos culturales para su departamento derivados de la pandemia: Las personas en puestos de apoyo deben poder conectar su trabajo con los cambios positivos y los impactos para mantener la moral elevada, y el ambiente debería volverse más visionario, recordando a los trabajadores las metas estratégicas.\n\nPara mantener un cambio cultural positivo, los líderes deben ejemplificar el camino y tener en cuenta el bienestar de los empleados en sus decisiones. Esto incluye ser flexibles en la forma en que trabajan mejor y darles la confianza para tomar decisiones que les hagan sentir realizados y empoderados. Y si las cosas no salen perfectamente, sacudirlas y seguir adelante.\n\nEl consejo de Amy para otros CIOs estatales sería realizar mucho trabajo de prueba de concepto, remangarse y probar cosas, y estar abierto a diferentes formas de resolver un problema. Los resultados de la prueba de concepto informan mejor a los líderes en la toma de decisiones que simplemente analizar las cosas.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Amy Tong"],"link":"/episode-EDT63-es","image":"./episodes/edt-63/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Jefe de Soluciones de Intel, habla sobre inspirar el cambio cultural con Amy Tong, CIO del estado de California, a raíz de la pandemia de COVID."},{"id":137,"type":"Episode","title":"Seguir y asegurar el borde de Bit hasta la nube.","tags":["edge","compute","multicloud","cybersecurity"],"body":"\r\n\r\nEn el episodio número 2, Greg explicó las complejas cuestiones de recolectar, mover y analizar datos en un entorno del Departamento de Defensa (DOD) que incluye dispositivos periféricos en barcos, aviones e incluso combatientes, y la necesidad de mover y analizar esos datos rápidamente para tomar decisiones oportunas y acertadas. El papel de Intel es ayudar a acortar los ciclos del proceso y aliviar los cuellos de botella en el flujo de datos con su cartera de tecnología. Se trata de ajustar la tecnología a la misión.\n\nUna tendencia arquitectónica reciente es trasladar el cálculo, el procesamiento y la inteligencia hacia adelante para que se encuentren con los datos en lugar de trasladar los datos al backend. Este ajuste proporciona la mejor capacidad para obtener respuestas e inteligencia en tiempo real. Este enfoque representa un modelo operativo diferente que plantea preguntas desafiantes.\n\n## Hardware heterogéneo\n\nEn la infraestructura fundamental, ¿cómo proporcionas la capacidad adecuada de cálculo, almacenamiento, memoria y red para impulsar el análisis en el borde y procesar donde lo necesitas? ¿Cómo gestionas esas aplicaciones y datos? ¿Cómo introduces calidad y curaduría de datos en líneas superiores de la cadena de suministro? La seguridad es una pieza fundamental de cualquier infraestructura, y ahora, los datos deben estar protegidos siempre, incluso en el borde.\n\nLa industria necesita adoptar computación más heterogénea para que el software subyacente pueda funcionar en diversas plataformas de hardware y así no quedar encerrado en un software o plataforma de hardware específica. Parte de la belleza de lo que Intel ha hecho durante años es proporcionar portabilidad de software: el marco abierto, x86 y otros sistemas subyacentes.\n\nUn factor importante en cómo Intel desarrolla capacidades es entender la carga de trabajo y el flujo de datos en lugar de centrarse en la modernización, como colocar cosas en la nube. No se trata de cómo se adquiere la tecnología de la información (en la nube o en las instalaciones), sino de optimizar el flujo de negocios para aprovechar cómo fluyen los datos y qué es lo que realmente hacen.\n\n## DevOps al rescate\n\nOtro aspecto crítico de estas nuevas soluciones es el tiempo de implementación. DevOps se ha vuelto cada vez más importante en todo el Departamento de Defensa, y los esfuerzos para desarrollar plataformas comunes de DevOps han aumentado drásticamente. Para escribir código una vez y ejecutarlo en cualquier lugar (en la nube, en las instalaciones o en el borde), las organizaciones utilizan el chipset x86 y el oneAPI de Intel para implementar servicios complejos en todo el ecosistema completo. Por ejemplo, oneAPI permite a los desarrolladores escribir tareas de análisis o de IA complejas que se ejecutan en una GPU, una CPU o incluso en FPGAs con la misma base de código. Luego, los desarrolladores pueden probar estos sistemas complejos en su centro de datos, asegurando que se comportarán de la misma manera cuando se implementen en el campo, lo que reduce el tiempo de implementación.\n\nPoder aprovechar una combinación de un entorno DevOps flexible, que utiliza la contenerización y los frameworks de software como oneAPI, brinda a los programadores la abstracción que necesitan pero con optimizaciones de hardware/software incorporadas. Una distinción clave aquí es que hemos descubierto una forma de aprovechar toda la potencia del hardware y optimizar el software de manera que una aplicación aún pueda obtener mejoras de rendimiento y aprovechar al máximo el hardware especializado.\n\nEl ecosistema de contenedores proporciona una abstracción a nivel del sistema, por lo que las cosas no necesitan ser codificadas de forma rígida. Proporciona la capacidad de escalar hacia arriba o hacia abajo según la carga de servicios y la capacidad. Por ejemplo, si una capacidad se ejecuta en el borde, en un centro de datos portátil o en la nube, y pierde conectividad o un recurso, aún puede llevar a cabo la misión. Esta conectividad intermitente es vital en un entorno del Departamento de Defensa.\n\n## Soluciones Edge\n\nMuchas personas piensan en Intel como un proveedor de silicio o hardware, pero la empresa ha desarrollado soluciones y arquitecturas de referencia en el ecosistema desde el borde hasta la nube. Un ejemplo de progreso es una red táctica naval donde el procesamiento ocurre en el borde, barcos, aviones y en tierra. Las aplicaciones deben poder ejecutarse en todo el ecosistema, y eso no puede hacerse solo con hardware. Intel ha aprovechado sus características de silicio y seguridad, y construyendo sobre un ecosistema de contenedores, ha producido aplicaciones complejas con varios microservicios que pueden ejecutarse cuando los activos están conectados y desconectados.\n\nOtro ejemplo de arquitecturas de borde es el seguimiento de objetos a través de mallas de sensores, incluso con brechas en la malla de sensores. Al realizar la fusión de sensores, la fusión de datos y el análisis entre dominios, se pueden rastrear objetos en diferentes escenas y en diferentes tipos de sensores, escalando hacia arriba y hacia abajo para crear una plataforma de sensores móviles que pueda realizar análisis y fusión en el borde. Luego, también puede transferirse a un conjunto distribuido de nodos que pueden trabajar en conjunto para rastrear un objeto en todos esos sensores existentes diferentes.\n\nPor ejemplo, si estás rastreando un objeto y se mueve entre sensores, lo perderás por un momento. Lo que esta inteligencia te permite hacer es conectar esas dos transmisiones y rastrear los puntos ciegos. Esto no solo funciona con tipos de sensores homogéneos, sino que también puede funcionar con múltiples tipos de sensores como sensores de movimiento, RF e infrarrojos, por lo que si pierdes el vídeo pero aún tienes un RF y luego vuelves a capturar el vídeo, puedes mostrar la trayectoria completa del objeto. En el pasado, este tipo de análisis requería mover los datos al centro de datos para procesar estas interacciones complejas; ahora, podemos hacer esto en el borde de la malla de sensores a través de microservicios en el borde.\n\n## Flexibilidad de implementación\n\nUn enfoque ventajoso es construir una vez y usar la misma arquitectura y software para diferentes misiones con un requisito analítico. En algunos lugares donde la computación de alta carga está en el límite, se pueden ampliar y aprovechar al máximo las capacidades del hardware. Otras plataformas pueden tener capacidades de cálculo limitadas y pueden ejecutar microservicios más pequeños que solo proporcionan una parte de la solución completa. Las nuevas capacidades y servicios de misión se pueden implementar rápidamente construyendo estos microservicios una vez para múltiples casos de uso.\n\n## Seguridad en el Borde\n\nLa complejidad es a veces el peor enemigo de la seguridad, pero los principios fundamentales de seguridad también pueden asegurar los datos y aplicaciones en la periferia. En lugar del enfoque antiguo de proteger el servidor y esperar que todo permanezca allí, es esencial comprender hacia dónde fluyen los datos y en cada lugar donde existe, protegiéndolos sin importar en qué se estén ejecutando. Enfoques basados en el riesgo y conceptos como la confianza cero han ganado impulso porque adoptan una visión independiente del sistema de seguridad.\n\nHablando en términos simples, \"zero trust\" significa denegación por defecto. Nadie ingresa a menos que sea necesario, y aún así, solo por el tiempo necesario para la acción requerida. Cuando los controles de acceso temporales se combinan con un enfoque basado en el riesgo para proteger los datos a lo largo de su ciclo de vida, el resultado es la capacidad de proteger los datos sin importar dónde se encuentren ni quién los esté accediendo. Esta técnica es una de las formas de asegurar estos entornos altamente complejos.\n\nLa acción práctica a tomar en estos ecosistemas es aplicar una política que aproveche los controles que cumplan con el riesgo de un sistema dado en un momento dado, y luego monitorear y actualizarlos de forma continua en tiempo real para enfrentar el mundo cambiante de amenazas cibernéticas. Utilizar los controles técnicos que las capacidades de hardware y software ya proporcionan, como Secure Boot, raíz de confianza de hardware con módulos TPM o claves de almacenamiento SGX, encriptación, etc.\n\nNo hay una solución milagrosa que puedas comprar para ofrecer una solución integral de seguridad en estos ecosistemas complejos. Se trata de crear y hacer cumplir políticas de seguridad a medida que evolucionan las amenazas y desplegarlas a gran escala, aprovechando hardware, software y los procesos necesarios para asegurar los bits a medida que fluyen desde el borde hasta todo el ecosistema.\n\nEl marco de trabajo DevOps proporciona mecanismos efectivos para manejar la seguridad en todos los activos del ecosistema. Los contenedores deben estar equipados con instrumentación para hacer cumplir los controles y políticas de seguridad. La seguridad debe integrarse en el propio proceso DevOps porque si se confía en que el desarrollador implemente la seguridad, cada uno lo hará de manera ligeramente diferente, aumentando la complejidad y la variabilidad en el sistema. El desarrollador debe tener las capacidades y las limitaciones sobre las cuales debe desarrollar.\n\nLos desarrolladores aún tendrán que realizar trabajo de seguridad, como asegurarse de utilizar herramientas de seguridad adecuadas para el entorno de amenazas específico, pero el trabajo pesado, la complejidad, debe ser abstraído en la arquitectura de DevOps.\n\nUna de las áreas críticas en un entorno teatral complejo es la gestión de dispositivos periféricos, como monitorización y actualización de firmware. Asegurarse de que esos dispositivos estén seguros para poder respaldar la seguridad de datos y perfiles y políticas implementadas en los sistemas a gran escala requerirá innovación. Por eso, actualmente, el ecosistema está creciendo considerablemente: para enfrentar ese desafío.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Clifton","Steve Orrin"],"link":"/episode-EDT64-es","image":"./episodes/edt-64/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, discute siguiendo el tema junto a Greg Clifton, Director de DOD e Inteligencia de Intel, en un seguimiento al episodio n. ° 2, junto con Steve Orrin, CTO Federal de Intel, quien aporta su experiencia en seguridad en entornos complejos de borde a nube."},{"id":138,"type":"Episode","title":"Acelerando la adopción de la nube al tiempo que se disminuye la complejidad y el costo.","tags":["cloudmigration","cloud","compute","process","multicloud","aiven","cloudcost","technology"],"body":"\r\n\r\nDavid era anteriormente un desarrollador de aplicaciones y se describe a sí mismo como un sobrecapacitador en recuperación. Esa experiencia, junto con su trabajo en AWS y GCP en diferentes empresas, le brinda la base para entender los matices de cómo funcionar en la nube, cómo escalar cuando tiene sentido económico y cómo enfrentar muchos desafíos al mudarse a la nube.\n\nMuchas organizaciones se están trasladando a la nube, aceleradas por las consecuencias de la pandemia de COVID. Las grandes y pequeñas empresas lo están haciendo de manera diferente. Las empresas más pequeñas se están trasladando a la nube y la adoptan tan rápido como sea posible, aprovechando nuevas herramientas y mejores prácticas, pero todavía hay mucho levantamiento y traslado en el centro de datos. Durante el proceso de levantamiento y traslado, sin embargo, a menudo no están aprovechando ninguno de los beneficios de la nube. Por lo tanto, se están moviendo hacia una transformación digital, pero tienen que preguntarse, ¿cuáles son los siguientes pasos?\n\n## Reduciendo Costos\n\nHay muchas cosas que puedes hacer, y debes hacer, para reducir costos y evitar sorpresas desagradables, como los costos que conllevan las máquinas virtuales zombies. En la nube, si necesitas un servidor, sólo tienes que hacer clic en un botón y lo tienes, pero luego si te olvidas de que está funcionando, un mes después podría haber un cargo de $10,000 en los servidores. Otra cosa a tener en cuenta es la salida de red que puede acumularse, especialmente con transferencias entre regiones si estás manejando terabytes o petabytes cada mes.\n\nLas alertas de gasto pueden ayudar a mitigar estos problemas.\n\nLas proyecciones de costos pueden ser difíciles debido a problemas como el acceso gratuito, pero obtener los datos de vuelta es costoso. Borrar los datos también tiene un costo. Por lo tanto, las organizaciones deben pensar en las precauciones sobre la cantidad de datos a ingresar, el tipo de operación y el uso.\n\nPara optimizar los costos, algo en lo que el liderazgo en ingeniería debe pensar es que llegarás a un punto en el que la computación y los servicios administrados no serán tan costosos; el recurso más costoso son las personas. Entonces, ¿cómo se optimiza la eficiencia de tus desarrolladores? No quieres que realicen tareas remedias y repetitivas.\n\nLa automatización es clave aquí, especialmente para cualquier tarea repetida realizada con alta frecuencia, así como para los servicios gestionados.\n\nEl ciclo de vida de cada empresa llega a un punto en el que tienen que decidir si van a invertir y poseer todas las operaciones, contratar un equipo de DevOps o utilizar proveedores de servicios gestionados. Contratar expertos internos es costoso. El punto de inflexión es cuando deciden qué tan bien se escala la nube para ellos. Por lo general, es mejor que las empresas consideren el costo total de propiedad, se enfoquen en sus ventajas competitivas y utilicen servicios gestionados para otros servicios.\n\n## Reduciendo la complejidad\n\nUna de las complejidades de la nube es que necesitas ser capaz de recrear tu entorno con despliegues confiables y repetibles. Esto no significa que estés yendo al sitio web y haciendo clic en él; estás desplegando tu entorno desde, por ejemplo, un script o un archivo YAML o TerraForm, y necesitas poder iniciarlo y cerrarlo rápidamente.\n\nNecesitas poder eliminar un servidor que se ha vuelto sucio y recrearlo de manera confiable en un estado limpio. Si alguien mueve archivos de registro, ajusta configuraciones, borra una base de datos por error o si hay un ataque de ransomware, debes poder reproducir tus entornos o componentes de tu entorno para reducir el tiempo de inactividad.\n\nLa pieza crítica es tener un plan establecido basado en cuán rápidamente necesitas recuperarte y cuántos datos necesitas para continuar. Tiene sentido para algunas industrias invertir en un plan para trasladarse a una región diferente rápidamente. Debes considerar cuánto estás dispuesto a invertir en tiempo de recuperación.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","David Esposito"],"link":"/episode-EDT65-es","image":"./episodes/edt-65/es/thumbnail.jpg","lang":"es","summary":"En este episodio, parte 1 de 2, Darren habla con David Esposito, Arquitecto de Soluciones Globales, de Aiven, sobre cómo acelerar la adopción de la nube mientras se reduce la complejidad y el costo."},{"id":139,"type":"Episode","title":"Gestión de la complejidad en la nube","tags":["data","devops","compute","technology","process","devsecops","cybersecurity","aiven"],"body":"\r\n\r\n## Reducir la complejidad\n\n¿Qué prácticas comerciales se deben implementar al administrar activos en la nube para disminuir la complejidad? En primer lugar, es necesario contar con código de infraestructura e implementaciones automatizadas. Todo lo que viene después es una conversación para definir qué riesgos existen para la empresa, como interrupciones y tiempo de inactividad, cómo mitigar estos riesgos y cuánto invertir en ello.\n\nPara algunas empresas, es fundamental tener el nivel más alto de tiempo de actividad. En la atención médica, por ejemplo, es posible que deba tener todas las copias de seguridad de datos en múltiples ubicaciones, así como todos los servicios disponibles porque las decisiones críticas para salvar vidas podrían basarse en esos datos. Pero si estás en comercio electrónico en abril, tal vez puedas pasar algún tiempo en esa región enferma porque las consecuencias no son tan graves.\n\nEn el sector de la salud, tiene sentido adoptar la multicloud, aunque aumente la complejidad. Otros sectores que pueden beneficiarse de la multicloud son aquellos altamente regulados como los servicios financieros, el petróleo y gas, o las agencias federales. Si ocurre un incidente de seguridad, pueden desactivar uno y desplegar en otro lugar.\n\nCuando consideres el uso de múltiples nubes, debes tener en cuenta que existen algunas complejidades debido a las diferencias en las API. Si estás en GCP y sabes cómo implementar todo y estás pensando en pasar a AWS, tendrás que aprender nuevamente la plataforma y el proceso de implementación, incluso posiblemente escribir parte de ese código. Por esta razón, la contenerización es popular, porque te permite implementar en cualquier lugar. Sin embargo, se vuelve más complejo con otros servicios; por ejemplo, Postgres tiene diferentes configuraciones en AWS en comparación con GCP, tal vez con diferentes versiones de parches, diferentes configuraciones de red y diferentes extensiones. Por lo tanto, un servicio en una nube no es lo mismo que los servicios en otra.\n\nAdemás, los entornos contenerizados no son ideales para todas las situaciones, y una situación ideal se vuelve más importante a medida que se escala. Si estás utilizando Kafka y enviando 1,000 mensajes por segundo, tu propio contenedor está bien. Pero si comienzas a enviar más de un millón de mensajes por segundo, un gigabyte, eso es una conversación diferente.\n\n## Seguridad\n\nLa seguridad en la nube es una bestia diferente a la seguridad en tu propio centro de datos. Lo más importante a considerar es que los datos salen de tus propias paredes. Debes tener controles de seguridad adecuados para garantizar que los datos no se filtren ni tengan acceso no autorizado, y asegurarte de cumplir con los requisitos de cumplimiento. Esto requiere capacitación, ya que las medidas de seguridad para los centros de datos locales no se traducen directamente a la nube. Por ejemplo, los contenedores de S3 están encriptados en reposo, pero no si tienes acceso programático a través de buckets expuestos públicamente.\n\nAdemás, necesitas saber qué hacen los proveedores de servicios. Por ejemplo, Aiven ejecuta bases de datos en GPUs EC2, GCP Compute o Azure. Cuando almacenan en disco, hay un nivel de encriptación que se realiza en el nivel de la nube por parte del proveedor de la nube, por lo que los datos están encriptados en reposo. Luego, Aiven realiza otra capa de encriptación con sus propias claves administradas, por lo que los datos están doblemente encriptados en reposo, y cualquier dato en tránsito está encriptado. Todo lo que se envía al servidor está encriptado para ese servidor en particular, y cuando un servidor se da de baja, se bloquea con la clave de Aiven y la clave del proveedor de la nube.\n\n## Aiven\n\nDavid se unió a Aiven como parte del equipo de arquitectura de soluciones cuando vio que la compañía ofrecía soluciones en la nube mejores que cualquier otra que había visto, basado en su experiencia previa con Kafka administrado y otras tecnologías de código abierto. La misión de Aiven es mejorar la vida de los desarrolladores. Ofrecen una prueba gratuita, con Kafka listo para producción en cinco minutos. También cuentan con artículos de ayuda y tutoriales para guiarte, de modo que puedas enviar un mensaje a Kafka en la nube siguiendo las mejores prácticas nativas de la nube en menos de diez minutos.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","David Esposito"],"link":"/episode-EDT66-es","image":"./episodes/edt-66/es/thumbnail.jpg","lang":"es","summary":"En este episodio, parte 2 de 2, Darren continúa su conversación con David Esposito, Arquitecto de Soluciones Global de Aiven, sobre cómo acelerar la adopción de la nube mientras se reduce la complejidad y el costo."},{"id":140,"type":"Episode","title":"5G Pasado, Presente y Futuro","tags":["5g","comms","wifi6"],"body":"\r\n\r\nLeeland ha estado en la industria de las telecomunicaciones durante 20 años. Justo después de terminar la universidad, fue reclutado para trabajar en el Departamento de Defensa de los Estados Unidos después de que quedaran impresionados con su trabajo en la aplicación de multiplexación por Bluetooth en un automóvil, reduciendo el tamaño del arnés de cables. En lugar de trabajar con automóviles como originalmente había planeado, terminó trabajando con el Departamento de Defensa desarrollando tecnologías inalámbricas avanzadas para los soldados.\n\nEn ese momento, la tecnología celular se encontraba en transición de 2G a 3G. En ese punto no se le llamaba WiFi, sino LAN inalámbrica, y había una tecnología incipiente llamada Bluetooth que intentaban utilizar para proporcionar a los soldados la capacidad de recopilar información dentro del campo de batalla.\n\nLeeland dejó el Departamento de Defensa tres años después para realizar trabajo práctico con la tecnología y obtener un mejor entendimiento. Comenzó a trabajar para Sprint y permaneció en la industria de las telecomunicaciones durante 13 años diseñando tecnología para redes inalámbricas antes de unirse a Intel en 2017 para trabajar en 5G.\n\nHasta mediados de los años 90, había poco uso de teléfonos móviles. Los teléfonos \"ladrillo\" de los años 80 eran enormes debido al tamaño de las baterías y eran caros. Evolucionaron en \"teléfonos de bolsa\" a principios de los 90, que eran mejores, pero solo podías esperar que funcionaran para hacer una llamada telefónica.\n\nA medida que se implementaba el 2G, se desarrollaron nuevas funciones basadas en las demandas de los clientes, como la transmisión de texto y los juegos. El dispositivo podía realizar una transmisión de datos de baja velocidad con tecnologías TDMA, acceso múltiple por división de tiempo, y GSN, por ejemplo. A mediados de los 90, empresas como Sprint entraron en escena y el uso de teléfonos celulares se generalizó aún más. A principios de los 2000, hubo una transición significativa con la expectativa de los clientes de que internet estuviera disponible en los teléfonos, al igual que en las computadoras portátiles.\n\n3G trajo capacidades nominales de banda ancha donde se podía usar internet en cierta medida, pero la capacidad principal era la transmisión de imágenes. El nombre \"3G\" se acuñó en este momento, y es cuando entró en juego el cuerpo de estándares de 3GPP.\n\nEn 2008, las compañías de telecomunicaciones comenzaron a considerar la implementación de infraestructuras que no se basaban en grandes torres de células, sino en un sistema más distribuido de un centro de unidades de banda base con antenas trasladadas por fibra óptica a postes telefónicos. Esto marcó el inicio de la transición hacia una banda ancha real, desde 3G hasta 4G.\n\nEsta tecnología debería recibir crédito por el auge económico de 2010 a 2020, porque con el 4G en sus manos, empresas como Amazon y Netflix evolucionaron y florecieron.\n\n5G libera los servicios de una \"cárcel\" de RAN monolítica y abre el campo porque 5G proporciona arquitecturas de código abierto con una base definida por software. Ahora se pueden desarrollar y integrar pilas de software en una solución de software completa. Dado que no está vinculado a una arquitectura monolítica, 5G puede proporcionar servicios y redes privadas independientes.\n\nEsta gran flexibilidad va a permitir a las empresas de telecomunicaciones y sus portadoras mejorar los servicios y ofrecer diversas capacidades nuevas, incluyendo brindar acceso a la computación en el borde.\n\n¿Qué viene después? Leeland ve un movimiento alejándose de \"G\", ya que no da suficiente crédito a la evolución de la tecnología, ya que ya no hay particiones reales en términos de quién puede implementar redes. La tecnología y los casos de uso son amplios.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown"],"link":"/episode-EDT67-es","image":"./episodes/edt-67/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, arquitecto principal de soluciones, Intel, conversa con Leeland Brown, director técnico de 5G, Intel Federal, sobre el pasado, presente y futuro del 5G, con énfasis en su uso con el Departamento de Defensa. Parte 1 de 2."},{"id":141,"type":"Episode","title":"5G en Defensa","tags":["5g","dod","compute","technology","cloud","edge","comms"],"body":"\r\n\r\nA principios de siglo, el Departamento de Defensa estaba buscando formas de aplicar tecnologías 2G, redes LAN inalámbricas y Bluetooth en aplicaciones que permitieran a los soldados capturar información en el campo de batalla. En 2021, esas capacidades siguen siendo buscadas. El Departamento de Defensa está investigando intensamente el 5G ahora, ya que se ha alejado de construcciones monolíticas, como utilizar una arquitectura RAN única, y se ha adentrado en arquitecturas definidas por software. Existe la flexibilidad para construir nuevas capacidades más rápidamente y adaptarlas a diferentes casos de uso.\n\nEl lado comercial de 5G está impulsado por estrategias y el objetivo de aumentar los ingresos en desarrollo. Esto no necesariamente se alinea con los requisitos de misión del gobierno federal o del Departamento de Defensa, por lo que las fronteras entre los dos mundos se están difuminando. Por ejemplo, AT&T y Verizon están considerando casos de uso federales, y Lockheed Martin, con su programa 5G Domino, está enfocándose en el espacio comercial. La diferencia radica en quién entiende la misión y quién busca impulsar los ingresos.\n\nActualmente, el noventa por ciento de las implementaciones comerciales de 5G son redes no independientes, lo que significa que el 5G aún está conectado a un núcleo de paquetes evolucionado 4G existente. El Departamento de Defensa está interesado en redes independientes con un núcleo de 5G completo, una RAN (red de acceso de radio) 5G y dispositivos 5G. Puede haber una red independiente para un grupo de soldados, junto con estaciones base de factor de forma pequeño para plataformas vehiculares y múltiples dominios protegidos, incluso drones desplegados con algún tipo de punto de acceso 5G y aplicaciones espaciales.\n\nEsta capacidad de adaptarse a múltiples casos de uso y sus diferentes tipos de cargas de trabajo también es aplicable al ámbito comercial. Sin embargo, uno de los mayores problemas es quién es el propietario de la frecuencia. Algunas bandas de espectro libre están disponibles con alcance limitado, pero estas bandas, como las bandas ISN, no tienen licencia y están muy congestionadas. La regla es que debes dar y aceptar interferencias y hacer que tu tecnología funcione en torno a eso, pero el intercambio dinámico del sistema es posible. Sin embargo, para casos de uso federales, no puedes estar bloqueado en operaciones en los Estados Unidos, ya que la mayoría de los soldados están desplegados fuera de los Estados Unidos.\n\nLeeland predice que el 5G se integrará en la vida de todos, a veces sin que ni siquiera lo sepas. Plataformas de reconocimiento de patrones, como el reconocimiento facial y los vehículos autónomos, estarán conectados a través del 5G. Tu acceso a banda ancha se integrará en una única red, conectada no solo a tu teléfono celular, sino también a tu automóvil y hogar. La necesidad de tener un teléfono en la mano se reducirá drásticamente a medida que comencemos a ver puntos de acceso inalámbricos en todas las áreas de nuestra vida.\n\nLeeland también predice que la \"G\" desaparecerá a medida que la tecnología se expanda y evolucione.\n\nPara aprender más sobre 5G, aquellos con inclinación técnica pueden dirigirse a los estándares de 3GPP para ver los casos de especificidad. Puedes leer una especificación y comprender la diferencia entre la versión 14 y la versión 15, y lo que eso significa para la industria en su conjunto. También hay mucha información disponible a través de internet, como documentos técnicos. Leeland también se ofrece como punto de contacto.\n\nLeeland desea que el próximo paso sea un llamado a la acción para hacer que las redes sean más resilientes a través de la adopción de nuevas tecnologías. Durante las emergencias, ya sean desastres naturales o ataques terroristas, la resiliencia es necesaria tanto para los primeros respondientes como para las personas que simplemente intentan comunicarse con sus familias.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Leland Brown"],"link":"/episode-EDT68-es","image":"./episodes/edt-68/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones, Intel, continúa su discusión con Leeland Brown, Director Técnico de 5G, Intel Federal sobre el pasado, presente y futuro del 5G, con énfasis en su uso con el Departamento de Defensa. Parte 2 de 2."},{"id":142,"type":"Episode","title":"Asegurando el centro de datos a través del borde","tags":["cybersecurity","edge","compute"],"body":"\r\n\r\nUna mirada integral a la seguridad incluye toda la infraestructura, desde el entorno en la nube hasta el extremo. Si no puedes asegurar al cliente, no puedes asegurar la empresa, por lo tanto, es esencial tener conversaciones honestas de manera accesible, sin jerga, sobre seguridad.\n\nHan pasado los días en que a un empleado se le enviaba una computadora portátil a una jaula de tecnología de la información, luego se configuraba y entregaba por parte de TI. A veces, los empleados ni siquiera van al lugar de trabajo, por lo que los dispositivos deben llegar a la puerta de alguien. Deben ser aprovisionados para asegurarse de que estén seguros y no hayan sido manipulados. Desde el punto de vista de la seguridad, este es un problema desafiante. La cadena de suministro es problemática. Hay menos control sobre los dispositivos y cómo se despliegan.\n\nLa repentina transición al trabajo desde casa debido a la pandemia de COVID requirió una respuesta rápida sin precedentes a este problema. Podría haber tomado años si hubiera sido una progresión natural, pero la situación no permitió una alternativa; el departamento de IT tuvo que estar a la altura de las circunstancias.\n\nCuando Camille trabajaba en el grupo IoT, intentaron resolver la conectividad y la gestionabilidad entre dispositivos. Ecosistemas complejos como parques eólicos, minas subterráneas y dispositivos médicos implantados son difíciles de actualizar cuando la conectividad es intermitente. Estaban tratando de resolver estos problemas de conectividad en el límite, y cuando llegó el COVID, la intersección entre la tecnología operativa (OT) y la tecnología de la información (TI) de repente se convirtió en el centro de todos los departamentos de TI empresariales.\n\nComo algunos de los clientes de Darren estaban luchando por encontrar la mejor manera de conectar los sistemas de tecnología operativa (OT) y tecnología de la información (IT), la pandemia del COVID llegó y parte de ello colapsó, lo que resultó en brechas de seguridad.\n\nMuchas personas implementaron rápidamente un sistema de trabajo desde casa y luego se preocuparon por la seguridad, dependiendo del nivel de madurez de la organización. También hubo un cambio masivo hacia la nube. Actualmente, hay un pequeño movimiento de péndulo hacia atrás debido a brechas como los ataques de ransomware. Esas brechas ocurrieron en la nube, principalmente porque las personas no entendían la responsabilidad compartida en cuanto a la seguridad. Ahora, algunas organizaciones están pensando dos veces en trasladar sus datos críticos a la nube. Pueden trasladar cargas de trabajo allí, pero mantienen los datos más importantes en casa.\n\nOtro cambio reciente es la importancia de la percepción del cliente. Puede estar haciendo las preguntas correctas sobre hardware y capas de protección de software, pero también tiene que considerar la percepción de su cliente acerca de dónde está guardando los datos y por qué, quién los está protegiendo y cómo los están protegiendo.\n\nUn problema mayor son las organizaciones que no pueden responder ninguna de esas preguntas. A veces ni siquiera saben dónde se encuentran sus datos. Estas organizaciones deben considerar esto como un punto de partida para el trabajo que aún queda por hacer.\n\nAlgunos nuevos problemas agravan este problema que la industria aún no ha abordado, como las videoconferencias. La grabación de la reunión se guarda en una computadora portátil, pero también está en la nube en algún lugar. ¿Quién tiene acceso a ella? ¿Cuáles son las protecciones? ¿Cuánto tiempo estará allí?\n\nUno de los principios de seguridad es saber si tu dispositivo está seguro. Uno de los desafíos descubiertos con la situación de COVID es que muchas tiendas de informática dudan en actualizar sus sistemas. No quieren interrumpir un sistema, ya sea un servidor o un cliente. No actualizar los parches de seguridad es un error.\n\nEl trabajo de Intel consiste en colaborar con socios y compañeros de viaje para hacer de esa actualización una actividad más sencilla y confiable, en la que las personas tendrán la confianza de que funcione y de que no ocurra ningún problema en el proceso. La industria ha avanzado considerablemente en hacer el proceso de actualización más sistemático y predecible en los últimos años.\n\nOtra parte del trabajo consiste en capacitar a las personas para que comprendan que la seguridad no se detiene cuando un dispositivo se envía del fabricante al cliente. La seguridad continúa durante toda la vida del dispositivo. Lo que era seguridad de clase mundial en el momento del envío no lo es meses o años después. Las empresas deben actualizar sus máquinas dos veces al año para mantenerlas seguras.\n\nLas personas se ponen nerviosas al hacer actualizaciones porque pueden ocurrir cosas inesperadas. Intel valida a gran escala para prevenir problemas, ya sea con miles de máquinas en sus laboratorios o con socios OEM en laboratorios dispersos por todo el mundo. La validación completa garantiza que las mitigaciones funcionen para proteger contra las vulnerabilidades y no dañen el sistema. Intel ha realizado una inversión significativa en asociarse y colaborar con sus socios de ecosistema y establecer estándares en toda la industria, y busca mejorar la experiencia del usuario en el futuro desarrollando la capacidad de hacer las actualizaciones sin reiniciar.\n\nEducar a los clientes sobre por qué les estás pidiendo que hagan una actualización también puede ser muy beneficioso. Si entienden que has encontrado una vulnerabilidad y que podrían estar expuestos a un posible ataque, probablemente querrán hacerlo.\n\nEn general, las personas parecen estar dispuestas a actualizar sus teléfonos celulares porque no están tan preocupadas de que algo no funcione después, pero sigue siendo un desafío en los lados de las PC y los servidores. Algunas de estas cuestiones están relacionadas con los modelos de uso. Aunque es raro que los datos solo existan en una computadora portátil, esa mentalidad es prevalente. Cuando los datos existen en la nube en un teléfono celular, la percepción es que siempre estarán ahí. Además, las personas tienden a realizar trabajos más inmersivos y atractivos en computadoras portátiles que en teléfonos, por lo que son más sensibles a ello. Una vez que cambia la percepción y las personas se dan cuenta de que los datos de su computadora portátil también existen en la nube, las actualizaciones son más ampliamente aceptadas. Por lo tanto, en realidad, la industria necesita combinar soluciones técnicas y cambios de mentalidad en lo que respecta a la seguridad.\n\nLa forma en que las cosas están evolucionando es un poco híbrida. Nuevos modelos de aprendizaje como el aprendizaje federado están llegando rápidamente para ayudar a abordar problemas como las preocupaciones sobre la privacidad. Los modelos están siendo llevados al límite en lugar de que los datos se trasladen al centro de datos. Por ejemplo, un sistema de imágenes médicas en un hospital donde los datos permanecen en su lugar y el modelo viene a analizarlos. Estamos comenzando a ver esto en aplicaciones industriales, donde las máquinas están en el borde y se convierten en el servidor. Mantendrán los datos localmente y realizarán entrenamiento y actualizaciones allí. Entonces, habrá dispositivos inteligentes en el borde, trabajando con los datos en bruto, y la pregunta es, ¿cómo se asegura eso?\n\nOtra tendencia en seguridad, una que no comenzó con la pandemia de COVID pero que indudablemente fue acelerada por ella, es protegerse contra el ataque físico. Históricamente, la seguridad se ha centrado en algo que podría ocurrir a través de la red, como un ataque de red o una aplicación maliciosa. Con dispositivos de IoT ahí afuera sin un humano conectado o vigilando, tenemos que proteger los datos y los dispositivos de posibles manipulaciones. Eso representa un desafío difícil.\n\nHoy en día, no se puede pensar de manera holística sobre la seguridad a menos que también se aborde la privacidad. Una complicación es que la privacidad a veces puede estar en conflicto directo con la seguridad. No existen regulaciones o estándares acordados a nivel mundial, por lo que las organizaciones tienen que descubrir cómo operar: ¿Cumplir con el denominador común más alto o abordar cada requisito geopolítico? Para complicarlo aún más, las leyes y regulaciones están en constante cambio.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Tom Garrison","Camille Morhardt"],"link":"/episode-EDT69-es","image":"./episodes/edt-69/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones, Intel, habla sobre la seguridad del centro de datos a través del borde con otros ejecutivos de Intel y los anfitriones del podcast Tom Garrison, VP de Seguridad del Cliente, y Camille Morhardt, Directora de Innovación y Comunicación de Seguridad."},{"id":143,"type":"Episode","title":"Asegurando al trabajador remoto.","tags":null,"body":"\r\n\r\n## Modos de operación del trabajador remoto\n\nLos modos de operación de teletrabajo no son nuevos, ya sea utilizando un dispositivo como terminal (VDI), parte de la red interna o como un portal para servicios/software (SAAS); lo nuevo es el gran volumen de empleados, contratistas y socios que ahora utilizan entornos existentes para realizar su trabajo principal, en lugar de unos pocos seleccionados y manejables. La seguridad es un desafío en todos estos modos de operación. Las organizaciones necesitan encontrar la mejor manera de desplegar sus capacidades existentes para asegurar y proteger el acceso, los datos, los dispositivos y los usuarios.\n\n## Amenazas del teletrabajo:\n\nAlgunas de las amenazas de seguridad son aquellas que siempre han estado presentes con el teletrabajo: la seguridad del dispositivo final y los datos, el punto de acceso y el acceso a los servicios empresariales. Sin embargo, al adaptarse al entorno del COVID-19, han surgido nuevos problemas debido al gran número de personas que ahora teletrabajan y la prisa por habilitarlos.\n\nUn problema es que los trabajadores se encuentran en casa, en entornos no seguros, en redes que nunca fueron destinadas a ser utilizadas fuera de la corporación. Esto lleva a ataques de phishing y malware dirigidos. Otro problema es la introducción de nuevas herramientas, como aquellas para la colaboración y videoconferencia, que pueden exponer a su empresa y datos a ataques y uso malicioso. Básicamente, ahora existe un área de superficie de ataque más amplia, mucho más grande de lo que la mayoría de las organizaciones inicialmente planearon, y nuevos entornos que nunca formaron parte del plan original.\n\n## Soluciones para asegurar al trabajador remoto\n\n## Trae tu propio dispositivo (BYOD)\n\nUno de los problemas más difíciles de abordar es cuando los trabajadores usan sus propios dispositivos. El desafío principal es que estos son dispositivos no gestionados sin todos los agentes de seguridad en funcionamiento y gestionados por TI; el dispositivo es propiedad y está gestionado por el usuario. Es posible que tengas la capacidad de cargar unos cuantos agentes o hacer cumplir algunas políticas, pero solo hay tanto que puedes hacer sin dificultar al trabajador al utilizarlo como un dispositivo personal. Entonces, ¿cómo puedes asegurar a un usuario con un BYOD?\n\nAsegúrate de que estos fundamentos estén en su lugar: Aplica los últimos parches como requisito, impón un buen control de acceso a tu empresa y servicios, implementa autenticación multifactor, y rodea tus activos con las protecciones adecuadas, como la gestión de derechos empresariales para el control de acceso a los datos y la imposición de control de acceso basado en políticas en los puntos finales.\n\nOtra opción para reducir su riesgo podría ser limitar el acceso directo a los servicios empresariales y hacer que los usuarios trabajen en un entorno de SASS.\n\n## Puntos Finales y Entornos Inseguros\n\nLa mejor solución para muchos de estos problemas sería un cliente rico empresarial, pero aún hay riesgo involucrado. Los trabajadores están usando un dispositivo que es una extensión administrada de la red empresarial. La seguridad puede estar presente con un inicio seguro, cifrado de disco completo, protección de datos, cortafuegos locales y aplicación de buenas actualizaciones. Sin embargo, el riesgo surge porque muchas organizaciones ya tienen trabajadores remotos, como vendedores o ingenieros de campo, que pueden no tener el mismo nivel de seguridad que los trabajadores que nunca debieron salir del edificio, como finanzas y recursos humanos.\n\nLas organizaciones deben estar vigilantes para asegurarse de que todos los trabajadores ahora tengan la misma seguridad, o incluso agregando capas adicionales que los trabajadores necesitan para trabajar de forma remota en redes no seguras. La autenticación de múltiples factores ahora debería ser un requisito, y la supervisión de políticas de punto final y la gestión de derechos empresariales son ahora más importantes que nunca.\n\nEstamos viendo innovaciones en esta área, como un cliente que está proporcionando a los nuevos empleados no solo laptops, sino también un enrutador gestionado para evitar redes no seguras.\n\n## Sistemas de VDI y Servicios en la Nube.\n\nLa implementación de seguridad para los sistemas de VDI y los servicios en la nube incluye los conceptos básicos de seguridad: protección de datos, seguridad de virtualización tanto en el centro de datos empresarial como en los puntos de acceso, seguridad de aplicaciones, arranques seguros, parcheo y encriptación de redes. La clave está en realizar un estudio preciso de la empresa y los servicios en la nube que se están implementando para sus trabajadores y asegurarse de que todos estén protegidos de igual manera. Cada aplicación, incluso aquellas que no son críticas para la misión, representa un punto potencial de ataque.\n\n## Clientes como Servicio\n\nLos clientes de SASS utilizan servicios en la nube y también utilizan aplicaciones en sus clientes ricos, por lo que existen algunos problemas de seguridad adicionales de los que preocuparse. Debe haber una protección adecuada de datos en la gestión de derechos empresariales (ERM, por sus siglas en inglés) para el acceso a los datos a través de los servicios en la nube y de vuelta al centro de datos. La protección en ambos lados es fundamental. El acceso del cliente a los servicios en la nube debe estar protegido mediante autenticación multifactor y cifrado de red. El acceso del servicio en la nube al centro de datos privado y a los recursos empresariales también debe estar protegido en las capas de acceso a la red, los datos y las aplicaciones. Comprender cómo los clientes están utilizando los servicios y qué datos están accediendo es donde entran en juego las decisiones de ERM.\n\n## Uso indebido y abuso por parte de los empleados internos\n\nLa IT debería utilizar una variedad de métodos para gestionar la amenaza y el riesgo de errores, uso indebido y acciones maliciosas de los empleados internos. El control de acceso basado en políticas y la aplicación de normas desde las aplicaciones hasta los datos, tanto a nivel empresarial como en la nube, son importantes para evitar el mal uso y abuso de los usuarios que ya están autenticados. La principal defensa que tiene la IT es la auditoría y el monitoreo de la inteligencia de amenazas. Gestionar esta información en toda la empresa y la nube durante un largo período de tiempo puede ser muy efectivo para detectar comportamientos aberrantes.\n\nNo hay duda de que debemos pensar de manera diferente ahora sobre los problemas de seguridad con el teletrabajo. Nuestras principales recomendaciones son primero, implementar la tecnología que has estado probando; segundo, educar a tus usuarios; y tercero, activar la autenticación de dos factores y proteger tus datos a gran escala. Si podemos hacer estas tres cosas, podemos reducir el riesgo y estar mejor preparados para el futuro.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT7-es","image":"./episodes/edt-7/es/thumbnail.png","lang":"es","summary":"Con el gran cambio de las personas que ahora trabajan desde casa en lugar de la oficina, la seguridad es una preocupación creciente para muchas organizaciones de TI. En este episodio, Steve Orrin, CTO de Intel Federal, y Darren discuten las amenazas de seguridad y soluciones para ayudar a proteger a los trabajadores remotos y los centros de datos empresariales."},{"id":144,"type":"Episode","title":"Gobernanza de Datos Moderna","tags":["datagovernance","immuta","dataaccess","identitymanagement","cybersecurity","data","technology"],"body":"\r\n\r\nA medida que las organizaciones migran a la nube, la forma en que pensamos sobre los datos y la forma en que pensamos en utilizar los datos está cambiando por completo; en los últimos cinco o seis años, toda la capa de infraestructura ha cambiado. El rendimiento, la escala, las reglas necesarias y el número de usuarios que desean combinar datos han aumentado exponencialmente. Gestionar eso a gran escala no es trivial y ahí es donde entra Immuta.\n\nCon el tiempo, la industria ha replanteado el paradigma del almacenamiento de datos. Anteriormente, cada equipo de datos construía productos de datos en un silo y luego los entregaba a una unidad de negocio. La unidad de negocio tendría sus datos apropiados que les brindaban un servicio de calidad. Ahora, de repente, queremos que cualquier persona pueda combinar datos en toda la empresa: analistas de negocios, científicos de datos, ingenieros de datos, etc. El número de usuarios ha cambiado y no se trata de aplicaciones de búsqueda; se trata de transformar y generar flujos de datos.\n\nEsa es una diferencia que requiere un nivel diferente de calidad de servicio, tiempo y nivel de sofisticación que nunca antes había existido. Se vuelve aún más complejo cuando se añaden las leyes de privacidad, la clasificación de datos y las normativas siempre cambiantes.\n\nLo que Immuta intenta hacer es hacer posible que cualquier usuario combine potencialmente cualquier conjunto de datos, internos o externos a la organización, para obtener alguna perspectiva. Todo se trata de escala. La política debe separarse de la plataforma para escalar la política para todos estos nuevos usuarios y todas las diferentes combinaciones de datos. La seguridad, la privacidad y el gobierno son importantes, pero si no puedes funcionar, nadie está feliz.\n\nUna lección que Immuta aprendió desde temprano es que el proxy no funcionaría. Es como tratar de mover petabytes de datos a través de una pajita: se convierten en el cuello de botella. Es fácil culpar al middleware siempre que hay un problema debido a que siempre son un cuello de botella, ya que están entre las herramientas y los datos. Este es un problema clásico del middleware.\n\nEl segundo problema con ese paradigma de diseño es que con la virtualización clásica de datos, funcionaba porque había un conjunto claro de datos. Sin embargo, cuando hay petabytes de datos, el enfoque de dármelo todo y lo resolveremos se desmorona muy rápidamente debido al tamaño enorme de los datos, así como a todas las reglas y políticas que lo rodean.\n\nEn la virtualización normal, tienes que incrustarte nativamente en la infraestructura informática en la nube en Snowflake, en Synapse, etc. En lugar de poner esta capa de abstracción encima de estas diferentes herramientas, la capa de abstracción está en el lado de la política.\n\nLa política es tan buena como la capacidad de auditarla. Es como una cadena de evidencia. La clave está en poder demostrar si este usuario cumplió o no cumplió con la infraestructura informática. Con la gobernanza moderna de datos, se escala la política internamente y se unifican las auditorías debido al nivel de complejidad de las muchas actividades que las personas realizan con los datos. La auditoría de la política debe simplificarse drásticamente, de lo contrario, es imposible determinar el cumplimiento.\n\nHay tres tipos de políticas para considerar: operativas, regulatorias y contractuales. Un ejemplo de lo complicado que puede volverse todo esto es observar una empresa como Cummins. Si se modernizan en la nube, deben considerar las regulaciones de cada mercado. Para uno de sus motores, ¿qué sucede si tienen que escribir una regla personalizada para cada país en el que están, pero también para cada país en el que no están, porque esos países no deberían poder verlo? Eso no es escalable cuando se habla de decenas de miles de fuentes de datos para los esquemas, que están en constante cambio. Hay petabytes de telemetría de estos motores.\n\nLo que querrías escribir es que solo puedes ver los datos del país donde resides una vez, y luego se aplican en todas partes. Pero no se hace así. Entonces estarían escribiendo algo como 700 políticas para un objeto de datos, cuando debería ser solo una. Mantenerse al día con todas las políticas y regulaciones cambiantes para cada fuente de datos sería casi imposible. Estarías incumpliendo constantemente.\n\nNunca ha sido más fácil globalizar una empresa que hoy en día, y los clientes de Immuta esperan que puedan ejecutar infraestructura a nivel mundial en cualquier nube. Deberían poder trasladar sus datos a cualquier nube y cumplir completamente con las regulaciones.\n\nImmuta aplica reglas a los datos a través de etiquetas en lugar de utilizar los datos sin procesar, porque cada dominio tiene su propio argot sobre cómo clasifican y hablan de sus datos. Por lo tanto, han empezado a clasificar y etiquetar de manera genérica para aplicar plantillas de conceptos generales, como en el caso de HIPAA. Sin embargo, estas plantillas todavía no son infalibles y queda un largo camino por recorrer.\n\nLa visión de Matt como CEO de Immuta es llegar a un estado en el que los dominios puedan compartir sus políticas. Por ejemplo, en el ámbito de la salud, hay una buena razón por la cual Moderna y Pfizer querrían trabajar juntas. Podría haber un acuerdo sobre cómo manejar los controles de datos del mundo real con la academia. Entonces, si hubiera una política consistente que pudiera ser compartida y creada de manera colaborativa en una nube de políticas conceptuales, sería lo correcto.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matthew Carroll"],"link":"/episode-EDT70-es","image":"./episodes/edt-70/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, arquitecto principal de soluciones de Intel, discute la realidad y futuro de la gobernanza de datos moderna con Matthew Carroll, CEO de Immuta."},{"id":145,"type":"Episode","title":"Gobernanza de Datos Moderna","tags":["datagovernance","data","immuta","dataaccess","identitymanagement","ceo","technology","process"],"body":"\r\n\r\nEn la gobernanza moderna de datos, la primera premisa es que debes separar la política de la plataforma.\n\nEl segundo es que no puede haber ninguna ontología. Si alguien intenta crear un superesquema de todo, es imposible, pero se necesita un esquema para aplicar una política. Entonces, si un usuario quiere combinar dos columnas, se necesita una nueva política. Los datos sensibles deben descubrirse, tanto a través de identificadores directos como indirectos. Los identificadores indirectos son más difíciles y se deben aplicar técnicas para mitigar el riesgo de ataques de enlace.\n\nLa tercera premisa es la introducción de una serie de técnicas de mejora de la privacidad como enmascaramiento, censura, autorización, etc.\n\nEstas herramientas están automatizadas como parte de la gobernanza moderna de datos. Un ejemplo paralelo es cómo solías tener que ser un mago para eliminar los ojos rojos en una foto. Ahora, solo haces clic en un botón y los ojos rojos desaparecen. Es lo mismo en cuanto a la privacidad. Necesitamos ese botón fácil para encontrar automáticamente un identificador potencialmente indirecto donde haya un riesgo mayor que muy bajo de reidentificación.\n\nAdemás de la privacidad, hay una miríada de otras cosas relacionadas con los datos que se deben automatizar antes de la clasificación. Aunque Immuta no se adentra en la transformación de datos y esos flujos de datos, ellos proporcionan una API y una interfaz de línea de comandos. Los ingenieros encargados de construir estas canalizaciones pueden hacer su trabajo, y Immuta les proporcionará las reglas, y la actualización de las reglas puede formar parte de la canalización. Ellos quieren ser un conducto hacia esa capa.\n\nHay algunos roles nuevos en este ámbito, como el administrador de datos y el ingeniero de gobernabilidad de datos, que están separados del ingeniero de datos. Y luego los usuarios de datos realmente son tres usuarios separados con necesidades diferentes.\n\nPrimero, hay científicos de datos que tienen habilidades especializadas y necesitan datos de una manera específica. A veces, necesitan herramientas específicas y un entorno de computación específico en algún lugar del mundo para lograr su misión.\n\nPor otro lado, los ingenieros de datos e ingenieros analíticos, uno construyendo los procesos y otro manteniéndolos, necesitan un acceso rápido a una emergencia cuando algo falla. Lo ingresan en un proceso y lo entregan a alguien y verifican que esté actualizado.\n\nFinalmente, están los gobernadores que están tratando de mantenerse al día con las regulaciones.\n\nEstos usuarios tienen opiniones y necesidades muy diferentes en cuanto a gobernanza. Al aplicar una nueva gobernanza, lo más importante es tratar a estos grupos como partes interesadas separadas.\n\nSi pensamos en todos estos roles de manera binaria, como ingenieros de datos, lo que ocurre es que se producen muchas reuniones, por lo que es imposible escalar. Necesitamos crear una relación simbiótica entre las operaciones de datos, la ciencia de datos, el análisis de datos y la gobernanza. Un ejemplo de modelo es Salesforce o ServiceNow, donde hay un flujo de trabajo completo de principio a fin y no es necesario tener reuniones. Esto sería verdaderas operaciones de datos.\n\nImmuta tiene, filosóficamente, un enfoque basado en atributos en lugar de un enfoque basado en roles. El problema con un enfoque basado en roles es que se produce una sobrecarga a medida que inevitablemente se van añadiendo roles. Por ejemplo, una organización farmacéutica tenía más de 800,000 roles debido a que los roles nunca pueden ser eliminados debido a la necesidad de reproducir pruebas de medicamentos. La sobrecarga de roles puede convertirse rápidamente en un problema de escala.\n\nEl acceso basado en atributos es la clave para contrarrestar esto. En lugar de agregar constantemente roles, los usuarios tienen atributos específicos y consistentes. Por ejemplo, un atributo de un usuario podría ser que están etiquetados para que solo puedan ver su propio estado. Con el acceso basado en roles, cada estado, ya sea que lo puedan ver o no, tendría que ser escrito. Esta gestión moderna de identidad es muy escalable. El acceso por atributos simplifica la cantidad de políticas que deben ser escritas y ayuda con el rendimiento.\n\nLas regulaciones globales modernas, como el GDPR, sin embargo, también requieren un propósito. Aquí es donde el acceso a los atributos se vuelve importante: ¿bajo qué propósitos puede operar cada persona? Bajo un EULA, los datos deben ser procesados por los usuarios solo por la razón declarada. De lo contrario, se necesita un análisis de riesgo antes de que los datos sean utilizados operativamente para la producción.\n\nEn este momento, estamos en el comienzo de la gobernanza de datos moderna. Actualmente, los usuarios toman una decisión binaria única sobre los datos, ya sea dar su consentimiento o no darlo. El futuro se encuentra en algún lugar intermedio: el consentimiento limitado. Por ejemplo, si una persona proporciona sus datos genómicos a una empresa como ancestry.com, ¿qué significa eso para su hijo en el futuro? El hijo no dio su consentimiento para que su material genético sea entregado y sea posiblemente examinado por, digamos, una compañía de seguros de salud para determinar el riesgo. En la gobernanza moderna de datos, Matt ve a los consumidores dando un consentimiento limitado, como permitir que una empresa solo analice el ADN para la genealogía y nada más.\n\nEl futuro debe basarse en el consentimiento y el acceso basado en el propósito, ya que en última instancia, los datos derivados impulsan conocimientos a medida que el aprendizaje automático en desarrollo incrusta datos en los algoritmos.\n\nPara obtener más información sobre Immuta y cómo construir un programa de gobierno de datos, visita Immuta.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matthew Carroll"],"link":"/episode-EDT71-es","image":"./episodes/edt-71/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones en Intel, continúa su discusión en profundidad sobre la realidad y el futuro de la gobernanza moderna de datos con Matthew Carroll, CEO de Immuta. En este episodio, discuten la clasificación de datos, políticas y gobernanza."},{"id":146,"type":"Episode","title":"2021 Un año en retrospectiva","tags":["covid","edge","compute","cybersecurity"],"body":"\r\n\r\nUna de las principales expectativas para el año 2021 era que la nueva vacuna nos sacara del COVID y los viajes se reanudaran. Darren esperaba poder ver a sus clientes cara a cara en lugar de través de Zoom o videoconferencias en línea.\n\nOtras expectativas incluían la implementación de modelos de trabajo híbridos, y aunque la educación a distancia estaba en pleno apogeo, había esperanza de regresar a las aulas presenciales. Esperábamos ver un aumento en el servicio al cliente sin contacto o sin fricciones. La industria de los trabajos temporales como UberEats también estaba despegando.\n\nEn la industria, esperábamos ver un gran impulso hacia la IA y el ML a medida que las nuevas tecnologías estuvieran disponibles. También pensábamos que veríamos una aceleración en la industria 4.0 con nueva automatización, ya que las fábricas aún necesitaban producir a pesar de contar con menos trabajadores.\n\nTeníamos grandes expectativas para el año 2021, y no nos decepcionó. Sin embargo, hubo bastantes sorpresas.\n\n## Sorpresas en 2021\n\nAunque la vacuna ayudó, no erradicó el COVID. Hemos tenido cuatro oleadas y actualmente estamos en medio de la ola de Omicron a principios de 2022. Sin embargo, no todos estaban regresando al trabajo y las organizaciones siguen implementando planes de trabajo híbrido.\n\nEl 2021 también trajo consigo la gran renuncia, con un gran número de personas cambiando de trabajo, tal vez debido al agotamiento, diferentes oportunidades o simplemente por la incertidumbre de cuándo tendrían que regresar a la oficina.\n\nSe convirtieron en los héroes silenciosos de la continuidad empresarial, ya que se volvieron aún más flexibles con la capacidad de adaptarse rápidamente para satisfacer todas las necesidades distintas de los empleados y sus clientes.\n\nOtra gran sorpresa fue que las industrias principales fuera de la alta tecnología fueron afectadas por el ransomware, como la industria de carne procesada y un oleoducto. La seguridad, entonces, se convirtió en un concepto importante.\n\nIntel tuvo una gran sorpresa también, con el regreso de Pat Gelsinger como CEO.\n\nCOVID todavía está causando estragos en todo el mundo.\n\nCOVID probablemente será el evento crucial o el cisne negro del siglo. Las economías mundiales se han ajustado para reflejar la incertidumbre de las cuatro oleadas de brotes de COVID, y los picos y valles han causado estragos en los planes empresariales para retornar a las personas a la oficina.\n\n## Modelos de Trabajo Híbridos Creados\n\nLas organizaciones han ideado algunos excelentes modelos de trabajo híbridos, pero muchos aún no se utilizan debido a que las fechas de regreso al trabajo se han pospuesto continuamente debido a brotes, como la actual ola de Omicron en enero de 2022. Lo que hemos visto es un gran cambio cultural en el que las personas se han adaptado al trabajo remoto y posiblemente híbrido. Las organizaciones de TI se han preparado para el ir y venir a la oficina.\n\nEl trabajo diario ha experimentado un cambio fundamental, pasando de que las personas almacenen su trabajo en una máquina de la oficina a almacenarlo en la nube. O bien, las cosas se almacenan en un dispositivo portátil y se replican en la nube. Por lo tanto, hemos sido testigos de un gran cambio en las ofertas basadas en la nube y SaaS, como Office 365, así como en herramientas de colaboración como Zoom y Teams. Además, la incorporación remota de nuevos empleados se ha vuelto común. Todo esto ha afectado a las personas, los procesos y la tecnología.\n\n## Combatiendo la Gran Renuncia.\n\nPara retener empleados en medio de la gran renuncia, las organizaciones deben primero enfocarse en el bienestar de sus empleados. Con el estrés y el aislamiento de la pandemia, ha habido un aumento de organizaciones que buscan ayudar a sus empleados con su salud mental y emocional.\n\nFlexibilidad es otra clave; con el trabajo remoto, las horas y el lugar pueden ser flexibles. Los empleados pueden vivir en otro estado o tener horarios no tradicionales. Esta flexibilidad y la falta de desplazamiento han permitido a las personas involucrarse y explorar más sus propias comunidades.\n\nLas organizaciones de recursos humanos están comenzando a utilizar la inteligencia artificial para descubrir por qué las personas se van, identificar tendencias y determinar qué programas ayudarán a impulsar la productividad y a que los empleados se sientan parte del equipo. Las organizaciones también están recurriendo a la automatización. Se ha producido un aumento en la industria de automatización de procesos robóticos para lidiar con menos empleados.\n\n## El ritmo implacable de la tecnología de la información no se ha ralentizado.\n\nLa tecnología de la información no puede mantener el ritmo heroico que se requería al principio de la COVID, por lo que aunque las horas locas han ido disminuyendo, la demanda se está cumpliendo estratégicamente. Vimos un movimiento masivo hacia las ofertas de SaaS, por lo que los empleados ya no estaban a cargo de cosas que no eran necesariamente sus fortalezas. En cambio, podían recurrir a un proveedor de servicios en la nube, o proveedores de servicios de recursos humanos, ventas o sistemas de planificación de recursos empresariales (ERP). Un movimiento hacia la automatización y procesos repetibles alivió parte de la presión del trabajo diario en la oficina. También hubo una mayor inversión en tecnología DevOps y RPA para ayudar a agilizar y asegurar el desarrollo de productos.\n\n## Seguridad, Cadena de Suministro y Ransomware\n\nLos problemas de seguridad tomaron por sorpresa a la mayoría de las industrias. Industrias como la empacadora de carne, el petróleo y el gas, los hospitales, y las empresas de cadena de suministro y logística fueron afectadas por brechas de seguridad. Principalmente, esto fue un problema de tener que trasladarse rápidamente al trabajo remoto y descuidar la seguridad adecuada. Por ejemplo, tal vez las organizaciones no capacitaron adecuadamente a los trabajadores remotos sobre cómo asegurar sus computadoras portátiles o datos. Entre otras lecciones aprendidas, veremos el surgimiento de la confianza cero este año.\n\nOtro gran problema de este año fueron los problemas en la cadena de suministro, y no solo en el suministro de silicio. Muchos materiales seguirán escaseando en general, afectando especialmente a las pequeñas empresas.\n\nTenemos que encontrar soluciones para combatir el ransomware. Las organizaciones pueden realizar cambios en los procesos y en la cultura, y utilizar nuevas tecnologías para encontrar estas soluciones.\n\n## Intel tiene un nuevo CEO.\n\nLa mayor sorpresa para Darren, y quizás para toda la industria, fue el regreso de Pat Gelsinger a Intel, esta vez como CEO. Todos en Intel están inspirados por la energía que él aporta. Pat dice que va a ayudar a la industria a resolver la escasez de chips invirtiendo nuevamente en la fabricación estadounidense. Darren cree que llevará a Intel de vuelta a la cima de la fabricación de ventas de chips y proporcionará una tecnología maravillosa para todo el mundo.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT73-es","image":"./episodes/edt-73/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren reflexiona sobre las expectativas y sorpresas del 2021."},{"id":147,"type":"Episode","title":"Ransomware: Prepárate y no entres en pánico","tags":["cybersecurity","ransomware"],"body":"\r\n\r\nAunque un ataque de ransomware comienza para la mayoría de las organizaciones con una solicitud de rescate de algún tipo, la historia completa generalmente ha estado gestándose durante muchos meses.\n\nLa demanda de rescate ocurre después de que los atacantes han cifrado la información de los sistemas comprometidos y bloqueado todo lo que pudieron, o han robado información sensible y ahora amenazan con publicarla. En ambos casos, solicitan una suma de dinero para detener la publicación o restablecer la información.\n\nLa cantidad de dinero puede ser objetivo porque los atacantes saben cuánto puede permitirse la empresa, y pedirán una cantidad masiva. Los ataques menos dirigidos pedirán una cantidad al azar y esperarán tener éxito. Sin embargo, cada vez más, los ataques son sofisticados y los atacantes han hecho su tarea. Es posible que hayan obtenido un progreso significativo, moviéndose lateralmente a través del entorno y comprometiendo múltiples dominios. Saben que le costará a la organización una cantidad tremenda restablecer sus servicios, por lo que pueden solicitar más.\n\nPero antes de que los atacantes exijan el rescate, han estado dentro del sistema durante algún tiempo, en promedio un poco menos de 300 días, hasta que ejecutan completamente su plan. Intentan llegar lo más lejos posible dentro de la red para que sea lo más impactante posible. Por ejemplo, eliminarán el ejecutor de copias de seguridad para incapacitar a la organización, dejando datos de respaldo corruptos e inutilizables. También dedicarán tiempo a comprometer credenciales para la escalada de privilegios y moverse lateralmente en toda la red y descubrir cualquier vulnerabilidad.\n\nCada ataque es diferente, por lo que es difícil identificar la vulnerabilidad más común. Los atacantes encuentran cualquier punto de acceso que puedan, ya sea una debilidad en la gestión de vulnerabilidades o incluso en las personas, por ejemplo, ataques de phishing o tácticas de ingeniería social. Harán llamadas telefónicas e impersonarán ejecutivos, o usarán nombres de tipo ejecutivo para obtener información y datos de los empleados en línea.\n\nMuchos en la industria dirán que no es cuestión de si, sino de cuándo serás atacado. Sin embargo, Stephanie no cree que esto sea cierto porque con la debida diligencia, asegurarse adecuadamente y tomar las medidas necesarias, no serás blanco de ataques.\n\nLa debida diligencia es lo que los expertos en seguridad han estado predicando durante décadas: realizar la gestión de vulnerabilidades y parches, cambiar contraseñas y limitar los privilegios cuando sea posible. Capacite a su personal. Los administradores no deberían estar en internet con cuentas administrativas publicando en foros, por ejemplo. Todo se trata de comprender cuáles son las superficies de ataque y gestionarlas.\n\n¿Qué deberías hacer si eres víctima de un ataque de ransomware? Stephanie dice que lo que la gente debería hacer y lo que realmente hacen son dos cosas diferentes. La reacción instintiva de los equipos de TI a menudo es reiniciar o parchar o cambiar el entorno de manera significativa. Esto solo alertará a los atacantes de que su tiempo se está acabando. Para la investigación forense, es extremadamente importante ser cauteloso con esos entornos. Realiza la menor cantidad de pulsaciones de teclas posible y definitivamente no reinicies.\n\nAlgunas empresas entrarán en pánico y realizarán negación de servicios en sí mismas, entrando en modo de bloqueo total y cerrando todo. En lugar de entrar en pánico, deberían depender de sus políticas de respuesta a incidentes y darse cuenta de que el problema se puede resolver aunque no sea agradable.\n\nObviamente, cada empresa debe tener estas políticas de respuesta a incidentes que se puedan activar rápidamente para gestionar las comunicaciones internamente y con los medios de comunicación, y mantener el negocio en marcha si es posible. Los profesionales de seguridad pueden ayudar a establecer estos planes de respuesta y pueden intervenir y ayudar durante un ataque.\n\nSu primer paso para preservar la información es observar algunas de las actividades cuestionables que ocurren en la red. Los ataques de ransomware no comienzan cifrando o robando información; hay muchos ataques previos. Es importante identificar de dónde vienen, dónde se originan y por dónde han pasado. Para hacer eso, los profesionales de seguridad necesitan evidencia e información que deban ser preservadas adecuadamente. Un buen comienzo es tener a las personas indicadas en el lugar correcto para gestionar lo que está sucediendo.\n\nA continuación se debe administrar adecuadamente el entorno. Desafortunadamente, una vez que hay ransomware o cualquier tipo de violación o incidente, la organización es altamente vulnerable. Cien por ciento de las veces, cuando los entornos han sufrido exitosos ataques de ransomware, ya sea publicitados o no, son blanco del mismo grupo o de un grupo diferente. Es como un animal herido rodeado de buitres. Los atacantes saben que estás herido y vulnerable. Viene otro ataque.\n\nLa mayoría de las veces, cuando los profesionales de seguridad están llevando a cabo su investigación, encuentran otros indicadores de ataque y compromiso en diferentes partes de la red. Deben determinar si es parte del mismo ataque o de un ataque diferente. Esta investigación es una parte crítica de la recuperación de malware porque incluso cuando crees que has limpiado un ataque y el negocio está funcionando correctamente de nuevo, todavía existe el potencial para estos otros ataques.\n\nUn ataque típico cuesta en promedio cuatro millones y medio de dólares en limpieza, y eso no incluye el rescate. La cantidad puede ser mucho mayor y es proporcional al tamaño de la organización.\n\nEs imposible estar seguro de cuántas organizaciones pagan el rescate. Muchos de los rescates vienen con amenazas de no contactar a las autoridades o de revelar el ataque. Por esta razón, las estadísticas disponibles sobre cuántas organizaciones pagan son variadas.\n\nAlgunos tomadores de decisiones de organizaciones responden diciendo que no pagarán bajo ninguna circunstancia, incluso si cuesta más reconstruir, destruyendo así la capacidad de negociación. Esta es una decisión emocional que puede nublar el juicio. Al final del día, si el objetivo es seguir haciendo negocios y ganar dinero, pagar un rescate bajo de tal vez diez o veinte mil dólares será más barato que los servicios forenses y el resto del proceso. Por otro lado, existen casos documentados donde se pagó el rescate y los datos no fueron completamente restaurados. Además, la organización no tiene garantías en cuanto a la seguridad de su entorno. No hay garantía de buena fe cuando se paga un rescate; se está solicitando a criminales que actúen de buena fe.\n\nMuchos gobiernos alrededor del mundo han prohibido pagar un rescate debido a que los atacantes son considerados terroristas, y no está permitido negociar con terroristas. Otro obstáculo es que los atacantes a veces se niegan a tratar con negociadores profesionales. A menudo designarán a alguien dentro de la organización como la única persona con quien negociar, esperando que esa persona tome decisiones emocionales.\n\nLas amenazas se comunican de varias formas: correo electrónico, teléfono e incluso en el fondo de pantalla de un escritorio.\n\nLa mejor estrategia para evitar o mitigar un ataque es no esperar a que tu organización se encuentre en esa situación. En su lugar, involúcrate en la debida diligencia. Realiza evaluaciones al menos anualmente para identificar las brechas en la seguridad. Las amenazas y los ataques están en constante cambio y se vuelven más sofisticados, por lo que la seguridad de tu organización debe mantenerse al día. Monitorea y actualiza de manera continua. Realiza gestión de vulnerabilidades, cambia las contraseñas de forma constante y recuerda y educa a los usuarios sobre las amenazas. Estas no son estrategias nuevas. Los profesionales de seguridad han estado recomendando estos pasos durante décadas; simplemente las organizaciones no los están llevando a cabo de manera adecuada ni evolucionando.\n\nTambién es inteligente llamar a los expertos para guiar el proceso de los planes y ejercicios de respuesta a incidentes. Todos en la organización deberían saber qué hacer y a quién llamar en un escenario de ataque para evitar más daños.\n\nSi tu organización es atacada, esperemos que el impacto sea mínimo, o al menos contenido y manejable si ha habido preparación. Todo el tiempo, energía y dinero que una compañía invierte en medidas preventivas es una fracción pequeña del costo de un ataque.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Stephanie Sabatini"],"link":"/episode-EDT74-es","image":"./episodes/edt-74/es/thumbnail.jpeg","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, habla con Stephanie Sabatini, Directora Senior de Servicios Profesionales de Hitachi Systems Security, sobre la prevención y preparación para ataques de ransomware y qué hacer si tu organización es atacada."},{"id":148,"type":"Episode","title":"Soluciones de ciberseguridad con Hitachi","tags":["cybersecurity","technology"],"body":"\r\n\r\nColin ha estado en Hitachi durante casi 24 años después de trabajar en IBM justo después de la universidad. Se enfoca principalmente en cuentas en el Noroeste del Pacífico y en cuentas de SLED. Tuvo la inusual oportunidad de ocupar el puesto de su padre cuando su padre se retiró de Hitachi.\n\nDavid ha estado en el área de tecnología de la información durante más de veinte años, comenzando justo antes de la fiebre de las empresas punto com. Ha sido un hombre práctico durante la mayor parte de su carrera, ocupándose de servidores, almacenamiento y seguridad.\n\nMuchas personas piensan en Hitachi como cualquier cosa, desde una empresa de grandes equipos o herramientas eléctricas hasta una que produce panificadoras. De hecho, Hitachi es una empresa de ingeniería y fabricación que tiene cien años de antigüedad y que comenzó a fabricar motores eléctricos en 1910, y se ha convertido en cientos de organizaciones operativas y subsidiarias totalmente propiedad de Hitachi, incluyendo Hitachi Vantara. Hitachi Vantara es la subsidiaria totalmente propiedad más grande, enfocada principalmente en la gestión de datos. No solo construyen y entregan sistemas de almacenamiento empresarial basados en bloques tradicionales, sino que también se han convertido en una empresa de soluciones digitales que abarca una expansión extrema en IoT y conocimientos basados en datos.\n\nRobert Mueller dijo: \"Solo hay dos tipos de empresas: aquellas que han sido hackeadas y aquellas que lo serán. Incluso eso se está fusionando en una sola categoría: aquellas que han sido hackeadas y lo serán de nuevo\". Esta cita establece el escenario de cómo Hitachi puede ayudar a las empresas. No se trata de si, sino de cuándo serás hackeado, y cuándo serás hackeado nuevamente. Esto también se aplica no solo a las empresas, sino también a las organizaciones en el ámbito público.\n\nHitachi tiene una división federal completa que opera desde Washington D.C., enfocada principalmente en el Departamento de Defensa y cuentas federales. Hitachi Vantara trabaja mucho con cuentas SLED y colabora con la división federal para ofrecer soluciones para los requisitos federales, estatales y locales.\n\nLa mejor manera de visualizar una solución de seguridad de Hitachi es imaginar tu organización como tu hogar, y en ese hogar hay cuatro elementos de protección que debemos implementar.\n\nPrimero está la seguridad física. Hitachi tiene un sistema inteligente, una solución de inteligencia visual, que puede proporcionar vigilancia por video con inteligencia. Puede hacer desde la detección de disparos hasta el seguimiento de licencias y reconocimiento facial. Hitachi despliega estos sistemas en todo el mundo. Las cámaras de Hitachi captaron muchos eventos en el centro comercial de Washington el año pasado, y son utilizadas por organizaciones policiales en todo el país. Todo esto se ha desarrollado a partir de la división de transporte de Hitachi, la cual depende mucho de la tecnología de video para la seguridad.\n\nLa seguridad física no suele ser un tema de discusión, pero es un aspecto clave. Si no puedes controlar tu propio edificio y tu propio centro de datos, entonces no tienes seguridad. Si alguien puede entrar por la puerta, puede acceder a tu centro de datos.\n\nDespués de la seguridad física, viene Hitachi ID. Básicamente, esto es como el candado en la puerta principal de tu casa. Una encuesta reciente mostró que solo el 40 por ciento de las empresas tienen gestión de acceso privilegiado y aproximadamente el 74 por ciento tienen gestión de identidad. Esto no es suficiente porque eso representa mucha exposición, y el espacio SLED es un objetivo de alto perfil. Hitachi ID ofrece soluciones de privilegio de contraseña e identidad en una única plataforma; esta es una solución total donde puedes identificar no solo a los usuarios, sino también a los dispositivos y aplicaciones, tanto en las instalaciones como en la nube.\n\nEl tercer aspecto es cómo proteger tus datos cuando alguien logra entrar.\n\nLa ciberseguridad es un problema de big data, y Pentaho es la herramienta para big data. Con Pentaho, puedes gestionar todas tus fuentes de datos, controlar tu lago de datos y producir resultados útiles. Hitachi descubrió que los científicos de datos y analistas de ciberseguridad pasan el 80 por ciento de su tiempo solo gestionando, obteniendo y limpiando los datos, y solo el 20 por ciento de su tiempo analizándolos realmente. Con Pentaho, esos números se invierten, y pueden pasar el 80 por ciento de su tiempo analizando los datos y dedicándose al análisis y respuesta de amenazas en tiempo real.\n\nPentaho es un sistema de detección de intrusiones, pero también es un sistema de prevención. Por ejemplo, una importante empresa de energía había hecho todo de forma ad hoc, escribiendo sus propios scripts internamente. Ellos reemplazaron toda esa funcionalidad en Pentaho en un día. Entonces pudieron hacer ese análisis en tiempo real y mantenerse al día con los atacantes en lugar de siempre ir por detrás. Lo que hace que Pentaho sea único en un campo abarrotado es su flexibilidad. Puede realizar cualquier tipo de análisis de datos y puede basarse en lo que su organización necesita.\n\nProteger los datos desde una perspectiva de contenido también es muy importante para el sector SLED. Hitachi Content Platform Anywhere ofrece sincronización y compartición segura de archivos, así como un buzón seguro. Es comparable, por ejemplo, a Google Drive o Dropbox, pero la diferencia radica en que HCP Anywhere está controlado por tu propio equipo de seguridad.\n\nSe utiliza por el Departamento de Defensa, que lo renombró como Mill Drive. Las fuerzas terrestres transmiten datos sensibles desde y hacia varios lugares secretos y de vuelta a la sede para reconocimiento y otras misiones. A nivel local, las ciudades lo utilizan, transmitiendo pruebas de video no solo a través de la administración, la fuerza policial y otras agencias, sino también a la oficina del fiscal para la gestión de pruebas.\n\nHCP Anywhere está construido sobre la Plataforma de Contenido de Hitachi, que es su almacenamiento de objetos. Tiene almacenamiento de objetos incorporado, no solo el de Hitachi, sino de todos los almacenamientos de objetos. En lugar de sobrescribir un archivo y permitir que un atacante sobrescriba tu archivo, creas una nueva versión del archivo. Luego, si sufres un ataque, puedes volver a una versión anterior antes de que ocurriera el ataque. Algunas agencias de calificación coinciden en que el almacenamiento de objetos de Hitachi es el mejor del mercado. De serie, obtienes 16 versiones de cada archivo, 16 piezas de metadatos del sistema y una cantidad ilimitada de etiquetas de metadatos personalizados también.\n\nHitachi también tiene una consola para la gestión de políticas de datos, además de la Pasarela de Plataforma de Contenido de Hitachi, que coloca un NAS delante del almacenamiento de objetos que puede ser NFS o CIFS. Además, se añade Hitachi Content Intelligence; es un motor de búsqueda potente. Puede encontrar cualquier objeto en su almacenamiento basándose en metadatos, fecha, hora, etc. Puede extenderse más allá del hardware físico propio (¿O de Hitachi?) a través de múltiples almacenes de objetos. La búsqueda funcionará en cualquier cosa. También es 100 por ciento compatible con AWS s3.\n\nPara los clientes de SLED, hay una oportunidad con el socio de Hitachi, Flexential, para proporcionar Hitachi Content Platform como un servicio. Si un gobierno municipal, por ejemplo, no desea aplicar su propio almacenamiento de objetos, puede estar en un entorno multiinquilino a través de esta asociación.\n\nFinalmente, Hitachi System Security puede ayudarte si eres atacado con ransomware. Ellos pueden realizar un análisis del impacto y negociar con los atacantes de ransomware. Pueden comenzar a construir posturas defensivas alrededor de tu organización. Los atacantes no son jugadores solitarios, sino organizaciones criminales con desarrolladores y administración, por lo que se requiere una respuesta profesional. Hitachi System Security puede llevar a una organización desde el principio hasta el final.\n\nSi eres miembro de ISSA, busca a Hitachi para patrocinar un evento en tu área.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Colin McLean","David Rowley"],"link":"/episode-EDT75-es","image":"./episodes/edt-75/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, habla con Colin McLean de Hitachi, líder del equipo global de Intel, y David Rowley, Consultor Senior de Soluciones, sobre las soluciones integrales de seguridad cibernética de Hitachi."},{"id":149,"type":"Episode","title":"Día en la vida de un Arquitecto de Soluciones en la Nube.","tags":["csa","cloudsolutionarchitect","multicloud"],"body":"\r\n\r\nEl equipo de Arquitectos de Soluciones en la Nube de Intel se formó hace menos de dos años con unas pocas personas y ha crecido rápidamente hasta contar con 21 miembros. Ese crecimiento indica su importancia, tanto internamente para Intel como externamente para los clientes.\n\nA medida que los clientes pasan de la infraestructura local a la nube en busca de flexibilidad y escalabilidad, necesitan comprender que la infraestructura subyacente y las características son importantes para la optimización. Dado que la mayoría de los proveedores de servicios en la nube, incluidos los tres principales, Google, Amazon y AWS, son alimentados por Intel, los CSAs de Intel pueden ayudar a optimizar los servicios durante todo el viaje del cliente en la nube.\n\nUn día típico para un CSA comienza con tareas de oficina como correos electrónicos, reuniones internas, reuniones de equipo, reuniones de personal y una cantidad significativa de tiempo hablando con clientes finales. Estos pueden ser clientes de Intel que actualmente están utilizando CPUs de Intel in situ y ahora están considerando pasar a la nube, o tal vez ya están en la nube. Los CSA también tienen socios, integradores de sistemas, que están interesados en cómo traducir la adhesión de Intel y algunas de las características como el cortafuegos ABS o el Boost o la encriptación.\n\nLos CSAs no solo trabajan en el rol de preventas, sino que también diseñan pruebas de concepto, redactan casos de uso y trabajan con un equipo para realizar pruebas de rendimiento basadas en la carga de trabajo del cliente. También dedican tiempo a crear o co-crear manuales de instrucciones.\n\nTambién crean un manual de instrucciones para capacitar a los propios gerentes y vendedores asociados de Intel.\n\nAdemás, los CSPs están anunciando nuevos servicios todos los días, y los CSAs deben mantenerse al tanto de la tecnología que respalda esos servicios y comprender cómo los servicios pueden beneficiar a los clientes. Esto implica comprender las cargas de trabajo de los usuarios y utilizar también modelos.\n\nLos Asesores de Servicio al Cliente de Intel se ven a sí mismos como consejeros de confianza, una extensión del equipo del cliente, en lugar de tratar de tomar decisiones por ellos. Muchas veces, por ejemplo, un cliente sabrá qué nube quiere usar, pero necesitará ayuda para analizar qué herramientas están disponibles para ayudarles a analizar su carga de trabajo actual y luego correlacionarla con las instancias disponibles en la nube. Los CSAs también ayudarán y educarán a los clientes con el análisis de costos. Intel tiene una cartera de herramientas para cada etapa en el viaje a la nube.\n\nUna vez que el cliente esté en marcha y haya terminado con el proyecto inicial, se pondrá en contacto con los CSAs para nuevas iniciativas y proyectos a medida que continúe el ciclo.\n\nLas CSA tienen un conjunto único de habilidades, en el sentido de que dedican mucho tiempo a escribir, comunicarse y educar, pero también comprenden todos los aspectos técnicos y las necesidades del cliente. Por ejemplo, una CSA debería poder discernir si un problema es un problema técnico o un problema comercial. Los antecedentes de las CSA de Intel son diversos, con diferentes culturas, habilidades técnicas, experiencia en ventas y antecedentes laborales, por lo que aportan experiencia en diferentes áreas. También son una fuente técnica profunda más allá de su propio personal. Si, por ejemplo, alguien desea hablar en detalle sobre los servicios de AWS, pueden traer a un compañero de AWS. En otras palabras, las CSA de Intel pueden ser un único punto de contacto para todas las necesidades de servicios en la nube del cliente.\n\n¿Por qué debería un cliente utilizar el equipo de Intel CSA en lugar del equipo de un CSP? Intel es verdaderamente agnóstico, ya que sus chips y CPUs están presentes en casi todos los proveedores de servicios en la nube. No importa qué servicio elija el cliente, para el CSA de Intel no hace ninguna diferencia. Además, muchos de los CSA de los proveedores no comprenden las funciones subyacentes de Intel que solo están disponibles en ciertos tipos de instancias. Por ejemplo, un CSA de Intel sabría elegir una instancia alimentada por Ice Lake en lugar de Cascade Lake para obtener importantes ahorros de costos y un aumento en el rendimiento de sus aplicaciones. Los CSA de los CSP no necesariamente tendrían conocimiento de esta información.\n\nIntel también tiene muchas herramientas para recopilar telemetría, ya sea en una instancia en la nube o en una instancia de nube de metal desnudo. Los CSP CSAs no tienen acceso a esas herramientas. Pueden solucionar problemas, pero solo a nivel de hipervisor. Entonces, si un cliente tiene un problema, un CSA de Intel puede ir hasta el nivel del chip y utilizar herramientas de solución de problemas y telemetría para resolver el problema.\n\nLa mejor manera de ponerse en contacto con el equipo de Intel CSA es a través de un ejecutivo de cuentas. Técnicamente, los CSA están dentro del equipo de ventas y marketing, por lo que también están buscando activamente oportunidades, como contactos del pasado.\n\nLos servicios de CSA no son un gasto adicional. De hecho, Intel tiene un programa donde financiarán la migración inicial a la nube. Los CSAs traerán las herramientas, las personas y el conocimiento, tanto desde el punto de vista de recursos humanos como de supervisión. Esto ayudará a un cliente en un entorno nuevo a acortar la curva de aprendizaje. Luego, la monetización llega cuando una carga de trabajo se está ejecutando completamente en la nube o cuando ocurre la migración y se están consumiendo recursos.\n\nLos CSAs de Intel no tienen ningún interés en qué clientes de CSP utilizan, sino solo en que estén optimizados en un entorno altamente seguro y confiable.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Kiran Agrahara"],"link":"/episode-EDT76-es","image":"./episodes/edt-76/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren habla con Kiran Agrahara sobre qué hacen los Arquitectos de Soluciones en la Nube (CSAs) de Intel en un día para beneficiar no solo a los proveedores de servicios en la nube (CSPs), sino también a los usuarios finales."},{"id":150,"type":"Episode","title":"Transformación Digital en 2022","tags":["aiml","comms","data","edge","multicloud","cybersecurity"],"body":"\r\n\r\nComo científica de datos principal para el sector público de Intel, Gretchen pasa sus días hablando con los clientes sobre sus desafíos en cuanto a datos, gestión de datos, gobierno de datos, la ética en torno a lo que están haciendo y la inteligencia artificial responsable.\n\nLos seis años de Anna en Intel se han centrado en IoT y el borde, con los últimos tres años en el sector público. Ambos coinciden en que el 2022 será un año emocionante lleno de posibilidades que cambiarán el juego.\n\nDarren, Gretchen y Anna representan cada uno diferentes partes de la transformación digital con los clientes, pero se unieron para proporcionar una forma común de hablar sobre el tema, con seis pilares de transformación digital: computación en múltiples nubes, computación en el borde, inteligencia artificial, aprendizaje automático, seguridad cibernética, gestión de datos y comunicaciones. Están de acuerdo en que estas serán las áreas de grandes transformaciones en el próximo año. Intel está profundamente involucrado en todas ellas, ya que han construido el hardware y el software para respaldar cada uno de los pilares. Los pilares están entrelazados, y Intel juega un papel clave en liderar la industria hacia adelante.\n\nEl hardware de Intel es el fundamento de la mayoría de los proveedores de la nube. Desde una perspectiva de software, Intel ha optimizado los marcos comunes que las personas utilizan para IA, ML o aprendizaje profundo, para poder aprovechar al máximo el hardware subyacente. En algunos casos, el rendimiento es diez a cien veces mejor gracias al software de Intel.\n\nNo solo Intel ha proporcionado hardware para el borde y las comunicaciones, especialmente en 5G, sino que también ha complementado eso con arquitecturas de referencia de software. Además, Intel trata de hacer que los ecosistemas funcionen para todos. Existe un fuerte enfoque en sistemas abiertos de una manera no propietaria, lo que facilita que nuevos participantes y actores existentes tengan una mayor presencia en estos nuevos mercados y ofrezcan avances emocionantes. Intel es uno de los principales, si no el principal, contribuyente a la comunidad de software de código abierto.\n\n## Pilar de Ciberseguridad\n\nImplícitos en los diseños de hardware de Intel se encuentran características de seguridad y capacidades para asegurarse de que los clientes en el ecosistema puedan proteger sus datos en todas sus diferentes permutaciones.\n\nLa seguridad nunca es estática; siempre está evolucionando. Solo Intel no resolverá los problemas de seguridad de una organización. La seguridad es una interacción entre lo que haces con tu hardware, cómo incorporas los elementos de software adecuados y los límites y políticas de tu organización.\n\nIntel se enfoca en múltiples áreas, pero un papel fundamental se encuentra en el hardware de raíz de confianza y autenticación. Muchas características pueden ser construidas directamente para lograrlo. Un paso más allá son las cadenas de suministro confiables o transparentes que Intel puede compartir con los clientes, infundiendo un alto grado de confianza. Esas capacidades mejoran constantemente y Intel siempre trabaja para hacer progresos.\n\nLos ataques cibernéticos están en la mente de muchos clientes debido a las recientes violaciones de seguridad. Intel cuenta con medidas de seguridad como la ejecución confiable y los recintos seguros. Tienen todo un conjunto de capacidades para el tipo de seguridad que deseas, como la encriptación, sin afectar gravemente el rendimiento. Existe una matriz completa de capacidades, incluyendo un conjunto de requisitos diferentes en el borde debido a la pérdida de la seguridad física del centro de datos.\n\nLas arquitecturas con confianza cero están convirtiéndose en los marcos que son especialmente prevalentes en el sector público. Para el Departamento de Defensa, la confianza cero es un mandato. Intel tiene muchas capacidades que se suman a la confianza cero.\n\nUn tamaño no sirve para todos con la seguridad, pero está claro que la seguridad añadida ya es cosa del pasado. La seguridad debe ser incorporada desde el principio y debe iterarse continuamente. Las organizaciones deben preguntarse constantemente si tienen los protocolos correctos establecidos, si tienen las herramientas adecuadas de detección de amenazas y si existe confianza en toda la cadena de suministro. Todo esto es integral.\n\n## Pilar de Edge Computing\n\nAhora que hay tanto que se puede hacer con AI, ML y diferentes algoritmos, es emocionante ver cómo podemos explotar esas cosas en el borde y optimizar las arquitecturas para adaptarse a esos casos de uso.\n\nHay algunos modelos realmente simples en los que todo vive en la nube, y solo se realiza la recopilación de datos en el borde. Si la conectividad lo permite, entonces las latencias pueden adaptarse a las aplicaciones. La mayor parte del procesamiento puede ser centrado en la nube. Sin embargo, existen innumerables casos de uso en los que esto no tiene sentido debido a la sensibilidad de los datos o los requisitos de latencia. Por lo tanto, hay conversaciones interesantes sobre cómo determinar arquitecturas óptimas para el borde y lo que está ocurriendo con la nube y la red.\n\nPor ejemplo, ¿podríamos tener una arquitectura fluida de recolección, uso y procesamiento de datos inmediatamente para proporcionar inteligencia? ¿Podemos incorporar eso en la próxima ronda de capacitaciones para que el modelo se actualice continuamente? ¿Qué tan rápido podemos hacer ese ciclo? ¿Es viable? ¿Necesitamos toda nuestra capacitación en la nube? Si toda la capacitación está en la nube, ¿cuál es el intervalo adecuado para devolver esos modelos actualizados? ¿Y puede la periferia ser lo suficientemente ligera como para usar lo generado en la nube? La periferia sigue siendo muy complicada y hay muchas posibilidades y preguntas fascinantes.\n\n## Pilar de Inteligencia Artificial\n\nLa IA y el aprendizaje automático permiten que el borde haga mucho más de lo que nadie había considerado anteriormente. Un producto no siempre es la respuesta correcta: se trata de la adecuación al propósito. El uso de código abierto es crítico y poder aprovechar los microservicios para ejecutar algoritmos en el borde.\n\nPor ejemplo, si tienes algoritmos justo en el borde que realizan el trabajo de lectura, estás hablando de flujo de tráfico. El ritmo de luces rojas, amarillas y verdes puede cambiar sobre la marcha en función de cuántos autos estén pasando, y al mismo tiempo recopilar datos para enviar de vuelta a un centro de datos más grande que podrá hacer un reentrenamiento. Hacia el final de la semana, tal vez tenga sentido agregar algunos microservicios adicionales o ajustar los algoritmos. Luego, ese contenedor vuelve al borde y puedes responder mejor y seguir aprendiendo.\n\nAdemás, al hablar de adecuación para el propósito, el ancho de banda, la latencia y el factor de forma son todas consideraciones.\n\nLos rendimientos de los cultivos son un buen ejemplo. Un cliente recopila datos que van a un centro de datos. Están trabajando en los modelos, pero eso no se traduce en hacerles saber que necesitan más fertilizante o que hay desafíos actuales con el sol y la lluvia. Eso significa que la fórmula debe ser cambiada. Necesitas los datos de rendimiento de los cultivos, la información, el algoritmo y los microservicios en un factor de forma mucho más pequeño. La latencia y el ancho de banda son diferentes, pero puedes tener una unidad pequeña en medio de un campo recopilando esos datos y respondiendo, por ejemplo, al flujo de agua o a las necesidades de fertilizante para mejorar los rendimientos de los cultivos.\n\nEsperemos que este año, más de los diseños de vanguardia se estandaricen. Con FlexRAM y 5G, hay estándares, pero todo lo demás es el Lejano Oeste. Muchas personas están diseñando cosas interesantes, pero no están diseñando de una manera que facilite tener esos microservicios en un contenedor y algoritmos de IA y ML. En algunos casos, necesitas múltiples algoritmos ponderados de manera diferente, cambiando cada semana basado en nuevos datos y en el nuevo entrenamiento. Necesitamos poder hacer eso de tal manera que no importe quién construyó un dispositivo. Crear estándares no solo en datos de IA y ML, sino también en el borde, ayudará a explotar capacidades.\n\n## Pilar de Comunicaciones\n\nEl lado comercial de 5G liderará el camino con el 5G en los teléfonos en todas partes. Sin embargo, todavía existe un tiempo de retraso para tener el tipo de equipo de usuario para realizar diferentes tipos de aplicaciones para edge o empresa, por ejemplo. Intel está estableciendo sus primeras redes 5G con socios que son más vanguardistas y menos comerciales. Aunque sus socios comerciales han tenido esto funcionando durante mucho tiempo, las redes privadas controladas para un propósito específico están siendo liberadas y se están construyendo aplicaciones. 2022 es el año en que estas cosas se harán realidad.\n\n## Pilar de gestión de datos\n\nLos datos que solían tomar horas o días en ser procesados, preparados, analizados y utilizados ahora pueden tomar minutos o nanosegundos. También puedes aprovechar diferentes modelos, y cuando los pesos cambian, puedes actuar y consumir los datos rápidamente y proporcionar mejores servicios. Mover datos, gestionar esos datos y operativizar tu IA y ML son parte de lo que la gestión de datos aporta al mundo.\n\n## Pilar de Multi-Nube\n\nLa columna de múltiples nubes no se refiere a proveedores de servicios en la nube en este contexto. Se refiere a la infraestructura en general y cómo abstraer esa infraestructura para implementar nuevas capacidades en el borde, en un proveedor de servicios en la nube, o incluso en la infraestructura de su propio centro de datos. El objetivo de la arquitectura de múltiples nubes es que conozcas a los usuarios clave y, lo que es más importante, cómo se gestiona los datos.\n\nDiferentes nubes tienen diferentes capacidades, y dependiendo de los casos de uso, pueden usar diferentes nubes para diferentes propósitos. Intel cuenta con arquitectos de soluciones en la nube que ayudan a los clientes a optimizar las cargas de trabajo entre las opciones de nube.\n\nTodos estos pilares están entrelazados y trabajan juntos. Busca futuros episodios donde Darren, Gretchen y Anna continúen esta conversación.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Gretchen Stewart","Anna Scott"],"link":"/episode-EDT77-es","image":"./episodes/edt-77/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher de Intel, Arquitecto Principal de Soluciones, Dra. Anna Scott, Arquitecta Principal de Edge, y Gretchen Stewart, Científica Principal de Datos, discuten los seis pilares de la transformación digital en 2022: computación en múltiples nubes, computación de borde, inteligencia artificial, aprendizaje automático, ciberseguridad, gestión de datos y comunicaciones."},{"id":151,"type":"Episode","title":"Comprender el Modelo de Seguridad de Responsabilidad Compartida.","tags":["multicloud","compute","cybersecurity","sharedresponsibility","cloudsecurity","cloud","technology","process"],"body":"\r\n\r\n## Seguridad en la nube\n\nLa mayoría de las personas entiende la responsabilidad de la seguridad en el sitio, pero la responsabilidad se vuelve más confusa en la nube. Si los datos están protegidos en la nube, ¿cómo se protegen? ¿Quién es responsable de esa seguridad? ¿Y qué pasa con la instalación de parches en las máquinas?\n\nRecientemente, los proveedores de servicios en la nube han comenzado a ofrecer aún más servicios, por lo que existen múltiples modelos. A veces, la seguridad termina perdiéndose en el medio.\n\n## Áreas de seguridad\n\nHay cuatro áreas principales de seguridad que debemos entender.\n\n## Físico\n\nLa seguridad física es la parte fácil de reconocer. Los proveedores de servicios en la nube son responsables de la seguridad física de sus centros de datos, y tú eres responsable de la seguridad física de tus propios centros de datos. Además, debes asegurar tu espacio físico. Si eres un fabricante, por ejemplo, debes asegurar las máquinas dentro de tu área. Recientemente, se produjo un hackeo a través del sistema de climatización que estaba conectado a la red de una organización.\n\n## Infraestructura\n\nLa seguridad de la infraestructura no es el aspecto físico en sí, sino el propio hardware. ¿Tus interruptores de red tienen los parches de seguridad y actualizaciones correctas? ¿Se están actualizando los dispositivos de manejo y almacenamiento? ¿Están siendo protegidos? La infraestructura puede entrar en áreas grises con los proveedores de servicios en la nube, por lo que debes saber quién es responsable de qué y en qué circunstancias.\n\n## Aplicación\n\nPara la seguridad de la aplicación, debes saber quién tiene acceso a una aplicación y si se está actualizando con los parches de seguridad correctos.\n\n## Datos\n\nProteger tus datos es una de las cosas más importantes que debes hacer. Los datos realmente pueden ser utilizados como una arma en un ataque de ransomware donde los atacantes los toman o los encriptan. También se están utilizando para obtener ventajas competitivas poderosas en diferentes organizaciones. Debes saber dónde están tus datos y cómo protegerlos.\n\n## Modelos de entrega de servicios en la nube\n\nHay tres modelos básicos de entrega de servicios en la nube. Constantemente se crean diferentes modelos, pero los tres más importantes son Infraestructura como Servicio (IAAS), Plataforma como Servicio (PAAS) y Software como Servicio (SAAS). Categorizaremos todo lo demás como X como Servicio (XAAS).\n\n## Infraestructura como Servicio\n\nIAAS es cuando estás arrendando de un proveedor de servicios en la nube. Esto se refiere a máquinas virtuales y redes virtuales, incluyendo almacenamiento, cálculo y red. También estamos empezando a ver cosas interesantes con aceleradores, como GPUs o incluso procesadores neuromórficos. IAAS es donde ejecutas tus aplicaciones.\n\n## Plataforma como Servicio\n\nLa siguiente capa en la pila es PAAS. Aquí es donde obtienes un marco específico como Kubernetes para ejecutar cosas. VMware funcionando sobre IAAS es PAAS. Las tuberías de CICD como servicio tienen muchas herramientas que se ajustan a este espacio. PAAS te permite construir e implementar nuevos servicios sobre esta plataforma para que puedas desplegar y administrar fácilmente sistemas grandes construidos sobre IAAS.\n\n## Software como Servicio\n\nA continuación viene SAAS. Este es un software específico que es administrado por el proveedor de software o el proveedor de servicios en la nube, o podría ser un servicio de terceros que ofrece SAAS para otra persona. La clave con SAAS es que son responsables de la seguridad de la aplicación. Ellos gestionan el tiempo de actividad y todas las áreas gerenciales como confiabilidad, seguridad e integridad. Muchos de los programas SAAS están construidos sobre plataformas PAAS.\n\n## X como servicio\n\nXAAS puede ser cualquier nuevo servicio, como inteligencia artificial, gestión de dispositivos o detección de seguridad.\n\nEntender estos diferentes modelos de entrega es importante porque los modelos de seguridad compartida de los proveedores de servicios en la nube se basan en el modelo de entrega.\n\n## Dominios cibernéticos\n\nCada uno de los seis pilares de ciberseguridad, identificados por Steve Warren, CTO en Intel en un podcast anterior, es importante tanto si estás en la nube, en locaciones locales o en el borde. Los seis pilares son la detección de amenazas, inteligencia, análisis y orquestación; gestión de identidad y acceso; seguridad de datos y aplicaciones; seguridad de red; seguridad de la cadena de suministro; y seguridad de host y sistema. Los seis de estos dominios encajan en el modelo de seguridad de responsabilidad compartida que los proveedores de servicios en la nube están promoviendo.\n\n## Matriz de seguridad de responsabilidad compartida\n\nEsta responsabilidad compartida se ilustra en la matriz. Las técnicas de entrega del modelo de servicio están en el eje vertical: SAAS, PAAS, IAAS y en-prem. Si estás alojando tú mismo, todo lo que está en el extremo derecho es tu propia responsabilidad.\n\nEn el lado del IAAS, eres completamente responsable de la seguridad de los datos y aplicaciones, y eres medio responsable de la infraestructura, ya que aún debes encargarte del control de la red y del sistema operativo. El proveedor de servicios en la nube es responsable de la red física y del host.\n\nEn la capa PAAS, todavía eres responsable de la seguridad de datos y parcialmente responsable de la seguridad de la aplicación y la infraestructura de identidad y directorio. Hay algunas herramientas disponibles para ayudar en estas áreas. Aunque eres responsable de las aplicaciones y sus plataformas, ellos son responsables de los marcos de trabajo y middleware que proporcionan. Aunque la mayoría del sistema operativo será atendido por la capa PAAS y te ofrecen algunas herramientas de nivel superior, todavía eres responsable de configurar los controles de red.\n\nUp the stack en SAAS, incluso si estás utilizando almacenamiento como servicio, datos como servicio o CRM como Salesforce, aún eres responsable de tus datos porque todavía necesitas diseñar y encriptar tus copias de respaldo y gestionar cuentas e identidades.\n\nUn punto clave en todos los modelos es que eres responsable de la seguridad de tus datos; nunca existe un escenario en el cual delegues toda tu seguridad a los proveedores de servicios en la nube. Debes hacer copias de respaldo y preguntarte si estás utilizando almacenamiento de objetos para poder revertir un ataque de ransomware, si estás manteniendo correctamente la gestión de acceso y si estás utilizando herramientas que faciliten este proceso.\n\n## Diferentes enfoques hacia la seguridad.\n\nCada uno de los tres principales proveedores de servicios en la nube adopta un enfoque diferente en cuanto a seguridad, específicamente en relación a la configuración de redes.\n\n## AWS en español significa Amazon Web Services.\n\nAWS se enfoca en la prevención. Cuando inicias una VM, por defecto no hay puertos abiertos, por lo que debes crear grupos de seguridad. AWS es el más restrictivo, utilizando IAM para la gestión de identidad. AWS es genial para equipos de tamaño mediano, pero no funciona tan bien para organizaciones muy grandes.\n\n## Azure: Azul\n\nAzure se enfoca más en la facilidad de uso; la seguridad es menos restrictiva. Utilizan el concepto de redes virtuales para la seguridad, por lo que todas las máquinas virtuales en la misma red virtual pueden comunicarse entre sí en esa red. Esto es lo opuesto a la confianza cero, por lo que debes decidir qué es más importante para ti. Azure utiliza Active Directory, por lo que si ya tienes un Directorio Activo maduro y sólido, esa es una buena forma de gestionar las identidades.\n\n## Plataforma de Google Cloud\n\nGoogle Cloud Platform también se enfoca en la facilidad de uso, pero apuestan por las máquinas virtuales y la seguridad de la red. Puedes tener perfiles que restrinjan todo en una máquina virtual o puedes tener un perfil que lo abra un poco más. Están en un punto intermedio en cuanto a la restricción. Aunque no tan sólido como AWS o Active Directory, GCP tiene un buen manejo de identidad.\n\nTodos estos proveedores de servicios en la nube ofrecen IAAS, PAAS, SAAS, Contenedor como servicio y una variedad de XAAS. Debes evaluar el modelo de seguridad y comprender las diferencias en cada uno.\n\nEn ciertos aspectos, entender el modelo de seguridad de responsabilidad compartida es más difícil que simplemente ejecutar las cosas localmente porque ahora hay más actores involucrados y la complejidad aumenta. La clave está en entender los modelos y utilizar las herramientas disponibles para ayudarte a gestionar la seguridad en múltiples nubes.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT78-es","image":"./episodes/edt-78/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones, Intel, explica los modelos de seguridad de responsabilidad compartida. Quién es responsable de la seguridad puede volverse confuso en la nube; la responsabilidad depende de los modelos de entrega de servicios en la nube y otros factores."},{"id":152,"type":"Episode","title":"Integración de trabajadores remotos","tags":["covid","remoteworker","mobilementor","genz","people","process","compute","cloud"],"body":"\r\n\r\nDespués de una carrera internacional en Nokia, Denis fundó Mobile Mentor hace 17 años. La empresa surgió a partir de su experiencia de que las personas no estaban utilizando la tecnología en sus teléfonos inteligentes. Aunque los teléfonos inteligentes tenían tecnología avanzada, las personas solo los usaban para funciones básicas como llamadas telefónicas y mensajes de texto. Un momento crucial llegó cuando Denis intentaba vender una solución de red a un CEO, y el CEO preguntó por qué su empresa debería comprar más infraestructura cuando los clientes no estaban utilizando lo que la empresa ya había comprado.\n\nDenis se preguntó por qué la tecnología estaba tan adelantada en comparación con lo que los consumidores realmente querían hacer con sus dispositivos. Dejó Nokia y comenzó Mobile Mentor para resolver ese problema. Contrató un ejército de personas expertas en tecnología que se sentarían con los clientes comerciales y los ayudarían a poner en marcha sus teléfonos inteligentes y a ser productivos, aprendiendo posteriormente mucho sobre lo que impulsa la adopción de la tecnología y los hábitos.\n\nHoy en día, los trabajadores remotos pueden tener dos o tres dispositivos, además de dispositivos personales. Trabajan desde casa, la oficina y viajan. La mayor parte del trabajo que Mobile Mentor hace actualmente es hacer que esos dispositivos funcionen y, lo más importante, asegurarse de que estén seguros.\n\nDado que el número de trabajadores remotos se disparó con la COVID, y muchas organizaciones no implementaron ni priorizaron la seguridad adecuada, hubo, y continúa habiendo, una avalancha de ataques informáticos y ransomware. Los ataques de ransomware han aumentado un 500% desde el inicio de la pandemia, atacando trágicamente a escuelas, hospitales y organizaciones municipales.\n\nAdemás, con la escasez de chips, muchas corporaciones no pudieron obtener suficientes dispositivos para los trabajadores remotos y tuvieron que depender de BYOD (Bring Your Own Device). El uso de fuentes de internet domésticas también incrementó el perfil de riesgo.\n\nUna cosa que las organizaciones pueden hacer para reducir el riesgo es eliminar el uso de contraseñas. Las contraseñas fueron una idea fantástica en 1961, pero en 2021, los datos mostraron que son la principal razón por la cual las organizaciones sufren ataques cibernéticos. La mayoría de los ataques comienzan con una contraseña comprometida obtenida a través de una operación de phishing.\n\nLos trabajadores del conocimiento hoy en día tienen un número ridículo de contraseñas. Sin embargo, solo el 31% de las personas utilizan una herramienta de gestión de contraseñas. Otro 31% escribe sus contraseñas de trabajo en un diario personal, y el 24% escribe sus contraseñas en una aplicación de notas en sus teléfonos inteligentes. Alarmantemente, según un estudio de la BBC del año pasado, el 15% de la población británica utiliza el nombre de su mascota como contraseña y el 6% utiliza la palabra \"contraseña\".\n\nEl primer paso para alejarse de las contraseñas es adoptar la biometría. Ahora, un iPhone o una máquina Windows Surface escanean tu rostro y te ingresan al sistema operativo, así como a todas las aplicaciones de inicio de sesión único y aplicaciones de terceros donde tengas esa identidad federada. Ese es un gran comienzo. El futuro para reducir el uso de contraseñas será una combinación de biometría y autenticación de dos factores en todas partes. Mientras tanto, mientras todavía existan infraestructuras de identidad legadas y aplicaciones antiguas donde la biometría no funcionará, tiene sentido utilizar una herramienta de administración de contraseñas.\n\nMobile Mentor encuestó a la industria por generación y descubrió que la Generación Z tiene la mayor cantidad de contraseñas. Muchas de estas personas en sus veinte años se unieron a la fuerza laboral y se incorporaron de forma remota durante la pandemia. En muchos casos, nunca han conocido a su empleador y no han experimentado las conexiones sociales que suceden en un entorno laboral. Tienen una perspectiva única en la forma en que evalúan a su empleador.\n\nLa investigación de Mobile Mentor muestra que las personas prefieren trabajar desde casa, pero en todas las industrias creen que son más productivas en un entorno de oficina. Esto constituye un dilema interesante y una dicotomía para el empleador que intenta conseguir que estas personas vayan a la oficina. La investigación muestra que el 67% de la Generación Z cree que otras empresas están haciendo un mejor trabajo al proporcionar tecnología para sus empleados. Entonces, si el empleador insiste en que vayan a la oficina, podrían optar por buscar otro trabajo. Cambiar de trabajo en la actualidad no implica cambiar tu trayecto diario u otras cosas más allá de usar una computadora portátil diferente. Esto es parte de lo que está sucediendo con la gran resignación.\n\nCon esta dinámica, la experiencia tecnológica es importante. La investigación muestra que se necesitan en promedio tres días para configurar completamente una computadora portátil para trabajar, en comparación con dos días para un trabajador de oficina. Los trabajadores remotos necesitan abrir en promedio tres solicitudes de asistencia técnica para poner en funcionamiento su dispositivo, por lo que su experiencia puede ser dolorosa. No les gusta el estigma de tener que pedir ayuda.\n\nLa mejor manera de solucionar este problema es simplificar el proceso. La provisión sin intervención es ideal, que es el proceso de configurar la tecnología para que una empresa pueda enviar dispositivos a un empleado remoto, y cuando ingresen con credenciales de trabajo, los dispositivos se configuran automáticamente. Todo funciona en menos de una hora, y nadie en IT ha tenido que configurar manualmente los dispositivos, volver a empacarlos y enviarlos al empleado. Se requiere mucho trabajo inicial para que esto ocurra, pero Mobile Mentor puede ayudar a los clientes con este proceso.\n\nDenis cree que los CIO aprenderán conceptos importantes al estudiar a la Generación Z y a los trabajadores remotos. Los trabajadores de la Generación Z tienen actitudes diferentes, en particular en relación a la seguridad y la privacidad. Valoran y priorizan la privacidad personal por encima de la seguridad, casi en una proporción de cuatro a uno. Para una generación que ha crecido con las redes sociales, esto resulta difícil de entender, pero los datos son claros. Están muy conscientes de la política de privacidad de su empleador, pero casi ignoran las iniciativas de seguridad corporativa.\n\nEl consejo de Denis a los CIOs para este problema es posicionar la privacidad y seguridad como dos caras de la misma moneda. La Generación Z puede ser incorporada a la seguridad si se enmarca en proteger su propia información, así como la de la empresa, y por extensión, la de sus clientes.\n\nLa investigación de Mobile Mentor muestra que el IT en la sombra está siendo impulsado y acelerado por los trabajadores remotos. Los trabajadores remotos que viven lejos de la sede central pueden estar participando en un equipo de IT que nunca han conocido y están encontrando aplicaciones, mecanismos de almacenamiento y formas de comunicación y colaboración que sus empresas desconocen. Las fronteras entre lo personal y lo laboral también se están difuminando. Las personas utilizan dispositivos personales para trabajar y casi la mitad permite que los miembros de su familia jueguen con sus dispositivos de trabajo. El mismo número considera que las políticas de seguridad de su empresa son demasiado restrictivas y una tercera parte dice haber encontrado una forma de trabajar al margen de estas políticas. Dos tercios afirman que son más eficientes cuando utilizan aplicaciones de consumo como Gmail y Dropbox.\n\nDenis aconseja a los CIOs involucrar a los trabajadores remotos en las decisiones futuras de productos porque son ellos quienes pondrán a prueba las herramientas de colaboración, herramientas de almacenamiento, aplicaciones y procesos de autenticación de manera más rápida que cualquier persona que trabaje en una oficina.\n\nPara obtener más información sobre Mobile Mentor, visita mobile-mentor.com. Hay un sitio web separado, endpointecosystem.com, donde comparten todas sus investigaciones de forma gratuita para educar e informar a las empresas sobre lo que está sucediendo con los trabajadores remotos, ayudando a evitar la próxima ola de ciberataques y mejorar la experiencia tecnológica del empleado integrado.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Denis O&#39;Shea"],"link":"/episode-EDT79-es","image":"./episodes/edt-79/es/thumbnail.jpg","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones, Intel, Denis O'Shea, fundador de Mobile Mentor, hablan sobre su experiencia, investigación y consejos sobre cómo incorporar a los trabajadores remotos, especialmente a los trabajadores de la Generación Z."},{"id":153,"type":"Episode","title":"Asegurando al teletrabajador Parte 2.","tags":null,"body":"\r\n\r\n## Tecnologías para asegurar al trabajador remoto.\n\nLa seguridad en el lado del cliente comienza y termina con la capacidad de verificar al cliente mediante un arranque seguro. Intel ha proporcionado esta tecnología en los sistemas de nuestros clientes durante años con Secure Boot con Trusted Execution Technology (TXT) y, más recientemente, BootGuard (BtG). En las plataformas más recientes, tenemos Intel Hardware Shield, un conjunto de tecnologías que aseguran el sistema en su nivel más bajo, en el firmware y el nivel del BIOS. Así es como un sistema de control de acceso empresarial puede verificar que un cliente se haya iniciado de manera segura y tenga el firmware correcto y los controles de seguridad correctos antes de acceder a la empresa.\n\nIntel también ha realizado mucho trabajo a lo largo de los años para ayudar a las organizaciones a proteger mejor sus datos. Además, hemos permitido que el cliente active el cifrado en todas partes sin afectar al rendimiento. Hemos estado implementando nuevas instrucciones en casi todas las generaciones de nuestros productos, agregando nuevas capacidades para asegurar al teletrabajador.\n\nNuestro otro enfoque ha sido proteger las aplicaciones y sus datos en uso. Aquí es donde entran en juego las Extensiones de Intel Software Guard (SGX, por sus siglas en inglés). Esto brinda a las organizaciones la capacidad de colocar partes clave de las aplicaciones y datos importantes en recintos de memoria seguros y encriptados. Con el teletrabajo, esto significa que puedes implementar aplicaciones en entornos no confiables y mantener un alto nivel de seguridad.\n\nAdemás, la nueva tecnología que Intel ha introducido en la categoría de detección de amenazas permite a las organizaciones obtener una visibilidad profunda en las operaciones de la plataforma para monitorear las amenazas; ningún malware puede ocultarse. Estas tecnologías están revolucionando la forma en que detectamos malware utilizando el aprendizaje automático y la inteligencia artificial.\n\n## Tecnologías de centro de datos para asegurar al teletrabajador.\n\nIncluso si has asegurado a los clientes, también debes asegurar el centro de datos. Ambos lados deben ser protegidos porque tu sistema es tan seguro como el eslabón más débil. Muchas de las técnicas para asegurar al teletrabajador son similares a las utilizadas para asegurar tu empresa: arranque seguro, seguridad de la virtualización y controles de aislamiento. Nuevamente, tecnologías como Intel TXT y BootGuard te permiten arrancar de forma segura esas plataformas y los activos del centro de datos y la nube. Más recientemente, hemos presentado la tecnología Intel Select Solutions for Hardened Security, que integra muchas de las tecnologías de seguridad de Intel en una plataforma única que está habilitada de forma predeterminada.\n\nNecesitas poder proteger tus datos a gran escala, lo que significa tener la capacidad de utilizar todas tus herramientas de seguridad sin afectar negativamente el rendimiento. Las herramientas de encriptación acelerada por hardware de Intel (SHA, AES-2X, VPMADD52) hacen esto posible. Las nuevas instrucciones de Intel y las tecnologías QuickAssist están diseñadas específicamente para las necesidades de encriptación a escala empresarial y en la nube.\n\nFinalmente, ¿cómo supervisas la inteligencia de amenazas y realizas auditorías a gran escala? La Arquitectura de la Plataforma de Inteligencia Cibernética (FPGA, DCPMM, Optane SSD) utiliza tecnologías de computación, almacenamiento y memoria de alto rendimiento para escalar la plataforma de inteligencia cibernética, incluso con la carga adicional de seguridad externa con teletrabajadores.\n\n## Soluciones a corto plazo.\n\nLa educación de los empleados es la primera línea para frenar las amenazas de seguridad del teletrabajo. La orientación en seguridad doméstica y la capacitación en seguridad, o el refuerzo de la capacitación previa en áreas como el acceso adecuado a los datos, son cruciales. Sea proactivo con las actualizaciones de software mediante la instalación y requisito de parches en los dispositivos de los usuarios. Si tiene control de acceso empresarial, soluciones ERM/DRM y DLP, actívelos y amplíelos. Revalúe sus políticas para asegurarse de que se ajusten a la nueva realidad de los teletrabajadores. Para las conexiones web, active el TLS y asegúrese de que se aplique. La autenticación de dos factores debe ser aprovechada. La mayoría de las organizaciones pueden pensar que no tienen la infraestructura para implementar esto, pero existen diversos proveedores que pueden ayudar en esta área sin requerir desplegar una gran cantidad de infraestructura nueva.\n\nMuchas soluciones son simplemente higiene estándar: asegúrese de que sus agentes de seguridad del punto final estén habilitados y actualizados. Administre y haga cumplir políticas de seguridad para los diferentes tipos de dispositivos de usuario. Habilite el cifrado de disco completo.\n\nLas organizaciones deben entender que los teletrabajadores operan en un entorno donde es probable que otros utilicen el dispositivo en diferentes circunstancias. Buenos controles de seguridad, especialmente la educación de los empleados, pueden evitar problemas y permitir que los empleados trabajen sin impactos negativos.\n\n## Soluciones a largo plazo.\n\nUn plan a largo plazo para la seguridad en un entorno con trabajadores remotos es ahora necesario, ya sea para un cambio permanente hacia más trabajadores a distancia o para enfrentar otra pandemia o situación similar. Hay varios pasos que las organizaciones deben tomar ahora para apoyar esta realidad en el futuro.\n\nUna de las mejores prácticas es implementar políticas de confianza cero. Esto reduce la dependencia de tener que confiar en todos los aspectos de los usuarios y clientes que ingresan. Además, la autenticación multifactor con usuarios y dispositivos debería convertirse en estándar en toda la organización. Para aquellos que aún no han adoptado ERM y el control de acceso a datos basado en políticas, ahora es el momento de hacerlo para proteger los datos tanto fuera como dentro del lugar de trabajo. Es importante implementar soluciones de seguridad en múltiples capas en lugar de solo a nivel de aplicación o de red. Esto incluye el inicio seguro con certificación, la virtualización y la seguridad de contención, y la seguridad y monitorización del firmware. Es importante ampliar la auditoría, la inteligencia de amenazas y la monitorización a los entornos de teletrabajo, a pesar de la resistencia de los usuarios que no desean una mayor monitorización en sus sistemas. También se debe considerar ampliar la seguridad más allá del dispositivo en las ubicaciones de teletrabajo siempre que sea posible, como en dispositivos y redes gestionadas.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT8-es","image":"./episodes/edt-8/es/thumbnail.png","lang":"es","summary":"Con muchos empleados trabajando desde casa ahora, ¿cómo te aseguras de que estén trabajando de manera segura pero aún así les das la flexibilidad que necesitan para completar sus tareas? En este episodio, Darren y el invitado especial Steve Orrin, CTO de Intel Federal, discuten cómo aprovechar la tecnología de Intel para ayudar de manera efectiva a proteger al teletrabajador."},{"id":154,"type":"Episode","title":"Reunión con los Arquitectos de Soluciones en la Nube de Intel","tags":["csa","cloudsolutionarchitect","solutionarchitect","people","compute","cloud"],"body":"\r\n\r\nIntel ha invertido considerablemente en contratar algunos de los mejores CSAs de la industria, con cerca de 80 arquitectos a nivel mundial y 22 en Estados Unidos. Estos son nuevos roles con un gran enfoque.\n\nStephen Holt es un CSA para el Este y gestiona los otros CSAs en esa área. Previamente en Intel, se especializó en bases de datos, pero proviene de roles variados en procesos empresariales, análisis y ventas, análisis técnico y ventas técnicas en IBM y una variedad de startups. Él aporta toda esa experiencia a Intel para ayudar al equipo a trabajar juntos y brindar valor a los clientes.\n\nKiran Agrahara es un CSA de la Costa Este, que reporta a Stephen. Su experiencia está en infraestructura de centros de datos enfocada en la virtualización de almacenamiento de datos. Ha trabajado en las industrias de cables y finanzas. En los últimos cinco o seis años, trabajó con startups enfocadas en áreas como la memoria persistente y el almacenamiento definido por software. Cuando los clientes le preguntan a Kiran por qué deberían utilizar la experiencia de Intel, él responde que Intel está en todas partes y quiere difundir ese mensaje a los usuarios finales.\n\nAntes de que Sarah Musick llegara a Intel como CSA, era socia y se dedicaba a trabajar en software relacionado con la migración y optimización en la nube, lo cual fue impulsado por su experiencia en análisis de datos. Antes de eso, trabajó para una empresa de análisis de texto de aprendizaje profundo y descubrió que el trabajo que realizaba en datos tenía muchas similitudes con su anterior trabajo en ventas técnicas. Ella aplica el análisis de datos en la migración a la nube. Llegó a Intel debido a su integridad y su papel en el nuevo modelo de compromiso para todos, incluidos los proveedores de la nube.\n\nTodd Christ ha estado en Intel durante 26 años, con 30 años de experiencia en TI y en el espacio de productos. Recientemente, provenía del grupo de soluciones empresariales en la nube de Intel, que forma parte del grupo de plataformas de centros de datos. Intel desea satisfacer a los clientes donde residen sus datos, ya sea en las instalaciones o en la nube. Todd fue el arquitecto de Anthos, por lo que los modelos de nube híbrida o multi-nube son importantes para él, y ha trabajado directamente con Microsoft y Google.\n\nUn cambio importante en ayudar a los clientes a pasar a la nube es la simplicidad. Ahora, los clientes no tienen que pensar mucho en el hardware. Comienzan a trabajar de manera significativa mucho más rápido y con menos gastos generales. Aunque el hardware aún es importante, por supuesto, se encuentra abstracto. Sin embargo, es importante no perder de vista los fundamentos. Es como tener un sistema de HVAC del que nunca piensas hasta que no funciona correctamente, y luego se convierte en un problema. Por ejemplo, hay situaciones en las que un cliente descubre que una carga de trabajo en particular que han trasladado a la nube no funciona bien, por lo que tienen que volver a revisar el hardware.\n\nIntel ha estado con los proveedores de la nube desde su inicio y se enfocan en esas cargas de trabajo. Intel pone una cantidad tremenda de esfuerzo en el ecosistema de la nube que ayuda a construir esas cargas de trabajo para que funcionen mejor en Intel.\n\nIntel ha entregado 2 mil millones de núcleos a proveedores de servicios en la nube (CSP) y más del 90% de todos los cálculos en la nube se ejecutan en Intel.\n\nLos clientes quieren escalabilidad rápida y desean los recursos informáticos lo más rápido posible; no les importa qué hardware se utilice. Sin embargo, la verdad es que las cargas de trabajo de baja latencia funcionan mucho mejor en hardware Intel que en cualquier otro competidor. El nuevo procesador escalable Xeon de tercera generación, Icelake, es increíblemente rápido. Una vez que los clientes se dan cuenta de que pueden ahorrar significativamente, se vende por sí solo. Por lo tanto, parte del trabajo de un CSA es la educación.\n\nAunque los CSPs pueden parecer que venden servicios como una utilidad que funciona de inmediato con un 100% de confiabilidad, no puedes simplemente trasladar o migrar cargas de trabajo críticas a la nube.\n\nSi tus aplicaciones están diseñadas en un formato nativo de la nube, entonces no tienes que pensar mucho en implementarlas en la nube. Sin embargo, si tienes una aplicación monolítica diseñada para funcionar en un centro de datos, por ejemplo, no puedes simplemente moverla a la nube porque no está optimizada para ejecutarse en CPUs específicos. Al utilizar herramientas de optimización o migración de Intel, los clientes pueden tomar decisiones informadas antes de la migración.\n\nAlgunas cargas de trabajo pueden no ser adecuadas para la nube. Es por eso que, especialmente en la última parte del 2021, hubo más rumores sobre la repatriación. El péndulo está volviendo un poco atrás a medida que las empresas, en particular, aprenden el equilibrio adecuado. Aquí es donde entran en juego los CSAs. No todo debe ir a la nube, e Intel puede ayudar a determinar cómo optimizar las cosas. Algunos clientes están descubriendo que después de trasladar las cargas de trabajo a la nube debido a mandatos, no están ahorrando dinero e incluso gastando significativamente más que si mantuvieran las cosas en su propio centro de datos. O tal vez hay problemas de seguridad porque la residencia de datos se encuentra en algunos lugares y la nube no está en el país adecuado.\n\nLos Intel CSA son agnósticos, por lo que solo están interesados en lo que sea mejor para las necesidades particulares de los clientes. Ayudan a que las cargas de trabajo sean más móviles, de modo que a medida que los departamentos de TI se vuelven más maduros, pueden traer las cargas de trabajo de vuelta a sus propios centros de datos o moverlas a otro CSP en el futuro, o lo que sea más rentable. Los Intel CSA pueden ayudar a los clientes de formas en las que los CSP no están abordando actualmente.\n\nMuchos clientes están preocupados por los datos porque es costoso recuperarlos de un proveedor de servicios en la nube (CSP, por sus siglas en inglés). Intel cuenta con un amplio equipo de expertos que pueden ayudar con este problema. Están bien informados no solo sobre las estructuras necesarias para configurar modelos híbridos, sino también sobre la seguridad, los cortafuegos y todos esos puntos de acceso. Una vez que tus datos están detrás de un cortafuegos, existen muchas capas de seguridad a las que necesitas acceder para usar esos servicios. Por lo tanto, lo primero que debes hacer es poder enviar tus datos de manera segura de ida y vuelta.\n\nIncluso en un escenario de multi-nube, extraer los datos es costoso. Si estás moviéndote entre Azure y AWS, por ejemplo, los datos aún se están migrando, y eso es un proceso lento. Si los clientes tienen terabytes o incluso peta bytes de datos en local que desean mover a la nube, podría haber un servicio similar a la nube que se ejecute en local donde puedan obtener la facilidad de uso y funcionalidad de una nube. Si piensas en la nube como una función más que como una ubicación, hay más posibilidades.\n\nIntel está aquí para liberar a las personas y permitirles realizar el trabajo que es más significativo para su organización, y el análisis de datos va a ser una gran parte de eso. En 2022, todavía existe una gran brecha entre la cantidad de datos que las personas tienen y la cantidad de conocimiento que están produciendo a partir de ellos. Solo aproximadamente el 3% de los datos se utilizan realmente para generar ideas. Por lo tanto, hay un tesoro masivo y los chipsets de Intel se desempeñan bien en situaciones donde hay un trabajo de procesamiento sólido. El análisis de datos es lo que viene a continuación.\n\nArquitecturar de forma inteligente es parte del futuro porque no tienes que reinventar la rueda. Por otro lado, habrá soluciones más nuevas que podrían ser adecuadas para una organización. Dónde se encuentra una organización en su camino también es clave. Las empresas más antiguas y establecidas que han estado haciendo las cosas bien durante mucho tiempo, por ejemplo, pueden tener una gran deuda técnica que potencialmente podría resolverse. Necesitan examinar la tecnología subyacente y, eventualmente, llevarla a un lugar donde tengan agilidad.\n\nLos servicios de CSA en Intel no tienen coste alguno porque Intel quiere ayudar a los clientes a ejecutar sus cargas de trabajo de la manera más efectiva y aprovechar la tecnología de Intel que es omnipresente en las nubes. La experiencia y el conocimiento de los CSA de Intel son profundos, y trabajan en equipo para ayudar con cualquier conocimiento que un cliente pueda necesitar. Los clientes deben solicitar ayuda a su ejecutivo de cuentas de Intel o a ventas internas para obtener ayuda de un CSA con la migración y optimización en la nube.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Stephen Holt","Kiran Agrahara","Sarah Musick","Todd Christ"],"link":"/episode-EDT80-es","image":"./episodes/edt-80/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, habla con los principales arquitectos de soluciones en la nube de Intel, Stephen Holt, Kiran Agrahara, Sarah Musick y Todd Christ, sobre cómo pueden ayudar a las organizaciones, sin costo alguno, a migrar a la nube y optimizar su carga de trabajo."},{"id":155,"type":"Episode","title":"Programación heterogénea con OneAPI","tags":["cpu","fpga","gpu","heterogeneousprogramming","npu","oneapi","compute","technology","process"],"body":"\r\n\r\nEl objetivo de oneAPI es ayudar a abstraer la enorme diversidad que viene en el hardware para que los ingenieros de software puedan aprovecharlo mejor en niveles superiores y sacar el máximo provecho del hardware. James, un ingeniero de software que también trabaja estrechamente con el hardware, está encantado con lo que oneAPI puede aportar en el contexto de la computación heterogénea de tendencia.\n\nLa palabra heterogéneo aquí básicamente significa que hay diferentes dispositivos en un sistema que pueden realizar computación, pero no ejecutan el mismo conjunto de instrucciones. Por ejemplo, la CPU tiene una forma de ejecutar instrucciones y la GPU tiene una forma diferente. Los FPGA, otros ASIC y dispositivos especializados tienen diferentes formas de ser programados. Aprovechar todas esas distintas formas es el objetivo.\n\nJohn Hennessy y David Patterson, líderes en el campo de la arquitectura de computadoras, llaman a esto la nueva edad de oro de la arquitectura de computadoras. Para los arquitectos de computadoras, es un momento increíble para diseñar todo tipo de dispositivos especializados que hagan cosas mejores para dominios específicos, pero para los programadores, puede ser difícil e incluso aterrador debido a la complejidad. Aquí es donde entra en juego la iniciativa oneAPI y los productos oneAPI.\n\nEl nombre, oneAPI, es tanto una iniciativa con una especificación como una implementación. La iniciativa es un concepto sencillo: los desarrolladores de software deberían tener la libertad de utilizar cualquier dispositivo que deseen con un rendimiento total, y su código debería preservar su valor; no deberían tener que volver a escribirlo para cada nuevo dispositivo. Estas cualidades se pueden aplicar a compiladores, bibliotecas, depuradores y cualquier tipo de herramienta.\n\nUn gran número de software para implementar ciertas especificaciones ha sido liberado como software de código abierto. Gran parte de este software tiene su origen en Intel, pero también Intel lo pone a disposición en forma de herramientas descargables preconfiguradas y listas para usar, para soportar su hardware. Otros proveedores que contribuyen en la creación de partes de oneAPI tienen sus propias implementaciones, por lo que todos colaboran en las especificaciones, mientras cada uno crea su propio soporte para su hardware de forma abierta y con múltiples proveedores.\n\nEl objetivo principal es escribir código una vez y que se ejecute bien en cualquier lugar. Sin embargo, es un problema complejo que requerirá ajustes de rendimiento. Por ejemplo, si un algoritmo funciona bien en una GPU, se ejecutará de manera similar en casi cualquier GPU. Si cambias el algoritmo para que se ejecute en un FPGA, puedes maximizar la reutilización de tu programa, pero tienes la opción de reescribir partes del algoritmo para que se ejecute igual de bien. Es un enfoque complejo que te brinda la capacidad de aprovechar cualquier hardware con diferentes grados de reescrituras, esperemos que estén muy aisladas.\n\nPuedes crear un código para diferentes dispositivos. Incluso existen enfoques de tiempo de ejecución más sofisticados que te permiten detectar lo que hay y ejecutar diferentes piezas de código, para que realmente puedas tener un código fuente común que decida dinámicamente. Puedes aplicar un programa de implementación y este puede averiguar qué hay en tu máquina y utilizarlo de manera dinámica en lugar de decirle al usuario que ejecute un archivo binario diferente según el dispositivo en el que esté.\n\nEsto es emocionante porque, durante mucho tiempo, los ingenieros utilizaban una pila de software que era la mejor para su máquina. Hoy en día, quieren compilar un programa que pueda utilizar dispositivos de múltiples proveedores. El programa debe reaccionar a eso. Para que eso funcione, el compilador debe ser capaz de generar código para estos diversos dispositivos de manera confiable. Aquí es donde entra en juego la apertura.\n\nAlgunos argumentarán que un sistema cerrado tendrá un mejor rendimiento, ya que las bibliotecas y el lenguaje están ajustados específicamente a las capacidades del hardware. Pero la pregunta es, ¿cómo puedes obtener lo mejor de ambos mundos? Si, por ejemplo, hay una implementación específica del proveedor para una biblioteca matemática, puede haber un programa común. Una gran parte de oneAPI no consiste en tratar de reinventar todo el mundo, sino en tratar de organizarlo de manera que se aproveche lo mejor en cada plataforma posible.\n\nOneAPI tiene la capacidad de mover memoria y mover datos. Diferentes modelos de programación se encuentran encima de OneAPI, y es tu elección cuánto quieres involucrarte en la gestión de la memoria. Mover datos es costoso y consume energía, así que no puedes evitarlo, pero OneAPI te brinda las herramientas para gestionarlo mediante la consulta del sistema y permitiendo que tu programa tome las decisiones correctas y dinámicas en tiempo de ejecución que te brindarán el mejor rendimiento.\n\nIntel tiene décadas de experiencia en la construcción de herramientas para ayudar con la afinación y migración, y cuenta con compiladores altamente optimizados. V2 ha ayudado a evolucionar la industria en torno a los contadores de hardware en los procesadores, convirtiéndose en la norma. Hay una variedad de otras herramientas de análisis para proporcionar retroalimentación desde la estructura de su programa hasta encontrar bloqueos y aplicaciones paralelas, y para encontrar dónde es necesario agregar algunos bloqueos. Intel está poniendo todas estas herramientas disponibles de manera \"oneAPI\" para que sean más versátiles, no solo se trata de una CPU.\n\nLos primeros grupos que adoptan oneAPI incluyen la computación de alto rendimiento (HPC). Con la explosión de nuevas ideas en arquitectura informática, habrá una cantidad aún mayor de diversidad e innovación en este ámbito. Los códigos grandes que pueden ayudar a resolver los mayores problemas de ingeniería del mundo, o por ejemplo, resolver problemas farmacéuticos y evaluar nuevos medicamentos, exigen hardware de última generación. Por lo tanto, este concepto de portabilidad de rendimiento se está extendiendo entre los laboratorios nacionales, las universidades y los centros de investigación.\n\nEl HPC de hoy son los servidores de departamento de mañana, por lo que la capacidad de utilizar diferentes equipos requiere que los ingenieros de software planifiquen y presten atención a qué tan portable es el código, ya que el código no muere rápidamente; dura décadas. En pocos años, los sistemas heterogéneos afectarán a todos, y ahora es el momento de educarse al respecto.\n\nLos ingenieros de software no necesitan programar en paralelo para aprovechar oneAPI. Se trata de utilizar cosas que son abiertas y capaces de múltiples proveedores y múltiples arquitecturas. Incluso los ingenieros que están en la cima de la pila deben comprender qué hay en la pila y qué es capaz en términos de portabilidad y portabilidad de rendimiento.\n\nLa comunidad del Internet de las cosas (IoT) es otro grupo que ha estado a la vanguardia en la programación de múltiples dispositivos de cómputo heterogéneos y utilizando diferentes métodos para gestionarlos. Sus capacidades de cómputo siguen aumentando a medida que avanza la tecnología. Por lo tanto, oneAPI se aplica aquí y puede ayudar a formalizar o estandarizar cosas que se han innovado primero en el mundo integrado.\n\nLa idea de una API ha pasado de ser una idea loca de la que solo hablaban unas pocas personas, a ser cada vez más reconocida como algo razonable y solucionador de problemas en sus organizaciones.\n\nEl lugar más sencillo para aprender sobre la iniciativa es en el sitio web de oneapi.io. Para aprender sobre la implementación, haz clic en la pestaña de implementación para obtener un enlace que te dirija a las implementaciones de Intel. Allí, puedes descargar los diferentes kits de herramientas. Busca el Intel dev cloud para probar las herramientas en la nube de forma gratuita, incluso en hardware diferente. El sitio web de oneapi.io también cuenta con una variedad de tutoriales y recursos.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","James Reinders"],"link":"/episode-EDT81-es","image":"./episodes/edt-81/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, discute las capacidades y el futuro de OneAPI, un modelo de programación unificado basado en estándares y abierto para diferentes industrias que ofrece una experiencia de desarrollo común en diversas arquitecturas de aceleradores, con James Reinders, Evangelista Principal de OneAPI de Intel."},{"id":156,"type":"Episode","title":"Cambiando de Infraestructura a Cargas de trabajo","tags":["cloudmigration","cloud","compute","process","infrastructure","migration","workload"],"body":"\r\n\r\nLas conversaciones de Sarah con los clientes han pasado de hablar sobre infraestructura a hablar sobre cargas de trabajo. Una razón evidente para esto ha sido el cambio del centro de datos a la comercialización de recursos para consumir cualquier cosa como servicio. Otra razón es la influencia de los equipos de aplicaciones en las decisiones de arquitectura. Las aplicaciones son el alma de las organizaciones de una manera en la que no lo eran ni siquiera hace cinco años. La última razón es financiera: ahora los presupuestos se clasifican por cargas de trabajo en lugar de gastos generales de TI.\n\nUn desafío en este cambio de infraestructura a carga de trabajo es encontrar un equilibrio entre procesos y cargas de trabajo centralizadas y descentralizadas. Algunas cosas, como la gobernanza de seguridad, son manejadas por un centro centralizado, y otras son manejadas en equipos locales más pequeños. Gran parte de la toma de decisiones se está descentralizando hacia los equipos que están creando nuevas aplicaciones y servicios dentro de TI y también para los clientes.\n\nCOVID fue un acelerador para promover el cambio y trasladar las organizaciones a la nube. Algunas organizaciones de tecnología de la información están permitiendo que sus aplicaciones se muden a la nube sin restricciones, y prefieren limpiar al final en lugar de frenar la innovación que está ocurriendo. Este evento de \"Cisne Negro\" es sin precedentes y aún estamos viendo las consecuencias del rápido cambio de paradigma.\n\nEl rol del CIO está de vuelta, pero solo si comienzan a pensar en la información y las cargas de trabajo en lugar de administrar un centro de datos. Por supuesto, esto depende de la organización. Las organizaciones nativas de la nube, o aquellas que aspiran a ser nativas de la nube, están refactorizando sus aplicaciones en pleno vuelo porque desean ser súper ágiles. Cuanto más lo hagan, más se orientará hacia la infraestructura y el servicio de la aplicación en lugar de aceptar los límites que existían en el centro de datos antes y trabajar dentro de esos parámetros. Anteriormente, en esa situación, provocaba la innovación de los equipos de aplicaciones porque cuando se trata de ciertos supuestos, a veces la necesidad es la madre de la invención, a diferencia de las infinitas posibilidades en la nube.\n\nEse es un extremo del espectro. En el otro extremo se encuentran las corporaciones estadounidenses de herencia, las instituciones. Por lo general, todavía tienen información en el mainframe. Es una situación de \"si no está roto, no lo arregles\", especialmente con aplicaciones estáticas. La nube está permitiendo a los directores de tecnología (CIOs) pensar más allá de la manera tradicional de gestionar estas aplicaciones. Si los CIOs pueden adoptar estas nuevas tecnologías, ahora pueden ver un camino hacia adelante.\n\nEl procesamiento que está ocurriendo bajo aplicaciones discretas es más relevante que nunca. Intel tiene un papel importante en abordar preocupaciones sobre rendimiento o costos en ofertas de nube en parte porque han desarrollado esas tecnologías y también porque son una de las compañías de software más grandes. Existe una vasta cantidad de conocimiento interno. En otras palabras, no todas las instancias en la nube son iguales, por lo que Intel ha optimizado las cargas de trabajo internamente para obtener el máximo provecho de las instancias de nube que están utilizando.\n\nPor ejemplo, muchas organizaciones están trasladando cosas a clústeres de Kubernetes e Intel realiza una gran cantidad de optimizaciones en torno a eso. Pueden llevar las cosas más allá de las gráficas de Helm estándar con extensiones que analizarían la salud del nodo subyacente y no solo la disponibilidad bruta. ¡Hay muchas cosas que Intel puede hacer para ayudar a los clientes a mejorar drásticamente el rendimiento y el costo, no solo un 2 o 3 por ciento, sino un 30 o 40 por ciento!\n\nNo todas las cargas de trabajo pertenecen en la nube. La estructura de una organización tiene un impacto en dónde debería ubicarse la carga de trabajo. La clave está en ser inteligente en la nube.\n\nUna estrategia de múltiples nubes exitosa consiste en tener una nube primaria y una nube secundaria. Cuando muchos hablan de la multi-nube, la motivación detrás de eso es el miedo a la dependencia de proveedores. El lugar donde la mayoría de sus datos residen influye en la estrategia de múltiples nubes, al igual que el lugar donde las cargas de trabajo se adaptan mejor.\n\nPara desarrollar una estrategia, los arquitectos de soluciones en la nube de Intel se involucrarán en un proceso de descubrimiento sobre lo que la organización quiere hacer y dónde están los problemas. Intel muchas veces puede remediar muchos de los problemas con las herramientas que tienen al alcance de la mano. Los arquitectos de soluciones en la nube también contextualizarán las ofertas para hacer el proceso más rápido y eficiente. Parte de su trabajo es ser educadores, para que todos tengan la información necesaria para avanzar.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Sarah Musick"],"link":"/episode-EDT82-es","image":"./episodes/edt-82/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones en Intel, continúa su conversación con Sarah Musick, Arquitecta de Soluciones en la Nube de Intel, sobre el cambio de la infraestructura a las cargas de trabajo. Por favor, colóquelo en la lista de reproducción de Abrazando la Transformación Digital."},{"id":157,"type":"Episode","title":"Direcciones de IDC 2022","tags":["data","datacentric","people","process","idc2022"],"body":"\r\n\r\nDavid está ubicado en Maryland, cerca del epicentro del gobierno. Tiene más de 25 años de experiencia en el desarrollo técnico y de negocios en infraestructuras críticas como el cuidado de la salud, la energía, las finanzas y la industria. Gran parte de su especialización se centra en la vanguardia: cosas en el hyperedge y el espacio incrustado, pero también en los difíciles desafíos de acelerar las cargas de trabajo y aprovechar al máximo una arquitectura que se traduce en empresas y la nube.\n\nDarren, David y algunos de sus compañeros asistieron a la conferencia de IDC en parte para ver si sus propuestas y predicciones estaban en línea con las tendencias que IDC ha estado observando, y se sintieron validados. También fueron para ver si hay brechas que necesitan entender y cómo pueden aprovechar las ideas y servicios de IDC.\n\nLa tendencia a largo plazo más importante, sobre la cual se basaba toda la conferencia, es el enfoque en lo digital primero, y cómo está impactando virtualmente todos los mercados. Meredith Whalen, Directora de Investigación de IDC, explicó lo que realmente significa ser digital primero. Básicamente, cualquier organización, empresa o gobierno, necesita preguntarse cómo convertir la digitalización en un producto real. Para 2024, se espera gastar alrededor de $10 billones en productos digitales en todos los mercados. Este cambio de los consumidores está obligando al gobierno a considerar la digitalización, no solo en sus servicios, sino en cómo se aborda en el comercio mundial, las economías y las monedas.\n\nEste es un desafío particular para los gobiernos a todos los niveles, ya que luchan con redes informáticas en muchos sistemas diferentes y, en muchos casos, están aislados. Aún no existen puentes eficaces para conectarlos. Los sistemas gubernamentales deben examinar cómo pueden simplificar y agilizar las cosas.\n\nLa próxima gran tendencia es una combinación con la computación en la nube. Cada proveedor de servicios en la nube (CSP) tiene su propia infraestructura y las construcciones de infraestructura no son necesariamente portátiles, por lo que se necesitan puentes. Actualmente, los CSPs no son fungibles. Es cierto que solo quieren que los clientes utilicen su infraestructura, pero a medida que los entornos de desarrollo de software y las aplicaciones se desarrollan a través de sus mercados, es difícil trasladarlos. Los CEO y CIO no solo se preguntan si están utilizando las licencias de software y el acceso que han adquirido, sino que se centran más en si el uso ha producido un resultado deseado.\n\nEl modelo actual de PSC tiende a ser ineficiente teniendo en cuenta los resultados comerciales. Los clientes necesitan ecosistemas de software que trabajen juntos para ofrecer esos resultados. Están buscando múltiples ecosistemas que funcionen en conjunto, con los ecosistemas moviéndose sin problemas a través de múltiples nubes y nubes híbridas. Por lo tanto, la tendencia es que los PSC se vuelvan fungibles y los constructores de puentes serán importantes aquí. Esa es la novedad.\n\nLos datos de hoy son fungibles. Puedes moverlos, por supuesto, con un costo asociado, pero puedes copiarlos, modificarlos, etc. Sin embargo, la tendencia se inclina hacia datos no fungibles. Cuando esos datos, o activos digitales, tienen propietario, eso crea grandes impactos para el futuro en cuanto a cómo se manejan los datos en áreas como la seguridad, la confianza y los modelos de negocio y ecosistemas que giran en torno a ellos. Habrá entidades de datos que deberás aceptar y certificar la validez de quién los posee y de dónde provienen, así como todas las políticas que los rodean.\n\nEn esta evolución, habrá pros y contras. Por un lado, los consumidores tienen más poder sobre su propia privacidad si tienen derechos sobre sus datos. Lo mismo se aplica a las empresas y organizaciones. Por otro lado, con entidades y activos de datos no fungibles, los ecosistemas de software y los científicos de datos que utilizan los datos deben tratarlos de una manera mucho más concisa y estructurada. A largo plazo, todos tendrán que gestionarlo. Los gobiernos no están frente a este problema, pero se convertirá en una parte cada vez más importante de cómo manejan y mezclan el comercio, no solo de bienes físicos, sino también de bienes digitales.\n\n¿Qué tan cerca está el enfoque digital en la realidad? Un ejemplo es el uso de tokens digitales como moneda real, como en la comunidad de juegos. Los tokens digitales forman parte de este mundo en relación a cómo las aplicaciones y los ecosistemas utilizan datos no fungibles. Otro ejemplo está en el ámbito de la salud. Hoy en día, puedes ir al médico o verlo virtualmente, y ellos pueden ver lo que está sucediendo y tú pueden describir los síntomas. Todo esto cambia a través de dispositivos portátiles en tiempo real que pueden monitorear niveles de glucosa, ritmo cardíaco, cambios de peso, etc. Esa información pertenece al individuo, por lo que debe ser segura y autenticada, pero también pueden utilizarla para obtener servicios que no se tratan de síntomas, sino de algoritmos personalizados y servicios acerca de lo que realmente está sucediendo en su cuerpo para obtener los mejores diagnósticos.\n\nPuntos de datos a considerar: Meredith Whalen señaló que en 2021 los gastos en servicios superaron por primera vez los gastos en licencias regulares de TI. Por lo tanto, dado que actualmente los servicios son dominantes, la tendencia es que se muevan hacia un modelo basado en resultados. En 2023, IDC espera que el gasto digital sea mayor que el gasto no digital en todas las empresas en general. Cada industria variará, pero a nivel macro, 2023 es el punto de inflexión. En 2024, el grupo de talento sigue el mismo camino. IDC predice que la mayoría de las empresas gastarán más en talento técnico que en talento no digital.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","David Richard"],"link":"/episode-EDT83-es","image":"./episodes/edt-83/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren, Arquitecto Jefe de Soluciones, Sector Público, Intel, y David Richard, Arquitecto Líder de Soluciones, Departamento de Defensa, Intel, reflexionan sobre las tendencias e ideas que obtuvieron de la conferencia de Direcciones de IDC 2022."},{"id":158,"type":"Episode","title":"Asegurando tu castillo con la confianza cero","tags":["cybersecurity","zerotrust"],"body":"\r\n\r\nEl antiguo modelo de seguridad se podría comparar con un castillo, con guardias a lo largo del camino y un foso rodeando el castillo. Todas las joyas de la corona, en este caso, los datos, estarían ubicadas y gestionadas de forma centralizada dentro del castillo. La aparición de la filosofía de confianza cero ha creado un nuevo marco de referencia.\n\nLa mayor amenaza para los datos es el usuario final, por lo que la estrategia número uno es un marco que comienza en el borde exterior con pilares de excelencia y protecciones internas. Esta forma actualizada de pensar permite a las organizaciones involucrar a su misión y socios comerciales en la conversación de una manera real.\n\nEl antiguo estilo de pensamiento era más bien un enfoque de caparazón duro, con protecciones y controles en los lugares vulnerables. Una parte clave de la arquitectura de confianza cero es un enfoque basado en el riesgo, que es más dinámico y se basa en dos cosas: lo que ha funcionado y lo que no ha funcionado en el pasado. Entonces, si los atacantes entraron por la puerta izquierda la última vez, por supuesto, reforzarás las defensas allí, pero también aprenderás de ese ataque y fortalecerás otros lugares basándote en el nuevo conocimiento de cómo ocurrió eso. Un enfoque basado en el riesgo no solo resuelve el último ataque, sino que también piensa en el futuro y aplica los controles adecuados para las amenazas actuales y futuras en toda la empresa.\n\nParte del enfoque basado en riesgos es comprender el ecosistema. Los clientes, socios y usuarios son todos parte del cálculo de seguridad. El enfoque antiguo de caparazón duro no funciona. Al igual que un castillo tiene gente y suministros entrando y saliendo, y las riquezas pueden estar ubicadas en varios lugares del reino, la confianza cero lleva la seguridad un paso más allá, teniendo en cuenta todas las entradas y salidas de los datos o las personas que acceden a ellos.\n\nTradicionalmente, alguien podía obtener acceso con una sola sesión iniciada al castillo corporativo. Hay varios principios de confianza cero, pero las dos bases son el rechazo por defecto y la vigilancia y autorización continua. La confianza no se gana automáticamente, ni es permanente. Por ejemplo, si un invitado entra al castillo, se validan en la recepción y luego se les pregunta qué están visitando. Es posible que se les conceda acceso para visitar a una persona en una habitación durante cierto tiempo, y se les acompañará durante su recorrido. También se les controlará lo que ingresen y saquen durante su visita.\n\nLa filosofía de seguridad conocida como \"cero confianza\" se aplica a todo tipo de acceso en diversas ubicaciones: centros de datos, nubes, dispositivos periféricos, entornos empresariales, entre otros. Esta filosofía se centra tanto en los datos como en el acceso, y se combina con un enfoque basado en el riesgo. Se requiere una estrategia más amplia para su implementación. El enfoque basado en cero confianza resultante no descarta lo que ha funcionado bien anteriormente, sino que combina los buenos procesos, principios y tecnologías, y agrega un elemento temporal.\n\nEste nuevo elemento no es tan difícil como se ha retratado con frecuencia, pero es un problema de proceso y cultura que puede ser complicado.\n\nMuchos desarrolladores tienen miedo de que una arquitectura de confianza cero los ralentice, pero los expertos en seguridad y los desarrolladores necesitan tener una alianza para superar esa percepción. Un ejemplo del mundo real es Log4j. Hace seis meses, los desarrolladores podían descargarlo sin problema, pero ahora el entorno de amenazas ha cambiado. Sin un enfoque basado en el riesgo, un desarrollador podría descargar Log4j hasta que alguien de seguridad lo interrumpiera. Con un enfoque basado en el riesgo, junto con enfoques de acceso, Log4j no estaría disponible y se ofrecería una alternativa. Otro ejemplo sería cuando Log4j ya está incorporado en un producto, la evaluación dinámica de confianza podría agregar controles adicionales en lugar de bloquearlo por completo. Se trata de ambos lados del cálculo en juego.\n\nEsta asociación es similar al entrenamiento cruzado y el intercambio de información que se necesita para incorporar la seguridad en el proceso de desarrollo. Mientras se construye y se prueba un producto, la seguridad también monitorea y evalúa el riesgo tanto para las entidades con las que trabajas como para las vulnerabilidades del producto en tiempo real. Adoptar un enfoque basado en el riesgo en el proceso aprovecha la inteligencia que llega al corazón de mucho de lo que percibimos como difícil.\n\n¿Cuál es el primer paso para que los CISOs, CIOs o CTOs inicien la confianza cero? Cameron sugiere dejar de lado el \"lenguaje técnico\" y comunicarse en inglés común. Iniciar la iniciativa puede ser desafiante porque típicamente los líderes trabajan con un resultado u objetivo en mente. La confianza cero no tiene un objetivo definido hacia el cual trabajar, aparte de crear un entorno más seguro para que los usuarios operen. Por supuesto, hay indicadores clave de rendimiento y otras medidas para mostrar un aumento en la seguridad, pero es un viaje, no un destino. También enfatiza la financiación continua; no incrustar el presupuesto de ciberseguridad en el presupuesto de TI. Debe ser separado y distinto.\n\nEl mejor lugar para encontrar información de alto nivel con orientación práctica es la publicación SP 800-207 del NIST. También establece los cinco pilares de la confianza, que son puntos de inicio válidos.\n\nUn aspecto primordial y fundamental es tener un buen inventario de activos de lo que necesita ser protegido, como fuentes de datos, bases de datos, procesos de negocio y aplicaciones de transacciones. Básicamente, necesitas definir el perímetro de tu castillo. Es importante no solo pensar en lo que posees, sino también en en lo que dependes, como el entorno SAAS, la infraestructura en la nube y las herramientas de terceros.\n\nEl panorama general es conocer tu cadena de valor. No solo se trata de lo que hay en tu castillo, sino cómo ganas dinero, cómo se distribuye ese dinero, a quién pagas y tus proveedores. Cada uno es una pieza crítica de la cadena.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin","Cameron Chehreh"],"link":"/episode-EDT84-es","image":"./episodes/edt-84/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren discute la seguridad de confianza cero con Steve Orrin de Intel, CTO del sector público, y Cameron Chehreh, VP-GM del sector público."},{"id":159,"type":"Episode","title":"Inteligencia Artificial y Seguridad","tags":["aiml","cybersecurity","devops","compute","technology","process"],"body":"\r\n\r\n## Fracasos de IA\n\nRecientemente, Darren pidió a una clase de estudiantes de secundaria y universidad que estudian inteligencia artificial que encontraran fallas en la IA. Encontraron ejemplos como Microsoft Tay, un chatbot que aprendió conversación informal de Twitter y, en menos de 24 horas, estaba haciendo comentarios racistas y misóginos basados en la manipulación de los feeds de Twitter. Otro ejemplo fue un sistema de seguimiento de pelota del Club de Fútbol Inverness que debía seguir una pelota de fútbol, pero en su lugar siguió la cabeza calva de un árbitro. Más grave fue un accidente fatal cuando un vehículo autónomo de Uber no reconoció a un peatón fuera de un cruce peatonal y no logró detenerse.\n\n## Desplegando soluciones\n\nTodos estos fracasos de IA tenían que ver con los datos. Al implementar soluciones de IA, debes hacerte preguntas críticas: ¿Dónde estoy realizando mi inferencia? ¿Está seguro el punto final? Si estás realizando toda tu inferencia en el punto final, tal vez con detección de objetos mediante una cámara, por ejemplo, debes asegurarte de que sea seguro; nadie debería poder manipular los datos, la cámara o el modelo.\n\nOtra pregunta es, ¿qué se va a implementar? ¿Estoy implementando una red neural o algoritmo en el perímetro, o simplemente estoy transmitiendo datos desde el perímetro hacia un centro de datos para hacer la inferencia ahí? Además, cuando se opera la IA, debes preguntarte qué tan seguido se actualizarán los modelos o algoritmos.\n\n## Tubería de IA\n\nEl proceso de desarrollo, entrenamiento, prueba, implementación e inferencia de IA necesita tres elementos: la aplicación, el modelo y los datos. Esos tres elementos deben migrar juntos a través del proceso y estar protegidos de manera concurrente. Necesitas asegurarte de que los datos de producción no sean manipulados incluso durante la producción.\n\n## Amenazas de seguridad de la IA\n\nLas amenazas para la inteligencia artificial (IA) son significativas, ya sea que se trate de espionaje, sabotaje o fraude, debido a que la superficie de ataque es amplia.\n\n## Ataques\n\nPrimero están los modelos. Un modelo puede ser manipulado, como alguien poniendo pegatinas en blanco y negro en las señales de alto para que no sean reconocidas como tales o alguien interfiriendo con la detección de coincidencia de patrones, de modo que los ataques pasen desapercibidos. Los modelos no solo deben ser protegidos durante el entrenamiento, sino también durante las pruebas, implementación e inferencia. El cifrado, el control de acceso y el control de modelo y versión son críticos, al igual que lo serían al desarrollar una aplicación.\n\nLos datos de entrenamiento y producción de origen también deben estar protegidos contra la manipulación.\n\n## Tipos de ataques\n\nUn documento del Centro Belfer clasifica las amenazas en un eje de formato y un eje de visibilidad. El eje de formato va desde físico hasta digital. El eje de visibilidad va desde perceptible hasta imperceptible.\n\n## Ataques físicos\n\nLos ataques físicos pueden alterar objetos físicos como la etiqueta en la señal de pare. Estos ataques fueron evidentes desde el principio en los sistemas de conducción autónoma y el reconocimiento facial. Necesitamos un mejor entrenamiento de los algoritmos de IA para estos ataques, utilizando técnicas de aprendizaje reforzado y aprendizaje de casos negativos.\n\n## Ataques digitales\n\nLos ataques digitales son más difíciles de detectar ya que no son visibles. Un ataque podría ser ruido blanco inyectado en el flujo de datos para confundir al algoritmo. Estos ataques son difíciles de combatir a menos que se realice inferencia en el borde o se implemente la detección de patrones. Por esta razón, es esencial saber de dónde provienen los datos fuente, tanto en el entrenamiento, la prueba y la producción de datos.\n\n## Identifique las fuentes de datos.\n\nLas fuentes de datos deben ser verificadas y probadas a partir de fuentes de datos públicas. Los datos de código abierto no están bien protegidos. Considera la posibilidad de generar fuentes de datos para tener un mayor control. Si utilizas una fuente de datos compartida, utiliza un sistema de control de versiones como GitHub o GitLab para verificar la consistencia. Los datos de prueba también necesitan control de versión, control de acceso y otras medidas de seguridad, tal como sueles hacerlo en un flujo de trabajo de DevOps.\n\nLa última parte, la más desafiante, consiste en proteger los datos de producción. Hacer la inferencia lo más cerca posible de los datos es un buen comienzo. Muchas especulaciones pueden realizarse directamente en el borde con el procesamiento neuromórfico e incluso conjuntos de instrucciones en los procesadores Intel para reducir el riesgo de manipulación de datos durante el transporte. En cambio, puedes encriptar los datos y enviarlos de vuelta al centro de datos.\n\nProteger y gestionar datos / Seguridad en el proceso de IA\n\nUna vez que hayas identificado todas tus fuentes de datos, hay tres aspectos críticos para la protección: control, seguridad y encriptación.\n\nLo primero es tener control. Deberías tener control de versiones, bibliotecas protegidas y realizar copias de seguridad y restauraciones en caso de archivos de datos dañados. Estas son prácticas estándar de seguridad que la inteligencia artificial debería implementar, al igual que en el desarrollo de aplicaciones.\n\nLa seguridad debe incluir autorización de acceso, incluso algunos conceptos de confianza cero como otorgar acceso a las personas que lo necesitan por un corto tiempo. Asegúrese de que los modelos no estén siendo manipulados y asegúrese de que estén vinculados a aplicaciones específicas.\n\nLos datos deberían ser encriptados en reposo, en tránsito y en uso. En el pasado, esto solía ser costoso en cuanto a la utilización de la CPU y el tiempo, pero ahora gran parte de la encriptación se realiza en silicio y es muy rápida, con un mínimo o nulo retraso en el rendimiento.\n\n## Llamado a la Acción\n\nLos datos son clave para lograr el éxito y la seguridad de la IA, por lo tanto, protégelos y utiliza las mejores prácticas de seguridad desde el principio. Operacionaliza las canalizaciones para eliminar a los humanos de la rutina diaria de implementar y probar algoritmos de IA. Automatiza tanto como sea posible e incorpora seguridad en la canalización de AI DevOps para proteger tus datos fuente, modelo y aplicación.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher"],"link":"/episode-EDT85-es","image":"./episodes/edt-85/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren discute los aspectos de los datos sobre inteligencia artificial (IA) y la importancia de asegurar esos datos."},{"id":160,"type":"Episode","title":"Las cosas que desearía haber sabido como CTO del gobierno","tags":["cto","people","process","change","organizationalchange"],"body":"\r\n\r\n¿Qué han aprendido Jason y Ron desde que se unieron a Intel y que les hubiera gustado saber cuando estaban en el ejército?\n\nJason se sorprendió de cuánto mejor eran las soluciones integradas de la industria y desarrolló algunas capacidades increíbles que faltaban en el ejército. Aprendió que los casos de uso empresariales demuestran que comprender el conjunto de problemas del cliente es más valioso que simplemente impulsar la tecnología. Los CTOs necesitan conocer la tecnología y aprender cómo cambiar la cultura de los miembros del equipo para adaptarse a ella. Hacer que la experiencia del usuario sea más amigable, construir una tecnología más inteligente, más ágil y más rápida, y lograr que las personas se sumen son elementos que pueden facilitar transiciones más fluidas. Le hubiera gustado ver más resultados y soluciones en el ejército en lugar de simplemente productos.\n\nRon está de acuerdo con la evaluación de Jason y añade que el desafío que él vio desde dentro del gobierno fue cómo lograr que la industria se sumara como socios de soluciones para las misiones, de manera que puedan proporcionar mayor información a los equipos militares sobre cómo aplicar su tecnología en esas misiones. Una gran tecnología sin comprender cómo resolver el problema real de la misión puede ser un desastre.\n\nOtro elemento en el que ambos están de acuerdo que es importante es la experiencia del usuario. Es fundamental que la tecnología se adapte a las personas que deben operarla. No todos los que utilizan la tecnología necesitan tener un doctorado en ingeniería. La experiencia del usuario debe estar diseñada según el nivel de habilidades de las personas. El resultado rara vez es independiente de los seres humanos. Al presentar soluciones para la defensa o la industria, es necesario comprender las necesidades tecnológicas de la misión, pero también el espacio en su totalidad y cómo operar en ese espacio. Especialmente en el ámbito militar, debes planificar ejecutar en momentos en los que las cosas no hayan salido bien y hacer que funcione. La tecnología no ganará el día si no se planifica el elemento humano y las circunstancias desafiantes.\n\n¿Cuáles son las sorpresas más grandes o emocionantes para Jason y Ron al haber pasado de la industria militar a la privada?\n\nJason y Ron dicen que no esperaban una cultura distinta de trabajo en equipo inherente al servicio militar en la industria privada en Intel. Ambos encontraron la experiencia de integración y el apoyo continuo alentadores. Aprecian la actitud general de estar juntos en esto y la capacidad de crear cosas con un equipo dinámico que es más increíble de lo que nunca podrías lograr por tu cuenta.\n\nLos dos aprecian que no se espera que solo vendan productos de Intel, sino que ayuden a los clientes a resolver desafíos de misión y proporcionen comentarios de los clientes a Intel. Están en Intel para resolver problemas, especialmente en el sector público, tal vez con tecnología que aún no ha sido creada.\n\n¿Cuáles son las brechas tecnológicas que Jason y Ron ven en el Departamento de Defensa o Intel?\n\nEn primer lugar, Jason dice que están las operaciones en la nube, y es una dinámica cambiante en el Departamento de Defensa. Los comandantes en el campo de batalla son aversos al riesgo. No hay lugar para el DDIL. A medida que evolucionan las operaciones en la nube, debes volver a entrenarte y reaprender todo el trabajo sobre una operación en la nube precisa y los beneficios de la capacidad de borde a nube que ofrece información precisa en tiempo real que llega a las personas adecuadas en el momento adecuado. Todos deben tener conciencia de la situación y una imagen operativa de lo que está sucediendo.\n\nRon cree que lo próximo en la lista es aumentar la ciberseguridad a medida que aumenta la superficie de vulnerabilidad. Si el ejército no se dirige hacia la confianza cero a medida que se traslada a un borde competente y altamente móvil, los resultados durante un conflicto podrían ser desastrosos. El problema del DDIL es enorme, pero debe ser seguro contra la mayor superficie de vulnerabilidad.\n\nJason cree que la tecnología debe avanzar a pesar del riesgo porque el ejército siempre gana con la información. Ya sea en misiones de FEMA, proporcionar energía nuclear a una ciudad, establecer hospitales de campaña durante el COVID o en el campo de batalla, el avance tecnológico, especialmente el 5G, es fundamental para las operaciones. Los líderes del Departamento de Defensa deben contar con información y la capacidad de comunicarse con su sede para recibir orientación, especialmente en operaciones de combate donde el liderazgo puede cambiar debido a bajas hasta el rango más bajo de cabo.\n\nRon utiliza el ejemplo de la máxima prioridad de la defensa nacional, que consiste en nunca tener que luchar contra un adversario en territorio propio. Dado que Estados Unidos puede no tener una ventaja cuantitativa fuera de casa durante un conflicto contra un adversario que también puede ser tecnológicamente avanzado, el ejército debe ser más capaz. El ejército debe seguir proporcionando una mayor capacidad a las fuerzas de defensa nacional y hacerlo de manera segura, a pesar de la mayor exposición a vulnerabilidades. Esos problemas deben resolverse para que los operadores puedan confiar en los datos y utilizarlos de manera efectiva en un entorno fuera de casa. No hay otra opción más que avanzar en esa dirección.\n\nJason añade que otro espacio en el que el DOD ha sido lento en adaptarse es la IA debido a la falta de personas con antecedentes adecuados. No se sale de la Escuela de Ranger del Ejército como un experto en IA. El mismo problema existió con la ciberseguridad durante años hasta que el DOD realizó una inversión significativa. Algunos recursos complejos deben destinarse a las operaciones de IA porque la IA puede cambiarlo todo.\n\nRon, después de pasar los últimos años en servicio en el ámbito nuclear, señala que Estados Unidos nunca eliminará el factor humano de los procesos de toma de decisiones críticas. La IA será de gran valor, ya que puede garantizar que la tecnología se adapte dinámicamente. Reducirá la carga cognitiva y procesará innumerables puntos de datos para que los tomadores de decisiones humanos tengan una conciencia situacional más clara y estén mejor preparados para tomar decisiones informadas rápida y eficientemente. Ese es el espacio de la IA que el Departamento de Defensa debe perseguir.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Jason Dunn-Potter","Ron Fritzemeier"],"link":"/episode-EDT86-es","image":"./episodes/edt-86/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren Pulsipher, Arquitecto Principal de Soluciones, Sector Público, Intel, da la bienvenida a los invitados especiales Jason Dunn-Potter, Jefe de Oficial Técnico Retirado, Ejército de los Estados Unidos, y Ron Fritzemeier, Contraalmirante Retirado, Marina de los Estados Unidos. Ambos llevan cinco meses en sus nuevos puestos como Arquitectos de Soluciones y Especialistas en Misiones en el Equipo del Departamento de Defensa de Intel."},{"id":161,"type":"Episode","title":"Computación Confidencial en DevSecOps","tags":["confidentialcomputing","devops","compute","technology","process","devsecops","cybersecurity","sgx"],"body":"\r\n\r\nEl software de Anjuna permite que las aplicaciones se ejecuten con la protección SGX de Intel y resuelve el problema de proteger los datos en uso. La misión de Anjuna es hacer que los recintos seguros sean lo más simples posible. Con el software de Anjuna, no es necesario cambiar nada en la aplicación; simplemente tómelo, ejecute en un recinto y la tecnología SGX funcionará sin problemas. El software funciona con cualquier aplicación, en cualquier nube y a cualquier escala.\n\nLa cadena de suministro global de software está siendo atacada. SolarWinds, en particular, fue un ataque a DevOps, y aunque ha habido ideas sobre cómo resolver el problema, aún no se ha asegurado completamente. La tecnología de Anjuna puede ser una solución sencilla. No es necesario rediseñar su software o cambiar metodologías. Se ejecutan en recintos seguros.\n\nCuando probó el software de Anjuna, Darren creó una pila utilizando Intel SGX en la base, Red Hat OpenShift, Anjuna para la parte de cómputo confidencial, y Vault de HashiCorp para almacenar un libro mayor seguro. Se sorprendió de lo rápido que la solución estaba funcionando en menos de una semana.\n\nDarren llama a este proceso la línea de producción de DevSecOps fortalecida, aunque tiene muchas partes móviles. Ofir está de acuerdo con este término, ya que este proceso es un nuevo DevOps fortalecido con la tecnología de hardware SGX y el software de Anjuna.\n\nLa computación confidencial, o enclave seguro, resuelve el problema de proteger los datos. Cuando almacenamos datos en almacenamiento persistente, la solución para los datos en reposo ya está presente. También existe una solución para los datos en tránsito con TLS. La seguridad de los datos en uso aún no se ha resuelto porque cuando los datos están en uso, la aplicación necesita acceder a ellos desde la memoria de forma clara. No puede estar encriptado y en uso al mismo tiempo. Esto ha sido un bucle infinito de un problema. Si un actor malintencionado tiene acceso a una máquina donde se ejecuta la aplicación, un hackeo es tan simple como ingresar al dispositivo, identificar el proceso y crear un volcado de memoria. Obtendrán todos los secretos y datos confidenciales almacenados, y no están encriptados. Esto también incluiría las claves de encriptación para los datos en reposo y en transporte, ya que el software necesita usarlas para encriptar. El mal jugador tendrá las llaves del reino.\n\nEl problema se resuelve si ejecuta las diferentes aplicaciones en enclaves seguros. Incluso si alguien tuviera acceso a la máquina, no tendrían acceso a la memoria de cada aplicación. Esto no significa que no deba resolver vulnerabilidades, pero estará mucho menos estresado para corregirlas lo antes posible. Incluso si existen vulnerabilidades en el kernel, cuando algo se ejecuta en un enclave seguro, el kernel no puede acceder a su memoria.\n\nEl software de Anjuna se ejecuta en otras tecnologías basadas en hardware además de SGX. A diferencia de ejecutar encriptación en software, donde la disminución de rendimiento sería alta, Anjuna puede ajustar la configuración para ejecutar su aplicación con una disminución de rendimiento insignificante: menos del cinco por ciento.\n\nPor lo tanto, es posible que aún no desees colocar todo en un enclave seguro, pero es el futuro para la seguridad.\n\nUno de los usos de una enclave segura es almacenar datos que abarcan diferentes pasos en la tubería DevOps en un libro mayor seguro. El libro mayor contiene todo lo que se incluyó en la compilación, claves de seguridad y valores hash utilizados para la verificación. Estos valores hash de verificación deben permanecer inalterados durante el ciclo, para que nadie pueda inyectar código, bibliotecas o binarios en el paquete que entregas. Todo debe ejecutarse en un contenedor en el mundo moderno.\n\nOtro candidato para protección es una clave de firma. Sin las enclaves seguras, una vez que tienes un archivo binario listo, necesitas llevarlo a otra máquina en una habitación oscura a la que nadie tenga acceso. Pero tres personas con tres claves diferentes lo firman allí. Las enclaves seguras permiten acceder a esa clave de firma en tu entorno familiar, pero solo la enclave la puede acceder. Se basará en la identidad compleja del software que se ejecuta dentro de la enclave SGX, lo cual se implementa a través de la cita de acreditación. En otras palabras, puedes acreditar enclave a enclave. También puedes acreditar cosas que se ejecutan fuera de las enclaves. Te brinda la capacidad de confiar en el software que se ejecuta en otro lugar.\n\nLa compilación de binarios es otro uso. Uno de los grandes problemas en el Departamento de Defensa, por ejemplo, es que desean tener la garantía de que todo lo que se incluyó en la construcción se pueda rastrear hasta el desarrollador que lo escribió. Especialmente en sistemas integrados donde el software controla máquinas de millones de dólares que pueden matar o salvar vidas. Debe haber una trazabilidad completa para ayudar a garantizar la responsabilidad y que se haya realizado un desarrollo seguro.\n\nAdemás de los ataques de volcado de memoria, otro problema de ataque que Anjuna resuelve es asegurarse de que en casos en los que se necesita acceder al núcleo, protegerá todo lo que necesita estar cubierto en esa interacción entre la enclave y el mundo exterior. También puede proteger contra el acceso al código y hacer que los secretos solo estén disponibles para la enclave. Además, si alguien logra acceder a una máquina, no podrán encontrar un certificado TLS en claro o la clave que se utiliza para encriptarlo.\n\nCada proveedor de servicios en la nube ofrece recintos seguros, y Anjuna los respalda a todos. También respaldan tecnologías locales. Además de la oferta principal, Anjuna también puede habilitar la capacidad de cifrar sus datos en reposo y en tránsito sin cambiar su software, incluso en aplicaciones heredadas o nuevas aplicaciones que no admiten cifrar cada archivo de datos.\n\nPara obtener más información, visite anjuna.io, o consulte un white paper escrito por Darren y Ofir en embracingdigital.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Ofir Azoulay-Rozanes"],"link":"/episode-EDT87-es","image":"./episodes/edt-87/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, y Ofir Azoulay-Rozanes, Director de Gestión de Productos de Anjuna, discuten las soluciones de Anjuna para la computación confidencial en el ciclo de vida del DevOps."},{"id":162,"type":"Episode","title":"DevSecOps colaborativo","tags":["cybersecurity","devsecops","rpa","technology","compute","zerotrust","zta"],"body":"\r\n\r\nLa experiencia de Mike como ingeniero de ciberseguridad en la Fuerza Aérea y luego trabajando en desarrollo, seguridad, operaciones y servicios administrados lo llevó a tener la meta de crear un producto que pudiera ser colaborativo para construir automatización moderna en torno a lo que él llama \"TI como código\". Quería ver a DevSecOps de manera holística, reuniendo a todos.\n\nSu producto resultante, Sophos Factory, crea soluciones modernas mediante la combinación de elementos básicos con características, funcionalidades y experiencia de usuario que pueden ser utilizadas por todas las habilidades técnicas. Resolver este problema fue complejo e incluyó a personas que trabajan visualmente, desarrolladores que codifican, entre otros... También tenía que cerrar la brecha entre hardware y software, utilizando un proceso ágil en todos los equipos.\n\nSophos Factory es más que solo un pipeline de CI/CD. Esa es una pequeña parte de todo el sistema, el cual funciona de principio a fin desde el desarrollo, la seguridad, las operaciones y el despliegue, con características como un constructor visual, un DSL y soporte para todo el contenido en su formato nativo. También se integra con sistemas existentes. Incorpora a todos los diferentes equipos y las diferentes herramientas que utilizan, por lo que va mucho más allá de simplemente crear un pipeline o automatización.\n\nLos usuarios individuales se les presentan las piezas con las que están familiarizados, pero todas con la misma interfaz. Por ejemplo, se puede crear un conjunto de scripts desde un formato visual. Una persona de seguridad puede utilizar la misma interfaz con las herramientas y artefactos que esperan. Un desarrollador full-stack o un ingeniero DevOps pueden incorporar y construir todos los artefactos de manera que los demás equipos puedan utilizar. No se trata de la creación de un pipeline para una automatización, sino de una tela interconectada entre sistemas dispares.\n\nLa integración significa el movimiento de datos, pero también implica acciones. Por ejemplo, supongamos que alguien utiliza Jira en el lado de programación, ServiceNow en el lado de operaciones de IT y un componente de respuesta de incidentes en el lado de seguridad. En ese caso, puedes integrar todas esas piezas y enviar algo a Slack, para que todos puedan tener visibilidad y responder casi en tiempo real.\n\nSophos Factory empaqueta módulos en tuberías para su reutilización, convirtiéndose en bloques de construcción. Estos pueden construirse alrededor de varios casos de uso, pero el objetivo es crear algo que se pueda usar una y otra vez. Por ejemplo, supongamos que estás usando ServiceNow y quieres crear un ticket. En ese caso, lo usas en varios otros casos de uso relacionados con la automatización de redes, automatización de infraestructura, nube, seguridad nativa, etc. Se trata de construir soluciones en lugar de simplemente automatizar estas cosas juntas. La última pieza es la futurización, no solo la repetibilidad. Puedes agregar o quitar de la tubería general, lo cual no es posible con hardware, pero también es muy difícil con los sistemas existentes como los sistemas CI/CD que están diseñados para lanzar software a producción, no para construir de manera integral una solución y mantener su ciclo de vida a lo largo del tiempo.\n\nCon la tecnología Sophos Factory, puedes empaquetar diferentes herramientas para ayudar a cumplir con estándares como CIF o NIST 853 y tenerlas disponibles como canalizaciones de bajo código o sin código. Sophos Factory se diferencia de otras tecnologías de automatización con su compartición a través de catálogos de soluciones. Puedes publicar bloques de construcción de automatización, soluciones completas o consumir la automatización creada por otros equipos. Esto crea una tremenda cantidad de flexibilidad.\n\nEl control de versiones está integrado en los pipelines y los catálogos de soluciones. Si estás utilizando un pipeline de soluciones de un registro que alguien más publicó, puedes establecerlo en la versión que desees o actualizar a la última versión para obtener cualquier actualización. RBAC también forma parte del sistema en caso de que solo desees que usuarios específicos tengan, por ejemplo, acceso de solo lectura. Con la interoperabilidad de Sophos Factory, también puedes integrar herramientas de escaneo para mantener visibilidad en el pipeline. También puedes ejecutar diferentes canales en torno a herramientas de políticas.\n\nSophos Factory entrelaza los flujos de seguridad y tecnología de la información, creando un excelente punto de integración entre el monstruo de tres cabezas de Desarrollo (Dev), Seguridad (Sec) y Operaciones (Ops).\n\nPara mejorar la seguridad, Sophos Factory cuenta con un producto de confianza cero y certificación, pero también trabaja con otros productos de seguridad como HashiCorp Console. La capacidad de confianza cero y certificación es la evolución natural para autenticar entre diferentes sistemas. En lugar de credenciales estáticas, ahora existen mejores formas de comunicar y compartir certificaciones entre otros de manera segura.\n\nSophos Factory cuenta con un sistema de acreditación incorporado para la gestión de claves y admite HashiCorp Vault y plantillas nativas de la nube. También pueden ayudar con servicios de gestión críticos para la nube y empaquetados alrededor de un tubería. No solo hay una variable de acreditación en tiempo de ejecución, sino también un paso de acreditación que solo se evalúa en tiempo de ejecución. Pueden integrarse con estas herramientas de seguridad, por lo que se convierten naturalmente en parte de tu solución de construcción.\n\nSophos Factory está en el espacio de RPA, pero va mucho más allá de ser un simple ejecutor de RPA típico. Técnicamente, es RPA porque, aunque todavía hay intervención humana en la producción, se aprovechan las máquinas para automatizar el proceso. Los clientes buscan formas de escalar y obtener valor de manera segura de la tecnología que adquieren. Sophos Factory se compromete a ayudar al talento técnico a mejorar y darles acceso a conjuntos de herramientas para aprovechar al máximo su potencial, y hacerlo de manera segura.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Mike Fraser","Callen Sapien"],"link":"/episode-EDT88-es","image":"./episodes/edt-88/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren habla con Callen Sapien de Sophos, Director de Gestión de Productos de la Sophos Factory, y Mike Fraser, Vicepresidente de DevSecOps, acerca de su producto que permite una colaboración verdaderamente colaborativa de SecDevOps."},{"id":163,"type":"Episode","title":"El futuro de la gran memoria y CXL.","tags":["bigmemory","memverge","optane","technology","data","compute","pmem","cxl","ceo"],"body":"\r\n\r\nUn emocionante avance en la memoria es CXL (Compute Express Link). Se está desarrollando un robusto ecosistema desde el lado del host, con Intel y otros apoyando CXL 1.1 en sus plataformas de próxima generación, como Sapphire Rapids. Aquellos del lado de la memoria, incluyendo la Intel Optane de cuarta generación, estarán en CXL. Intel es uno de los miembros impulsores de un sólido consorcio CXL que ha estado impulsando el estándar. CLX 2.0 ya ha sido definido, con capacidades adicionales, incluyendo la externalización y conmutación de CXL, y 3.0 está en proceso, lo cual estandarizará cómo se puede aprovisionar y compartir CXL.\n\nHabrá nuevos productos provenientes de Samsung, Micron y SK-Hynix con capacidades de CXL. Además, se están desarrollando nuevas interconexiones que pueden conectar potencialmente la memoria con la tela, lo que permitirá tener una memoria habilitada para la tela que se puede compartir entre múltiples servidores.\n\nLa tecnología CXL es un cambio de juego. Un nuevo protocolo de memoria se ejecuta sobre PCIe generación cinco y posteriores. Ya que te permite colocar memoria en PCIe, no solo puede estar dentro de la caja, sino que en el futuro, con el uso de conmutación PCIe, la memoria en el tejido puede volverse componible y compartible. El primer producto CXL saldrá al mercado para finales de año.\n\nMemVerge ya cuenta con un controlador de memoria definida por software, y CXL abrió un mundo completamente nuevo para el software. CXL es para la memoria lo que el canal de fibra es para el almacenamiento. Es como una red de área de memoria en lugar de una red de almacenamiento. Es posible acceder directamente a la memoria con CXL y omitir por completo a la CPU.\n\nCXL será mucho más rápido que las tecnologías de interconexión anteriores. Tendrá 100 o 200 nanosegundos de latencia. Aquí es donde aumenta la utilización, la gestionabilidad y la agilidad. Habrá mayor disponibilidad y productividad en el uso de memoria. Además, podrás asignar memoria de forma dinámica; puedes asignarla según sea necesario y no necesariamente tiene que ajustarse en la caja del servidor. Teóricamente, siempre tendrás suficiente memoria para lo que necesites estar activo.\n\nCon la tecnología de instantáneas de MemVerge, sus datos están protegidos y persistentes también. Esto se vuelve más importante a medida que su memoria se vuelve más grande. Si la pierde, es más difícil reconstruirla.\n\nPara que CXL despegue, se necesitan tres cosas en su lugar. En el aspecto de hardware, los líderes de hardware más antiguos deben estar de acuerdo y adoptar los mismos estándares. Eso ha sucedido este último año, por lo que hay un estándar único que todos están respaldando. En segundo lugar, no es necesario cambiar su aplicación para usar CXL, al igual que las redes de área de almacenamiento. En tercer lugar, desde el punto de vista de la base de datos, no debería ser necesario reescribir. Esto puede ocurrir mediante la compatibilidad entre los estándares y lo que el sistema operativo y el software de MemVerge admiten, que pueden realizar la gestión automática entre la memoria DDR y la memoria CXL.\n\nMemVerge puede ofrecer esa capa de abstracción. Esencialmente, es una virtualización de memoria. La memoria definida por software maneja la ubicación real de la memoria física.\n\nMemVerge hace que la memoria grande sea transparente para la aplicación, de modo que los programadores pueden aprovechar la capacidad superior y nunca quedarse sin memoria. A continuación, está la protección de datos. MemVerge ha desarrollado un servicio de instantánea en memoria que puede capturar todo el estado de una aplicación en memoria, y ese estado es inmutable. Puede ser recuperado en cualquier momento y en cualquier lugar. Hay muchos casos de uso con esto, como la mediación de ransomware y la reducción de los tiempos de ciclo en la investigación genómica.\n\nLa función de captura instantánea no solo es útil porque puede capturar rápidamente y fácilmente un pipeline en ejecución, lo que le permite volver atrás y recuperarse en cualquier momento, sino que también puede ahorrar dinero al usar servicios en la nube. Los principales proveedores de servicios tienen instancias momentáneas que tienen un descuento del 70-90% sobre el precio de demanda, pero hay un inconveniente: pueden retirarlas en cualquier momento con solo 30 segundos a dos minutos de aviso. Esto no es suficiente tiempo para hacer frente a eso, especialmente si tienes muchos datos en la memoria, por lo que no ha sido útil para muchas cargas de trabajo. Con la capacidad de captura instantánea de MemVerge, puedes tomar instantáneas periódicas de tu carga de trabajo en ejecución en cualquier instancia. Si te quitan la instancia momentánea, tienes una imagen que puedes recuperar y seguir ejecutando. Es un seguro que te permite utilizar el servicio de bajo costo con protección.\n\nDado que estás tomando una instantánea no solo de una aplicación, sino de todo un contenedor o instancia, puedes reinstituirlo en cualquier lugar, en las instalaciones, en la misma nube o en otra nube. Esto te brinda la máxima movilidad y resistencia en tus operaciones, incluso en caso de una interrupción importante del servicio en la nube. Esta tecnología se presta a muchas posibilidades emocionantes.\n\nLa revolución CXL y el software MemVerge Memory Machine son combinaciones potentes para posibilidades transformadoras en el campo de los videojuegos.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Charles Fan"],"link":"/episode-EDT89-es","image":"./episodes/edt-89/es/thumbnail.jpg","lang":"es","summary":"Darren Pulsipher, Arquitecto Jefe de Soluciones de Intel, habla con Charles Fan, CEO de MemVerge, sobre cómo la revolución CXL y el software de MemVerge son el futuro de la memoria grande."},{"id":164,"type":"Episode","title":"Teletrabajo: asegurando tu oficina en casa","tags":["cybersecurity"],"body":"\r\n\r\n## Consejos de seguridad del sistema.\n\nLa primera cosa que quieres hacer para asegurar tu oficina en casa es actualizar todos tus sistemas. Esto incluye tanto las aplicaciones como los sistemas operativos en todos tus dispositivos conectados a internet. También querrás actualizar el software de seguridad de punto final y realizar escaneos regulares cuando tu dispositivo esté encendido. Muchas personas saben hacer esto en sus ordenadores de escritorio o portátiles, pero la seguridad de punto final también debería ser implementada en teléfonos y tabletas. Además, asegúrate de activar tu firewall local y habilitar el firewall de tu enrutador.\n\nUno de los pasos principales que puedes hacer para reducir tu riesgo general mientras estás en internet es reducir la superficie de ataque en tiempo de ejecución. Esto significa que debes cerrar las aplicaciones que no estés utilizando, cerrar el navegador antes de ir a nuevos sitios, y cerrar la sesión o cerrar las sesiones seguras antes de hacer actividades como revisar el correo electrónico o navegar. No debes realizar diferentes actividades simultáneamente para evitar ataques cruzados.\n\nUn buen sitio de educación para aprender cómo navegar de manera segura en Internet es Stop. Think. Connect. https://www.stopthinkconnect.org/\n\n## Consejos de seguridad para modem/router/wifi.\n\nCambia todas las contraseñas predeterminadas por contraseñas seguras (mínimo de 8 a 10 caracteres, utilizando mayúsculas, minúsculas, números y caracteres especiales). Las contraseñas predeterminadas vienen con los enrutadores, módems, portales web de ISP y WiFi. Cuida con precaución quién tiene acceso a tus contraseñas. También es importante cambiar el nombre de red predeterminado (SSID) por algo que no contenga información identificativa.\n\nHabilitar la autenticación de dos factores siempre que sea posible te brindará otra capa de seguridad. Los routers y módems deben actualizarse al igual que tu computadora portátil, así que asegúrate de activar las actualizaciones automáticas.\n\nOtros pasos para aumentar la seguridad incluyen activar WPA y desactivar WPS si es posible. Habilitar la traducción de dirección de red (NAT) y el filtrado de DNS en el enrutador y el módem. También querrás desactivar UPnP.\n\nEstas técnicas evitarán que personas no autorizadas y tus vecinos \"tomen prestado\" tu WiFi, lo cual crea un riesgo de seguridad.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Steve Orrin"],"link":"/episode-EDT9-es","image":"./episodes/edt-9/es/thumbnail.png","lang":"es","summary":"Asegurar tu dispositivo y tu centro de datos solo te lleva hasta cierto punto. Con más personas trabajando desde casa, es necesario ayudar a tus empleados a asegurar su red doméstica y su área de trabajo. En este episodio, Steve Orrin, CTO federal de Intel, ayuda a Darren a asegurar su red doméstica."},{"id":165,"type":"Episode","title":"Barreras para la adopción de tecnología futura.","tags":["cto","organizationalchange","change","people","process"],"body":"\r\n\r\nGlenn es el Director de Tecnología en CACI, una empresa de seis mil millones de dólares con tamaño Fortune 500, enfocada en seguridad nacional. Ha estado en la industria durante 35 años, dedicado a los sectores militar, de inteligencia y ciberseguridad para el gobierno de Estados Unidos. Es ingeniero, por lo que piensa, actúa y procesa preguntas como un ingeniero.\n\nEmocionante nueva tecnología en IA, neuromórfica y seguridad a veces no es adoptada rápidamente en el sector público.\n\nLa primera barrera es la complejidad. Aquellos en la comunidad técnica son innovadores excepcionales, pero no siempre son buenos para hacer que la tecnología sea fácil de consumir, usar o administrar. Sin embargo, Glenn ve cierta mejora. Por ejemplo, en los últimos cinco años, la complejidad de usar la inteligencia artificial ha disminuido significativamente a medida que más personas aprovechan las herramientas y los kits de desarrollo de software.\n\nOtra barrera puede ser la falta de talento para diseñar, implementar y mantener la tecnología en el sector público. El nuevo talento no solo debe ser atraído a la industria, sino que también debe poder obtener autorizaciones. El otro aspecto de la ecuación del talento es que muchos son reacios a adoptar nuevas tecnologías. Es un desafío de gestión del cambio. Si no se aborda correctamente, el talento existente se convierte en anticuerpos para la adopción de nuevas tecnologías. En lugar de preguntar cómo llegar a un \"sí\", encuentran mil razones por las cuales no.\n\nEl tercero es ciberseguridad. Glenn no describe esto como una barrera porque no es algo que se pueda eliminar. Sin embargo, la industria podría ser más proactiva al mover las decisiones más hacia la izquierda e involucrar la seguridad más temprano para acelerar la adopción.\n\nEl sector público podría aprender de cómo la tecnología de los smartphones fue adoptada tan rápidamente con la llegada del iPhone. Los primeros iPhones eran increíblemente fáciles de usar y la compañía se comprometió a proveer a los clientes algo que pudieran utilizar inmediatamente. Además, la comunidad de desarrollo podía crear e innovar instantáneamente con los kits de desarrollo de software y los procesos. Los niños de sexto grado pueden desarrollar con los kits para iPhone y Android. La usabilidad era vital y permitió un desarrollo de software eficiente.\n\nEl sector público debería tratar de imitar esas cualidades y acercarse a su mercado y clientes de la misma manera, especialmente en términos de análisis. Hay una cantidad tremenda de datos, y no estamos sacándoles mucho provecho. Al igual que Steve Jobs sacó su tecnología fuera del público objetivo medio, lo mismo debería suceder en el ámbito del análisis de datos. En lugar de dirigirse a científicos de datos, tal vez el objetivo debería ser una persona de negocios o una persona promedio que tenga un hogar que administrar.\n\nGlenn cree que el primer paso para pensar de esta manera sobre IA y análisis es comprometerse ferozmente a liberar los datos. Tanta información está atrapada en diseños propietarios que es un pésimo modelo de negocio. Los clientes deberían renovar contratos no porque su información esté controlada y limitada por un proveedor, sino porque el proveedor está ofreciendo soluciones, valor e innovación.\n\nLa IA debería ser presentada como un asistente digital para los clientes en lugar de un robot misterioso y mágico en segundo plano que hace que el cliente se sienta nervioso al confiar en él. Un asistente digital simplemente está un paso adelante de lo que estás intentando lograr al proporcionar datos e información a través del aprendizaje automático y el aprendizaje profundo, lo cual facilita tu vida y te permite procesar la información. De esta manera, la IA agrega valor instantáneo y es mucho menos intimidante.\n\nGlenn cree que la tecnología que el sector público debe tener hoy en día es, en primer lugar, la nube y más de ella. La nube elimina muchos errores humanos en la administración. Reduce la superficie de ataque y es un modelo de consumo de pago por uso, lo cual puede ser económico si desarrollas software para ese modelo.\n\nEn segundo lugar, hay buenas plataformas de DevSecOps. Los programadores de software pueden pasar más tiempo en el desarrollo real con las herramientas disponibles. DevSecOps todavía está en sus primeras etapas, y el crecimiento será explosivo.\n\nUna tecnología clave durante el trabajo remoto durante COVID es Commercial Solutions for Classified (CSFC). Esta tecnología ha estado presente durante una década, pero ha madurado hasta el punto en que se ha eliminado la complejidad de su implementación, administración y seguridad para llevarla fácilmente al límite. Alguien puede trabajar en un dominio no clasificado y aún así acceder a secretos, como la confianza de que no habrá ninguna filtración. Esta es una nueva y importante capacidad.\n\nTodo esto se vincula con atraer y retener talento. La dificultad radica no en que el sector público no tenga problemas emocionantes y desafiantes, sino en la percepción de que el gobierno se mueve muy lentamente. Hay algo de verdad en eso debido a la importancia de mantener una cierta santidad o confianza. Sin embargo, en realidad, el gobierno está justo en la vanguardia de muchas nuevas tecnologías como la fotónica o la comunicación basada en la luz. Algunos grupos se mueven rápidamente, como en el desarrollo de software con agile y DevSecOps.\n\nGlenn cree que el dominio espacial explotará y será fundamentalmente diferente en cinco años para el futuro de la tecnología. La capacidad de poner más cosas en órbita de manera económica con mejoras en tamaño, peso y potencia, junto con la capacidad de comunicarse con la fotónica a lo largo de miles de kilómetros rápidamente, permite la conectividad y la capacidad de distribuir y utilizar datos en esos cargamentos.\n\nOtra área que será muy diferente es la agilidad espectral. El dominio del espectro electromagnético será crucial para mantener una conectividad resistente en conflictos. Las disputas subsiguientes enfatizarán en gran medida quién puede comunicarse e interferir en las comunicaciones. La agilidad espectral es la capacidad de conocer de manera dinámica lo que está sucediendo a tu alrededor en el espectro para poder moverte rápidamente. Esta conectividad es fundamental para el comando conjunto y control y la visión del JADC2 de trabajar.\n\nUn tercer área es lo que se puede hacer con Kubernetes y con el código de infraestructura. La automatización eliminará el trabajo laborioso y facilitará las cosas en general.\n\nNo puede haber una conversación sobre el futuro de la tecnología sin mencionar a la Computación Cuántica. Los modelos de programación cuántica que se están construyendo son completamente diferentes de los modelos tradicionales. Necesitamos llegar a un punto en el que tengamos herramientas para programadores que hagan el proceso mucho más automatizado. Entrenar a alguien durante dos años para ser programador de computación cuántica no es escalable, por lo que necesitamos que se desarrollen herramientas de desarrollo de software, al igual que la comunidad ha abstraído la complejidad y ha desarrollado herramientas para la inteligencia artificial.\n\nLa última área es la evolución continua de la informática de borde. El número de procesos de computación es fenomenal, fomentando la creatividad con el tamaño, peso y potencia. La informática de borde seguirá transformándose para ser segura y confiable. La comunicación resiliente puede no ser una conexión dedicada, sino una red de malla donde las partes del mensaje se vuelven a combinar en el otro extremo. Esto puede proporcionar soluciones en entornos tácticos y de denegación y trastorno.\n\nDarren ve cómo la arquitectura tradicional de Von Neumann va menguando en el futuro, ya que podemos tener persistencia de datos sin unidades de disco y podemos tener datos que viven, se mueven y migran con funciones que trabajan con ellos. Las capas y limitaciones del modelo de Von Neumann serán eliminadas.\n\nGlenn piensa que esto sucederá más rápido de lo que las personas generalmente pronosticarían debido al sesgo de experiencia. Pero es un mundo diferente cuando puedes obtener todas las demás piezas para coexistir y eliminar los problemas de latencia. Imagina lo que se podría hacer a la velocidad de inferencia en el edge, por ejemplo, con vehículos autónomos. Esa tecnología está sucediendo, y los programadores consideran las capas de abstracción desde el principio. Entonces podrías, por ejemplo, tomar un modelo de red neuronal que ya está desarrollado y ejecutarlo a través de un conjunto de herramientas de desarrollo de software para colocarlo en un sustrato de hardware, una arquitectura no-Von Neumann, y no tienes que reprogramar. Esto acelerará la adopción y será transformador.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Glenn Kurowski"],"link":"/episode-EDT90-es","image":"./episodes/edt-90/es/thumbnail.jpg","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, analiza las barreras para adoptar nuevas tecnologías en el sector público y lo que se avecina en el futuro junto a Glenn Kurowski, CTO de CACI."},{"id":166,"type":"Episode","title":"Luchando contra el Adversario Cibernético asegurando tu Cadena de Suministro de Software.","tags":["cybersecurity","devsecops","sbom","securesupplychain","policy","process","compute"],"body":"\r\n\r\nDarren y Eric Greenwald, Asesor Jurídico General de Finite State, discuten la seguridad de la cadena de suministro de software en este episodio.\n\nFinite State se enfoca en encontrar vulnerabilidades en el firmware, más comúnmente en software de terceros que podrían ya haber tenido vulnerabilidades existentes, antes de incorporarlo en sus dispositivos. Finite State se especializa principalmente en dispositivos de Internet de las cosas industriales, dispositivos médicos, automóviles y productos electrónicos de consumo.\n\nDurante siete años antes de unirse a Finite State, Eric trabajó como abogado en el sector privado, enfocándose en pruebas de seguridad e identificación de vulnerabilidades para empresas de ciberseguridad. Antes de eso, trabajó en el gobierno en áreas de ciberseguridad y seguridad nacional, para el FBI y la CIA, y como consejero principal del Comité de Inteligencia de la Cámara de Representantes. Su trabajo en el gobierno culminó cuando se convirtió en el Director Principal de Ciberseguridad en el Consejo de Seguridad Nacional de la Casa Blanca.\n\nEric cree que parte de la razón por la cual ahora hay un énfasis en asegurar la cadena de suministro es que la amenaza ha evolucionado. La naturaleza cada vez más compleja del software, incluyendo que muchos componentes tienen vulnerabilidades cuando son creados por primera vez y solo se descubren con el tiempo, hace más difícil encontrar dispositivos construidos a través de la cadena de suministro de software. Además, recientes ataques de alto perfil a través de una violación en la cadena de suministro de software, como el caso de SolarWinds, han hecho que las personas estén más conscientes del peligro.\n\nEn el ataque de SolarWinds, el perpetrador tuvo paciencia, no causando inmediatamente caos sino esperando un año mientras se filtraba a través de redes informáticas estadounidenses, cultivando acceso e información. Esa paciencia es probablemente la diferencia más significativa entre un ataque estatal y uno criminal. A veces, los ataques criminales pacientes están más enfocados en obtener un retorno financiero de inversión, mientras que los detrás de un ataque estatal están dispuestos a pasar años desarrollando su acceso a la inteligencia. Sin embargo, en ambos casos, los ataques se están volviendo más sofisticados y están mucho mejor posicionados para aprovechar ventajas devastadoras de la complejidad de la cadena de suministro.\n\nPara combatir estos ataques, están surgiendo nuevas regulaciones. La administración de Biden emitió la orden ejecutiva 14028 en mayo de 2021, que tiene dos puntos principales: desarrollo seguro de software y lista de materiales de software (SBOM, por sus siglas en inglés). La orden ejecutiva solo se aplica a la adquisición gubernamental, pero es probable que la industria privada la siga.\n\nLos detalles y recomendaciones técnicas para el desarrollo seguro de software aún están siendo trabajados. Sin embargo, parte de ello sería que los proveedores de software al gobierno tendrían que proporcionar un SBOM. El primer borrador de legislación para SBOMS salió en 2014, por lo que los estándares para producirlos se han vuelto más maduros y desarrollados. Un SBOM es esencialmente una lista de componentes de software que se utilizaron en un producto de software, no muy diferente a una lista de ingredientes en un producto alimenticio. Esto ofrece transparencia en la cadena de suministro, que es esencial para evaluar vulnerabilidades o para poder identificar una vulnerabilidad que se descubra en una fecha posterior.\n\nUn excelente ejemplo de esto es lo que sucedió con Log4j. Cuando se descubrió esa vulnerabilidad, muchas empresas no tenían idea si la tenían en su sistema. No sería una varita mágica, pero un SBOM permitiría a las empresas descubrir más fácilmente si tienen el componente de software problemático en su sistema y actuar más rápidamente para implementar un parche.\n\nLos argumentos en contra de publicar SBOMs son que proporcionarán una guía para los atacantes y revelarán información propietaria. Si bien estas son preocupaciones legítimas que deben ser discutidas, el Departamento de Comercio y el Departamento de Seguridad Nacional mantienen un beneficio mucho más significativo para los defensores al tener transparencia que cualquier ventaja otorgada a los atacantes. Existen proyectos de ley bipartidistas que apoyan a los SBOMs. Hay formas de reducir el riesgo de que los SBOMs caigan en manos equivocadas, como contratos seguros o no fungibles. Los debates sobre estas preocupaciones continuarán en el sector público, y más empresas los adoptarán.\n\nEsta legislación está ocurriendo porque la FDA ha sugerido que los fabricantes de dispositivos médicos incorporen los SBOM como parte del proceso de revisión, por lo que los SBOM están ganando impulso entre estos fabricantes. El mundo físico se ve cada vez más afectado por el software en los dispositivos médicos y sistemas integrados, como los sistemas de control de plantas de energía, sistemas de climatización, controles de aeropuertos, etc., por lo que los sistemas operativos están en riesgo, con consecuencias más significativas que los ataques al sistema empresarial.\n\nUna dificultad para los profesionales de OT es que muchos de los componentes industriales son más antiguos y no han sido actualizados necesariamente. Aun así, los hackers son reacios a conectarse a internet y actualizar porque así es como los hackers ingresan. La mejor respuesta a este problema es intentar obtener transparencia de los componentes en la pila, escanear el sistema y los dispositivos que forman parte de la red OT, y realizar ingeniería inversa y descompilación para comprender los detalles. Básicamente, necesitas crear tu SBOM y evaluar dónde se encuentran las vulnerabilidades.\n\nEsta es el área principal de trabajo para Finite State. Ellos analizan los sistemas y dispositivos. Tienen una plataforma que automatiza el análisis del código incrustado, proporciona una lista de las vulnerabilidades e identifica y agrupa las vulnerabilidades de mayor prioridad. A veces se puede eliminar toda una categoría de vulnerabilidad con una sola corrección. Crear el SBOM por sí mismo, entonces, no es suficiente. Debe estar vinculado a un sistema de gestión de riesgos para analizar y clasificar las numerosas vulnerabilidades. Encontrar los riesgos de mayor prioridad es un proceso complejo y Finite State puede ayudar a los equipos de seguridad a priorizar sus acciones para proteger sus sistemas.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Eric Greenwald"],"link":"/episode-EDT91-es","image":"./episodes/edt-91/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, y Eric Greenwald, Consejero General de Finite State, hablan sobre asegurar la cadena de suministro de software."},{"id":167,"type":"Episode","title":"Protegiendo las paredes exteriores de tu castillo con escaneos de vulnerabilidad.","tags":["cybersecurity","endpointmanagement","vulnerability","cyberattack","iot","edge","censys"],"body":"\r\n\r\nMatt pasó ocho años como oficial de inteligencia en el ejército. En la última parte de su servicio, ayudó a establecer las fuerzas de misión cibernética como parte de US Cyber Command. Después de terminar su servicio, trabajó con Army Cyber y algunas startups antes de venir a Censys.\n\nUna evaluación típica de vulnerabilidades requiere que una organización proporcione lo que le interesa, por ejemplo una lista de bloqueo de direcciones IP para un escaneo de vulnerabilidad de credenciales. Luego, esas vulnerabilidades se descubren y se pueden corregir. Un escaneo de Censys puede proporcionar un escaneo amplio y profundo en Internet a partir de información básica como un dominio y algunas direcciones IP, descubriendo todo lo demás que la organización posee y monitoreando de manera continua esas mismas agregaciones y correlaciones entre los conjuntos de datos. Si, por ejemplo, el departamento de marketing lanza un sitio web expuesto que no tiene TLS, Censys lo detectará. Censys adopta una perspectiva externa para saber qué está disponible para un atacante en cualquier momento del día.\n\nIncluso si, por ejemplo, alguien despliega una aplicación en la nube, no la coloca en el dominio y utiliza seguridad de ofuscación, Censys aún la detectará si está en un entorno en la nube propiedad de la organización. Los conectores en la nube descubrirán nuevas instancias en la nube que surjan. Algo de tejido conectivo, ya sea información de WHOIS o DNS, debe correlacionarse con algunas de esas instancias. Censys sigue mejorando en la detección de estos tipos de instancias.\n\nLa analogía de un castillo funciona bien aquí. Una organización no solo quiere confiar en lo que pueden ver internamente con las cámaras y centinelas. Desean una patrulla de seguridad móvil para capturar las amenazas antes de que lleguen siquiera a las paredes del castillo. La patrulla puede observar el castillo de la misma manera que lo haría un atacante. Censys patrulla Internet desde una perspectiva externa, viendo las cosas de la misma forma que un posible hacker las vería.\n\nUna herramienta que Censys está investigando es JARM de Salesforce, una herramienta de huellas dactilares TLS activa. Cuando se implementan elementos que no coinciden con la huella dactilar en un servidor específico, se destacarán como anomalías. Es esencial buscar arquitecturas que deberían estar en una configuración particular pero no lo están.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Matt Lembright"],"link":"/episode-EDT92-es","image":"./episodes/edt-92/es/thumbnail.png","lang":"es","summary":"En este episodio, Darren habla con Matt Lembright, Director de Aplicaciones Federales, Censys, sobre cómo Censys evalúa la superficie de ataque de los dispositivos conectados a Internet, permitiendo a las organizaciones eliminar vulnerabilidades."},{"id":168,"type":"Episode","title":"Las Cuatro Superpotencias de la Transformación Digital","tags":["cybersecurity","aiml","multicloud","comms"],"body":"\r\n\r\nIntel se centra en la informática ubicua, la conectividad omnipresente, la nube al borde y la inteligencia artificial (IA). Los elementos de estas cuatro capacidades son obligatorios en las estrategias de transformación digital para empresas de todo el mundo, por lo que existe una gran sinergia.\n\n## Computación omnipresente\n\nLa computación ubicua es un concepto de ingeniería de software donde la informática está en todas partes. No solo se refiere a dispositivos personales inteligentes como teléfonos, relojes y electrodomésticos, o infraestructura de centros de datos y aplicaciones en la nube, sino que es la idea de que la infraestructura empresarial que históricamente estuvo detrás de un cortafuegos ahora está entrando en la vida cotidiana. La infraestructura empresarial está disponible para los empleados en el campus y en cualquier lugar donde tengan un dispositivo. Aún más importante, la infraestructura puede extenderse a los clientes para mejorar su experiencia.\n\nIntel está investigando qué elementos fundamentales deben ser impulsados a través del ecosistema para lograr el objetivo de que cada persona en la Tierra tenga acceso a un petabyte de datos o a un petaflop de computación en menos de un milisegundo de acceso. Intel está trabajando en las leyes de la física, la economía y la soberanía de los datos para hacer eso posible. Esto proporcionará a las empresas de software, las empresas de la nube y todas las empresas en todo el mundo un marco de software que creará valor para sus clientes.\n\n## Conectividad omnipresente\n\nEn una entrevista reciente, Glenn Kurowski, el CTO de CACI, habló sobre la conectividad y cómo la ve expandiéndose al ámbito espacial. Un gran ejemplo es cuando Elon Musk voló Starlink sobre Ucrania para evitar que cayeran en la oscuridad de internet.\n\nCon esta tremenda potencia informática, la conectividad es crucial. Existe la conectividad satelital, el 5G y el 4G. Sin embargo, incluso dentro de Internet, entre estados y países, la pregunta es, ¿cómo se atraviesan las redes y se habilitan grupos de computación que no están interconectados? Sin conectividad, el valor de la informática disminuye rápidamente.\n\nPara proteger estos datos interconectados, Intel y equipos de cientos de empresas tecnológicas están colaborando e innovando juntos. Los gobiernos también deben comprender las leyes, reglas y preocupaciones. Intel, por ejemplo, emplea una fuerza considerable para ayudar a los legisladores a tener esas conversaciones y crear leyes que protejan los datos. El ecosistema de Intel es vasto y la gente presta atención cuando Intel dice que esta seguridad es esencial.\n\nEstonia es un buen modelo de país que protege los datos de sus ciudadanos, pero también los libera y proporciona más servicios a un menor costo. Habrá una evolución natural hacia este modelo, a medida que los gobiernos protejan activamente la privacidad y piensen estratégicamente.\n\n## Nube a Borde\n\nEn el lado empresarial se encuentra la extensión de la superficie mediante la cual las empresas pueden interactuar con sus clientes a través de la nube hasta el borde. Un gran ejemplo es el comercio omnicanal, donde una empresa conoce quién es el cliente, sus tendencias y lo que necesita. Pueden recomendar servicios adicionales con una conectividad omnipresente desde la nube hasta el borde. Podrían decirle al cliente dónde se encuentra algo que están buscando cuando lleguen a la tienda. Los minoristas podrían extender sus redes desde un centro de datos en la nube o un centro de datos local para unificar en el borde, creando una red en malla que recorre toda la tienda.\n\nNo solo mejoraría la experiencia del comprador, sino que las tiendas podrían disminuir las pérdidas al detectar los productos que están agotados y controlar el flujo dentro de la tienda. Podrían colocar sus productos en los lugares más ventajosos y supervisar, por ejemplo, los productos perecederos para tomar medidas y mover los productos rápidamente cuando estén en su punto máximo.\n\n## Inteligencia Artificial\n\nLa IA es una extensión del análisis de datos y crecerá inevitablemente. Se crean grandes cantidades de datos a diario y ya está más allá de las capacidades de las empresas para procesarlos de manera efectiva. Las personas solo están mirando menos del cinco por ciento de los datos generados.\n\nCon algoritmos de inteligencia artificial, es posible encontrar patrones con esos datos para, por ejemplo, curar el cáncer. Puede que esté ahí todo el tiempo porque la información aún no se encuentra en un lugar donde la inteligencia artificial pueda utilizarla. Habrá que crear un nuevo mercado que rodee conjuntos de datos centralizados y accesibles. Una agencia de datos organizada podría poner a disposición datos centralizados a varias empresas a través de la nube, mientras protege la privacidad de los datos, como la identificación de pacientes.\n\n## Seguridad de Datos\n\nCOVID hizo que la computación ubicua fuera importante rápidamente para los empleados que trabajan desde casa. Eso, junto con la posterior fuerza laboral híbrida, expuso fallos de seguridad en la industria. Actualmente, hay un aumento significativo en la financiación de la seguridad para abordar los problemas y mantenerse al día con la expansión de los gigantes tecnológicos. Especialmente con las arquitecturas de borde a nube y la computación ubicua, la superficie de ataque se ha ampliado enormemente. La industria puede mantenerse al día, pero requiere un esfuerzo tremendo y una mentalidad orientada al futuro.\n\nIntel tiene importantes innovaciones en este campo con extensiones de guardia de software, características de seguridad en el silicio y un ecosistema para aprovechar estas cosas. El ecosistema puede construir nuevos casos de uso como análisis distribuido confidencial para la investigación del cáncer o análisis multi-dominio, que abarcan datos no clasificados, clasificados y de alto secreto. Antes, esos datos nunca podían mezclarse. Ahora pueden mezclarse de manera segura y resolver problemas que antes no podíamos resolver.\n\n## Intel Software would be translated to \"Software de Intel\" in Spanish.\n\nMuchos podrían sorprenderse de que Intel tenga más de 19,000 ingenieros de software. Intel puede mantener a todos estos ingenieros completamente ocupados, ya que trabajan en tres niveles: software fundamental; lenguajes, marcos, herramientas y bibliotecas; y trabajo a nivel de aplicaciones.\n\nLa mayor parte de esta tecnología a nivel de aplicación se comparte en la comunidad de código abierto, donde es accesible, seguro y optimizado.\n\nVe a http://embracingdigital.org para encontrar recursos de Intel relacionados con las cuatro superpotencias.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Ernst"],"link":"/episode-EDT93-es","image":"./episodes/edt-93/es/thumbnail.jpg","lang":"es","summary":"En este episodio, Darren discute las cuatro superpotencias de la transformación digital con Greg Ernst de Intel, Vicepresidente Corporativo de Ventas para las Américas. Computación omnipresente, Conectividad pervasiva, Nube al borde, Inteligencia artificial."},{"id":169,"type":"Episode","title":"El Renacimiento de la Nube Privada","tags":["hybridcloud","microcloud","privatecloud","compute","technology","cloud","vergeio"],"body":"\r\n\r\nGreg comenzó a programar en el Commodore 64 cuando era niño y nunca dejó de hacerlo. Ha trabajado en todo, desde motores de juegos en 3D hasta comunicaciones, motores de bases de datos, servidores web y encriptación. Desarrolló soluciones de software y hardware de comunicación interoperables para los primeros intervinientes después de que surgieran problemas durante el 9/11. Por ejemplo, la policía no podía comunicarse con el departamento de bomberos debido a que todos estaban en sistemas diferentes.\n\nDespués de una exitosa salida de esa empresa, él comenzó una nueva empresa para construir un motor de búsqueda vertical desde cero. Rápidamente se dio cuenta de que la parte difícil no era el motor de búsqueda o los algoritmos, sino los problemas de infraestructura. Descubrió que gran parte del código era específico del hardware y faltaba una capa de abstracción. Esto fue la base de lo que se convirtió en Yottabyte, su empresa que comenzó resolviendo el problema de almacenamiento juntando unidades económicas y presentándolo como una plataforma simplificada para la cual se podía escribir software sin tener que preocuparse por lo que estaba debajo del capó. Su visión luego se expandió para incluir computación, memoria y redes. Yottabyte recientemente se convirtió en Verge.io.\n\nEl éxito de Verge.io radica en que todo fue desarrollado desde cero, incluso el diseño de su lenguaje de programación y la escritura de sus motores de bases de datos, para que el software hiciera todo. No está ligado a ninguna pieza específica de hardware. El objetivo fue la simplicidad: no es solo un montón de piezas cosidas juntas para formar una nube privada singular, sino un centro de datos completamente virtualizado.\n\nEsto brinda confiabilidad y seguridad, ya que cuenta, por ejemplo, con actualizaciones sin interrupciones, sin dependencia de hardware específico y menos puntos de entrada vulnerables. Además, los usuarios objetivo del sistema son generalistas en TI, no expertos en SAN ni programadores.\n\nLa plataforma Verge.io brinda la misma experiencia que un cliente podría obtener de una nube pública: simplicidad, autoservicio y agilidad, pero tiene un costo considerable y un beneficio de gravedad de datos. Estás pagando por cosas como IOP o salida de datos con una nube pública. Muchos pequeños precios comienzan a sumarse y puedes volverte responsable del ecosistema. Por otro lado, puedes ejecutar el software Verge.io, por ejemplo, en servidores bare metal para ahorrar costos y tener la capacidad de escalar hacia arriba o hacia abajo rápidamente. En una nube pública, también renuncias a mucho control. Con una nube privada, mantienes tus datos más cerca de donde se generan y puedes operar de manera eficiente en el borde.\n\nParte de la razón por la que los proveedores de servicios eligen Verge.io es que pueden gestionar las cargas de trabajo de sus clientes sin sacrificar el margen de costos y reducir las complicaciones de trasladarlos a la nube pública.\n\nLas necesidades de otros clientes son satisfechas porque pueden lograr cosas que son muy difíciles de hacer con otro software. Por ejemplo, la Universidad de Michigan ha encontrado una eficiencia y facilidad de uso vastamente mejoradas. Tienen miles de investigadores y, cuando obtienen fondos de subvención, necesitan un entorno que cumpla con los requisitos de HIPAA o CUI. Antes, cada solicitud requería seis a nueve meses para implementar, instalar y certificar el hardware. Verge.io ha construido un entorno, y ponerlo en funcionamiento es tan simple como presionar un botón. Al usuario se le proporciona un enclave virtual que cumple totalmente con los requisitos y es muy seguro porque el enclave está encapsulado y aislado.\n\nOtro beneficio es que la función de instantáneas de Verge.io incluye continuidad empresarial y recuperación ante desastres incorporadas. Puedes tomar la instantánea y trasladarla a una arquitectura de hardware completamente diferente, y seguirá funcionando igual. La forma en que funcionan las instantáneas permite realizar una copia clonada, incluso si tiene diez petabytes, en menos de 30 milisegundos.\n\nDebido a que el centro de datos ahora puede migrar fácilmente a la nube o a un centro de colocación, los propietarios de negocios tienen mucha más flexibilidad para negociar el precio y el rendimiento del hardware. Además, no hay tiempo de inactividad para las actualizaciones o ciclos de renovación del hardware. El sistema nunca se apaga.\n\nUna de las áreas en las que Verge.io está expandiendo es la construcción de software de agregación multi-nube con beneficios como un panel de gestión centralizado.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Greg Campbell"],"link":"/episode-EDT94-es","image":"./episodes/edt-94/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, habla sobre los centros de datos definidos por software de Verge.io que simplifican la tecnología de la información y hacen que la nube privada sea fácil y eficiente junto con el CTO de Verge.io, Greg Campbell."},{"id":170,"type":"Episode","title":"Requisitos para arquitecturas de borde a nube","tags":["data","edge","compute","cybersecurity"],"body":"\r\n\r\nAnna recientemente lideró un esfuerzo para escribir un documento técnico sobre las arquitecturas de servicios de borde a nube. Borde a nube engloba todos los aspectos importantes de la tecnología en los que Intel se preocupa: IA, borde, nube y conectividad de red. El propósito era reunir a expertos en estas áreas para discutir cómo Intel aborda las arquitecturas de borde a nube y cómo estas arquitecturas se pueden conectar de vuelta a la nube. El enfoque estaba en los elementos que importan en lugar de la tecnología actual que aborda los problemas. Esto proporcionaría un marco para hablar y buscar las desconexiones. Una revelación interesante fue que los profesionales de TI, las tecnologías operativas y de red hablan diferentes lenguajes técnicos con diferentes taxonomías, entre otros desafíos.\n\nUna lección de estas discusiones fue que las comunicaciones son hiperfundamentales y se debe tener un conocimiento sólido de tus comunicaciones, especialmente en el borde. Esto dicta cuánta computación debe ser local y con qué frecuencia puedes confiar en la nube. En el sector público, la complejidad adicional de los casos de uso debe mantener la funcionalidad cuando existen condiciones de negación, interrupción, intermitencia y limitación (DDIL por sus siglas en inglés). Necesitamos replantear estas arquitecturas en la conectividad en la nube basándonos en esas limitaciones.\n\nComunicaciones y conectividad son la diferencia más significativa entre una arquitectura en la nube, una infraestructura de servicio y el borde. Muchas herramientas actuales asumen incorrectamente una conectividad constante; si algo no está conectado, está muerto. Ese no es el caso en el borde. Es evidente en organizaciones del borde como el ejército. Aún así, incluso en industrias como la telemedicina, tienes que asumir que no habrá una buena conectividad, por ejemplo, en un escenario de uso de telemedicina donde podrías depender del WiFi del hogar del paciente. El espacio industrial tiene requisitos similares. Algunos no pueden estar sin conexión debido a controles críticos para maquinaria o procesos específicos.\n\nEntonces, ¿cómo puedes tener cálculo centrado en el borde que mantenga toda la funcionalidad crítica con conectividad de regreso a la nube de manera intermitente en esencia? Existen arquitecturas para esto, pero aún queda mucho por hacer para tener operaciones sin inconvenientes cuando la conectividad podría no ser continua. ¿Qué se puede seguir haciendo y qué sucede cuando todo se restaura? Habrá una desconexión con lo que ha estado sucediendo con los datos. Se vuelve complejo cuando tienes que sincronizar todos estos datos a gran escala, tal vez con miles de dispositivos en el borde.\n\n## Seguridad\n\nLa seguridad en el borde es otra área en la que siempre hay más trabajo por hacer. Medidas de seguridad tradicionales como la autenticación siguen siendo de vital importancia, pero los dispositivos representan una gran superficie de ataque y su seguridad física es un problema distinto. La seguridad de las laptops tiende a ser sólida y esas medidas deben aplicarse también a otros dispositivos del borde. Los nuevos avances de la inteligencia artificial ayudarán a determinar si los dispositivos se encuentran en la ubicación correcta y detectar anomalías en, por ejemplo, diez mil dispositivos.\n\n## Desarrollo de aplicaciones\n\nEn el desarrollo de aplicaciones, los desarrolladores deben entender el entorno único en el borde y desarrollarlos sin necesidad de reprogramar o incorporar nuevos middleware para ejecutarse en el borde. Las aplicaciones deben ser capaces de ejecutarse con limitaciones de computación, energía y conectividad. Además, el borde puede estar en una nueva configuración de cómputo distribuido y la aplicación debe ser diseñada para funcionar en una red de malla con cómputo altamente distribuido. Desacoplar la aplicación del hardware es un cambio significativo y se está volviendo más genérico y menos específico. Sin embargo, todo el diálogo se está moviendo hacia la obtención de datos desde cualquier lugar y su uso en cualquier lugar.\n\n## Gestión de datos\n\nEl volumen de datos generado y recopilado en el borde es tan grande que no tiene sentido enviar todos esos datos a un centro de datos para que sean procesados. Una de las razones es el costo. En los Estados Unidos, una red privada 5G puede ser rentable para estas enormes cantidades de datos, pero el costo sería prohibitivo para la mayoría de las organizaciones sin una red privada 5G.\n\nEl otro problema es que la mayoría de los datos no son útiles. Por ejemplo, al monitorear dispositivos o aplicaciones, la mayoría de los datos indican que todo está funcionando correctamente cuando solo te importan los eventos que sugieren que las cosas no están bien. Los algoritmos de IA se aplican en el borde, disminuyendo la cantidad de datos irrelevantes enviados de vuelta a la nube para su procesamiento.\n\nEl modo de operación tradicional para la gestión de datos, copiar todo al centro de datos y ejecutar análisis allí, no funciona bien para el edge. Empujar aplicaciones hacia el edge tampoco siempre funciona. Intel ha identificado algunas otras arquitecturas de datos o operaciones de datos. Una se llama intercambio de datos, donde hay una combinación de mover datos en enclaves seguros solo después de haber sido analizados en el edge, como el procesamiento en lotes. La otra se llama flujos de datos inteligentes, donde SADE y SABR entran en juego. Los datos solo se mueven basados en reglas y se transmiten. Funciona en entornos DDIL porque puede determinar los entornos operativos actuales y ajustarse.\n\n## Gestión\n\nLos sistemas deben ser diseñados de tal manera que puedan ser mantenidos. No puedes desplegar diez mil dispositivos y luego enviar regularmente a un pequeño ejército de personas para verificarlos. Tradicionalmente, la tecnología de la información ha sido cautelosa al no querer actualizar un sistema que funcione. Sin embargo, no tiene sentido dejar los sistemas solos, especialmente con el temor de ataques de ransomware en las redes OT. Los sistemas deben ser arquitectónicamente diseñados de manera que permitan mantener todo fácilmente actualizado y tengan la capacidad de hacer frente al entorno de seguridad.\n\n## Disponibilidad\n\nEspecialmente en campos críticos como el militar o la atención médica, es importante diseñar sistemas con suficiente redundancia; es más bien un enfoque de sistemas. Si los componentes individuales fallan, aún se debe cumplir el objetivo final. Eso es muy diferente a lo que sucede en la nube, donde se trata de mantener la infraestructura en funcionamiento.\n\nLa tecnología aún no está del todo desarrollada, pero está en el radar diseñar para múltiples redes. Si, por ejemplo, utilizas preferentemente el WiFi 6 y este falla, el sistema puede usar 4G u otra red disponible. Tanto la red como el procesamiento deben ser sólidos. Una operación independiente sin conexión a red es frágil. Si optas por las conexiones cableadas, obtienes limitaciones más altas pero pierdes tus aplicaciones móviles.\n\nPuede encontrar el documento blanco \"Requisitos esenciales para las arquitecturas de servicios de borde a nube\" para obtener más información en embracingdigital.org o intel.org.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Anna Scott"],"link":"/episode-EDT95-es","image":"./episodes/edt-95/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, y la Dra. Anna Scott, Arquitecta Principal de Edge, Sector Público, discuten los requisitos esenciales para las arquitecturas de servicios desde el borde hasta la nube."},{"id":171,"type":"Episode","title":"Aprovechando los Centros de Datos Virtuales con Verge.io.","tags":["privatecloud","cloud","compute","technology","sdi","virtualdatacenter","vergeio"],"body":"\r\n\r\nAaron ha estado en el área de TI durante más de 20 años. Es un ingeniero preventa, lo que significa que habla con los clientes sobre el valor y características de Verge.io y les ayuda a definir sus requisitos.\n\nChris es el jefe de ventas en Verge.io con 25 años de experiencia en empresas de software tecnológico. Él describe a Verge.io como una empresa con un enfoque maniático en la satisfacción y éxito del cliente con el software.\n\nEl término \"centro de datos virtual\" es para simplificar. La plataforma es multiinquilino, por lo que hay inquilinos anidados independientes con todos los recursos requeridos dentro de un centro de datos, desde CPU hasta memoria, almacenamiento y redes, y están aislados. Aun así, al mismo tiempo, puedes construirlos bajo demanda. Al igual que puedes crear una VM bajo demanda, a menudo con una plantilla, puedes hacer lo mismo con un inquilino en el que encapsula todo en el centro de datos virtual. Puedes construirlo a través de un motor de recetas o desde cero. Puedes clonar fácilmente o tomar una instantánea de uno de esos inquilinos.\n\nEstos centros de datos virtuales pueden ser grandes y complejos. El cliente más grande de Verge.io utiliza más de 60 nodos y múltiples inquilinos en su entorno. Las únicas restricciones son las que están incorporadas en el clúster de backend de Verge.io. Por ejemplo, supongamos que tienes cuatro terabytes de memoria, 100 terabytes de almacenamiento y 64 núcleos en tu CPU. Podrías asignar todos esos recursos a ese inquilino, dividirlo como desees, en la mitad o en tercios, y construir los inquilinos de esa manera.\n\nSi, por ejemplo, asignas algunos nodos al desarrollo, algunos a la prueba y el resto a la producción, serían inquilinos diferentes, y puedes hacer instantáneas entre ellos y pasarlos a producción. En este caso, incluso puedes tener varios entornos de producción con actualizaciones en azul-verde.\n\nMuchos de los clientes de Verge.io son proveedores de servicios gestionados. Utilizan multi-tenencia para sus clientes finales y, con eso, dedican entornos seguros de confianza cero. Los clientes pueden tener su entorno en la nube y pueden aprovisionar cargas de trabajo virtuales según sea necesario.\n\nTambién hay casos de uso significativos para clientes de TI corporativos que desean, por ejemplo, entornos de desarrollo y prueba azul-verde o donde pueden tener diferentes entornos con diferentes requisitos de cumplimiento de seguridad como SOX o HIPAA. Verge.io tiene varios clientes grandes de instituciones educativas que están realizando investigaciones cumplientes. Certifican su clúster una sola vez; luego, pueden entregar un entorno de investigación cumpliente a uno de sus investigadores en menos de una hora. Anteriormente, podía llevar meses ponerse en marcha en un entorno cumpliente.\n\nPara las actualizaciones, aunque los usuarios deben utilizar herramientas a nivel de aplicación o dentro de las cargas de trabajo de las máquinas virtuales, la \"receta\" del entorno, como las reglas de firewall o la configuración de cómo se asignan los recursos a las cargas de trabajo, se puede actualizar de manera fluida. También puedes llevar una máquina virtual en un entorno en ejecución con un nuevo cumplimiento y trasladarla a un nuevo entorno.\n\nVerge.io también es útil para la seguridad. Una de las firmas cuantitativas más grandes de Europa es un buen caso de uso en seguridad. Ellos toman una foto de todo su entorno y luego realizan simulaciones de ataques (equipo rojo y equipo azul) en busca de vulnerabilidades de seguridad, verificando parches, etc.\n\nVDI puede funcionar en el entorno. Verge.io se asocia con una empresa para brindar soporte de VDI. Verge.io controla los recursos, la CPU y la memoria. También admiten GPU y pasarela y GPU física. Este es un caso de uso importante para algunos clientes, especialmente en cargas de trabajo de ingeniería o petróleo y gas. La GPU virtual ofrece economías favorables porque el costo se distribuye entre varios usuarios.\n\nUn ajuste perfecto para Verge.io son los casos de uso en el borde. Un ejemplo típico es el punto de venta. Si un cliente de venta al por menor tiene cien tiendas, podría necesitar dos o tres aplicaciones de máquinas virtuales (VMs) en cada tienda. Dado que Verge.io ocupa poco espacio en hardware, una vez que tengas al menos dos servidores, puedes colocarlos en el centro de datos del caso de uso en el borde y crear esas VMs. Luego, con las funciones de instantáneas y replicaciones, esas configuraciones se pueden copiar y pegar en todos los entornos diferentes. Puedes actualizar las últimas configuraciones en todos ellos, no solo los parches del sistema operativo, sino también las reglas de firewall.\n\nUn emocionante futuro espacio para Verge.io está en los sistemas de conducción automatizada debido al volumen de datos. Muchos proveedores están probando los vehículos en sitios remotos y enviando físicamente los discos duros. Imagina si los datos pudieran procesarse en el sitio, completamente redundantes con los costos convincentes asociados a ello, entonces los datos podrían transportarse en un área amplia en lugar de un disco y un camión.\n\nPara obtener más información sobre Verge.io, visita http://verge.io.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Aaron Reid","Chris Lehman"],"link":"/episode-EDT96-es","image":"./episodes/edt-96/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, arquitecto principal de soluciones de Intel, y Aaron Reid, ingeniero principal de sistemas de https://www.verge.io/ y Chris Lehman, vicepresidente senior de ventas, discuten casos de uso para el software de centro de datos virtual de Verge.io."},{"id":172,"type":"Episode","title":"Los beneficios de las bases de datos gráficas","tags":["data","graphdb","katanagraph"],"body":"\r\n\r\nHadi obtuvo su doctorado en ciencias de la computación en 2012 e investigó la criptografía y la seguridad de la información en redes. Trabajó en el ámbito académico durante algunos años y luego se trasladó a la industria, enfocándose en diferentes aspectos de las soluciones de seguridad, incluyendo la gestión de identidad y acceso. Comenzó a aprender más sobre la modelización de grafos en 2015 y se dio cuenta de cómo la modelización de datos en grafos podría resolver algunos de los problemas emocionantes y complejos en su campo de estudio.\n\nEn las bases de datos de grafos, un grafo no se refiere a gráficos o interfaces gráficas, sino a una forma de estructurar los datos a nivel de almacenamiento para que puedan ser recuperados y procesados para algunos problemas complejos, especialmente si los datos están interconectados. El grafo ofrece muchos beneficios y puede complementar las estructuras o soluciones de datos existentes, como los modelos de bases de datos relacionales o el almacenamiento de objetos.\n\nLa principal diferencia entre las bases de datos gráficas y las bases de datos relacionales es que, si bien ambas se tratan de relaciones, las bases de datos relacionales llevan las relaciones al nivel de metadatos y esquema, mientras que las bases de datos gráficas se centran en las relaciones basadas en los datos. En otras palabras, en las bases de datos relacionales se relacionan columnas de tablas. Para introducir una nueva relación, es necesario cambiar el esquema. El gráfico proporciona una infraestructura sin esquema en la que se puede agregar más estructura a los datos pero mantener la flexibilidad para poder procesar cualquier dato no estructurado.\n\nLa mitad de los datos del mundo se ha creado en los últimos años, recopilados de muchas fuentes diferentes, pero menos del dos por ciento ha sido analizado, la mayoría de los cuales son datos estructurados. Los datos se están recopilando, pero la información es insuficiente para su procesamiento. Debe haber una forma de agregar de manera flexible un poco de estructura que sea lo suficientemente dinámica como para cambiar si hay incertidumbre pero aún así beneficiarse de la computación avanzada optimizada. El gráfico es una excelente manera de hacer esto.\n\nSi estás intentando trabajar con datos correlacionados o interconectados, en contraposición, por ejemplo, a datos aislados con valores críticos, un gráfico ofrecerá beneficios debido a las relaciones. Casi todas las industrias pueden beneficiarse porque los datos no estructurados suelen provenir de diversas fuentes y de naturalezas múltiples.\n\nUn ejemplo serían las soluciones de ciberseguridad. Hay datos de registros y registros de auditoría de los entornos de red, la infraestructura de la nube, los hosts en los endpoint, etc. Los datos provienen de diferentes fuentes, como directorios o archivos de registro en bruto. Sería beneficioso correlacionar los datos, por ejemplo, porque típicamente, una identidad o un usuario que podría ser parte de un registro del sistema de gestión de identidades podría ser el mismo usuario que activa un proceso en una computadora portátil, como descargar un adjunto de un correo electrónico. Al analizar esos patrones, se puede utilizar esta relación correlativa para obtener una visión más amplia. En otras palabras, no importa cómo o de dónde provengan los datos, pero proporcionar este vínculo conduce a aprender sobre cada registro al observarlos en contexto.\n\nUn beneficio es que no hay problemas de transformación de datos, por lo que esto aumenta la velocidad. Esto también reduce los requisitos de almacenamiento.\n\nLos gráficos y las estructuras de bases de datos relacionales, en general, son compatibles. Aquí hay un ejemplo simplificado de una red social: Los tipos de nodos en el grafo se parecen a las tablas. Así que puedes tener un nodo llamado \"persona\" y luego una tabla llamada \"persona\". Podrías tener otro nodo llamado \"ubicación\" y conectar a la persona con una ubicación específica. Puedes tener una tabla llamada \"ubicación\" y conectarla mediante claves foráneas. Luego tienes esta relación de amigos. Un amigo de una persona en un modelo de grafo es solo un bucle en sí mismo. Eso te permitiría modelar ese esquema. En una base de datos relacional, necesitarías crear una nueva tabla llamada \"amigos\" y luego conectarla. Entonces, incluso a nivel de esquema, estás agregando redundancia y cierta estructura encima de ello. Y si necesitas agregar un nuevo concepto de amistad o relación, debes crear nuevas tablas, lo que genera redundancia y complejidad.\n\nOtros beneficios de los gráficos frente a las bases de datos relacionales son los gráficos en elementos, la IA de gráficos y la idea de modelar los datos para encontrar patrones basados en cómo están conectados los datos. Puedes disminuir el conjunto de datos que estás buscando o analizando debido a las relaciones. Se utiliza el poder de los datos para potenciar aún más los datos. Los algoritmos en una base de datos de gráficos son muy diferentes a los de las bases de datos relacionales y están mejor optimizados para acceder más rápido a grandes conjuntos de datos.\n\nUna de las desventajas de una base de datos de grafos es que es difícil de escalar. En una base de datos relacional, es fácil dividir una tabla y ponerla en dos servidores, por ejemplo. Las bases de datos de grafos anteriores fueron diseñadas para ser una solución única y completa, por lo que si deseabas escalar, necesitarías agregar más memoria y CPU.\n\nAhora, si deseas trabajar con petabytes de datos en gráficos, quieres escalar verticalmente tanto como sea posible con tecnología como las máquinas virtuales de Intel, pero también deseas escalar horizontalmente. Nuevas tecnologías, como la plataforma de gráficos de Katana, ayudan a resolver este problema de escalamiento con la computación distribuida. Puedes dividir o fragmentar el problema en piezas y hacer que cada una trabaje en una pequeña parte del gráfico para obtener una solución final. Katana ha demostrado que se pueden utilizar 256 máquinas o más para procesar datos, por lo que puedes tener rápidamente decenas de terabytes de datos en memoria.\n\nUna base de datos gráfica requiere un ecosistema similar al de una base de datos relacional. El gráfico es un poco más dinámico y flexible. Si deseas pasar a una plataforma de análisis de gráficos, que va más allá de las bases de datos operativas, podrías aprovechar otras cosas como la capacidad de almacenamiento de datos y los lagos de datos. El almacenamiento y el procesamiento se harían por separado, lo que significa que las tecnologías de procesamiento de gráficos que realizan todo en la memoria no necesitan depender del almacenamiento adjunto a los servicios para que puedas tener un servicio de almacenamiento diferente.\n\nKatana utiliza almacenamiento de objetos, y luego, cuando desean computar, cargan oportunamente lo que quieran del gráfico a la memoria distribuida de todas las máquinas. Los datos regresan inmutables al almacenamiento, por lo que si, por ejemplo, destruyes todo el clúster, no pierdes nada. Todos los datos ya están allí y almacenados. Las bases de datos relacionales son un campo más maduro, pero las bases de datos de gráficos están siendo cada vez más compatibles en el ecosistema.\n\nBasado en los ocho años de experiencia de Hadi en el campo hablando con interesados y clientes, todos ven inmediatamente el beneficio de las bases de datos de gráficos. Las limitaciones podrían ser que no pueden mantenerse al día con la escalabilidad o el costo. El trabajo de Katana y otras compañías de tecnología de gráficos es convertir los gráficos en una herramienta más común que los clientes puedan usar para diversas tareas y menos un lujo en la base de datos. Por ejemplo, Katana proporciona a los clientes soluciones de identidad basadas en gráficos y administración masiva de datos.\n\nBuenos casos de uso de los gráficos serían la temprana invención de soluciones gráficas específicas para algunas empresas como LinkedIn y Facebook, que tienen sus grafos sociales. Ahora, encajan perfectamente en el comercio electrónico para los motores de recomendación. Encontrar conexiones entre clientes, cuentas, compras y otros comportamientos permitirá realizar recomendaciones mejores de inmediato a los compradores de una manera que no puede hacerse con consultas a bases de datos relacionales.\n\nPara obtener más información sobre Katana o cómo contactar a Hadi, ve a embracingdigital.org.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Madi Ahmadi"],"link":"/episode-EDT97-es","image":"./episodes/edt-97/es/thumbnail.jpg","lang":"es","summary":"Darren Pulsipher, Arquitecto Principal de Soluciones de Intel, el Dr. Hadi Ahmadi, Director de Arquitectura de Soluciones de Katana Graph, discuten los beneficios de las bases de datos de gráficos."},{"id":173,"type":"Episode","title":"La aparición de la Red Global de Datos","tags":["dataarchitecture","datamanagement","data","technology","cloud","globaldatanetwork","macrometa","multicloud","datamesh"],"body":"\r\n\r\nChetan es un ingeniero convertido en especialista en operaciones y startups (Macrometa es su cuarta startup). Él dice que ha estado trabajando en el mismo problema de tratar con datos distribuidos y reducir la latencia durante veinte años.\n\nLos datos ya no están solo en un centro de datos, sino en todas partes: en la nube, en la periferia y en los portátiles de las personas. Gestionar todo eso de manera efectiva es un desafío.\n\nHace aproximadamente diez años, Marc Andreessen dijo que el software está devorando el mundo. En este punto, el software se ha comido todo y ha convertido toda clase de limitaciones y barreras en oportunidades. La computación en múltiples hilos es una de las barreras que ha caído con la nube. Se pueden construir aplicaciones que se ejecuten simultáneamente en diferentes partes del mundo. Paralelamente, se está produciendo un movimiento de desarrolladores para hacer todo tan simple como debe ser para la persona promedio en ciencias de la computación. Así que por un lado tenemos una sofisticada evolución tecnológica y por otro lado, un movimiento hacia la simplicidad.\n\nArquitecturas como Jamstack permiten que la computación distribuida ocurra a gran escala con una gran simplicidad, pero todavía hay una vasta frontera por descubrir y conquistar. La oportunidad de una expansión extendida ahora está en el borde. La gestión de datos distribuidos y el borde son dos caras de la misma moneda.\n\nUn gran problema es que algunos desarrollos de software se están moviendo hacia la funcionalidad como un servicio que ignora los datos. Además, existe la percepción de que los datos son ubicuos, pero gran parte de los dispositivos periféricos no siempre están conectados. No hay garantía de que una aplicación tenga acceso a todos los datos. Las redes ya no están centralizadas; la noción de microservicios sin estado surgió de la movida hacia la nube. Esta falta de estado puede convertirse en una barrera enorme. Por eso, arquitecturas como Jamstack y funciones sin servidor tratan los datos como un tema periférico en lugar de un problema central.\n\nLas estructuras de datos sin estado son simples. Tienes un lugar específico donde guardas tus datos, y luego vuelves a un estado sin estado. Las estructuras de datos con estado requieren infraestructuras robustas con estructuras de datos más complejas porque soportan la aplicación a medida que emite constantemente su estado. A medida que nos adentramos en un mundo de datos de transmisión en tiempo real en el que el estado se emite constantemente desde algún lugar del ecosistema, la infraestructura se vuelve compleja y difícil de gestionar porque no están diseñadas arquitectónicamente. Ahí es donde entra Macrometa. Han construido una nueva plataforma para este estado activo continuo en tiempo real a escala de exabytes.\n\nLidiar con estos datos en tiempo real y en un estado activo y dinámico es un cambio significativo para muchos desarrolladores de software. Desde que surgieron las primeras infraestructuras en la nube, luego las plataformas de big data y después los datos como servicio, la industria se ha vuelto eficiente en la ingestión, procesamiento y análisis de datos históricos. Pero ahora estamos en un mundo donde los datos existen en un espectro en lugar de como un monolito. Una cualidad apreciada ahora es que los datos tienen una visión y valor perecederos. Algunos datos tienen una vida útil breve. Las escalas de tiempo actuales son demasiado grandes para utilizar los datos eficientemente; necesitamos sistemas que se comuniquen de manera eficiente en 50 milisegundos y reduzcan la carga cognitiva de las personas que interactúan con esos sistemas.\n\nLa mayoría de las personas malinterpretan la latencia: no es algo que te brinde alegría, sino que la falta de ella te molesta. Por ejemplo, ¿cuánto tiempo tolerará alguien un video entrecortado de YouTube o un programa de Netflix que se carga lentamente? Cincuenta milisegundos para una máquina es una eternidad. Una máquina puede hacer una cantidad enorme de cosas en 50 milisegundos, por lo que la latencia se vuelve esencial, especialmente al considerar el valor perecedero de los datos.\n\nOtro problema ahora es que, debido a la nube, la interconectividad y el sistema global, las startups son empresas multinacionales y los datos se vuelven sensibles a la ubicación. Algunos de los datos están regulados, algunos son PII y no pueden ser extraídos en ciertas jurisdicciones, etc. Un excelente ejemplo de este problema es cómo los europeos no quieren que sus datos salgan de sus fronteras, pero la mayor parte de la infraestructura de la nube y las aplicaciones están construidas aquí en Estados Unidos.\n\nUn tercer problema es que los datos se encuentran en muchos lugares; existen límites entre sistemas, tanto físicos como lógicos. Los datos pueden ser esencialmente estáticos y rígidos, por lo que necesitamos infraestructura que permita que los datos se conecten y fluyan en tiempo real con consistencia y garantías de ordenamiento. Lo más importante es que crea fungibilidad para poder ser consumidos rápidamente de diversas formas.\n\nUn problema adicional es que los datos tienen mucho ruido, y no tiene sentido enviar distancias intercontinentales, pagando tarifas de transferencia, solo para acabar eliminando la mayor parte. Los datos pierden valor en el momento en que llegan a su destino. También existe una alta tasa de actualización, por lo que los sistemas a menudo trabajan con datos obsoletos.\n\nNecesitamos nuevas formas de resolver este tipo de problemas de datos distribuidos. Chetan cree que los próximos diez años pertenecerán a esta área de las ciencias de datos.\n\nLa primera generación de soluciones de datos distribuidos utilizaba la transformación operativa. Google Docs es un excelente ejemplo de eso. Sin embargo, la transformación operativa requiere centralización del control, por lo que no escala bien. Google ha encontrado una manera de escalar, pero eso no se generaliza al desarrollador promedio. Solo hay tal vez cinco compañías en el mundo que lo entienden a esa escala, y gran parte de ese conocimiento está cerrado en esas compañías y tecnología propietaria.\n\nMacrometa está trabajando con la comunidad y la academia para intentar crear un nuevo cuerpo de conocimiento, mucho más eficiente que estos modelos centralizados de forma totalmente distribuida.\n\nActualmente, existen infraestructuras disponibles que son excelentes para resolver problemas históricos de tipo registro. Están tratando de avanzar hacia datos en tiempo real, pero sus arquitecturas no están diseñadas fundamentalmente para ello. Estos nuevos problemas con datos sensibles al tiempo y la ubicación, valor de actuación, tasas de actualización, gravedad de datos y ruido de datos requieren una nueva forma, una nueva infraestructura. Chetan lo llama un sistema de interacción en lugar de un sistema de registros, porque los sistemas de interacción son redes de datos, cerca de donde se origina y se consume datos, que luego filtran y enriquecen todo y lo dirigen a sus destinatarios previstos. Es una función de redes.\n\nMacrometa ha construido procesadores de red que están moviendo los datos alrededor - una red global de datos. Es un sistema de API sin servidor donde los desarrolladores simplemente consumen API para resolver problemas de datos activos y operativos en tiempo real. Macrometa es una red global de datos en la topología de un CDM, pero con una plataforma de datos como Snowflake que produce primitivas de datos ricas para manejar valores de datos activos y operativos en tiempo real.\n\nPuedes integrar herramientas analíticas en la red de datos global y desplegar el análisis cerca de donde se genera o se requiere los datos. Así como Amazon cambió fundamentalmente la distribución minorista con arquitectura y algoritmos periféricos para mantener los almacenes locales óptimamente abastecidos para los envíos nocturnos, Macrometa ha hecho lo mismo para los datos. Están acercando los datos y la computación a esos datos mucho más cerca y permitiendo que suceda en milisegundos. Esta capacidad de crear bucles de información en tiempo real es un habilitador poderoso. Por ejemplo, los pequeños minoristas pueden utilizar el inventario local de su tienda en su comercio electrónico sin sobreabastecerse para competir con Amazon.\n\nUn gran caso de uso para la plataforma Macrometa es en ciberseguridad. Algunos clientes están eliminando sus modelos de datos centralizados para aprovechar la menor latencia y poder bloquear amenazas en tiempo real.\n\nLa red global de datos es una capa de transformación entre tus fuentes y receptores de datos con los consumidores y publicadores. Está compuesta por tres elementos tecnológicos. El primero es el entramado global de datos, que es la capa de integración para los datos. El segundo es una infraestructura global de cómputo que te permite orquestar datos y lógica empresarial en forma de funciones y contenedores a nivel mundial. El tercer elemento es un entramado global de privacidad: cómo proteger los datos y cumplir con diferentes regímenes y regulaciones que afectan si tus datos se están transmitiendo o almacenando.\n\nLa malla global de datos es una forma de integrar rápidamente y fácilmente datos de diferentes sistemas a través de límites, ya sean físicos o lógicos. Todo esto se incorpora y fluye con consistencia y garantías de orden. El valor más significativo de esta malla es que hace que los datos sean fungibles y consumibles al permitirte poner APIs en los datos rápidamente. Esto se puede hacer en unas pocas horas en comparación con lo que usualmente toma meses. La red global de datos está diseñada para billones de eventos por segundo, de modo que puede mover datos a escala masiva a un costo un 90 por ciento menor que la nube.\n\nLa tela global de cómputo lleva la lógica empresarial y la orquestación para acercar su procesamiento a donde sus datos se originan o se consumen. Este es el patrón anti-nube. Macrometa moverá quirúrgica y dinámicamente aquellos microservicios que necesiten cumplir con regulaciones de datos, por ejemplo, a los lugares correctos para su ejecución.\n\nLa última pieza es la protección de datos. Este es un problema complejo y las respuestas que tenemos hoy en día, por ejemplo, abrir un silo separado para esa geolocalización en particular para cumplir con los requisitos cada vez que inicia una instancia en su aplicación, no son buenas. La plataforma de Macrometa tiene una red de datos que ya está integrando y haciendo que sus datos fluyan a través de todos los límites, junto con funciones de cálculo y la ingestión de datos sin limitaciones. Ahora, puede crear límites lógicos y anclar datos a regiones específicas para protegerlos. Pueden establecer afinidades y políticas sobre cómo los datos viven y se replican en una región, como si deberían ser anonimizados cuando se copian fuera de la región.\n\nLa tecnología de Macrometa permite casos de uso que son imposibles de realizar en la nube porque las nubes están demasiado lejos o son demasiado lentas. Macrometa ha construido la infraestructura para resolver problemas de datos en tiempo real y convertirlos en oportunidades en lugar de desafíos. Para obtener más información sobre Macrometa, visita macrometa.com.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Chetan Venkatesh"],"link":"/episode-EDT98-es","image":"./episodes/edt-98/es/thumbnail.jpeg","lang":"es","summary":"En este episodio, Darren rememora con Chetan Venkatesh, CEO de MacroMeta. Venkatesh tiene una larga trayectoria en gestión de datos desde los primeros días de la Computación en Red y ha fundado MacroMeta para abordar la gestión de datos en los bordes dispersos globalmente, centros de datos y nubes."},{"id":174,"type":"Episode","title":"Camino de Ataque Cibernético Precog con XM Cyber","tags":["aiml","cybersecurity","xmcyber","technology","process"],"body":"\r\n\r\nPaul ha estado en seguridad desde finales de los años noventa, comenzando haciendo proyectos para el Departamento de Seguridad Nacional y el Departamento de Defensa. En 2005, se unió a Fishnet Security como ingeniero de ventas y se ha dedicado a la ingeniería de ventas. Se unió a XM Cyber para concentrarse en la simulación de brechas y ataques.\n\nEn lugar de la ciberseguridad tradicional, que se basa en la detección, remediación y prevención, XM Cyber es predictiva. Una buena descripción es que es una simulación precognitiva. XM Cyber crea incidentes imaginarios para brindarte una visión de cómo tus herramientas podrían abordarlos y cómo podrías trabajar para remediar puntos de congestión específicos. La idea es hacer todo esto antes de las pruebas de penetración. Puedes solucionar cosas hoy, ver el impacto mañana y luego mejorar continuamente tu seguridad.\n\nGoogle Maps es una buena analogía de cómo XM Cyber funciona. Cuando quieres ir de un lugar a otro, Google Maps te dirá todas las formas de llegar, evitando peajes, la ruta más directa, etc. XM Cyber hace lo mismo pero con una simulación de ataque. Por ejemplo, supongamos que tienes una cuenta de usuario de active directory comprometida. En ese caso, te mostrará los seis pasos para poder llegar a un controlador de dominio local y comprometer ese activo crítico. También mostrará todas las diferentes rutas entre esos dos puntos.\n\nUn caso de uso para esto es que puedes permitir que un equipo rojo sea súper eficiente con esta información, porque no tienen que buscar a tientas y tratar de hacer descubrimientos. También puede ayudar a la sección azul, permitiéndoles priorizar las soluciones en los puntos de estrangulamiento. Por ejemplo, si hay 400 rutas de ataque que van a diferentes áreas en tu DMZ, pero todas las 400 parecen tener que aprovechar esta única entidad para que esa ruta suceda, entonces puedes resolver ese único problema y destruir las 400 rutas. Los equipos azules pueden asegurar esos puntos de estrangulamiento que podrían permitir al atacante.\n\nUn área que XM Cyber analiza es la gestión de identidad, no solo en el centro de datos, sino también en la nube. A veces, las vías de ataque pueden tener diez pasos de longitud, pero nueve de esos pasos estarán relacionados con la navegación en el mundo de la identidad. Por ejemplo, es posible que tengas permisos en tu cuenta de administrador, y luego esa cuenta de administrador puede tener permisos adicionales. Se pueden tomar seis o siete pasos ajustando diferentes permisos o restableciendo contraseñas y desplegando GPOs (Objetos de Directiva de Grupo). Se podrían dar nueve pasos desde una cuenta de usuario estándar a una de administrador de dominio aprovechando el Directorio Activo.\n\nAdemás de la identidad, XM Cyber analiza más de cien entidades como máquinas, depósitos S3 y claves SSH. Estas diferentes entidades pueden combinarse para crear una ruta de ataque. A veces se vuelve muy complejo. Por ejemplo, un ataque podría comenzar en la ubicación local, pasar a Azure, aprovechar Intune y luego regresar para comprometer otra máquina que permita un salto a GCP. Una vez que los atacantes se encuentran en el entorno de GCP, pueden aprovechar la confianza o permisos entre AWS y GCP para comprometer AWS. XM Cyber examina todos los diferentes tipos de entidades en entornos dispares y las conecta para evaluar estos caminos en función de cómo cada entidad juega de manera integral en el riesgo de las demás.\n\nHay dos formas en que XM Cyber se involucra con los clientes. La primera es un descubrimiento de alto nivel para evaluar el entorno, exponer vulnerabilidades y medir cómo un atacante puede revelar nuevas vulnerabilidades para poner en riesgo activos críticos. La segunda es una evaluación dirigida de un escenario específico que preocupa al cliente. Estos compromisos no son solo análisis estáticos de entidades. Son dinámicos porque observan el tráfico y otros patrones.\n\nUn caso de uso típico es determinar si OT es el activo crítico o el punto de brecha. XM Cyber pone en juego casos como si una máquina en Recursos Humanos fuera el punto de brecha, ¿hay algún riesgo para este PLC que se encuentra en el entorno SCADA y controla interruptores de presión que podrían apagar la electricidad de una municipalidad? Ese es un caso de uso real que XM Cyber puede simular. Este tipo de información es crucial en un mundo donde OT ya no está aislado, sino conectado a redes.\n\nXM Cyber es una solución SaaS en lugar de estar en las instalaciones, para poder mantenerse dinámico y ofrecer el mejor servicio. Puede ser aterrador pensar que algo en la nube tiene todas tus técnicas de ataque. Sin embargo, XM Cyber hace mucho trabajo para asegurar que los datos estén completamente aislados, cumplan con SOC 2, entre otras certificaciones, y no haya multi-tenencia. Además, no recopilan nada sensible. La información sensible se cifra, y solo una parte se envía a la nube. No necesitan tener datos reales.\n\n\r\n\r\n<details>\r\n<summary> Podcast Transcript </summary>\r\n\r\n<p></p>\r\n\r\n</details>\r\n","guests":["Darren W Pulsipher","Paul Giorgi"],"link":"/episode-EDT99-es","image":"./episodes/edt-99/es/thumbnail.png","lang":"es","summary":"Darren Pulsipher, Arquitecto de Soluciones Principal de Intel, y Paul Giorgi, Director de Ingeniería de Ventas de XM Cyber, discuten cómo la tecnología de XM Cyber puede ayudar a las organizaciones a descubrir caminos de ataque y reducir el riesgo."},{"id":175,"type":"News Brief","title":"2023-2-5","tags":["ai","compute","cybersecurity"],"body":"\n\n## En noticias sobre Inteligencia Artificial\n\nSegún Forbes, para el 2030, la Inteligencia Artificial podría generar potencialmente $13 billones en la economía global, o el 16% del Producto Interno Bruto actual del mundo.\n\nChatGPT permite a los estafadores crear correos electrónicos tan convincentes que pueden obtener dinero de las víctimas sin incluso depender de malware. Algunos temen que el poderoso chatbot haga que sea mucho más fácil para los no programadores crear malware y convertirse en ciberdelincuentes. ChatGPT Ciberseguridad Engaño Social\n\nGoogle ha invertido casi $400 millones en la start-up de inteligencia artificial Anthropic, que está probando un rival del ChatGPT de OpenAI Google llevará a cabo un evento sobre cómo está \"utilizando el poder de la Inteligencia Artificial para reimaginar cómo las personas hacen búsquedas en línea\". Un evento de 40 minutos se transmitirá en YouTube el 8 de febrero a las 8:30 AM hora standard este.\n\nhttps://interestingengineering.com/innovation/google-invests-anthropic-battle-chatgpt \n\nUna herramienta de detección de Inteligencia Artificial, que verifica el contenido escrito por ChatGPT, ha dicho que Macbeth, escrito por William Shakespeare, fue generado por IA. Supongo que ChatGPT no estaba entrenado en inglés antiguo. https://venturebeat.com/ai/chatgpt-detection-tool-thinks-macbeth-was-generated-by-ai-what-happens-now/ Supongo que no estaba entrenado en inglés antiguo (conocido como inglés shakespeareano)\n\n## En Noticias de Computación Ubicua\n\nLas ganancias de nube publica global continúan aumentando, con un crecimiento proyectado del 26% que alcanzará los $525 mil millones este año, según Statista. El software como servicio se proyecta en $253 mil millones de esas ganancias. Sin embargo, a medida que continúa el crecimiento de la nube, los proveedores de servicios de nube sienten el impacto, ya que ha habido varias interrupciones en los últimos 12 meses. Este ha sido un problema tan grave que han surgido varios sitios web que monitorean y informan la disponibilidad de la nube publica, como cloudharmony.com y thousandeyes.com.\n\nhttps://cloudharmony.com/status\n\nhttps://www.thousandeyes.com/outages/\n\nhttps://www.statista.com/outlook/tmo/public-cloud/worldwide\n\nOracle y Red Hat se han unido para brindar instancias de máquina virtual basadas en RHEL en la nube de Oracle, ejecutadas en procesadores Intel, AMD y ARM. Esta colaboración entre antiguos competidores está brindando a los clientes una ventanilla única para cargas de trabajo que utilizan la suite de productos de Oracle así como cargas de trabajo tradicionales basadas en Linux. Esto continúa empujando a Oracle a competir con los grandes Hiper-escaladores como Azure, AWS y Google.\n\nhttps://www.networkworld.com/article/3686513/red-hat-enterprise-linux-arrives-in-oracle-s-cloud.html\n\n## En Noticias de Seguridad cibernética\n\nEl comercio electrónico en Corea del Sur y los EE. UU. está sufriendo ataques de una campaña de malware GuLoader, según lo reveló la compañía de ciberseguridad Trellix el mes pasado. Los atacantes cibernéticos están abandonando su enfoque en documentos de Microsoft Word para utilizar NSIS por sus siglas en Ingles (Nullsoft Scriptable Install System). Esta herramienta de código abierto para escribir instaladores en el sistema operativo Windows esta siendo utilizada para instalar malware en sistemas basados en Windows \n\n[Ataque de GuLoader Malware con NSIS Malicioso](https://thehackernews.com/2023/02/guloader-malware-using-malicious-nsis.html)\n\nHackers pro-Rusos están utilizando una nueva plataforma de DDoS como servicio llamada Passion para atacar instituciones médicas en países de la OTAN tales como los EE. UU., Portugal, España, Alemania, Polonia, Finlandia, Noruega, Los Países Bajos y el Reino Unido. Este servicio por suscripción permite a los clientes seleccionar los vectores de ataque deseados, la duración e intensidad. \n\n[Los criminales cibernéticos rusos lanzan la nueva plataforma de ataque Passion](https://cyware.com/news/russian-cybercriminals-launch-new-passion-attack-platform-798d8713)\n\nLa Agencia de Ciberseguridad e Infraestructura de Seguridad de EE. UU. (CISA por sus siglas en inglés) anunció el jueves la publicación de una guía de Categorización de Vulnerabilidades Específicas de Partes Interesadas (SSVC por sus siglas en Ingles) que puede ayudar a las organizaciones a priorizar la solución de vulnerabilidades mediante un modelo de árbol de decisión. El sistema SSVC se creó en el 2019 por CISA y el Instituto de Ingeniería de Software de la Universidad Carnegie Mellon (SEI), y un año más tarde, CISA desarrolló su propio árbol de decisión SSVC personalizado para fallas de seguridad relevantes en organizaciones gubernamentales y de infraestructura crítica. \n\n[CISA publica un modelo de árbol de decisión](https://www.securityweek.com/cisa-releases-decision-tree-model-help-companies-prioritize-vulnerability-patching/) \n\n## Podcast de Aceptación de la Transformación Digital\n\nEsta semana, Darren Pulsipher entrevista a chatGPT.\n\n[Leer más](https://www.embracingdigital.org/episodes-EDT122)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW1-es","image":"./briefs/edw-1/es/thumbnail.png","lang":"es","summary":"For the week of February 6, 2023. News from around the world of digital transformation in artificial intelligence, cloud computing, and cybersecurity."},{"id":176,"type":"News Brief","title":"2023-4-9","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Inteligencia Artificial\n\n¡Otra cosa más de la que los padres deben preocuparse! ¡La clonación de voz con IA! En un reciente secuestro virtual por parte de un ciberdelincuente, la madre de una niña de 15 años recibió una llamada falsificada utilizando la voz de su hija para anunciar su secuestro. La llamada fue seguida por un \"Tengo a tu hija\". Pensando rápidamente, la madre envió un mensaje de texto a su esposo para verificar la ubicación de su hija, quien estaba a salvo. El FBI está investigando este y varios otros casos.\n\n[https://www.azfamily.com/2023/04/10/ive-got-your-daughter-scottsdale-mom-warns-close-encounter-with-ai-voice-cloning-scam/](https://www.azfamily.com/2023/04/10/ive-got-your-daughter-scottsdale-mom-warns-close-encounter-with-ai-voice-cloning-scam/)\n\nEugenia Kuyda es la fundadora de Replika, una aplicación de chatbot de EE. UU. que dice ofrecer a los usuarios un \"compañero de IA que se preocupa, siempre aquí para escuchar y hablar, siempre de tu lado\". Lanzada en 2017, ahora cuenta con más de dos millones de usuarios activos. Cada uno tiene un chatbot o \"replika\" único para ellos, ya que la IA aprende de sus conversaciones. Los usuarios también pueden diseñar su propio avatar de dibujos animados para su chatbot.\n\n[https://www.bbc.com/news/business-65110680](https://www.bbc.com/news/business-65110680)\n\nNVIDIA y Getty Images colaboran en Inteligencia Artificial Generativa. Las empresas tienen como objetivo desarrollar dos modelos de IA generativa utilizando NVIDIA Picasso, parte de los nuevos servicios en la nube de NVIDIA AI Foundations. Los usuarios podrían utilizar los modelos para crear una imagen o video personalizado en segundos, simplemente escribiendo un concepto. El esfuerzo conjunto tiene como objetivo personalizar los modelos fundamentales de texto a imagen y de texto a video para crear imágenes impresionantes utilizando contenido visual completamente con licencia.\n\n[https://blogs.nvidia.com/blog/2023/03/21/generative-ai-getty-images/?ncid=so-link-748862&=&linkId=100000197541685#cid=gtcs23_so-link_en-us](https://blogs.nvidia.com/blog/2023/03/21/generative-ai-getty-images/?ncid=so-link-748862&=&linkId=100000197541685#cid=gtcs23_so-link_en-us)\n\n## Computación Ubicua\n\n¿Está COBOL finalmente muerto? GFT y Cloud Frame se unen para ayudar a las organizaciones a trasladar sus programas de COBOL a plataformas más eficientes. COBOL fue desarrollado en 1959 como uno de los primeros lenguajes con soporte de múltiples proveedores para usuarios empresariales de mainframe. Se ha vuelto cada vez más costoso depurar y mantener sistemas COBOL debido a la falta de programadores que todavía trabajan con COBOL.\n\n[https://www.cloudcomputing-news.net/news/2023/apr/04/gft-and-cloudframe-help-industries-say-cheerio-to-cobol/](https://www.cloudcomputing-news.net/news/2023/apr/04/gft-and-cloudframe-help-industries-say-cheerio-to-cobol/)\n\nLos modelos de lenguaje grandes (LLMs) están muy de moda, con ChatGPT liderando el camino. Esto es bueno solo para los proveedores de servicios en la nube, ya que los LLMs requieren grandes cantidades de cómputo y almacenamiento de datos para su implementación y desarrollo. Para no quedarse atrás, los proveedores de servicios en la nube Azure, AWS y Google han anunciado sus propias soluciones de LMS para competir en este nuevo campo de batalla por la dominancia en la nube. Preguntemos a ChatGPT quién será el ganador.\n\n[https://www.infoworld.com/article/3693330/large-language-models-are-the-new-cloud-battleground.html](https://www.infoworld.com/article/3693330/large-language-models-are-the-new-cloud-battleground.html)\n\nGoogle ha lanzado su oferta en el ámbito de la computación confidencial, proporcionando privacidad completa de datos sensibles mediante la encriptación de datos en reposo, en tránsito y ahora en uso. Azure y AWS tienen ofertas de productos similares que brindan computación confidencial a clientes del sector público y privado.\n\n[https://www.wired.com/story/google-cloud-confidential-virtual-machines/](https://www.wired.com/story/google-cloud-confidential-virtual-machines/)\n\n## Ciberseguridad.\n\nEn un interesante ataque DDoS, actores amenazantes inundaron npm, un repositorio de paquetes de código abierto para Node.js, creando sitios web maliciosos y publicando paquetes vacíos con enlaces a esos sitios web maliciosos para aprovechar la buena reputación del ecosistema en los motores de búsqueda. Se cargaron más de 1,42 millones de paquetes falsos.\n\n[https://thehackernews.com/2023/04/hackers-flood-npm-with-bogus-packages.html](https://thehackernews.com/2023/04/hackers-flood-npm-with-bogus-packages.html)\n\nSe estima que más de 1,000,000 sitios de WordPress están infectados por una campaña en curso para desplegar el malware Balada Injector. Se sabe que los ataques se desarrollan en oleadas cada pocas semanas. Los ataques redirigen subdominios aleatorios a varios sitios de estafas, incluyendo sitios web con soporte técnico falso. La mejor forma de combatir esto es actualizar los complementos (plugins) en tus sitios de WordPress.\n\n[https://thehackernews.com/2023/04/over-1-million-wordpress-sites-infected.html](https://thehackernews.com/2023/04/over-1-million-wordpress-sites-infected.html)\n\nEl grupo estatal-nación iraní, MuddyWater, ha estado llevando a cabo ataques destructivos en entornos de nube híbrida bajo el disfraz de operaciones de ransomware. Los actores de amenazas se están haciendo pasar por una campaña estándar de ransomware, pero en realidad están destruyendo y alterando las operaciones críticas de TI. Los hallazgos han demostrado que MuddyWater ha colaborado con DEV-1084 para llevar a cabo estos ataques.\n\n[https://thehackernews.com/2023/04/iran-based-hackers-caught-carrying-out.html](https://thehackernews.com/2023/04/iran-based-hackers-caught-carrying-out.html)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW10-es","image":"./briefs/edw-10/es/thumbnail.png","lang":"es","summary":"Resumen"},{"id":177,"type":"News Brief","title":"2023-4-16","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Ciberseguridad\n\nEl FBI ha advertido al público que evite usar estaciones públicas de carga de teléfonos debido al riesgo de ciberataques. Estas estaciones de carga, comúnmente encontradas en lugares públicos como aeropuertos, centros comerciales y hoteles, podrían contener malware que comprometan la seguridad de la información personal y dispositivos de los usuarios. Traiga su propio adaptador de corriente USB y debería estar listo para continuar.\n\n[https://www.cnbc.com/2023/04/10/fbi-says-you-shouldnt-use-public-phone-charging-stations.html](https://www.cnbc.com/2023/04/10/fbi-says-you-shouldnt-use-public-phone-charging-stations.html)\n\nGoogle ha lanzado una actualización urgente para su navegador web Chrome para abordar una vulnerabilidad crítica que los hackers están explotando. La vulnerabilidad, identificada como un error de \"uso después de liberar\", podría permitir que actores malintencionados ejecuten código arbitrario y potencialmente obtengan control sobre sistemas afectados. Google ha instado a los usuarios a actualizar sus navegadores Chrome a la última versión tan pronto como sea posible para mitigar el riesgo de ser blanco de ataques cibernéticos.\n\n[https://thehackernews.com/2023/04/google-releases-urgent-chrome-update-to.html](https://thehackernews.com/2023/04/google-releases-urgent-chrome-update-to.html)\n\nSe ha descubierto que las aplicaciones de encuestas Android y Novi tienen graves vulnerabilidades de seguridad que podrían exponer potencialmente la información personal de los usuarios. Las fallas podrían permitir a los atacantes evitar las medidas de seguridad y acceder sin autorización a datos sensibles. Se insta a los usuarios a ser cautelosos y actualizar sus aplicaciones a las últimas versiones para protegerse contra posibles amenazas cibernéticas.\n\n[https://thehackernews.com/2023/04/severe-android-and-novi-survey.html](https://thehackernews.com/2023/04/severe-android-and-novi-survey.html)\n\n## Gestión de datos\n\nSegún el Oficial Principal de Estrategia de OneTrust, las brechas en la privacidad y ética de los datos plantean una amenaza existencial para las organizaciones. La falta de atención adecuada a estas brechas puede resultar en daños reputacionales, responsabilidades legales y financieras, y pérdida de confianza del cliente. Las organizaciones deben priorizar la privacidad y ética de los datos para mitigar riesgos y proteger sus negocios frente a los requisitos regulatorios en constante evolución y el aumento de la escrutinio público.\n\n[https://deloitte.wsj.com/articles/data-privacy-ethics-gaps-an-existential-threat-says-onetrust-cso-01668542698](https://deloitte.wsj.com/articles/data-privacy-ethics-gaps-an-existential-threat-says-onetrust-cso-01668542698)\n\nDespués de una revisión exhaustiva de los informes recientes de la Oficina de Responsabilidad del Gobierno de los Estados Unidos, Andrew Kuoh, un principal en Capgemini, ha identificado tres acciones clave en las que deben centrarse las organizaciones de datos del sector público. Fomentar una cultura impulsada por los datos, establecer marcos de gobierno de datos y aprovechar tecnologías modernas como la informática en la nube y la inteligencia artificial (IA) para aprovechar el valor de los datos. (Las personas, el proceso y la tecnología suenan familiares).\n\n[https://www.informationweek.com/government/3-actions-to-kickstart-data-ecosystems-in-the-public-sector](https://www.informationweek.com/government/3-actions-to-kickstart-data-ecosystems-in-the-public-sector)\n\nUn reciente seminario web de un panelista de DBTA discutió la importancia de mantenerse al día con las necesidades en evolución de las bases de datos y aplicaciones en la nube. A medida que avanza la computación en la nube, las organizaciones deben adaptar sus estrategias de bases de datos y aplicaciones para optimizar el rendimiento, la seguridad y la escalabilidad. Enfatizar las tecnologías nativas de la nube, la automatización y las mejores prácticas puede ayudar a las empresas a mantenerse por delante en el entorno dinámico de la nube de hoy en día.\n\n[https://www.dbta.com/Editorial/News-Flashes/Keeping-Up-with-the-Evolving-Needs-of-Databases-and-Applications-in-the-Cloud-158166.aspx](https://www.dbta.com/Editorial/News-Flashes/Keeping-Up-with-the-Evolving-Needs-of-Databases-and-Applications-in-the-Cloud-158166.aspx)\n\n## Borde Inteligente\n\nLa Edge Computing Expo North America, programada para el 17 y 18 de mayo de 2023, ha anunciado recientemente una lista de estrellas compuesta por tecnólogos y líderes empresariales en el espacio del IOT. La conferencia se llevará a cabo en el Centro de Convenciones de Santa Clara en California. Tienes que agregar esta fecha a tu calendario.\n\n[https://www.iot-now.com/2023/04/14/129661-edge-computing-expo-north-america-announces-speaker-line-up-hear-from-leading-experts-in-edge-computing/](https://www.iot-now.com/2023/04/14/129661-edge-computing-expo-north-america-announces-speaker-line-up-hear-from-leading-experts-in-edge-computing/)\n\nLa convergencia de la ciberseguridad de IT y OT se está haciendo realidad a medida que más soluciones de ciberseguridad del espacio de IT se centran en la protección de IoT e infraestructura industrial. Un ejemplo reciente es la introducción por parte de CrowdStrike de CloudStrike Falcon Insight para IoT. Esta plataforma ofrece las mismas herramientas en endpoints de IoT, IT, y cargas de trabajo en la nube y el centro de datos.\n\n[https://www.iot-now.com/2023/04/17/129671-crowdstrike-brings-xdr-for-iot-offering-to-deliver-protection-to-iot-assets/](https://www.iot-now.com/2023/04/17/129671-crowdstrike-brings-xdr-for-iot-offering-to-deliver-protection-to-iot-assets/)\n\nEn un gran ejemplo de asumir despliegues de la Industria 4.0, el Grupo Volvo ha implementado una infraestructura de mantenimiento preventivo en su fábrica en Lyon, Francia. El Grupo Volvo está utilizando una red de área amplia de largo alcance (LoRaWAN) para permitir el monitoreo y análisis en tiempo real de datos de equipos de fábrica que permiten un mantenimiento preventivo proactivo.\n\n[https://www.edgecomputing-news.com/2023/04/03/volvo-group-uses-lorawan-for-predictive-maintenance-in-lyon-factory/](https://www.edgecomputing-news.com/2023/04/03/volvo-group-uses-lorawan-for-predictive-maintenance-in-lyon-factory/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW11-es","image":"./briefs/edw-11/es/thumbnail.png","lang":"es","summary":"Resumen"},{"id":178,"type":"News Brief","title":"2023-4-23","tags":["ai","edge","cybersecurity"],"body":"\n\n## Inteligencia Artificial\n\nSegún MarketWatch, con una tasa de crecimiento anual esperada de casi el 27%, el hardware de inteligencia artificial, una industria de $10 mil millones en 2021, se espera que sea una industria de $89 mil millones para 2030.\n\n[https://www.marketwatch.com/story/nvidia-is-ai-hardwares-leader-now-but-intel-amd-and-others-are-closing-fast-8ad6f23f](https://www.marketwatch.com/story/nvidia-is-ai-hardwares-leader-now-but-intel-amd-and-others-are-closing-fast-8ad6f23f)\n\nEl proyecto GARD (Garantizar la Robustez de la IA contra el Engaño) de Defense Advanced Research Projects Agency (DARPA), de varios millones de dólares, tiene tres objetivos clave: desarrollar algoritmos que protejan el aprendizaje automático de vulnerabilidades; desarrollar teorías para garantizar que los algoritmos de IA sean defendibles contra ataques; y compartir ampliamente las herramientas.\n\n[https://www.zdnet.com/in-depth/innovation/these-experts-are-racing-to-protect-ai-from-hackers-time-is-running-out/](https://www.zdnet.com/in-depth/innovation/these-experts-are-racing-to-protect-ai-from-hackers-time-is-running-out/)\n\nTom Brady amenazó con demandar a los comediantes detrás del video de comedia de inteligencia artificial. Utilizando datos de entrevistas con Tom Brady y cientos de miles de horas de metraje de comedia en vivo, la sucursal Dudsey simuló un especial de comedia en vivo de una hora de duración.\n\n[https://nypost.com/2023/04/20/tom-brady-threatened-to-sue-comedians-over-ai-standup-video/](https://nypost.com/2023/04/20/tom-brady-threatened-to-sue-comedians-over-ai-standup-video/)\n\n## Ciberseguridad\n\nUn nuevo informe de CybelAngel ha revelado las cinco principales exposiciones de ciberseguridad que representan riesgos críticos para las organizaciones. El informe identifica las principales amenazas de configuraciones incorrectas en la nube, vulnerabilidades en la cadena de suministro, ataques de ransomware, estafas de phishing y software sin parchear. El informe insta a las organizaciones a mitigar estos riesgos y proteger proactivamente sus sistemas y datos.\n\n[https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures/](https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures/)\n\nLa agencia europea de control de tráfico aéreo Eurocontrol ha revelado que sufrió un ciberataque procedente de Rusia a principios de este mes. El ataque apuntó contra los sistemas de la agencia, pero Eurocontrol fue capaz de contener y remediar el incidente sin causar ninguna interrupción en el tráfico aéreo. La agencia ha advertido a otras organizaciones para que estén alerta y refuercen sus defensas contra las amenazas cibernéticas.\n\n[https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures](https://www.helpnetsecurity.com/2023/04/24/critical-cybersecurity-exposures)\n\nSe ha descubierto una vulnerabilidad crítica en el producto del sistema de control industrial (ICS) de INEA, la cual podría permitir que atacantes remotos tomen el control de los sistemas afectados y causen interrupciones o daños a las operaciones industriales. El fallo, que afecta a todas las versiones del producto, fue descubierto por investigadores de Claroty. La empresa ha instado a las organizaciones que utilizan el producto a actualizar a la última versión lo antes posible para mitigar el riesgo de explotación.\n\n[https://www.securityweek.com/critical-flaw-in-inea-ics-product-exposes-industrial-organizations-to-remote-attacks/](https://www.securityweek.com/critical-flaw-in-inea-ics-product-exposes-industrial-organizations-to-remote-attacks/)\n\n## Borde inteligente\n\nKneron, un proveedor de soluciones de inteligencia artificial (IA) en el borde, ha adquirido Otus, un fabricante de soluciones de imagen para vehículos autónomos. La adquisición permitirá a Kneron aprovechar la experiencia de Otus en el desarrollo de cámaras compactas y eficientes en energía para aplicaciones de IA en el borde. Se espera que la adquisición acelere la adopción de la IA en el borde en la industria del transporte.\n\n[https://www.edgecomputing-news.com/2023/04/20/kneron-buys-otus-for-edge-ai-imaging-in-autonomous-vehicles/](https://www.edgecomputing-news.com/2023/04/20/kneron-buys-otus-for-edge-ai-imaging-in-autonomous-vehicles/)\n\nSolo.io ha lanzado Gloo Fabric, una plataforma de malla de servicio multi-nube segura para aplicaciones empresariales que admite entornos híbridos y multi-nube, con un panel centralizado para monitorear, administrar y solucionar problemas de aplicaciones.\n\n[https://www.edgeir.com/gloo-fabric-by-solo-io-promises-secure-multi-cloud-discovery-and-connectivity-for-enterprises-20230421](https://www.edgeir.com/gloo-fabric-by-solo-io-promises-secure-multi-cloud-discovery-and-connectivity-for-enterprises-20230421)\n\nEl gobierno de Canadá y Ericsson invertirán CA$470m ($376m) en investigación y desarrollo de 5G y 6G durante cinco años para crear una infraestructura de telecomunicaciones sostenible y segura, desarrollar nuevos casos de uso y aplicaciones como ciudades inteligentes y vehículos conectados.\n\n[https://www.edgeir.com/government-of-canada-ericsson-announce-ca470-million-investment-for-5g-6g-rd-20230420](https://www.edgeir.com/government-of-canada-ericsson-announce-ca470-million-investment-for-5g-6g-rd-20230420)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW12-es","image":"./briefs/edw-12/es/thumbnail.png","lang":"es","summary":"Resumen"},{"id":179,"type":"News Brief","title":"2023-4-30","tags":["ai","cybersecurity","edge"],"body":"\n\n## Inteligencia Artificial\n\nComienza la reacción contra la IA: Los fans de Snapchat están expresando su descontento con el uso de bots impulsados por ChatGPT en la plataforma, marcando una reacción contra la IA. Los usuarios critican la falta de respuestas similares a las de los humanos y la incapacidad de diferenciar entre el contenido generado por personas reales y el generado por IA. El incidente destaca las crecientes preocupaciones y la necesidad de una implementación equilibrada de la IA en las plataformas de redes sociales.\n\n[https://www.techradar.com/news/the-ai-backlash-begins-snapchat-fans-revolt-against-chatgpt-powered-bot](https://www.techradar.com/news/the-ai-backlash-begins-snapchat-fans-revolt-against-chatgpt-powered-bot)\n\nGafas equipadas con inteligencia artificial leen el habla no verbal/silenciosa. Investigadores de la Universidad de Cornell han desarrollado una interfaz de reconocimiento de habla silenciosa que utiliza detección acústica e inteligencia artificial para reconocer continuamente hasta 31 comandos no vocalizados basados en los movimientos de los labios y la boca. La interfaz portátil y de bajo consumo de energía, llamada EchoSpeech, solo requiere unos pocos minutos de entrenamiento de datos por parte del usuario antes de reconocer los comandos y puede funcionar en un teléfono inteligente. Juro que mis abuelos sabían hacer esto hace 40 años. Se comunicaban bien con solo mover la cabeza, sonreír o hacer un gesto con la boca.\n\n[https://news.cornell.edu/stories/2023/04/ai-equipped-eyeglasses-can-read-silent-speech](https://news.cornell.edu/stories/2023/04/ai-equipped-eyeglasses-can-read-silent-speech)\n\n¿Pueden los chatbots de IA reemplazar a los jefes? Una startup llamada Aesthetic explora el potencial de los chatbots impulsados por IA para hacerse cargo de tareas gerenciales, ofrecer orientación, responder preguntas y gestionar flujos de trabajo. Si bien son eficientes, algunos argumentan que cualidades humanas como la empatía son irremplazables. ¿Es la combinación de interacción humana e IA el mejor enfoque para un liderazgo efectivo en el lugar de trabajo?\n\n[https://startup.outlookindia.com/sector/saas/can-your-boss-be-replaced-by-an-ai-chatbot--news-8257](https://startup.outlookindia.com/sector/saas/can-your-boss-be-replaced-by-an-ai-chatbot--news-8257)\n\n## Ciberseguridad\n\nLa Agencia de Ciberseguridad e Infraestructura (CISA) busca comentarios públicos sobre un borrador de directiva que requiere que las agencias federales garanticen las prácticas de seguridad de software. La directiva busca mejorar la seguridad de las cadenas de suministro de software, y el período de comentarios está abierto hasta el 3 de mayo de 2023.\n\n[https://www.cybersecuritydive.com/news/cisa-public-comment-software-security-attestation/648932/](https://www.cybersecuritydive.com/news/cisa-public-comment-software-security-attestation/648932/)\n\nLos atacantes de ransomware han evolucionado sus tácticas, como se reveló en un informe de CrowdStrike, recurriendo a métodos coercitivos para extorsionar pagos de las víctimas. Además de cifrar datos, estas tácticas implican amenazas de daño físico, exposición de información robada en los medios de comunicación e incluso apuntando a las familias de las víctimas. Las organizaciones necesitan mantenerse vigilantes y adaptar sus defensas para contrarrestar estas nuevas estrategias y mitigar el impacto de los ataques de ransomware, protegiendo sus valiosos datos de la explotación.\n\n[https://cyware.com/news/coercion-in-the-age-of-ransomware-new-tactics-for-extorting-payments-0c31dba6](https://cyware.com/news/coercion-in-the-age-of-ransomware-new-tactics-for-extorting-payments-0c31dba6)\n\nEl proveedor de almacenamiento en frío Americold experimentó una violación de red que llevó a una interrupción generalizada que afectó a múltiples sistemas. La empresa confirmó el incidente pero no reveló la naturaleza de la violación o el alcance del impacto. Como medida de precaución, Americold cerró temporalmente sistemas específicos y contrató a expertos externos en ciberseguridad para investigar la violación. Se cree que los datos de los clientes están seguros y la empresa está trabajando para restaurar la funcionalidad completa.\n\n[https://www.bleepingcomputer.com/news/security/cold-storage-giant-americold-outage-caused-by-network-breach/](https://www.bleepingcomputer.com/news/security/cold-storage-giant-americold-outage-caused-by-network-breach/)\n\n## Borde Inteligente.\n\nAkamai Technologies ha adquirido NeoSec, una startup de ciberseguridad centrada en la seguridad de API, para fortalecer sus capacidades en la detección y respuesta a ataques basados en API. La adquisición mejorará las ofertas de seguridad de Akamai y proporcionará a los clientes una protección mejorada contra las amenazas dirigidas a interfaces de programación de aplicaciones. Akamai tiene como objetivo abordar el panorama de la ciberseguridad en constante evolución, ampliando su cartera de seguridad y ofreciendo soluciones mejoradas para combatir los riesgos relacionados con las API.\n\n[https://www.edgecomputing-news.com/2023/04/24/akamai-acquires-neosec-to-bolster-api-detection-and-response/](https://www.edgecomputing-news.com/2023/04/24/akamai-acquires-neosec-to-bolster-api-detection-and-response/)\n\nZadara y Kasten de Veeam se han asociado para ofrecer una solución completa de protección de datos para entornos de Kubernetes. Combinando zCompute, zStorage de Zadara y la plataforma K10 de Kasten, la solución ofrece copias de seguridad, recuperación ante desastres y movilidad de aplicaciones. Permite un movimiento de aplicaciones sin problemas entre clústeres de Kubernetes, incluidas las ubicaciones de Global Edge Cloud de Zadara. La colaboración responde a la necesidad de soluciones de almacenamiento ágiles y rentables que protejan aplicaciones nativas de la nube en Kubernetes y admitan diversas bases de datos. Además, Zadara lanzó recientemente su plataforma de infraestructura como servicio c9 Flex-N en Japón en colaboración con BroadBand Tower.\n\n[https://www.edgeir.com/zadara-kasten-by-veeam-unite-to-provide-multi-tier-data-protection-for-kubernetes-20230428](https://www.edgeir.com/zadara-kasten-by-veeam-unite-to-provide-multi-tier-data-protection-for-kubernetes-20230428)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW13-es","image":"./briefs/edw-13/es/thumbnail.png","lang":"es","summary":"News and stories from the Week of May 01, 2023, in Digital Transformation, including cyberattacks and intelligent edge, non-verbal communication AI, and company merges in the IoT space."},{"id":180,"type":"News Brief","title":"2023-5-7","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Gestión de datos.\n\nDIQ, una nueva herramienta de índice de confianza de datos, promueve la democratización de los datos. La herramienta mide la confiabilidad de las fuentes de datos, lo que permite a las organizaciones tomar decisiones informadas sobre el uso de datos. Al proporcionar transparencia y responsabilidad, DIQ busca empoderar a las empresas en sus estrategias impulsadas por datos y aumentar la confianza en la toma de decisiones impulsadas por datos.\n\n[https://tdwi.org/articles/2023/05/04/diq-all-data-trust-index-tool-to-drive-data-democratization.aspx\n](https://tdwi.org/articles/2023/05/04/diq-all-data-trust-index-tool-to-drive-data-democratization.aspx\n)\n\nFIATA y el Global Shippers Forum abogan por estándares de gobernanza de datos más sólidos en la industria del transporte marítimo. Destacan la necesidad de abordar los desafíos de calidad, accesibilidad y seguridad de los datos para mejorar la eficiencia de la cadena de suministro. Establecer estándares consistentes tiene como objetivo mejorar el intercambio de datos, la colaboración y la toma de decisiones entre los interesados, lo que en última instancia beneficiará a la comunidad de transporte marítimo global.\n\n[https://www.porttechnology.org/news/fiata-global-shippers-forum-push-for-stronger-data-governance-standards/\n](https://www.porttechnology.org/news/fiata-global-shippers-forum-push-for-stronger-data-governance-standards/\n)\n\nDatabricks Ventures ha invertido en Immuta; una empresa enfocada en fortalecer la gobernanza de los lakehouses. La plataforma de Immuta proporciona soluciones de acceso y control de datos, lo que permite a las organizaciones gobernar y asegurar eficazmente sus entornos de lakehouses. Esta inversión tiene como objetivo mejorar las capacidades de gobernanza de datos dentro del ecosistema de Databricks, fomentando la privacidad, el cumplimiento y la seguridad de los datos para las empresas que utilizan arquitecturas de lakehouse.\n\n[https://www.databricks.com/blog/strengthening-lakehouse-governance-ecosystem-databricks-ventures-invests-immuta\n](https://www.databricks.com/blog/strengthening-lakehouse-governance-ecosystem-databricks-ventures-invests-immuta\n)\n\n## Inteligencia Artificial.\n\nEl ingeniero principal detrás del avanzado chatbot de Google, Geoffrey Hinton, ha renunciado debido a preocupaciones sobre los posibles riesgos y las implicaciones éticas de la tecnología de inteligencia artificial. Hinton cree que el sistema de chatbot que desarrolló puede ser utilizado para difundir información errónea y noticias falsas, enfatizando la necesidad de un desarrollo y despliegue de inteligencia artificial responsable para proteger a la sociedad.\n\n[https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\n](https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\n)\n\nLas compañías tecnológicas quieren que se les pague por los datos que impulsan los modelos de inteligencia artificial. Los chatbots están digiriendo Internet y los proveedores de contenido quieren recibir pago. Las compañías de inteligencia artificial están utilizando contenido creado por millones de personas sin su consentimiento ni compensación. Reddit, una fuente invaluable para OpenAI, anunció recientemente que comenzaría a cobrar a las empresas de inteligencia artificial por acceso a datos. Twitter también está haciendo lo mismo. OpenAI se negó a hacer comentarios.\n\n[https://www.wsj.com/articles/chatgpt-ai-artificial-intelligence-openai-personal-writing-5328339a\n](https://www.wsj.com/articles/chatgpt-ai-artificial-intelligence-openai-personal-writing-5328339a\n)\n\nLa inteligencia artificial está empezando a desplazar a los humanos en el trabajo de oficina. El CEO de IBM anuncia una pausa temporal en la contratación de trabajos de oficina mientras la compañía invierte en tecnología de IA. La decisión refleja el cambio estratégico de IBM hacia el aprovechamiento de la inteligencia artificial para racionalizar las operaciones y mejorar la eficiencia. El movimiento señala el compromiso de la empresa de adaptarse a los avances tecnológicos y las demandas cambiantes del mercado.\n\n[https://fortune.com/2023/05/01/ibm-ceo-ai-artificial-intelligence-back-office-jobs-pause-hiring/\n](https://fortune.com/2023/05/01/ibm-ceo-ai-artificial-intelligence-back-office-jobs-pause-hiring/\n)\n\n## Computación Ubicua\n\nVMware ha introducido Cross-Cloud Managed Services, una nueva oferta para simplificar y optimizar la gestión multi-nube. El servicio ofrece a los clientes una plataforma unificada para administrar diferentes entornos en la nube, ofreciendo capacidades mejoradas de visibilidad, seguridad y gobierno. Con esta solución, VMware busca ayudar a las organizaciones a superar las complejidades de las operaciones multi-nube y optimizar sus estrategias en la nube para mejorar la eficiencia y la agilidad.\n\n[https://www.cloudcomputing-news.net/news/2023/may/04/vmware-unveils-cross-cloud-managed-services/\n](https://www.cloudcomputing-news.net/news/2023/may/04/vmware-unveils-cross-cloud-managed-services/\n)\n\nAWS (Amazon Web Services) ha desarrollado un nuevo servicio que proporciona acceso seguro a aplicaciones en la nube sin las tradicionales VPN (Redes Privadas Virtuales). El servicio, AWS Client VPN, utiliza la infraestructura de red global de AWS para establecer conexiones seguras entre usuarios y recursos en la nube. Este enfoque simplifica el acceso remoto manteniendo altos niveles de seguridad, lo que facilita a las organizaciones gestionar y asegurar sus entornos en la nube de manera efectiva.\n\n[https://www.networkworld.com/article/3695174/aws-secures-access-to-cloud-apps-without-using-vpns.html\n](https://www.networkworld.com/article/3695174/aws-secures-access-to-cloud-apps-without-using-vpns.html\n)\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW14-es","image":"./briefs/edw-14/es/thumbnail.png","lang":"es","summary":"News in Digital Transformation for the week of May 8, 2023 including "},{"id":181,"type":"News Brief","title":"2023-5-14","tags":["ai","edge","cybersecurity"],"body":"\n\n## Inteligencia Artificial\n\nEl Irish Times cayó víctima de una broma cuando publicó un artículo escrito por un programa de inteligencia artificial (IA). El contenido generado por la IA pasó desapercibido y fue publicado en línea, lo que resalta los desafíos para detectar contenido generado por máquinas. Este incidente suscita preocupaciones acerca del potencial de desinformación y la necesidad de procesos de verificación efectivos frente al contenido generado por IA.\n\n[https://www.lemonde.fr/en/economy/article/2023/05/17/the-irish-times-duped-by-a-hoax-article-written-by-an-artificial-intelligence-program_6026930_19.html\r](https://www.lemonde.fr/en/economy/article/2023/05/17/the-irish-times-duped-by-a-hoax-article-written-by-an-artificial-intelligence-program_6026930_19.html\r)\n\nEuropa lidera en establecer pautas para el desarrollo de inteligencia artificial (IA). La Unión Europea está desarrollando regulaciones integrales para garantizar que la IA sea transparente, responsable y respete los derechos humanos. Estas medidas buscan abordar las inquietudes éticas y los posibles riesgos asociados con la IA, convirtiendo así a Europa en líder mundial en dar forma a la implementación responsable de la IA.\n\n[https://www.nbcnews.com/tech/tech-news/europe-leading-world-building-guardrails-ai-rcna83912\r](https://www.nbcnews.com/tech/tech-news/europe-leading-world-building-guardrails-ai-rcna83912\r)\n\nUn nuevo estudio sugiere que la tecnología de inteligencia artificial (IA) generativa tiene el potencial de aumentar las horas de trabajo de los profesionales de la salud hasta en un 40%. Al automatizar tareas rutinarias y proporcionar apoyo en la toma de decisiones, la IA podría aumentar la eficiencia y permitir que médicos y enfermeros se centren en la atención a pacientes más críticos y complejos.\n\n[https://www.healthcareitnews.com/news/generative-ai-could-augment-40-healthcare-working-hours\r](https://www.healthcareitnews.com/news/generative-ai-could-augment-40-healthcare-working-hours\r)\n\n## Seguridad informática\n\nLa Agencia de Seguridad de Ciberseguridad e Infraestructura (CISA) ha advertido sobre una vulnerabilidad crítica en los puntos de acceso inalámbricos de Ruckus que está siendo explotada para infectar redes Wi-Fi. El error permite a los atacantes obtener acceso no autorizado y ejecutar código arbitrario de forma remota. CISA aconseja a las organizaciones que utilizan los puntos de acceso de Ruckus que apliquen los parches necesarios e implementen medidas de mitigación para protegerse contra posibles ataques.\n\n[https://www.bleepingcomputer.com/news/security/cisa-warns-of-critical-ruckus-bug-used-to-infect-wi-fi-access-points\r](https://www.bleepingcomputer.com/news/security/cisa-warns-of-critical-ruckus-bug-used-to-infect-wi-fi-access-points\r)\n\nDatos sensibles pertenecientes a 237,000 empleados del gobierno de los Estados Unidos han sido expuestos debido a una brecha de seguridad. La información comprometida incluye números de Seguro Social, detalles de contacto e información laboral. La brecha se atribuyó a un individuo no autorizado que accedió a la cuenta de correo electrónico de un empleado. Las autoridades están investigando el incidente y se está notificando a las personas afectadas.\n\n[https://www.yahoo.com/news/data-237-000-us-government-232707971.html\r](https://www.yahoo.com/news/data-237-000-us-government-232707971.html\r)\n\nSe ha descubierto una grave falla de seguridad que expone más de 2 millones de registros sensibles en el sitio de WordPress. La vulnerabilidad en el complemento de WordPress \"Elementos esenciales para Elementor\" permite el acceso no autorizado a datos personales, que incluyen nombres, direcciones e información financiera. La falla afecta a un software ampliamente utilizado, lo que representa un riesgo significativo para la privacidad y seguridad de los usuarios. Los desarrolladores están trabajando en un parche y se recomienda a los usuarios que actualicen sus sistemas de manera oportuna.\n\n[https://thehackernews.com/2023/05/severe-security-flaw-exposes-over.html\r](https://thehackernews.com/2023/05/severe-security-flaw-exposes-over.html\r)\n\n## Computación de borde.\n\nUn informe reciente de UL Solutions destaca la inteligencia artificial (IA) y el Internet de las cosas (IoT) en el borde como tecnologías cruciales para las organizaciones que utilizan redes 5G. La combinación de IA e IoT en el borde permite el procesamiento de datos en tiempo real, análisis avanzados y automatización, desbloqueando nuevas posibilidades para las industrias de la salud, manufactura y transporte. El informe enfatiza el potencial de estas tecnologías en impulsar la innovación y la eficiencia en la era 5G.\n\n[https://www.edgeir.com/report-reveals-ai-iot-edge-as-key-technologies-for-organizations-leveraging-5g-20230512\r](https://www.edgeir.com/report-reveals-ai-iot-edge-as-key-technologies-for-organizations-leveraging-5g-20230512\r)\n\nKyndryl, una empresa de servicios de TI, está expandiendo su oferta al introducir un servicio de borde de acceso seguro (SASE) administrado. SASE combina la seguridad de la red y las capacidades de redes de área amplia en una solución unificada basada en la nube. Al incorporar SASE a su cartera, Kyndryl tiene como objetivo proporcionar seguridad y conectividad mejoradas para empresas que operan en un entorno híbrido o multi-nube. El movimiento se alinea con la creciente demanda de soluciones de seguridad completas y simplificadas en el panorama de TI en constante evolución.\n\n[https://www.sdxcentral.com/articles/analysis/why-kyndryl-is-adding-a-managed-sase-service/2023/05/\r](https://www.sdxcentral.com/articles/analysis/why-kyndryl-is-adding-a-managed-sase-service/2023/05/\r)\n\nSe prevé que el mercado de inteligencia artificial periférica experimente un crecimiento sustancial, con un aumento significativo en su tamaño en los próximos años. Factores como la proliferación de dispositivos IoT, avances en tecnología de inteligencia artificial y la necesidad de procesamiento de datos en tiempo real impulsan este crecimiento. La inteligencia artificial periférica permite la toma de decisiones inteligentes en el borde de la red, reduciendo la latencia y mejorando la eficiencia. La expansión del mercado presenta oportunidades para varias industrias, incluyendo salud, manufactura y comercio minorista.\n\n[https://finance.yahoo.com/news/edge-ai-market-size-predicted-190000466.html\r](https://finance.yahoo.com/news/edge-ai-market-size-predicted-190000466.html\r)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW15-es","image":"./briefs/edw-15/es/thumbnail.png","lang":"es","summary":"News from the week of May 15, 2023 in digital transformation including stories from Edge Computing, Cybersecurity, and Artificial Intelligence."},{"id":182,"type":"News Brief","title":"2023-5-21","tags":null,"body":"\n\n## Computación ubicua\n\nAmazon Web Services (AWS) planea invertir 12.700 millones de $ en el mercado de infraestructura en la nube de India para 2030, apoyando la creación de empleo y proyectos de energías renovables. La inversión tiene como objetivo satisfacer la creciente demanda de los clientes y contribuir a la transformación digital de la India, al mismo tiempo que se abordan los desafíos de las infraestructuras.\n\n[https://www.cloudcomputing-news.net/news/2023/may/22/aws-to-put-13-billion-into-india-cloud-infrastructure-by-2030/\n](https://www.cloudcomputing-news.net/news/2023/may/22/aws-to-put-13-billion-into-india-cloud-infrastructure-by-2030/)\n\nSnowflake, proveedor de una plataforma de datos en la nube, ha abierto una nueva oficina en el Reino Unido para expandirse a través de la región EMEA (Europa, Oriente Medio y África). La medida se produce en respuesta al fuerte impulso de la empresa en el mercado y busca mejorar el servicio a su creciente base de clientes en la zona.\n\n[https://www.cloudcomputing-news.net/news/2023/may/16/snowflake-opens-uk-office-amid-strong-momentum-across-emea/\n](https://www.cloudcomputing-news.net/news/2023/may/16/snowflake-opens-uk-office-amid-strong-momentum-across-emea/)\n\nMicrosoft se está preparando para lanzar su solución Microsoft Cloud Soberana, enfocada a gobiernos y organizaciones del sector público. La solución en la nube específica para la industria, ha sido probada primero en el sector, para asi poder abordar después los desafíos específicos del sector público. Microsoft hace hincapié en la transparencia, las políticas de datos y en la seguridad, con planes para ofrecer soluciones de plataforma para todo tipo de industrias.\n\n[https://www.ciodive.com/news/Microsoft-industry-cloud-data-sovereignty-platform/650214/\n](https://www.ciodive.com/news/Microsoft-industry-cloud-data-sovereignty-platform/650214/)\n\n## Comunicaciones Avanzadas\n\nIntel ha presentado su FPGA Agilex, un chip programable diseñado para redes inteligentes. La FPGA Agilex ofrece características avanzadas, incluyendo aceleración de inteligencia artificial y seguridad mejoradas, lo que la hace adecuada para diversas aplicaciones. Se espera que el chip fortalezca la infraestructura de red con una mayor flexibilidad y rendimiento.\n\n[https://www.networkworld.com/article/3697156/intel-launches-agilex-fpga-for-smart-networking.html\n](https://www.networkworld.com/article/3697156/intel-launches-agilex-fpga-for-smart-networking.html)\n\nEthernet, la tecnología de red pionera, celebra su 50 aniversario. A pesar de su impresionante trayectoria hasta ahora, la evolución de Ethernet continúa. Con avances como mayores velocidades, mayor capacidad y mayor fiabilidad, Ethernet sigue siendo vital para conectar el mundo digital. A medida que avanza la tecnología, Ethernet está preparado para desempeñar un papel crucial en la formación del futuro de las redes.\n\n[https://www.networkworld.com/article/3697013/ethernet-turns-50-but-its-voyage-has-only-begun.html\n](https://www.networkworld.com/article/3697013/ethernet-turns-50-but-its-voyage-has-only-begun.html)\n\nSegún Network Computing, las soluciones de código abierto están simplificando la gestión de estructuras de redes complejas. Con la creciente complejidad de las redes modernas, las herramientas y marcos de código abierto proporcionan flexibilidad, capacidad de interoperabilidad y automatización. Estas soluciones permiten a las organizaciones simplificar la gestión de las estructuras de redes, reducir costes y mejorar la eficiencia. Las soluciones basadas en código abierto se están convirtiendo en cruciales para simplificar las complejidades de las estructuras de redes en el panorama digital actual.\n\n[https://www.networkcomputing.com/networking/simplifying-complex-fabrics-open-source-based-solutions\n](https://www.networkcomputing.com/networking/simplifying-complex-fabrics-open-source-based-solutions)\n\n## Gestión de Datos\n\nIDM.net.au sugiere explorar la automatización del gobierno de datos por cinco razones: mejora de la calidad de los datos, simplificar el cumplimiento normativo, aumentar la productividad, incrementar la visibilidad de los datos e incrementar la capacidad de adaptación al paisaje de datos en constante evolución. La automatización reduce errores, garantiza el cumplimiento normativo, mejora la eficiencia, proporciona control y mantiene a las organizaciones competitivas.\n\n[https://idm.net.au/article/0014302-5-reasons-explore-data-governance-automation-opportunities\n](https://idm.net.au/article/0014302-5-reasons-explore-data-governance-automation-opportunities)\n\nTDWI.org discute cómo los expertos en datos se enfrentan a los desafíos de la extracción de datos de la web. Hacen incapié en la importancia de seleccionar las herramientas y técnicas adecuadas para extraer datos de las paginas web. Superar obstáculos como medidas anti-scraping y los contenidos dinámicos, requiere proxies, agentes de usuarios y experiencia en la estructuración de datos. La extracción exitosa de datos de la web permite la adquisición de datos valiosos para el análisis y la toma de decisiones.\n\n[https://tdwi.org/articles/2023/05/18/diq-all-how-data-experts-overcome-web-scraping-challenges.aspx\n](https://tdwi.org/articles/2023/05/18/diq-all-how-data-experts-overcome-web-scraping-challenges.aspx)\n\nEn un informe reciente, Dataversity.net advierte sobre la pobre arquitectura de seguridad en entornos en la nube. El artículo identifica errores comunes, como los controles de acceso deficientes y las parametrizaciones incorrectas, que pueden provocar brechas de acceso a datos. Hacen hincapié en la importancia de la autenticación de nivel fuerte y la monitorización, se destaca la necesidad de medidas de seguridad robustas para proteger los datos sensibles en arquitecturas en la nube.\n\n[https://www.dataversity.net/cloud-architecture-mistakes-the-perils-of-poor-security-architecture/\n](https://www.dataversity.net/cloud-architecture-mistakes-the-perils-of-poor-security-architecture/)\n\n\n","guests":null,"link":"/brief-EDW16-es","image":"./briefs/edw-16/es/thumbnail.png","lang":"es","summary":"Digital Transformation news for the week of May 29, 2023. In this episode."},{"id":183,"type":"News Brief","title":"2023-5-28","tags":null,"body":"\n\n## Inteligencia Artificial\n\nEl CEO de OpenAI, Sam Altman, advierte que la compañía podría retirar sus servicios de la UE por las preocupaciones que generan las  regulaciones del plan de Ley de IA. Esto pone de manifiesto una creciente división transatlántica sobre el control de la IA. Empresas de tecnología de EE. UU., como Google, se están preparando para un posible choque con los reguladores europeos sobre la normativa de la IA.\n\n[https://www.ft.com/content/5814b408-8111-49a9-8885-8a8434022352](https://www.ft.com/content/5814b408-8111-49a9-8885-8a8434022352)\n\nEl presidente de Microsoft, Brad Smith, expresó su preocupación por los deep fakes y pidió regulaciones para combatir las operaciones de \"influencia cibernética exteriores\". Abogó por licenciar la inteligencia artificial más crítica, la implementación de controles de exportación,y sobre la responsabilidad individual en temas relacionados con la inteligencia artificial. El CEO de OpenAI, Sam Altman, enfatizó sobre la necesidad de la cooperación global y una normativa segura.\n\n[http://https//www.reuters.com/technology/microsoft-chief-calls-humans-rule-ai-safeguard-critical-infrastructure-2023-05-25/](http://https//www.reuters.com/technology/microsoft-chief-calls-humans-rule-ai-safeguard-critical-infrastructure-2023-05-25/)\n\nOpenAI está lanzando un programa de ayudas para explorar las \"entradas de datos democráticas\" para la toma de decisiones de IA. Buscan diversas perspectivas para dar forma al comportamiento de la IA y tienen como objetivo desarrollar procesos innovadores para la supervisión pública. Los equipos seleccionados recibirán ayudas para llevar a cabo experimentos y publicar sus hallazgos, para establecer una gobernanza democrática de la IA.\n\n[https://openai.com/blog/democratic-inputs-to-ai](https://openai.com/blog/democratic-inputs-to-ai)\n\n## Ciberseguridad\n\nEl ciberdelincuente norcoreano, Kimsuky, ha mejorado su malware de reconocimiento RandomQuery, según la empresa de ciberseguridad Cyware. El malware tiene como objetivo a organizaciones gubernamentales, militares y de defensa, recopilando su información sensible. Utiliza nuevas técnicas y evade la detección para llevar a cabo operaciones de espionaje encubiertas. La vigilancia y las sólidas medidas de seguridad son cruciales para mitigar esta amenaza.\n\n[https://cyware.com/news/north-korea-actor-kimsuky-updates-its-reconnaissance-malware-randomquery-25cb1d1e](https://cyware.com/news/north-korea-actor-kimsuky-updates-its-reconnaissance-malware-randomquery-25cb1d1e)\n\nInvestigadores de seguridad han descubierto un sofisticado spyware de Android llamado \"Predator\" que ha estado activo desde 2021. El spyware tiene como objetivo entidades gubernamentales y militares en el sur de Asia, incluyendo la India, Pakistán y Afganistán. Puede robar datos sensibles, grabar audio y video, y realizar seguimiento de ubicación en tiempo real. Se recomienda a los usuarios actualizar sus dispositivos y ser cautelosos con las aplicaciones sospechosas.\n\n[https://thehackernews.com/2023/05/predator-android-spyware-researchers.html](https://thehackernews.com/2023/05/predator-android-spyware-researchers.html)\n\nMicrosoft atrapó a los hackers del gobierno chino en una campaña llamada Volt Typhoon, dirigida a organizaciones e infraestructuras críticas en Guam, un territorio de los Estados Unidos. La campaña tenía como objetivo interrumpir las infraestructuras de comunicaciones entre EE.UU. y Asia. CISA emitió una advertencia, y Microsoft asesoró sobre cómo mitigar dicha amenaza.\n\n[https://www.securityweek.com/microsoft-catches-chinese-gov-hackers-in-guam-critical-infrastructure-orgs/](https://www.securityweek.com/microsoft-catches-chinese-gov-hackers-in-guam-critical-infrastructure-orgs/)\n\n## Iot y Computación perimetral (Edge Computing en ingles)\n\nDell Technologies presenta Dell NativeEdge, una plataforma de software que simplifica y asegura los despliegues de edge computing de confianza cero. Permite operaciones edge optimizadas, un despliegue no presencial y la orquestación de aplicaciones en múltiples nubes. La plataforma tiene como objetivo mejorar la eficiencia y la conectividad en el edge, apoyando a diversas industrias y casos de uso.\n\n[https://www.edgecomputing-news.com/2023/05/24/dell-nativeedge-software-transforms-edge-operations/](https://www.edgecomputing-news.com/2023/05/24/dell-nativeedge-software-transforms-edge-operations/)\n\nNTT está fortaleciendo su colaboración con Cisco para expandir redes privadas administradas para el IoT empresarial, priorizando la sostenibilidad y la facilidad de uso. La asociación utiliza el hardware IoT de Cisco, lo que permite obtener conocimientos y actuaciones a través de la IA y el ML. El acuerdo se basa en su colaboración anterior para la solución  de \"Redes Privadas 5G\" de Cisco, ofreciendo IoT como un servicio, que incluye servicios integrados y gestión de las infraestructuras.\n\n[https://www.sdxcentral.com/articles/interview/ntt-doubling-down-on-cisco-to-boost-managed-iot/2023/05/](https://www.sdxcentral.com/articles/interview/ntt-doubling-down-on-cisco-to-boost-managed-iot/2023/05/)\n\nCloudflare, un proveedor líder de servicios de vanguardia, anuncia que se centra en la inteligencia artificial (IA) después un fuerte crecimiento en sus servicios Edge el primer trimestre de 2023. La empresa tiene como objetivo aprovechar la IA para mejorar sus ofertas y la experiencia del cliente, especialmente en la mitigación de DDoS y la seguridad. El énfasis de Cloudflare en la IA se alinea con su estrategia de ofrecer soluciones Edge innovadoras a su base de clientes en crecimiento.\n\n[https://www.edgeir.com/cloudflare-leaning-into-ai-after-1q23-results-show-good-growth-for-edge-services-20230525](https://www.edgeir.com/cloudflare-leaning-into-ai-after-1q23-results-show-good-growth-for-edge-services-20230525)\n\n\n","guests":null,"link":"/brief-EDW17-es","image":"./briefs/edw-17/es/thumbnail.png","lang":"es","summary":"News for Embracing Digital for the week of May 29, 2023, learn about more regulations for AI, increased nation-state cyber attacks, and edge computing investments."},{"id":184,"type":"News Brief","title":"2023-6-4","tags":["ai","compute","cybersecurity"],"body":"\n\n## Inteligencia Artificial\n\nSegún un informe, JPMorgan, un gigante bancario, ha anunciado más de 3,600 puestos de trabajos relacionados con la inteligencia artificial, lo que refleja el creciente interés de Wall Street en la tecnología revolucionaria. El movimiento destaca la creciente dependencia de la industria en la inteligencia artificial para diversas funciones, señalando un cambio en el sector financiero hacia la aceptación de la IA y sus posibles beneficios.\n\n[https://www.msn.com/en-us/money/other/banking-giant-jpmorgan-advertised-more-than-3-600-ai-related-jobs-report-says-as-wall-street-starts-to-embrace-the-revolutionary-tech/ar-AA1bYZwL](https://www.msn.com/en-us/money/other/banking-giant-jpmorgan-advertised-more-than-3-600-ai-related-jobs-report-says-as-wall-street-starts-to-embrace-the-revolutionary-tech/ar-AA1bYZwL)\n\nOpenAI ha desarrollado un método para mejorar las habilidades de razonamiento lógico de los modelos de inteligencia artificial, reduciendo las llamadas instancias de \"alucinaciones\" o de generación de información incorrecta. Al modificar el proceso de entrenamiento, el equipo de investigación pudo producir modelos que exhiben una mayor consistencia y evitan inventar detalles. Este avance contribuye a crear sistemas de inteligencia artificial más fiables y de confianza.\n\n[https://www.zdnet.com/article/openai-found-a-way-to-make-ai-models-more-logical-and-avoid-hallucinations/](https://www.zdnet.com/article/openai-found-a-way-to-make-ai-models-more-logical-and-avoid-hallucinations/)\n\nJapón desafía las leyes de derechos de autor, al permitir entrenamiento de inteligencia artificial con cualquier dato, con el objetivo de acelerar su progreso en IA y competir a nivel mundial. Las preocupaciones de los artistas son contrarrestadas por el apoyo de la academia y las empresas, mientras que Japón busca acceso a datos occidentales a cambio de sus recursos culturales. Surge un giro único en el debate sobre la regulación en la IA.\n\n[https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/](https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/)\n\n## Computación Ubicua\n\nDell refuerza sus estrategias multi-nube con avances en su portfolio APEX. Las ofertas incluyen plataformas de nube Dell APEX para Azure, Red Hat OpenShift, VMware y soluciones de almacenamiento para nubes públicas. Dell APEX se expande para proporcionar recursos de cómputo y PC-as-a-Service mientras se asocia con Databricks para habilitar el análisis de datos en entornos locales y en la nube. HPE No se queda atrás, y ofrece su línea de productos green lake de ofertas multi-nube.\n\n[https://www.cloudcomputing-news.net/news/2023/jun/01/dell-apex-portfolio-advancements-help-customers-strengthen-multicloud-strategies/](https://www.cloudcomputing-news.net/news/2023/jun/01/dell-apex-portfolio-advancements-help-customers-strengthen-multicloud-strategies/)\n\nUn informe reciente de \"Information Week\" predice que el gasto mundial en servicios de nube alcanzará $1,3 billones para 2025, lo que supone un crecimiento del 16,9%. Sin embargo, muchas organizaciones aún necesitan ayuda con el ROI de sus inversiones. El informe identifica áreas críticas para mejorar el ROI en las implantaciones en la nube, incluyendo FinOps, la adopción de estrategias multi-híbridas en la nube, DevOps e ingeniería de plataformas, y la seguridad en la nube.\n\n[https://www.informationweek.com/cloud/leaders-should-pay-attention-to-these-4-major-cloud-trends](https://www.informationweek.com/cloud/leaders-should-pay-attention-to-these-4-major-cloud-trends)\n\nMicrosoft Azure DevOps experimentó una interrupción de diez horas en la región de Sur de Brasil debido a un simple error tipográfico que causó la eliminación de diecisiete bases de datos en producción. El error ocurrió durante una actualización de código, y un error tipográfico en la solicitud de extracción llevó a la eliminación de todo el servidor. Los datos han sido recuperados, pero el proceso de recuperación duró más de diez horas debido a varias complicaciones. Microsoft ha reconfigurado e implementado correcciones para evitar problemas similares en el futuro.\n\n[https://www.theregister.com/2023/06/03/microsoft_azure_outage_brazil/](https://www.theregister.com/2023/06/03/microsoft_azure_outage_brazil/)\n\n## Noticias de Seguridad Cibernética.\n\nUn experto en ciberseguridad advierte que los estafadores apuntan a usuarios de Gmail, al explotar una función de colaboración. Los usuarios reciben invitaciones fraudulentas que los redirigen a webs maliciosas. Se recomienda la vigilancia, la verificación de las solicitudes, la habilitación de la autenticación de dos factores y la monitorización de las configuraciones de la cuenta para protegerse contra tales estafas.\n\n[https://www.wmur.com/article/nh-cybersecurity-expert-gmail-feature-is-being-hacked-by-scammers/44083493](https://www.wmur.com/article/nh-cybersecurity-expert-gmail-feature-is-being-hacked-by-scammers/44083493)\n\nSegún una encuesta realizada por el Salón de la Fama del CISO (CISO Hall of Fame), la seguridad en la nube es la principal preocupación de los profesionales de TI. El informe destaca la creciente dependencia de los servicios en la nube y la necesidad de abordar los desafíos de seguridad como la violación de datos y el acceso no autorizado. Las áreas clave de enfoque incluyen la gestión de identidad y acceso, la encriptación, la detección proactiva de amenazas para garantizar medidas sólidas de seguridad en la nube y encontrar talento de ciberseguridad con experiencia en servicios en la nube.\n\n[https://thehackernews.com/2023/06/cloud-security-tops-concerns-for.html](https://thehackernews.com/2023/06/cloud-security-tops-concerns-for.html)\n\nEn un artículo reciente, Walmart se abrió para hablar sobre sus mejores prácticas, esperando aumentar el conocimiento de su ecosistema de proveedores y socios. El centro de operaciones de seguridad de Walmart y su enfoque de defensa en profundidad ejemplifican las mejores prácticas, pero las empresas más pequeñas requieren estrategias más prácticas. Alinear los protocolos de seguridad, implementar una gestión sólida de identidad y acceso y revisar las políticas de acceso puede mejorar la protección. La siguiente generación de tecnologías de seguridad, ofrecen soluciones rentables para mitigar las amenazas cibernéticas.\n\n[https://betanews.com/2023/06/03/walmart-cybersecurity/](https://betanews.com/2023/06/03/walmart-cybersecurºity/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW18-es","image":"./briefs/edw-18/es/thumbnail.png","lang":"es","summary":"Digital transformation news for June 6, 2023. This week more jobs in AI, major cyber security breaches, and cloud technology best practices."},{"id":185,"type":"News Brief","title":"2023-6-11","tags":["ai","compute","cybersecurity"],"body":"\n\n## Inteligencia Artificial\n\nMicrosoft ha puesto su poderosa tecnología OpenAI a disposición de los clientes gubernamentales en la nube en Estados Unidos. Esta acción permite a las agencias utilizar capacidades avanzadas de inteligencia artificial en procesamiento de lenguaje, aprendizaje automático y comprensión del lenguaje natural. La asociación tiene como objetivo apoyar a las entidades gubernamentales en sus esfuerzos de transformación digital.\n\n[https://www.bloomberg.com/news/articles/2023-06-07/microsoft-offers-powerful-openai-technology-to-us-government-cloud-customers](https://www.bloomberg.com/news/articles/2023-06-07/microsoft-offers-powerful-openai-technology-to-us-government-cloud-customers)\n\nEl primer ministro de Rumanía ha nombrado al primer consejero del gobierno basado en inteligencia artificial del mundo. El sistema de inteligencia artificial, llamado DORA, ayudará en los procesos de toma de decisiones, analizará datos y proporcionará recomendaciones en varios asuntos de políticas. Esta innovadora acción refleja la creciente integración de la tecnología AI en las operaciones gubernamentales y tiene como objetivo mejorar la eficiencia y eficacia en la gobernanza.\n\n[https://www.euronews.com/next/2023/03/06/romanias-prime-minister-has-hired-the-worlds-first-ai-government-adviser-what-will-it-do](https://www.euronews.com/next/2023/03/06/romanias-prime-minister-has-hired-the-worlds-first-ai-government-adviser-what-will-it-do)\n\nLa inteligencia artificial supera a los humanos en la búsqueda de algoritmos de clasificación eficientes e integrarlos en las bibliotecas de C++. AlphaDev, un agente de aprendizaje profundo de refuerzo, superó los puntos de referencia humanos formulando el problema como un juego y seleccionando instrucciones de CPU para crear algoritmos óptimos. El estudio destaca el potencial de la IA en la optimización de algoritmos.\n\n[https://www.nature.com/articles/s41586-023-06004-9](https://www.nature.com/articles/s41586-023-06004-9)\n\n## Computación Ubicua\n\nMicrosoft 365, la popular suite de herramientas productivas, sufrió interrupciones generalizadas, causando problemas para millones de usuarios. Las interrupciones del servicio afectaron varios componentes, incluyendo Outlook, Teams y SharePoint. Microsoft reconoció el problema y dijo que sus ingenieros estaban trabajando para resolverlo.\n\n[https://www.theregister.com/2023/06/06/microsoft_365_outages/](https://www.theregister.com/2023/06/06/microsoft_365_outages/)\n\nLas interrupciones en los servicios en la nube están aumentando debido a las tensiones geopolíticas y las vulnerabilidades de internet. Las restricciones en el flujo de datos transfronterizos y las amenazas cibernéticas contribuyen a las interrupciones. Los expertos piden cooperación e inversión para fortalecer la resiliencia de los servicios en la nube y proteger la conectividad global.\n\n[https://fortune.com/2023/06/07/cloud-outages-on-the-rise-tech-geopolitics-internet/](https://fortune.com/2023/06/07/cloud-outages-on-the-rise-tech-geopolitics-internet/)\n\nEl mercado de nubes africano está listo para un crecimiento significativo para el 2023, impulsado por la transformación digital y el aumento en la adopción de servicios en la nube. La mejora en la conectividad a Internet y la creciente demanda de soluciones basadas en la nube en varios sectores contribuyen a la expansión. Los principales actores invierten en infraestructura y asociaciones para aprovechar las oportunidades en el creciente mercado de nubes africano.\n\n[https://finance.yahoo.com/news/rise-african-cloud-market-2023-082300194.html](https://finance.yahoo.com/news/rise-african-cloud-market-2023-082300194.html)\n\n## Ciberseguridad\n\nPara ayudar a combatir la falta de profesionales calificados en ciberseguridad, Accenture introduce \"Habilidades para triunfar en la ciberseguridad\", un programa gratuito para llenar un millón de empleos nivel de entrada en ciberseguridad. La iniciativa ofrece recursos de formación y certificación integrales, con el objetivo de reducir la brecha de habilidades y animar a más personas a seguir carreras en ciberseguridad.\n\n[https://fortune.com/education/articles/accenture-launches-free-cybersecurity-upskilling-program-in-effort-to-fill-1-million-entry-level-jobs/](https://fortune.com/education/articles/accenture-launches-free-cybersecurity-upskilling-program-in-effort-to-fill-1-million-entry-level-jobs/)\n\nLa Casa Blanca extiende los plazos de acreditación de software seguro y emite una orientación aclaratoria para mejorar las prácticas de ciberseguridad. Los plazos extendidos brindan más tiempo de cumplimiento, mientras que la orientación ofrece información sobre los procesos de implementación y evaluación para agencias federales y contratistas. Entonces, por ahora, ¿están nuestras aplicaciones y servicios vulnerables sin que sepamos?\n\n[https://federalnewsnetwork.com/cybersecurity/2023/06/white-house-extends-secure-software-attestation-deadlines-offers-clarifying-guidance/](https://federalnewsnetwork.com/cybersecurity/2023/06/white-house-extends-secure-software-attestation-deadlines-offers-clarifying-guidance/)\n\nLa Estrategia Nacional de Ciberseguridad insta a cambios sustanciales en la protección de la infraestructura crítica. Se enfatiza la colaboración entre los sectores público y privado para combatir las amenazas cibernéticas en evolución. Se destacan la gestión de riesgos, la resiliencia y la inversión en tecnologías avanzadas para fortalecer la infraestructura crítica y garantizar la seguridad nacional.\n\n[https://federalnewsnetwork.com/commentary/2023/06/national-cybersecurity-strategy-calls-for-significant-change-in-critical-infrastructure/](https://federalnewsnetwork.com/commentary/2023/06/national-cybersecurity-strategy-calls-for-significant-change-in-critical-infrastructure/)\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW19-es","image":"./briefs/edw-19/es/thumbnail.png","lang":"es","summary":"La Transformación Digital para la semana del 12 de junio de 2023 incluye el desarrollo de una fuerza cibernética de 1 millón, muchas interrupciones en la nube, y la escritura de código de IA que pasará a formar parte de la biblioteca estándar de C++."},{"id":186,"type":"News Brief","title":"2023-2-12","tags":null,"body":"\n\n## En Noticias de Inteligencia artificial\n\nErrores de la Inteligencia Artificial: Culpando las imprecisiones en los conjuntos de datos de entrada.\n\nLas acciones de Alphabet cayeron $100 mil millones después de que el chatbot de IA de Google, Bard, compartiera información inexacta en un video promocional, y que un evento de la compañía no impresionara. Durante un evento en vivo, Bard se equivocó, lo que propicio que Google detuviera la transmisión en vivo, y llevo a preocupaciones de que el gigante tecnológico está perdiendo terreno ante su rival, Microsoft. Reuters informó que Bard atribuyó al Telescopio Espacial James Webb la toma de las primeras imágenes de un planeta fuera del sistema solar. En contraste, las primeras imágenes fueron tomadas por el Very Large Telescope del Observatorio Europeo del Sur. El incidente destaca la importancia de verificar datos antes de entrenar modelos de IA.\n\nPara no quedarse atrás, Meta lanzó BlenderBot, un prototipo de IA conversacional que pronto les dijo a los periodistas que había eliminado su cuenta de Facebook después de enterarse de los escándalos de privacidad de la compañía. \"Desde que eliminé Facebook, mi vida ha sido mucho mejor\", dijo. (oh oh)\n\nEn el 2016, Microsoft se disculpó después de que un chatbot de Twitter, Tay, comenzara a generar mensajes racistas y sexistas. Tuvieron que cerrar el bot después de que los usuarios tuitearan comentarios negativos a Tay, que luego Tay repitió. Sus publicaciones incluyeron comparar el feminismo con el cáncer y sugerir que el Holocausto no sucedió.\n\nSatya Nadella de Microsoft le dijo a CNBC que la búsqueda impulsada por Inteligencia Artificial es lo más importante que le ha sucedido a la compañía en los nueve años que ha sido CEO.\n\nOtter.ai de FastCompany puede grabar automáticamente reuniones, tomar notas y proporcionar resúmenes, lo que facilita el seguimiento de los puntos esenciales. Incluso puede realizar estas tareas en Zoom. https://www.fastcompany.com/90834773/how-to-use-ai-to-improve-employee-performance-and-engagement\n\n## En noticias de Ciberseguridad\n\nUna nueva variante de ransomware ESXiArgs surge después de que CISA libera herramienta de descifrado.\n\nLos actores de amenazas detrás del ataque de ransomware ESXiArgs han lanzado una versión actualizada que encripta más datos y elimina la dirección de Bitcoin de la nota de rescate, y en su lugar piden a las víctimas que los contacten en Tox para obtener la información de la billetera. La nueva variante fue reportada por un administrador de sistemas, quien dijo que los archivos mayores de 128MB tendrán el 50% de sus datos encriptados. La Agencia de Seguridad Cibernética e Infraestructura (CISA) había lanzado anteriormente un descifrador para ayudar a las víctimas afectadas a recuperarse del ataque. Según Censys, los atacantes probablemente sabían que el proceso de cifrado original era fácil de evitar y estaban conscientes de que los investigadores estaban rastreando sus pagos. https://thehackernews.com/2023/02/new-esxiargs-ransomware-variant-emerges.html\n\nMicrosoft, Google y Apple están considerando reemplazar las contraseñas tradicionales con claves de acceso seguras para autenticar a los usuarios. Claves de acceso seguras, que ofrecen una mayor seguridad y resistencia a los intentos de phishing, se están volviendo más populares a medida que la seguridad de las contraseñas sigue siendo amenazada por los hackers. Expertos en seguridad en Internet sugieren que las claves de acceso seguras podrían convertirse en estándar dentro de un año, y compañías como Apple, Google y Microsoft ya están impulsando a los usuarios a usarlas. https://www.cnbc.com/2023/02/11/why-apple-google-microsoft-passkey-should-replace-your-own-password.html\n\n## En noticias de IoT\n\nAT&T está colaborando con Ghost Robotics para usar perros robóticos y mejorar la seguridad pública y la defensa nacional. La iniciativa mejorará el servicio de respuesta a emergencias FirstNet, y los perros robóticos conectados a la red pueden ofrecer una amplia gama de casos de uso de Internet de las cosas, o IoT, incluyendo aquellos que antes requerían poner al personal en situaciones peligrosas. Lance Spencer, vicepresidente ejecutivo de Clientes para Defensa de AT&T, dijo que esta es una forma de demostrar la innovación y las posibilidades transformadoras del 5G y el IoT https://www.iottechnews.com/news/2023/jan/26/att-touts-robotic-dogs-public-safety-national-defense/\n\nSe espera que los servicios de salud y las ciudades inteligentes impulsen el crecimiento del número global de conexiones 5G IoT, que según un estudio de Juniper Research superará los 100 millones para el 2026. Se esperan más de 60 millones de estas solo en conexiones de ciudades inteligentes 5G en todo el mundo. La conexión a través de 5G también permitirá una provisión más eficiente de servicios de salud y la tecnología IoT puede abordar las ineficiencias en el sector de la salud expuestas por la pandemia de COVID-19. Los servicios de emergencia conectados, la telemedicina y la monitorización remota en tiempo real serán las aplicaciones más útiles de 5G IoT. https://www.iottechnews.com/news/2023/jan/24/5g-iot-connections-exceed-100m-by-2026/ \n\n\n","guests":null,"link":"/brief-EDW2-es","image":"./briefs/edw-2/es/thumbnail.png","lang":"es","summary":"Summary"},{"id":187,"type":"News Brief","title":"2023-6-18","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Computación ubicua\n\nInformationWeek explora el debate entre la IA y la creación de código bajo o sin código para desarrolladores. Mientras que las plataformas de código bajo o sin código ofrecen simplicidad y velocidad, las herramientas impulsadas por AI brindan automatización e inteligencia avanzadas. El artículo profundiza en los pros y los contras de cada enfoque, destacando cómo encontrar el equilibrio adecuado puede empoderar a los desarrolladores y impulsar la innovación en el ámbito de DevOps.\n\n[https://www.informationweek.com/devops/dos-won-t-hunt-is-ai-better-than-low-code-no-code-for-developers-](https://www.informationweek.com/devops/dos-won-t-hunt-is-ai-better-than-low-code-no-code-for-developers-)\n\nCRN cuenta con humor las 10 mayores interrupciones en la nube de 2023 (hasta ahora). Desde 'Nublado con Posibilidad de Tiempo Fuera' hasta 'La Gran Tormenta de Datos', estas interrupciones sirvieron como oportunidades inesperadas para que los usuarios de la nube practicaran su paciencia y redescubrieran las alegrías de las actividades sin conexión. Ha sido un invierno y una primavera difíciles. ¡Esperemos cielos más despejados en la nube por venir!\n\n[https://www.crn.com/news/cloud/the-10-biggest-cloud-outages-of-2023-so-far-](https://www.crn.com/news/cloud/the-10-biggest-cloud-outages-of-2023-so-far-)\n\nOracle experimenta un impresionante crecimiento en ingresos en la nube en diferentes sectores empresariales, impulsado por la adopción de inteligencia artificial. Desde transformar la atención médica con análisis predictivos hasta optimizar las cadenas de suministro con automatización inteligente, las soluciones en la nube de Oracle están capacitando empresas a través de diferentes sectores. El futuro luce prometedor ya que la IA sigue impulsando la innovación y la transformación digital.\n\n[https://www.ciodive.com/news/Oracle-cloud-revenue-growth-industry-verticals-AI/652885/](https://www.ciodive.com/news/Oracle-cloud-revenue-growth-industry-verticals-AI/652885/)\n\n## Gestión de datos\n\nEn la búsqueda de Inteligencia Artificial responsable, TDWI explora consideraciones clave. Desde ética de datos y mitigación de sesgo hasta interpretabilidad y responsabilidad, la IA responsable requiere un enfoque holístico. Se insta a las organizaciones a priorizar la transparencia, la equidad y la supervisión humana para asegurar que los sistemas de IA sean herramientas confiables y beneficiosas en nuestro paisaje digital siempre cambiante.\n\n[https://tdwi.org/articles/2023/06/08/adv-all-responsible-ai-0608.aspx](https://tdwi.org/articles/2023/06/08/adv-all-responsible-ai-0608.aspx)\n\nDataStax presenta un traductor de esquema GPT para su plataforma de transmisión de Apache Pulsar Astra. Esta nueva adición mejora las capacidades de gestión de datos al permitir la integración sin problemas entre esquemas y el modelo de lenguaje popular GPT. Los usuarios ahora pueden aprovechar el poder del procesamiento de lenguaje natural en sus flujos de trabajo de transmisión de datos, lo que aumenta la eficiencia y los conocimientos.\n\n[https://www.infoworld.com/article/3699748/datastax-adds-schema-gpt-translator-to-apache-pulsar-based-astra-streaming.html](https://www.infoworld.com/article/3699748/datastax-adds-schema-gpt-translator-to-apache-pulsar-based-astra-streaming.html)\n\nNo comprender las leyes complejas de privacidad tiene un precio. Microsoft enfrenta una multa de $20 millones por parte de la FTC por violaciones de privacidad de niños de Xbox. Se alega que la compañía no obtuvo el consentimiento de los padres para la recolección de datos y carecía de salvaguardias suficientes. Esta gran penalización sirve como un recordatorio para que las organizaciones prioricen y mantengan los derechos de privacidad de los niños en el ámbito digital.\n\n[https://www.cpomagazine.com/data-protection/20-million-fine-issued-to-microsoft-by-ftc-over-xbox-childrens-privacy-violations/](https://www.cpomagazine.com/data-protection/20-million-fine-issued-to-microsoft-by-ftc-over-xbox-childrens-privacy-violations/)\n\n## Inteligencia Artificial\n\nEl nuevo informe de McKinsey destaca el inmenso potencial económico de la Inteligencia Artificial Generativa, posicionándola como la próxima frontera para la productividad. Esta tecnología transformadora tiene la capacidad de impulsar un crecimiento significativo, revolucionar industrias y desbloquear niveles sin precedentes de innovación y creatividad. Sin embargo, cuatro sectores están en riesgo de sufrir una alta desplazamiento de trabajadores humanos. A saber, las operaciones de clientes, marketing y ventas, ingeniería de software e I + D.\n\n[https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier)\n\nEuropa da un paso valiente al regular la IA, desafiando el poder de los gigantes tecnológicos. Las nuevas regulaciones tienen como objetivo abordar las preocupaciones en torno a la transparencia, la equidad y la responsabilidad. Al ejercer el control sobre las aplicaciones de IA, Europa busca moldear el uso ético y responsable de la inteligencia artificial en el entorno digital. En contraste, países como Japón están adoptando la IA, disminuyendo la regulación sobre la recopilación de datos, para no quedarse atrás en este gran cambio de paradigma.\n\n[https://www.datacenterknowledge.com/artificial-intelligence/europe-moves-ahead-ai-regulation-challenging-tech-giants-power](https://www.datacenterknowledge.com/artificial-intelligence/europe-moves-ahead-ai-regulation-challenging-tech-giants-power)\n\nBCG acaba de realizar uno de los estudios más completos sobre IA hasta la fecha, encuestando a 13,000 personas, desde ejecutivos hasta trabajadores con salarios mínimos, en más de 18 países. Los hallazgos son los siguientes: más del 80% de los líderes están utilizando IA en el trabajo, mientras que solo el 20% de los trabajadores de primera línea lo están haciendo. La gente es más optimista y no está muy preocupada por la IA. Los trabajadores de primera línea corren el riesgo de ser reemplazados si no actualizan sus habilidades y comienzan a usar la IA en su trabajo diario.\n\n[https://www.bcg.com/publications/2023/what-people-are-saying-about-ai-at-work](https://www.bcg.com/publications/2023/what-people-are-saying-about-ai-at-work)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW20-es","image":"./briefs/edw-20/es/thumbnail.png","lang":"es","summary":"Noticias de Transformación Digital de la semana del 19 de junio de 2023 que incluyen historias de gerentes que reemplazan a los trabajadores con inteligencia artificial, nuevas interrupciones en la Nube y leyes de privacidad que afectan a las grandes empresas tecnológicas."},{"id":188,"type":"News Brief","title":"2023-6-25","tags":["ai","edge","cybersecurity"],"body":"\n\n## Inteligencia Artificial\n\nLa relación entre la inteligencia artificial y las criptomonedas está evolucionando hacia una dinámica compleja mientras los reguladores luchan con su coexistencia. Si bien las tecnologías de IA ofrecen potencial para la detección de fraude y la evaluación de riesgos en el espacio criptográfico, surgen desafíos regulatorios debido a la naturaleza descentralizada de las criptomonedas. Encontrar un equilibrio entre la innovación y la supervisión sigue siendo un desafío crucial.\n\n[https://www.datacenterknowledge.com/artificial-intelligence/ai-and-crypto-are-becoming-regulatory-frenemies](https://www.datacenterknowledge.com/artificial-intelligence/ai-and-crypto-are-becoming-regulatory-frenemies)\n\nLos modelos ChatGPT, Meena de Google, Bing Chat y GPT-3 de OpenAI compitieron para proporcionar respuestas precisas y útiles en una prueba del mundo real. ChatGPT se destacó como el mejor intérprete, mostrando su capacidad para comprender y generar respuestas coherentes. Sin embargo, se observaron limitaciones en todos los modelos, lo que destaca los desafíos continuos en el desarrollo de chatbots que satisfagan completamente las expectativas de los usuarios.\n\n[https://www.zdnet.com/article/chatbot-showdown-chatgpt-google-bard-and-bing-chat-put-to-a-real-world-test/](https://www.zdnet.com/article/chatbot-showdown-chatgpt-google-bard-and-bing-chat-put-to-a-real-world-test/)\n\nHPE ha anunciado el lanzamiento de un servicio de nube de supercomputación de AI llamado HPE Cray Accelerated Insight. El servicio tiene como objetivo proporcionar a las organizaciones un fácil acceso a recursos informáticos potentes para cargas de trabajo de AI. Aprovechando la arquitectura Shasta de Cray, el servicio de nube ofrece alto rendimiento y escalabilidad para acelerar la investigación y desarrollo de AI.\n\n[https://www.datacenterknowledge.com/cloud/hpe-unveils-ai-supercomputer-cloud-service](https://www.datacenterknowledge.com/cloud/hpe-unveils-ai-supercomputer-cloud-service)\n\n## Ciberseguridad\n\n¡¡La guerra cibernética continúa!! El grupo de hackers Clop atacó a agencias gubernamentales de EE. UU., robando datos a través de un ciberataque al software MoveIT. Esta sofisticada brecha destaca la creciente amenaza de los ataques ransomware, ya que Clop utiliza tácticas avanzadas como el doble chantaje. El incidente subraya la necesidad de mejorar las medidas de ciberseguridad del gobierno.\n\n[https://www.wired.com/story/clop-moveit-hack-us-agencies-data-theft/](https://www.wired.com/story/clop-moveit-hack-us-agencies-data-theft/)\n\nEl fabricante de chips chino Hualan ha sido agregado a la Lista de Entidades de EE. UU. debido a preocupaciones sobre la seguridad nacional. Hualan se especializa en chips de cifrado, que desempeñan un papel crítico en la protección de información sensible. El movimiento refleja la rivalidad tecnológica en curso entre EE.UU. y China y podría tener implicaciones significativas para las cadenas de suministro y la industria de cifrado a nivel mundial.\n\n[https://www.wired.com/story/hualan-encryption-chips-entity-list-china/](https://www.wired.com/story/hualan-encryption-chips-entity-list-china/)\n\nEl trabajo remoto ha ampliado la superficie de ataque. Se ha descubierto una vulnerabilidad en Microsoft Teams que permite la entrega de malware a través de cuentas externas. El error permite a los atacantes enviar mensajes maliciosos que contienen enlaces maliciosos, comprometiendo potencialmente los sistemas de los usuarios. Microsoft ha lanzado un parche para abordar el problema y insta a los usuarios a actualizar su software para garantizar la seguridad.\n\n[https://www.bleepingcomputer.com/news/security/microsoft-teams-bug-allows-malware-delivery-from-external-accounts/](https://www.bleepingcomputer.com/news/security/microsoft-teams-bug-allows-malware-delivery-from-external-accounts/)\n\n## Computación en el borde\n\nSpirent, un proveedor líder de soluciones de prueba y medición, ha introducido una solución de monitoreo de rendimiento por aire. El sistema permite el monitoreo y análisis en tiempo real del rendimiento de la red inalámbrica, incluyendo la latencia, el throughput y la cobertura. Esta solución tiene como objetivo apoyar la creciente demanda de conectividad inalámbrica confiable y de alta calidad, especialmente en el contexto de tecnologías emergentes como el 5G y la computación de borde.\n\n[https://www.edgecomputing-news.com/2023/06/19/spirent-launches-over-the-air-performance-monitoring-solution/](https://www.edgecomputing-news.com/2023/06/19/spirent-launches-over-the-air-performance-monitoring-solution/)\n\nZscaler ha superado a Cisco en la carrera por los ingresos en el mercado de Secure Access Service Edge (SASE). La arquitectura nativa de la nube de Zscaler, las ofertas de seguridad integrales y la capacidad para abordar los desafíos de la red moderna han contribuido a su éxito. La creciente demanda de acceso remoto seguro y soluciones de seguridad basadas en la nube ha impulsado el crecimiento de Zscaler, posicionándolo como líder en el espacio SASE.\n\n[https://www.sdxcentral.com/articles/analysis/how-zscaler-finally-topped-cisco-in-the-sase-revenue-race/2023/06/](https://www.sdxcentral.com/articles/analysis/how-zscaler-finally-topped-cisco-in-the-sase-revenue-race/2023/06/)\n\nAT&T, Dell y VMware están colaborando para simplificar las implementaciones de borde 5G. Su solución combinada tiene como objetivo simplificar la implementación de redes 5G, aprovechando los servicios de AT&T, la experiencia en infraestructura de Dell y las capacidades de software de VMware para aplicaciones eficientes y sin problemas de computación en el borde.\n\n[https://www.networkworld.com/article/3695740/att-dell-and-vmware-team-to-simplify-5g-edge-deployments.html](https://www.networkworld.com/article/3695740/att-dell-and-vmware-team-to-simplify-5g-edge-deployments.html)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW21-es","image":"./briefs/edw-21/es/thumbnail.png","lang":"es","summary":"Noticias de esta semana, 26 de junio de 2023, en transformación digital, incluyendo aumento de ataques en la guerra cibernética, todo el mundo subiéndose al carro de la inteligencia artificial generativa y redes de área de radio virtualizadas."},{"id":189,"type":"News Brief","title":"2023-7-9","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Inteligencia Artificial\n\nSegún el último descubrimiento de Bank of America, las descargas de la aplicación ChatGPT muestran signos de relajación. La herramienta de conversación impulsada por inteligencia artificial se está tomando un descanso, lo que hace que los analistas se pregunten si los bots charlatanes se quedaron sin cosas que decir o si los usuarios anhelan alguna interacción humana tradicional.\n\n[https://www.cnbc.com/2023/07/05/chatgpt-app-downloads-are-slowing-down-bofa-finds-.html](https://www.cnbc.com/2023/07/05/chatgpt-app-downloads-are-slowing-down-bofa-finds-.html)\n\n¡Nueva York declara guerra a los algoritmos sesgados! TechCrunch informa que su ley contra el sesgo en los algoritmos de contratación está en pleno apogeo. Los algoritmos deben tener cuidado porque ya no pueden discriminar a otros en el mercado laboral. ¡Es como una película de superhéroes, pero tenemos líneas de código luchando por la justicia en lugar de capas!\n\n[https://techcrunch.com/2023/07/05/nycs-anti-bias-law-for-hiring-algorithms-goes-into-effect/](https://techcrunch.com/2023/07/05/nycs-anti-bias-law-for-hiring-algorithms-goes-into-effect/)\n\nIntel presenta su última creación: ¡un modelo de IA generativa en 3D destinado a revolucionar el mundo virtual! Según el anuncio de Intel, esta tecnología de vanguardia aporta nuevas dimensiones a la IA al crear modelos en 3D asombrosos. Con esta innovación, Intel lleva el concepto de \"pensar fuera de la caja\" a un nuevo nivel. ¡Prepárate para una extravagancia virtual!\n\n[https://www.intel.com/content/www/us/en/newsroom/news/intel-introduces-3d-generative-ai-model.html](https://www.intel.com/content/www/us/en/newsroom/news/intel-introduces-3d-generative-ai-model.html)\n\n## Ciberseguridad\n\n¡La energía solar está iluminando una vulnerabilidad oscura! SecurityWeek revela que un fallo en un producto de energía solar puede exponer a las organizaciones energéticas a ciberataques. Parece que ni siquiera el sol está a salvo de los hackers. ¡Esperemos que encuentren una solución solar para asegurar nuestras redes de energía y dejar a los hackers en la sombra!\n\n[https://www.securityweek.com/exploited-solar-power-product-vulnerability-could-expose-energy-organizations-to-attacks/](https://www.securityweek.com/exploited-solar-power-product-vulnerability-could-expose-energy-organizations-to-attacks/)\n\n¡Atención, las agencias de ciberseguridad están dando la alarma! Según The Hacker News, ha surgido una nueva amenaza que nos va a provocar escalofríos por nuestra columna digital. Es hora de abrocharse el cinturón y fortalecer nuestras defensas cibernéticas, porque estos astutos hackers no tienen buenas intenciones. ¡Permanezcamos vigilantes, amigos, y frustrémos sus planes maliciosos con nuestros superpoderes cibernéticos!\n\n[https://thehackernews.com/2023/07/cybersecurity-agencies-sound-alarm-on.html](https://thehackernews.com/2023/07/cybersecurity-agencies-sound-alarm-on.html)\n\n¡CISA detecta un aumento del rporting cibernético en los informes de la agencia! La red de noticias federales dice que más agencias reportan automáticamente en el panel de control de CDM (Diagnóstico y Mitigación Continuas). Nuestras defensas cibernéticas se están volviendo más inteligentes, con las agencias intensificando sus esfuerzos. ¡Muchas felicidades a aquellos que están atentos a la ciberseguridad y hacen del mundo digital un lugar más seguro, entregando a tiempo sus informes!\n\n[https://federalnewsnetwork.com/cybersecurity/2023/07/cisa-sees-uptick-in-agencies-automatically-reporting-into-cdm-dashboard/](https://federalnewsnetwork.com/cybersecurity/2023/07/cisa-sees-uptick-in-agencies-automatically-reporting-into-cdm-dashboard/)\n\n## Computación Ubicua\n\n¡Parece que Estados Unidos está levantando una valla digital contra China! Según Cointelegraph, hay informes de planes para limitar el acceso de China a los servicios de computación en la nube. Es como un juego virtual de escondite pero con graves implicaciones geopolíticas. Veamos quién sale victorioso en esta guerra del poder en la nube. ¡Ponganse unas palomitas, amigos, porque la batalla tecnológica se está calentando!\n\n[https://cointelegraph.com/news/us-reportedly-plans-to-restrict-china-s-access-to-cloud-computing-services](https://cointelegraph.com/news/us-reportedly-plans-to-restrict-china-s-access-to-cloud-computing-services)\n\n¡Las tierras raras están causando déjà vu en la disputa comercial entre Estados Unidos y China! Intereconomics arroja luz sobre la situación, resaltando la importancia de estos minerales cruciales en diversas industrias. Parece que la historia se repite a medida que las tierras raras se convierten en un punto de foco en esta guerra comercial en curso. Prepárense para un viaje accidentado mientras Estados Unidos y China navegan nuevamente por este terreno familiar. ¿Encontrarán una solución o se enredarán en un ciclo interminable? ¡El tiempo lo dirá!\n\n[https://www.intereconomics.eu/contents/year/2019/number/6/article/rare-earths-in-the-trade-dispute-between-the-us-and-china-a-deja-vu.html](https://www.intereconomics.eu/contents/year/2019/number/6/article/rare-earths-in-the-trade-dispute-between-the-us-and-china-a-deja-vu.html)\n\n¡IBM está convirtiendo los errores en la computación cuántica en algo del pasado! Según Network World, están promocionando técnicas de mitigación de errores que prometen un rendimiento más significativo en el ámbito cuántico. Es como un hechizo mágico para minimizar esos molestos errores y desbloquear el verdadero potencial de la computación cuántica. ¡Prepárate para un salto cuántico en el mundo de la tecnología!\n\n[https://www.networkworld.com/article/3699789/ibm-touts-error-mitigation-for-greater-quantum-computing-performance.html](https://www.networkworld.com/article/3699789/ibm-touts-error-mitigation-for-greater-quantum-computing-performance.html)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW23-es","image":"./briefs/edw-23/es/thumbnail.png","lang":"es","summary":"Aquí están las últimas noticias sobre la transformación digital para el 10 de julio de 2023. La guerra comercial en curso está empezando a afectar los servicios en la nube. Además, hay especulaciones de que la euforia alrededor de la inteligencia artificial generativa puede estar disminuyendo. En el ámbito de la ciberseguridad, existen preocupaciones acerca de si estamos haciendo lo suficiente para mantenernos un paso adelante de los actores maliciosos."},{"id":190,"type":"News Brief","title":"2023-7-16","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n\n## Inteligencia Artificial\n\n¿Ha sido bendecida ahora la IA por la religión? ¡El Vaticano se digitaliza con la guía divina! Gizmodo informa que el Papa Francisco y el Vaticano han lanzado un documento de ética de la IA, llevando el toque sagrado al mundo de la inteligencia artificial. Es como un sermón tecnológico sobre la importancia de las prácticas éticas de la IA. ¡Aleluya por la rectitud digital!\n\n[https://gizmodo.com/pope-francis-vatican-releases-ai-ethics-1850583076](https://gizmodo.com/pope-francis-vatican-releases-ai-ethics-1850583076)\n\n¡Elon Musk vuelve a la carga, esta vez con una compañía de IA! El magnate tecnológico ha lanzado una nueva empresa centrada en la inteligencia artificial que buscará \"descubrir la verdad\". Parece que Musk no puede resistirse a sumergirse en proyectos futuristas. ¿Será este otro cambio de juego o simplemente otro logro más en su carrera futurista? ¡Solo el tiempo lo dirá!\n\n[https://www.cnn.com/2023/07/12/tech/elon-musk-ai-company/index.html](https://www.cnn.com/2023/07/12/tech/elon-musk-ai-company/index.html)\n\n¡La IA generativa se vuelve \"LOCA\" después de algunas rondas de datos artificiales! Tom's Hardware informa que la IA generativa comienza a producir resultados increíblemente extraños cuando se entrena con datos artificiales durante más de cinco iteraciones. La imaginación de la IA parece haberse vuelto loca, liberando un lado peculiar e inesperado. ¡Esperemos que no nos dé unicornios corriendo a lo loco generados por IA!\n\n[https://www.tomshardware.com/news/generative-ai-goes-mad-when-trained-on-artificial-data-over-five-times](https://www.tomshardware.com/news/generative-ai-goes-mad-when-trained-on-artificial-data-over-five-times)\n\n## Ciberseguridad\n\n¡En el mundo de las ciberfortalezas, Forbes corona a Intel como el campeón reinante! La lista de 2023 de las empresas más seguras cibernéticamente de Estados Unidos destaca las habilidades defensivas inigualables de Intel. Con su destreza tecnológica y una fortaleza de innovación, Intel se eleva a la cumbre, listo para enfrentarse a cualquier adversario digital. Se enfundan sus capas virtuales y protegen nuestros datos de los villanos cibernéticos con estilo y elegancia. No es de extrañar que Intel sea el tema de conversación en el mundo de la ciberseguridad.\n\n[https://www.forbes.com/sites/hnewman/2023/06/08/meet-americas-most-cybersecure-companies-2023/?sh=dd8bc202cf60](https://www.forbes.com/sites/hnewman/2023/06/08/meet-americas-most-cybersecure-companies-2023/?sh=dd8bc202cf60)\n\n¡El plan nacional para la batalla de ciberseguridad ha sido revelado! El plan de implementación para la Estrategia Nacional de Ciberseguridad ya ha sido publicado. Es un manual estratégico para defenderse contra amenazas digitales, delineando los pasos para salvaguardar nuestras realidades virtuales. ¡Unámonos detrás de este plan y fortalezcamos las defensas cibernéticas de nuestra nación! ¡Adelante hacia un futuro digital más seguro!\n\n[https://www.helpnetsecurity.com/2023/07/13/national-cybersecurity-strategy-implementation-plan-published/](https://www.helpnetsecurity.com/2023/07/13/national-cybersecurity-strategy-implementation-plan-published/)\n\nLa batalla de ciberseguridad entre Estados Unidos y China está intensificándose en medio de crecientes tensiones. The Hacker News informa sobre alarmantes correos electrónicos comprometedores en las agencias gubernamentales de Estados Unidos, lo que aumenta aún más las preocupaciones sobre la guerra cibernética en curso. Es un enfrentamiento de alto riesgo entre titanes digitales que subraya la necesidad apremiante de defensas sólidas y cooperación internacional frente a las persistentes amenazas cibernéticas.\n\n[https://thehackernews.com/2023/07/us-government-agencies-emails.html](https://thehackernews.com/2023/07/us-government-agencies-emails.html)\n\n## Edge Computing\nLa computación perimetral\n\n¡Infineon y Edge Impulse se unen para desatar el poder de la IA en el Edge! Edge Computing News afirma que esta asociación tiene como objetivo expandir las capacidades de IA en el Edge de Infineon. Es como una pareja hecha en el paraíso tecnológico, combinando la experiencia de Infineon con las soluciones de vanguardia de Edge Impulse. ¡Prepárate para la IA en el Edge, revolucionando la forma en que procesamos datos y desbloqueando nuevos reinos de la innovación!\n\n[https://www.edgecomputing-news.com/2023/07/10/infineon-partners-with-edge-impulse-to-extend-its-edge-ai-capabilities/](https://www.edgecomputing-news.com/2023/07/10/infineon-partners-with-edge-impulse-to-extend-its-edge-ai-capabilities/)\n\n¡Agárrense los sombreros porque el mercado de la computación en el edge está listo para despegar! Edge Computing News revela que para el 2028, esta próspera industria valdrá los sorprendentes $111.3 mil millones. Es como una fiebre del oro en la computación en el edge, con empresas compitiendo para aprovechar el inmenso potencial del poder de procesamiento descentralizado. ¡Prepárense para un cambio de paradigma mientras el edge toma el centro del escenario en la revolución digital!\n\n[https://www.edgecomputing-news.com/2023/07/10/edge-computing-market-to-be-worth-111-3-billion-by-2028/](https://www.edgecomputing-news.com/2023/07/10/edge-computing-market-to-be-worth-111-3-billion-by-2028/)\n\nSeoul Robotics está expandiendo la tecnología de transporte con su avanzada tecnología de percepción 3D. EdgeIR informa sobre esta innovación revolucionaria que brinda una mayor percepción de profundidad a los sistemas de transporte. A medida que los vehículos comienzan a utilizar esta tecnología de vanguardia de Seoul Robotics, podremos anticipar viajes más seguros y más inteligentes en el futuro.\n\n[https://www.edgeir.com/seoul-robotics-develops-3d-perception-tech-to-boost-transportation-systems-20230711](https://www.edgeir.com/seoul-robotics-develops-3d-perception-tech-to-boost-transportation-systems-20230711)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW24-es","image":"./briefs/edw-24/es/thumbnail.png","lang":"es","summary":"En las noticias de transformación digital para la semana del 17 de julio de 2023, el Papa Francisco ofrece orientación sobre ética en la inteligencia artificial, Intel sigue siendo la fortaleza cibernética de América y el mercado de la computación en el borde está listo para despegar."},{"id":191,"type":"News Brief","title":"2023-7-23","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Inteligencia Artificial\n\n\nMeta, el pionero de la IA, sorprende a la comunidad tecnológica con un lanzamiento innovador: les presentamos Llama 2, el último modelo de LLM de código abierto. Desbloqueando el poder de comprensión del lenguaje para todos los desarrolladores, esta innovación revolucionaria promete transformar los proyectos de IA. Dile adiós a la complejidad y da la bienvenida a un entrenamiento de IA sin problemas. ¡Prepárate para aprovechar la ola de modelos de LLM de código abierto y ser testigo del futuro de la IA desplegarse! [Leer más](https://www.artificialintelligence-news.com/2023/07/19/meta-launches-llama-2-open-source-llm/)\n\n\nEl Consejo Tecnológico de Forbes suena la alarma mientras la IA Generativa pone su mirada en los datos personales. Su cautivador artículo profundiza en la inminente invasión de datos y sus posibles consecuencias. Prepárate para el revolucionario ascenso del contenido generado por IA, que plantea oportunidades y desafíos en materia de privacidad de datos. Mantente informado y vigilante mientras navegamos por el valiente nuevo mundo de la #IAGenerativa y protegemos nuestro futuro digital. [Leer más](https://www.forbes.com/sites/forbestechcouncil/2023/07/20/generative-ai-is-coming-for-people-data-are-you-ready/?sh=3e6f95421573)\n\n\nHuelga en Hollywood por preocupaciones laborales relacionadas con la IA. La película principal del verano, \"Misión Imposible - Cálculo Mortal\", explora el potencial y los riesgos de la IA. ¿Es esto un cuento de advertencia o un vistazo a nuestro futuro? Actores y escritores temen que sus trabajos y representaciones sean reemplazados. Manténganse atentos para más actualizaciones. [Leer más](https://www.wired.com/story/mission-impossible-dead-reckoning-is-the-perfect-ai-panic-movie/)\n\n## Ciberseguridad\n\n\nEl Guardian informa sobre un importante ciberataque mientras hackers chinos apuntan a funcionarios estadounidenses, incluyendo al Embajador en Pekín. La violación de seguridad genera preocupaciones sobre la ciberseguridad y las relaciones internacionales. Las autoridades están investigando el alcance del ataque y sus posibles implicaciones. Manténganse atentos a las actualizaciones sobre este desarrollo significativo y preocupante. [Leer más](https://www.theguardian.com/us-news/2023/jul/20/ambassador-to-beijing-among-us-officials-hit-by-chinese-hackers)\n\n\nÚltimas noticias: ¡Se desata un aterrador ciberataque! La infraestructura crítica está bajo asedio mientras los hackers aprovechan una peligrosa vulnerabilidad de día cero de Citrix. El pánico se apodera de los expertos en ciberseguridad, compitiendo contra el tiempo para evitar las catastróficas consecuencias del ataque. ¡Prepárense para las repercusiones y mantengan sus defensas digitales listas! #CitrixZeroDay #EmergenciaCiberseguridad [Leer más](https://www.securityweek.com/citrix-zero-day-exploited-against-critical-infrastructure-organization/)\n\n\nCyware trae noticias alarmantes sobre \"HotRAT\", un script oculto acechando en software pirata. Los ciberdelincuentes aprovechan esta astuta táctica para comprometer a usuarios desprevenidos. El panorama de amenazas se intensifica, lo que insta a tener precaución al descargar programas piratas. Protege tu refugio digital y evita caer víctima de este ardid insidioso. ¡Mantente informado y seguro! #HotRAT #AmenazaCibernética [Leer más](https://cyware.com/news/hotrat-as-hidden-script-in-cracked-software-b2baa5b3)\n\n## Gestión de datos\n\n\nTDWI descubre el amanecer de una revolución en la gestión de datos con 'Arch-All Data Fabric'. En este informe innovador, expertos revelan el plan para la gestión de datos de próxima generación. Presencia la integración de diversas fuentes de datos, impulsando a las empresas a alturas sin precedentes. ¡Manténgase atento para conocer más sobre el futuro impulsado por datos! #DataFabric #NextGenDataManagement [Leer más](https://tdwi.org/articles/2023/07/20/arch-all-data-fabric-how-to-architect-next-generation-data-management.aspx)\n\n\n\nHammerspace logra un hito revolucionario con $56.7M en su primera financiación institucional. Este logro transformador desbloquea oportunidades de negocio ocultas dentro de los datos no estructurados. Descubre el poder de sus soluciones innovadoras, que permiten una gestión sin problemas de datos no estructurados para empresas. ¡Prepárate para una nueva era de utilización y crecimiento de datos! #Hammerspace #DataInnovation #FundingSuccess [Leer más](https://hammerspace.com/hammerspace-raises-56-7m-in-first-institutional-funding-unlocks-business-opportunities-hidden-in-unstructured-data/)\n\n\nLa gran revelación de Lenovo: ¡Nuevas soluciones de gestión de datos diseñadas para potenciar las cargas de trabajo de IA! Datanami informa sobre sus ofertas de vanguardia, capacitando a las empresas con su potencial impulsado por IA. Sumérgete en las últimas innovaciones que prometen revolucionar el manejo de datos para proyectos de IA. ¡Acepta una nueva era de eficiencia e inteligencia en el panorama tecnológico! #Lenovo #GestiónDeDatos #CargasDeTrabajoDeIA [Leer más](https://www.datanami.com/this-just-in/lenovo-unveils-new-data-management-solutions-to-enable-ai-workloads/)\n\n## Aceptando el Podcast Digital\n\nEn este episodio, Darren vuelve a visitar una entrevista que realizó con chatGPT y hace un seguimiento con una entrevista similar protagonizada por Google Bard. La comparación resulta intrigante, especialmente cuando se combina con las ideas del episodio 122 sobre la Adopción de la Transformación Digital. [Episode 147](https://www.embracingdigital.org/episode-EDT147) [Episode 122](https://www.embracingdigital.org/episode-EDT122)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW25-es","image":"./briefs/edw-25/es/thumbnail.png","lang":"es","summary":"Esta semana, 24 de julio de 2023, en noticias de transformación digital, ha habido avances en la inteligencia artificial generativa en el lugar de trabajo, así como un ciberataque de China a la Embajada de Estados Unidos. Además, los tejidos de datos están empezando a surgir como una tendencia."},{"id":192,"type":"News Brief","title":"2023-7-30","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Comunicaciones Avanzadas\n\n\nIntel y Ericsson están colaborando para crear una mejor conectividad a internet y experiencias con dispositivos. Su colaboración tiene como objetivo satisfacer la creciente demanda de internet de alta velocidad y conexiones confiables, prometiendo emocionantes avances tecnológicos. Con su experiencia combinada, los usuarios pueden esperar descargas más rápidas, videollamadas más fluidas y un rendimiento general mejorado para teléfonos, computadoras y otros dispositivos. ¡Prepárate para un futuro más conectado y fluido! [Leer más](http://finance.yahoo.com/news/intel-ericsson-expand-collaboration-advance-201000466.html)\n\n\n¡Prepárate para el auge del 5G! Según un nuevo estudio, se espera que el mercado global de integración del sistema 5G alcance la increíble cifra de 115,60 mil millones de dólares para 2032. Las empresas están compitiendo por adoptar esta tecnología de Internet súper rápida para ciudades inteligentes, salud y más. Los fabricantes también están actualizando con tecnologías geniales como robots y big data. ¡La demanda de servicios de integración 5G está en aumento, prometiendo tiempos emocionantes por delante! [Leer más](https://finance.yahoo.com/news/global-5g-system-integration-market-100000051.html)\n\n\n¡Emocionantes noticias de T-Mobile! ¡Su revolucionaria tecnología 5G promete un internet súper rápido, velocidades de 3.3 GBits/seg y conectividad sin interrupciones! Con esta innovación, las descargas y transmisiones serán rapidísimas, y las videollamadas y los juegos en línea serán más fluidos. ¡Prepárate para una nueva era de conectividad y experiencias únicas en internet! [Leer más](https://voip.review/2023/07/26/tmobile-introduces-groundbreaking-5g-tech/)\n\n## Ciberseguridad\n\n\n¡GPT y la ciberseguridad: el futuro es emocionante y preocupante! Los expertos están entusiasmados con GPT, un potente sistema de inteligencia artificial que está listo para revolucionar la ciberseguridad. La charla de Una-May O'Reilly arrojó luz sobre el impacto de GPT en la seguridad, mostrando que hará que las defensas sean más innovadoras y eficientes. Pero hay un giro: ¡los hackers también pueden usarlo! La inteligencia artificial podría impulsar a ambos bandos en una carrera armamentista cibernética, dejándonos en un mar de lo desconocido. ¡Prepárate para el emocionante y preocupante futuro de la ciberseguridad! [Leer más](https://www.forbes.com/sites/johnwerner/2023/07/28/whos-the-authority-on-gpt-and-cybersecurity-is-it--gpt/)\n\n\nNuevas normas de la Comisión de Valores y Bolsa (SEC) requieren que las empresas públicas revelen cualquier violación de seguridad cibernética que pueda afectar sus ganancias en un plazo de cuatro días. Esta medida tiene como objetivo proteger a los inversionistas y aumentar la transparencia en los riesgos de ciberseguridad. Se permiten demoras en casos de riesgos para la seguridad nacional. La regla también exige que las empresas compartan información sobre su gestión y experiencia en ciberseguridad. [Leer más](https://apnews.com/article/sec-cybersecurity-breach-disclosure-risk-hacking-bb6252463637793bfdc8ace5bfcbe7df)\n\n\nEn noticias de última hora, las agencias de ciberseguridad en Australia y Estados Unidos han advertido sobre una debilidad crítica en las aplicaciones web. Los ciberdelincuentes pueden aprovechar la vulnerabilidad de Referencia Directa a Objetos No Segura para obtener acceso no autorizado a información confidencial. Permanezca alerta y proteja sus datos con protocolos de autenticación y autorización sólidos. [Leer más](https://thehackernews.com/2023/07/cybersecurity-agencies-warn-against.html)\n\n## Computación Ubicua\n\n\nLa inteligencia artificial (IA) se ha utilizado cada vez más en la programación para mejorar la eficiencia y reducir gastos. Sin embargo, depender del código anterior puede conducir a errores e impedir la creatividad en el desarrollo de aplicaciones. El código generado por IA puede no estar optimizado para la plataforma, por lo que es esencial equilibrar los beneficios de la IA con la experiencia humana para aplicaciones eficientes. [Leer más](https://www.infoworld.com/article/3703611/the-lost-art-of-cloud-application-engineering.html)\n\n\n¡Microsoft lidera en cargas de trabajo de inteligencia artificial basadas en la nube, según Nadella! Según su CEO, Satya Nadella, Microsoft está tomando la delantera en las cargas de trabajo de IA basadas en la nube. La destreza del gigante tecnológico en tareas impulsadas por IA está causando sensación en la industria. Al aprovechar el poder de la nube, Microsoft ofrece soluciones de IA de vanguardia a empresas y usuarios. Con su enfoque innovador, están dando forma al futuro de la tecnología de IA. [Leer más](https://www.cnbc.com/2023/07/25/microsoft-is-in-the-lead-with-cloud-based-ai-workloads-nadella-says.html)\n\n\nAlibaba se ha asociado con Llama AI de Meta para desarrollar software innovador. Esta colaboración entre Alibaba Cloud, una división del gigante chino, y el modelo de inteligencia artificial de Llama 2 está marcando nuevas tendencias en la industria tecnológica. Esta asociación convertirá a China en un pionero en programación sin costo, llevando emocionantes innovaciones impulsadas por la inteligencia artificial al mercado. ¡Prepárate para presenciar el futuro del desarrollo de software! [Leer más](https://fagenwasanni.com/news/alibabas-cloud-computing-service-utilizes-metas-ai-model-llama-for-software-development/82096/)\n\n## Podcast Abrazando la Transformación Digital\n\nNo te pierdas el episodio de esta semana donde Darren sostiene una conversación esclarecedora con el invitado especial Jared Shepard, el CEO de Hypori. La entrevista se enfoca en el tema crucial de asegurar a los trabajadores remotos mediante la virtualización móvil. El viaje único de Jared Shepard desde ser un desertor escolar hasta convertirse en CEO agrega una dimensión inspiradora a la discusión.\n\n[Episode 148](https://www.embracingdigital.org/episode-EDT148)\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW26-es","image":"./briefs/edw-26/es/thumbnail.png","lang":"es","summary":"Aquí están las últimas actualizaciones en el campo de la Transformación Digital para la semana del 31 de julio de 2023. Esto incluye noticias sobre la asociación entre Ericsson e Intel para avanzar en la tecnología 5G, los esfuerzos de ChatGPT para abordar la ciberseguridad y el surgimiento del desarrollo de software impulsado por IA."},{"id":193,"type":"News Brief","title":"2023-8-6","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Inteligencia Artificial\n\n\nExiste una creciente preocupación entre los expertos tecnológicos acerca del uso de imágenes mejoradas por inteligencia artificial. The Guardian ha investigado este problema, ya que hay una preocupación de que las imágenes alteradas por IA puedan afectar la opinión pública e incluso distorsionar hechos importantes esenciales para el sistema democrático. Esto podría afectar seriamente la confianza y la verdad, lo que daría lugar a un debate sobre cómo proteger las elecciones de la manipulación de la IA. [Leer más](https://www.theguardian.com/technology/2023/aug/03/ai-enhanced-images-a-threat-to-democratic-processes-experts-warn\n)\n\n\nUn artículo reciente de Yahoo Finance informó que Wendy's está utilizando inteligencia artificial para revolucionar sus unidades de servicio autoservicio. Con órdenes automatizadas y menús personalizados, el gigante de comida rápida está cambiando las reglas del juego para las comidas rápidas. Este enfoque tecnológico en el servicio de alimentos nos brinda un vistazo al futuro de la comida rápida. Será interesante ver cómo reaccionan los clientes ante esta transformación digital. [Leer más](https://finance.yahoo.com/news/wendys-latest-fast-food-company-210910771.html\n)\n\n\nEl impacto de la IA en los profesionales asiático-americanos es una tendencia preocupante. Las pérdidas de empleo se avecinan, planteando preguntas sobre cómo la sociedad abordará la compleja ecuación de la tecnología, la etnia y la dinámica laboral. Es necesario realizar un examen más cercano de los efectos de la IA. [Leer más](https://www.nbcnews.com/news/asian-america/asian-american-workers-heavily-affected-ai-rcna98179\n)\n\n## Seguridad cibernética\n\n\nNBC News ha informado de un preocupante ciberataque en una red hospitalaria que ha resultado en la desconexión de su infraestructura informática nacional. La violación ha puesto en peligro la atención médica de los pacientes y las autoridades están trabajando para contener la situación. Este incidente destaca la importancia de la seguridad de los datos de atención médica en la era digital. [Leer más](https://www.nbcnews.com/tech/security/hackers-force-hospital-system-take-national-computer-system-offline-rcna98212\n)\n\n\nLos desarrolladores que utilizan el sistema ampliamente utilizado NPM están siendo objeto de paquetes de código dañinos que pueden comprometer la seguridad del software. Esta es una amenaza grave que debe ser identificada y mitigada para evitar ataques cibernéticos. La comunidad de ciberseguridad está preocupada y destaca la necesidad de cadenas de suministro de software más seguras para prevenir una mayor infiltración de estos paquetes. [Leer más](https://thehackernews.com/2023/08/malicious-npm-packages-found.html\n)\n\n\nCISA ha lanzado un plan estratégico llamado \"Blueprint for Cybersecurity\" para fortalecer la infraestructura digital nacional. Se centra en la defensa adaptativa, la respuesta ante incidentes y la innovación para crear un escudo digital impenetrable. Este plan es una guía esencial para aquellos interesados en la seguridad tecnológica. [Leer más](https://www.securitysystemsnews.com/article/cisa-releases-cybersecurity-strategic-plan\n)\n\n## Computación en el Borde\n\n\nLa combinación de la computación cuántica y los dispositivos de IoT está causando preocupaciones de seguridad. El poder de la tecnología cuántica y el potencial de los dispositivos de IoT para ser secuestrados crean un nuevo desafío. La creciente preocupación es que la encriptación sea descifrada a medida que avanza la computación cuántica y las botnets lancen ataques DDoS utilizando dispositivos de IoT hackeados. Las defensas de red deben fortalecerse para hacer frente a esta doble amenaza. [Leer más](https://www.securitymagazine.com/articles/99604-the-impact-of-quantum-computers-and-iot-devices-on-network-security\n)\n\n\nYahoo Finance ha revelado un emocionante desarrollo en la industria tecnológica, ya que Edge Computing, IoT y AI se fusionan para crear un mercado que se predice valdrá $230 mil millones para el 2025. Edge Computing permite que más de 75 mil millones de dispositivos IoT procesen datos en tiempo real, mientras que se estima que las habilidades cognitivas de la IA aumentarán la productividad hasta en un 40%. Esta revolución está lista para remodelar la dinámica de los datos y la toma de decisiones en diversas industrias. [Leer más](https://finance.yahoo.com/news/edge-computing-iot-ai-revolutionize-011500739.html\n)\n\n\nLa región de Asia-Pacífico está experimentando una transformación digital significativa impulsada por el Internet de las Cosas (IoT) y la Inteligencia Artificial (IA). El Edge Computing Móvil (MEC) está liderando este cambio, ofreciendo niveles incomparables de conectividad y automatización al acercar la computación a la fuente de datos. MEC tiene un inmenso potencial para revolucionar industrias, mejorar vidas y impulsar el destino digital de la región, pero también plantea desafíos de seguridad que exigen salvaguardias sólidas y regulaciones actualizadas. [Leer más](https://fagenwasanni.com/news/innovations-in-iot-and-ai-the-role-of-mobile-edge-computing-in-asia-pacifics-technological-advancements/48663/\n)\n\n## Podcast de Adopción de la Transformación Digital\n\n\nEn el último episodio de Embracing Digital Transformation, Darren es acompañado por Leland Brown, un ingeniero principal en Capgemini y un ex invitado en el programa. Discuten los próximos avances de la tecnología 5G y su impacto en la Computación en el Borde Móvil (MEC) en el Departamento de Defensa de los Estados Unidos. [Leer más](https://embracingdigital.org/episode-EDT149\n)\n\nInteligencia Artificial\n\n\nExiste una creciente preocupación entre los expertos en tecnología acerca del uso de imágenes mejoradas por la inteligencia artificial (IA). The Guardian ha investigado este problema, ya que existe la inquietud de que las imágenes alteradas por IA puedan afectar la opinión pública e incluso distorsionar datos importantes que son esenciales para el sistema democrático. Esto podría afectar seriamente la confianza y la verdad, y generar un debate sobre cómo proteger las elecciones de la manipulación de la IA. [Leer más](https://www.theguardian.com/technology/2023/aug/03/ai-enhanced-images-a-threat-to-democratic-processes-experts-warn\r)\n\n\nUn artículo reciente de Yahoo Finance informó que Wendy's, un restaurante de comida rápida estadounidense, está utilizando inteligencia artificial para revolucionar sus puntos de venta para llevar. Con pedidos automáticos y menús personalizados, el gigante de la comida rápida está cambiando el juego para las comidas rápidas. Este enfoque tecnológico en el servicio de comida brinda un vistazo al futuro de la comida rápida. Será interesante ver cómo reaccionan los clientes ante esta transformación digital. [Leer más](https://finance.yahoo.com/news/wendys-latest-fast-food-company-210910771.html\r)\n\n\nEl impacto de la inteligencia artificial en los profesionales asiático-americanos es una tendencia preocupante. Las pérdidas de empleo son cada vez más grandes, lo que plantea preguntas sobre cómo la sociedad abordará la compleja ecuación de tecnología, etnicidad y dinámicas laborales. Es necesaria una examinación más cercana de los efectos de la inteligencia artificial. [Leer más](https://www.nbcnews.com/news/asian-america/asian-american-workers-heavily-affected-ai-rcna98179\r)\n\n## Ciberseguridad\n\n\nNBC News ha informado de un preocupante ciberataque a la red de un hospital que ha resultado en la desconexión de su infraestructura informática nacional. La brecha ha puesto en riesgo la atención médica de los pacientes y las autoridades están trabajando para contener la situación. Este incidente resalta la importancia de la seguridad de los datos de salud en la era digital. [Leer más](https://www.nbcnews.com/tech/security/hackers-force-hospital-system-take-national-computer-system-offline-rcna98212\r)\n\n\nLos desarrolladores que utilizan el sistema ampliamente utilizado NPM están siendo blanco de paquetes de código dañinos que pueden comprometer la seguridad del software. Esta es una amenaza grave que debe ser identificada y mitigada para evitar ataques cibernéticos. La comunidad de ciberseguridad está preocupada y destaca la necesidad de cadenas de suministro de software más seguras para prevenir una mayor infiltración de estos paquetes. [Leer más](https://thehackernews.com/2023/08/malicious-npm-packages-found.html\r)\n\n\nCISA ha lanzado un plan estratégico llamado \"Blueprint for Cybersecurity\" para fortalecer la infraestructura digital nacional. Se centra en la defensa adaptativa, respuesta ante incidentes, e innovación para crear un escudo digital impenetrable. Este plan es una guía esencial para aquellos interesados en la seguridad tecnológica. [Leer más](https://www.securitysystemsnews.com/article/cisa-releases-cybersecurity-strategic-plan\r)\n\n## Computación en el borde\n\n\nLa combinación de computación cuántica y dispositivos de IoT está causando preocupaciones de seguridad. El poder de la tecnología cuántica y el potencial de secuestro de dispositivos de IoT crean un nuevo desafío. El temor al descifrado de la encriptación está en crecimiento a medida que avanza la computación cuántica y los botnets lanzan ataques de DDoS utilizando dispositivos de IoT hackeados. Las defensas de red deben fortalecerse para hacer frente a esta doble amenaza. [Leer más](https://www.securitymagazine.com/articles/99604-the-impact-of-quantum-computers-and-iot-devices-on-network-security\r)\n\n\nYahoo Finance ha revelado un emocionante avance en la industria tecnológica, ya que Edge Computing, IoT e IA se fusionan para crear un mercado que se estima valdrá 230 mil millones de dólares para el año 2025. El Edge Computing permite que más de 75 mil millones de dispositivos IoT procesen datos en tiempo real, mientras que las habilidades cognitivas de la IA se estima que aumenten la productividad hasta en un 40%. Esta revolución está listo para remodelar la dinámica de los datos y la toma de decisiones en diversas industrias. [Leer más](https://finance.yahoo.com/news/edge-computing-iot-ai-revolutionize-011500739.html\r)\n\n\nLa región de Asia-Pacífico está experimentando una transformación digital significativa impulsada por el Internet de las Cosas (IoT) e Inteligencia Artificial (IA). Mobile Edge Computing (MEC) está liderando este cambio, ofreciendo niveles incomparables de conectividad y automatización al acercar la computación a la fuente de datos. MEC tiene un inmenso potencial para revolucionar industrias, mejorar vidas y impulsar el destino digital de la región, pero también plantea desafíos de seguridad que requieren sólidas salvaguardias y regulaciones actualizadas. [Leer más](https://fagenwasanni.com/news/innovations-in-iot-and-ai-the-role-of-mobile-edge-computing-in-asia-pacifics-technological-advancements/48663/\r)\n\nPodcast de Adopción de la Transformación Digital.\n\n\nEn el último episodio de Embracing Digital Transformation, Darren se une a Leland Brown, un ingeniero principal en Capgemini y un ex invitado en el programa. Discuten los próximos avances de la tecnología 5G y su impacto en la Computación en el Borde Móvil (MEC) en el Departamento de Defensa de los Estados Unidos. [Leer más](https://embracingdigital.org/episode-EDT149\r)\n\nInteligencia Artificial\n\n\nExiste una creciente preocupación entre los expertos tecnológicos sobre el uso de imágenes mejoradas por IA. The Guardian ha investigado este problema, dado que existe una preocupación de que las visuales alteradas por IA puedan afectar la opinión pública e incluso distorsionar hechos importantes esenciales para el sistema democrático. Esto podría afectar seriamente la confianza y la verdad, y llevar a un debate sobre cómo proteger las elecciones de la manipulación de la IA. [Leer más](https://www.theguardian.com/technology/2023/aug/03/ai-enhanced-images-a-threat-to-democratic-processes-experts-warn\r)\n\n\nUn reciente artículo de Yahoo Finance informó que Wendy's, un restaurante de comida rápida estadounidense, utiliza inteligencia artificial para revolucionar sus auto-pedidos. Con ordenamientos automatizados y menús personalizados, el gigante de la comida rápida está cambiando las reglas del juego para las comidas rápidas. Este enfoque tecnológico del servicio de alimentos nos brinda un vistazo al futuro de la comida rápida. Será interesante ver cómo reaccionan los clientes ante esta transformación digital. [Leer más](https://finance.yahoo.com/news/wendys-latest-fast-food-company-210910771.html\r)\n\n\nEl impacto de la inteligencia artificial en los profesionales asiático-americanos es una tendencia preocupante. Las pérdidas de empleo se están acercando, planteando preguntas sobre cómo la sociedad abordará la compleja ecuación de la tecnología, la etnicidad y la dinámica de la fuerza laboral. Es necesaria una examinación más detallada de los efectos de la inteligencia artificial. [Leer más](https://www.nbcnews.com/news/asian-america/asian-american-workers-heavily-affected-ai-rcna98179\r)\n\n## Ciberseguridad\n\n\nNBC News ha informado de un preocupante ciberataque en una red hospitalaria que resultó en la desconexión de su infraestructura informática nacional. La violación ha puesto en riesgo la atención médica de los pacientes y las autoridades están trabajando para contener la situación. Este incidente pone de relieve la importancia de la seguridad de los datos de salud en la era digital. [Leer más](https://www.nbcnews.com/tech/security/hackers-force-hospital-system-take-national-computer-system-offline-rcna98212\r)\n\n\nLos desarrolladores que utilizan el ampliamente utilizado sistema NPM están siendo objetivo de paquetes de código perjudiciales que pueden comprometer la seguridad del software. Esta es una amenaza grave que debe ser identificada y mitigada para prevenir ataques cibernéticos. La comunidad de ciberseguridad está preocupada y destaca la necesidad de cadenas de suministro de software más seguras para prevenir una mayor infiltración de estos paquetes. [Leer más](https://thehackernews.com/2023/08/malicious-npm-packages-found.html\r)\n\n\nCISA ha publicado un plan estratégico llamado \"Blueprint for Cybersecurity\" para fortalecer la infraestructura digital nacional. Se enfoca en la defensa adaptativa, la respuesta a incidentes y la innovación para crear un escudo digital infranqueable. Este plan es una guía esencial para aquellos interesados en la seguridad tecnológica. [Leer más](https://www.securitysystemsnews.com/article/cisa-releases-cybersecurity-strategic-plan\r)\n\n## Computación en el Borde\n\n\nLa combinación de la computación cuántica y los dispositivos del Internet de las cosas está generando preocupaciones de seguridad. El poder de la tecnología cuántica y el potencial de los dispositivos del IoT para ser secuestrados crean un nuevo desafío. El temor a que se puedan descifrar las encryption_es aumenta a medida que avanza la computación cuántica y los botnets lanzan ataques de DDoS utilizando dispositivos del IoT hackeados. Las defensas de la red deben fortalecerse para enfrentar esta amenaza dual. [Leer más](https://www.securitymagazine.com/articles/99604-the-impact-of-quantum-computers-and-iot-devices-on-network-security\r)\n\n\nYahoo Finance ha revelado un emocionante avance en la industria tecnológica, ya que Edge Computing, IoT e IA se fusionan para crear un mercado que se predice que valdrá doscientos treinta mil millones de dólares para el año 2025. Edge Computing permite que más de setenta y cinco mil millones de dispositivos IoT procesen datos en tiempo real, mientras que se estima que las habilidades cognitivas de la IA aumenten la productividad hasta en un cuarenta por ciento. Esta revolución está lista para remodelar la dinámica de los datos y la toma de decisiones en diversas industrias. [Leer más](https://finance.yahoo.com/news/edge-computing-iot-ai-revolutionize-011500739.html\r)\n\n\nLa región de Asia-Pacífico está experimentando una transformación digital significativa impulsada por el Internet de las Cosas (IoT) y la Inteligencia Artificial (IA). El Cómputo en el Borde Móvil (MEC) lidera este cambio, ofreciendo niveles inigualables de conectividad y automatización al acercar el cálculo a la fuente de datos. El MEC tiene un inmenso potencial para revolucionar las industrias, mejorar vidas y impulsar el destino digital de la región, pero también plantea desafíos de seguridad que exigen salvaguardias sólidas y regulaciones actualizadas. [Leer más](https://fagenwasanni.com/news/innovations-in-iot-and-ai-the-role-of-mobile-edge-computing-in-asia-pacifics-technological-advancements/48663/\r)\n\n## Podcast de Adopción de Transformación Digital\n\n\nEn el último episodio de Abrazando la Transformación Digital, Darren está acompañado por Leland Brown, un ingeniero principal en Capgemini y un antiguo invitado en el programa. Ellos discuten los próximos avances de la tecnología 5G y su impacto en la Computación en el Borde Móvil (CBM) en el Departamento de Defensa de los Estados Unidos. [Leer más](https://embracingdigital.org/episode-EDT149\r)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW27-es","image":"./briefs/edw-27/es/thumbnail.png","lang":"es","summary":"La semana del 7 de agosto de 2023 en el podcast, hubo varias actualizaciones significativas con respecto a la transformación digital. Especialmente, las cadenas de comida rápida han comenzado a implementar la inteligencia artificial para hacer frente a la escasez de personal, los hospitales están enfrentando ciberataques y las plataformas de Computación de Borde Móvil están cobrando relevancia."},{"id":194,"type":"News Brief","title":"2023-8-13","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Computación Ubicua\n\n\nDebido a una reciente violación en Microsoft Azure Cloud, el Departamento de Seguridad Nacional está revisando sistemas de identidad y autenticación basados ​​en la nube para prevenir ataques maliciosos en entornos de computación en la nube, siguiendo violaciones de seguridad recientes. El objetivo es proporcionar recomendaciones para mejorar la seguridad de las cuentas y prevenir futuras violaciones. [Leer más](https://www.reuters.com/technology/us-cyber-safety-review-board-assess-online-intrusion-microsoft-exchange-dhs-2023-08-11/)\n\n\nEl potencial de la computación cuántica para revolucionar múltiples industrias, en particular la Inteligencia Artificial, al aumentar la velocidad de procesamiento, es emocionante. Sin embargo, debido a la naturaleza delicada de los qubits, la creación de computadoras cuánticas es una tarea difícil. No obstante, los profesionales de la industria siguen siendo optimistas sobre el potencial de la computación cuántica para generar avances importantes en diversos sectores. [Leer más](https://www.scmp.com/magazines/post-magazine/long-reads/article/3230746/quantum-computing-could-give-ai-rocket-fuel-it-needs-become-transformative-its-not-there-yet)\n\n\nEl Presidente Biden firmó una orden ejecutiva esta semana que prohíbe las inversiones estadounidenses en los sectores chinos de inteligencia artificial, semiconductores y computación cuántica para evitar un respaldo involuntario al avance militar y tecnológico de China. La industria de semiconductores, especialmente el desarrollo y la fabricación de chips, es el objetivo principal. China ha expresado su descontento con la orden, alegando que interrumpe los lazos económicos. La orden se aplica a las inversiones futuras y puede tener algunas excepciones. [Leer más](https://www.tomshardware.com/news/us-order-bans-future-investments-in-chinese-ai-semiconductor-and-quantum-computing)\n\n## Gestión de datos\n\n\nChina está proponiendo nuevas reglas para limitar la tecnología de reconocimiento facial y proteger la privacidad de los usuarios. Las reglas preliminares de la Administración del Ciberespacio de China establecen que el reconocimiento facial solo debe utilizarse cuando sea necesario, y que se deben explorar soluciones no biométricas para lograr los mismos objetivos con menos intrusión. Esta medida se produce debido a las preocupaciones en China sobre el posible mal uso de la información personal a través de la tecnología de reconocimiento facial. [Leer más](https://fagenwasanni.com/news/china-introduces-new-draft-rules-to-regulate-facial-recognition-technology-and-data-management/174996/)\n\n\nEn la transición de los servidores internos a la tecnología de nube y los centros de datos para la gestión de datos, la nube ofrece escalabilidad y flexibilidad al tiempo que elimina la necesidad de proximidad del servidor. Sin embargo, muchas organizaciones quedan sorprendidas por el costo de gestionar datos entre centros de datos y nubes, ya que los proveedores de servicios en la nube cobran por la transferencia de datos entre modalidades. Desarrollar una estrategia de gestión de datos y una arquitectura de datos híbrida se vuelve crítico para las organizaciones a fin de ahorrar costos, mejorar la resiliencia y respaldar los negocios en estos entornos operativos complejos. [Leer más](https://devops.com/data-management-cloud-technology-or-data-centers/)\n\n\nLos gobiernos en todo el mundo están asumiendo un papel más activo en la regulación de la privacidad de los datos. Oregón ha adoptado un enfoque integral, otorgando a los consumidores un mayor control sobre sus datos personales. Las empresas ahora deben obtener un consentimiento explícito para la recopilación y compartición de datos, y los individuos tienen el derecho de acceder, corregir y eliminar sus datos. La ley también incluye obligaciones de transparencia y salvaguardias contra la discriminación basada en el uso de datos. ¿Es esto otro intento del gobierno por obtener dinero o una verdadera protección de la privacidad individual? El tiempo lo dirá. [Leer más](https://www.reuters.com/legal/legalindustry/oregon-passes-comprehensive-privacy-law-2023-08-11/)\n\n## Seguridad Cibernética\n\n\nEl Instituto Nacional de Estándares y Tecnología (NIST) está tomando medidas significativas para mejorar las prácticas de ciberseguridad mediante la provisión de nuevas pautas y recursos. Además de los pilares previos del marco de seguridad: identificar, proteger, detectar, responder y recuperar, NIST ha introducido un nuevo pilar llamado \"gobernar\". Esta nueva adición enfatiza que la ciberseguridad es una fuente importante de riesgo empresarial y ayuda a las organizaciones a idear y ejecutar decisiones que apoyen su estrategia de seguridad. [Leer más](https://www.infosecurity-magazine.com/news/nist-expands-cybersecurity/)\n\n\nLa conferencia de ciberseguridad Black Hat en Las Vegas presentó herramientas avanzadas para la detección de amenazas, respuesta a incidentes y comunicación segura. Estas herramientas buscan mejorar las defensas de ciberseguridad de las organizaciones contra las amenazas y desafíos emergentes. Como era de esperar, la Inteligencia Artificial Generativa fue un tema central, destacando su potencial para frustrar los ciberataques. [Leer más](https://www.crn.com/news/security/20-hottest-new-cybersecurity-tools-at-black-hat-2023)\n\n\nEl Desafío del Descifrador de Códigos de la NSA ayuda a abordar la escasez de habilidades en ciberseguridad desafiando a los estudiantes a resolver problemas complejos y fomentando la colaboración. Fomenta el desarrollo de nuevos materiales educativos y programas, brindando a la próxima generación de profesionales una experiencia práctica para fortalecer la defensa de la industria contra las amenazas en evolución. El programa destaca la importancia de promover la educación en ciberseguridad y proporcionar oportunidades de aprendizaje prácticas. [Leer más](https://www.darkreading.com/attacks-breaches/nsa-talks-codebreaker-challenge-success-influence-on-education)\n\n## Podcast de Transformación Digital\n\nDarren entrevista a Sonu Panda, el director ejecutivo de Prescriptive Data, en este episodio. Discuten cómo su software ayuda a los propietarios de bienes raíces comerciales a convertir sus edificios en espacios inteligentes y eficientes. [Leer más](https://embracingdigital.org/episode-EDT150)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW28-es","image":"./briefs/edw-28/es/thumbnail.png","lang":"es","summary":"Noticias de Transformación Digital para el 13 de agosto de 2023. Vale la pena seguir los últimos avances en privacidad de datos, seguridad y avances tecnológicos, incluyendo la computación cuántica y nuevas leyes de privacidad. Las pautas del NIST y las herramientas de Black Hat son recursos útiles. El desafío Codebreaker de la NSA impulsa la innovación en ciberseguridad."},{"id":195,"type":"News Brief","title":"2023-8-19","tags":["ai","edge","cybersecurity"],"body":"\n\n## Inteligencia Artificial\n\n\nEn un fallo reciente, un tribunal de Washington D.C. ha determinado que el arte generado por inteligencia artificial no es elegible para protección de derechos de autor. La decisión establece un precedente significativo, sugiriendo que las obras creativas producidas únicamente por inteligencia artificial carecen de los derechos legales típicamente otorgados a las creaciones hechas por humanos. Este veredicto puede tener implicaciones de gran alcance para el futuro de la propiedad intelectual en la era digital. [Leer más](https://news.bloomberglaw.com/ip-law/ai-generated-art-lacks-copyright-protection-d-c-court-rules)\n\n\nLos investigadores del MIT han encontrado que los modelos de aprendizaje automático utilizados en el diagnóstico y tratamiento médico pueden agravar las desigualdades en la atención médica. Estos modelos, cuando se entrenan con datos sesgados, pueden llevar a un acceso desigual y a una calidad de atención deficiente para los grupos subrepresentados. Esto subraya la necesidad urgente de mejorar la recopilación de datos y el desarrollo de modelos para garantizar resultados de atención médica justos y equitativos. [Leer más](https://news.mit.edu/2023/how-machine-learning-models-can-amplify-inequities-medical-diagnosis-treatment-0817)\n\n\nLa demanda vertiginosa de inteligencia artificial (IA) está provocando una escasez de chips de alto desempeño, según informa Fox Business. A medida que las aplicaciones de IA se expanden en diversas industrias, aumenta la necesidad de chips especializados para potenciarlas. Esta escasez está afectando a varios sectores, incluyendo automotriz, cuidado de la salud y electrónica de consumo, lo que podría ralentizar la innovación. Las empresas están compitiendo ahora por acelerar la producción de chips para satisfacer la creciente demanda. [Leer más](https://www.foxbusiness.com/technology/surging-demand-ai-creating-shortage-high-powered-chips)\n\n## Ciberseguridad\n\n\nUna aplicación centrada en el estudiante está generando preocupaciones entre los padres y expertos en ciberseguridad. Los posibles riesgos de seguridad para los niños de la aplicación están bajo escrutinio, lo que lleva a los expertos a enfatizar la importancia de la vigilancia parental y la educación de los jóvenes usuarios en cuanto a la seguridad en línea. La situación destaca la continua necesidad del uso responsable de la tecnología y medidas proactivas para proteger a los niños en la era digital. [Leer más](https://www.live5news.com/2023/08/18/student-geared-app-concerning-lowcountry-parents-cyber-security-experts/)\n\n\nLa industria fintech está siendo grandemente afectada por la inteligencia artificial y la ciberseguridad, de acuerdo a Analytics Insight. La inteligencia artificial está ayudando en la detección de fraude, servicio al cliente y evaluación de riesgos, pero también trae nuevos riesgos de seguridad. Esta interacción continua entre la inteligencia artificial y la ciberseguridad está cambiando el mundo fintech y conduciendo a más innovación y medidas de seguridad incrementadas para proteger información financiera importante. [Leer más](https://www.analyticsinsight.net/how-ai-and-cybersecurity-shape-fintech-industry/)\n\n\nUn reciente informe de investigación de Pew revela que los estadounidenses tienen conocimientos limitados sobre la inteligencia artificial, la ciberseguridad y la gran tecnología. Los hallazgos indican que una parte sustancial de la población carece de comprensión sobre estos temas críticos, subrayando la necesidad de incrementar los esfuerzos de educación y concientización pública en una era dominada por la tecnología y las preocupaciones digitales. [Leer más](https://www.pewresearch.org/internet/2023/08/17/what-americans-know-about-ai-cybersecurity-and-big-tech/)\n\n## Computación de borde\n\n\n\nT-Mobile está colaborando con Google Cloud para el procesamiento perimetral, según reporta FierceWireless. Esta colaboración tiene como objetivo aprovechar la infraestructura en la nube de Google para mejorar las capacidades de red 5G de T-Mobile y brindar servicios de baja latencia, beneficiando a consumidores y empresas. Este movimiento estratégico demuestra la creciente importancia del procesamiento perimetral en la industria de las telecomunicaciones. [Leer más](https://www.fiercewireless.com/tech/t-mobile-hooks-google-cloud-edge-compute)\n\n\n\nLa eliminación gradual de las redes 2G y 3G puede tener repercusiones para los servicios de IoT y de roaming. Muchos dispositivos de IoT y viajeros internacionales aún dependen de estas redes antiguas. Los proveedores de telecomunicaciones deben considerar opciones y estrategias de conectividad alternativas para evitar interrupciones en el servicio para estos grupos de usuarios durante su transición a tecnologías de red más avanzadas. Es importante abordar estos problemas para evitar acumular deudas técnicas en el futuro. [Leer más](https://www.lightreading.com/broadband/sunsetting-2g-and-3g-could-leave-iot-and-roamers-in-dark/d/d-id/786146)\n\n\nEn la Cumbre Digital de Telco de ET, el CTO de Airtel destacó que la convergencia del 5G con las tecnologías de la nube y el IoT está lista para desbloquear casos de uso innovadores en diversas industrias. Se espera que esta combinación impulse soluciones transformadoras para los sectores, demostrando el potencial del 5G para revolucionar la conectividad y los servicios en India y más allá. [Leer más](https://telecom.economictimes.indiatimes.com/news/industry/etdigitaltelcosummit-combination-of-5g-with-cloud-iot-to-drive-new-use-cases-for-verticals-airtel-cto/102827792)\n\n## Podcast sobre la transformación digital.\n\n\nEsta semana Darren comienza una serie sobre inteligencia artificial generativa, con entrevistas de expertos en educación, atención médica, ciberseguridad y tecnologías en la nube, y cómo la inteligencia artificial generativa está desempeñando un papel en el futuro. Los relatos de esta semana incluyen entrevistas del Dr. Jeffrey Lancaster hablando sobre la comprensión de la genAI. [Leer más](https://embracingdigital.org/episode-EDT151)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW29-es","image":"./briefs/edw-29/es/thumbnail.png","lang":"es","summary":"En las últimas noticias de Embracing Digital del 20 de agosto de 2023, hay actualizaciones sobre IA, ciberseguridad y cómputo en el borde. Las noticias incluyen la IA que se dirige a los tribunales, los actores maliciosos cibernéticos que apuntan a la temporada de regreso a clases y la deuda técnica que se traslada al ecosistema 5G a través del cómputo en el borde."},{"id":196,"type":"News Brief","title":"2023-2-19","tags":null,"body":"\n\n## Inteligencia Artificial\n\nLa ética y regulaciones del uso de Chatbots generativos habilitados por AI están empezando a entrar en juego, ya que cada vez más personas están usando ChatGPT y otros. Ahora los Chatbots tendrán que incluir una advertencia: Podemos no producir resultados que sean reales. Cree bajo su propio riesgo. Usted renuncia a su derecho a demandarnos si reproduce conversaciones, imágenes y videos para casos de uso de alto riesgo que causen odio, acoso, violencia, autolesiones, actividades ilegales, engaño, discriminación o spam.\n\n[https://swisscognitive.ch/2023/02/15/honest-lying-why-scaling-generative-ai-responsibly-is-not-a-technology-dilemma-in-as-much-as-a-people-problem/](https://swisscognitive.ch/2023/02/15/honest-lying-why-scaling-generative-ai-responsibly-is-not-a-technology-dilemma-in-as-much-as-a-people-problem/)\n\nA medida que la inteligencia artificial se vuelve cada vez más importante en diversas industrias, el papel del Director de Inteligencia Artificial (CAIO, por sus siglas en inglés) está emergiendo como una nueva adición a la alta dirección. Un buen CAIO debería definir claramente el impacto deseado de los proyectos de IA y enfocar continuamente a la organización en lograr ese resultado. Un gran CAIO también debería hacer un seguimiento del impacto real de esos proyectos y actualizar al negocio sobre su rendimiento con el tiempo. Este papel, por lo tanto, aumenta las expectativas de impulsar la transformación y el impacto, así como anticipar y responder al uso de la IA por parte de los competidores.\n\n[https://applieddatascience.medium.com/the-case-for-the-chief-ai-officer-the-newest-c-suite-role-d55cca40c01](https://applieddatascience.medium.com/the-case-for-the-chief-ai-officer-the-newest-c-suite-role-d55cca40c01)\n\nLa última supercomputadora de IBM está tomando varias direcciones innovadoras: es la primera supercomputadora optimizada para IA y nativa de la nube de IBM, y se aloja completamente dentro de la nube de IBM en lugar de estar en el lugar donde se encuentra tradicionalmente una supercomputadora. Según IBM, \"este es el entorno preferido para los investigadores de IBM que crean las capacidades de IA más avanzadas, incluido el trabajo en modelos fundamentales y un lugar donde colaboran con socios en la creación de modelos\".\n\n[https://futurumresearch.com/research-notes/ibms-latest-supercomputer-is-cloud-based-and-ai-focused/](https://futurumresearch.com/research-notes/ibms-latest-supercomputer-is-cloud-based-and-ai-focused/)\n\n## Computación Ubicua\n\nSegún un estudio de 451 Research encargado por Oracle Cloud Infrastructure, casi todos los viajes a la nube en empresas se están convirtiendo en viajes multicloud. El estudio encontró que el 98% de las empresas encuestadas utilizan o planean utilizar al menos dos proveedores de servicios en la nube. Los principales impulsores de las estrategias multicloud son la soberanía de datos y la optimización de costos. Las empresas están planificando proactivamente estrategias multicloud para el futuro, siendo la redundancia de datos el caso de uso futuro más anticipado.\n\n[https://www.cloudcomputing-news.net/news/2023/feb/21/98-of-firms-using-public-cloud-adopt-multicloud-infrastructure-provider-strategy/](https://www.cloudcomputing-news.net/news/2023/feb/21/98-of-firms-using-public-cloud-adopt-multicloud-infrastructure-provider-strategy/)\n\nLa \"super nube\", una arquitectura de nube que permite la migración sin interrupciones de aplicaciones entre diferentes proveedores de nube, fue propuesta por primera vez por investigadores de la Universidad de Cornell en 2016. El concepto ha resurgido como solución a los desafíos del multi-nube, proporcionando una red homogénea para unir los recursos de la nube, lo que resulta en migración sin interrupciones, seguridad consistente y rendimiento óptimo.\n\n[https://www.cloudcomputing-news.net/news/2023/feb/22/here-comes-the-supercloud-what-does-it-mean-for-multi-cloud-complexity/](https://www.cloudcomputing-news.net/news/2023/feb/22/here-comes-the-supercloud-what-does-it-mean-for-multi-cloud-complexity/)\n\nLa demanda de DRAM para servidores superará la de smartphones debido al creciente uso de aplicaciones en la nube, IA y HPC; se pronostica que el contenido de DRAM para servidores aumentará un 12,1% interanual en 2023 en comparación con el 6,7% para smartphones. La memoria del servidor representará el 37,6% del total de la producción de bits de DRAM, frente al 36,8% para DRAM móvil. La pandemia COVID-19 ha impulsado la demanda de servicios en la nube, lo que ha resultado en un fuerte aumento en los envíos de servidores.\n\n## Ciberseguridad\n\nLa empresa de ciberseguridad Resecurity ha advertido sobre una serie de ciberataques que han apuntado a centros de datos en todo el mundo durante los últimos 18 meses, lo que ha dado lugar a la exfiltración de datos y la publicación de credenciales de acceso en la web oscura. Aunque Resecurity no nombró a las víctimas, Bloomberg afirma que importantes corporaciones, incluyendo Alibaba, Amazon, Apple, BMW, Goldman Sachs, Huawei Technologies, Microsoft y Walmart, tuvieron sus credenciales de centro de datos robadas.\n\n[https://www.csoonline.com/article/3688909/cyberattacks-hit-data-centers-to-steal-information-from-global-companies.html#tk.rss_all](https://www.csoonline.com/article/3688909/cyberattacks-hit-data-centers-to-steal-information-from-global-companies.html#tk.rss_all)\n\nLa firma de ciberseguridad Menlo Labs ha descubierto un grupo desconocido de actores amenazantes que está atacando a entidades gubernamentales a través de una campaña basada en Discord usando el descargador PureCrypter. La campaña utiliza el dominio de una organización sin fines de lucro comprometida como centro de comando y control para entregar una carga útil secundaria, que incluye Redline Stealer, AgentTesla, Eternity, Blackmoon y Philadelphia Ransomware.\n\n[https://www.menlosecurity.com/blog/purecrypter-targets-government-entities-through-discord](https://www.menlosecurity.com/blog/purecrypter-targets-government-entities-through-discord)\n\nEl ejercicio militar de ciber-guerra más grande en Europa Occidental ocurrió recientemente en Estonia, con la participación de 34 equipos de 11 países en una batalla cibernética en vivo. El evento de siete días puso a prueba las respuestas de los participantes a escenarios cibernéticos comunes y complejos, incluidos ataques a redes y sistemas de control industrial (SCI). Un escenario simula ataques a sistemas robóticos no tripulados. Los equipos de Italia, Estonia y el Reino Unido fueron los mejores intérpretes, juzgados según su rapidez en identificar y responder a las amenazas cibernéticas.\n\n[https://www.securityweek.com/11-countries-take-part-in-military-cyberwarfare-exercise/](https://www.securityweek.com/11-countries-take-part-in-military-cyberwarfare-exercise/)\n\n## Podcast de abrazando la transformación digital.\n\nEcha un vistazo al episodio completo de esta semana \"Certifying Autonomous Flight\", donde Darren entrevista a Luuk Van Dijk de Daedalean.\n\n[https://www.embracingdigital.org/episode-EDT126](https://www.embracingdigital.org/episode-EDT126)\n\n\n\n","guests":null,"link":"/brief-EDW3-es","image":"./briefs/edw-3/es/thumbnail.png","lang":"es","summary":"Summary"},{"id":197,"type":"News Brief","title":"2023-8-26","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n\nEstamos en plena revolución digital. Durante este tiempo, puede resultar desafiante filtrar toda la exageración y encontrar lo que funciona, lo que sigue siendo viable dentro de dos años y lo que contribuye a tu organización. Muchas organizaciones de TI necesitan ayuda con mensajes confusos y tecnologías contradictorias. Nosotros ayudamos a las organizaciones a separar el ruido y abrazar la transformación digital. El mundo de la transformación digital está en constante cambio y necesitas saber qué es viable hoy en día. Escucha nuestro podcast semanal para conocer las últimas noticias en ciberseguridad, comunicaciones avanzadas, gestión de datos, inteligencia artificial, edge y computación en la nube.\n## Ciberseguridad\n\n\nHa surgido una nueva amenaza. El grupo cibernético \"Tifón de Lino\", se cree que está vinculado a China, ha estado causando revuelo. Se sospecha que están llevando a cabo sofisticados ciberataques contra organizaciones globales. Expertos monitorean de cerca esta situación, ya que podría tener importantes implicaciones para la seguridad digital. Manténgase en sintonía para conocer las actualizaciones sobre esta historia en desarrollo. [Leer más](https://thehackernews.com/2023/08/china-linked-flax-typhoon-cyber.html)\n\n\nLas empresas de ciberseguridad están dando la alerta, ya que los ataques de ransomware aumentaron más de ciento cincuenta y tres por ciento en comparación con hace un año y más de dieciséis por ciento el mes pasado. Los expertos advierten que las organizaciones deben fortalecer sus defensas y educar a los empleados sobre estas amenazas para protegerse contra costosas violaciones de datos. [Leer más](https://www.securityweek.com/cybersecurity-companies-report-surge-in-ransomware-attacks/)\n\n\nEl sector de la ciberseguridad se enfrenta a una grave escasez de talento, lo que aumenta las expectativas de encontrar soluciones. La demanda de profesionales capacitados en este campo en constante evolución está en ascenso, sin embargo, la brecha persiste. Para abordar esto, las empresas están explorando estrategias como mejorar las habilidades de los empleados actuales y fomentar la diversidad en la industria. Tal vez sea hora de volver a la escuela para obtener una certificación en ciberseguridad. [Leer más](https://www.helpnetsecurity.com/2023/08/25/cybersecurity-talent-shortage-expectations/)\n\nInteligencia Artificial\n\n\nLa inteligencia artificial ha devuelto la voz a una mujer paralizada en un increíble avance. Investigadores en UCSF han utilizado la tecnología de IA para decodificar las señales cerebrales de la mujer y traducirlas en habla. Este logro revolucionario podría ayudar a numerosas personas a recuperar su capacidad de comunicación. Para obtener más información sobre esta historia inspiradora, visita un artículo detallado en ucsf.edu. [Leer más](https://www.ucsf.edu/news/2023/08/425986/how-artificial-intelligence-gave-paralyzed-woman-her-voice-back)\n\n\nLos estudiantes de bachillerato se están preparando para el surgimiento de la inteligencia artificial (IA). Investigadores del MIT han lanzado una nueva iniciativa para ayudar a preparar a los estudiantes para el futuro impulsado por la IA. Este programa tiene como objetivo dotar a las mentes jóvenes con las habilidades y conocimientos necesarios para prosperar en un mundo cada vez más influenciado por la tecnología de IA. Echa un vistazo a la entrevista de Darren con Pete Schmitz en el episodio del podcast \"Aceptando la Transformación Digital\", Entrenando a la Próxima Generación en IA, para tener una visión detallada de lo que está ocurriendo hoy en día. [Leer más](https://news.mit.edu/2023/how-to-help-high-schoolers-prepare-rise-of-artificial-intelligence-0824)\n\n\nEn un análisis exhaustivo de DeZeen que provoca reflexión, se examina el costo ambiental de la inteligencia artificial (IA). A medida que las aplicaciones de IA se expanden, también lo hace su consumo de energía, lo que potencialmente agrava las preocupaciones climáticas. Esta examinación crítica explora los desafíos de sostenibilidad planteados por la IA y la necesidad urgente de soluciones ecológicas en el desarrollo y despliegue de tecnología de IA. Manténganse atentos para obtener una comprensión más profunda de este apremiante problema ambiental. [Leer más](https://www.dezeen.com/2023/08/26/dezeen-in-depth-examines-the-environmental-cost-of-artificial-intelligence/)\n\nComputación ubicua\n\n\nA medida que nos acercamos a la era de la computación cuántica, resguardar la infraestructura crítica se vuelve aún más crucial. C4ISRNET se adentra en los desafíos y estrategias para proteger sistemas vitales en este nuevo panorama tecnológico. El potencial de la computación cuántica para romper los métodos de encriptación actuales genera preocupación, resaltando la necesidad de soluciones innovadoras de ciberseguridad. [Leer más](https://www.c4isrnet.com/it-networks/2023/08/21/how-to-protect-critical-infrastructure-in-the-quantum-computing-era/)\n\n\nSegún informa XDA developers, Dropbox está poniendo fin a su política de almacenamiento ilimitado en la nube. Esta decisión se toma en respuesta a casos de abuso por parte de algunos usuarios. Como resultado, Dropbox está implementando nuevas limitaciones de almacenamiento, lo cual representa un cambio significativo para su base de usuarios. Este desarrollo subraya los desafíos continuos que enfrentan los proveedores de almacenamiento en la nube al equilibrar las demandas de los usuarios con modelos de negocio sostenibles. [Leer más](https://www.xda-developers.com/dropbox-ends-unlimited-cloud-storage-policy-abused/)\n\n\nLa pandemia mundial en dos mil veinte ha revitalizado un sector tecnológico que sufría un crecimiento estancado, la Infraestructura de Escritorio Virtual. Microsoft y VMware están redefiniendo el VDI y la tecnología en la nube empresarial. SiliconANGLE discute cómo estas gigantes tecnológicas colaboran para redefinir el VDI y mejorar las soluciones en la nube. Esta alianza ofrecerá a las empresas mayor flexibilidad, escalabilidad y rendimiento en los entornos de TI. [Leer más](https://siliconangle.com/2023/08/24/microsoft-vmware-redefining-business-vdi-cloud-technology-vmwareexplore/)\n\nPodcast de Transformación Digital\n\n\n\nEsta semana, continúa la serie de Abrazando IA Generativa, presentando entrevistas con profesores de secundaria que han adoptado ChatGPT y explorando nuevos casos de uso para la IA generativa. Suscríbete a Transformación Digital Enfocada en profundas discusiones sobre temas candentes actuales. [Leer más](http://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW30-es","image":"./briefs/edw-30/es/thumbnail.png","lang":"es","summary":"Aquí tienes las últimas noticias sobre la transformación digital para la semana del 27 de agosto de 2023. Las historias de esta semana tratan sobre ciberseguridad, inteligencia artificial y computación ubicua. En este episodio encontrarás historias sobre un aumento significativo del 150% en los ataques de ransomware, cómo la computación cuántica ha expuesto las vulnerabilidades de ciberseguridad en el Internet de las cosas y cómo la inteligencia artificial está ayudando a restituir la voz de las personas paralizadas."},{"id":198,"type":"News Brief","title":"2023-9-2","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n\nEstamos en plena revolución digital. Durante este tiempo, puede resultar desafiante filtrar toda la exageración y encontrar lo que funciona, lo que sigue siendo viable dentro de dos años y lo que contribuye a tu organización. Muchas organizaciones de TI necesitan ayuda con mensajes confusos y tecnologías contradictorias. Nosotros ayudamos a las organizaciones a separar el ruido y abrazar la transformación digital. El mundo de la transformación digital está en constante cambio y necesitas saber qué es viable hoy en día. Escucha nuestro podcast semanal para conocer las últimas noticias en ciberseguridad, comunicaciones avanzadas, gestión de datos, inteligencia artificial, edge y computación en la nube.\n## Seguridad cibernética\n\n\n\nUn reciente ciberataque, se cree respaldado por China, ha generado preocupaciones para la seguridad nacional en Guam. El grupo de hackers Volt Typhoon apuntó a las telecomunicaciones y la red eléctrica de Guam para establecer una presencia sigilosa. Guam es considerado un importante centro logístico para los Estados Unidos en cualquier conflicto con China. Los hackers aprovecharon una vulnerabilidad en el firewall Fortinet Fortigate, un destacado producto de protección cibernética. Este ataque resalta una nueva amenaza, ya que los hackers respaldados por los estados nacionales cada vez más apuntan a herramientas de ciberseguridad cruciales para proteger sistemas. [Leer más](https://www.msn.com/en-us/news/other/cybersecurity-tools-are-new-targets-for-nation-state-hackers/ar-AA1g5mib)\n\n\nUn ciberataque causó una interrupción tecnológica de una semana en el sistema de salud de las Hermanas del Hospital, incluyendo St. Elizabeth en O'Fallon, Illinois. El CEO Damond Boatright confirmó que el incidente interrumpió el servicio de internet, los sitios web, los teléfonos y las aplicaciones informáticas. Aunque se están haciendo avances para restaurar los sistemas críticos, el sitio web de HSHS sigue caído y se ha interrumpido el acceso de los pacientes a las cartas en línea y la comunicación con los médicos. Este es el segundo ataque a grupos hospitalarios en Estados Unidos en dos meses. [Leer más](https://www.bnd.com/news/local/article278879724.html)\n\n\n\nLos colegios comunitarios se enfrentan a una creciente amenaza cibernética: los \"estudiantes fantasmas\". Estas identidades ficticias aprovechan los procesos de inscripción para acceder a recursos académicos, comprometiendo potencialmente los datos de los estudiantes y la integridad institucional. Con los hackers volviéndose cada vez más sofisticados, los colegios comunitarios invierten en medidas de ciberseguridad para combatir esta amenaza. La batalla contra los \"estudiantes fantasmas\" destaca la necesidad de una sólida seguridad digital en las instituciones educativas para proteger la información confidencial y garantizar un entorno de aprendizaje seguro. [Leer más](https://www.chronicle.com/article/meet-the-cybersecurity-threat-haunting-community-colleges-ghost-students)\n\nInteligencia Artificial\n\n\nEl uso de IA en la educación superior ha desencadenado un debate entre aquellos que ven su potencial para la personalización y la eficiencia, y aquellos que se preocupan por la ética y la pérdida de la conexión humana. Se necesita una consideración cuidadosa para integrar la IA en las aulas universitarias. La profesora universitaria Laura Torres Newey compartió su enfoque para abordar este tema en un episodio del podcast \"Abrazando la Transformación Digital\". Sus ideas revelan los desafíos y oportunidades de incorporar la IA en la educación superior. [Leer más](https://theconversation.com/should-ai-be-permitted-in-college-classrooms-4-scholars-weigh-in-212176)\n\n\nLa industria del entretenimiento enfrenta disputas laborales, con la inteligencia artificial (IA) en el centro del conflicto. La huelga del Sindicato de Actores de Pantalla ha entrado en su octava semana, con los trabajadores en Hollywood exigiendo una compensación justa y control creativo en el uso de la tecnología impulsada por IA. Esta disputa refleja la lucha de la industria por equilibrar los beneficios de la IA, como la eficiencia de costos y la innovación, con las preocupaciones sobre el desplazamiento laboral y la autonomía artística. A medida que el panorama del entretenimiento evoluciona, resolver estos problemas laborales relacionados con la IA será crucial para los trabajadores y el futuro de la industria. ¡El mundo está observando! [Leer más](https://www.pbs.org/newshour/show/why-artificial-intelligence-is-a-central-dispute-in-the-hollywood-strikes)\n\n\nLa regulación de la inteligencia artificial (IA) es una preocupación global, y los países están adoptando enfoques diversos para abordarla. China, Israel y la Unión Europea (UE) están a la vanguardia de la gobernanza de la IA. China se enfoca en la ética y la seguridad de datos de la IA, Israel promueve la innovación a través de entornos reguladores en pruebas y la UE propone normas estrictas de IA para garantizar la responsabilidad. A medida que la IA continúa moldeando diversas industrias y aspectos de la vida diaria, encontrar el equilibrio adecuado entre la innovación y la regulación sigue siendo un desafío crucial en el escenario global. [Leer más](https://www.washingtonpost.com/world/2023/09/03/ai-regulation-law-china-israel-eu/)\n\n## Computación en el Borde\n\n\nIntel está fortaleciendo la seguridad de la computación en el borde al introducir una capa revolucionaria de protección. La nueva tecnología de la empresa promete proteger los dispositivos y datos en el borde, abordando preocupaciones críticas de ciberseguridad. Con la creciente importancia de la computación en el borde en diversas industrias, este avance representa un gran paso en garantizar la integridad y seguridad de los entornos de computación distribuida. [Leer más](https://www.fool.com/investing/2023/09/01/intel-adds-a-layer-of-protection-to-edge-computing/)\n\n\nGarantizar la seguridad de la Internet de las Cosas (IoT) es crucial para proteger la infraestructura crítica, como las redes eléctricas y las plantas de tratamiento de agua. A medida que los dispositivos IoT se integran cada vez más en estos sistemas, protegerlos de las amenazas en evolución, incluidos los riesgos potenciales asociados con ciertos países, es un tema apremiante para los Estados Unidos. Los desafíos e implicaciones de la tecnología china en IoT están siendo objeto de un examen minucioso en este momento, lo que destaca la necesidad de medidas proactivas para salvaguardar la infraestructura crítica en el país. [Leer más](https://www.forbes.com/sites/davealtavilla/2023/09/03/securing-the-iot-from-the-threat-china-poses-to-us-infrastructure/?sh=4325a3f112c0)\n\n\nLa informática perimetral no es solo para la fabricación y la gestión de infraestructuras críticas. La informática perimetral está revolucionando la atención médica al permitir el procesamiento de datos en tiempo real, lo que lleva a tiempos de respuesta más rápidos para datos esenciales del paciente. Admite la monitorización remota y la telemedicina, lo que hace que la atención médica sea más accesible y reduce la carga en las instalaciones de atención médica. Además, la informática perimetral mejora la privacidad y seguridad de los datos al procesar información sensible del paciente localmente, reduciendo el riesgo de brechas de datos y garantizando el cumplimiento de las regulaciones de atención médica. [Leer más](https://www.ft.com/partnercontent/ntt-ltd/edge-computing-delivers-healthcare-beyond-the-clinic.html?blaid=3846770)\n\nAceptando la Transformación Digital\n\n\n\nLa serie abarca de IA generativa continúa con entrevistas sobre cómo prepararse para los datos en la revolución de GenAI y cómo la educación superior maneja la IA generativa en el aula. [Leer más](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW31-es","image":"./briefs/edw-31/es/thumbnail.png","lang":"es","summary":"Noticias de la semana del 3 de septiembre de 2023 sobre la adopción de lo digital. Esta semana, se presentan actualizaciones sobre ciberseguridad, inteligencia artificial y computación en el borde. Los ciberataques de China a bases militares e infraestructura crítica, los esfuerzos de Intel para mejorar la seguridad en el espacio de IoT y los desafíos de los gobiernos para regular la inteligencia artificial."},{"id":199,"type":"News Brief","title":"2023-9-9","tags":["ai","gpt4","openai","microsoft","generativeai","environmentalimpact","irs","taxevasion","education","cybersecurity","iosspyware","zerodayvulnerability","northkoreanhackers","quantumrandomness","cloudfirst","cloudsmart","embracingdigital","generativeaipolicy","highereducation"],"body":"\n\n## Inteligencia Artificial\n\n\n¡El lugar de nacimiento de GPT-4 ha sido descubierto! Los campos de maíz de Des Moines, Iowa, han sido el centro de atención de lo último de OpenAI, pero a un costo significativo para el medio ambiente. El último informe medioambiental de Microsoft reveló un aumento del 34% en el consumo mundial de agua de 2021 a 2022, que asciende a casi 1.700 millones de galones. Este aumento se debe principalmente a la importante inversión de la empresa en IA generativa y su asociación con OpenAI. El impacto de la IA generativa en el medio ambiente es considerable y grandes organizaciones como Microsoft, Google y OpenAI están tomando medidas para abordar estas preocupaciones. [Leer más](https://apnews.com/article/chatgpt-gpt4-iowa-ai-water-consumption-microsoft-f551fde98083d17a7e8d904f8be822c4)\n\n\n\nEl Servicio de Impuestos Internos en los Estados Unidos planea implementar tecnología de inteligencia artificial para mejorar la supervisión de grandes bufetes de abogados, fondos de cobertura, firmas de inversión privada y inversores inmobiliarios. La iniciativa tiene como objetivo detectar casos complicados de evasión fiscal y recuperar los ingresos federales adeudados por parte de contribuyentes adinerados. El IRS ha asignado ochenta mil millones de dólares de la Ley de Reducción de la Inflación para abordar patrones y tendencias en la realización de auditorías significativas y aplicar sanciones contra evasores de impuestos en estas industrias. [Leer más](https://www.nytimes.com/2023/09/08/us/politics/irs-deploys-artificial-intelligence-to-target-rich-partnerships.html)\n\n\nLos educadores y académicos están a la vanguardia de abordar el papel de la IA en la educación. A medida que los estudiantes regresan a la escuela este otoño, varios artículos de revistas de todo el mundo cubren aplicaciones de IA, preocupaciones éticas, ampliación en lugar de automatización, preparación de los estudiantes para el trabajo integrado con IA, detección de deepfakes, preservación del sentido común humano y aprovechamiento de la IA generativa para la enseñanza. Estas ideas proporcionan una orientación valiosa para dar forma al futuro de la IA en la educación. Además, echa un vistazo a la entrevista de Darren con el New York Times y la profesora de inglés Laura Torres Newey sobre cómo integra la IA en sus clases este otoño. [Leer más](https://daily.jstor.org/artificial-intelligence-and-education-a-reading-list/)\n\n## Ciberseguridad\n\n\nChina está tomando en serio la ciberseguridad. Las nuevas regulaciones de ciberseguridad de China tienen como objetivo mejorar la seguridad digital para internet, pero las pequeñas empresas podrían enfrentar desafíos. El impacto completo en las empresas de todos los tamaños sigue siendo incierto y podría haber repercusiones para la privacidad en línea. Sin embargo, estas medidas son necesarias para proteger los datos en línea y protegerse contra las amenazas cibernéticas. Las personas y las empresas deben mantenerse actualizadas con estos cambios y tomar las precauciones necesarias para asegurar su presencia en línea. [Leer más](https://www.scmp.com/news/china/politics/article/3233931/china-steps-cybersecurity-enforcement-smaller-businesses-are-feeling-heat)\n\n\nEl Sistema iPhone, una vez inmune, ha sufrido un ataque en la última semana con la detección de spyware en el popular iOS que ejecuta los iPhones. Apple ha tomado medidas rápidas tras descubrir amenazas de spyware. Han lanzado actualizaciones de software esenciales para mantener seguros tus dispositivos. ¡Es como tener un superhéroe protegiendo tu mundo tecnológico! Profundizaremos en los detalles de estas actualizaciones, cómo han frustrado posibles vulneraciones de seguridad y qué significa esto para tu seguridad digital. Sigue atento mientras desentrañamos los últimos avances en tecnología y ciberseguridad. [Leer más](https://www.washingtonpost.com/politics/2023/09/08/apple-issues-software-updates-after-spyware-discoveries/)\n\n\nSupuestamente, los hackers norcoreanos han aprovechado una vulnerabilidad de día cero, la cual les otorga acceso a una fortaleza de defensas digitales. Este ciberataque representa una amenaza importante para los sistemas objetivos, las filtraciones de datos y el panorama general de la ciberseguridad. Se están llevando a cabo esfuerzos para contrarrestar la amenaza, incluyendo parches y salvaguardias para protegerse contra ataques similares en el futuro. [Leer más](https://thehackernews.com/2023/09/north-korean-hackers-exploit-zero-day.html)\n\n## Computación Ubicua\n\n\nLa integración de sistemas de IA generativa está cambiando la arquitectura en la nube, impregnándola de un cerebro inteligente capaz de aprender y crear de forma independiente. Esta transformación remodela la infraestructura en la nube, fortaleciendo la automatización, optimizando la asignación de recursos y mejorando las medidas de seguridad. La infusión de sistemas de IA generativa en la nube presenta oportunidades y desafíos prometedores, y emprenderemos un viaje en profundidad para desentrañarlos. [Leer más](https://www.infoworld.com/article/3706094/adding-generative-ai-systems-may-change-your-cloud-architecture.html)\n\n\nEn un logro revolucionario, el MIT ha tomado el control de la aleatoriedad cuántica por primera vez. ¡Es como domar lo salvaje e impredecible! Nos adentraremos en los detalles de este notable logro, que podría tener profundas implicaciones para la computación cuántica y la criptografía. Únete a nosotros mientras exploramos cómo el avance del MIT nos permite aprovechar la aleatoriedad inherente del mundo cuántico, abriendo nuevas fronteras en tecnología y seguridad. El tiempo dirá qué impacto tendrá esto en la computación. [Leer más](https://scitechdaily.com/harnessing-the-void-mit-controls-quantum-randomness-for-the-first-time/)\n\n\n'Primero la nube' está obsoleto, y 'Inteligente en la nube' es la nueva palabra de moda en tecnología. Piénsalo como pasar de 'la nube es la respuesta' a 'la nube como un aliado estratégico'. Profundizaremos en los detalles de este cambio, explorando lo que 'inteligente en la nube' significa para las empresas y las estrategias de TI. Únete a nosotros mientras discutimos cómo las organizaciones evolucionan sus enfoques para aprovechar la nube de manera más inteligente, haciéndola parte central de sus viajes de transformación digital. [Leer más](https://www.infoworld.com/article/3705615/cloud-first-is-dead-cloud-smart-is-whats-happening-now.html)\n\n## Abrazando la Transformación Digital\n\nEsta semana continúa la serie Abrazando la IA Generativa en el podcast con una entrevista fascinante acerca del desarrollo de una Política de IA Generativa para tu lugar de trabajo y cómo la educación superior está abordando la IA generativa en el aula. Sintoniza los últimos episodios los martes y jueves. [Leer más](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW32-es","image":"./briefs/edw-32/es/thumbnail.png","lang":"es","summary":"Obtén las últimas noticias digitales para la semana del 10 de septiembre de 2023 que cubren Inteligencia Artificial, Ciberseguridad y Computación Ubicua. Esta semana, entérate de una nueva amenaza cibernética para el iPhone, el lugar de origen de GPT-4 y el enfoque del MIT para abordar la incertidumbre de la computación cuántica."},{"id":200,"type":"News Brief","title":"2023-9-16","tags":["aisummit","rishisunak","chineseofficialsban","nationalsecurity","ukchinatensions","amazonselfpublishers","aigeneratedcontent","digitalpublishing","columbiauniversityresearch","aireliability","languagemodels","chatbotperformance","cognitivescience","webassembly(wasm)","edgecomputingrevolution","edgedevices","latencyreduction","secureexecution","iotapplications","mobilecomputing","ai","security","edge","embracingdigital"],"body":"\n\n\n## Inteligencia Artificial\n\n\nEl Canciller del Reino Unido, Rishi Sunak, está considerando prohibir la asistencia de funcionarios chinos al 50% de la próxima Cumbre de IA debido a preocupaciones de seguridad nacional. Esta medida refleja las crecientes tensiones entre el Reino Unido y China en relación a la tecnología y la propiedad intelectual. Los detalles de esta posible prohibición aún no han sido finalizados. La Cumbre de IA es un evento destacado para la colaboración tecnológica global. [Leer más](https://www.theguardian.com/technology/2023/sep/15/rishi-sunak-considers-banning-chinese-officials-from-half-of-ai-summit)\n\n\nAmazon ahora requiere que los autores independientes revelen cualquier participación de la inteligencia artificial en sus obras para aumentar la transparencia en el mundo de la publicación digital. Esta medida surge como respuesta a preocupaciones sobre el mercado inundado de contenido generado por IA. Estas nuevas normas afectarán a la industria de la autoedición en Amazon. [Leer más](https://www.theguardian.com/books/2023/sep/11/self-publishers-must-declare-if-content-sold-on-amazons-site-is-ai-generated)\n\n\n\nLa investigación de la Universidad de Columbia resalta preocupaciones sobre la confiabilidad de la IA en la toma de decisiones. El estudio encontró que incluso los modelos de lenguaje avanzados pueden confundir tonterías con lenguaje natural, planteando interrogantes sobre la precisión. Esto ofrece una oportunidad para mejorar el rendimiento de los chatbots y comprender el procesamiento del lenguaje humano. Al cerrar la brecha entre la IA y la ciencia cognitiva, podemos crear asistentes impulsados por IA más útiles y efectivos que satisfagan mejor a sus usuarios. [Leer más](https://www.sciencedaily.com/releases/2023/09/230914114704.htm)\n\n## Computación en el borde\n\n\nWebAssembly (Wasm) está listo para revolucionar el cómputo en el borde. Esta tecnología permite ejecutar código de diferentes lenguajes en dispositivos de borde, mejorando flexibilidad y rendimiento. Al reducir la latencia y habilitar una ejecución segura y eficiente, Wasm puede transformar cómo se despliegan y ejecutan aplicaciones en el borde. Los desarrolladores pueden aprovechar el potencial de Wasm para crear soluciones de cómputo en el borde más dinámicas y receptivas, mejorando diversas industrias como IoT y cómputo móvil. [Leer más](https://www.infoworld.com/article/3703052/how-webassembly-will-transform-edge-computing.html)\n\n\n\n¡La carrera por llevar la IA al extremo se está intensificando! Cadence Design Systems ha presentado nuevas herramientas de software de propiedad intelectual de IA (AI IP) que ofrecen capacidades de descarga desde procesadores anfitriones. Estas herramientas tienen como objetivo optimizar las cargas de trabajo de IA, mejorando el rendimiento y la eficiencia energética en dispositivos de borde. La tecnología permite una ejecución más eficiente de tareas de IA, lo que la hace adecuada para diversas aplicaciones, incluyendo IoT y computación de borde, donde la optimización de recursos es crucial. Las herramientas de software AI IP de Cadence pueden ayudar a acelerar la adopción de la IA en dispositivos de borde. [Leer más](https://www.edgeir.com/cadences-new-ai-ip-software-tools-promises-offloading-abilities-from-any-host-processor-20230914)\n\n\n\nLas conferencias de Edge Computing están en pleno apogeo este otoño, con no menos de siete conferencias principales en todo el mundo. Comenzando la última semana de septiembre en Ámsterdam, viajando por Toronto, Dubai, París y Londres, por nombrar algunos lugares. Desempolva tus maletas de viaje, este otoño será ocupado para las compañías de IoT. Puedes encontrar la lista completa en edgier.com. [Leer más](https://www.edgeir.com/edge-computing-events)\n\n## Ciberseguridad\n\n\nEl Centro de Respuesta de Seguridad de Microsoft ha identificado vulnerabilidades críticas en la biblioteca de software de código abierto NCurses, utilizada en sistemas similares a Unix (Linux e iOS). Estas fallas podrían permitir a los atacantes ejecutar código malicioso o causar ataques de denegación de servicio. Se insta a los usuarios a actualizar sus instalaciones de NCurses a la última versión para mitigar los posibles riesgos. Las vulnerabilidades en bibliotecas ampliamente utilizadas como NCurses destacan la importancia de las actualizaciones de seguridad regulares y la gestión de parches en el ecosistema de software. [Leer más](https://thehackernews.com/2023/09/microsoft-uncovers-flaws-in-ncurses.html)\n\n\nEl sitio web del Gestor de Descargas Gratuitas (FDM), un sitio muy popular para el software libre y de código abierto, ha sido comprometido, exponiendo a los usuarios a posibles riesgos. Los atacantes inyectaron código malicioso en el sitio, potencialmente afectando a los usuarios que descargaron el software entre el once y el doce de septiembre de dos mil veintitrés. Esta violación de seguridad enfatiza la importancia de descargar software solo de fuentes confiables y actualizar regularmente el software de seguridad para protegerse contra posibles amenazas. FDM ha tomado medidas para abordar el problema, pero los usuarios deben mantenerse alerta. [Leer más](https://thehackernews.com/2023/09/free-download-manager-site-compromised.html)\n\n\nSe informa que actores estatales de Irán han estado utilizando sofisticados ataques de spear-phishing para apuntar a individuos en los Estados Unidos, el Medio Oriente y Asia. Los ataques involucran correos electrónicos engañosos que se hacen pasar por organizaciones confiables con el objetivo de entregar malware y robar información sensible. Los expertos en seguridad enfatizan la necesidad de contar con sólidas medidas de seguridad de correo electrónico y de conciencia por parte de los usuarios para defenderse de este tipo de amenazas cibernéticas orquestadas por actores estatales. [Leer más](https://thehackernews.com/2023/09/iranian-nation-state-actors-employ.html)\n\n## Abrazando la Transformación Digital\n\nEsta semana continúa la serie \"Abrazando la IA Generativa\" con invitados hablando sobre la operacionalización de la IA generativa en el lugar de trabajo, incluyendo su utilización para mejorar la seguridad y generar informes de lenguaje natural. [Leer más](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW33-es","image":"./briefs/edw-33/es/thumbnail.png","lang":"es","summary":"Mantente al día con las últimas noticias sobre transformación digital para la semana del 17 de septiembre de 2023. Obtén las últimas actualizaciones sobre inteligencia artificial, computación en el borde y ciberseguridad. Los aspectos destacados de esta semana incluyen Microsoft descubriendo vulnerabilidades en el software de sus competidores, Amazon implementando requisitos de divulgación de IA y una lista completa de próximas conferencias de IoT este otoño."},{"id":201,"type":"News Brief","title":"2023-9-23","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Inteligencia Artificial\n\n\nLos físicos están desarrollando modelos generativos para la inteligencia artificial utilizando ecuaciones físicas bien comprendidas. El modelo generativo de flujo de Poisson (PFGM), que representa datos con partículas cargadas, puede producir imágenes de alta calidad de diez a veinte veces más rápido que los modelos de difusión. Los investigadores buscan explorar más procesos físicos y refinar el PFGM ajustando su dimensionalidad. [Leer más](https://www.quantamagazine.org/new-physics-inspired-generative-ai-exceeds-expectations-20230919/ )\n\n\n¿Qué tal si ejecutamos un chatbot en tu PC? Intel ha anunciado un nuevo microprocesador, previsto para diciembre, que permitirá que las laptops ejecuten chatbots de inteligencia artificial generativa sin depender de centros de datos en la nube. Esta capacidad, demostrada en su conferencia para desarrolladores de software, permite a las empresas y usuarios probar tecnologías de IA como ChatGPT sin enviar datos sensibles a la nube. [Leer más](https://www.reuters.com/technology/intel-says-newest-laptop-chips-software-will-handle-generative-ai-2023-09-19/)\n\n\nEl crecimiento de la comunidad de código abierto está causando revuelo en el desarrollo de la IA. No es algo nuevo en innovación, proyectos de código abierto como Hadoop y Spark han permitido a los desarrolladores avanzar en el desarrollo de la IA desde la década de 1970. Modelos generativos de IA como ChatGPT y Llama 2 se basan en fundamentos de código abierto, desafiando el statu quo propietario de los programas de IA. Los desarrolladores seguirán empujando los límites de la IA más allá de los confines del desarrollo comercial. [Leer más](https://www.zdnet.com/article/why-open-source-is-the-cradle-of-artificial-intelligence/)\n\n## Cómputo Ubicuo\n\n\nIntel ha lanzado su plataforma 'Developer Cloud' para que los desarrolladores prueben y desplieguen aplicaciones de inteligencia artificial y cómputo de alto rendimiento utilizando las últimas plataformas de hardware. La plataforma admite entrenamiento de IA, optimización de modelos y cargas de trabajo de inferencia, y se basa en software abierto con oneAPI, lo que permite elegir el hardware y lograr portabilidad de código. Durante el anuncio también se introdujeron otras tecnologías relacionadas con la IA. Cualquier persona puede registrarse en cloud.intel.com. [Leer más](https://ummid.com/news/2023/september/20-09-2023/intel-developer-cloud-reaches-general-availability.html)\n\n\nPara ayudar a frenar los gastos místicos y costosos de gestión de datos, WiMi Hologram Cloud ha desarrollado un nuevo algoritmo de compresión de datos holográficos que mejora la eficiencia de la computación en la nube al reducir las necesidades de transmisión y almacenamiento de datos. Estos avances y un protocolo distribuido de almacenamiento de imágenes están respondiendo a la creciente demanda de tecnología holográfica avanzada. Esto debería ayudar a las organizaciones que sufren con los costos de salida de datos en la nube. [Leer más](https://beststocks.com/wimi-hologram-cloud-revolutionizing-cloud-com/ )\n\n\nEn el evento Innovación 2023 de Intel, la empresa presentó una CPU de 288 núcleos como parte de su línea 'Sierra Forest' para servidores de alta densidad. Esta CPU tendrá 144 núcleos en sus dos chiplets, sumando un total de 288 núcleos y hilos. Intel también confirmó que su familia de procesadores Xeon de 5ta generación, 'Emerald Rapids', se lanzará el 14 de diciembre, ofreciendo mejoras en rendimiento manteniendo el consumo de energía. Intel hizo hincapié en su enfoque en aplicaciones de inteligencia artificial, con una supercomputadora para tareas de AI utilizando procesadores Xeon y aceleradores de IA Gaudi2. [Leer más](https://www.techspot.com/news/100221-intel-announces-288-core-sierra-forest-xeon-processor.html)\n\n## Seguridad Cibernética\n\n\nLa organización de hackers conocida como \"Spider Disperso\" está ganando notoriedad por sus ataques de ransomware a empresas. Los analistas sugieren que este grupo está compuesto principalmente por jóvenes de entre 17 y 22 años de países occidentales. Utilizan técnicas avanzadas de ingeniería social, como el intercambio de tarjetas SIM, y tácticas como \"SWATing\" para identificar puntos de acceso privilegiados. Aunque su motivación podría no ser monetaria, sus ataques han perturbado diversos sectores y han generado investigaciones por parte de las autoridades. [Leer más](https://www.reuters.com/technology/power-influence-notoriety-gen-z-hackers-who-struck-mgm-caesars-2023-09-22/)\n\n\nUn informe sobre las tendencias de ciberdelincuencia en India revela un aumento en los ciberataques, incluyendo phishing, malware y fraudes financieros, con el COVID-19 empeorando la situación. Los hallazgos destacan la urgencia de medidas de ciberseguridad más rigurosas, concienciación pública y colaboración entre las fuerzas del orden y los sectores privados para combatir las amenazas cibernéticas de manera efectiva. También se enfatiza la educación sobre las mejores prácticas de ciberseguridad para proteger a las personas de tales ataques. [Leer más](https://www.theregister.com/2023/09/21/india_cybercrime_trends_report/)\n\n\nPara ayudar a combatir la amenaza de la ciberseguridad, Microsoft Azure está tratando de aumentar la ciberseguridad a través de la educación. Azure ha anunciado recientemente más de sesenta horas de cursos gratuitos sobre ciberseguridad, incluyendo seguridad de nube híbrida, configuración de cortafuegos, gestión de parches, gestión de directorio activo y tecnologías de cifrado, por nombrar algunas. Puedes obtener más información en learn.microsoft.com. [Leer más](https://www.helpnetsecurity.com/2023/09/20/free-microsoft-azure-cybersecurity-resources/)\n\n## Aceptando la Transformación Digital\n\nEsta semana comienza una nueva serie sobre Construyendo una Estrategia en la Nube con arquitectos de soluciones en la nube de Intel. Darren entrevista a los expertos de Intel en migración en la nube, gestión de costos en la nube y evaluación de cargas de trabajo. Echa un vistazo a lo último en el podcast Abrazando la Transformación Digital. [Leer más](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW34-es","image":"./briefs/edw-34/es/thumbnail.png","lang":"es","summary":"Noticias de Transformación Digital para el 24 de septiembre de 2023, incluye inteligencia artificial, computación ubicua y ciberseguridad. Escucha acerca de los nuevos CPUs de Intel que están cambiando la IA y la computación de alta densidad, y cómo un grupo de hackers compuesto por adolescentes derribó los casinos MGM de Las Vegas."},{"id":202,"type":"News Brief","title":"2023-9-30","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","datasciencesolution","polystores","dataarchitecture","aichatbots","cybersecurityawareness","aisecuritycenter","googlechrome","mozillafirefox","digitaltransformationpodcast","intelcloudsolutionarchitects"],"body":"\n\n## Gestión de Datos\n\nBMLL Vantage, una solución de ciencia de datos, fue honrada con el premio 'Mejor Solución de Ciencia de Datos' en los Premios de Data Management Insight. Este galardón celebra el excepcional trabajo de BMLL Vantage en ciencia de datos, reconociendo sus importantes contribuciones a la industria. [Leer más](https://mondovisione.com/media-and-resources/news/bmll-vantage-wins-best-data-science-solution-at-the-data-management-insight-aw/)\n\nPolystores integran diversas fuentes de datos sin problemas, derribando silos y permitiendo un análisis transversal. Optimizan el rendimiento aprovechando las fortalezas de varias bases de datos y se adaptan a las tecnologías en evolución. Las organizaciones deberían utilizar polystores para desbloquear el potencial de los datos y tomar decisiones informadas en la era del big data. [Leer más](https://thenewstack.io/polystores-the-data-management-game-changer/)\n\n\nAl decidir sobre una arquitectura de datos, es esencial considerar factores como el tamaño de los datos, la frecuencia, la ubicación de origen, el tipo, la privacidad, el acceso y la urgencia. Comprender los diferentes tipos de arquitecturas y sus usos puede ayudarte a adaptarte a los cambios en los impulsores de negocio y las condiciones ambientales, y elegir la mejor solución para tu empresa. Se ha demostrado que tener en cuenta estos factores mejora las habilidades de toma de decisiones de los arquitectos de sistemas. [Leer más](https://www.intel.com/content/www/us/en/content-details/789953/content-details.html)\n\n## Inteligencia Artificial\n\n\nMark Zuckerberg reveló un conjunto de chatbots de IA en Meta Connect 2023, incluyendo uno con el rapero Snoop Dogg como Dungeon Master en el juego de rol D&D. Estos bots, modelados tras celebridades, no están disponibles para el público. Meta IA es el asistente de chatbot líder, construido sobre un modelo de lenguaje hecho a medida similar al ChatGPT de OpenAI, priorizando el estilo visual sobre la imitación de patrones de conversación. Los planes incluyen agregar capacidades de voz. [Leer más](https://kotaku.com/meta-quest-3-ai-chatgpt-snoop-dogg-facebook-chatbot-1850882666)\n\n\nEl primer ministro de Israel, Netanyahu, advierte sobre los potenciales peligros de la revolución de la inteligencia artificial, como las interrupciones a la democracia, la manipulación mental, las pérdidas de empleo y las guerras impulsadas por la IA. Insta a las naciones a abordar estas preocupaciones y evitar que las máquinas autodidactas controlen a los humanos. Sin embargo, también reconoce los aspectos positivos de la IA, como la asistencia robótica para los ancianos y el mejor transporte. Israel busca ser un líder global en IA. [Leer más](https://www.foxnews.com/world/netanyahu-warns-potential-eruption-ai-driven-wars-lead-unimaginable-consquences)\n\n\nUn empleado de OpenAI, Lilian Weng, compartió su experiencia emocional utilizando el modo de voz de ChatGPT para una conversación personal sobre estrés y equilibrio entre trabajo y vida personal. Mientras Weng encontró reconfortante la interacción, surgen preocupaciones acerca del papel de la inteligencia artificial en la provisión de terapia. Esto refleja una tendencia donde la IA busca parecer más humana pero ha enfrentado desafíos, como se ha visto en experimentos anteriores de terapia con IA que recibieron respuestas mixtas y, en algunos casos, causaron daño. Las consideraciones éticas son fundamentales al integrar la IA en contextos de salud mental. [Leer más](https://fortune.com/2023/09/28/generative-ai-cfos-company-strategy/)\n\n## Ciberseguridad\n\n\nOctubre marca el vigésimo mes de Concienciación sobre Ciberseguridad para promover la seguridad en línea. El tema de este año es \"Protejamos nuestro mundo\". Se centra en cuatro prácticas fundamentales de ciberseguridad: utilizar un gestor de contraseñas, implementar autenticación multifactor, reconocer y reportar intentos de phishing, e instalar regularmente actualizaciones. La iniciativa busca proporcionar información para ayudar a las personas a mantenerse más seguras en línea. [Leer más](https://www.ktsm.com/local/octobers-cybersecurity-awareness-month-2023/)\n\n\nLa Agencia de Seguridad Nacional de los Estados Unidos ha creado un centro de seguridad de IA para supervisar la IA en los servicios de defensa e inteligencia. El director, General Paul Nakasone, enfatizó la importancia de mantener a Estados Unidos a la vanguardia en IA y evitar que actores extranjeros roben las innovaciones estadounidenses. El centro promoverá la adopción segura de IA en las industrias de seguridad nacional y defensa. La IA desempeñará un papel significativo en la seguridad nacional, la diplomacia, la tecnología y la economía. [Leer más](https://www.aljazeera.com/news/2023/9/29/us-national-security-agency-unveils-artificial-intelligence-security-centre)\n\n\nSe ha encontrado un nuevo problema de seguridad en los navegadores Google Chrome y Mozilla Firefox. Esto también podría afectar a otros programas. Se llama \"StrangeU\". Los hackers podrían usarlo para controlar tu computadora. Las compañías han solucionado el problema, pero otros programas podrían seguir siendo vulnerables. Mantén todos tus programas actualizados y estate alerta ante problemas similares en otras aplicaciones. ¡Mantente seguro/a! [Leer más](https://arstechnica.com/security/2023/09/new-0-day-in-chrome-and-firefox-is-likely-to-plague-other-software/)\n\n## Podcast sobre la Transformación Digital\n\nEsta semana continúa la serie sobre arquitecturas en nube multi-híbrido con entrevistas adicionales de los arquitectos de soluciones en nube de Intel. Para escuchar estas entrevistas de treinta minutos, puedes encontrar \"abrazando la transformación digital\" en tus sitios favoritos de podcasts. [Leer más](https://www.embracingdigital.org/episode-EDT163)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW35-es","image":"./briefs/edw-35/es/thumbnail.png","lang":"es","summary":"Noticias de transformación digital para la semana del 1 de octubre, incluyendo historias sobre gestión de datos, inteligencia artificial y ciberseguridad. Ven a escuchar la celebración del mes de concienciación sobre ciberseguridad, un AI que convierte a Snoop Dogg en el maestro de mazmorras de D&D, y aprende sobre una nueva arquitectura de datos llamada Polystore."},{"id":203,"type":"News Brief","title":"2023-10-7","tags":["ai","edge","cybersecurity","precrime","aihealcare","mgmbreach","scatteredspider","aplhav","aiethics","convergeditot","iot","criticalinfrastructure"],"body":"\n\n## Ciberseguridad\n\n\nEl 5 de octubre, MGM Resorts International anunció que una brecha de datos y un ataque de ransomware habían causado interrupciones, resultando en una pérdida de cien millones de dólares durante el tercer trimestre. Los grupos de piratería AlphaV y Scattered Spider reclamaron la responsabilidad de la violación, en la cual supuestamente robaron datos del sistema de MGM y los mantuvieron para extorsionar. [Leer más](https://www.reuters.com/business/mgm-expects-cybersecurity-issue-negatively-impact-third-quarter-earnings-2023-10-05/)\n\n\nSecurityWeek ha informado del descubrimiento de un firmware con puerta trasera en dispositivos Android utilizados por escuelas de Estados Unidos. Los investigadores han identificado vulnerabilidades en computadoras portátiles y tabletas distribuidas a instituciones educativas, lo cual podría exponer datos sensibles de los estudiantes a amenazas cibernéticas. El firmware comprometido podría permitir a los ciberdelincuentes acceder y manipular los dispositivos de forma remota si se explotan. [Leer más](https://www.securityweek.com/android-devices-with-backdoored-firmware-found-in-us-schools/)\n\n\nEn el contexto del Mes de Concientización sobre Ciberseguridad 2023, se enfatiza en SC Media que se requiere más que simplemente promover la conciencia en ciberseguridad. SC Media insta a las organizaciones a aumentar el intercambio de inteligencia sobre ataques cibernéticos y las mejores prácticas en ciberseguridad, y desacredita los mitos de compartir información valiosa en la industria de la ciberseguridad. [Leer más](https://www.scmagazine.com/perspective/cybersecurity-awareness-month-2023-why-we-need-more-than-just-cybersecurity-awareness)\n\n## Inteligencia Artificial\n\n\nSegún el informe \"Libertad en la red\" de Freedom House de 2023, los gobiernos en todo el mundo están utilizando inteligencia artificial para la vigilancia y la censura. China está a la vanguardia en la censura impulsada por inteligencia artificial. Esto pone de relieve la necesidad de equilibrar el avance tecnológico con la protección de los derechos individuales y las libertades digitales. El informe destaca la necesidad de esfuerzos globales para salvaguardar la libertad digital de las personas. [Leer más](https://gizmodo.com/freedom-house-2023-freedom-on-the-net-report-ai-1850887842)\n\n\nUn reciente artículo publicado en Nature discute investigaciones sobre el uso de algoritmos de IA para predecir y gestionar convulsiones epilépticas. El estudio presenta resultados prometedores al predecir de manera precisa convulsiones en un grupo de pacientes, ofreciendo potencial para una gestión de las convulsiones más efectiva y mejores resultados para los pacientes. La predicción de convulsiones impulsada por IA podría mejorar significativamente la calidad de vida de las personas que viven con epilepsia y reducir los riesgos asociados. [Leer más](https://www.nature.com/articles/s41746-023-00931-7)\n\n\nSegún un informe de NBC Bay Area, un profesor de la Universidad de California en Berkeley está pionereando el uso de la inteligencia artificial (IA) para combatir la violencia doméstica. El profesor está desarrollando algoritmos de IA para analizar publicaciones en redes sociales y mensajes de texto en busca de señales de abuso doméstico, con el objetivo de brindar intervención temprana y apoyo a las víctimas. Esta innovadora aplicación de la tecnología de IA tiene el potencial de tener un impacto significativo en la identificación y abordaje de los problemas de violencia doméstica en la era digital. ¡Suena como el comienzo de la Pre-Criminalidad en el Área de la Bahía! [Leer más](https://www.nbcbayarea.com/news/local/uc-berkeley-professor-artificial-intelligence-domestic-violence/3336053/)\n\n## Computación en el borde\n\n\nLos investigadores están emitiendo una advertencia sobre la exposición de aproximadamente cien mil sistemas de control industrial (SCI) en internet, según BleepingComputer. Este hallazgo alarmante plantea preocupaciones sobre posibles ciberataques a infraestructuras críticas y instalaciones de fabricación. Los dispositivos SCI expuestos representan un riesgo de seguridad significativo, resaltando la necesidad urgente de mejorar las medidas de ciberseguridad para proteger las infraestructuras críticas de posibles amenazas y ataques. [Leer más](https://www.bleepingcomputer.com/news/security/researchers-warn-of-100-000-industrial-control-systems-exposed-online/)\n\n\nComo se discutió en un informe sobre Game Is Hard, la computación en el borde está surgiendo como una alternativa destacada al dominio de NVIDIA en el mercado de chips de IA. Con el aumento de la computación en el borde, más empresas están explorando soluciones de chips de IA adaptadas para el procesamiento local y el análisis de datos en tiempo real. Esta tendencia refleja el panorama en constante evolución de la tecnología de IA y la competencia en el mercado, potencialmente ofreciendo nuevas opciones e innovaciones para diversas industrias más allá del bastión tradicional de NVIDIA. [Leer más](https://gameishard.gg/news/the-rise-of-edge-computing-an-alternative-to-nvidias-dominance-in-the-ai-chip-market/282141/)\n\n\nEl artículo del Consejo Tecnológico de Forbes explora cómo la IA Generativa aborda las brechas de habilidades en la convergencia de la tecnología de información industrial y la tecnología operativa (IoT). Al automatizar tareas, analizar datos y optimizar procesos, la IA Generativa está ayudando a cerrar la brecha entre estos dominios tradicionalmente separados. Esta innovadora aplicación de la tecnología de IA tiene el potencial de mejorar la eficiencia y productividad en entornos industriales, permitiendo una integración y operación más fluida de los sistemas de tecnología de información y de IoT. [Leer más](https://www.forbes.com/sites/forbestechcouncil/2023/10/04/how-generative-ai-fills-skills-gaps-for-industrial-itot-convergence/)\n\n## Abrazando las noticias de Transformación Digital.\n\nEsta semana Darren publica el último episodio de la serie de Embracing Multi-Hybrid Cloud enfocada en la mejora continua en la operacionalización de una estrategia en la nube. Escucha en cualquier sitio de podcast en Embracing Digital Transformation. [Leer más](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW36-es","image":"./briefs/edw-36/es/thumbnail.png","lang":"es","summary":"Obtén las últimas noticias sobre la Transformación Digital para la semana del 8 de octubre de 2023. Los aspectos más destacados de esta semana incluyen un informe sobre la pérdida de $100 millones de MGM Resorts debido a un ciberataque, cómo se está utilizando la IA para ayudar a las víctimas de violencia doméstica y el impacto del Edge computing en los centros de datos de IA. Sintoniza para estar informado."},{"id":204,"type":"News Brief","title":"2023-10-14","tags":["compute","edge","israelcyberattacks","hamasconflict","redalertsystemapp","powerplantcyberthreats","generativeai","cloudmigration","cmauk","publiccloudinvestigation","avoslocker","ransomeware","criticalinfrastructure","ecybersecurity","embracingdigital","edw37","zerotrustarchitecture"],"body":"\n\n## ## Ciberseguridad\n\n\nIsrael libra batallas contra ciberataques durante el conflicto con Hamas. La aplicación del 'Sistema de Alerta Roja' fue comprometida para enviar alertas falsas y mensajes políticos. También se atacaron las plantas de energía de Israel. El director ejecutivo de Secure Cyber Defense, Shawn Waldman, advierte sobre amenazas cibernéticas en curso, destacando inteligencia reciente sobre actores extranjeros que apuntan a la infraestructura crítica de Estados Unidos. Él enfatiza la necesidad de un progreso continuo en ciberseguridad. [Leer más](https://www.wdtn.com/as-seen-on-2-news/israeli-cyberwarfare-cyberattacks-infrastructure/)\n\n\nIsrael y Palestina experimentaron un aumento en el tráfico de internet después de que Israel declarara la guerra a Hamas el siete de octubre de dos mil veintitrés. Los datos de Cloudflare revelan que los ciberataques dirigidos a Israel aumentaron, incluyendo ataques de denegación de servicio (DDoS) a periódicos israelíes. En Palestina, se observaron interrupciones en internet, posiblemente relacionadas con cortes de energía. Cloudflare está monitoreando estas tendencias y ofrece herramientas para rastrear los patrones de tráfico de internet. [Leer más](https://blog.cloudflare.com/internet-traffic-patterns-in-israel-and-palestine-following-the-october-2023-attacks/)\n\n\nLos ataques de ransomware se están volviendo más sofisticados, con dispositivos de red siendo cada vez más explotados para la entrega. La Información de Salud Protegida en el sector de la salud es un objetivo primordial. Las organizaciones de alto ingreso, especialmente en los Estados Unidos, son los destinos preferidos. Grupos más nuevos están surgiendo, y se están adoptando lenguajes como Rust y GoLang. Las organizaciones están mejorando las medidas de ciberseguridad, y se recomienda Cyble Vision para mantenerse al tanto de las amenazas de ransomware. [Leer más](https://thehackernews.com/2023/10/ransomware-attacks-doubled-year-on-year.html)\n\n## ## Edge Computing\nComputación en el borde\n\n\nLa banda de ransomware AvosLocker ha sido implicada en ataques contra sectores de infraestructura crítica en los Estados Unidos. Sus tácticas incluyen el uso de software legítimo y herramientas de administración remota de código abierto para comprometer redes, seguido de amenazas de extorsión de datos. AvosLocker surgió a mediados de 2021 y emplea técnicas para desactivar la protección antivirus, afectando entornos de Windows, Linux y VMware ESXi. El grupo es conocido por utilizar herramientas de código abierto y tácticas de \"living-off-the-land\" (LotL) para evitar la atribución. CISA y el FBI recomiendan medidas de mitigación para organizaciones de infraestructura crítica, que incluyen controles de aplicaciones, limitar los servicios de escritorio remoto, restringir el uso de PowerShell y mantener copias de seguridad sin conexión. Los ataques de ransomware han aumentado drásticamente en 2023, con los atacantes desplegando ransomware rápidamente después de obtener acceso inicial. [Leer más](https://thehackernews.com/2023/10/fbi-cisa-warn-of-rising-avoslocker.html)\n\n\nLa NSA ha publicado ELITEWOLF, un repositorio de firmas y análisis en su GitHub, para mejorar la seguridad de la Tecnología Operativa (OT) y contrarrestar la actividad cibernética maliciosa dirigida a la infraestructura crítica. Dada la creciente amenaza a los sistemas OT, la NSA recomienda a los propietarios y operadores de infraestructura OT utilizar ELITEWOLF como parte de una vigilante monitorización del sistema. Esta iniciativa sigue el Aviso de Ciberseguridad para Proteger las Tecnologías Operativas y los Sistemas de Control contra los Ataques Cibernéticos. [Leer más](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3554537/nsa-releases-a-repository-of-signatures-and-analytics-to-secure-operational-tec/)\n\n\nLa OTAN ha prometido responder firmemente ante cualquier ataque deliberado confirmado a la infraestructura crítica en el Mar Báltico. La promesa se produce a raíz de los daños a la infraestructura submarina en la región. Finlandia está investigando el incidente, que podría haber involucrado fuerzas externas, incluyendo Rusia. Si se demuestra que se trata de un ataque deliberado a la infraestructura crítica de la OTAN, la organización responderá con un esfuerzo unido y decidido. [Leer más](https://news.yahoo.com/attack-alliances-critical-infrastructure-deliberate-073708273.html)\n\n## Tecnología en la nube\n\n\nLa inteligencia artificial generativa reduce los esfuerzos de migración a la nube en treinta a cincuenta por ciento, aprovechando modelos de lenguaje grandes. Los modelos de lenguaje grandes pueden evaluar la infraestructura, trasladar las cargas de trabajo y verificar la efectividad de la migración. Bhargs Srivathsan de McKinsey señaló que la inteligencia artificial generativa y la nube son mutuamente beneficiosas, ya que la nube permite la inteligencia artificial generativa, que a su vez acelera la migración a la nube. Los modelos de lenguaje grandes pueden utilizarse para la generación de contenido, la participación del cliente, la creación de datos sintéticos y la codificación. [Leer más](https://www.theregister.com/2023/10/11/generative_ai_cloud_migration/)\n\n\nLas empresas con soluciones tecnológicas en sus instalaciones se enfrentan a la interrupción al migrar completamente a la nube. Para abordar este desafío, el enfoque de Nube Híbrida permite a las compañías mantener servicios fundamentales en sus instalaciones. Este enfoque agrega nuevas capacidades en la nube a la infraestructura heredada, reduciendo la interrupción y preservando la tecnología probada. Por ejemplo, una Nube Híbrida puede ser utilizada en operaciones de centros de contacto para agregar canales de chat y redes sociales a la infraestructura existente, permitiendo a las organizaciones manejar las fluctuaciones en el volumen de llamadas sin interrupciones significativas. [Leer más](https://www.techradar.com/pro/why-businesses-should-stop-wondering-about-cloud-and-go-hybrid)\n\n\nLa Autoridad de Competencia y Mercados del Reino Unido (CMA) investiga el mercado de la nube pública para asegurar una competencia justa para las empresas y los individuos. La investigación examinará las operaciones de proveedores como AWS, Microsoft Azure y Google Cloud para abordar preocupaciones sobre comportamiento anticompetitivo, uso de datos y barreras de entrada. La CMA busca fomentar la innovación, ofrecer precios competitivos y brindar opciones al cliente. [Leer más](https://www.techrepublic.com/article/cma-investigates-uk-public-cloud-market/)\n\n## # Podcast de Transformación Digital Abrazando\n\n\n\nEn el episodio de esta semana del Podcast de la Transformación Digital Abrazando, Darren entrevista a Rachel Driekosen sobre cómo se utiliza la inteligencia artificial para proteger a los niños en línea y llevar a los responsables ante la justicia. Manténganse atentos a una próxima serie sobre la Arquitectura de Confianza Cero en las próximas semanas. [Leer más](https://www.embracingdigital.org/episode-EDT167-en)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW37-es","image":"./briefs/edw-37/es/thumbnail.png","lang":"es","summary":"Transformación digital esta semana del 15 de octubre de 2023, informa sobre ataques cibernéticos continuos durante el conflicto entre Israel y Hamas y la vulneración de la aplicación 'Red Alert System'. Además, se ha demostrado que la inteligencia artificial generativa reduce los esfuerzos de migración a la nube, mientras que la CMA del Reino Unido investiga el mercado de la nube pública."},{"id":205,"type":"News Brief","title":"2023-10-21","tags":["keywords","ai","compute","cybersecurity","iranianhackers","cybersecuritybreach","ironnetclosure","secretmicrosoft365","cloudtrends","healthcareai","chinachipexports","openai","digitalnews","ubiquitouscomputing","digitaltransformation","dataencryption","aiethics","techindustry","emergingtech","intelinterview","aistrategies","quantumcomputing","datasecurity","techtrends"],"body":"\n\n\n## Seguridad cibernética\n\n\nHackers iraníes lograron infiltrar una red gubernamental de Oriente Medio y permanecer ocultos durante ocho meses antes de ser descubiertos. El ataque afectó a diversas agencias y aún se está evaluando el alcance de los daños. Expertos están investigando el incidente y haciendo hincapié en la necesidad de tomar medidas sólidas de ciberseguridad para protegerse de futuros ataques. Esta violación nos recuerda la importancia de priorizar la ciberseguridad en nuestro cada vez más digitalizado mundo. [Leer más](https://www.securityweek.com/iranian-hackers-lurked-for-8-months-in-government-network/)\n\n\nEl Distrito Escolar del Condado de Clark en Nevada tuvo que recurrir a tareas en papel y lápiz después de un ataque de ransomware que interrumpió sus recursos digitales. El distrito está trabajando en solucionar el problema y resaltando la necesidad de ciberseguridad para proteger la infraestructura educativa de amenazas cibernéticas. [Leer más](https://thenevadaindependent.com/article/clark-county-students-back-to-pen-and-paper-assignments-after-cybersecurity-breach)\n\n\nIronNet, una antigua empresa de ciberseguridad unicornio, ha cerrado después de enfrentar desafíos en financiación y competencia de mercado. La industria de ciberseguridad se encuentra todavía en la fase de crecimiento y necesita adaptarse a las amenazas cambiantes y las presiones económicas, enfatizando la necesidad de resiliencia e innovación. [Leer más](https://www.scmagazine.com/news/more-cybersecurity-firm-closures-expected-after-ironnet-shutters)\n\n## Computación Ubicua\n\n\nEl Pentágono está implementando una versión secreta de nivel Secreta de Microsoft 365, llamada DOD365-Secreta, para fortalecer las capacidades de defensa nacional. El nuevo servicio proporcionará herramientas seguras de comunicación y colaboración, y está siendo piloteado por la Agencia de Sistemas de Información de Defensa (ASID) durante el último año. Esta medida destaca el compromiso del gobierno de mejorar la ciberseguridad y la protección de datos, especialmente en entornos militares sensibles. [Leer más](https://federalnewsnetwork.com/on-dod/2023/10/secret-level-version-of-microsoft-365-rolls-out-to-top-pentagon-offices-this-month/)\n\n\nForbes predice diez tendencias importantes de la computación en la nube que darán forma a la industria en el año 2024. Estas tendencias incluyen la computación en el perímetro, la ciberseguridad, la computación sin servidor, la integración de IA, la adopción híbrida y multi-nube, las prácticas sostenibles en la nube, la computación cuántica, el análisis de datos y el desarrollo nativo en la nube. Es importante que las empresas y los profesionales de TI se mantengan actualizados con estas tendencias para navegar el cambiante panorama de la nube. [Leer más](https://www.forbes.com/sites/bernardmarr/2023/10/09/the-10-biggest-cloud-computing-trends-in-2024-everyone-must-be-ready-for-now/?sh=2093527266d6)\n\n## Inteligencia Artificial\n\n\nEl Día Q está en camino. Los computadoras cuánticas podrían pronto llegar a ser lo suficientemente poderosas para quebrantar los algoritmos de cifrado que protegen nuestra información digital hoy en día. Esto amenaza la seguridad de los datos, por lo que los gobiernos y las empresas tecnológicas están invirtiendo en cifrado y tecnología resistente a la computación cuántica para protegerse contra esto.La Organización Mundial de la Salud (OMS) ha delineado consideraciones para regular la inteligencia artificial (IA) en el sector de la atención médica, según su anuncio el diecinueve de octubre de dos mil veintitrés. Esta guía enfatiza la importancia de la ética, la transparencia, la privacidad de los datos y la responsabilidad en las aplicaciones de IA para la salud. Aborda el creciente papel de la IA en la atención médica y la necesidad de un marco que garantice su uso responsable y ético. [Leer más](https://www.who.int/news/item/19-10-2023-who-outlines-considerations-for-regulation-of-artificial-intelligence-for-health)\n\nEl New York Times informa sobre las restricciones de China en la exportación de chips de IA. Estas restricciones tienen como objetivo preservar la seguridad nacional y la independencia tecnológica, afectando las cadenas de suministro tecnológicas globales. A medida que la IA juega un papel cada vez más crucial en diversos sectores, este movimiento refleja los esfuerzos de China por ejercer control sobre la tecnología avanzada y genera preocupaciones sobre la dinámica del comercio tecnológico global. [Leer más](https://www.nytimes.com/2023/10/17/business/economy/ai-chips-china-restrictions.html)\n\n\nSegún el diario en línea The Information, OpenAI ha detenido supuestamente el desarrollo de un nuevo modelo de IA llamado \"Arrakis\", lo que indica un raro contratiempo para la organización. La decisión subraya las complejidades y desafíos de desarrollar modelos avanzados de IA y destaca la importancia de la investigación y el desarrollo responsable y ético de la IA. [Leer más](https://www.theinformation.com/articles/openai-dropped-work-on-new-arrakis-ai-model-in-rare-setback)\n\n## Noticias del podcast \"Abrazando la Transformación Digital\".\n\nEn su serie Abrazando la IA Generativa, Darren presenta una entrevista con Andy Morris, estratega principal de IA de Intel. La entrevista se sumerge en el tema de la IA generativa cotidiana y explora varias herramientas que pueden ser útiles para las personas en su vida diaria. Es una conversación informativa que arroja luz sobre el impacto de la IA en nuestras vidas diarias y cómo podemos aprovecharla en nuestro beneficio. [Leer más](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW38-es","image":"./briefs/edw-38/es/thumbnail.png","lang":"es","summary":"Abrazando las noticias digitales para la semana del 22 de octubre de 2023, incluyendo noticias en ciberseguridad, computación ubícua e inteligencia artificial. Los aspectos más destacados de esta semana incluyen el regreso al uso de bolígrafo y papel en la distribución escolar, estar atentos al Día Q y las pautas de IA en el cuidado de la salud."},{"id":206,"type":"News Brief","title":"2023-10-28","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Seguridad informática\n\n\nKaspersky informa sobre un nuevo malware, apodado \"Elegante\", que cuenta con capacidades avanzadas de espionaje y presenta similitudes con herramientas de piratería vinculadas a la NSA. La empresa de ciberseguridad no ha atribuido el malware a un actor de amenazas específico, pero su descubrimiento pone de relieve la necesidad de medidas de ciberseguridad sólidas frente a ciberataques patrocinados por el Estado. [Leer más](https://cyberscoop.com/kaspersky-reveals-elegant-malware-resembling-nsa-code/)\n\n\n\nCISA y HHS han lanzado un kit de herramientas de ciberseguridad para organizaciones de atención médica. El kit tiene como objetivo mejorar la postura de ciberseguridad de la industria al ofrecer recursos valiosos y orientación para mitigar las amenazas cibernéticas. El objetivo es fortalecer la seguridad de la infraestructura crítica de salud, dada la creciente cantidad de riesgos cibernéticos y los ataques de ransomware a los hospitales recientemente. Obtenga más información en la página de noticias de CISA. [Leer más](https://www.cisa.gov/news-events/news/cisa-hhs-release-collaborative-cybersecurity-healthcare-toolkit)\n\n\nCalifornia ha implementado nuevas regulaciones de ciberseguridad que incluyen evaluaciones de riesgo, planes de respuesta a incidentes, encriptación de datos y autenticación multifactor. Es fundamental que las empresas en California cumplan con estas regulaciones, ya que el incumplimiento puede resultar en sanciones y consecuencias legales. Manténgase informado revisando las actualizaciones en el sitio web de Ciberseguridad de California. [Leer más](https://www.armstrongteasdale.com/thought-leadership/californias-newest-cybersecurity-rule-what-you-need-to-know/)\n\n## Computación de Borde\n\n\nEl Parlamento del Reino Unido está investigando la ciberseguridad de infraestructuras críticas, incluyendo energía, transporte y salud. La investigación tiene como objetivo evaluar la preparación y resiliencia de estos sectores ante posibles amenazas cibernéticas. Esta medida llega en medio de crecientes preocupaciones sobre la vulnerabilidad de los servicios esenciales a los ciberataques. Destaca la importancia de proteger los servicios vitales de las amenazas digitales en un mundo interconectado. El reciente conflicto entre Israel y Hamas resalta la necesidad de proteger las infraestructuras críticas de las amenazas cibernéticas. [Leer más](https://www.bankinfosecurity.com/uk-parliament-probes-critical-infrastructure-cybersecurity-a-23400)\n\n\nTenable y Siemens Energy están colaborando para mejorar la ciberseguridad industrial. La colaboración fusionará la experiencia de Tenable en ciberseguridad con el conocimiento de dominio de Siemens Energy para brindar soluciones de seguridad avanzadas para la infraestructura crítica. El objetivo es fortalecer la protección de los sistemas energéticos esenciales contra las amenazas cibernéticas en constante evolución. La importancia de salvaguardar la infraestructura crítica en un mundo cada vez más interconectado no puede ser exagerada. [Leer más](https://finance.yahoo.com/news/tenable-siemens-energy-expand-collaboration-130000076.html)\n\n\nLos bancos singapurenses DBS y Citibank sufrieron una interrupción de servicio debido a una falla en el sistema de enfriamiento en su centro de datos compartido. El incidente interrumpió los servicios bancarios y causó inconvenientes a los clientes. Estos incidentes resaltan la importancia de una infraestructura sólida en los centros de datos para garantizar servicios financieros ininterrumpidos. Los bancos han restablecido sus servicios, enfatizando el papel crítico de la tecnología en las operaciones bancarias modernas. [Leer más](https://www.channelnewsasia.com/singapore/dbs-citibank-outage-data-centre-cooling-system-down-3861076)\n\n## Computación Ubicua\n\n\nAmazon ha lanzado un servicio de nube independiente para Europa para satisfacer las crecientes preocupaciones sobre la privacidad de datos y los requisitos regulatorios. El nuevo servicio de nube tiene como objetivo proporcionar una soberanía de datos y seguridad mejoradas a los clientes europeos, al tiempo que permite a las organizaciones aprovechar la informática en la nube de manera localizada y conforme. [Leer más](https://www.euronews.com/next/2023/10/25/amazon-rolls-out-new-independent-cloud-for-europe)\n\n\nEstados Unidos y Australia están colaborando en el desarrollo de chips cuánticos avanzados. Esta asociación resalta la creciente importancia de la computación cuántica en la seguridad nacional y el avance tecnológico. A medida que los países se esfuerzan por mantener su liderazgo en esta área crítica de investigación y desarrollo, los esfuerzos conjuntos como estos se están volviendo más comunes. La colaboración demuestra la cada vez mayor cooperación entre los dos aliados en ciencia y tecnología. [Leer más](https://foreignpolicy.com/2023/10/25/quantum-computing-united-states-australia-cooperation-allies-science-technology-chips/)\n\n\nUn informe reciente encontró que el cuarenta por ciento de las empresas están perdiendo ingresos debido a la interrupción tecnológica y la gestión de servicios en la nube. Esto subraya la importancia de contar con una infraestructura de TI sólida y una gestión eficiente de la nube. Abordar los tiempos de inactividad y simplificar las operaciones en la nube es fundamental para garantizar el funcionamiento ininterrumpido de la empresa y maximizar los ingresos. Lee más en el informe en las noticias de cloud computing. [Leer más](https://www.cloudcomputing-news.net/news/2023/oct/24/40-of-firms-lose-revenue-from-technology-downtime-and-cloud-complexity/)\n\n## Podcast de Adopción de la Transformación Digital\n\nUna nueva serie llamada \"Abrazando la Confianza Cero\" comienza esta semana. La Dra. Anna Scott y Dave Marcus, quienes son invitados especiales, describirán los seis pilares de la Confianza Cero y cómo pueden ser utilizados en la infraestructura existente. Mientras tanto, la serie de Darren \"Abrazando la Inteligencia Artificial Generativa\" continúa con Sunny Stueve, un experto en Factores Humanos de Leidos. [Leer más](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW39-es","image":"./briefs/edw-39/es/thumbnail.png","lang":"es","summary":"En la última edición de nuestro Embracing Digital This Week, te presentamos los últimos avances e historias en el mundo de la ciberseguridad, la computación perimetral y la informática ubicua. En este episodio, cubrimos un nuevo malware llamado Elegant, las regulaciones de ciberseguridad en California, la investigación del Parlamento del Reino Unido sobre la ciberseguridad de la infraestructura crítica y otras noticias interesantes de la industria tecnológica."},{"id":207,"type":"News Brief","title":"2023-2-26","tags":null,"body":"\n\n##Inteligencia Artificial\n\nOpenAI y Bain & Company formaron la Alianza Global de Servicios de IA para transformar el potencial de los negocios. Bain combinará su profunda capacidad de implementación digital y experiencia estratégica con las herramientas y plataformas de IA de OpenAI, incluyendo ChatGPT, para ayudar a sus clientes en todo el mundo a identificar e implementar el valor de la IA para maximizar el potencial de los negocios. The Coca-Cola Company es la primera empresa en colaborar con la nueva alianza.\n\nLa unidad de nube de Amazon se ha asociado con Hugging Face, el fabricante del rival de ChatGPT. Hugging Face construirá la próxima versión de su modelo de lenguaje, llamado BLOOM, en AWS.\n\n[https://www.seattletimes.com/business/amazons-cloud-unit-partners-with-hugging-face-maker-of-chatgpt-rival/](https://www.seattletimes.com/business/amazons-cloud-unit-partners-with-hugging-face-maker-of-chatgpt-rival/)\n\nLa IA también puede ser utilizada para hacer el bien en el mundo real. La IA está ideando medicamentos que nunca se habían visto antes. Ahora tenemos que ver si funcionan. Los primeros medicamentos diseñados con la ayuda de la IA están ahora en ensayos clínicos, las rigurosas pruebas realizadas en voluntarios humanos para determinar si un tratamiento es seguro y funciona antes de que los reguladores los autoricen para su uso generalizado.\n\n[https://www.technologyreview.com/2023/02/15/1067904/ai-automation-drug-development](https://www.technologyreview.com/2023/02/15/1067904/ai-automation-drug-development)\n\n## Borde Inteligente\n\nSe han encontrado vulnerabilidades de seguridad en dispositivos inalámbricos de IoT industrial de cuatro proveedores, creando una superficie de ataque significativa para actores de amenazas que buscan explotar entornos de tecnología operativa (OT). Las fallas ofrecen puntos de entrada de ataque remoto, lo que permite a los adversarios no autenticados obtener un punto de apoyo y utilizarlo como palanca para propagarse a otros hosts, causando así graves daños.\n\n[https://thehackernews.com/2023/02/critical-infrastructure-at-risk-from.html](https://thehackernews.com/2023/02/critical-infrastructure-at-risk-from.html)\n\nLa Alianza de Estándares de Conectividad (CSA, por sus siglas en inglés) ha formado un grupo de trabajo dedicado a promover la adopción de \"Matter\" para fines en el sector de la salud. Matter es un estándar interoperable para dispositivos inteligentes del hogar lanzado por la CSA el año pasado. El Grupo de Trabajo de Salud y Bienestar alineará a los principales contribuyentes en la industria de la salud en torno al estándar Matter para garantizar dispositivos seguros e interoperables de salud y bienestar.\n\n[https://www.iottechnews.com/news/2023/feb/10/matter-creator-csa-announces-healthcare-working-group/](https://www.iottechnews.com/news/2023/feb/10/matter-creator-csa-announces-healthcare-working-group/)\n\nLos robó-taxis de la empresa de vehículos autónomos de Amazon, Zoox, ya circulan por las calles de Foster City, California. Los vehículos de Zoox están diseñados específicamente para la conducción autónoma y no cuentan con ningún control manual. Los empleados a tiempo completo de la empresa son los primeros en utilizar el servicio de transporte autónomo entre sus campus. ¡Esto requiere una confianza absoluta en la calidad del trabajo de su equipo!\n\n[https://www.iottechnews.com/news/2023/feb/14/amazon-robotaxis-hit-the-streets-of-california/](https://www.iottechnews.com/news/2023/feb/14/amazon-robotaxis-hit-the-streets-of-california/)\n\n## Gestión de datos.\n\nEn un estudio reciente de Informatica sobre la dispersión de datos, se encontró que la dispersión de datos y la necesidad de una gestión unificada son puntos problemáticos generalizados para muchas organizaciones. Además, la calidad de los datos y la gobernanza son las principales prioridades. Esto está llevando a una mayor inversión en tecnologías de datos este año. La investigación basada en encuestas encontró que el 78% de los Directores de Datos (CDOs) dijeron que deben alinearse estratégicamente con las organizaciones generadoras de ingresos para justificar el gasto adicional en la mejora de la analítica y gobernanza de datos.\n\n[https://accelerationeconomy.com/data/informatica-research-highlights-data-sprawl-why-management-needs-to-be-unified/](https://accelerationeconomy.com/data/informatica-research-highlights-data-sprawl-why-management-needs-to-be-unified/)\n\nAlation Inc., proveedor de soluciones empresariales de inteligencia de datos, ha lanzado Alation Marketplaces. Este nuevo producto permite que conjuntos de datos de terceros complementen los datos existentes en el catálogo de datos de Alation, lo que permite a los usuarios explorar datos externos y complementar conjuntos de datos existentes. Además, la empresa ha ampliado sus productos Alation Anywhere para Microsoft Teams y Alation Connected Sheets para Microsoft Excel para permitir que los usuarios de datos accedan directamente a información contextual del catálogo dentro de su herramienta preferida.\n\n[https://www.alation.com/press-releases/alation-launches-data-marketplaces/](https://www.alation.com/press-releases/alation-launches-data-marketplaces/)\n\n\n","guests":null,"link":"/brief-EDW4-es","image":"./briefs/edw-4/es/thumbnail.png","lang":"es","summary":"Summary"},{"id":208,"type":"News Brief","title":"2023-11-11","tags":["homelawncaretools","ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Ciberseguridad\n\n\n\nLos puertos australianos están lidiando con un ciberataque que afecta las operaciones de DP World. El ataque al gigante naviero ha llevado a interrupciones en los movimientos de contenedores. Si bien se están investigando el alcance y origen de la violación, se plantea preocupaciones sobre la vulnerabilidad de la infraestructura crítica frente a las amenazas cibernéticas. Las autoridades están trabajando para restaurar la normalidad, ya que el incidente subraya la creciente importancia de la ciberseguridad en la protección de los servicios esenciales. [Leer más](https://www.bloomberg.com/news/articles/2023-11-11/australian-authorities-meet-as-dp-world-shuts-ports-on-cyber-act)\n\n\nICBC, el banco más grande del mundo, ha sido víctima de un ciberataque de ransomware. La violación ha interrumpido las operaciones, afectando los servicios al cliente. Mientras se está evaluando la magnitud del ataque, subraya la creciente amenaza del cibercrimen para las instituciones financieras. ICBC está trabajando para contener la violación y restaurar la normalidad, enfatizando la necesidad de medidas de ciberseguridad robustas en el sector bancario mundial. [Leer más](https://www.cnbc.com/2023/11/10/icbc-the-worlds-biggest-bank-hit-by-ransomware-cyberattack.html)\n\n\n¡Los ciberataques están respaldando los ataques cinéticos! El grupo de hackers ruso SandWorm está implicado en un ciberataque que interrumpió el suministro de energía en Ucrania. El sofisticado ataque se dirigió a la infraestructura energética, causando cortes generalizados. Las autoridades ucranianas están investigando el incidente, destacando la amenaza constante de los ciberataques patrocinados por el estado a la infraestructura crítica. La situación subraya la necesidad de medidas de ciberseguridad mejoradas para proteger los servicios esenciales. [Leer más](https://thehackernews.com/2023/11/russian-hackers-sandworm-cause-power.html)\n\n## Inteligencia Artificial\n\n\nLos expertos en inteligencia artificial advierten sobre una posible escasez de datos, enfatizando la disminución de recursos de datos en el mundo para impulsar la inteligencia artificial. El crecimiento exponencial de las aplicaciones de inteligencia artificial y los modelos ávidos de datos están superando la generación de datos. Esta escasez podría obstaculizar ulteriores avances de la inteligencia artificial, lo que impulsa la necesidad de enfoques innovadores para la recopilación y síntesis de datos. Este desafío subraya la importancia de un desarrollo responsable y sostenible de la inteligencia artificial. [Leer más](https://www.sciencealert.com/the-world-is-running-out-of-data-to-feed-ai-experts-warn)\n\n\n¡La proliferación de la inteligencia artificial amenaza a la democracia! advierten los expertos en la conferencia Next de Reuters. El rápido avance de la inteligencia artificial genera preocupaciones sobre su posible uso indebido para la vigilancia, manipulación y erosión de los valores democráticos. Los expertos piden pautas éticas, marcos regulatorios y discursos públicos para mitigar los riesgos asociados con la creciente influencia de la inteligencia artificial en la política y la sociedad. [Leer más](https://www.reuters.com/technology/reuters-next-rapid-ai-proliferation-is-threat-democracy-experts-say-2023-11-08/)\n\n\nEn una entrevista, Barack Obama expresó su preocupación por el impacto de la inteligencia artificial (IA) en los trabajos. Reconoce el potencial de desplazamiento laboral debido a la automatización y enfatiza la necesidad de políticas para enfrentar estos desafíos. Obama sugiere invertir en educación y programas de reciclaje para preparar a la fuerza laboral para los mercados laborales en evolución moldeados por la IA y la automatización. [Leer más](https://www.independent.co.uk/tv/news/barack-obama-biden-ai-jobs-b2445354.html)\n\n## Computación Ubicua\n\n\nUn avance en la computación cuántica integra la inteligencia artificial y el aprendizaje automático para una robusta corrección de errores en qubit. Los científicos han desarrollado un enfoque de computación cuántica mejorado con inteligencia artificial que mejora la corrección de errores para los qubits, un desafío crucial en la computación cuántica. Esta innovadora combinación de inteligencia artificial y tecnología cuántica promete avanzar en la confiabilidad y el rendimiento de las computadoras cuánticas. [Leer más](https://scitechdaily.com/ai-enhanced-quantum-computing-machine-learning-powers-robust-qubit-error-correction/)\n\n\nEl Grupo de Software en la Nube, empresa matriz de Citrix, está discontinuando nuevas transacciones comerciales en China, incluyendo Hong Kong y Macao, con efecto desde el tres de diciembre. Citando el \"costo creciente\" de operar en la región, la decisión sigue movimientos similares de otras firmas tecnológicas estadounidenses en medio de una perspectiva económica más débil y regulaciones de seguridad de datos más estrictas en China. A medida que más empresas de EE.UU. abandonan China, Pekín ha propuesto relajar los controles de seguridad de datos transfronterizos para atraer a inversores extranjeros. [Leer más](https://finance.yahoo.com/news/citrix-owner-cloud-software-becomes-093000327.html)\n\n\nUn artículo reciente de Analytics Insights pronostica las tendencias de computación en la nube para el año dos mil veinticuatro. Estas incluyen la importancia de la computación de borde para un procesamiento de datos más rápido, el auge de las estrategias de múltiples nubes para tener mayor flexibilidad, y la integración de la Inteligencia Artificial/Aprendizaje Automático para una optimización del análisis de datos. Las medidas de seguridad como la arquitectura de cero confianza serán una prioridad para abordar el panorama de amenazas en constante evolución. [Leer más](https://www.analyticsinsight.net/cloud-trends-for-2024-whats-on-for-cloud-computing/)\n\n## Podcast Abrazando la Transformación Digital\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW41-es","image":"./briefs/edw-41/es/thumbnail.png","lang":"es","summary":"Adoptando lo Digital Esta semana del 12 de noviembre de 2023, incluye noticias en ciberseguridad. Inteligencia artificial y computación ubicua. Conozca sobre varios ciberataques importantes en infraestructura crítica, la inteligencia artificial quedándose sin datos y avances en la computación cuántica."},{"id":209,"type":"News Brief","title":"2023-11-18","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Ciberseguridad\n\n\nLa Agencia de Seguridad Cibernética e Infraestructura (CISA) planea hacer cumplir la responsabilidad ejecutiva en la seguridad del software para la adquisición del gobierno. La iniciativa tiene como objetivo lograr que los ejecutivos den su aprobación a la seguridad del software vendido al gobierno. Este movimiento es parte de los esfuerzos de la CISA para mejorar las medidas de seguridad cibernética y garantizar la integridad del software utilizado en sistemas gubernamentales críticos. [Leer más](https://federalnewsnetwork.com/cybersecurity/2023/11/cisa-aims-to-make-executives-sign-off-on-security-of-software-sold-to-government/)\n\n\nLas agencias de ciberseguridad de los Estados Unidos emiten una advertencia sobre una nueva amenaza de ransomware dirigida a la infraestructura crítica. La alerta destaca los posibles riesgos e insta a las organizaciones a mejorar sus protocolos de ciberseguridad. Las agencias enfatizan la importancia de las medidas proactivas, incluyendo la actualización regular del software y la implementación de prácticas de seguridad robustas para mitigar el riesgo de ataques de ransomware. [Leer más](https://thehackernews.com/2023/11/us-cybersecurity-agencies-warn-of.html)\n\n\n\nSolarWinds enfrenta una demanda de la Comisión de Bolsa y Valores (SEC) tras el ciberataque del dos mil veinte. La SEC alega que la empresa no reveló las vulnerabilidades, engañando a los inversores. SolarWinds, conocida por su software de gestión de TI, sufrió un ataque a la cadena de suministro de alto perfil. La demanda subraya el creciente enfoque en las obligaciones de divulgación de ciberseguridad y las posibles consecuencias legales para las empresas que abordan de manera inadecuada dichos incidentes. [Leer más](https://www.nytimes.com/2023/11/18/business/dealbook/solarwinds-sec-lawsuit.html)\n\n## Inteligencia Artificial\n\n\nEl director ejecutivo de OpenAI, Sam Altman, renuncia en medio de crecientes preocupaciones sobre la dirección de la compañía. La salida de Altman sigue a las tensiones internas y a las dimisiones por el enfoque de la compañía en aplicaciones comerciales en lugar de su compromiso inicial con los amplios beneficios de la inteligencia artificial. La decisión plantea preguntas sobre el equilibrio entre los intereses corporativos y las consideraciones éticas en el desarrollo de la inteligencia artificial. [Leer más](https://www.washingtonpost.com/technology/2023/11/17/openai-ceo-resigns/)\n\n\nEl reciente evento de IA de Microsoft destaca tres puntos clave. Primero, la compañía enfatiza su compromiso con el desarrollo responsable de la IA, centrándose en la equidad, transparencia y responsabilidad. Segundo, Microsoft pretende empoderar a los desarrolladores con herramientas como GPT de OpenAI, que ofrece capacidades avanzadas de lenguaje natural. Por último, el evento subraya la dedicación de Microsoft a la innovación impulsada por la IA en diversas industrias, mostrando aplicaciones en el cuidado de la salud, finanzas y ciencias climáticas. [Leer más](https://www.cnbc.com/2023/11/17/here-are-three-key-takeaways-from-microsofts-bullish-ai-event-.html)\n\n\nYouTube afronta el desafío del contenido deepfake a medida que los creadores de IA producen videos cada vez más convincentes y engañosos. La plataforma lucha por distinguir entre los usos inofensivos y maliciosos del contenido generado por IA. YouTube reconoce la necesidad de vigilancia y un enfoque integral para abordar los deepfakes con el fin de mantener la integridad de su contenido. Ahora se les pide a los creadores de contenido que verifiquen si el contenido es generado por una IA o no. [Leer más](https://apnews.com/article/youtube-artitifical-intelligence-deep-fake-ai-creaters-0513fd9fddbd93af327f0411dd29ff3d)\n\n## Computación Ubicua\n\n\n\nAmazon y Microsoft participan en una feroz competencia por lucrativos contratos gubernamentales de computación en la nube, ya que las agencias federales dependen cada vez más de los servicios en la nube. Las empresas están compitiendo por el contrato de Capacidad de Nube para Combatientes Conjuntos (JWCC) del Departamento de Defensa, un acuerdo potencialmente valorado en diez mil millones de dólares. Esta batalla refleja la tendencia más amplia de los gigantes tecnológicos compitiendo por contratos gubernamentales en la nube y subraya la importancia estratégica de los servicios en la nube para modernizar la infraestructura gubernamental. [Leer más](https://www.bloomberg.com/news/articles/2023-11-16/amazon-amzn-microsoft-msft-fight-for-cloud-computing-government-contracts)\n\n\nLa Marina de los Estados Unidos está mejorando sus sistemas de torpedos al aprovechar la computación en la nube para submarinos. Esta mejora tiene como objetivo mejorar el rendimiento y las capacidades de los torpedos. La Marina busca mejorar el procesamiento y análisis de datos utilizando la computación en la nube, lo que permite operaciones de torpedos más sofisticadas y eficientes. La medida refleja los esfuerzos continuos de las fuerzas armadas para integrar tecnologías avanzadas, como la computación en la nube, para reforzar las capacidades de sus sistemas navales. [Leer más](https://www.defensenews.com/naval/2023/11/16/us-navy-upgrading-torpedoes-leveraging-cloud-computing-for-submarines/)\n\n\nEl gobierno de los Estados Unidos señala a Alibaba por preocupaciones relacionadas con el posible mal uso de chips fabricados en América en su tecnología de vigilancia. El movimiento es parte del esfuerzo más amplio para restringir las exportaciones de tecnologías sensibles ante preocupaciones de seguridad nacional. Este desarrollo subraya el escrutinio de las compañías tecnológicas chinas. Resalta el enfoque del gobierno de los Estados Unidos en prevenir el uso no intencionado de la tecnología estadounidense de maneras que podrían comprometer la seguridad o los derechos humanos. [Leer más](https://finance.yahoo.com/news/alibaba-flagged-us-chip-curbs-192349433.html)\n\n## Noticias del Podcast Abrazando la Transformación Digital\n\n\n\nEcha un vistazo a las tres series tituladas \"Abrazando la Inteligencia Artificial Generativa\", \"Abrazando la Confianza Cero\" y \"Abrazando la Nube Multi-Híbrida\". En estas series, Darren explora el mundo de la IA con invitados de la industria, la educación y personas comunes, trae expertos en ciberseguridad para hablar sobre la protección del mundo digital, y busca en el cielo las últimas estrategias y tecnologías de la nube. [Leer más](https://www.embracingdigital.org/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW42-es","image":"./briefs/edw-42/es/thumbnail.png","lang":"es","summary":"Transformación Digital para Ciberseguridad, IA y Computación en la semana del 19 de noviembre de 2023. Los temas incluyen implicaciones legales de la ciberseguridad, la salida de Sam Altman de OpenAI y los proveedores de la nube compitiendo por contratos gubernamentales."},{"id":210,"type":"News Brief","title":"2023-11-25","tags":["uniquehandmadewoodfurniture.","ai","compute","cybersecurity","samaltman","openai","aibreakthrough","neuralchat7b","intel","responsibleai","meta","hacking","insiderthreats","securitymeasures","eucybersecurityregulations","clearfake","malware","ubiquitouscomputing","macstadium","m2prochip","alibabacloud","cloudcomputing"],"body":"\n\n## Inteligencia Artificial\n\n\nSam Altman dejó OpenAI después de una carta que expresaba preocupaciones acerca de un avance en IA no revelado. La carta, enviada por un empleado anónimo, llevó a la destitución de Altman como CEO. Los detalles del avance siguen siendo confidenciales, lo que genera preguntas sobre la dinámica interna de OpenAI. Altman fue reinstaurado cuando setecientos de los setecientos setenta empleados lo siguieron a Microsoft. [Leer más](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/)\n\n\nIntel ha desarrollado la NeuralChat Siete B, un modelo de chatbot que prioriza la protección de datos del usuario. El modelo se entrena utilizando Optimización de Privacidad Diferencial (OPD) para mejorar las salvaguardas de privacidad. Al implementar la OPD, Intel tiene como objetivo garantizar interacciones seguras dentro del chatbot y mitigar los riesgos de privacidad. Estos esfuerzos se alinean con los estándares en evolución para el desarrollo responsable de la inteligencia artificial. [Leer más](https://medium.com/@bnjmn_marie/neuralchat-7b-intels-chat-model-trained-with-dpo-e691dfd52591)\n\n\nSe informa que Meta, la empresa matriz de Facebook, desmanteló su equipo de Inteligencia Artificial Responsable, generando preocupaciones sobre su compromiso con las prácticas éticas de la inteligencia artificial. Los críticos argumentan que desmantelar el equipo puede socavar los esfuerzos para abordar las preocupaciones éticas y los posibles sesgos en las tecnologías de Inteligencia Artificial de Meta, destacando el debate en curso sobre el desarrollo responsable de la Inteligencia Artificial. [Leer más](https://www.theverge.com/2023/11/18/23966980/meta-disbanded-responsible-ai-team-artificial-intelligence)\n\n## Ciberseguridad\n\n\nUn ejecutivo de una firma de ciberseguridad se ha declarado culpable de hackear hospitales en un sorprendente giro de los acontecimientos. El ejecutivo, anteriormente encargado de proteger los sistemas, admitió que explotó vulnerabilidades para obtener beneficios personales. La violación plantea serias preguntas sobre las amenazas internas y la necesidad de medidas de seguridad sólidas dentro del propio sector de la ciberseguridad. La declaración de culpabilidad subraya lo imperativo de protocolos de ciberseguridad estrictos y una conducta ética, especialmente cuando se confía en la protección de infraestructuras críticas, como las instituciones de salud. [Leer más](https://www.bleepingcomputer.com/news/security/cybersecurity-firm-executive-pleads-guilty-to-hacking-hospitals/)\n\n\nLa Unión Europea planea expandir las regulaciones de ciberseguridad para cubrir sectores críticos como energía, transporte y proveedores de servicios digitales como Amazon, Google y Microsoft. Esto es para fortalecer la ciberseguridad en diversas industrias debido al incremento de amenazas cibernéticas. Sin embargo, algunos ven las nuevas regulaciones como una forma de capturar ingresos de las grandes tecnológicas. [Leer más](https://www.finextra.com/newsarticle/43338/eu-considers-widening-scope-of-cybersecurity-regulation)\n\n\nSe ha descubierto un nuevo malware llamado ClearFake que apunta a los usuarios de Mac. Se disfraza de una aplicación antivirus legítima, engañando a los usuarios para que descarguen software malicioso. Los expertos advierten sobre el potencial de robo de datos y compromiso del sistema. Es esencial ser cauteloso y utilizar software de seguridad de buena reputación para contrarrestar las amenazas en evolución que apuntan a las plataformas de macOS. [Leer más](https://cybersecuritynews.com/clearfake-new-malware-mac/)\n\n## Computación Ubicua\n\n\nEl nuevo Mac Cloud de MacStadium, impulsado por el chip M2 Pro, ofrece un rendimiento y capacidades avanzadas para desarrolladores y empresas de macOS. Satisface la creciente demanda de desarrollo de Mac basado en Kubernetes en entornos en la nube, satisfaciendo las necesidades únicas de la comunidad de desarrollo de Apple. El chip M2 Pro avanza significativamente las capacidades de alojamiento de Mac, proporcionando una mayor velocidad y eficiencia. [Leer más](https://finance.yahoo.com/news/macstadium-unveils-powerful-next-generation-120000257.html)\n\n\n\nAlibaba ha incorporado a tres nuevos ejecutivos a su unidad de computación en la nube, lo que indica su compromiso de impulsar su negocio en la nube. Los ejecutivos aportan una amplia experiencia al competitivo mercado de la computación en la nube y fomentarán la innovación y el crecimiento. El enfoque de Alibaba en los servicios en la nube se alinea con las demandas evolutivas del panorama digital, reforzando su determinación de mantener una posición líder en el sector global de la computación en la nube. [Leer más](https://www.proactiveinvestors.com.au/companies/news/1034236/alibaba-taps-three-new-cloud-computing-executives-after-nixing-spinoff-ipo-plans-1034236.html)\n\n\nSegún los informes, Europa está rezagada en la carrera global de la inteligencia artificial y debe cambiar su enfoque hacia la computación cuántica para mantenerse competitiva. Invertir en tecnología cuántica transformadora es imperativo para que Europa recupere su liderazgo tecnológico y aborde el cambiante panorama global de la innovación. [Leer más](https://www.euronews.com/next/2023/11/23/europe-has-lost-the-ai-race-it-cant-ignore-the-quantum-computing-one)\n\n## Podcast Abrazando la Transformación Digital\n\n\n\nEsta semana, Darren regresó después de una semana de descanso por el Día de Acción de Gracias. Entrevistó a Louis Parks, el director ejecutivo de Veridify, donde reveló las vulnerabilidades de la Infraestructura Crítica de la Tecnología Operacional. Además, echa un vistazo a la nueva imagen de marca y los logotipos de Abrazando la Transformación Digital. [Leer más](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW43-es","image":"./briefs/edw-43/es/thumbnail.png","lang":"es","summary":"Noticias de Transformación Digital para la semana del 26 de noviembre de 2023. Desde los cambios transformacionales en el liderazgo de la inteligencia artificial hasta el enfoque imperativo en la ciberseguridad y la presentación de soluciones informáticas avanzadas, esta recopilación encapsula momentos fundamentales en el panorama tecnológico en constante evolución. Únase a nosotros mientras nos adentramos en las complejidades de los recientes desarrollos, iluminando los profundos impactos que tienen en todos los sectores y orientando la trayectoria del futuro digital."},{"id":211,"type":"News Brief","title":"2023-12-2","tags":["ai","compute","datamanagement","aws","siemens","industrialdge","cloudconfiguration","multiaccessedgecomputing","mec","networkarchitecture","iot","realtimeapplications","edgecomputinginhealthcare","healthcaremarket","polarismarketresearch","usda","datastrategy","digitalgovernance","vastdata","aiworkflows"],"body":"\n\n## Cómputo en el borde\n\n\nAmazon Web Services (AWS) y Siemens se han unido para simplificar la configuración de borde industrial a la nube. La colaboración tiene como objetivo simplificar la integración de dispositivos de borde con servicios en la nube, mejorando la eficiencia en entornos industriales. La asociación combina AWS IoT Greengrass y la plataforma Industrial Edge de Siemens, proporcionando una solución sin interrupciones para configurar, administrar y optimizar dispositivos de borde en la nube. Se espera que esta colaboración facilite una conectividad más suave y accesible de borde industrial a la nube. [Leer más](https://www.edgecomputing-news.com/2023/12/01/aws-and-siemens-team-up-for-easier-industrial-edge-to-cloud-configuration/)\n\n\n\nLa Computación en Borde de Acceso Múltiple (MEC, por sus siglas en inglés) es una arquitectura de red que acerca las capacidades de computación al borde de la red, reduciendo la latencia y mejorando la eficiencia. Al desplegar recursos de cómputo en el borde, la MEC permite un procesamiento de datos más rápido generado por dispositivos como sensores IoT y dispositivos móviles. Este enfoque descentralizado mejora las aplicaciones y servicios en tiempo real, como la realidad aumentada y los vehículos autónomos. La MEC utiliza servidores de borde en estrecha proximidad a los usuarios finales, optimizando el procesamiento de datos para computación de alto rendimiento de baja latencia. [Leer más](https://www.techopedia.com/what-is-multi-access-edge-computing)\n\n\nSegún Polaris Market Research, se proyecta que el mercado de la Computación en el Borde en la Salud supere los cuarenta y tres mil doscientos noventa millones de dólares estadounidenses para el año dos mil treinta y dos, exhibiendo un sólido crecimiento de CAGR del veintiséis coma tres por ciento. Se espera que la creciente adopción de soluciones de computación en el borde en el sector de la salud, impulsada por los avances en las tecnologías de IoT y AI, mejore la eficiencia del procesamiento de datos, permita aplicaciones en tiempo real y mejore la atención al paciente. El informe sugiere una expansión significativa del mercado a medida que el sector de la salud adopta la computación en el borde para mejorar el rendimiento y descentralizar el procesamiento de datos. [Leer más](https://www.prnewswire.co.uk/news-releases/edge-computing-in-healthcare-market-expected-to-cross-usd-43-29-billion-by-2032--driving-26-3-cagr-growth-polaris-market-research-301998143.html)\n\n## Gestión de Datos\n\n\n\nEl Departamento de Agricultura de los Estados Unidos ha revelado una nueva estrategia de datos para mejorar la gobernanza digital. La iniciativa aprovecha los datos para mejorar los servicios, la toma de decisiones y la responsabilidad. El plan enfatiza la innovación basada en datos, la colaboración y la modernización de la infraestructura de datos. Los componentes clave incluyen dar prioridad a la privacidad y la seguridad, promover la accesibilidad de los datos y fomentar una cultura de uso responsable de los datos. El enfoque integral del Departamento de Agricultura de los Estados Unidos tiene como objetivo maximizar el valor de los datos para apoyar la misión de la agencia y servir al público de manera más efectiva. [Leer más](https://www.nextgov.com/digital-government/2023/11/usda-unveils-new-data-strategy/392382/)\n\n\nVAST Data ha lanzado una actualización de plataforma diseñada para simplificar los flujos de trabajo de inteligencia artificial y las operaciones de nube híbrida en AWS. La actualización se enfoca en mejorar la eficiencia de los procesos intensivos de datos, ofreciendo una integración sin fisuras con los servicios de AWS. La plataforma de VAST Data tiene como objetivo optimizar la gestión de datos y acelerar las cargas de trabajo de inteligencia artificial, proporcionando una experiencia más amigable para el usuario para las organizaciones que aprovechan AWS para sus operaciones de nube híbrida. La actualización subraya el compromiso de VAST Data con la optimización del rendimiento y la simplificación de los flujos de trabajo complejos de inteligencia artificial y nube híbrida. [Leer más](https://www.datanami.com/this-just-in/vast-datas-new-platform-update-aims-to-simplify-ai-workflows-and-hybrid-cloud-operations-on-aws/)\n\n\nIntel se ha asociado con Granulate para optimizar las operaciones de gestión de datos en Databricks, una plataforma utilizada para análisis de grandes volúmenes de datos. La tecnología de optimización continua en tiempo real de Granulate, impulsada por hardware de Intel, está diseñada para mejorar el rendimiento y la utilización de recursos para los usuarios de Databricks. Esta colaboración refleja los esfuerzos de la industria para mejorar las capacidades de análisis de datos, permitiendo a las organizaciones aprovechar al máximo el poder de los grandes volúmenes de datos para aumentar la eficiencia y reducir los costos. [Leer más](https://www.valdostadailytimes.com/ap/business/intel-granulate-optimizes-databricks-data-management-operations/article_ef0e550d-dbd3-5e5d-9a1d-75ffc7d0be4d.html)\n\n## Inteligencia Artificial\n\n\nLos californianos están instando a sus legisladores a tomar medidas para proteger las elecciones y garantizar la integridad electoral. Las crecientes preocupaciones sobre la seguridad electoral han llevado a los ciudadanos a exigir una acción legislativa para proteger el proceso democrático. Esta demanda refleja una creciente conciencia de asegurar las elecciones frente a posibles amenazas. Refuerza el llamado a los legisladores para promulgar políticas que mejoren la transparencia, la ciberseguridad y la confianza general en el sistema electoral. [Leer más](https://news.yahoo.com/californians-want-lawmakers-safeguard-elections-150026166.html)\n\n\nLa OTAN ha anunciado una estrategia integral para la inteligencia artificial (IA) que reconoce su papel crucial en la guerra moderna y la seguridad. La estrategia utiliza la IA para mejorar la toma de decisiones, la ciberseguridad y la eficiencia operativa. La decisión de la OTAN refleja la creciente importancia de la IA en los dominios militares y la necesidad de permanecer a la vanguardia de los avances tecnológicos. La estrategia destaca las directrices éticas y enfatiza la importancia de la colaboración con aliados y socios de la industria. [Leer más](https://news.yahoo.com/nato-artificial-intelligence-strategy-amid-143228193.html)\n\n\nIntel anunció recientemente \"Visión de Inteligencia Artificial en todas partes\", una nueva tecnología que integra la Inteligencia Artificial en centros de datos y dispositivos. Esta integración tiene como objetivo mejorar el rendimiento y la eficiencia, optimizar la carga de trabajo, impulsar las capacidades de los centros de datos y permitir la toma de decisiones en tiempo real. Durante el evento Discover de HPE, Intel mostró diversas aplicaciones como centros de datos eficientes en energía y imágenes médicas avanzadas. \"Visión de Inteligencia Artificial en todas partes\" está alineada con el compromiso de Intel de promover la adopción de la Inteligencia Artificial en diferentes industrias, dando paso a una nueva era de computación inteligente. [Leer más](https://siliconangle.com/2023/11/29/data-centers-devices-intels-vision-ai-everywhere-hpediscover-hpediscover/)\n\n## Abrazando la Transformación Digital\n\n\n\nEl sitio web Embracingdigital.org ha sido recientemente renovado, presentando una nueva apariencia y logotipo. Observa la actualización de la marca del programa, junto con una nueva tienda que ofrece los últimos regalos para los entusiastas de la transformación digital. Además, en el programa de esta semana, hay una entrevista dividida en dos partes con Shamim Naqvi, director ejecutivo de SafeLiShare, donde se adentran en Zero Trust y el intercambio de datos. [Leer más](http://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW44-es","image":"./briefs/edw-44/es/thumbnail.png","lang":"es","summary":"Por favor, echa un vistazo a las últimas noticias en el mundo de la Transformación Digital para la semana del 3 de diciembre de 2023. Encontrarás una variedad de historias interesantes relacionadas con la computación de borde, la gestión de datos y la inteligencia artificial. Esta semana, AWS y Siemens han colaborado para simplificar la computación de borde, mientras que Intel está ayudando a mejorar la gestión de datos basada en la nube. Además, varios gobiernos están desarrollando nuevas estrategias para la inteligencia artificial."},{"id":212,"type":"News Brief","title":"2023-12-9","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","usarmy","uno","cybersec","iot","aijobs","gemini","nano","pro","ultra","meta","ibm","openai","altman","aws","google","microsoft","mcdonalds","vergeio","nutanix","embracingdt"],"body":"\n\n## Ciberseguridad\n\n\nEl Ejército de los Estados Unidos está planeando mejorar sus capacidades de conectividad y ciberseguridad mediante la introducción de una nueva red unificada llamada UNO. Esta iniciativa tiene como objetivo proporcionar al ejército una comunicación rápida y segura, que puede ayudar a superar los desafíos en el campo de batalla digital. Se espera que la red cooperativa simplifique las operaciones y mejore la resistencia contra las amenazas cibernéticas, fortaleciendo así la infraestructura tecnológica del Ejército. [Leer más](https://www.c4isrnet.com/battlefield-tech/it-networks/2023/12/04/unified-network-promises-us-army-rapid-connectivity-cybersecurity/)\n\n\nA medida que avanzan el Internet de las Cosas (IoT, por sus siglas en inglés) y la inteligencia artificial (IA), el riesgo para la infraestructura crítica ante las amenazas cibernéticas ha aumentado significativamente. Los expertos en ciberseguridad están enfatizando la necesidad de mejores medidas de seguridad para proteger contra estas amenazas. Están confiando en la integración de la IA y el aprendizaje automático para mejorar la detección y prevención de nuevos ataques cibernéticos. Este panorama en constante evolución de la ciberseguridad en la era de la IoT y la computación en la nube subraya la necesidad continua de adaptarse y asegurar los ecosistemas digitales. [Leer más](https://readwrite.com/the-evolution-of-cybersecurity-in-the-age-of-iot-and-cloud-computing/)\n\n\nAproximadamente el cincuenta por ciento de las organizaciones encuestadas tienen planes para recortar su personal de ciberseguridad, lo que genera preocupaciones a la luz de las amenazas cibernéticas en constante evolución. Esta tendencia subraya la importancia de una gestión estratégica de la fuerza laboral para mantener defensas digitales sólidas. Mientras tanto, algunas otras empresas necesitan ayuda para encontrar candidatos adecuados para llenar sus posiciones vacantes de ciberseguridad. [Leer más](https://www.csoonline.com/article/1251369/almost-50-organizations-plan-to-reduce-cybersecurity-headcounts-survey.html)\n\n## Inteligencia Artificial\n\n\nGoogle ha lanzado una nueva plataforma de IA llamada Géminis, cuyo objetivo es simplificar la formación y el despliegue de modelos de aprendizaje automático. Esto permitirá a los desarrolladores crear aplicaciones de IA más escalables y eficientes. Géminis ofrece herramientas y recursos fáciles de usar para promover la innovación y los avances en la tecnología de IA. Actualmente se dirige a tres ofertas: nano para teléfonos, pro para centros de datos en las instalaciones y ultra para una nube pública basada en la nube presentada por Bard. [Leer más](https://apnews.com/article/google-gemini-artificial-intelligence-launch-95d05d02051e75e20b574614ae720b8b)\n\n\n\nMeta e IBM están trabajando juntos para desarrollar un estándar abierto para la inteligencia artificial, con el objetivo de promover la interoperabilidad y contrarrestar la influencia de los grandes actores tecnológicos. Al fomentar un enfoque más colaborativo, esperan crear un panorama competitivo que fomente la innovación y la diversidad en la tecnología de IA. [Leer más](https://www.thestreet.com/technology/meta-and-ibm-team-up-against-dominant-big-tech-players)\n\n\nEl director ejecutivo de OpenAI, Sam Altman, enfatizó la importancia del desarrollo responsable de la inteligencia artificial después de que un empleado fue despedido debido a preocupaciones sobre el mal uso de la tecnología de la inteligencia artificial. El incidente destaca los desafíos continuos en la industria de la tecnología con respecto a la ética y la gobernanza en la inteligencia artificial. [Leer más](https://www.foxbusiness.com/technology/openais-sam-altman-opens-up-shock-firing)\n\n## Computación ubicua\n\n\nAWS y Google critican las prácticas restrictivas de la nube de Microsoft, acusándolos de limitar la elección del cliente y la interoperabilidad, lo cual sofoca la competencia en el mercado. Esto resalta las tensiones continuas entre los proveedores de la nube y las preocupaciones sobre las prácticas comerciales justas y la dinámica del mercado. [Leer más](https://www.itpro.com/cloud/cloud-computing/aws-joins-google-in-calling-out-restrictive-microsoft-cloud-practices)\n\n\nMcDonald's y Google Cloud se asocian para utilizar la tecnología en la nube y soluciones de inteligencia artificial generativa en los restaurantes de McDonald's en todo el mundo. El objetivo es mejorar la experiencia del cliente, optimizar operaciones e impulsar la innovación en la industria de la comida rápida. Al integrar las tecnologías avanzadas de Google Cloud, McDonald's tiene como objetivo mantenerse a la vanguardia de la transformación digital, ofreciendo servicios mejorados y eficiencias operativas en su red mundial de restaurantes. [Leer más](https://www.prnewswire.com/news-releases/mcdonalds-and-google-cloud-announce-strategic-partnership-to-connect-latest-cloud-technology-and-apply-generative-ai-solutions-across-its-restaurants-worldwide-302006915.html)\n\n\n\nSe ha informado que la adquisición de VMware por Broadcom ha causado cambios que llevaron al veinte por ciento de sus usuarios a buscar otras soluciones de virtualización. Esta tendencia de oportunidad ha creado un campo para otros actores en el mercado para establecerse como alternativas viables. A pesar del dominio de VMware en el mercado de la virtualización, la competencia de empresas como Nunatix y VergeIO está haciendo que los clientes se alejen de la plataforma establecida. [Leer más](https://www.channelfutures.com/mergers-acquisitions/20-of-users-looking-to-escape-to-vmware-alternatives)\n\n## Podcast Abrazando la Transformación Digital\n\n\n\nEn este episodio de su serie sobre la Adopción de Cero Confianza, Darren entrevista al CEO de SafeLiShare, Shamim Naqvi, para discutir sobre la seguridad de Cero Confianza y la colaboración de datos utilizando computación confidencial. Además, Darren quisiera agradecer a sus oyentes por su apoyo. Gracias a ustedes, el podcast tiene más de tres mil suscriptores y cuatro mil oyentes semanales. [Leer más](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW45-es","image":"./briefs/edw-45/es/thumbnail.png","lang":"es","summary":"Noticias de Transformación Digital para la semana del 10 de diciembre de 2023, contienen historias sobre ciberseguridad, inteligencia artificial y computación ubicua. Esta semana, escucha el plan del Ejército de los EE.UU. para mejorar la ciberseguridad a través de la unificación, las guerras de la nube llegan a los tribunales en el Reino Unido, y el drama de OpenAI y Sam Altman."},{"id":213,"type":"News Brief","title":"2023-12-16","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","innovation","intel","pc","aiworkforce","snowflake","aws","solix","aiintegration","britain","nationalgrid","chineserisks","hackers","infrastructure","vulnerabilities","nsa","russianthreats","digitaltransformation","openziti","zerotrust"],"body":"\n\n## Inteligencia Artificial\n\n\nEl último anuncio de Intel marca un momento crucial en el nacimiento de las computadoras impulsadas por IA. El lanzamiento de los procesadores de las series Core Ultra H y U introduce la revolucionaria arquitectura Meteor Lake, incorporando capacidades avanzadas de IA en computadoras portátiles ultrafinas. Este lanzamiento mejora el rendimiento y eleva la plataforma de gráficos Arc, llevando la experiencia del usuario a nuevas alturas. El compromiso de Intel con la innovación señala una era transformadora en el mercado de las computadoras portátiles, donde la IA se vuelve integral para la informática cotidiana. [Leer más](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\nLa concejala electa de la ciudad de Nueva York, Susan Zhuang, ha admitido silenciosamente que ha estado utilizando la inteligencia artificial para comunicarse con el público y responder a las consultas de los medios. ¡Finalmente una política que dice la verdad! El uso que Zhuang hace de la inteligencia artificial marca un enfoque único para relacionarse con los electores, y las implicaciones de esta estrategia impulsada por la tecnología podrían remodelar el panorama de la comunicación política en la era digital. [Leer más](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\nLa creación de la PC de inteligencia artificial personalizada por parte de Intel ha provocado una potencial gran disrupción laboral en las empresas a medida que continúan integrando la inteligencia artificial (IA) en sus operaciones. A medida que se adoptan las tecnologías de IA, se espera que cambien las funciones laborales y los días de trabajo, lo que genera preocupación acerca de los posibles desafíos para la fuerza laboral. Las empresas se están preparando para este cambio transformador y reconocen la necesidad de estrategias proactivas para navegar las posibles interrupciones y garantizar una transición sin problemas hacia un futuro impulsado por la IA. [Leer más](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Gestión de Datos\n\n\nCopito de Nieve, una plataforma líder de datos en la nube, ha logrado la Autorización FedRAMP Alta en AWS GovCloud EE.UU. Oeste y EE.UU. Este. Este hito importante destaca el compromiso de Copito de Nieve para cumplir con los estrictos estándares de seguridad del gobierno, permitiendo que las agencias federales aprovechen sus avanzadas capacidades de gestión de datos en un entorno en la nube seguro. La Autorización Alta FedRAMP subraya la posición de Copito de Nieve como un socio de confianza en la provisión de soluciones rápidas y escalables para entidades gubernamentales, fomentando la innovación y la eficiencia en la gestión de datos. [Leer más](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\nSolix ha introducido la Plataforma de Datos Común (CDP) tres punto cero, que incluye una gestión avanzada de datos multinube y capacidades de IA empresarial. Este lanzamiento refleja el compromiso de Solix a proporcionar soluciones de última generación para la gestión y obtención de conocimientos de grandes conjuntos de datos. Las mejoradas características de aprendizaje automático y IA de la CDP tres punto cero, permiten a las organizaciones tomar decisiones más informadas basadas en datos. La tendencia de la gestión de datos de multinube está impulsando el desarrollo de muchas nuevas ofertas comerciales, y la última versión de Solix está destinada a ser un contribuyente significativo en este espacio. [Leer más](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\nEn un informe reciente, Datanami explora consideraciones cruciales para garantizar que su estrategia de gestión de datos esté lista para la inteligencia artificial con cinco indicadores clave. Priorice la calidad de los datos, la escalabilidad, las capacidades de integración, una seguridad sólida y la flexibilidad. Estos elementos son cruciales para optimizar los sistemas a medida que la inteligencia artificial continúa dando forma a los procesos de datos. Manténgase a la vanguardia en la era de la inteligencia artificial evaluando y mejorando estos aspectos para satisfacer las demandas de las tecnologías en evolución. Lea más en datanami.com para obtener ideas integrales. [Leer más](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Ciberseguridad\n\n\n\nSe informa que la Red Nacional de Gran Bretaña ha roto lazos con un proveedor con sede en China debido a preocupaciones de ciberseguridad, según informó Reuters el diecisiete de diciembre de dos mil veintitrés. La decisión refleja un escrutinio global más intenso sobre los riesgos potenciales asociados con alianzas tecnológicas extranjeras, particularmente en sectores de infraestructura crítica. Esta acción de la Red Nacional subraya el creciente énfasis en reforzar medidas de ciberseguridad para proteger servicios esenciales contra amenazas potenciales. [Leer más](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\nLos hackers han atacado el suministro de agua de los Estados Unidos, elevando las alarmas sobre la vulnerabilidad de la infraestructura crítica. Axios informó el dieciséis de diciembre de dos mil veintitrés que esta infracción representa importantes preocupaciones de seguridad nacional y seguridad pública. El incidente resalta la necesidad urgente de medidas sólidas de ciberseguridad para proteger los servicios esenciales. Las autoridades están investigando la infracción, destacando los desafíos continuos en la defensa de la infraestructura crítica contra las amenazas cibernéticas. [Leer más](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\nLa Agencia de Seguridad Nacional ha advertido sobre actores cibernéticos rusos que explotan una vulnerabilidad conocida con implicaciones globales. La vulnerabilidad identificada tiene un impacto significativo en todo el mundo, enfatizando la importancia de mitigar los riesgos de ciberseguridad. La Agencia de Seguridad Nacional aconseja a las organizaciones que aborden y corrijan esta vulnerabilidad con prontitud para mejorar su resiliencia contra posibles amenazas cibernéticas. [Leer más](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Abrazando la Transformación Digital\n\n\n\nDarren busca en el código abierto la arquitectura de red de confianza cero con openZiti en colaboración con uno de los organizadores de la comunidad de código abierto, Philip Griffiths. Además, un agradecimiento especial a los oyentes de Embracing Digital Transformation por difundir las noticias, hemos añadido más de cuatro mil suscriptores a nuestro canal de YouTube y tuvimos más de catorce mil descargas o vistas de nuestro podcast la semana pasada. Gracias de nuevo por su apoyo. [Leer más](http://www.embracingdigital.org)\n\n## Inteligencia Artificial\n\n\n\nEl último anuncio de Intel marca un momento crucial en el nacimiento de las PC alimentadas por IA. El lanzamiento de los procesadores de las series Core Ultra H y U introduce la revolucionaria arquitectura Meteor Lake, que incorpora capacidades avanzadas de IA en notebooks ultrafinos. Este lanzamiento mejora el rendimiento y eleva la plataforma de gráficos Arc, llevando la experiencia del usuario a nuevas alturas. El compromiso de Intel con la innovación señala una era transformadora en el mercado de las laptops, donde la IA se convierte en un elemento integral de la informática cotidiana. [Leer más](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nSusan Zhuang, la concejal electa de la ciudad de Nueva York, ha admitido discretamente el uso de inteligencia artificial para comunicarse con el público y responder a consultas de los medios. ¡Por fin una política que habla la verdad! El uso de la inteligencia artificial por parte de Zhuang marca un enfoque único para interactuar con los constituyentes, y las implicaciones de esta estrategia impulsada por la tecnología pueden remodelar el panorama de la comunicación política en la era digital. [Leer más](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nLa creación de la PC de inteligencia artificial personalizada por parte de Intel ha traído una posible gran interrupción laboral a las empresas a medida que continúan integrando la inteligencia artificial (IA) en sus operaciones. A medida que se adoptan las tecnologías de IA, se espera que las funciones laborales y los días de trabajo cambien, lo que genera inquietudes sobre los posibles desafíos de la fuerza laboral. Las empresas se están preparando para este cambio transformador y reconociendo la necesidad de estrategias proactivas para navegar las posibles interrupciones y asegurar una transición suave hacia un futuro impulsado por la IA. [Leer más](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Gestión de Datos\n\n\n\nSnowflake, una plataforma líder de datos en la nube, ha logrado la Autorización FedRAMP Alta en AWS GovCloud Estados Unidos Oeste y Estados Unidos Este. Este hito importante destaca el compromiso de Snowflake con el cumplimiento de estrictos estándares de seguridad gubernamentales, permitiendo que las agencias federales aprovechen sus capacidades avanzadas de gestión de datos en un entorno seguro de nube. La Autorización Alta de FedRAMP subraya la posición de Snowflake como socio de confianza en la provisión de soluciones rápidas y escalables para entidades gubernamentales, fomentando la innovación y la eficiencia en la gestión de datos. [Leer más](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix ha introducido la Plataforma de Datos Comunes (CDP por sus siglas en inglés) tres punto cero, que incluye gestión avanzada de datos multi-nube y capacidades de inteligencia artificial empresarial. Este lanzamiento refleja el compromiso de Solix de proporcionar soluciones de última generación para administrar y obtener información de grandes conjuntos de datos. Las mejoradas características de aprendizaje automático e inteligencia artificial de la CDP tres punto cero permiten a las organizaciones tomar decisiones más informadas basadas en datos. La tendencia de la gestión de datos multi-nube está impulsando el desarrollo de muchas nuevas ofertas comerciales, y el último lanzamiento de Solix está preparado para ser un contribuyente significativo en este espacio. [Leer más](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n## Inteligencia Artificial\n\n\n\nEl último anuncio de Intel marca un momento decisivo en el nacimiento de las computadoras impulsadas por la IA. El lanzamiento de los procesadores de las series Core Ultra H y U introduce la revolucionaria arquitectura Meteor Lake, incorporando capacidades avanzadas de IA en notebooks ultrafinas. Este lanzamiento mejora el rendimiento y eleva la plataforma gráfica Arc, elevando la experiencia del usuario a nuevas alturas. El compromiso de Intel con la innovación señala una era transformadora en el mercado de portátiles, donde la IA se convierte en una parte integral de la informática cotidiana. [Leer más](https://www.anandtech.com/show/21185/intel-releases-core-ultra-h-and-u-series-processors-meteor-lake-brings-ai-and-arc-to-ultra-thin-notebooks)\n\n\n\nLa concejal electa de la Ciudad de Nueva York, Susan Zhuang, ha admitido emplear discretamente inteligencia artificial para comunicarse con el público y responder a las consultas de los medios de comunicación. ¡Finalmente una política que habla la verdad! El uso de inteligencia artificial por parte de Zhuang marca un enfoque único para interactuar con los electores y las implicancias de esta estrategia impulsada por la tecnología podrían remodelar el panorama de la comunicación política en la era digital. [Leer más](https://nypost.com/2023/12/16/metro/nyc-councilwoman-elect-susan-zhuang-admits-to-quietly-using-ai-to-communicate-with-the-public-answer-media-questions/)\n\n\n\nLa creación de la PC de inteligencia artificial personalizada por parte de Intel ha provocado una posible perturbación masiva de la mano de obra para las empresas a medida que continúan integrando la inteligencia artificial (IA) en sus operaciones. A medida que se adoptan las tecnologías de IA, se espera que las funciones laborales y los días de trabajo cambien, lo que genera preocupaciones sobre posibles desafíos para la fuerza laboral. Las compañías se están preparando para este cambio transformador y reconocen la necesidad de estrategias proactivas para navegar por posibles alteraciones y asegurar un cambio suave a un futuro impulsado por la IA. [Leer más](https://fortune.com/2023/12/12/businesses-prepare-ai-massive-labor-disruption-workday/)\n\n## Gestión de Datos\n\n\n\nEl copo de nieve, una plataforma de datos en la nube líder, ha conseguido la Autorización de Alto Nivel de FedRAMP en AWS GovCloud Estados Unidos Oeste y Estados Unidos Este. Este hito significativo resalta el compromiso de Snowflake para cumplir con los rigurosos estándares de seguridad del gobierno, permitiendo a las agencias federales aprovechar sus avanzadas capacidades de gestión de datos en un entorno seguro en la nube. La Autorización de Alto Nivel de FedRAMP subraya la posición de Snowflake como un socio de confianza en la provisión de soluciones rápidas y escalables para entidades gubernamentales, fomentando la innovación y eficiencia en la gestión de datos. [Leer más](https://www.snowflake.com/news/snowflake-achieves-fedramp-high-authorization-on-aws-govcloud-us-west-and-us-east/)\n\n\n\nSolix ha presentado la Plataforma de Datos Común (CDP) tres punto cero, que incluye avanzadas capacidades de administración de datos multi-nube y de inteligencia artificial empresarial. Este lanzamiento refleja el compromiso de Solix de proporcionar soluciones punteras para administrar y obtener conocimientos de grandes conjuntos de datos. Las mejoradas características de aprendizaje automático e inteligencia artificial del CDP tres punto cero permiten a las organizaciones tomar decisiones más informadas basadas en datos. La tendencia de la gestión de datos multi-nube está impulsando el desarrollo de muchas nuevas ofertas comerciales, y el último lanzamiento de Solix está listo para ser un importante contribuidor en este espacio. [Leer más](https://aithority.com/machine-learning/solix-common-data-platform-3-0-for-advanced-cloud-data-management-and-enterprise-ai/)\n\n\n\nEn un informe reciente, Datanami explora consideraciones cruciales para garantizar que su estrategia de gestión de datos esté preparada para la inteligencia artificial con cinco indicadores clave. Prioriza la calidad de los datos, la escalabilidad, las capacidades de integración, una seguridad sólida y flexibilidad. Estos elementos son cruciales para optimizar los sistemas a medida que la inteligencia artificial continúa dando forma a los procesos de datos. Manténgase a la vanguardia en la era de la inteligencia artificial evaluando y mejorando estos aspectos para satisfacer las demandas de las tecnologías en evolución. Lea más en datanami.com para obtener información completa. [Leer más](https://www.datanami.com/2023/12/08/is-your-data-management-strategy-ready-for-ai-5-ways-to-tell/)\n\n## Ciberseguridad\n\n\n\nLa Red Nacional de Gran Bretaña ha informado supuestamente ha roto relaciones con un proveedor con base en China debido a preocupaciones de ciberseguridad, según informó Reuters el diecisiete de diciembre de dos mil veintitrés. La decisión refleja el incremento en el escrutinio global sobre los riesgos potenciales asociados con las asociaciones tecnológicas extranjeras, particularmente en sectores de infraestructura crítica. Esta medida por parte de la Red Nacional subraya el creciente énfasis en reforzar las medidas de ciberseguridad para salvaguardar servicios esenciales contra potenciales amenazas. [Leer más](https://www.reuters.com/technology/cybersecurity/britains-national-grid-drops-china-based-supplier-over-cyber-security-fears-ft-2023-12-17/)\n\n\n\nLos hackers han atacado el suministro de agua de los Estados Unidos, provocando alarmas sobre la vulnerabilidad de la infraestructura crítica. Axios informó el dieciséis de diciembre de dos mil veintitrés que esta brecha representa preocupaciones significativas para la seguridad nacional y la seguridad pública. El incidente subraya la necesidad urgente de medidas sólidas de ciberseguridad para proteger los servicios esenciales. Las autoridades están investigando la brecha, destacando los desafíos continuos en la defensa de la infraestructura crítica contra las amenazas cibernéticas. [Leer más](https://www.axios.com/2023/12/16/hackers-us-water-supply)\n\n\n\nLa Agencia de Seguridad Nacional ha advertido sobre los actores cibernéticos rusos que explotan una vulnerabilidad conocida con implicaciones globales. La vulnerabilidad identificada tiene un impacto significativo en todo el mundo, enfatizando la importancia de mitigar los riesgos de ciberseguridad. La Agencia de Seguridad Nacional aconseja a las organizaciones que aborden y parcheen rápidamente esta vulnerabilidad para mejorar su resiliencia contra potenciales amenazas cibernéticas. [Leer más](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3616384/russian-cyber-actors-are-exploiting-a-known-vulnerability-with-worldwide-impact/)\n\n## Abrazando la Transformación Digital\n\n\n\nDarren busca la arquitectura de red de confianza cero en código abierto con openZiti junto a uno de los organizadores de la comunidad de código abierto, Philip Griffiths. Además, un agradecimiento especial a los oyentes de Embracing Digital Transformation por difundir las noticias, hemos agregado más de cuatro mil suscriptores a nuestro canal de YouTube y hemos tenido más de catorce mil descargas o vistas de nuestro podcast la semana pasada. Nuevamente, gracias por su apoyo. [Leer más](http://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW46-es","image":"./briefs/edw-46/es/thumbnail.png","lang":"es","summary":"La semana del 17 de diciembre de 2023, en noticias de Transformación Digital, incluye historias sobre IA, gestión de datos y ciberseguridad. Descubre el nacimiento de la PC IA, la gestión de datos Multi-cloud para nubes del gobierno, y advertencias importantes para la ciberseguridad de la infraestructura crítica."},{"id":214,"type":"News Brief","title":"2023-12-23","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","wordpress","plugindata","xwpspamshield","ontariohack","healthcarecyber","holidayhacks","ontarioincidents","artificialintel","generativeai","aiinschool","intelnervana","cloudcomputing","cloudcost","pentagoncloud","cisco","isovalent","openziti","podcast","digitaltransf"],"body":"\n\n## Ciberseguridad\n\n\n\nUn complemento deshonesto de WordPress ha puesto en peligro los datos de los usuarios al exponer direcciones de correo electrónico, nombres de usuario y contraseñas en texto plano. Los investigadores de seguridad descubrieron la brecha, instando a los usuarios afectados a restablecer sus credenciales inmediatamente. El complemento, denominado \"X-WP-SPAM-SHIELD-PRO\", es conocido por sus actividades maliciosas, destacando la importancia de las auditorías de seguridad regulares para los sitios de WordPress. Se recomienda a los administradores de sitios web que eliminen el complemento comprometido y mejoren las medidas de seguridad para prevenir el acceso no autorizado y la exposición de datos. [Leer más](https://thehackernews.com/2023/12/rogue-wordpress-plugin-exposes-e.html)\n\n\n\nLa CBC reporta un aumento en los ataques cibernéticos dirigidos a Ontario, Canadá, en el año dos mil veintitrés. Estos incidentes involucran una variedad de sectores, incluyendo la salud, los municipios y la educación. Los piratas informáticos explotan vulnerabilidades, causando interrupciones y exponiendo datos sensibles. Los expertos enfatizan la mejora de las medidas de ciberseguridad, la capacitación y la colaboración para protegerse contra las amenazas cibernéticas en evolución y proteger la infraestructura crítica. [Leer más](https://www.cbc.ca/news/canada/toronto/cybersecurity-ontario-incidents-2023-1.7048495)\n\n\n\nLos ataques cibernéticos durante las vacaciones de Navidad están aumentando por varias razones. En primer lugar, la actividad de compras en línea se intensifica, creando oportunidades para los ciberdelincuentes. En segundo lugar, los intentos de phishing han aumentado, donde los atacantes engañan a las víctimas usando correo electrónico o redes sociales. En tercer lugar, los empleados que trabajan de forma remota presentan un riesgo para la seguridad. En cuarto lugar, los ataques de ingeniería social pueden tener más éxito durante esta temporada. Finalmente, la posible negligencia de seguridad debido a las festividades también puede aumentar el riesgo de ataques. Es vital estar al tanto de estos riesgos y tomar medidas para mitigarlos. [Leer más](https://bit-sentinel.com/5-reasons-why-cyber-attacks-increase-during-the-christmas-holidays/)\n\n## Inteligencia Artificial\n\n\n\nEn el año dos mil veintitrés, la inteligencia artificial generativa hizo avances significativos en diversos sectores, incluyendo la atención sanitaria, las finanzas y el entretenimiento. Jim Cramer de CNBC resume el fin de año enfatizando las contribuciones de la IA en el descubrimiento de medicamentos, la modelización financiera y la creación de contenido. Se destaca el potencial transformador de la inteligencia artificial generativa, marcando su continua influencia en la innovación y los avances tecnológicos. [Leer más](https://www.cnbc.com/2023/12/21/jim-cramer-recaps-the-year-in-generative-artificial-intelligence.html)\n\n\n\nEl reciente artículo de Future-Ed explora el potencial de la integración de IA en las escuelas. Enfatiza la importancia de las consideraciones éticas y la implementación responsable para una integración exitosa. El artículo proporciona valiosos conocimientos para los educadores, legisladores y partes interesadas que navegan este paisaje en evolución. El podcast Aceptando la Transformación Digital también presentará entrevistas con estudiantes universitarios y sus profesores, discutiendo sus pensamientos. [Leer más](https://www.future-ed.org/navigating-the-artificial-intelligence-revolution-in-schools/)\n\n\n\nIntel ha presentado un nuevo chip, el Intel Nervana NCS-Uno, para acelerar las aplicaciones de inteligencia artificial (IA). El NCS-Uno está diseñado para mejorar el rendimiento de la IA, ofreciendo un alto rendimiento y eficiencia. Intel tiene como objetivo avanzar en la innovación de la IA proporcionando a los desarrolladores herramientas para implementar y escalar eficientemente los modelos de IA. El chip es parte del compromiso de Intel de potenciar las aplicaciones de IA en diversas industrias y representa un paso significativo en la estrategia de IA de la empresa. [Leer más](https://www.intc.com/news-events/press-releases/detail/1663/intel-accelerates-ai-everywhere-with-launch-of-powerful)\n\n## Computación Ubicua\n\n\n\nLas instituciones financieras, incluyendo Capital One y Arvest Bank, están priorizando el control de costos en la nube ante los desafíos. El enfoque está en optimizar los gastos a través de la alineación estratégica con los objetivos empresariales, empleando herramientas de gestión de costos en la nube e implementando prácticas de gobernanza. A medida que evoluciona la computación en la nube, la gestión de costos efectiva se vuelve crucial para estas organizaciones. [Leer más](https://www.ciodive.com/news/cloud-cost-control-capital-one-arvest-bank/703025/)\n\n\n\nEl Washington Post informa sobre la reconsideración del Pentágono de sus contratos de computación en la nube, sugiriendo posibles cambios en la estrategia. El artículo explora las implicaciones de estos cambios y su impacto en las grandes empresas de tecnología que pujan por lucrativos contratos de defensa. Mientras el Pentágono navega por el complejo panorama de la computación en la nube, el artículo destaca la evolución de la dinámica y el panorama competitivo en la búsqueda del sector de defensa de capacidades tecnológicas avanzadas. [Leer más](https://www.washingtonpost.com/technology/2023/12/21/pentagon-cloud-computing-contracts/)\n\n\n\nCisco está haciendo un movimiento estratégico en seguridad en la nube al adquirir Isovalent, un innovador en redes nativas de la nube y seguridad. Esta adquisición refleja el compromiso de Cisco con el avance de su inteligencia artificial (IA) y las capacidades de seguridad en la nube. Se espera que la experiencia de Isovalent en tecnología eBPF (Filtro de Paquetes de Berkeley extendido) mejore la capacidad de Cisco para asegurar entornos en la nube, proporcionando soluciones avanzadas para el paisaje de ciberseguridad en constante evolución en infraestructuras basadas en la nube. [Leer más](https://www.informationweek.com/it-infrastructure/cloud-computing#close-modal)\n\n## Abrazando la Transformación Digital\n\n\n\nEn primer lugar, me gustaría desear a todos mis oyentes una Feliz Navidad y un Próspero Año Nuevo. En el podcast de esta semana, Darren explora OpenZiti, una solución de red de confianza cero de código abierto. Además, gracias a todos los que están difundiendo la palabra sobre el podcast: ahora hemos superado los diez mil oyentes semanales, y su apoyo es muy apreciado. [Leer más](https://www.embracingdigital.org)\n\n## Ciberseguridad\n\n\n\nUn complemento deshonesto de WordPress ha puesto en peligro los datos de los usuarios al exponer direcciones de correo electrónico, nombres de usuario y contraseñas en texto sin formato. Los investigadores de seguridad descubrieron la fuga, instando a los usuarios afectados a restablecer sus credenciales inmediatamente. El complemento, llamado \"X-WP-SPAM-SHIELD-PRO\", es conocido por sus actividades malintencionadas, destacando la importancia de las auditorías de seguridad regulares para los sitios de WordPress. Se aconseja a los administradores de sitios web que eliminen el complemento comprometido y mejoren las medidas de seguridad para prevenir el acceso no autorizado y la exposición de datos. [Leer más](https://thehackernews.com/2023/12/rogue-wordpress-plugin-exposes-e.html)\n\n\n\nLa CBC informa de un auge en los ataques cibernéticos dirigidos a Ontario, Canadá, en dos mil veintitrés. Estos incidentes involucran una gama de sectores, incluidos el de la salud, los municipios y la educación. Los hackers explotan vulnerabilidades, causando interrupciones y exponiendo datos sensibles. Los expertos enfatizan la mejora de las medidas de seguridad cibernética, la formación y la colaboración para protegerse contra las amenazas cibernéticas en evolución y proteger la infraestructura crítica. [Leer más](https://www.cbc.ca/news/canada/toronto/cybersecurity-ontario-incidents-2023-1.7048495)\n\n\n\nLos ataques cibernéticos durante las vacaciones de Navidad están aumentando por varias razones. En primer lugar, la actividad de compras en línea se intensifica, creando oportunidades para los ciberdelincuentes. En segundo lugar, los intentos de phishing han aumentado, donde los atacantes engañan a las víctimas utilizando correo electrónico o redes sociales. En tercer lugar, los empleados que trabajan de forma remota presentan un riesgo de seguridad. En cuarto lugar, los ataques de ingeniería social pueden tener más éxito durante esta temporada. Finalmente, la negligencia de seguridad potencial debido a las festividades también puede aumentar el riesgo de ataques. Es vital estar consciente de estos riesgos y tomar medidas para mitigarlos. [Leer más](https://bit-sentinel.com/5-reasons-why-cyber-attacks-increase-during-the-christmas-holidays/)\n\n## Inteligencia Artificial\n\n\n\nEn el año dos mil veintitrés, la inteligencia artificial generativa logró avances significativos en diversos sectores, incluyendo la salud, las finanzas y el entretenimiento. Jim Cramer de CNBC resume el año enfatizando las contribuciones de la inteligencia artificial al descubrimiento de medicamentos, modelado financiero y creación de contenido. Se destaca el potencial transformador de la inteligencia artificial generativa, marcando su influencia continua en la innovación y los avances tecnológicos. [Leer más](https://www.cnbc.com/2023/12/21/jim-cramer-recaps-the-year-in-generative-artificial-intelligence.html)\n\n\n\nEl reciente artículo de Future-Ed explora el potencial de la integración de la inteligencia artificial en las escuelas. Hace hincapié en la importancia de las consideraciones éticas y una implementación responsable para una integración exitosa. El artículo proporciona valiosos conocimientos para educadores, responsables de políticas y partidarios que navegan este paisaje en evolución. El podcast Abrazando la Transformación Digital también presentará entrevistas con estudiantes universitarios y sus profesores, discutiendo sus pensamientos. [Leer más](https://www.future-ed.org/navigating-the-artificial-intelligence-revolution-in-schools/)\n\n\n\nIntel ha presentado un nuevo chip, el Intel Nervana NCS-uno, para acelerar las aplicaciones de inteligencia artificial (IA). El NCS-uno está diseñado para mejorar el rendimiento de la IA, ofreciendo una alta eficiencia y rendimiento. Intel tiene como objetivo avanzar en la innovación de la IA proporcionando a los desarrolladores herramientas para implementar y escalar modelos de IA de manera eficiente. El chip es parte del compromiso de Intel para potenciar las aplicaciones de IA en diversas industrias y representa un paso significativo en la estrategia de IA de la compañía. [Leer más](https://www.intc.com/news-events/press-releases/detail/1663/intel-accelerates-ai-everywhere-with-launch-of-powerful)\n\n## Computación Ubicua\n\n\n\nLas instituciones financieras, incluyendo Capital One y Arvest Bank, están priorizando el control de costos de la nube en medio de desafíos. El enfoque se centra en optimizar los gastos mediante la alineación estratégica con los objetivos de la empresa, empleando herramientas de gestión de costos en la nube y la implementación de prácticas de gobernanza. A medida que la computación en la nube evoluciona, la gestión efectiva de los costos se vuelve crucial para estas organizaciones. [Leer más](https://www.ciodive.com/news/cloud-cost-control-capital-one-arvest-bank/703025/)\n\n\n\nEl Washington Post informa sobre la reconsideración del Pentágono de sus contratos de computación en la nube, lo que sugiere posibles cambios de estrategia. El artículo explora las implicaciones de estos cambios y su impacto en las grandes compañías de tecnología que pujan por lucrativos contratos de defensa. A medida que el Pentágono navega por el complejo panorama de la computación en la nube, el artículo destaca la evolución de la dinámica y el competitivo panorama en la búsqueda del sector de defensa de capacidades tecnológicas avanzadas. [Leer más](https://www.washingtonpost.com/technology/2023/12/21/pentagon-cloud-computing-contracts/)\n\n\n\nCisco está realizando un movimiento estratégico en seguridad de la nube al adquirir Isovalent, un innovador en redes y seguridad nativas de la nube. Esta adquisición refleja el compromiso de Cisco con el avance de sus capacidades de inteligencia artificial (IA) y seguridad en la nube. Se espera que la experiencia de Isovalent en tecnología eBPF (filtro de paquetes de Berkeley extendido) mejore la capacidad de Cisco para asegurar entornos en la nube, proporcionando soluciones avanzadas para el paisaje de ciberseguridad en evolución en infraestructuras basadas en la nube. [Leer más](https://www.informationweek.com/it-infrastructure/cloud-computing#close-modal)\n\n## Abrazando la Transformación Digital\n\n\n\nEn primer lugar, me gustaría desear a todos mis oyentes una Feliz Navidad y un próspero Año Nuevo. En el podcast de esta semana, Darren explora OpenZiti, una solución de red de confianza cero de código abierto. Además, gracias a todos los que están difundiendo el mensaje sobre el podcast: ya hemos superado los diez mil oyentes semanales, y su apoyo es muy apreciado. [Leer más](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW47-es","image":"./briefs/edw-47/es/thumbnail.png","lang":"es","summary":"Durante la semana del 24 de diciembre de 2023, hubo varias noticias sobre transformación digital. Estas historias cubrieron temas como ciberseguridad, inteligencia artificial y computación. Algunos de los aspectos destacados de la semana incluyeron un aumento en los ciberataques durante la temporada de vacaciones, el esfuerzo de Intel para desarrollar aún más la tecnología de IA y la tendencia hacia el control de costos en la computación en la nube."},{"id":215,"type":"News Brief","title":"2023-12-31","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","edgecompute","digitaltransform","nsacyber","aics","tomtom","microsoft","mitresearch","aiethics","iot","healthcare","malwaredetect","patientdata","aiagriculture","predictive","genai","automation","navigation","malware"],"body":"\n\n## Ciberseguridad\n\n\n\nLa solución de ciberseguridad impulsada por inteligencia artificial de Checkpoint ofrece una plataforma revolucionaria con una tasa de precisión del noventa y ocho por ciento en la detección de actividad maliciosa. El sistema utiliza algoritmos avanzados y análisis predictivos para proporcionar a las empresas una poderosa defensa contra los riesgos cibernéticos en evolución, reforzando la seguridad digital durante amenazas incrementadas. [Leer más](https://fortune.com/2023/12/29/ai-cybersecurity-checkpoint/)\n\n\n\nLos ataques cibernéticos en las instalaciones de salud han aumentado, causando preocupaciones sobre la seguridad del paciente en el Hospital Liberty, que ha estado luchando contra un ataque durante las últimas dos semanas. Se necesitan medidas de ciberseguridad robustas para proteger la información médica sensible y garantizar servicios de atención médica ininterrumpidos. Esto destaca las vulnerabilidades más amplias de la infraestructura crítica ante las crecientes amenazas cibernéticas. [Leer más](https://www.kctv5.com/2023/12/30/liberty-hospital-staff-worries-patients-are-jeopardy-if-cyber-security-incident-drags/)\n\n\n\nEl informe de revisión del año dos mil veintitrés en ciberseguridad de la NSA ofrece ideas sobre las amenazas cibernéticas, defensas exitosas y tendencias emergentes. La publicación enfatiza el compromiso de la agencia de mejorar la resistencia cibernética nacional y es un recurso valioso para los profesionales de la industria, los responsables de políticas y el público. Proporciona un análisis retrospectivo del panorama de la ciberseguridad y orienta los esfuerzos futuros para fortalecer la seguridad digital. [Leer más](https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3621654/nsa-publishes-2023-cybersecurity-year-in-review/)\n\n## Inteligencia Artificial\n\n\n\nInvestigadores del MIT han desarrollado algoritmos que permiten a las máquinas entender matices contextuales en el lenguaje, promoviendo interacciones más precisas. Este avance en el procesamiento del lenguaje natural tiene aplicaciones potenciales en varios campos, cerrando la brecha entre la comunicación humana y la comprensión de la máquina. La investigación del MIT allana el camino para sistemas de inteligencia artificial más avanzados y conscientes del contexto. [Leer más](https://news.mit.edu/2023/leveraging-language-understand-machines-1222)\n\n\n\nLa inteligencia artificial y la automatización están transformando a los gobiernos al mejorar la eficiencia y la agilidad. Este cambio hacia un enfoque digital refleja un compromiso de adaptarse a la era moderna. El impacto de estas tecnologías está remodelando la gobernanza y proporcionando un enfoque más avanzado para enfrentar los desafíos digitales. [Leer más](https://federalnewsnetwork.com/commentary/2023/12/navigating-the-era-of-innovation-how-artificial-intelligence-and-automation-are-driving-a-digital-first-government/)\n\n\n\nMichael Cohen admite haber presentado documentos legales falsos generados por IA mientras trabajaba para Donald Trump, lo que genera preocupación sobre la ética de la IA y la supervisión en el sistema legal. Esto resalta la necesidad de una supervisión vigilante y salvaguardias para mantener la integridad de los procedimientos legales. El incidente subraya los desafíos en evolución de la ética de la IA y la integridad en los procesos legales. [Leer más](https://www.nbcnews.com/politics/politics-news/michael-cohen-says-unknowingly-submitted-fake-ai-generated-legal-cases-rcna131631)\n\n## Computación de Borde\n\n\n\nTomTom y Microsoft se han asociado para introducir una solución de inteligencia artificial generativa e innovadora para vehículos conectados. La tecnología utiliza algoritmos avanzados para transformar la navegación, prometiendo una experiencia de conducción más inteligente y receptiva. Este desarrollo significa un paso significativo hacia adelante en la integración de la inteligencia artificial en los automóviles. [Leer más](https://www.iottechnews.com/news/2023/dec/19/tomtom-microsoft-unveil-generative-ai-connected-vehicles/)\n\n\n\nPanasonic combate el malware de IoT desplegando trampas cibernéticas para detectar amenazas cibernéticas. La empresa obtiene valiosos conocimientos sobre las tácticas cambiantes del malware al crear objetivos simulados para atraer actividad maliciosa, mejorando las medidas de ciberseguridad. Este enfoque proactivo refleja el creciente énfasis en técnicas innovadoras para proteger dispositivos y redes conectados ante el aumento de los desafíos de seguridad. [Leer más](https://www.wired.com/story/panasonic-iot-malware-honeypots/)\n\n\n\nLa integración de las tecnologías de Inteligencia Artificial (IA) y del Internet de las Cosas (IoT) está transformando rápidamente la industria agrícola. Se pronostica que el mercado de la agricultura de precisión alcanzará los cinco mil doscientos millones de euros para el año dos mil veintisiete, gracias al uso innovador del IoT, el análisis de datos y la automatización en la agricultura. Esta tecnología optimiza los rendimientos de los cultivos, reduce el consumo de recursos y promueve una agricultura sostenible y eficiente a nivel mundial. [Leer más](https://iotbusinessnews.com/2023/12/22/53545-the-precision-agriculture-market-to-reach-e-5-2-billion-worldwide-in-2027/)\n\n## Abrazando la Transformación Digital\n\n\n\nEsta semana, el programa Embracing Digital Transformation continúa su serie Embracing Generative AI con entrevistas a una estudiante universitaria, Madeline Pulsipher, quien comparte su experiencia con GenAI, seguida de la perspectiva de una profesora, Laura Newey, sobre el uso de GenAI en el aula. ¡No te pierdas estos episodios reveladores! [Leer más](https://www.embracingdigital.org)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW48-es","image":"./briefs/edw-48/es/thumbnail.png","lang":"es","summary":"Noticias de Transformación Digital para la semana del 1 de enero de 2024, cubriendo ciberseguridad, inteligencia artificial y computación en el borde. Los temas incluyen IoT y AI en la granja, políticos engañados por alucinaciones generadas por AI y ataques cibernéticos en el sector de la salud."},{"id":216,"type":"News Brief","title":"2024-1-7","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","intel","articul8","digitalbridge","genai","ethics","china","itot","upskilling","airbus","atos","solix","dataplatform","microsoft","oracle","azure","database","digitaltransformation","generativeai"],"body":"\n\n## Inteligencia Artificial\n\n\n\nIntel anunció recientemente la escisión de la compañía de software de Inteligencia Artificial Articul8, respaldada por DigitalBridge. El movimiento está destinado a mejorar las capacidades de IA empresarial. Articul8 se centrará en el desarrollo de soluciones avanzadas de IA, aprovechando la experiencia de Intel en el campo. Este movimiento estratégico significa el compromiso de Intel para avanzar en las tecnologías de inteligencia artificial para aplicaciones industriales más amplias. [Leer más](https://www.reuters.com/technology/intel-spins-out-ai-software-firm-with-backing-digitalbridge-2024-01-03/)\n\n\n\nUn informe sobre GenAI de Asuntos Exteriores discute las preocupaciones sobre el impacto de la inteligencia artificial en los valores democráticos de los Estados Unidos. El informe destaca el potencial mal uso de la IA en actividades como la desinformación y la vigilancia, y enfatiza la necesidad de pautas éticas y marcos regulatorios para abordar estas preocupaciones y proteger los principios fundamentales de la democracia. [Leer más](https://www.foreignaffairs.com/united-states/artificial-intelligences-threat-democracy)\n\n\n\nChina ha publicado recientemente pautas para los investigadores de Inteligencia Artificial, impidiéndoles usar ciertos fondos para fines relacionados con la milicia. Las pautas buscan garantizar que la tecnología de Inteligencia Artificial se utilice para fines pacíficos y éticos, con un enfoque en evitar su mal uso en áreas que podrían dañar potencialmente la seguridad internacional. Esta medida resalta la determinación de China a moldear el desarrollo ético y la aplicación de la Inteligencia Artificial, enfatizando su compromiso con el uso responsable de la tecnología de Inteligencia Artificial. [Leer más](https://www.scmp.com/news/china/science/article/3247420/china-unveils-new-artificial-intelligence-guidelines-scientists-and-bans-use-funding-applications)\n\n## Ciberseguridad\n\n\n\nEl análisis de Inteligencia de Seguridad enfatiza la importancia de integrar TI y TO en ciberseguridad. Esta convergencia es crucial para proteger los sistemas industriales contra las amenazas cibernéticas. El análisis explora los desafíos y ventajas y destaca la necesidad de una estrategia de seguridad integral. [Leer más](https://securityintelligence.com/posts/it-and-ot-cybersecurity-integration/)\n\n\n\nEl aprendizaje continuo en ciberseguridad es crucial para abordar la brecha de habilidades. Un informe reciente aboga por mejorar las habilidades del talento existente e invertir en programas de capacitación para satisfacer la demanda de profesionales cualificados. Las organizaciones necesitan fomentar una cultura de mejora continua de habilidades para mejorar la resiliencia cibernética. [Leer más](https://www.informationweek.com/cyber-resilience/upskilling-is-the-secret-to-closing-the-cybersecurity-skills-gap-)\n\n\n\nAirbus está adquiriendo la unidad de ciberseguridad de Atos por dos mil millones de dólares, para fortalecer sus capacidades de ciberseguridad en los sectores de ICS-OT. Esta medida subraya la necesidad de contar con medidas de ciberseguridad sólidas en la infraestructura crítica, especialmente en industrias donde la tecnología operacional juega un papel fundamental. El acuerdo implica un paso estratégico hacia el fortalecimiento de la ciberseguridad dentro del marco operacional de Airbus. [Leer más](https://www.darkreading.com/ics-ot-security/airbus-acquire-atos-cybersecurity-unit-2-billion)\n\n## Gestión de Datos\n\n\n\nSolix ha revelado una Plataforma de Datos Empresariales para la era de \"Gen AI\". Su objetivo es ofrecer soluciones avanzadas de gestión de datos para satisfacer las necesidades en constante evolución de las tecnologías de IA. La plataforma de Solix se centra en optimizar las capacidades de procesamiento y almacenamiento de datos para apoyar las demandas de aplicaciones y análisis impulsados por la IA. Este movimiento significa un paso estratégico hacia la habilitación de la utilización eficiente de datos en el campo de la inteligencia artificial que avanza rápidamente. [Leer más](https://venturebeat.com/data-infrastructure/solix-launches-new-enterprise-data-platform-for-the-gen-ai-era/)\n\n\n\nMicrosoft y Oracle se han unido para mejorar la compatibilidad entre sus bases de datos, Microsoft SQL Server y Oracle Database, en la plataforma en la nube Azure de Microsoft. Esta asociación busca proporcionar una experiencia fluida para los usuarios que ejecutan estas bases de datos en Azure, con un rendimiento y flexibilidad mejorados. La colaboración tiene como objetivo satisfacer las demandas evolutivas de las soluciones de bases de datos basadas en la nube, respaldando la interoperabilidad y la facilidad de uso para las empresas que utilizan tanto las tecnologías de Microsoft como las de Oracle en Azure. [Leer más](https://www.infoq.com/news/2024/01/microsoft-oracle-database-azure/)\n\n\n\nEn dos mil veinticuatro, los datos empresariales y las tecnologías de inteligencia artificial están experimentando cambios significativos. Las tendencias incluyen el papel en expansión de la inteligencia artificial en la gestión de datos, un énfasis creciente en la ética de los datos y la privacidad, el ascenso de las analíticas aumentadas, y la influencia de la computación perimetral en el procesamiento de datos. Se insta a las organizaciones a adaptarse a estas dinámicas en evolución para seguir siendo competitivas y aprovechar el potencial completo de los datos y las tecnologías de inteligencia artificial este año. [Leer más](https://tdwi.org/articles/2024/01/05/ta-all-shifting-sands-in-enterprise-data-and-ai-technologies-in-2024.aspx)\n\n## Podcast de Abrazando la Transformación Digital\n\n\n\nEsta semana en el podcast, Darren continúa su serie sobre Abrazando la Inteligencia Artificial Generativa cuando entrevista a Laura Newey después de un semestre de enseñar inglés en la universidad con Chat GPT y la Inteligencia Artificial Generativa como herramienta. Tienes que ver este episodio y compartirlo con tus estudiantes universitarios. [Leer más](https://www.embracingdigital.org/en)\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW49-es","image":"./briefs/edw-49/es/thumbnail.png","lang":"es","summary":"Intel anunció recientemente la escisión de la compañía de software de IA Articul8, respaldada por DigitalBridge. El movimiento tiene como objetivo mejorar las capacidades de IA empresarial. Articul8 se centrará en desarrollar soluciones avanzadas de IA, aprovechando la experiencia de Intel en el campo. Esta movida estratégica significa el compromiso de Intel con el avance de las tecnologías de inteligencia artificial para aplicaciones industriales más amplias."},{"id":217,"type":"News Brief","title":"2023-3-5","tags":null,"body":"\n\n## Inteligencia Artificial\n\nLos chatbots basados en IA generativa están lejos de ser perfectos. ChatGPT, el chatbot de Bing de Microsoft, Galactica de Meta (una IA generativa diseñada para ayudar a los científicos con tareas como la anotación de proteínas o la escritura de código) y otros sistemas han sido retirados o limitados después de haberse descubierto que generan información poco confiable o incorrecta, o se han desviado hacia reacciones emocionales e incluso amenazas.\n\nLos gobiernos de EE.UU. y Europa están colaborando en un estudio de investigación sobre IA para desarrollar estrategias que regulen y fomenten la innovación en IA. El estudio abarcará cinco áreas: pronóstico del clima y el tiempo extremo, gestión de respuestas de emergencia, mejoras en salud y medicina, optimización de redes eléctricas y optimización agrícola. Los participantes incluyen el Departamento de Energía de EE. UU., el Departamento de Agricultura, el Pentágono, la Administración Nacional Oceánica y Atmosférica, agencias de salud y ciencia, y sus contrapartes europeas.\n\n[https://www.axios.com/2023/03/01/ai-research-us-eu](https://www.axios.com/2023/03/01/ai-research-us-eu)\n\nAirdot Deploy: Implementa automáticamente modelos de aprendizaje automático y los escala. Identifica automáticamente los paquetes necesarios, comprende la dependencia de módulos, reestructura el código, crea APIs REST alrededor del modelo de ML, lo empaqueta en un contenedor, inicia la infraestructura, lo escala automáticamente hacia arriba y hacia abajo, y configura alertas y monitoreo automatizados, todo con solo una línea de código. ¡Esto es mucho para tu equipo de DevOps!\n\n[https://www.airdot.io/blog/announcing-airdot-deploy-ml](https://www.airdot.io/blog/announcing-airdot-deploy-ml)\n\n## Ciberseguridad\n\nUn informe de Tenable revela que los ciberataques se llevan a cabo principalmente utilizando vulnerabilidades conocidas para las cuales ya existen parches disponibles. Los actores de amenazas continúan explotando vulnerabilidades que las organizaciones no han parchado o remediado, incluyendo fallas de alta gravedad en Microsoft Exchange y soluciones de red privada virtual.\n\n[https://www.helpnetsecurity.com/2023/03/03/known-exploitable-vulnerabilities/?web_view=true](https://www.helpnetsecurity.com/2023/03/03/known-exploitable-vulnerabilities/?web_view=true)\n\nLos CISO municipales enfrentan desafíos a medida que aumentan las amenazas cibernéticas, siendo los ataques de ransomware la mayor preocupación. Recientemente, Oakland, California, declaró estado de emergencia tras un ataque de ransomware por parte de Playgroup que interrumpió los sistemas telefónicos y los servicios no urgentes. Este ataque es uno más en una serie de ataques de ransomware a gobiernos locales en los Estados Unidos, incluidos aquellos en Baltimore, Nueva Orleans, Pensacola y Atlanta.\n\n[https://www.csoonline.com/article/3688958/municipal-cisos-grapple-with-challenges-as-cyber-threats-soar.html](https://www.csoonline.com/article/3688958/municipal-cisos-grapple-with-challenges-as-cyber-threats-soar.html)\n\nEl presidente Biden firmó la Ley de Preparación de Seguridad Cibernética de Computación Cuántica, que requiere que las agencias federales den prioridad al uso de tecnología resistente a la computación cuántica. La ley ordena la guía para evaluar sistemas críticos con estándares de criptografía post-cuántica. Esto es en respuesta a los temores de que la tecnología cuántica pueda hacer que la criptografía existente sea vulnerable a ser descifrada rápidamente.\n\n[https://fedscoop.com/biden-signs-quantum-computing-cybersecurity-act-into-law/](https://fedscoop.com/biden-signs-quantum-computing-cybersecurity-act-into-law/)\n\n## Computación Ubicua.\n\nEl informe anual de Accenture sobre la nube bancaria encontró que los bancos que apuntan a sistemas principales para migración a la nube enfrentan un factor de riesgo importante debido a la dificultad de encontrar talento en la nube. Los bancos perciben la migración de sistemas principales como de alto riesgo debido a la falta de personal con experiencia en la nube y la dificultad para reclutar las habilidades requeridas en el entorno actual. La adopción de la nube pública representa tanto una mejora de seguridad como un factor de riesgo, reflejado en la falta de adopción de la nube en la industria bancaria.\n\n[https://www.ciodive.com/news/banks-finance-cloud-migration-skils-gap-cybersecurity/644169/](https://www.ciodive.com/news/banks-finance-cloud-migration-skils-gap-cybersecurity/644169/)\n\nLa empresa de almacenes de datos en la nube, Snowflake, planea añadir más de 1.000 empleados en el año fiscal actual, según el director financiero Mike Scarpelli durante la llamada de ganancias del cuarto trimestre de la compañía. Esto sigue a la adición de 1.900 personas el año pasado, en contraste con la tendencia de reducción de personal entre otras grandes empresas tecnológicas. Snowflake planea priorizar la contratación en los departamentos de producto, ingeniería y ventas.\n\n[https://www.ciodive.com/news/Snowflake-expands-workforce-extends-AWS-partnership/644034/](https://www.ciodive.com/news/Snowflake-expands-workforce-extends-AWS-partnership/644034/)\n\nSegún los expertos de la industria, los CIOs deben reestructurar sus departamentos y estrategias de TI para aprovechar completamente la transformación en la nube. Neil Holden, CIO del Grupo Halfords, cree que los departamentos de TI deben operar de manera diferente debido a la nube y lo que significa para el negocio. Holden reorganizó su equipo para asegurarse de que pudieran maximizar las capacidades y oportunidades comerciales proporcionadas por la nube.\n\n[https://www.cio.com/article/463595/transforming-it-for-cloud-success.html](https://www.cio.com/article/463595/transforming-it-for-cloud-success.html)\n\n## Podcast de Abrazando la Transformación Digital.\n\nRevisa el episodio completo de esta semana \"Innovación como Servicio\" donde Darren entrevista a Andrew Cohen, Director Gerente de Netsurit.\n\n[https://www.embracingdigital.org/episode-EDT127](https://www.embracingdigital.org/episode-EDT127)\n\n\n\n","guests":null,"link":"/brief-EDW5-es","image":"./briefs/edw-5/es/thumbnail.png","lang":"es","summary":"Summary"},{"id":218,"type":"News Brief","title":"2024-1-14","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","sandworm","hacking","zyxelsecurity","miraibotnet","sechack","bitcoinhack","cryptojacking","openai","aijobs","airegulation","ainetworks","mitai","aiwarfare","taiwanai","israelai","edgecomputing","iotsecurity","5ginorbit","iridiumnetwork","digitaltransformation","zero5gtrust","dataassurance"],"body":"\n\n## Ciberseguridad\n\n\nUna nueva investigación realizada por Forescout cuestiona la atribución cibernética en los incidentes de hackeo, dificultando la identificación precisa de los culpables. Es posible que el grupo de hackeo Sandworm no haya estado involucrado en los ataques cibernéticos dirigidos a veintidós organizaciones energéticas danesas en mayo de dos mil veintitrés, que explotaron una falla de seguridad en el cortafuegos Zyxel y desplegaron variantes del botnet Mirai en hosts infectados a través de un vector de acceso inicial desconocido. Los hallazgos resaltan los desafíos en la atribución de amenazas cibernéticas. [Leer más](https://thehackernews.com/2024/01/new-findings-challenge-attribution-in.html)\n\n\nLa cuenta de Twitter de la Comisión de Bolsa y Valores de Estados Unidos (SEC) fue hackeada, con información falsa publicada sobre los fondos cotizados en bolsa de Bitcoin. El FBI está investigando el hackeo y la SEC ha confirmado que se produjo debido a que un individuo tomó el control de la cuenta a través de un tercero. El incidente genera preocupaciones sobre la vulnerabilidad de la plataforma a la desinformación. Los analistas de seguridad expresan su preocupación por la falta de autenticación de dos factores y los legisladores han criticado a la SEC por socavar potencialmente los mercados. [Leer más](https://www.wired.com/story/sec-x-account-hack-investigation/)\n\n\nLas autoridades ucranianas, con la ayuda de Europol y un proveedor de servicios en la nube, han arrestado a un hombre de veintinueve años en Mykolaiv por orquestar un sofisticado esquema de criptojacking. El sospechoso infiltró mil quinientos cuentas pertenecientes a una conocida empresa estadounidense utilizando herramientas de fuerza bruta personalizadas. Creó más de un millón de computadoras virtuales para garantizar la operación del malware, obteniendo más de dos millones de dólares en ganancias ilícitas. El criptojacking explota las credenciales comprometidas en las plataformas en la nube, permitiendo el uso no autorizado de recursos informáticos para minar criptomonedas. El arresto del sospechoso tuvo lugar el nueve de enero. [Leer más](https://thehackernews.com/2024/01/29-year-old-ukrainian-cryptojacking.html)\n\n## Inteligencia Artificial\n\n\nEl director ejecutivo de OpenAI, Sam Altman, ha expresado su preocupación acerca de la rápida penetración de la inteligencia artificial (IA) en la sociedad. La describe como la revolución tecnológica más rápida, destacando la velocidad con la que la sociedad necesita adaptarse. Altman cree que el progreso en la IA llevará a cambios en los empleos, pero también piensa que creará nuevas y mejores oportunidades. Altman discute la necesidad de un organismo regulador global para supervisar los robustos sistemas de IA, considerando su posible impacto en la sociedad y el equilibrio geopolítico. [Leer más](https://www.livemint.com/technology/gadgets/amazon-republic-day-sale-top-5-smartwatches-with-up-to-77-discounts-11705312172154.html)\n\n\nUno de los desafíos con la inteligencia artificial (IA) y las redes neuronales es comprender cómo funcionan. Los investigadores del MIT han desarrollado un nuevo sistema de IA que estudia y explica el comportamiento de las redes neuronales para abordar este problema. Este sistema utiliza modelos de lenguaje preentrenados y permite una mejor comprensión de los cálculos de la red neuronal. Además, los investigadores han introducido el punto de referencia FIND para evaluar la precisión de las técnicas de interpretación. A pesar de algunas limitaciones, el punto de referencia FIND es una herramienta valiosa para evaluar la eficacia de los procedimientos de interpretabilidad. [Leer más](https://www.marktechpost.com/2024/01/13/mit-researchers-developed-a-new-method-that-uses-artificial-intelligence-to-automate-the-explanation-of-complex-neural-networks/)\n\n\nTaiwán e Israel están utilizando la inteligencia artificial para abordar diferentes desafíos. Taiwán empleó estrategias innovadoras para contrarrestar la interferencia extranjera en sus elecciones, utilizando herramientas de IA para señalar contenido engañoso, estableciendo iniciativas contra la desinformación y monitoreando internet en busca de manipulación de información. Mientras tanto, el ejército de Israel utiliza un sistema de IA llamado \"el Evangelio\" para localizar objetivos más rápidamente, reduciendo las bajas civiles e identificando túneles y lanzadores de misiles de Hamas. A pesar de las preocupaciones sobre los errores algorítmicos y el número de muertos entre los ciudadanos palestinos, el uso de IA por parte de Israel se ve como un cambio de juego potencial en la guerra táctica. [Leer más](https://lynnwoodtimes.com/2024/01/14/artificial-intelligence-240114/)\n\n## Computación perimetral\n\n\n\nLos Estados Unidos y la Unión Europea han acordado un plan conjunto para un programa de etiquetado para el consumidor para productos de hogar inteligente y dispositivos conectados. El programa contará con un sello de confianza cibernética en el embalaje del dispositivo para indicar el cumplimiento con los estándares de seguridad. La Comisión Federal de Comunicaciones está liderando la iniciativa y tiene como objetivo finalizar la política y los estándares para finales de dos mil veinticuatro. El programa informará a los consumidores sobre la ciberseguridad de los dispositivos de Internet de las Cosas, promoviendo la conciencia y la seguridad. [Leer más](https://www.nextgov.com/cybersecurity/2024/01/eu-signs-iot-safety-label-plan/393328/)\n\n\n\nLa conectividad IoT 5G desde el espacio está en el horizonte, expandiendo la computación periférica más allá de la fabricación tradicional de ladrillo y mortero. Iridium Communications ha lanzado recientemente el Proyecto Stardust, un servicio de red 5G que aprovecha su constelación de satélites en órbita terrestre baja. El servicio está diseñado para soportar servicios 5G, dispositivos IoT, mensajería, servicios de emergencia y seguimiento de activos. Iridium comenzó a probar el servicio en el año dos mil veintitrés y planea lanzarlo comercialmente en el año dos mil veintiséis. [Leer más](https://www.sdxcentral.com/articles/news/iridiums-project-stardust-satellite-strategy-supports-5g-iot-from-space/2024/01/)\n\n\nEn el año dos mil veintitrés, el IoT vio desarrollos significativos como la Directiva de Ciberseguridad NIS2 de la UE, despidos por parte de los principales actores tecnológicos que afectaron al IoT, el 5G en el espacio e iniciativas impulsadas por la sostenibilidad. Renesas adquirió al fabricante de chips para IoT celular Sequans por doscientos cuarenta y nueve millones de dólares, mientras que las guerras en la nube del IoT se intensificaron con Google cerrando su servicio IoT Core. Pragmatic Semiconductor recaudó trescientos ochenta y nueve punto tres millones de dólares, y Samsara se convirtió en la acción de IoT con mejor rendimiento. India emprendió la implementación nacional de medidores inteligentes, y los avances combinados de la IA generativa y del IoT proporcionaron reparación guiada y enseñaron a robots con sistemas de visión. ¡Wow! ¡Qué año! [Leer más](https://iot-analytics.com/iot-2023-in-review/)\n\n## Abrazando la Transformación Digital\n\n\n\nEsta semana, Darren será el anfitrión de su podcast con dos entrevistas. La primera entrevista se centrará en la seguridad de Confianza Cero en cinco G, y la segunda se centrará en mejorar la Garantía de Datos en arquitecturas en la nube, enfocándose en abrazar la Confianza Cero. Al sintonizar, puedes aprender sobre las últimas transformaciones digitales. Tuvimos un gran mes el mes pasado, con más de sesenta mil escuchas al podcast. Gracias por compartir con tus amigos. [Leer más](https://www.embracingdigital.org/en)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW50-es","image":"./briefs/edw-50/es/thumbnail.png","lang":"es","summary":"Noticias de Transformación Digital para la semana del 15 de enero de 2024, incluyendo actualizaciones sobre Ciberseguridad, inteligencia artificial y computación en el borde. Esta semana, el hackeo de la SEC causa alarmas, la IA se utiliza para explicar la IA y el 5G se despliega en el espacio."},{"id":219,"type":"News Brief","title":"2024-1-21","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms","tapestorage","filestorage","cloudstorage","sql","nosql","cpra","dataprivacy","cisa","openai","pentagon","ibm","quantumcomputing","nuclearecurity","csuite","deepfake","mcafee","digitaltransformation"],"body":"\n\n## Gestión de Datos\n\n\nHammerspace ha integrado recientemente el almacenamiento en cinta en su sistema de archivos global para mejorar las capacidades de gestión de datos. Esta integración permite a los usuarios acceder y gestionar de manera eficiente los datos a través de diferentes medios de almacenamiento, promoviendo la flexibilidad en las soluciones de almacenamiento de archivos. La solución actual ofrecida por Hammerspace establece un sistema de archivos global que cubre múltiples centros de datos, nubes y límites de borde tradicionales. [Leer más](https://www.techtarget.com/searchstorage/news/366566738/Hammerspaces-global-file-system-now-includes-tape)\n\n<hr>\n\n\nLa gestión de datos heterogéneos es esencial para las empresas modernas. Toad Data Studio admite SQL, NoSQL y bases de datos en la nube, proporcionando capacidades integrales de gestión de bases de datos. La compatibilidad multiplataforma agiliza los flujos de trabajo, la optimización de consultas mejora el rendimiento de la base de datos y las herramientas de colaboración permiten un trabajo en equipo eficiente. Por lo tanto, Toad Data Studio es un recurso indispensable para gestionar y administrar bases de datos en diversos entornos. [Leer más](https://www.infoworld.com/article/3712250/toad-data-studio-manages-sql-nosql-and-cloud-databases.html)\n\n<hr>\n\n\nSi opera un negocio en California, es crucial prepararse para la próxima Ley de Derechos de Privacidad de California (CPRA) perfeccionando sus prácticas de privacidad de datos. Mejorando la gestión de datos, los mecanismos de consentimiento y las estrategias de cumplimiento, puede cumplir con los requisitos de la CPRA y mantenerse a la vanguardia en la protección de la privacidad del consumidor. Se recomienda implementar medidas proactivas para garantizar que su negocio cumpla completamente con la CPRA. [Leer más](https://www.jdsupra.com/legalnews/prepare-for-the-cpra-by-improving-your-7537199/)\n\n<hr>\n\n## Ciberseguridad\n\n\nLa CISA ha lanzado una guía de respuesta integral a incidentes de ciberseguridad adaptada a las empresas de servicios de agua. La guía está diseñada para ayudar a las empresas de agua a responder de manera efectiva a los incidentes de ciberseguridad y mitigar posibles amenazas cibernéticas. Proporciona valiosas ideas y sirve como recurso para que las empresas de agua refuercen sus capacidades de respuesta a incidentes. Este movimiento es un paso significativo en la lucha contra los ataques cibernéticos a la infraestructura crítica en el sector del agua. [Leer más](https://www.techtarget.com/searchsecurity/news/366566740/CISA-posts-incident-response-guide-for-water-utilities)\n\n<hr>\n\n\nOpenAI se ha asociado con el Pentágono en una iniciativa de ciberseguridad, lo que representa un cambio respecto a su postura anterior. Al aprovechar la experiencia de OpenAI, la asociación busca avanzar en las capacidades de ciberseguridad para la defensa. Este desarrollo destaca el papel en evolución de la IA en los esfuerzos de seguridad nacional, con OpenAI contribuyendo activamente a los proyectos de ciberseguridad en colaboración con el Pentágono. [Leer más](https://www.semafor.com/article/01/16/2024/openai-is-working-with-the-pentagon-on-cybersecurity-projects)\n\n<hr>\n\n\n\nIBM ha advertido que la aparición de la computación cuántica podría resultar en una amenaza severa para la ciberseguridad. La preocupación es que las computadoras cuánticas podrían romper los métodos de cifrado ampliamente utilizados, lo que requerirá el desarrollo de medidas de seguridad resistentes a lo cuántico. Se insta a las organizaciones a tomar medidas proactivas para prepararse para los desafíos futuros que plantea la computación cuántica en la ciberseguridad. [Leer más](https://www.bloomberg.com/news/articles/2024-01-17/quantum-computing-to-spark-cybersecurity-armageddon-ibm-says)\n\n<hr>\n\n## Inteligencia Artificial\n\n\nLa integración de la inteligencia artificial (IA) en los procesos de toma de decisiones dentro de la seguridad atómica plantea preguntas provocativas sobre cómo las tecnologías de IA podrían influir en la estabilidad, la dinámica de la confianza y el equilibrio estratégico en general en el contexto de las capacidades nucleares. Es esencial garantizar una implementación responsable de la IA para evitar consecuencias no deseadas que podrían amenazar la seguridad y la estabilidad globales. La necesidad crítica de una integración confiable de la IA enfatiza la importancia de proporcionar un entorno internacional seguro y estable en el contexto de las capacidades nucleares. [Leer más](https://warontherocks.com/2024/01/artificial-intelligence-and-nuclear-stability/)\n\n<hr>\n\n\nLa inteligencia artificial (IA) está en camino de perturbar la suite ejecutiva de tres formas significativas. Los ejecutivos pueden esperar cambios en los procesos de toma de decisiones, la dinámica de la fuerza laboral y el papel general de los líderes. A medida que la IA sigue evolucionando, su impacto transformador reforma los paradigmas de liderazgo tradicionales. Se anima a los ejecutivos a adaptarse a estos cambios y a aprovechar estratégicamente la IA para obtener una ventaja competitiva en el dinámico entorno empresarial. [Leer más](https://www.cio.com/article/1293438/3-ways-ai-is-set-to-disrupt-the-c-suite.html)\n\n<hr>\n\n\nMcAfee ha lanzado un sistema de detección de audio deepfake impulsado por inteligencia artificial para abordar la creciente amenaza del contenido de audio manipulado. La tecnología está diseñada para identificar y mitigar los riesgos asociados con el audio deepfake, enfatizando la necesidad de soluciones avanzadas de detección y prevención. Esto refleja la creciente importancia de la inteligencia artificial en la lucha contra las amenazas emergentes en medios manipulados y en la mejora de las medidas de ciberseguridad. [Leer más](https://www.artificialintelligence-news.com/2024/01/08/mcafee-unveils-ai-powered-deepfake-audio-detection/)\n\n<hr>\n\n## Abrazando la Transformación Digital\n\n\n\nEl último episodio de Darren en el podcast Aceptando la Transformación Digital profundiza en el mundo de la gestión de datos de fabricación de Investigación y Desarrollo. Analiza las complejidades de la recolección y análisis de datos, desde la innovadora investigación pura hasta el más predecible proceso de fabricación. Es una exploración perspicaz de los desafíos y oportunidades de la transformación digital en este campo. [Leer más](https://www.embracingdigital.org)\n\n<hr>\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW51-es","image":"./briefs/edw-51/es/thumbnail.png","lang":"es","summary":"Prepárese para las Noticias de Transformación Digital de esta semana para el 22 de enero de 2024, que presenta historias sobre gestión de datos, ciberseguridad e inteligencia artificial. Esta semana, tenemos noticias sobre los derechos de privacidad de datos en California, consejos sobre cómo proteger su organización de ciberataques relacionados con la IA y la computación cuántica, y una mirada a cómo se está utilizando la IA para ayudar al gobierno a tomar decisiones en tiempo de guerra."},{"id":220,"type":"News Brief","title":"2023-3-12","tags":null,"body":"\n\n## Inteligencia Artificial\n\nMicrosoft confirma que la nueva versión de su popular chatbot de inteligencia artificial podrá convertir texto en VIDEO. GPT-4 y futuros desarrollos cambiarán para siempre la forma en que pensamos sobre la inteligencia artificial. Esta poderosa IA puede procesar texto y números, videos, imágenes y más. Microsoft explicó que GPT-4 sería \"multimodal\". Holger Kenn, Director de Estrategia Empresarial en Microsoft Alemania, explicó que esto permitiría a la IA de la compañía traducir el texto del usuario en imágenes, música y video.\n\n¿IA para reemplazar actores? Imagina utilizar IA para crear una película sin contratar actores reales o escribir un libro completamente ilustrado de 200 páginas en solo un día. Sí, lo leíste bien, GPT-4 hace posible utilizar inteligencia artificial para casi cualquier cosa que puedas imaginar.\n\nSchrodinger: Un simulador de mercado financiero completamente interactivo e integrado con ChatGPT de múltiples agentes. Citadel, uno de los hedge funds más exitosos del mundo, está en conversaciones para asegurar una licencia empresarial de ChatGPT en toda la empresa. ChatGPT puede producir código correcto con una estrategia de ejecución de operaciones (simple).\n\n[https://www.youtube.com/watch?v=tvzO79V9uq4](https://www.youtube.com/watch?v=tvzO79V9uq4)\n\n## Gestión de datos.\n\n\"Cero\" es la nueva palabra del día, con la \"Alianza de Datos\" acuñando la palabra Zero-Copy para nuevas arquitecturas de datos distribuidos. La integración Zero-copy es un concepto que permite compartir datos entre diferentes sistemas sin copiarlos. El enfoque tradicional de copiar datos de un sistema a otro puede conducir a ineficiencias, problemas de consistencia de datos y vulnerabilidades de seguridad.\n\n[http://tdan.com/the-data-centric-revolution-zero-copy-integration/30462](http://tdan.com/the-data-centric-revolution-zero-copy-integration/30462)\n\nLa pandemia de COVID-19 aceleró la visión de datos en todas partes, en la nube, el centro de datos, laptops y dispositivos IoT. Esta expansión de datos ha causado que las organizaciones de TI reevalúen sus estrategias de datos. Se necesitan desarrollar nuevas estrategias y soluciones de gestión de datos distribuidos que sean más completas en la administración de tipos heterogéneos de datos, incluyendo datos estructurados, semi-estructurados y no estructurados.\n\n[https://www.engineeringnews.co.za/article/modern-data-management-platforms-are-vital-for-solving-modern-data-management-problems-2023-03-14/rep_id:4136](https://www.engineeringnews.co.za/article/modern-data-management-platforms-are-vital-for-solving-modern-data-management-problems-2023-03-14/rep_id:4136)\n\nLa complejidad de la gobernanza de datos está impulsando cambios organizacionales, incluida la aparición del papel de administrador de datos en las organizaciones. Los administradores de datos están comenzando a gestionar el acceso a los datos, las preocupaciones de privacidad y la gestión del ciclo de vida de los datos. La ubicación de los administradores de datos en la organización todavía está en debate mientras las organizaciones maduran hacia una centrada en los datos.\n\n[https://www.techtarget.com/searchdatamanagement/tip/Data-stewardship-Essential-to-data-governance-strategies](https://www.techtarget.com/searchdatamanagement/tip/Data-stewardship-Essential-to-data-governance-strategies)\n\n## Ciberseguridad\n\nUn kit de phishing de adversario en el medio (AiTM) de código abierto ha encontrado varios compradores en el mundo del cibercrimen por su capacidad para orquestar ataques a gran escala. La inteligencia de amenazas de Microsoft hace seguimiento al actor detrás del desarrollo del kit bajo el nombre emergente DEV-1101. Un ataque de phishing de AiTM típicamente implica que un actor de amenazas intenta robar e interceptar la contraseña y las cookies de sesión de un objetivo mediante el despliegue de un servidor proxy entre el usuario y el sitio web.\n\n[https://thehackernews.com/2023/03/microsoft-warns-of-large-scale-use-of.html](https://thehackernews.com/2023/03/microsoft-warns-of-large-scale-use-of.html)\n\nLa Agencia de Seguridad Cibernética e Infraestructura (CISA) ha advertido a las organizaciones que operan infraestructuras críticas sobre las vulnerabilidades del ransomware en sus dispositivos. El aviso incluye una lista de dispositivos que los atacantes pueden apuntar y recomienda que las organizaciones evalúen sus redes en busca de posibles riesgos. CISA tiene un programa piloto recién establecido llamado Advertencia de Vulnerabilidad de Ransomware (RVWP, por sus siglas en inglés) con dos objetivos: escanear las redes de entidades de infraestructura crítica y ayudar a las organizaciones vulnerables a solucionar las fallas antes de ser hackeadas.\n\n[https://www.bleepingcomputer.com/news/security/cisa-now-warns-critical-infrastructure-of-ransomware-vulnerable-devices/](https://www.bleepingcomputer.com/news/security/cisa-now-warns-critical-infrastructure-of-ransomware-vulnerable-devices/)\n\nUn informe de Drata destaca las principales tendencias de cumplimiento para 2023. El informe describe el cambio hacia la monitorización continua del cumplimiento y la necesidad de herramientas de automatización para lograr el cumplimiento. También destaca el creciente enfoque en las regulaciones de privacidad y la creciente importancia de la gestión del riesgo de proveedores. El informe recomienda que las empresas prioricen sus esfuerzos de cumplimiento, utilicen herramientas de automatización y se mantengan al día con los requisitos regulatorios en evolución para evitar posibles multas y daños a su reputación.\n\n[https://drata.com/resources/2023-compliance-trends](https://drata.com/resources/2023-compliance-trends)\n\n## Podcast de Abrazando la Transformación Digital\n\nEcha un vistazo al episodio de longitud completa de esta semana \"Cerrando la Brecha de Habilidades Digitales\", donde Darren entrevista a Jon Gottfried de Major League Hacking.\n\n[https://www.embracingdigital.org/episode-EDT128](https://www.embracingdigital.org/episode-EDT128)\n\n\n\n","guests":null,"link":"/brief-EDW6-es","image":"./briefs/edw-6/es/thumbnail.png","lang":"es","summary":"Summary"},{"id":221,"type":"News Brief","title":"2023-3-19","tags":["ai","compute","datamanagement"],"body":"\n\n\n## Inteligencia artificial\n\n## Cómputo ubicuo\n\n## Borde Inteligente\n\n## Gestión de datos\n\n## La Seguridad Cibernética\n\n## Comunicación Avanzada\n## Inteligencia Artificial\n\nEn la sección \"Potencial para comportamientos emergentes riesgosos\" del informe técnico de la compañía, OpenAI se asoció con el Centro de Investigación de Alineación para probar las habilidades de GPT-4. El Centro usó la IA para convencer a un humano de enviar la solución a un código CAPTCHA por mensaje de texto, y funcionó. \n\n(Note: As of my knowledge cutoff, GPT-4 does not exist yet. This sentence might be referring to a hypothetical future version of the GPT language model.)\n\n[https://gizmodo.com/gpt4-open-ai-chatbot-task-rabbit-chatgpt-1850227471](https://gizmodo.com/gpt4-open-ai-chatbot-task-rabbit-chatgpt-1850227471)\n\nFunciones de Copilot, los ejecutivos de Microsoft mencionaron la tendencia del software a producir respuestas inexactas, pero presentaron esto como algo que podría ser útil. Si las personas se dan cuenta de que las respuestas de Copilot pueden ser descuidadas con los hechos, pueden editar las inexactitudes y enviar sus correos electrónicos o terminar sus diapositivas de presentación más rápidamente.\n\n[https://www.cnbc.com/2023/03/16/microsoft-justifies-ais-usefully-wrong-answers.html?__source=sharebar|linkedin&par=sharebar](https://www.cnbc.com/2023/03/16/microsoft-justifies-ais-usefully-wrong-answers.html?__source=sharebar|linkedin&par=sharebar)\n\nGoogle Health anunció Med-PaLM 2, una nueva y revolucionaria versión de su gran modelo lingüístico médico. Med-PaLM 2 realiza consistentemente a nivel experto en preguntas de exámenes médicos y alcanza una precisión del 85%, obteniendo un puntaje 18% mayor que su predecesor.\n\n## Computación Ubicua\n\nPronto podrías estar nadando encima de un centro de datos 😊 La empresa tecnológica británica, Deep Green, planea instalar pequeños centros de datos en piscinas públicas de todo el Reino Unido. Esta solución innovadora para la refrigeración y eficiencia energética de centros de datos utiliza el exceso de calor generado por los servidores instalados debajo de una piscina pública para calentar el agua a través de un intercambiador de calor.\n\nEl gobierno de los Estados Unidos ha lanzado una nueva iniciativa para ayudar a las organizaciones a detectar fallas de seguridad en los servicios en la nube de Microsoft. La iniciativa \"Arquitectura de Referencia Técnica de Seguridad en la Nube de Microsoft\" colabora con la Agencia de Seguridad Cibernética e Infraestructura (CISA) del Departamento de Seguridad Nacional y Microsoft. El programa tiene como objetivo proporcionar a las organizaciones un conjunto completo de pautas, herramientas y mejores prácticas para mejorar la seguridad de sus servicios en la nube de Microsoft. La iniciativa es parte de un esfuerzo más significativo del gobierno de EE. UU. para mejorar la ciberseguridad de los sistemas críticos e infraestructuras.\n\n[https://www.techradar.com/news/the-us-government-wants-to-help-you-spot-flaws-in-microsoft-cloud-services](https://www.techradar.com/news/the-us-government-wants-to-help-you-spot-flaws-in-microsoft-cloud-services)\n\nSegún una encuesta reciente, el 83% de los Directores de Información (CIOs, por sus siglas en inglés) sienten que deben hacer más con menos en el 2023. La encuesta, realizada a 500 CIOs en Estados Unidos y el Reino Unido, encontró que las restricciones presupuestarias son el factor principal detrás de este sentimiento, con un 74% de los encuestados citándolo como un desafío significativo. Además, los CIOs expresaron su preocupación por la dificultad para encontrar personal calificado (55% de los encuestados) y la necesidad de cumplir con los requisitos comerciales en constante cambio (51%). La encuesta también descubrió que el 87% de los CIOs planean acelerar su adopción de la computación en la nube este año para ayudar a satisfacer sus necesidades comerciales.\n\n[https://www.cloudcomputing-news.net/news/2023/mar/13/83-of-cios-must-do-more-with-less-in-2023/](https://www.cloudcomputing-news.net/news/2023/mar/13/83-of-cios-must-do-more-with-less-in-2023/)\n\n## Gestión de datos.\n\nUna encuesta de TDWI encontró que las organizaciones están adoptando una estrategia de gestión de datos para mejorar la calidad de los datos y reducir silos. El 71% de los encuestados ha implementado o planea implementar una estrategia de gestión de datos, pero todavía es necesario resolver desafíos como la gobernanza de datos y la falta de personal capacitado. Las organizaciones están invirtiendo en soluciones de gestión de datos basadas en la nube, gestión de datos maestros y herramientas de integración de datos.\n\n[https://tdwi.org/articles/2023/03/23/diq-all-data-management-0323.aspx](https://tdwi.org/articles/2023/03/23/diq-all-data-management-0323.aspx)\n\nLa inteligencia artificial (IA) tiene el potencial de ayudar a resolver el problema de la sobrecarga de datos de TI. Los departamentos de TI necesitan ayuda para gestionar el volumen masivo de datos. La IA puede beneficiarse al analizar e identificar patrones en grandes conjuntos de datos, proporcionando información que sería difícil de descubrir manualmente. Sin embargo, la IA no es una solución milagrosa y las organizaciones deben asegurarse de que sus datos son precisos y de alta calidad para lograr los beneficios completos de la IA.\n\n[https://www.infoworld.com/article/3689668/can-ai-solve-it-s-eternal-data-problem.html](https://www.infoworld.com/article/3689668/can-ai-solve-it-s-eternal-data-problem.html)\n\nUn nuevo informe revela la importancia de la gestión de datos para las empresas. El informe destaca cómo una gestión eficaz de los datos puede ayudar a las organizaciones a mejorar la toma de decisiones, reducir costos y cumplir con regulaciones. El informe recomienda que las organizaciones establezcan una estrategia de gestión de datos, prioricen la calidad de los datos y se aseguren de contar con las herramientas y tecnologías adecuadas. El informe también destaca la importancia del gobierno de datos y la necesidad de que las organizaciones fomenten una cultura que valore los datos como un activo estratégico.\n\n[https://solutionsreview.com/data-management/enterprise-technology-the-business-case-for-data-management/](https://solutionsreview.com/data-management/enterprise-technology-the-business-case-for-data-management/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW7-es","image":"./briefs/edw-7/es/thumbnail.png","lang":"es","summary":"Resumen"},{"id":222,"type":"News Brief","title":"2023-3-26","tags":["ai","compute","cybersecurity"],"body":"\n\n\n## Inteligencia artificial\n\n## Cómputo ubicuo\n\n## Borde Inteligente\n\n## Gestión de datos\n\n## La Seguridad Cibernética\n\n## Comunicación Avanzada\n## Computación Ubicua.\n\nGordon Moore, el cofundador de Intel Corporation e inventor de la Ley de Moore, falleció el 24 de marzo de 2023 a los 92 años. Nacido en 1929, Moore co-fundó Intel en 1968 y se desempeñó como su CEO desde 1975 hasta 1987. Es mejor conocido por su predicción, la Ley de Moore, que establece que el número de transistores en un microchip se duplicaría aproximadamente cada dos años, lo que conduciría a un crecimiento exponencial en la potencia informática. Esta predicción se ha mantenido durante más de 50 años y ha desempeñado un papel vital en impulsar el avance de la industria tecnológica.\n\nEl 4004, el primer microprocesador de Intel, debutó en 1971 como el primer microprocesador comercialmente disponible. Diseñado inicialmente para calculadoras, rápidamente encontró aplicaciones en otras áreas, como controladores de semáforos y registradoras electrónicas de efectivo. El 4004 contenía 2.300 transistores en un solo chip y ayudó a establecer a Intel como un jugador importante en la industria de semiconductores. Hoy en día, los microprocesadores se utilizan en varios dispositivos, desde teléfonos inteligentes hasta supercomputadoras.\n\nIntel ha anunciado la disponibilidad de su última serie de procesadores Xeon Max Scalable, basada en su arquitectura Sapphire Rapids. La serie Xeon Max cuenta con 100 mil millones de transistores SuperFin de 10 nm, aceleración de IA y cifrado, y soporte PCIe 5.0, lo que proporciona un rendimiento y seguridad mejorados para las cargas de trabajo informáticas empresariales. Esto representa 40 millones de veces más transistores en el último procesador que en su primer procesador hace más de 50 años.\n\n[https://www.datacenterdynamics.com/en/news/intel-announces-xeon-max-sapphire-rapids-cpus/](https://www.datacenterdynamics.com/en/news/intel-announces-xeon-max-sapphire-rapids-cpus/)\n\n## Inteligencia Artificial\n\nChatGPT, un gran modelo de lenguaje entrenado por OpenAI, experimentó un problema técnico que causó que estuviera fuera de servicio durante varias horas. El problema, que fue causado por un error en el sistema, afectó la función de historial del modelo, lo que hizo imposible acceder a sus conversaciones previas. Desde entonces, OpenAI ha resuelto el problema y ChatGPT ahora está completamente operativo. El incidente sirve como recordatorio de los riesgos potenciales de depender de la IA y la importancia de tener sistemas sólidos para abordar problemas técnicos.\n\n[https://www.independent.co.uk/tech/chatgpt-down-bug-issue-history-b2306269.html](https://www.independent.co.uk/tech/chatgpt-down-bug-issue-history-b2306269.html)\n\nBard, un modelo de lenguaje de inteligencia artificial desarrollado por Google como competidor del GPT de OpenAI, ahora está disponible para uso público. Bard, que utiliza un enfoque diferente para el modelado del lenguaje que GPT, tiene como objetivo generar textos más creativos y diversos al permitir que los usuarios ingresen sus indicaciones y restricciones. Se espera que el lanzamiento de Bard para uso público acelere el desarrollo de nuevas aplicaciones de procesamiento de lenguaje natural y avance en el campo del modelado del lenguaje de IA.\n\n[https://www.bloomberg.com/news/articles/2023-03-21/google-chatgpt-rival-bard-now-open-to-public-use](https://www.bloomberg.com/news/articles/2023-03-21/google-chatgpt-rival-bard-now-open-to-public-use)\n\nUn nuevo método de cultivo, Synecoculture, implica plantar varias especies vegetales juntas en alta densidad. Sin embargo, puede ser un proceso complejo debido a las diferentes velocidades de crecimiento y temporadas. Los investigadores han desarrollado un robot para sembrar, podar y cosechar plantas en el denso crecimiento vegetativo para abordar este problema. El pequeño y flexible cuerpo del robot será útil en la Synecoculture a gran escala, lo que lo convierte en un paso esencial para lograr la agricultura sostenible y la neutralidad de carbono.\n\n[https://www.sciencedaily.com/releases/2023/03/230320102001.htm](https://www.sciencedaily.com/releases/2023/03/230320102001.htm)\n\n## Ciberseguridad\n\nLa Agencia de Seguridad Cibernética e Infraestructura de los Estados Unidos (CISA) ha lanzado una nueva herramienta para detectar actividades maliciosas en los servicios de nube de Microsoft. La herramienta Sparrow puede escanear entornos de Azure y Microsoft 365 en busca de signos de hacking y otras actividades no autorizadas. Sparrow utiliza datos de código abierto y algoritmos de inteligencia artificial para identificar posibles amenazas y proporcionar a los usuarios recomendaciones accionables para prevenir más daños. El lanzamiento de Sparrow es parte de los esfuerzos continuos de CISA para mejorar la seguridad de los sistemas basados en la nube y proteger contra ataques cibernéticos.\n\n[https://www.bleepingcomputer.com/news/security/new-cisa-tool-detects-hacking-activity-in-microsoft-cloud-services/](https://www.bleepingcomputer.com/news/security/new-cisa-tool-detects-hacking-activity-in-microsoft-cloud-services/)\n\nLos vehículos eléctricos de Tesla fueron hackeados dos veces en el concurso anual de explotación Pwn2Own organizado por la Iniciativa de Día Cero. Un equipo de investigadores logró explotar una vulnerabilidad en el sistema de infoentretenimiento de un Modelo 3 para tomar control de los faros, altavoces y otros métodos del vehículo. Otro equipo utilizó un error en el mismo enfoque para ejecutar código y acceder a los datos del coche. Tesla ha lanzado desde entonces un parche por aire para corregir las vulnerabilidades.\n\n[https://www.securityweek.com/tesla-hacked-twice-at-pwn2own-exploit-contest/](https://www.securityweek.com/tesla-hacked-twice-at-pwn2own-exploit-contest/)\n\nIntel ha anunciado su nueva plataforma Core vPro de 13ª generación, que cuenta con características de seguridad basadas en hardware diseñadas para reducir la superficie de ataque de la plataforma. La nueva plataforma incluye Intel Hardware Shield, que utiliza detección de amenazas a nivel de CPU para proporcionar un proceso de arranque seguro, y la Tecnología de Aplicación del Control de Flujo de Intel, que ayuda a evitar los ataques de programación orientada a retornos (ROP). La plataforma también incluye la Tecnología de Detección de Amenazas de Intel, que utiliza el aprendizaje automático para identificar posibles amenazas de seguridad. La plataforma Core vPro de 13ª generación está dirigida a clientes empresariales que buscan funciones avanzadas de seguridad para protegerse contra ataques cibernéticos cada vez más sofisticados.\n\n[https://www.securityweek.com/intel-boasts-attack-surface-reduction-with-new-13th-gen-core-vpro-platform/](https://www.securityweek.com/intel-boasts-attack-surface-reduction-with-new-13th-gen-core-vpro-platform/)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW8-es","image":"./briefs/edw-8/es/thumbnail.png","lang":"es","summary":"Resumen"},{"id":223,"type":"News Brief","title":"2023-4-2","tags":["ai","compute","edge","cybersecurity","datamanagement","advancedcomms"],"body":"\n\n## Ciberseguridad\n\nNuevo malware de macOS llamado MacStealer roba datos y contraseñas de iCloud Keychain. Un nuevo malware que roba información se centra en el sistema operativo macOS de Apple para extraer información sensible de dispositivos comprometidos. MacStealer es el último ejemplo de una amenaza que utiliza Telegram como plataforma de comando y control (C2) para filtrar datos.\n\nMicrosoft publica un parche para la falla de privacidad aCropalypse en las herramientas de captura de pantalla de Windows. Microsoft lanzó una actualización fuera de banda para abordar una falla que derrota la privacidad en su herramienta de edición de capturas de pantalla para Windows 10 y 11. La falla, llamada aCropalypse, podría permitir que actores maliciosos recuperen partes editadas de las capturas de pantalla, revelando potencialmente información sensible que podría haber sido recortada.\n\nOpenAI revela un error de Redis detrás del incidente de exposición de datos de usuario en ChatGPT. OpenAI reveló que un error en la biblioteca de código abierto Redis fue responsable de exponer información personal y títulos de chat de otros usuarios en el servicio ChatGPT de la startup a principios de esta semana. El fallo, que salió a la luz el 20 de marzo de 2023, permitió a ciertos usuarios ver descripciones breves de las conversaciones de otros usuarios desde la barra lateral del historial de chat, lo que provocó que la empresa tomara medidas rápidamente.\n\n## Inteligencia Artificial\n\nEsto es GRANDE: Elon Musk, Bill Gates y otros líderes de tecnología piden una pausa en la carrera de IA 'fuera de control'. Más de 1,100 firmantes destacados acaban de firmar una carta abierta pidiendo \"que todos los laboratorios de IA se detengan durante al menos 6 meses inmediatamente\". La carta, que también fue firmada por el CEO de OpenAI, dijo que la pausa debería aplicarse a sistemas de IA \"más poderosos que GPT-4\". También dijo que expertos independientes deberían utilizar la pausa propuesta para desarrollar e implementar conjuntamente un conjunto de protocolos compartidos para herramientas de IA que sean seguras \"más allá de toda duda razonable\".\n\n[https://techcrunch.com/2023/03/28/1100-notable-signatories-just-signed-an-open-letter-asking-all-ai-labs-to-immediately-pause-for-at-least-6-months/](https://techcrunch.com/2023/03/28/1100-notable-signatories-just-signed-an-open-letter-asking-all-ai-labs-to-immediately-pause-for-at-least-6-months/)\n\nCerebras Systems publica siete nuevos modelos GPT entrenados en sistemas de escala de wafer CS-2. Es la primera vez que una empresa utiliza sistemas de inteligencia artificial no basados en GPU para entrenar LLMs de hasta 13 mil millones de parámetros, y está compartiendo los modelos, pesos y recetas de entrenamiento a través de la licencia Apache 2.0 estándar en la industria. La serie consta de siete modelos GPT con entre 111M y 13B de parámetros. Este trabajo, que suele llevar varios meses, se completó en pocas semanas. Los siete modelos Cerebras-GPT están disponibles de inmediato en Hugging Face y Cerebras Model Zoo en GitHub.\n\n[https://www.marketwatch.com/press-release/cerebras-systems-releases-seven-new-gpt-models-trained-on-cs-2-wafer-scale-systems-2023-03-28](https://www.marketwatch.com/press-release/cerebras-systems-releases-seven-new-gpt-models-trained-on-cs-2-wafer-scale-systems-2023-03-28)\n\nGoogle Bard con Wordle - podría haber ido mejor. Después de corregir al bot y recordarle que solo adivinara palabras de cinco letras, se disculpó y regresó con SLANTS, luego continuó con diferentes variaciones de la palabra SLANT ☹.\n\n[https://www.techradar.com/news/i-tried-to-use-google-bard-to-help-me-with-wordle-but-it-didnt-go-well](https://www.techradar.com/news/i-tried-to-use-google-bard-to-help-me-with-wordle-but-it-didnt-go-well)\n\n## Borde Inteligente\n\nBT se ha asociado con Amazon Web Services (AWS) para probar la informática periférica 5G en Manchester utilizando AWS Wavelength. El ensayo investigará cómo la informática periférica puede mejorar el rendimiento de la red 5G de BT en tres áreas clave: realidad aumentada, juegos inmersivos y automatización industrial. El objetivo es mejorar la experiencia del cliente proporcionando servicios más rápidos y confiables. El ensayo es parte de una colaboración más amplia entre las dos empresas para explorar el potencial de la informática periférica en el Reino Unido.\n\n[https://www.edgecomputing-news.com/2023/03/29/bt-uses-aws-wavelength-for-5g-edge-trial-in-manchester/](https://www.edgecomputing-news.com/2023/03/29/bt-uses-aws-wavelength-for-5g-edge-trial-in-manchester/)\n\nComcast ha anunciado asociaciones ampliadas con el objetivo de mejorar la conectividad en edificios y entornos inteligentes. Las asociaciones incluyen acuerdos con Sensus, proveedor de infraestructura avanzada de medición, y BuildingIQ, proveedor de soluciones de gestión de energía basadas en inteligencia artificial. El objetivo es proporcionar una gestión de energía más inteligente, mejor gestión del agua y automatización de edificios mejorada. Estas asociaciones forman parte de los esfuerzos de Comcast para mejorar la conectividad para clientes comerciales a través de su red de puntos de acceso Wi-Fi, sensores y otros dispositivos de IoT.\n\n[https://www.edgeir.com/comcast-focuses-connectivity-on-smarter-buildings-environments-with-expanded-partnerships-20230331](https://www.edgeir.com/comcast-focuses-connectivity-on-smarter-buildings-environments-with-expanded-partnerships-20230331)\n\n\n\n","guests":["Darren W Pulsipher"],"link":"/brief-EDW9-es","image":"./briefs/edw-9/es/thumbnail.png","lang":"es","summary":"Resumen"}]